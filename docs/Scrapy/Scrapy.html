<!DOCTYPE HTML>
<html lang="en" class="sidebar-visible no-js light">
    <head>
        <!-- Book generated using mdBook -->
        <meta charset="UTF-8">
        <title>Scrapy - KnowledgeBase</title>


        <!-- Custom HTML head -->
        
        <meta name="description" content="">
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <meta name="theme-color" content="#ffffff" />

        <link rel="icon" href="../favicon.svg">
        <link rel="shortcut icon" href="../favicon.png">
        <link rel="stylesheet" href="../css/variables.css">
        <link rel="stylesheet" href="../css/general.css">
        <link rel="stylesheet" href="../css/chrome.css">
        <link rel="stylesheet" href="../css/print.css" media="print">

        <!-- Fonts -->
        <link rel="stylesheet" href="../FontAwesome/css/font-awesome.css">
        <link rel="stylesheet" href="../fonts/fonts.css">

        <!-- Highlight.js Stylesheets -->
        <link rel="stylesheet" href="../highlight.css">
        <link rel="stylesheet" href="../tomorrow-night.css">
        <link rel="stylesheet" href="../ayu-highlight.css">

        <!-- Custom theme stylesheets -->

    </head>
    <body>
    <div id="body-container">
        <!-- Provide site root to javascript -->
        <script>
            var path_to_root = "../";
            var default_theme = window.matchMedia("(prefers-color-scheme: dark)").matches ? "navy" : "light";
        </script>

        <!-- Work around some values being stored in localStorage wrapped in quotes -->
        <script>
            try {
                var theme = localStorage.getItem('mdbook-theme');
                var sidebar = localStorage.getItem('mdbook-sidebar');

                if (theme.startsWith('"') && theme.endsWith('"')) {
                    localStorage.setItem('mdbook-theme', theme.slice(1, theme.length - 1));
                }

                if (sidebar.startsWith('"') && sidebar.endsWith('"')) {
                    localStorage.setItem('mdbook-sidebar', sidebar.slice(1, sidebar.length - 1));
                }
            } catch (e) { }
        </script>

        <!-- Set the theme before any content is loaded, prevents flash -->
        <script>
            var theme;
            try { theme = localStorage.getItem('mdbook-theme'); } catch(e) { }
            if (theme === null || theme === undefined) { theme = default_theme; }
            var html = document.querySelector('html');
            html.classList.remove('no-js')
            html.classList.remove('light')
            html.classList.add(theme);
            html.classList.add('js');
        </script>

        <!-- Hide / unhide sidebar before it is displayed -->
        <script>
            var html = document.querySelector('html');
            var sidebar = null;
            if (document.body.clientWidth >= 1080) {
                try { sidebar = localStorage.getItem('mdbook-sidebar'); } catch(e) { }
                sidebar = sidebar || 'visible';
            } else {
                sidebar = 'hidden';
            }
            html.classList.remove('sidebar-visible');
            html.classList.add("sidebar-" + sidebar);
        </script>

        <nav id="sidebar" class="sidebar" aria-label="Table of contents">
            <div class="sidebar-scrollbox">
                <ol class="chapter"><li class="chapter-item expanded "><a href="../Scrapy/Scrapy.html" class="active"><strong aria-hidden="true">1.</strong> Scrapy</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="../Scrapy/Scrapy1.html"><strong aria-hidden="true">1.1.</strong> Scrapy1</a></li><li class="chapter-item expanded "><a href="../Scrapy/Scrapy2.html"><strong aria-hidden="true">1.2.</strong> Scrapy2</a></li><li class="chapter-item expanded "><a href="../Scrapy/Scrapy3.html"><strong aria-hidden="true">1.3.</strong> Scrapy3</a></li><li class="chapter-item expanded "><a href="../Scrapy/Scrapy4.html"><strong aria-hidden="true">1.4.</strong> Scrapy4</a></li><li class="chapter-item expanded "><a href="../Scrapy/Scrapy5.html"><strong aria-hidden="true">1.5.</strong> Scrapy5</a></li><li class="chapter-item expanded "><a href="../Scrapy/Scrapy6.html"><strong aria-hidden="true">1.6.</strong> Scrapy6</a></li><li class="chapter-item expanded "><a href="../Scrapy/Scrapy7.html"><strong aria-hidden="true">1.7.</strong> Scrapy7</a></li><li class="chapter-item expanded "><a href="../Scrapy/Scrapy8.html"><strong aria-hidden="true">1.8.</strong> Scrapy8</a></li><li class="chapter-item expanded "><a href="../Scrapy/Scrapy9.html"><strong aria-hidden="true">1.9.</strong> Scrapy9</a></li><li class="chapter-item expanded "><a href="../Scrapy/Scrapy10.html"><strong aria-hidden="true">1.10.</strong> Scrapy10</a></li></ol></li><li class="chapter-item expanded "><a href="../ThinkPython/ThinkPython.html"><strong aria-hidden="true">2.</strong> ThinkPython</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="../ThinkPython/part_1.html"><strong aria-hidden="true">2.1.</strong> ThinkPython1</a></li><li class="chapter-item expanded "><a href="../ThinkPython/part_2.html"><strong aria-hidden="true">2.2.</strong> ThinkPython2</a></li><li class="chapter-item expanded "><a href="../ThinkPython/part_3.html"><strong aria-hidden="true">2.3.</strong> ThinkPython3</a></li><li class="chapter-item expanded "><a href="../ThinkPython/part_4.html"><strong aria-hidden="true">2.4.</strong> ThinkPython4</a></li><li class="chapter-item expanded "><a href="../ThinkPython/part_5.html"><strong aria-hidden="true">2.5.</strong> ThinkPython5</a></li><li class="chapter-item expanded "><a href="../ThinkPython/part_6.html"><strong aria-hidden="true">2.6.</strong> ThinkPython6</a></li><li class="chapter-item expanded "><a href="../ThinkPython/part_7.html"><strong aria-hidden="true">2.7.</strong> ThinkPython7</a></li><li class="chapter-item expanded "><a href="../ThinkPython/part_8.html"><strong aria-hidden="true">2.8.</strong> ThinkPython8</a></li><li class="chapter-item expanded "><a href="../ThinkPython/part_9.html"><strong aria-hidden="true">2.9.</strong> ThinkPython9</a></li><li class="chapter-item expanded "><a href="../ThinkPython/part_10.html"><strong aria-hidden="true">2.10.</strong> ThinkPython10</a></li></ol></li><li class="chapter-item expanded "><a href="../C-sharp-docs/C-sharp-docs.html"><strong aria-hidden="true">3.</strong> C-sharp-docs</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="../C-sharp-docs/part_1.html"><strong aria-hidden="true">3.1.</strong> Csharp1</a></li><li class="chapter-item expanded "><a href="../C-sharp-docs/part_2.html"><strong aria-hidden="true">3.2.</strong> Csharp2</a></li><li class="chapter-item expanded "><a href="../C-sharp-docs/part_3.html"><strong aria-hidden="true">3.3.</strong> Csharp3</a></li><li class="chapter-item expanded "><a href="../C-sharp-docs/part_4.html"><strong aria-hidden="true">3.4.</strong> Csharp4</a></li><li class="chapter-item expanded "><a href="../C-sharp-docs/part_5.html"><strong aria-hidden="true">3.5.</strong> Csharp5</a></li><li class="chapter-item expanded "><a href="../C-sharp-docs/part_6.html"><strong aria-hidden="true">3.6.</strong> Csharp6</a></li><li class="chapter-item expanded "><a href="../C-sharp-docs/part_7.html"><strong aria-hidden="true">3.7.</strong> Csharp7</a></li><li class="chapter-item expanded "><a href="../C-sharp-docs/part_8.html"><strong aria-hidden="true">3.8.</strong> Csharp8</a></li><li class="chapter-item expanded "><a href="../C-sharp-docs/part_9.html"><strong aria-hidden="true">3.9.</strong> Csharp9</a></li><li class="chapter-item expanded "><a href="../C-sharp-docs/part_10.html"><strong aria-hidden="true">3.10.</strong> Csharp10</a></li></ol></li></ol>
            </div>
            <div id="sidebar-resize-handle" class="sidebar-resize-handle"></div>
        </nav>

        <div id="page-wrapper" class="page-wrapper">

            <div class="page">
                                <div id="menu-bar-hover-placeholder"></div>
                <div id="menu-bar" class="menu-bar sticky bordered">
                    <div class="left-buttons">
                        <button id="sidebar-toggle" class="icon-button" type="button" title="Toggle Table of Contents" aria-label="Toggle Table of Contents" aria-controls="sidebar">
                            <i class="fa fa-bars"></i>
                        </button>
                        <button id="theme-toggle" class="icon-button" type="button" title="Change theme" aria-label="Change theme" aria-haspopup="true" aria-expanded="false" aria-controls="theme-list">
                            <i class="fa fa-paint-brush"></i>
                        </button>
                        <ul id="theme-list" class="theme-popup" aria-label="Themes" role="menu">
                            <li role="none"><button role="menuitem" class="theme" id="light">Light</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="rust">Rust</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="coal">Coal</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="navy">Navy</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="ayu">Ayu</button></li>
                        </ul>
                    </div>

                    <h1 class="menu-title">KnowledgeBase</h1>

                    <div class="right-buttons">
                        <a href="../print.html" title="Print this book" aria-label="Print this book">
                            <i id="print-button" class="fa fa-print"></i>
                        </a>

                    </div>
                </div>


                <!-- Apply ARIA attributes after the sidebar and the sidebar toggle button are added to the DOM -->
                <script>
                    document.getElementById('sidebar-toggle').setAttribute('aria-expanded', sidebar === 'visible');
                    document.getElementById('sidebar').setAttribute('aria-hidden', sidebar !== 'visible');
                    Array.from(document.querySelectorAll('#sidebar a')).forEach(function(link) {
                        link.setAttribute('tabIndex', sidebar === 'visible' ? 0 : -1);
                    });
                </script>

                <div id="content" class="content">
                    <main>
                        <hr />
<h2>generator: &quot;Docutils 0.17.1: http://docutils.sourceforge.net/&quot;
lang: en
title: Scrapy 2.11.0 documentation
viewport: width=device-width, initial-scale=1.0</h2>
<p>::: wy-grid-for-nav
::: wy-side-scroll
::: wy-side-nav-search
<a href="#">Scrapy</a>{.icon .icon-home}</p>
<p>::: version
master
:::
:::</p>
<p>::: {.wy-menu .wy-menu-vertical spy=&quot;affix&quot; role=&quot;navigation&quot; aria-label=&quot;Navigation menu&quot;}
[First steps]{.caption-text}</p>
<ul>
<li><a href="index.html#document-intro/overview">Scrapy at a glance</a>{.reference
.internal}</li>
<li><a href="index.html#document-intro/install">Installation guide</a>{.reference
.internal}</li>
<li><a href="index.html#document-intro/tutorial">Scrapy Tutorial</a>{.reference
.internal}</li>
<li><a href="index.html#document-intro/examples">Examples</a>{.reference .internal}</li>
</ul>
<p>[Basic concepts]{.caption-text}</p>
<ul>
<li><a href="index.html#document-topics/commands">Command line tool</a>{.reference
.internal}</li>
<li><a href="index.html#document-topics/spiders">Spiders</a>{.reference .internal}</li>
<li><a href="index.html#document-topics/selectors">Selectors</a>{.reference
.internal}</li>
<li><a href="index.html#document-topics/items">Items</a>{.reference .internal}</li>
<li><a href="index.html#document-topics/loaders">Item Loaders</a>{.reference
.internal}</li>
<li><a href="index.html#document-topics/shell">Scrapy shell</a>{.reference
.internal}</li>
<li><a href="index.html#document-topics/item-pipeline">Item Pipeline</a>{.reference
.internal}</li>
<li><a href="index.html#document-topics/feed-exports">Feed exports</a>{.reference
.internal}</li>
<li><a href="index.html#document-topics/request-response">Requests and
Responses</a>{.reference
.internal}</li>
<li><a href="index.html#document-topics/link-extractors">Link
Extractors</a>{.reference
.internal}</li>
<li><a href="index.html#document-topics/settings">Settings</a>{.reference
.internal}</li>
<li><a href="index.html#document-topics/exceptions">Exceptions</a>{.reference
.internal}</li>
</ul>
<p>[Built-in services]{.caption-text}</p>
<ul>
<li><a href="index.html#document-topics/logging">Logging</a>{.reference .internal}</li>
<li><a href="index.html#document-topics/stats">Stats Collection</a>{.reference
.internal}</li>
<li><a href="index.html#document-topics/email">Sending e-mail</a>{.reference
.internal}</li>
<li><a href="index.html#document-topics/telnetconsole">Telnet
Console</a>{.reference
.internal}</li>
</ul>
<p>[Solving specific problems]{.caption-text}</p>
<ul>
<li><a href="index.html#document-faq">Frequently Asked Questions</a>{.reference
.internal}</li>
<li><a href="index.html#document-topics/debug">Debugging Spiders</a>{.reference
.internal}</li>
<li><a href="index.html#document-topics/contracts">Spiders Contracts</a>{.reference
.internal}</li>
<li><a href="index.html#document-topics/practices">Common Practices</a>{.reference
.internal}</li>
<li><a href="index.html#document-topics/broad-crawls">Broad Crawls</a>{.reference
.internal}</li>
<li><a href="index.html#document-topics/developer-tools">Using your browser's Developer Tools for
scraping</a>{.reference
.internal}</li>
<li><a href="index.html#document-topics/dynamic-content">Selecting dynamically-loaded
content</a>{.reference
.internal}</li>
<li><a href="index.html#document-topics/leaks">Debugging memory
leaks</a>{.reference .internal}</li>
<li><a href="index.html#document-topics/media-pipeline">Downloading and processing files and
images</a>{.reference
.internal}</li>
<li><a href="index.html#document-topics/deploy">Deploying Spiders</a>{.reference
.internal}</li>
<li><a href="index.html#document-topics/autothrottle">AutoThrottle
extension</a>{.reference
.internal}</li>
<li><a href="index.html#document-topics/benchmarking">Benchmarking</a>{.reference
.internal}</li>
<li><a href="index.html#document-topics/jobs">Jobs: pausing and resuming
crawls</a>{.reference .internal}</li>
<li><a href="index.html#document-topics/coroutines">Coroutines</a>{.reference
.internal}</li>
<li><a href="index.html#document-topics/asyncio">asyncio</a>{.reference .internal}</li>
</ul>
<p>[Extending Scrapy]{.caption-text}</p>
<ul>
<li><a href="index.html#document-topics/architecture">Architecture
overview</a>{.reference
.internal}</li>
<li><a href="index.html#document-topics/addons">Add-ons</a>{.reference .internal}</li>
<li><a href="index.html#document-topics/downloader-middleware">Downloader
Middleware</a>{.reference
.internal}</li>
<li><a href="index.html#document-topics/spider-middleware">Spider
Middleware</a>{.reference
.internal}</li>
<li><a href="index.html#document-topics/extensions">Extensions</a>{.reference
.internal}</li>
<li><a href="index.html#document-topics/signals">Signals</a>{.reference .internal}</li>
<li><a href="index.html#document-topics/scheduler">Scheduler</a>{.reference
.internal}</li>
<li><a href="index.html#document-topics/exporters">Item Exporters</a>{.reference
.internal}</li>
<li><a href="index.html#document-topics/components">Components</a>{.reference
.internal}</li>
<li><a href="index.html#document-topics/api">Core API</a>{.reference .internal}</li>
</ul>
<p>[All the rest]{.caption-text}</p>
<ul>
<li><a href="index.html#document-news">Release notes</a>{.reference .internal}</li>
<li><a href="index.html#document-contributing">Contributing to
Scrapy</a>{.reference .internal}</li>
<li><a href="index.html#document-versioning">Versioning and API
stability</a>{.reference .internal}
:::
:::</li>
</ul>
<p>::: {.section .wy-nav-content-wrap toggle=&quot;wy-nav-shift&quot;}
<a href="#">Scrapy</a></p>
<p>::: wy-nav-content
::: rst-content
::: {role=&quot;navigation&quot; aria-label=&quot;Page navigation&quot;}</p>
<ul>
<li><a href="#"></a>{.icon .icon-home} »</li>
<li>Scrapy 2.11.0 documentation</li>
<li><a href="https://github.com/scrapy/scrapy/blob/master/docs/index">Edit on
GitHub</a>{.fa
.fa-github}</li>
</ul>
<hr />
<p>:::</p>
<p>::: {.document role=&quot;main&quot; itemscope=&quot;itemscope&quot; itemtype=&quot;http://schema.org/Article&quot;}
::: {itemprop=&quot;articleBody&quot;}
::: {#scrapy-version-documentation .section}
[]{#topics-index}</p>
<h1 id="scrapy-211-documentationheaderlink"><a class="header" href="#scrapy-211-documentationheaderlink">Scrapy 2.11 documentation<a href="#scrapy-version-documentation" title="Permalink to this heading">¶</a>{.headerlink}</a></h1>
<p>Scrapy is a fast high-level <a href="https://en.wikipedia.org/wiki/Web_crawler">web
crawling</a>{.reference
.external} and <a href="https://en.wikipedia.org/wiki/Web_scraping">web
scraping</a>{.reference
.external} framework, used to crawl websites and extract structured data
from their pages. It can be used for a wide range of purposes, from data
mining to monitoring and automated testing.</p>
<p>::: {#getting-help .section}
[]{#id1}</p>
<h2 id="getting-helpheaderlink"><a class="header" href="#getting-helpheaderlink">Getting help<a href="#getting-help" title="Permalink to this heading">¶</a>{.headerlink}</a></h2>
<p>Having trouble? We'd like to help!</p>
<ul>
<li>
<p>Try the <a href="index.html#document-faq">[FAQ]{.doc}</a>{.reference .internal}
-- it's got answers to some common questions.</p>
</li>
<li>
<p>Looking for specific information? Try the <a href="genindex.html">[Index]{.std
.std-ref}</a>{.reference .internal} or <a href="py-modindex.html">[Module
Index]{.std .std-ref}</a>{.reference .internal}.</p>
</li>
<li>
<p>Ask or search questions in <a href="https://stackoverflow.com/tags/scrapy">StackOverflow using the scrapy
tag</a>{.reference .external}.</p>
</li>
<li>
<p>Ask or search questions in the <a href="https://www.reddit.com/r/scrapy/">Scrapy
subreddit</a>{.reference .external}.</p>
</li>
<li>
<p>Search for questions on the archives of the <a href="https://groups.google.com/forum/#!forum/scrapy-users">scrapy-users mailing
list</a>{.reference
.external}.</p>
</li>
<li>
<p>Ask a question in the <a href="irc://irc.freenode.net/scrapy">#scrapy IRC
channel</a>{.reference .external},</p>
</li>
<li>
<p>Report bugs with Scrapy in our <a href="https://github.com/scrapy/scrapy/issues">issue
tracker</a>{.reference
.external}.</p>
</li>
<li>
<p>Join the Discord community <a href="https://discord.gg/mv3yErfpvq">Scrapy
Discord</a>{.reference .external}.
:::</p>
</li>
</ul>
<p>::: {#first-steps .section}</p>
<h2 id="first-stepsheaderlink"><a class="header" href="#first-stepsheaderlink">First steps<a href="#first-steps" title="Permalink to this heading">¶</a>{.headerlink}</a></h2>
<p>::: {.toctree-wrapper .compound}
[]{#document-intro/overview}</p>
<p>::: {#scrapy-at-a-glance .section}
[]{#intro-overview}</p>
<h3 id="scrapy-at-a-glanceheaderlink"><a class="header" href="#scrapy-at-a-glanceheaderlink">Scrapy at a glance<a href="#scrapy-at-a-glance" title="Permalink to this heading">¶</a>{.headerlink}</a></h3>
<p>Scrapy (/ˈskreɪpaɪ/) is an application framework for crawling web sites
and extracting structured data which can be used for a wide range of
useful applications, like data mining, information processing or
historical archival.</p>
<p>Even though Scrapy was originally designed for <a href="https://en.wikipedia.org/wiki/Web_scraping">web
scraping</a>{.reference
.external}, it can also be used to extract data using APIs (such as
<a href="https://affiliate-program.amazon.com/gp/advertising/api/detail/main.html">Amazon Associates Web
Services</a>{.reference
.external}) or as a general purpose web crawler.</p>
<p>::: {#walk-through-of-an-example-spider .section}</p>
<h4 id="walk-through-of-an-example-spiderheaderlink"><a class="header" href="#walk-through-of-an-example-spiderheaderlink">Walk-through of an example spider<a href="#walk-through-of-an-example-spider" title="Permalink to this heading">¶</a>{.headerlink}</a></h4>
<p>In order to show you what Scrapy brings to the table, we'll walk you
through an example of a Scrapy Spider using the simplest way to run a
spider.</p>
<p>Here's the code for a spider that scrapes famous quotes from website
<a href="https://quotes.toscrape.com">https://quotes.toscrape.com</a>{.reference
.external}, following the pagination:</p>
<p>::: {.highlight-python .notranslate}
::: highlight
import scrapy</p>
<pre><code>class QuotesSpider(scrapy.Spider):
    name = &quot;quotes&quot;
    start_urls = [
        &quot;https://quotes.toscrape.com/tag/humor/&quot;,
    ]

    def parse(self, response):
        for quote in response.css(&quot;div.quote&quot;):
            yield {
                &quot;author&quot;: quote.xpath(&quot;span/small/text()&quot;).get(),
                &quot;text&quot;: quote.css(&quot;span.text::text&quot;).get(),
            }

        next_page = response.css('li.next a::attr(&quot;href&quot;)').get()
        if next_page is not None:
            yield response.follow(next_page, self.parse)
</code></pre>
<p>:::
:::</p>
<p>Put this in a text file, name it to something like
[<code>quotes_spider.py</code>{.docutils .literal .notranslate}]{.pre} and run the
spider using the <a href="index.html#std-command-runspider">[<code>runspider</code>{.xref .std .std-command .docutils
.literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal} command:</p>
<p>::: {.highlight-default .notranslate}
::: highlight
scrapy runspider quotes_spider.py -o quotes.jsonl
:::
:::</p>
<p>When this finishes you will have in the [<code>quotes.jsonl</code>{.docutils
.literal .notranslate}]{.pre} file a list of the quotes in JSON Lines
format, containing text and author, looking like this:</p>
<p>::: {.highlight-default .notranslate}
::: highlight
{&quot;author&quot;: &quot;Jane Austen&quot;, &quot;text&quot;: &quot;\u201cThe person, be it gentleman or lady, who has not pleasure in a good novel, must be intolerably stupid.\u201d&quot;}
{&quot;author&quot;: &quot;Steve Martin&quot;, &quot;text&quot;: &quot;\u201cA day without sunshine is like, you know, night.\u201d&quot;}
{&quot;author&quot;: &quot;Garrison Keillor&quot;, &quot;text&quot;: &quot;\u201cAnyone who thinks sitting in church can make you a Christian must also think that sitting in a garage can make you a car.\u201d&quot;}
...
:::
:::</p>
<p>::: {#what-just-happened .section}</p>
<h5 id="what-just-happenedheaderlink"><a class="header" href="#what-just-happenedheaderlink">What just happened?<a href="#what-just-happened" title="Permalink to this heading">¶</a>{.headerlink}</a></h5>
<p>When you ran the command [<code>scrapy</code>{.docutils .literal
.notranslate}]{.pre}<code> </code>{.docutils .literal
.notranslate}[<code>runspider</code>{.docutils .literal
.notranslate}]{.pre}<code> </code>{.docutils .literal
.notranslate}[<code>quotes_spider.py</code>{.docutils .literal
.notranslate}]{.pre}, Scrapy looked for a Spider definition inside it
and ran it through its crawler engine.</p>
<p>The crawl started by making requests to the URLs defined in the
[<code>start_urls</code>{.docutils .literal .notranslate}]{.pre} attribute (in this
case, only the URL for quotes in <em>humor</em> category) and called the
default callback method [<code>parse</code>{.docutils .literal
.notranslate}]{.pre}, passing the response object as an argument. In the
[<code>parse</code>{.docutils .literal .notranslate}]{.pre} callback, we loop
through the quote elements using a CSS Selector, yield a Python dict
with the extracted quote text and author, look for a link to the next
page and schedule another request using the same [<code>parse</code>{.docutils
.literal .notranslate}]{.pre} method as callback.</p>
<p>Here you notice one of the main advantages about Scrapy: requests are
<a href="index.html#topics-architecture">[scheduled and processed asynchronously]{.std
.std-ref}</a>{.hoverxref .tooltip
.reference .internal}. This means that Scrapy doesn't need to wait for a
request to be finished and processed, it can send another request or do
other things in the meantime. This also means that other requests can
keep going even if some request fails or an error happens while handling
it.</p>
<p>While this enables you to do very fast crawls (sending multiple
concurrent requests at the same time, in a fault-tolerant way) Scrapy
also gives you control over the politeness of the crawl through <a href="index.html#topics-settings-ref">[a few
settings]{.std .std-ref}</a>{.hoverxref
.tooltip .reference .internal}. You can do things like setting a
download delay between each request, limiting amount of concurrent
requests per domain or per IP, and even <a href="index.html#topics-autothrottle">[using an auto-throttling
extension]{.std .std-ref}</a>{.hoverxref
.tooltip .reference .internal} that tries to figure out these
automatically.</p>
<p>::: {.admonition .note}
Note</p>
<p>This is using <a href="index.html#topics-feed-exports">[feed exports]{.std
.std-ref}</a>{.hoverxref .tooltip
.reference .internal} to generate the JSON file, you can easily change
the export format (XML or CSV, for example) or the storage backend (FTP
or <a href="https://aws.amazon.com/s3/">Amazon S3</a>{.reference .external}, for
example). You can also write an <a href="index.html#topics-item-pipeline">[item pipeline]{.std
.std-ref}</a>{.hoverxref .tooltip
.reference .internal} to store the items in a database.
:::
:::
:::</p>
<p>::: {#what-else .section}
[]{#topics-whatelse}</p>
<h4 id="what-elseheaderlink"><a class="header" href="#what-elseheaderlink">What else?<a href="#what-else" title="Permalink to this heading">¶</a>{.headerlink}</a></h4>
<p>You've seen how to extract and store items from a website using Scrapy,
but this is just the surface. Scrapy provides a lot of powerful features
for making scraping easy and efficient, such as:</p>
<ul>
<li>
<p>Built-in support for <a href="index.html#topics-selectors">[selecting and extracting]{.std
.std-ref}</a>{.hoverxref .tooltip
.reference .internal} data from HTML/XML sources using extended CSS
selectors and XPath expressions, with helper methods to extract
using regular expressions.</p>
</li>
<li>
<p>An <a href="index.html#topics-shell">[interactive shell console]{.std
.std-ref}</a>{.hoverxref .tooltip .reference
.internal} (IPython aware) for trying out the CSS and XPath
expressions to scrape data, very useful when writing or debugging
your spiders.</p>
</li>
<li>
<p>Built-in support for <a href="index.html#topics-feed-exports">[generating feed exports]{.std
.std-ref}</a>{.hoverxref .tooltip
.reference .internal} in multiple formats (JSON, CSV, XML) and
storing them in multiple backends (FTP, S3, local filesystem)</p>
</li>
<li>
<p>Robust encoding support and auto-detection, for dealing with
foreign, non-standard and broken encoding declarations.</p>
</li>
<li>
<p><a href="index.html#extending-scrapy">[Strong extensibility support]{.std
.std-ref}</a>{.hoverxref .tooltip
.reference .internal}, allowing you to plug in your own
functionality using <a href="index.html#topics-signals">[signals]{.std
.std-ref}</a>{.hoverxref .tooltip .reference
.internal} and a well-defined API (middlewares, <a href="index.html#topics-extensions">[extensions]{.std
.std-ref}</a>{.hoverxref .tooltip
.reference .internal}, and <a href="index.html#topics-item-pipeline">[pipelines]{.std
.std-ref}</a>{.hoverxref .tooltip
.reference .internal}).</p>
</li>
<li>
<p>Wide range of built-in extensions and middlewares for handling:</p>
<ul>
<li>
<p>cookies and session handling</p>
</li>
<li>
<p>HTTP features like compression, authentication, caching</p>
</li>
<li>
<p>user-agent spoofing</p>
</li>
<li>
<p>robots.txt</p>
</li>
<li>
<p>crawl depth restriction</p>
</li>
<li>
<p>and more</p>
</li>
</ul>
</li>
<li>
<p>A <a href="index.html#topics-telnetconsole">[Telnet console]{.std
.std-ref}</a>{.hoverxref .tooltip
.reference .internal} for hooking into a Python console running
inside your Scrapy process, to introspect and debug your crawler</p>
</li>
<li>
<p>Plus other goodies like reusable spiders to crawl sites from
<a href="https://www.sitemaps.org/index.html">Sitemaps</a>{.reference
.external} and XML/CSV feeds, a media pipeline for <a href="index.html#topics-media-pipeline">[automatically
downloading images]{.std
.std-ref}</a>{.hoverxref .tooltip
.reference .internal} (or any other media) associated with the
scraped items, a caching DNS resolver, and much more!
:::</p>
</li>
</ul>
<p>::: {#what-s-next .section}</p>
<h4 id="whats-nextheaderlink"><a class="header" href="#whats-nextheaderlink">What's next?<a href="#what-s-next" title="Permalink to this heading">¶</a>{.headerlink}</a></h4>
<p>The next steps for you are to <a href="index.html#intro-install">[install Scrapy]{.std
.std-ref}</a>{.hoverxref .tooltip .reference
.internal}, <a href="index.html#intro-tutorial">[follow through the tutorial]{.std
.std-ref}</a>{.hoverxref .tooltip .reference
.internal} to learn how to create a full-blown Scrapy project and <a href="https://scrapy.org/community/">join
the community</a>{.reference .external}.
Thanks for your interest!
:::
:::</p>
<p>[]{#document-intro/install}</p>
<p>::: {#installation-guide .section}
[]{#intro-install}</p>
<h3 id="installation-guideheaderlink"><a class="header" href="#installation-guideheaderlink">Installation guide<a href="#installation-guide" title="Permalink to this heading">¶</a>{.headerlink}</a></h3>
<p>::: {#supported-python-versions .section}
[]{#faq-python-versions}</p>
<h4 id="supported-python-versionsheaderlink"><a class="header" href="#supported-python-versionsheaderlink">Supported Python versions<a href="#supported-python-versions" title="Permalink to this heading">¶</a>{.headerlink}</a></h4>
<p>Scrapy requires Python 3.8+, either the CPython implementation (default)
or the PyPy implementation (see <a href="https://docs.python.org/3/reference/introduction.html#implementations" title="(in Python v3.12)">Alternate
Implementations</a>{.reference
.external}).
:::</p>
<p>::: {#installing-scrapy .section}
[]{#intro-install-scrapy}</p>
<h4 id="installing-scrapyheaderlink"><a class="header" href="#installing-scrapyheaderlink">Installing Scrapy<a href="#installing-scrapy" title="Permalink to this heading">¶</a>{.headerlink}</a></h4>
<p>If you're using
<a href="https://docs.anaconda.com/anaconda/">Anaconda</a>{.reference .external} or
<a href="https://docs.conda.io/projects/conda/en/latest/user-guide/install/index.html">Miniconda</a>{.reference
.external}, you can install the package from the
<a href="https://conda-forge.org/">conda-forge</a>{.reference .external} channel,
which has up-to-date packages for Linux, Windows and macOS.</p>
<p>To install Scrapy using [<code>conda</code>{.docutils .literal
.notranslate}]{.pre}, run:</p>
<p>::: {.highlight-default .notranslate}
::: highlight
conda install -c conda-forge scrapy
:::
:::</p>
<p>Alternatively, if you're already familiar with installation of Python
packages, you can install Scrapy and its dependencies from PyPI with:</p>
<p>::: {.highlight-default .notranslate}
::: highlight
pip install Scrapy
:::
:::</p>
<p>We strongly recommend that you install Scrapy in <a href="#intro-using-virtualenv">[a dedicated
virtualenv]{.std .std-ref}</a>{.hoverxref .tooltip
.reference .internal}, to avoid conflicting with your system packages.</p>
<p>Note that sometimes this may require solving compilation issues for some
Scrapy dependencies depending on your operating system, so be sure to
check the <a href="#intro-install-platform-notes">[Platform specific installation notes]{.std
.std-ref}</a>{.hoverxref .tooltip .reference
.internal}.</p>
<p>For more detailed and platform specifics instructions, as well as
troubleshooting information, read on.</p>
<p>::: {#things-that-are-good-to-know .section}</p>
<h5 id="things-that-are-good-to-knowheaderlink"><a class="header" href="#things-that-are-good-to-knowheaderlink">Things that are good to know<a href="#things-that-are-good-to-know" title="Permalink to this heading">¶</a>{.headerlink}</a></h5>
<p>Scrapy is written in pure Python and depends on a few key Python
packages (among others):</p>
<ul>
<li>
<p><a href="https://lxml.de/index.html">lxml</a>{.reference .external}, an
efficient XML and HTML parser</p>
</li>
<li>
<p><a href="https://pypi.org/project/parsel/">parsel</a>{.reference .external}, an
HTML/XML data extraction library written on top of lxml,</p>
</li>
<li>
<p><a href="https://pypi.org/project/w3lib/">w3lib</a>{.reference .external}, a
multi-purpose helper for dealing with URLs and web page encodings</p>
</li>
<li>
<p><a href="https://twistedmatrix.com/trac/">twisted</a>{.reference .external}, an
asynchronous networking framework</p>
</li>
<li>
<p><a href="https://cryptography.io/en/latest/">cryptography</a>{.reference
.external} and
<a href="https://pypi.org/project/pyOpenSSL/">pyOpenSSL</a>{.reference
.external}, to deal with various network-level security needs</p>
</li>
</ul>
<p>Some of these packages themselves depend on non-Python packages that
might require additional installation steps depending on your platform.
Please check <a href="#intro-install-platform-notes">[platform-specific guides below]{.std
.std-ref}</a>{.hoverxref .tooltip .reference
.internal}.</p>
<p>In case of any trouble related to these dependencies, please refer to
their respective installation instructions:</p>
<ul>
<li>
<p><a href="https://lxml.de/installation.html">lxml installation</a>{.reference
.external}</p>
</li>
<li>
<p><a href="https://cryptography.io/en/latest/installation/" title="(in Cryptography v42.0.0.dev1)">[cryptography installation]{.xref .std
.std-doc}</a>{.reference
.external}
:::</p>
</li>
</ul>
<p>::: {#using-a-virtual-environment-recommended .section}
[]{#intro-using-virtualenv}</p>
<h5 id="using-a-virtual-environment-recommendedheaderlink"><a class="header" href="#using-a-virtual-environment-recommendedheaderlink">Using a virtual environment (recommended)<a href="#using-a-virtual-environment-recommended" title="Permalink to this heading">¶</a>{.headerlink}</a></h5>
<p>TL;DR: We recommend installing Scrapy inside a virtual environment on
all platforms.</p>
<p>Python packages can be installed either globally (a.k.a system wide), or
in user-space. We do not recommend installing Scrapy system wide.</p>
<p>Instead, we recommend that you install Scrapy within a so-called
&quot;virtual environment&quot; (<a href="https://docs.python.org/3/library/venv.html#module-venv" title="(in Python v3.12)">[<code>venv</code>{.xref .py .py-mod .docutils .literal
.notranslate}]{.pre}</a>{.reference
.external}). Virtual environments allow you to not conflict with
already-installed Python system packages (which could break some of your
system tools and scripts), and still install packages normally with
[<code>pip</code>{.docutils .literal .notranslate}]{.pre} (without
[<code>sudo</code>{.docutils .literal .notranslate}]{.pre} and the likes).</p>
<p>See <a href="https://docs.python.org/3/tutorial/venv.html#tut-venv" title="(in Python v3.12)">Virtual Environments and
Packages</a>{.reference
.external} on how to create your virtual environment.</p>
<p>Once you have created a virtual environment, you can install Scrapy
inside it with [<code>pip</code>{.docutils .literal .notranslate}]{.pre}, just like
any other Python package. (See <a href="#intro-install-platform-notes">[platform-specific guides]{.std
.std-ref}</a>{.hoverxref .tooltip .reference
.internal} below for non-Python dependencies that you may need to
install beforehand).
:::
:::</p>
<p>::: {#platform-specific-installation-notes .section}
[]{#intro-install-platform-notes}</p>
<h4 id="platform-specific-installation-notesheaderlink"><a class="header" href="#platform-specific-installation-notesheaderlink">Platform specific installation notes<a href="#platform-specific-installation-notes" title="Permalink to this heading">¶</a>{.headerlink}</a></h4>
<p>::: {#windows .section}
[]{#intro-install-windows}</p>
<h5 id="windowsheaderlink"><a class="header" href="#windowsheaderlink">Windows<a href="#windows" title="Permalink to this heading">¶</a>{.headerlink}</a></h5>
<p>Though it's possible to install Scrapy on Windows using pip, we
recommend you to install
<a href="https://docs.anaconda.com/anaconda/">Anaconda</a>{.reference .external} or
<a href="https://docs.conda.io/projects/conda/en/latest/user-guide/install/index.html">Miniconda</a>{.reference
.external} and use the package from the
<a href="https://conda-forge.org/">conda-forge</a>{.reference .external} channel,
which will avoid most installation issues.</p>
<p>Once you've installed
<a href="https://docs.anaconda.com/anaconda/">Anaconda</a>{.reference .external} or
<a href="https://docs.conda.io/projects/conda/en/latest/user-guide/install/index.html">Miniconda</a>{.reference
.external}, install Scrapy with:</p>
<p>::: {.highlight-default .notranslate}
::: highlight
conda install -c conda-forge scrapy
:::
:::</p>
<p>To install Scrapy on Windows using [<code>pip</code>{.docutils .literal
.notranslate}]{.pre}:</p>
<p>::: {.admonition .warning}
Warning</p>
<p>This installation method requires &quot;Microsoft Visual C++&quot; for installing
some Scrapy dependencies, which demands significantly more disk space
than Anaconda.
:::</p>
<ol>
<li>
<p>Download and execute <a href="https://visualstudio.microsoft.com/visual-cpp-build-tools/">Microsoft C++ Build
Tools</a>{.reference
.external} to install the Visual Studio Installer.</p>
</li>
<li>
<p>Run the Visual Studio Installer.</p>
</li>
<li>
<p>Under the Workloads section, select <strong>C++ build tools</strong>.</p>
</li>
<li>
<p>Check the installation details and make sure following packages are
selected as optional components:</p>
<blockquote>
<div>
<ul>
<li>
<p><strong>MSVC</strong> (e.g MSVC v142 - VS 2019 C++ x64/x86 build tools
(v14.23) )</p>
</li>
<li>
<p><strong>Windows SDK</strong> (e.g Windows 10 SDK (10.0.18362.0))</p>
</li>
</ul>
</div>
</blockquote>
</li>
<li>
<p>Install the Visual Studio Build Tools.</p>
</li>
</ol>
<p>Now, you should be able to <a href="#intro-install-scrapy">[install Scrapy]{.std
.std-ref}</a>{.hoverxref .tooltip .reference
.internal} using [<code>pip</code>{.docutils .literal .notranslate}]{.pre}.
:::</p>
<p>::: {#ubuntu-14-04-or-above .section}
[]{#intro-install-ubuntu}</p>
<h5 id="ubuntu-1404-or-aboveheaderlink"><a class="header" href="#ubuntu-1404-or-aboveheaderlink">Ubuntu 14.04 or above<a href="#ubuntu-14-04-or-above" title="Permalink to this heading">¶</a>{.headerlink}</a></h5>
<p>Scrapy is currently tested with recent-enough versions of lxml, twisted
and pyOpenSSL, and is compatible with recent Ubuntu distributions. But
it should support older versions of Ubuntu too, like Ubuntu 14.04,
albeit with potential issues with TLS connections.</p>
<p><strong>Don't</strong> use the [<code>python-scrapy</code>{.docutils .literal
.notranslate}]{.pre} package provided by Ubuntu, they are typically too
old and slow to catch up with latest Scrapy.</p>
<p>To install Scrapy on Ubuntu (or Ubuntu-based) systems, you need to
install these dependencies:</p>
<p>::: {.highlight-default .notranslate}
::: highlight
sudo apt-get install python3 python3-dev python3-pip libxml2-dev libxslt1-dev zlib1g-dev libffi-dev libssl-dev
:::
:::</p>
<ul>
<li>
<p>[<code>python3-dev</code>{.docutils .literal .notranslate}]{.pre},
[<code>zlib1g-dev</code>{.docutils .literal .notranslate}]{.pre},
[<code>libxml2-dev</code>{.docutils .literal .notranslate}]{.pre} and
[<code>libxslt1-dev</code>{.docutils .literal .notranslate}]{.pre} are required
for [<code>lxml</code>{.docutils .literal .notranslate}]{.pre}</p>
</li>
<li>
<p>[<code>libssl-dev</code>{.docutils .literal .notranslate}]{.pre} and
[<code>libffi-dev</code>{.docutils .literal .notranslate}]{.pre} are required
for [<code>cryptography</code>{.docutils .literal .notranslate}]{.pre}</p>
</li>
</ul>
<p>Inside a <a href="#intro-using-virtualenv">[virtualenv]{.std
.std-ref}</a>{.hoverxref .tooltip .reference
.internal}, you can install Scrapy with [<code>pip</code>{.docutils .literal
.notranslate}]{.pre} after that:</p>
<p>::: {.highlight-default .notranslate}
::: highlight
pip install scrapy
:::
:::</p>
<p>::: {.admonition .note}
Note</p>
<p>The same non-Python dependencies can be used to install Scrapy in Debian
Jessie (8.0) and above.
:::
:::</p>
<p>::: {#macos .section}
[]{#intro-install-macos}</p>
<h5 id="macosheaderlink"><a class="header" href="#macosheaderlink">macOS<a href="#macos" title="Permalink to this heading">¶</a>{.headerlink}</a></h5>
<p>Building Scrapy's dependencies requires the presence of a C compiler and
development headers. On macOS this is typically provided by Apple's
Xcode development tools. To install the Xcode command line tools open a
terminal window and run:</p>
<p>::: {.highlight-default .notranslate}
::: highlight
xcode-select --install
:::
:::</p>
<p>There's a <a href="https://github.com/pypa/pip/issues/2468">known
issue</a>{.reference .external}
that prevents [<code>pip</code>{.docutils .literal .notranslate}]{.pre} from
updating system packages. This has to be addressed to successfully
install Scrapy and its dependencies. Here are some proposed solutions:</p>
<ul>
<li>
<p><em>(Recommended)</em> <strong>Don't</strong> use system Python. Install a new, updated
version that doesn't conflict with the rest of your system. Here's
how to do it using the <a href="https://brew.sh/">homebrew</a>{.reference
.external} package manager:</p>
<ul>
<li>
<p>Install <a href="https://brew.sh/">homebrew</a>{.reference .external}
following the instructions in
<a href="https://brew.sh/">https://brew.sh/</a>{.reference .external}</p>
</li>
<li>
<p>Update your [<code>PATH</code>{.docutils .literal .notranslate}]{.pre}
variable to state that homebrew packages should be used before
system packages (Change [<code>.bashrc</code>{.docutils .literal
.notranslate}]{.pre} to [<code>.zshrc</code>{.docutils .literal
.notranslate}]{.pre} accordingly if you're using
<a href="https://www.zsh.org/">zsh</a>{.reference .external} as default
shell):</p>
<p>::: {.highlight-default .notranslate}
::: highlight
echo &quot;export PATH=/usr/local/bin:/usr/local/sbin:$PATH&quot; &gt;&gt; ~/.bashrc
:::
:::</p>
</li>
<li>
<p>Reload [<code>.bashrc</code>{.docutils .literal .notranslate}]{.pre} to
ensure the changes have taken place:</p>
<p>::: {.highlight-default .notranslate}
::: highlight
source ~/.bashrc
:::
:::</p>
</li>
<li>
<p>Install python:</p>
<p>::: {.highlight-default .notranslate}
::: highlight
brew install python
:::
:::</p>
</li>
<li>
<p>Latest versions of python have [<code>pip</code>{.docutils .literal
.notranslate}]{.pre} bundled with them so you won't need to
install it separately. If this is not the case, upgrade python:</p>
<p>::: {.highlight-default .notranslate}
::: highlight
brew update; brew upgrade python
:::
:::</p>
</li>
</ul>
</li>
<li>
<p><em>(Optional)</em> <a href="#intro-using-virtualenv">[Install Scrapy inside a Python virtual
environment]{.std .std-ref}</a>{.hoverxref
.tooltip .reference .internal}.</p>
</li>
</ul>
<blockquote>
<div>
<p>This method is a workaround for the above macOS issue, but it's an
overall good practice for managing dependencies and can complement the
first method.</p>
</div>
</blockquote>
<p>After any of these workarounds you should be able to install Scrapy:</p>
<p>::: {.highlight-default .notranslate}
::: highlight
pip install Scrapy
:::
:::
:::</p>
<p>::: {#pypy .section}</p>
<h5 id="pypyheaderlink"><a class="header" href="#pypyheaderlink">PyPy<a href="#pypy" title="Permalink to this heading">¶</a>{.headerlink}</a></h5>
<p>We recommend using the latest PyPy version. For PyPy3, only Linux
installation was tested.</p>
<p>Most Scrapy dependencies now have binary wheels for CPython, but not for
PyPy. This means that these dependencies will be built during
installation. On macOS, you are likely to face an issue with building
the Cryptography dependency. The solution to this problem is described
<a href="https://github.com/pyca/cryptography/issues/2692#issuecomment-272773481">here</a>{.reference
.external}, that is to [<code>brew</code>{.docutils .literal
.notranslate}]{.pre}<code> </code>{.docutils .literal
.notranslate}[<code>install</code>{.docutils .literal
.notranslate}]{.pre}<code> </code>{.docutils .literal
.notranslate}[<code>openssl</code>{.docutils .literal .notranslate}]{.pre} and then
export the flags that this command recommends (only needed when
installing Scrapy). Installing on Linux has no special issues besides
installing build dependencies. Installing Scrapy with PyPy on Windows is
not tested.</p>
<p>You can check that Scrapy is installed correctly by running
[<code>scrapy</code>{.docutils .literal .notranslate}]{.pre}<code> </code>{.docutils .literal
.notranslate}[<code>bench</code>{.docutils .literal .notranslate}]{.pre}. If this
command gives errors such as [<code>TypeError:</code>{.docutils .literal
.notranslate}]{.pre}<code> </code>{.docutils .literal .notranslate}[<code>...</code>{.docutils
.literal .notranslate}]{.pre}<code> </code>{.docutils .literal
.notranslate}[<code>got</code>{.docutils .literal .notranslate}]{.pre}<code> </code>{.docutils
.literal .notranslate}[<code>2</code>{.docutils .literal
.notranslate}]{.pre}<code> </code>{.docutils .literal
.notranslate}[<code>unexpected</code>{.docutils .literal
.notranslate}]{.pre}<code> </code>{.docutils .literal
.notranslate}[<code>keyword</code>{.docutils .literal
.notranslate}]{.pre}<code> </code>{.docutils .literal
.notranslate}[<code>arguments</code>{.docutils .literal .notranslate}]{.pre}, this
means that setuptools was unable to pick up one PyPy-specific
dependency. To fix this issue, run [<code>pip</code>{.docutils .literal
.notranslate}]{.pre}<code> </code>{.docutils .literal
.notranslate}[<code>install</code>{.docutils .literal
.notranslate}]{.pre}<code> </code>{.docutils .literal
.notranslate}[<code>'PyPyDispatcher&gt;=2.1.0'</code>{.docutils .literal
.notranslate}]{.pre}.
:::
:::</p>
<p>::: {#troubleshooting .section}
[]{#intro-install-troubleshooting}</p>
<h4 id="troubleshootingheaderlink"><a class="header" href="#troubleshootingheaderlink">Troubleshooting<a href="#troubleshooting" title="Permalink to this heading">¶</a>{.headerlink}</a></h4>
<p>::: {#attributeerror-module-object-has-no-attribute-op-no-tlsv1-1 .section}</p>
<h5 id="attributeerror-module-object-has-no-attribute-op_no_tlsv1_1headerlink"><a class="header" href="#attributeerror-module-object-has-no-attribute-op_no_tlsv1_1headerlink">AttributeError: 'module' object has no attribute 'OP_NO_TLSv1_1'<a href="#attributeerror-module-object-has-no-attribute-op-no-tlsv1-1" title="Permalink to this heading">¶</a>{.headerlink}</a></h5>
<p>After you install or upgrade Scrapy, Twisted or pyOpenSSL, you may get
an exception with the following traceback:</p>
<p>::: {.highlight-default .notranslate}
::: highlight
[…]
File &quot;[…]/site-packages/twisted/protocols/tls.py&quot;, line 63, in <module>
from twisted.internet._sslverify import _setAcceptableProtocols
File &quot;[…]/site-packages/twisted/internet/_sslverify.py&quot;, line 38, in <module>
TLSVersion.TLSv1_1: SSL.OP_NO_TLSv1_1,
AttributeError: 'module' object has no attribute 'OP_NO_TLSv1_1'
:::
:::</p>
<p>The reason you get this exception is that your system or virtual
environment has a version of pyOpenSSL that your version of Twisted does
not support.</p>
<p>To install a version of pyOpenSSL that your version of Twisted supports,
reinstall Twisted with the [<code>tls</code>{.code .docutils .literal
.notranslate}]{.pre} extra option:</p>
<p>::: {.highlight-default .notranslate}
::: highlight
pip install twisted[tls]
:::
:::</p>
<p>For details, see <a href="https://github.com/scrapy/scrapy/issues/2473">Issue
#2473</a>{.reference
.external}.
:::
:::
:::</p>
<p>[]{#document-intro/tutorial}</p>
<p>::: {#scrapy-tutorial .section}
[]{#intro-tutorial}</p>
<h3 id="scrapy-tutorialheaderlink"><a class="header" href="#scrapy-tutorialheaderlink">Scrapy Tutorial<a href="#scrapy-tutorial" title="Permalink to this heading">¶</a>{.headerlink}</a></h3>
<p>In this tutorial, we'll assume that Scrapy is already installed on your
system. If that's not the case, see <a href="index.html#intro-install">[Installation guide]{.std
.std-ref}</a>{.hoverxref .tooltip .reference
.internal}.</p>
<p>We are going to scrape
<a href="https://quotes.toscrape.com/">quotes.toscrape.com</a>{.reference
.external}, a website that lists quotes from famous authors.</p>
<p>This tutorial will walk you through these tasks:</p>
<ol>
<li>
<p>Creating a new Scrapy project</p>
</li>
<li>
<p>Writing a <a href="index.html#topics-spiders">[spider]{.std
.std-ref}</a>{.hoverxref .tooltip .reference
.internal} to crawl a site and extract data</p>
</li>
<li>
<p>Exporting the scraped data using the command line</p>
</li>
<li>
<p>Changing spider to recursively follow links</p>
</li>
<li>
<p>Using spider arguments</p>
</li>
</ol>
<p>Scrapy is written in <a href="https://www.python.org/">Python</a>{.reference
.external}. If you're new to the language you might want to start by
getting an idea of what the language is like, to get the most out of
Scrapy.</p>
<p>If you're already familiar with other languages, and want to learn
Python quickly, the <a href="https://docs.python.org/3/tutorial">Python
Tutorial</a>{.reference .external} is a
good resource.</p>
<p>If you're new to programming and want to start with Python, the
following books may be useful to you:</p>
<ul>
<li>
<p><a href="https://automatetheboringstuff.com/">Automate the Boring Stuff With
Python</a>{.reference .external}</p>
</li>
<li>
<p><a href="http://openbookproject.net/thinkcs/python/english3e/">How To Think Like a Computer
Scientist</a>{.reference
.external}</p>
</li>
<li>
<p><a href="https://learnpythonthehardway.org/python3/">Learn Python 3 The Hard
Way</a>{.reference
.external}</p>
</li>
</ul>
<p>You can also take a look at <a href="https://wiki.python.org/moin/BeginnersGuide/NonProgrammers">this list of Python resources for
non-programmers</a>{.reference
.external}, as well as the <a href="https://www.reddit.com/r/learnpython/wiki/index#wiki_new_to_python.3F">suggested resources in the
learnpython-subreddit</a>{.reference
.external}.</p>
<p>::: {#creating-a-project .section}</p>
<h4 id="creating-a-projectheaderlink"><a class="header" href="#creating-a-projectheaderlink">Creating a project<a href="#creating-a-project" title="Permalink to this heading">¶</a>{.headerlink}</a></h4>
<p>Before you start scraping, you will have to set up a new Scrapy project.
Enter a directory where you'd like to store your code and run:</p>
<p>::: {.highlight-default .notranslate}
::: highlight
scrapy startproject tutorial
:::
:::</p>
<p>This will create a [<code>tutorial</code>{.docutils .literal .notranslate}]{.pre}
directory with the following contents:</p>
<p>::: {.highlight-default .notranslate}
::: highlight
tutorial/
scrapy.cfg            # deploy configuration file</p>
<pre><code>    tutorial/             # project's Python module, you'll import your code from here
        __init__.py

        items.py          # project items definition file

        middlewares.py    # project middlewares file

        pipelines.py      # project pipelines file

        settings.py       # project settings file

        spiders/          # a directory where you'll later put your spiders
            __init__.py
</code></pre>
<p>:::
:::
:::</p>
<p>::: {#our-first-spider .section}</p>
<h4 id="our-first-spiderheaderlink"><a class="header" href="#our-first-spiderheaderlink">Our first Spider<a href="#our-first-spider" title="Permalink to this heading">¶</a>{.headerlink}</a></h4>
<p>Spiders are classes that you define and that Scrapy uses to scrape
information from a website (or a group of websites). They must subclass
<a href="index.html#scrapy.Spider" title="scrapy.Spider">[<code>Spider</code>{.xref .py .py-class .docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal} and define the initial requests to make, optionally how to
follow links in the pages, and how to parse the downloaded page content
to extract data.</p>
<p>This is the code for our first Spider. Save it in a file named
[<code>quotes_spider.py</code>{.docutils .literal .notranslate}]{.pre} under the
[<code>tutorial/spiders</code>{.docutils .literal .notranslate}]{.pre} directory in
your project:</p>
<p>::: {.highlight-python .notranslate}
::: highlight
from pathlib import Path</p>
<pre><code>import scrapy


class QuotesSpider(scrapy.Spider):
    name = &quot;quotes&quot;

    def start_requests(self):
        urls = [
            &quot;https://quotes.toscrape.com/page/1/&quot;,
            &quot;https://quotes.toscrape.com/page/2/&quot;,
        ]
        for url in urls:
            yield scrapy.Request(url=url, callback=self.parse)

    def parse(self, response):
        page = response.url.split(&quot;/&quot;)[-2]
        filename = f&quot;quotes-{page}.html&quot;
        Path(filename).write_bytes(response.body)
        self.log(f&quot;Saved file {filename}&quot;)
</code></pre>
<p>:::
:::</p>
<p>As you can see, our Spider subclasses <a href="index.html#scrapy.Spider" title="scrapy.Spider">[<code>scrapy.Spider</code>{.xref .py
.py-class .docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal} and defines some attributes and methods:</p>
<ul>
<li>
<p><a href="index.html#scrapy.Spider.name" title="scrapy.Spider.name">[<code>name</code>{.xref .py .py-attr .docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal}: identifies the Spider. It must be unique within a
project, that is, you can't set the same name for different Spiders.</p>
</li>
<li>
<p><a href="index.html#scrapy.Spider.start_requests" title="scrapy.Spider.start_requests">[<code>start_requests()</code>{.xref .py .py-meth .docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal}: must return an iterable of Requests (you can return a
list of requests or write a generator function) which the Spider
will begin to crawl from. Subsequent requests will be generated
successively from these initial requests.</p>
</li>
<li>
<p><a href="index.html#scrapy.Spider.parse" title="scrapy.Spider.parse">[<code>parse()</code>{.xref .py .py-meth .docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal}: a method that will be called to handle the response
downloaded for each of the requests made. The response parameter is
an instance of <a href="index.html#scrapy.http.TextResponse" title="scrapy.http.TextResponse">[<code>TextResponse</code>{.xref .py .py-class .docutils
.literal
.notranslate}]{.pre}</a>{.reference
.internal} that holds the page content and has further helpful
methods to handle it.</p>
<p>The <a href="index.html#scrapy.Spider.parse" title="scrapy.Spider.parse">[<code>parse()</code>{.xref .py .py-meth .docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal} method usually parses the response, extracting the
scraped data as dicts and also finding new URLs to follow and
creating new requests ([<code>Request</code>{.xref .py .py-class .docutils
.literal .notranslate}]{.pre}) from them.</p>
</li>
</ul>
<p>::: {#how-to-run-our-spider .section}</p>
<h5 id="how-to-run-our-spiderheaderlink"><a class="header" href="#how-to-run-our-spiderheaderlink">How to run our spider<a href="#how-to-run-our-spider" title="Permalink to this heading">¶</a>{.headerlink}</a></h5>
<p>To put our spider to work, go to the project's top level directory and
run:</p>
<p>::: {.highlight-default .notranslate}
::: highlight
scrapy crawl quotes
:::
:::</p>
<p>This command runs the spider with name [<code>quotes</code>{.docutils .literal
.notranslate}]{.pre} that we've just added, that will send some requests
for the [<code>quotes.toscrape.com</code>{.docutils .literal .notranslate}]{.pre}
domain. You will get an output similar to this:</p>
<p>::: {.highlight-default .notranslate}
::: highlight
... (omitted for brevity)
2016-12-16 21:24:05 [scrapy.core.engine] INFO: Spider opened
2016-12-16 21:24:05 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2016-12-16 21:24:05 [scrapy.extensions.telnet] DEBUG: Telnet console listening on 127.0.0.1:6023
2016-12-16 21:24:05 [scrapy.core.engine] DEBUG: Crawled (404) &lt;GET https://quotes.toscrape.com/robots.txt&gt; (referer: None)
2016-12-16 21:24:05 [scrapy.core.engine] DEBUG: Crawled (200) &lt;GET https://quotes.toscrape.com/page/1/&gt; (referer: None)
2016-12-16 21:24:05 [scrapy.core.engine] DEBUG: Crawled (200) &lt;GET https://quotes.toscrape.com/page/2/&gt; (referer: None)
2016-12-16 21:24:05 [quotes] DEBUG: Saved file quotes-1.html
2016-12-16 21:24:05 [quotes] DEBUG: Saved file quotes-2.html
2016-12-16 21:24:05 [scrapy.core.engine] INFO: Closing spider (finished)
...
:::
:::</p>
<p>Now, check the files in the current directory. You should notice that
two new files have been created: <em>quotes-1.html</em> and <em>quotes-2.html</em>,
with the content for the respective URLs, as our [<code>parse</code>{.docutils
.literal .notranslate}]{.pre} method instructs.</p>
<p>::: {.admonition .note}
Note</p>
<p>If you are wondering why we haven't parsed the HTML yet, hold on, we
will cover that soon.
:::</p>
<p>::: {#what-just-happened-under-the-hood .section}</p>
<h6 id="what-just-happened-under-the-hoodheaderlink"><a class="header" href="#what-just-happened-under-the-hoodheaderlink">What just happened under the hood?<a href="#what-just-happened-under-the-hood" title="Permalink to this heading">¶</a>{.headerlink}</a></h6>
<p>Scrapy schedules the [<code>scrapy.Request</code>{.xref .py .py-class .docutils
.literal .notranslate}]{.pre} objects returned by the
[<code>start_requests</code>{.docutils .literal .notranslate}]{.pre} method of the
Spider. Upon receiving a response for each one, it instantiates
<a href="index.html#scrapy.http.Response" title="scrapy.http.Response">[<code>Response</code>{.xref .py .py-class .docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal} objects and calls the callback method associated with the
request (in this case, the [<code>parse</code>{.docutils .literal
.notranslate}]{.pre} method) passing the response as argument.
:::
:::</p>
<p>::: {#a-shortcut-to-the-start-requests-method .section}</p>
<h5 id="a-shortcut-to-the-start_requests-methodheaderlink"><a class="header" href="#a-shortcut-to-the-start_requests-methodheaderlink">A shortcut to the start_requests method<a href="#a-shortcut-to-the-start-requests-method" title="Permalink to this heading">¶</a>{.headerlink}</a></h5>
<p>Instead of implementing a <a href="index.html#scrapy.Spider.start_requests" title="scrapy.Spider.start_requests">[<code>start_requests()</code>{.xref .py .py-meth
.docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal} method that generates [<code>scrapy.Request</code>{.xref .py .py-class
.docutils .literal .notranslate}]{.pre} objects from URLs, you can just
define a <a href="index.html#scrapy.Spider.start_urls" title="scrapy.Spider.start_urls">[<code>start_urls</code>{.xref .py .py-attr .docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal} class attribute with a list of URLs. This list will then be
used by the default implementation of <a href="index.html#scrapy.Spider.start_requests" title="scrapy.Spider.start_requests">[<code>start_requests()</code>{.xref .py
.py-meth .docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal} to create the initial requests for your spider.</p>
<p>::: {.highlight-python .notranslate}
::: highlight
from pathlib import Path</p>
<pre><code>import scrapy


class QuotesSpider(scrapy.Spider):
    name = &quot;quotes&quot;
    start_urls = [
        &quot;https://quotes.toscrape.com/page/1/&quot;,
        &quot;https://quotes.toscrape.com/page/2/&quot;,
    ]

    def parse(self, response):
        page = response.url.split(&quot;/&quot;)[-2]
        filename = f&quot;quotes-{page}.html&quot;
        Path(filename).write_bytes(response.body)
</code></pre>
<p>:::
:::</p>
<p>The <a href="index.html#scrapy.Spider.parse" title="scrapy.Spider.parse">[<code>parse()</code>{.xref .py .py-meth .docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal} method will be called to handle each of the requests for
those URLs, even though we haven't explicitly told Scrapy to do so. This
happens because <a href="index.html#scrapy.Spider.parse" title="scrapy.Spider.parse">[<code>parse()</code>{.xref .py .py-meth .docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal} is Scrapy's default callback method, which is called for
requests without an explicitly assigned callback.
:::</p>
<p>::: {#extracting-data .section}</p>
<h5 id="extracting-dataheaderlink"><a class="header" href="#extracting-dataheaderlink">Extracting data<a href="#extracting-data" title="Permalink to this heading">¶</a>{.headerlink}</a></h5>
<p>The best way to learn how to extract data with Scrapy is trying
selectors using the <a href="index.html#topics-shell">[Scrapy shell]{.std
.std-ref}</a>{.hoverxref .tooltip .reference
.internal}. Run:</p>
<p>::: {.highlight-default .notranslate}
::: highlight
scrapy shell 'https://quotes.toscrape.com/page/1/'
:::
:::</p>
<p>::: {.admonition .note}
Note</p>
<p>Remember to always enclose urls in quotes when running Scrapy shell from
command-line, otherwise urls containing arguments (i.e. [<code>&amp;</code>{.docutils
.literal .notranslate}]{.pre} character) will not work.</p>
<p>On Windows, use double quotes instead:</p>
<p>::: {.highlight-default .notranslate}
::: highlight
scrapy shell &quot;https://quotes.toscrape.com/page/1/&quot;
:::
:::
:::</p>
<p>You will see something like:</p>
<p>::: {.highlight-default .notranslate}
::: highlight
[ ... Scrapy log here ... ]
2016-09-19 12:09:27 [scrapy.core.engine] DEBUG: Crawled (200) &lt;GET https://quotes.toscrape.com/page/1/&gt; (referer: None)
[s] Available Scrapy objects:
[s]   scrapy     scrapy module (contains scrapy.Request, scrapy.Selector, etc)
[s]   crawler    &lt;scrapy.crawler.Crawler object at 0x7fa91d888c90&gt;
[s]   item       {}
[s]   request    &lt;GET https://quotes.toscrape.com/page/1/&gt;
[s]   response   &lt;200 https://quotes.toscrape.com/page/1/&gt;
[s]   settings   &lt;scrapy.settings.Settings object at 0x7fa91d888c10&gt;
[s]   spider     &lt;DefaultSpider 'default' at 0x7fa91c8af990&gt;
[s] Useful shortcuts:
[s]   shelp()           Shell help (print this help)
[s]   fetch(req_or_url) Fetch request (or URL) and update local objects
[s]   view(response)    View response in a browser
:::
:::</p>
<p>Using the shell, you can try selecting elements using
<a href="https://www.w3.org/TR/selectors">CSS</a>{.reference .external} with the
response object:</p>
<p>::: {.highlight-pycon .notranslate}
::: highlight
&gt;&gt;&gt; response.css(&quot;title&quot;)
[<Selector query='descendant-or-self::title' data='<title>Quotes to Scrape</title>'>]
:::
:::</p>
<p>The result of running [<code>response.css('title')</code>{.docutils .literal
.notranslate}]{.pre} is a list-like object called <a href="index.html#scrapy.selector.SelectorList" title="scrapy.selector.SelectorList">[<code>SelectorList</code>{.xref
.py .py-class .docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal}, which represents a list of [<code>Selector</code>{.xref .py .py-class
.docutils .literal .notranslate}]{.pre} objects that wrap around
XML/HTML elements and allow you to run further queries to fine-grain the
selection or extract the data.</p>
<p>To extract the text from the title above, you can do:</p>
<p>::: {.highlight-pycon .notranslate}
::: highlight
&gt;&gt;&gt; response.css(&quot;title::text&quot;).getall()
['Quotes to Scrape']
:::
:::</p>
<p>There are two things to note here: one is that we've added
[<code>::text</code>{.docutils .literal .notranslate}]{.pre} to the CSS query, to
mean we want to select only the text elements directly inside
[<code>&lt;title&gt;</code>{.docutils .literal .notranslate}]{.pre} element. If we don't
specify [<code>::text</code>{.docutils .literal .notranslate}]{.pre}, we'd get the
full title element, including its tags:</p>
<p>::: {.highlight-pycon .notranslate}
::: highlight
&gt;&gt;&gt; response.css(&quot;title&quot;).getall()
['<title>Quotes to Scrape</title>']
:::
:::</p>
<p>The other thing is that the result of calling [<code>.getall()</code>{.docutils
.literal .notranslate}]{.pre} is a list: it is possible that a selector
returns more than one result, so we extract them all. When you know you
just want the first result, as in this case, you can do:</p>
<p>::: {.highlight-pycon .notranslate}
::: highlight
&gt;&gt;&gt; response.css(&quot;title::text&quot;).get()
'Quotes to Scrape'
:::
:::</p>
<p>As an alternative, you could've written:</p>
<p>::: {.highlight-pycon .notranslate}
::: highlight
&gt;&gt;&gt; response.css(&quot;title::text&quot;)[0].get()
'Quotes to Scrape'
:::
:::</p>
<p>Accessing an index on a <a href="index.html#scrapy.selector.SelectorList" title="scrapy.selector.SelectorList">[<code>SelectorList</code>{.xref .py .py-class .docutils
.literal
.notranslate}]{.pre}</a>{.reference
.internal} instance will raise an <a href="https://docs.python.org/3/library/exceptions.html#IndexError" title="(in Python v3.12)">[<code>IndexError</code>{.xref .py .py-exc
.docutils .literal
.notranslate}]{.pre}</a>{.reference
.external} exception if there are no results:</p>
<p>::: {.highlight-pycon .notranslate}
::: highlight
&gt;&gt;&gt; response.css(&quot;noelement&quot;)[0].get()
Traceback (most recent call last):
...
IndexError: list index out of range
:::
:::</p>
<p>You might want to use [<code>.get()</code>{.docutils .literal .notranslate}]{.pre}
directly on the <a href="index.html#scrapy.selector.SelectorList" title="scrapy.selector.SelectorList">[<code>SelectorList</code>{.xref .py .py-class .docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal} instance instead, which returns [<code>None</code>{.docutils .literal
.notranslate}]{.pre} if there are no results:</p>
<p>::: {.highlight-pycon .notranslate}
::: highlight
&gt;&gt;&gt; response.css(&quot;noelement&quot;).get()
:::
:::</p>
<p>There's a lesson here: for most scraping code, you want it to be
resilient to errors due to things not being found on a page, so that
even if some parts fail to be scraped, you can at least get <strong>some</strong>
data.</p>
<p>Besides the <a href="index.html#scrapy.selector.SelectorList.getall" title="scrapy.selector.SelectorList.getall">[<code>getall()</code>{.xref .py .py-meth .docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal} and <a href="index.html#scrapy.selector.SelectorList.get" title="scrapy.selector.SelectorList.get">[<code>get()</code>{.xref .py .py-meth .docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal} methods, you can also use the <a href="index.html#scrapy.selector.SelectorList.re" title="scrapy.selector.SelectorList.re">[<code>re()</code>{.xref .py .py-meth
.docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal} method to extract using <a href="https://docs.python.org/3/library/re.html" title="(in Python v3.12)">[regular expressions]{.xref .std
.std-doc}</a>{.reference
.external}:</p>
<p>::: {.highlight-pycon .notranslate}
::: highlight
&gt;&gt;&gt; response.css(&quot;title::text&quot;).re(r&quot;Quotes.*&quot;)
['Quotes to Scrape']
&gt;&gt;&gt; response.css(&quot;title::text&quot;).re(r&quot;Q\w+&quot;)
['Quotes']
&gt;&gt;&gt; response.css(&quot;title::text&quot;).re(r&quot;(\w+) to (\w+)&quot;)
['Quotes', 'Scrape']
:::
:::</p>
<p>In order to find the proper CSS selectors to use, you might find it
useful to open the response page from the shell in your web browser
using [<code>view(response)</code>{.docutils .literal .notranslate}]{.pre}. You can
use your browser's developer tools to inspect the HTML and come up with
a selector (see <a href="index.html#topics-developer-tools">[Using your browser's Developer Tools for
scraping]{.std .std-ref}</a>{.hoverxref
.tooltip .reference .internal}).</p>
<p><a href="https://selectorgadget.com/">Selector Gadget</a>{.reference .external} is
also a nice tool to quickly find CSS selector for visually selected
elements, which works in many browsers.</p>
<p>::: {#xpath-a-brief-intro .section}</p>
<h6 id="xpath-a-brief-introheaderlink"><a class="header" href="#xpath-a-brief-introheaderlink">XPath: a brief intro<a href="#xpath-a-brief-intro" title="Permalink to this heading">¶</a>{.headerlink}</a></h6>
<p>Besides <a href="https://www.w3.org/TR/selectors">CSS</a>{.reference .external},
Scrapy selectors also support using
<a href="https://www.w3.org/TR/xpath/all/">XPath</a>{.reference .external}
expressions:</p>
<p>::: {.highlight-pycon .notranslate}
::: highlight
&gt;&gt;&gt; response.xpath(&quot;//title&quot;)
[<Selector query='//title' data='<title>Quotes to Scrape</title>'>]
&gt;&gt;&gt; response.xpath(&quot;//title/text()&quot;).get()
'Quotes to Scrape'
:::
:::</p>
<p>XPath expressions are very powerful, and are the foundation of Scrapy
Selectors. In fact, CSS selectors are converted to XPath under-the-hood.
You can see that if you read closely the text representation of the
selector objects in the shell.</p>
<p>While perhaps not as popular as CSS selectors, XPath expressions offer
more power because besides navigating the structure, it can also look at
the content. Using XPath, you're able to select things like: <em>select the
link that contains the text &quot;Next Page&quot;</em>. This makes XPath very fitting
to the task of scraping, and we encourage you to learn XPath even if you
already know how to construct CSS selectors, it will make scraping much
easier.</p>
<p>We won't cover much of XPath here, but you can read more about <a href="index.html#topics-selectors">[using
XPath with Scrapy Selectors here]{.std
.std-ref}</a>{.hoverxref .tooltip .reference
.internal}. To learn more about XPath, we recommend <a href="http://zvon.org/comp/r/tut-XPath_1.html">this tutorial to
learn XPath through
examples</a>{.reference
.external}, and <a href="http://plasmasturm.org/log/xpath101/">this tutorial to learn &quot;how to think in
XPath&quot;</a>{.reference .external}.
:::</p>
<p>::: {#extracting-quotes-and-authors .section}</p>
<h6 id="extracting-quotes-and-authorsheaderlink"><a class="header" href="#extracting-quotes-and-authorsheaderlink">Extracting quotes and authors<a href="#extracting-quotes-and-authors" title="Permalink to this heading">¶</a>{.headerlink}</a></h6>
<p>Now that you know a bit about selection and extraction, let's complete
our spider by writing the code to extract the quotes from the web page.</p>
<p>Each quote in
<a href="https://quotes.toscrape.com">https://quotes.toscrape.com</a>{.reference
.external} is represented by HTML elements that look like this:</p>
<p>::: {.highlight-html .notranslate}
::: highlight
<div class="quote">
<span class="text">“The world as we have created it is a process of our
thinking. It cannot be changed without changing our thinking.”</span>
<span>
by <small class="author">Albert Einstein</small>
<a href="/author/Albert-Einstein">(about)</a>
</span>
<div class="tags">
Tags:
<a class="tag" href="/tag/change/page/1/">change</a>
<a class="tag" href="/tag/deep-thoughts/page/1/">deep-thoughts</a>
<a class="tag" href="/tag/thinking/page/1/">thinking</a>
<a class="tag" href="/tag/world/page/1/">world</a>
</div>
</div>
:::
:::</p>
<p>Let's open up scrapy shell and play a bit to find out how to extract the
data we want:</p>
<p>::: {.highlight-default .notranslate}
::: highlight
scrapy shell 'https://quotes.toscrape.com'
:::
:::</p>
<p>We get a list of selectors for the quote HTML elements with:</p>
<p>::: {.highlight-pycon .notranslate}
::: highlight
&gt;&gt;&gt; response.css(&quot;div.quote&quot;)
[<Selector query="descendant-or-self::div[@class and contains(concat(' ', normalize-space(@class), ' '), ' quote ')]" data='<div class="quote" itemscope itemtype...'>,
<Selector query="descendant-or-self::div[@class and contains(concat(' ', normalize-space(@class), ' '), ' quote ')]" data='<div class="quote" itemscope itemtype...'>,
...]
:::
:::</p>
<p>Each of the selectors returned by the query above allows us to run
further queries over their sub-elements. Let's assign the first selector
to a variable, so that we can run our CSS selectors directly on a
particular quote:</p>
<p>::: {.highlight-pycon .notranslate}
::: highlight
&gt;&gt;&gt; quote = response.css(&quot;div.quote&quot;)[0]
:::
:::</p>
<p>Now, let's extract [<code>text</code>{.docutils .literal .notranslate}]{.pre},
[<code>author</code>{.docutils .literal .notranslate}]{.pre} and the
[<code>tags</code>{.docutils .literal .notranslate}]{.pre} from that quote using
the [<code>quote</code>{.docutils .literal .notranslate}]{.pre} object we just
created:</p>
<p>::: {.highlight-pycon .notranslate}
::: highlight
&gt;&gt;&gt; text = quote.css(&quot;span.text::text&quot;).get()
&gt;&gt;&gt; text
'“The world as we have created it is a process of our thinking. It cannot be changed without changing our thinking.”'
&gt;&gt;&gt; author = quote.css(&quot;small.author::text&quot;).get()
&gt;&gt;&gt; author
'Albert Einstein'
:::
:::</p>
<p>Given that the tags are a list of strings, we can use the
[<code>.getall()</code>{.docutils .literal .notranslate}]{.pre} method to get all
of them:</p>
<p>::: {.highlight-pycon .notranslate}
::: highlight
&gt;&gt;&gt; tags = quote.css(&quot;div.tags a.tag::text&quot;).getall()
&gt;&gt;&gt; tags
['change', 'deep-thoughts', 'thinking', 'world']
:::
:::</p>
<p>Having figured out how to extract each bit, we can now iterate over all
the quotes elements and put them together into a Python dictionary:</p>
<p>::: {.highlight-pycon .notranslate}
::: highlight
&gt;&gt;&gt; for quote in response.css(&quot;div.quote&quot;):
...     text = quote.css(&quot;span.text::text&quot;).get()
...     author = quote.css(&quot;small.author::text&quot;).get()
...     tags = quote.css(&quot;div.tags a.tag::text&quot;).getall()
...     print(dict(text=text, author=author, tags=tags))
...
{'text': '“The world as we have created it is a process of our thinking. It cannot be changed without changing our thinking.”', 'author': 'Albert Einstein', 'tags': ['change', 'deep-thoughts', 'thinking', 'world']}
{'text': '“It is our choices, Harry, that show what we truly are, far more than our abilities.”', 'author': 'J.K. Rowling', 'tags': ['abilities', 'choices']}
...
:::
:::
:::
:::</p>
<p>::: {#extracting-data-in-our-spider .section}</p>
<h5 id="extracting-data-in-our-spiderheaderlink"><a class="header" href="#extracting-data-in-our-spiderheaderlink">Extracting data in our spider<a href="#extracting-data-in-our-spider" title="Permalink to this heading">¶</a>{.headerlink}</a></h5>
<p>Let's get back to our spider. Until now, it doesn't extract any data in
particular, just saves the whole HTML page to a local file. Let's
integrate the extraction logic above into our spider.</p>
<p>A Scrapy spider typically generates many dictionaries containing the
data extracted from the page. To do that, we use the [<code>yield</code>{.docutils
.literal .notranslate}]{.pre} Python keyword in the callback, as you can
see below:</p>
<p>::: {.highlight-python .notranslate}
::: highlight
import scrapy</p>
<pre><code>class QuotesSpider(scrapy.Spider):
    name = &quot;quotes&quot;
    start_urls = [
        &quot;https://quotes.toscrape.com/page/1/&quot;,
        &quot;https://quotes.toscrape.com/page/2/&quot;,
    ]

    def parse(self, response):
        for quote in response.css(&quot;div.quote&quot;):
            yield {
                &quot;text&quot;: quote.css(&quot;span.text::text&quot;).get(),
                &quot;author&quot;: quote.css(&quot;small.author::text&quot;).get(),
                &quot;tags&quot;: quote.css(&quot;div.tags a.tag::text&quot;).getall(),
            }
</code></pre>
<p>:::
:::</p>
<p>To run this spider, exit the scrapy shell by entering:</p>
<p>::: {.highlight-default .notranslate}
::: highlight
quit()
:::
:::</p>
<p>Then, run:</p>
<p>::: {.highlight-default .notranslate}
::: highlight
scrapy crawl quotes
:::
:::</p>
<p>Now, it should output the extracted data with the log:</p>
<p>::: {.highlight-default .notranslate}
::: highlight
2016-09-19 18:57:19 [scrapy.core.scraper] DEBUG: Scraped from &lt;200 https://quotes.toscrape.com/page/1/&gt;
{'tags': ['life', 'love'], 'author': 'André Gide', 'text': '“It is better to be hated for what you are than to be loved for what you are not.”'}
2016-09-19 18:57:19 [scrapy.core.scraper] DEBUG: Scraped from &lt;200 https://quotes.toscrape.com/page/1/&gt;
{'tags': ['edison', 'failure', 'inspirational', 'paraphrased'], 'author': 'Thomas A. Edison', 'text': &quot;“I have not failed. I've just found 10,000 ways that won't work.”&quot;}
:::
:::
:::
:::</p>
<p>::: {#storing-the-scraped-data .section}
[]{#storing-data}</p>
<h4 id="storing-the-scraped-dataheaderlink"><a class="header" href="#storing-the-scraped-dataheaderlink">Storing the scraped data<a href="#storing-the-scraped-data" title="Permalink to this heading">¶</a>{.headerlink}</a></h4>
<p>The simplest way to store the scraped data is by using <a href="index.html#topics-feed-exports">[Feed
exports]{.std .std-ref}</a>{.hoverxref
.tooltip .reference .internal}, with the following command:</p>
<p>::: {.highlight-default .notranslate}
::: highlight
scrapy crawl quotes -O quotes.json
:::
:::</p>
<p>That will generate a [<code>quotes.json</code>{.docutils .literal
.notranslate}]{.pre} file containing all scraped items, serialized in
<a href="https://en.wikipedia.org/wiki/JSON">JSON</a>{.reference .external}.</p>
<p>The [<code>-O</code>{.docutils .literal .notranslate}]{.pre} command-line switch
overwrites any existing file; use [<code>-o</code>{.docutils .literal
.notranslate}]{.pre} instead to append new content to any existing file.
However, appending to a JSON file makes the file contents invalid JSON.
When appending to a file, consider using a different serialization
format, such as <a href="http://jsonlines.org">JSON Lines</a>{.reference
.external}:</p>
<p>::: {.highlight-default .notranslate}
::: highlight
scrapy crawl quotes -o quotes.jsonl
:::
:::</p>
<p>The <a href="http://jsonlines.org">JSON Lines</a>{.reference .external} format is
useful because it's stream-like, you can easily append new records to
it. It doesn't have the same problem of JSON when you run twice. Also,
as each record is a separate line, you can process big files without
having to fit everything in memory, there are tools like
<a href="https://stedolan.github.io/jq">JQ</a>{.reference .external} to help do
that at the command-line.</p>
<p>In small projects (like the one in this tutorial), that should be
enough. However, if you want to perform more complex things with the
scraped items, you can write an <a href="index.html#topics-item-pipeline">[Item Pipeline]{.std
.std-ref}</a>{.hoverxref .tooltip
.reference .internal}. A placeholder file for Item Pipelines has been
set up for you when the project is created, in
[<code>tutorial/pipelines.py</code>{.docutils .literal .notranslate}]{.pre}. Though
you don't need to implement any item pipelines if you just want to store
the scraped items.
:::</p>
<p>::: {#following-links .section}</p>
<h4 id="following-linksheaderlink"><a class="header" href="#following-linksheaderlink">Following links<a href="#following-links" title="Permalink to this heading">¶</a>{.headerlink}</a></h4>
<p>Let's say, instead of just scraping the stuff from the first two pages
from
<a href="https://quotes.toscrape.com">https://quotes.toscrape.com</a>{.reference
.external}, you want quotes from all the pages in the website.</p>
<p>Now that you know how to extract data from pages, let's see how to
follow links from them.</p>
<p>First thing is to extract the link to the page we want to follow.
Examining our page, we can see there is a link to the next page with the
following markup:</p>
<p>::: {.highlight-html .notranslate}
::: highlight
<ul class="pager">
<li class="next">
<a href="/page/2/">Next <span aria-hidden="true">→</span></a>
</li>
</ul>
:::
:::</p>
<p>We can try extracting it in the shell:</p>
<p>::: {.doctest .highlight-default .notranslate}
::: highlight
&gt;&gt;&gt; response.css('li.next a').get()
'<a href="/page/2/">Next <span aria-hidden="true">→</span></a>'
:::
:::</p>
<p>This gets the anchor element, but we want the attribute
[<code>href</code>{.docutils .literal .notranslate}]{.pre}. For that, Scrapy
supports a CSS extension that lets you select the attribute contents,
like this:</p>
<p>::: {.highlight-pycon .notranslate}
::: highlight
&gt;&gt;&gt; response.css(&quot;li.next a::attr(href)&quot;).get()
'/page/2/'
:::
:::</p>
<p>There is also an [<code>attrib</code>{.docutils .literal .notranslate}]{.pre}
property available (see <a href="index.html#selecting-attributes">[Selecting element attributes]{.std
.std-ref}</a>{.hoverxref .tooltip
.reference .internal} for more):</p>
<p>::: {.highlight-pycon .notranslate}
::: highlight
&gt;&gt;&gt; response.css(&quot;li.next a&quot;).attrib[&quot;href&quot;]
'/page/2/'
:::
:::</p>
<p>Let's see now our spider modified to recursively follow the link to the
next page, extracting data from it:</p>
<p>::: {.highlight-python .notranslate}
::: highlight
import scrapy</p>
<pre><code>class QuotesSpider(scrapy.Spider):
    name = &quot;quotes&quot;
    start_urls = [
        &quot;https://quotes.toscrape.com/page/1/&quot;,
    ]

    def parse(self, response):
        for quote in response.css(&quot;div.quote&quot;):
            yield {
                &quot;text&quot;: quote.css(&quot;span.text::text&quot;).get(),
                &quot;author&quot;: quote.css(&quot;small.author::text&quot;).get(),
                &quot;tags&quot;: quote.css(&quot;div.tags a.tag::text&quot;).getall(),
            }

        next_page = response.css(&quot;li.next a::attr(href)&quot;).get()
        if next_page is not None:
            next_page = response.urljoin(next_page)
            yield scrapy.Request(next_page, callback=self.parse)
</code></pre>
<p>:::
:::</p>
<p>Now, after extracting the data, the [<code>parse()</code>{.docutils .literal
.notranslate}]{.pre} method looks for the link to the next page, builds
a full absolute URL using the <a href="index.html#scrapy.http.Response.urljoin" title="scrapy.http.Response.urljoin">[<code>urljoin()</code>{.xref .py .py-meth .docutils
.literal
.notranslate}]{.pre}</a>{.reference
.internal} method (since the links can be relative) and yields a new
request to the next page, registering itself as callback to handle the
data extraction for the next page and to keep the crawling going through
all the pages.</p>
<p>What you see here is Scrapy's mechanism of following links: when you
yield a Request in a callback method, Scrapy will schedule that request
to be sent and register a callback method to be executed when that
request finishes.</p>
<p>Using this, you can build complex crawlers that follow links according
to rules you define, and extract different kinds of data depending on
the page it's visiting.</p>
<p>In our example, it creates a sort of loop, following all the links to
the next page until it doesn't find one -- handy for crawling blogs,
forums and other sites with pagination.</p>
<p>::: {#a-shortcut-for-creating-requests .section}
[]{#response-follow-example}</p>
<h5 id="a-shortcut-for-creating-requestsheaderlink"><a class="header" href="#a-shortcut-for-creating-requestsheaderlink">A shortcut for creating Requests<a href="#a-shortcut-for-creating-requests" title="Permalink to this heading">¶</a>{.headerlink}</a></h5>
<p>As a shortcut for creating Request objects you can use
<a href="index.html#scrapy.http.TextResponse.follow" title="scrapy.http.TextResponse.follow">[<code>response.follow</code>{.xref .py .py-meth .docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal}:</p>
<p>::: {.highlight-python .notranslate}
::: highlight
import scrapy</p>
<pre><code>class QuotesSpider(scrapy.Spider):
    name = &quot;quotes&quot;
    start_urls = [
        &quot;https://quotes.toscrape.com/page/1/&quot;,
    ]

    def parse(self, response):
        for quote in response.css(&quot;div.quote&quot;):
            yield {
                &quot;text&quot;: quote.css(&quot;span.text::text&quot;).get(),
                &quot;author&quot;: quote.css(&quot;span small::text&quot;).get(),
                &quot;tags&quot;: quote.css(&quot;div.tags a.tag::text&quot;).getall(),
            }

        next_page = response.css(&quot;li.next a::attr(href)&quot;).get()
        if next_page is not None:
            yield response.follow(next_page, callback=self.parse)
</code></pre>
<p>:::
:::</p>
<p>Unlike scrapy.Request, [<code>response.follow</code>{.docutils .literal
.notranslate}]{.pre} supports relative URLs directly - no need to call
urljoin. Note that [<code>response.follow</code>{.docutils .literal
.notranslate}]{.pre} just returns a Request instance; you still have to
yield this Request.</p>
<p>You can also pass a selector to [<code>response.follow</code>{.docutils .literal
.notranslate}]{.pre} instead of a string; this selector should extract
necessary attributes:</p>
<p>::: {.highlight-python .notranslate}
::: highlight
for href in response.css(&quot;ul.pager a::attr(href)&quot;):
yield response.follow(href, callback=self.parse)
:::
:::</p>
<p>For [<code>&lt;a&gt;</code>{.docutils .literal .notranslate}]{.pre} elements there is a
shortcut: [<code>response.follow</code>{.docutils .literal .notranslate}]{.pre}
uses their href attribute automatically. So the code can be shortened
further:</p>
<p>::: {.highlight-python .notranslate}
::: highlight
for a in response.css(&quot;ul.pager a&quot;):
yield response.follow(a, callback=self.parse)
:::
:::</p>
<p>To create multiple requests from an iterable, you can use
<a href="index.html#scrapy.http.TextResponse.follow_all" title="scrapy.http.TextResponse.follow_all">[<code>response.follow_all</code>{.xref .py .py-meth .docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal} instead:</p>
<p>::: {.highlight-python .notranslate}
::: highlight
anchors = response.css(&quot;ul.pager a&quot;)
yield from response.follow_all(anchors, callback=self.parse)
:::
:::</p>
<p>or, shortening it further:</p>
<p>::: {.highlight-python .notranslate}
::: highlight
yield from response.follow_all(css=&quot;ul.pager a&quot;, callback=self.parse)
:::
:::
:::</p>
<p>::: {#more-examples-and-patterns .section}</p>
<h5 id="more-examples-and-patternsheaderlink"><a class="header" href="#more-examples-and-patternsheaderlink">More examples and patterns<a href="#more-examples-and-patterns" title="Permalink to this heading">¶</a>{.headerlink}</a></h5>
<p>Here is another spider that illustrates callbacks and following links,
this time for scraping author information:</p>
<p>::: {.highlight-python .notranslate}
::: highlight
import scrapy</p>
<pre><code>class AuthorSpider(scrapy.Spider):
    name = &quot;author&quot;

    start_urls = [&quot;https://quotes.toscrape.com/&quot;]

    def parse(self, response):
        author_page_links = response.css(&quot;.author + a&quot;)
        yield from response.follow_all(author_page_links, self.parse_author)

        pagination_links = response.css(&quot;li.next a&quot;)
        yield from response.follow_all(pagination_links, self.parse)

    def parse_author(self, response):
        def extract_with_css(query):
            return response.css(query).get(default=&quot;&quot;).strip()

        yield {
            &quot;name&quot;: extract_with_css(&quot;h3.author-title::text&quot;),
            &quot;birthdate&quot;: extract_with_css(&quot;.author-born-date::text&quot;),
            &quot;bio&quot;: extract_with_css(&quot;.author-description::text&quot;),
        }
</code></pre>
<p>:::
:::</p>
<p>This spider will start from the main page, it will follow all the links
to the authors pages calling the [<code>parse_author</code>{.docutils .literal
.notranslate}]{.pre} callback for each of them, and also the pagination
links with the [<code>parse</code>{.docutils .literal .notranslate}]{.pre} callback
as we saw before.</p>
<p>Here we're passing callbacks to <a href="index.html#scrapy.http.TextResponse.follow_all" title="scrapy.http.TextResponse.follow_all">[<code>response.follow_all</code>{.xref .py
.py-meth .docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal} as positional arguments to make the code shorter; it also
works for [<code>Request</code>{.xref .py .py-class .docutils .literal
.notranslate}]{.pre}.</p>
<p>The [<code>parse_author</code>{.docutils .literal .notranslate}]{.pre} callback
defines a helper function to extract and cleanup the data from a CSS
query and yields the Python dict with the author data.</p>
<p>Another interesting thing this spider demonstrates is that, even if
there are many quotes from the same author, we don't need to worry about
visiting the same author page multiple times. By default, Scrapy filters
out duplicated requests to URLs already visited, avoiding the problem of
hitting servers too much because of a programming mistake. This can be
configured by the setting <a href="index.html#std-setting-DUPEFILTER_CLASS">[<code>DUPEFILTER_CLASS</code>{.xref .std .std-setting
.docutils .literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal}.</p>
<p>Hopefully by now you have a good understanding of how to use the
mechanism of following links and callbacks with Scrapy.</p>
<p>As yet another example spider that leverages the mechanism of following
links, check out the <a href="index.html#scrapy.spiders.CrawlSpider" title="scrapy.spiders.CrawlSpider">[<code>CrawlSpider</code>{.xref .py .py-class .docutils
.literal
.notranslate}]{.pre}</a>{.reference
.internal} class for a generic spider that implements a small rules
engine that you can use to write your crawlers on top of it.</p>
<p>Also, a common pattern is to build an item with data from more than one
page, using a <a href="index.html#topics-request-response-ref-request-callback-arguments">[trick to pass additional data to the callbacks]{.std
.std-ref}</a>{.hoverxref
.tooltip .reference .internal}.
:::
:::</p>
<p>::: {#using-spider-arguments .section}</p>
<h4 id="using-spider-argumentsheaderlink"><a class="header" href="#using-spider-argumentsheaderlink">Using spider arguments<a href="#using-spider-arguments" title="Permalink to this heading">¶</a>{.headerlink}</a></h4>
<p>You can provide command line arguments to your spiders by using the
[<code>-a</code>{.docutils .literal .notranslate}]{.pre} option when running them:</p>
<p>::: {.highlight-default .notranslate}
::: highlight
scrapy crawl quotes -O quotes-humor.json -a tag=humor
:::
:::</p>
<p>These arguments are passed to the Spider's [<code>__init__</code>{.docutils
.literal .notranslate}]{.pre} method and become spider attributes by
default.</p>
<p>In this example, the value provided for the [<code>tag</code>{.docutils .literal
.notranslate}]{.pre} argument will be available via
[<code>self.tag</code>{.docutils .literal .notranslate}]{.pre}. You can use this to
make your spider fetch only quotes with a specific tag, building the URL
based on the argument:</p>
<p>::: {.highlight-python .notranslate}
::: highlight
import scrapy</p>
<pre><code>class QuotesSpider(scrapy.Spider):
    name = &quot;quotes&quot;

    def start_requests(self):
        url = &quot;https://quotes.toscrape.com/&quot;
        tag = getattr(self, &quot;tag&quot;, None)
        if tag is not None:
            url = url + &quot;tag/&quot; + tag
        yield scrapy.Request(url, self.parse)

    def parse(self, response):
        for quote in response.css(&quot;div.quote&quot;):
            yield {
                &quot;text&quot;: quote.css(&quot;span.text::text&quot;).get(),
                &quot;author&quot;: quote.css(&quot;small.author::text&quot;).get(),
            }

        next_page = response.css(&quot;li.next a::attr(href)&quot;).get()
        if next_page is not None:
            yield response.follow(next_page, self.parse)
</code></pre>
<p>:::
:::</p>
<p>If you pass the [<code>tag=humor</code>{.docutils .literal .notranslate}]{.pre}
argument to this spider, you'll notice that it will only visit URLs from
the [<code>humor</code>{.docutils .literal .notranslate}]{.pre} tag, such as
[<code>https://quotes.toscrape.com/tag/humor</code>{.docutils .literal
.notranslate}]{.pre}.</p>
<p>You can <a href="index.html#spiderargs">[learn more about handling spider arguments here]{.std
.std-ref}</a>{.hoverxref .tooltip .reference
.internal}.
:::</p>
<p>::: {#next-steps .section}</p>
<h4 id="next-stepsheaderlink"><a class="header" href="#next-stepsheaderlink">Next steps<a href="#next-steps" title="Permalink to this heading">¶</a>{.headerlink}</a></h4>
<p>This tutorial covered only the basics of Scrapy, but there's a lot of
other features not mentioned here. Check the <a href="index.html#topics-whatelse">[What else?]{.std
.std-ref}</a>{.hoverxref .tooltip .reference
.internal} section in <a href="index.html#intro-overview">[Scrapy at a glance]{.std
.std-ref}</a>{.hoverxref .tooltip .reference
.internal} chapter for a quick overview of the most important ones.</p>
<p>You can continue from the section <a href="index.html#section-basics">[Basic concepts]{.std
.std-ref}</a>{.hoverxref .tooltip .reference
.internal} to know more about the command-line tool, spiders, selectors
and other things the tutorial hasn't covered like modeling the scraped
data. If you prefer to play with an example project, check the
<a href="index.html#intro-examples">[Examples]{.std .std-ref}</a>{.hoverxref
.tooltip .reference .internal} section.
:::
:::</p>
<p>[]{#document-intro/examples}</p>
<p>::: {#examples .section}
[]{#intro-examples}</p>
<h3 id="examplesheaderlink"><a class="header" href="#examplesheaderlink">Examples<a href="#examples" title="Permalink to this heading">¶</a>{.headerlink}</a></h3>
<p>The best way to learn is with examples, and Scrapy is no exception. For
this reason, there is an example Scrapy project named
<a href="https://github.com/scrapy/quotesbot">quotesbot</a>{.reference .external},
that you can use to play and learn more about Scrapy. It contains two
spiders for
<a href="https://quotes.toscrape.com">https://quotes.toscrape.com</a>{.reference
.external}, one using CSS selectors and another one using XPath
expressions.</p>
<p>The <a href="https://github.com/scrapy/quotesbot">quotesbot</a>{.reference
.external} project is available at:
<a href="https://github.com/scrapy/quotesbot">https://github.com/scrapy/quotesbot</a>{.reference
.external}. You can find more information about it in the project's
README.</p>
<p>If you're familiar with git, you can checkout the code. Otherwise you
can download the project as a zip file by clicking
<a href="https://github.com/scrapy/quotesbot/archive/master.zip">here</a>{.reference
.external}.
:::
:::</p>
<p><a href="index.html#document-intro/overview">[Scrapy at a glance]{.doc}</a>{.reference .internal}</p>
<p>:   Understand what Scrapy is and how it can help you.</p>
<p><a href="index.html#document-intro/install">[Installation guide]{.doc}</a>{.reference .internal}</p>
<p>:   Get Scrapy installed on your computer.</p>
<p><a href="index.html#document-intro/tutorial">[Scrapy Tutorial]{.doc}</a>{.reference .internal}</p>
<p>:   Write your first Scrapy project.</p>
<p><a href="index.html#document-intro/examples">[Examples]{.doc}</a>{.reference .internal}</p>
<p>:   Learn more by playing with a pre-made Scrapy project.
:::</p>
<p>::: {#basic-concepts .section}
[]{#section-basics}</p>
<h2 id="basic-conceptsheaderlink"><a class="header" href="#basic-conceptsheaderlink">Basic concepts<a href="#basic-concepts" title="Permalink to this heading">¶</a>{.headerlink}</a></h2>
<p>::: {.toctree-wrapper .compound}
[]{#document-topics/commands}</p>
<p>::: {#command-line-tool .section}
[]{#topics-commands}</p>
<h3 id="command-line-toolheaderlink"><a class="header" href="#command-line-toolheaderlink">Command line tool<a href="#command-line-tool" title="Permalink to this heading">¶</a>{.headerlink}</a></h3>
<p>Scrapy is controlled through the [<code>scrapy</code>{.docutils .literal
.notranslate}]{.pre} command-line tool, to be referred here as the
&quot;Scrapy tool&quot; to differentiate it from the sub-commands, which we just
call &quot;commands&quot; or &quot;Scrapy commands&quot;.</p>
<p>The Scrapy tool provides several commands, for multiple purposes, and
each one accepts a different set of arguments and options.</p>
<p>(The [<code>scrapy</code>{.docutils .literal .notranslate}]{.pre}<code> </code>{.docutils
.literal .notranslate}[<code>deploy</code>{.docutils .literal .notranslate}]{.pre}
command has been removed in 1.0 in favor of the standalone
[<code>scrapyd-deploy</code>{.docutils .literal .notranslate}]{.pre}. See
<a href="https://scrapyd.readthedocs.io/en/latest/deploy.html">Deploying your
project</a>{.reference
.external}.)</p>
<p>::: {#configuration-settings .section}
[]{#topics-config-settings}</p>
<h4 id="configuration-settingsheaderlink"><a class="header" href="#configuration-settingsheaderlink">Configuration settings<a href="#configuration-settings" title="Permalink to this heading">¶</a>{.headerlink}</a></h4>
<p>Scrapy will look for configuration parameters in ini-style
[<code>scrapy.cfg</code>{.docutils .literal .notranslate}]{.pre} files in standard
locations:</p>
<ol>
<li>
<p>[<code>/etc/scrapy.cfg</code>{.docutils .literal .notranslate}]{.pre} or
[<code>c:\scrapy\scrapy.cfg</code>{.docutils .literal .notranslate}]{.pre}
(system-wide),</p>
</li>
<li>
<p>[<code>~/.config/scrapy.cfg</code>{.docutils .literal .notranslate}]{.pre}
([<code>$XDG_CONFIG_HOME</code>{.docutils .literal .notranslate}]{.pre}) and
[<code>~/.scrapy.cfg</code>{.docutils .literal .notranslate}]{.pre}
([<code>$HOME</code>{.docutils .literal .notranslate}]{.pre}) for global
(user-wide) settings, and</p>
</li>
<li>
<p>[<code>scrapy.cfg</code>{.docutils .literal .notranslate}]{.pre} inside a
Scrapy project's root (see next section).</p>
</li>
</ol>
<p>Settings from these files are merged in the listed order of preference:
user-defined values have higher priority than system-wide defaults and
project-wide settings will override all others, when defined.</p>
<p>Scrapy also understands, and can be configured through, a number of
environment variables. Currently these are:</p>
<ul>
<li>
<p>[<code>SCRAPY_SETTINGS_MODULE</code>{.docutils .literal .notranslate}]{.pre}
(see <a href="index.html#topics-settings-module-envvar">[Designating the settings]{.std
.std-ref}</a>{.hoverxref
.tooltip .reference .internal})</p>
</li>
<li>
<p>[<code>SCRAPY_PROJECT</code>{.docutils .literal .notranslate}]{.pre} (see
<a href="#topics-project-envvar">[Sharing the root directory between projects]{.std
.std-ref}</a>{.hoverxref .tooltip .reference
.internal})</p>
</li>
<li>
<p>[<code>SCRAPY_PYTHON_SHELL</code>{.docutils .literal .notranslate}]{.pre} (see
<a href="index.html#topics-shell">[Scrapy shell]{.std .std-ref}</a>{.hoverxref
.tooltip .reference .internal})
:::</p>
</li>
</ul>
<p>::: {#default-structure-of-scrapy-projects .section}
[]{#topics-project-structure}</p>
<h4 id="default-structure-of-scrapy-projectsheaderlink"><a class="header" href="#default-structure-of-scrapy-projectsheaderlink">Default structure of Scrapy projects<a href="#default-structure-of-scrapy-projects" title="Permalink to this heading">¶</a>{.headerlink}</a></h4>
<p>Before delving into the command-line tool and its sub-commands, let's
first understand the directory structure of a Scrapy project.</p>
<p>Though it can be modified, all Scrapy projects have the same file
structure by default, similar to this:</p>
<p>::: {.highlight-none .notranslate}
::: highlight
scrapy.cfg
myproject/
<strong>init</strong>.py
items.py
middlewares.py
pipelines.py
settings.py
spiders/
<strong>init</strong>.py
spider1.py
spider2.py
...
:::
:::</p>
<p>The directory where the [<code>scrapy.cfg</code>{.docutils .literal
.notranslate}]{.pre} file resides is known as the <em>project root
directory</em>. That file contains the name of the python module that
defines the project settings. Here is an example:</p>
<p>::: {.highlight-ini .notranslate}
::: highlight
[settings]
default = myproject.settings
:::
:::
:::</p>
<p>::: {#sharing-the-root-directory-between-projects .section}
[]{#topics-project-envvar}</p>
<h4 id="sharing-the-root-directory-between-projectsheaderlink"><a class="header" href="#sharing-the-root-directory-between-projectsheaderlink">Sharing the root directory between projects<a href="#sharing-the-root-directory-between-projects" title="Permalink to this heading">¶</a>{.headerlink}</a></h4>
<p>A project root directory, the one that contains the
[<code>scrapy.cfg</code>{.docutils .literal .notranslate}]{.pre}, may be shared by
multiple Scrapy projects, each with its own settings module.</p>
<p>In that case, you must define one or more aliases for those settings
modules under [<code>[settings]</code>{.docutils .literal .notranslate}]{.pre} in
your [<code>scrapy.cfg</code>{.docutils .literal .notranslate}]{.pre} file:</p>
<p>::: {.highlight-ini .notranslate}
::: highlight
[settings]
default = myproject1.settings
project1 = myproject1.settings
project2 = myproject2.settings
:::
:::</p>
<p>By default, the [<code>scrapy</code>{.docutils .literal .notranslate}]{.pre}
command-line tool will use the [<code>default</code>{.docutils .literal
.notranslate}]{.pre} settings. Use the [<code>SCRAPY_PROJECT</code>{.docutils
.literal .notranslate}]{.pre} environment variable to specify a
different project for [<code>scrapy</code>{.docutils .literal .notranslate}]{.pre}
to use:</p>
<p>::: {.highlight-none .notranslate}
::: highlight
$ scrapy settings --get BOT_NAME
Project 1 Bot
$ export SCRAPY_PROJECT=project2
$ scrapy settings --get BOT_NAME
Project 2 Bot
:::
:::
:::</p>
<p>::: {#using-the-scrapy-tool .section}</p>
<h4 id="using-the-scrapydocutils-literal-notranslatepre-toolheaderlink"><a class="header" href="#using-the-scrapydocutils-literal-notranslatepre-toolheaderlink">Using the [<code>scrapy</code>{.docutils .literal .notranslate}]{.pre} tool<a href="#using-the-scrapy-tool" title="Permalink to this heading">¶</a>{.headerlink}</a></h4>
<p>You can start by running the Scrapy tool with no arguments and it will
print some usage help and the available commands:</p>
<p>::: {.highlight-none .notranslate}
::: highlight
Scrapy X.Y - no active project</p>
<pre><code>Usage:
  scrapy &lt;command&gt; [options] [args]

Available commands:
  crawl         Run a spider
  fetch         Fetch a URL using the Scrapy downloader
[...]
</code></pre>
<p>:::
:::</p>
<p>The first line will print the currently active project if you're inside
a Scrapy project. In this example it was run from outside a project. If
run from inside a project it would have printed something like this:</p>
<p>::: {.highlight-none .notranslate}
::: highlight
Scrapy X.Y - project: myproject</p>
<pre><code>Usage:
  scrapy &lt;command&gt; [options] [args]

[...]
</code></pre>
<p>:::
:::</p>
<p>::: {#creating-projects .section}</p>
<h5 id="creating-projectsheaderlink"><a class="header" href="#creating-projectsheaderlink">Creating projects<a href="#creating-projects" title="Permalink to this heading">¶</a>{.headerlink}</a></h5>
<p>The first thing you typically do with the [<code>scrapy</code>{.docutils .literal
.notranslate}]{.pre} tool is create your Scrapy project:</p>
<p>::: {.highlight-none .notranslate}
::: highlight
scrapy startproject myproject [project_dir]
:::
:::</p>
<p>That will create a Scrapy project under the [<code>project_dir</code>{.docutils
.literal .notranslate}]{.pre} directory. If [<code>project_dir</code>{.docutils
.literal .notranslate}]{.pre} wasn't specified, [<code>project_dir</code>{.docutils
.literal .notranslate}]{.pre} will be the same as [<code>myproject</code>{.docutils
.literal .notranslate}]{.pre}.</p>
<p>Next, you go inside the new project directory:</p>
<p>::: {.highlight-none .notranslate}
::: highlight
cd project_dir
:::
:::</p>
<p>And you're ready to use the [<code>scrapy</code>{.docutils .literal
.notranslate}]{.pre} command to manage and control your project from
there.
:::</p>
<p>::: {#controlling-projects .section}</p>
<h5 id="controlling-projectsheaderlink"><a class="header" href="#controlling-projectsheaderlink">Controlling projects<a href="#controlling-projects" title="Permalink to this heading">¶</a>{.headerlink}</a></h5>
<p>You use the [<code>scrapy</code>{.docutils .literal .notranslate}]{.pre} tool from
inside your projects to control and manage them.</p>
<p>For example, to create a new spider:</p>
<p>::: {.highlight-none .notranslate}
::: highlight
scrapy genspider mydomain mydomain.com
:::
:::</p>
<p>Some Scrapy commands (like <a href="#std-command-crawl">[<code>crawl</code>{.xref .std .std-command .docutils
.literal .notranslate}]{.pre}</a>{.hoverxref .tooltip
.reference .internal}) must be run from inside a Scrapy project. See the
<a href="#topics-commands-ref">[commands reference]{.std .std-ref}</a>{.hoverxref
.tooltip .reference .internal} below for more information on which
commands must be run from inside projects, and which not.</p>
<p>Also keep in mind that some commands may have slightly different
behaviours when running them from inside projects. For example, the
fetch command will use spider-overridden behaviours (such as the
[<code>user_agent</code>{.docutils .literal .notranslate}]{.pre} attribute to
override the user-agent) if the url being fetched is associated with
some specific spider. This is intentional, as the [<code>fetch</code>{.docutils
.literal .notranslate}]{.pre} command is meant to be used to check how
spiders are downloading pages.
:::
:::</p>
<p>::: {#available-tool-commands .section}
[]{#topics-commands-ref}</p>
<h4 id="available-tool-commandsheaderlink"><a class="header" href="#available-tool-commandsheaderlink">Available tool commands<a href="#available-tool-commands" title="Permalink to this heading">¶</a>{.headerlink}</a></h4>
<p>This section contains a list of the available built-in commands with a
description and some usage examples. Remember, you can always get more
info about each command by running:</p>
<p>::: {.highlight-none .notranslate}
::: highlight
scrapy <command> -h
:::
:::</p>
<p>And you can see all available commands with:</p>
<p>::: {.highlight-none .notranslate}
::: highlight
scrapy -h
:::
:::</p>
<p>There are two kinds of commands, those that only work from inside a
Scrapy project (Project-specific commands) and those that also work
without an active Scrapy project (Global commands), though they may
behave slightly different when running from inside a project (as they
would use the project overridden settings).</p>
<p>Global commands:</p>
<ul>
<li>
<p><a href="#std-command-startproject">[<code>startproject</code>{.xref .std .std-command .docutils .literal
.notranslate}]{.pre}</a>{.hoverxref .tooltip
.reference .internal}</p>
</li>
<li>
<p><a href="#std-command-genspider">[<code>genspider</code>{.xref .std .std-command .docutils .literal
.notranslate}]{.pre}</a>{.hoverxref .tooltip
.reference .internal}</p>
</li>
<li>
<p><a href="#std-command-settings">[<code>settings</code>{.xref .std .std-command .docutils .literal
.notranslate}]{.pre}</a>{.hoverxref .tooltip
.reference .internal}</p>
</li>
<li>
<p><a href="#std-command-runspider">[<code>runspider</code>{.xref .std .std-command .docutils .literal
.notranslate}]{.pre}</a>{.hoverxref .tooltip
.reference .internal}</p>
</li>
<li>
<p><a href="#std-command-shell">[<code>shell</code>{.xref .std .std-command .docutils .literal
.notranslate}]{.pre}</a>{.hoverxref .tooltip
.reference .internal}</p>
</li>
<li>
<p><a href="#std-command-fetch">[<code>fetch</code>{.xref .std .std-command .docutils .literal
.notranslate}]{.pre}</a>{.hoverxref .tooltip
.reference .internal}</p>
</li>
<li>
<p><a href="#std-command-view">[<code>view</code>{.xref .std .std-command .docutils .literal
.notranslate}]{.pre}</a>{.hoverxref .tooltip
.reference .internal}</p>
</li>
<li>
<p><a href="#std-command-version">[<code>version</code>{.xref .std .std-command .docutils .literal
.notranslate}]{.pre}</a>{.hoverxref .tooltip
.reference .internal}</p>
</li>
</ul>
<p>Project-only commands:</p>
<ul>
<li>
<p><a href="#std-command-crawl">[<code>crawl</code>{.xref .std .std-command .docutils .literal
.notranslate}]{.pre}</a>{.hoverxref .tooltip
.reference .internal}</p>
</li>
<li>
<p><a href="#std-command-check">[<code>check</code>{.xref .std .std-command .docutils .literal
.notranslate}]{.pre}</a>{.hoverxref .tooltip
.reference .internal}</p>
</li>
<li>
<p><a href="#std-command-list">[<code>list</code>{.xref .std .std-command .docutils .literal
.notranslate}]{.pre}</a>{.hoverxref .tooltip
.reference .internal}</p>
</li>
<li>
<p><a href="#std-command-edit">[<code>edit</code>{.xref .std .std-command .docutils .literal
.notranslate}]{.pre}</a>{.hoverxref .tooltip
.reference .internal}</p>
</li>
<li>
<p><a href="#std-command-parse">[<code>parse</code>{.xref .std .std-command .docutils .literal
.notranslate}]{.pre}</a>{.hoverxref .tooltip
.reference .internal}</p>
</li>
<li>
<p><a href="#std-command-bench">[<code>bench</code>{.xref .std .std-command .docutils .literal
.notranslate}]{.pre}</a>{.hoverxref .tooltip
.reference .internal}</p>
</li>
</ul>
<p>::: {#startproject .section}
[]{#std-command-startproject}</p>
<h5 id="startprojectheaderlink"><a class="header" href="#startprojectheaderlink">startproject<a href="#startproject" title="Permalink to this heading">¶</a>{.headerlink}</a></h5>
<ul>
<li>
<p>Syntax: [<code>scrapy</code>{.docutils .literal
.notranslate}]{.pre}<code> </code>{.docutils .literal
.notranslate}[<code>startproject</code>{.docutils .literal
.notranslate}]{.pre}<code> </code>{.docutils .literal
.notranslate}[<code>&lt;project_name&gt;</code>{.docutils .literal
.notranslate}]{.pre}<code> </code>{.docutils .literal
.notranslate}[<code>[project_dir]</code>{.docutils .literal
.notranslate}]{.pre}</p>
</li>
<li>
<p>Requires project: <em>no</em></p>
</li>
</ul>
<p>Creates a new Scrapy project named [<code>project_name</code>{.docutils .literal
.notranslate}]{.pre}, under the [<code>project_dir</code>{.docutils .literal
.notranslate}]{.pre} directory. If [<code>project_dir</code>{.docutils .literal
.notranslate}]{.pre} wasn't specified, [<code>project_dir</code>{.docutils .literal
.notranslate}]{.pre} will be the same as [<code>project_name</code>{.docutils
.literal .notranslate}]{.pre}.</p>
<p>Usage example:</p>
<p>::: {.highlight-none .notranslate}
::: highlight
$ scrapy startproject myproject
:::
:::
:::</p>
<p>::: {#genspider .section}
[]{#std-command-genspider}</p>
<h5 id="genspiderheaderlink"><a class="header" href="#genspiderheaderlink">genspider<a href="#genspider" title="Permalink to this heading">¶</a>{.headerlink}</a></h5>
<ul>
<li>
<p>Syntax: [<code>scrapy</code>{.docutils .literal
.notranslate}]{.pre}<code> </code>{.docutils .literal
.notranslate}[<code>genspider</code>{.docutils .literal
.notranslate}]{.pre}<code> </code>{.docutils .literal
.notranslate}[<code>[-t</code>{.docutils .literal
.notranslate}]{.pre}<code> </code>{.docutils .literal
.notranslate}[<code>template]</code>{.docutils .literal
.notranslate}]{.pre}<code> </code>{.docutils .literal
.notranslate}[<code>&lt;name&gt;</code>{.docutils .literal
.notranslate}]{.pre}<code> </code>{.docutils .literal
.notranslate}[<code>&lt;domain</code>{.docutils .literal
.notranslate}]{.pre}<code> </code>{.docutils .literal
.notranslate}[<code>or</code>{.docutils .literal
.notranslate}]{.pre}<code> </code>{.docutils .literal
.notranslate}[<code>URL&gt;</code>{.docutils .literal .notranslate}]{.pre}</p>
</li>
<li>
<p>Requires project: <em>no</em></p>
</li>
</ul>
<p>::: versionadded
[New in version 2.6.0: ]{.versionmodified .added}The ability to pass a
URL instead of a domain.
:::</p>
<p>Create a new spider in the current folder or in the current project's
[<code>spiders</code>{.docutils .literal .notranslate}]{.pre} folder, if called
from inside a project. The [<code>&lt;name&gt;</code>{.docutils .literal
.notranslate}]{.pre} parameter is set as the spider's [<code>name</code>{.docutils
.literal .notranslate}]{.pre}, while [<code>&lt;domain</code>{.docutils .literal
.notranslate}]{.pre}<code> </code>{.docutils .literal .notranslate}[<code>or</code>{.docutils
.literal .notranslate}]{.pre}<code> </code>{.docutils .literal
.notranslate}[<code>URL&gt;</code>{.docutils .literal .notranslate}]{.pre} is used to
generate the [<code>allowed_domains</code>{.docutils .literal .notranslate}]{.pre}
and [<code>start_urls</code>{.docutils .literal .notranslate}]{.pre} spider's
attributes.</p>
<p>Usage example:</p>
<p>::: {.highlight-none .notranslate}
::: highlight
$ scrapy genspider -l
Available templates:
basic
crawl
csvfeed
xmlfeed</p>
<pre><code>$ scrapy genspider example example.com
Created spider 'example' using template 'basic'

$ scrapy genspider -t crawl scrapyorg scrapy.org
Created spider 'scrapyorg' using template 'crawl'
</code></pre>
<p>:::
:::</p>
<p>This is just a convenience shortcut command for creating spiders based
on pre-defined templates, but certainly not the only way to create
spiders. You can just create the spider source code files yourself,
instead of using this command.
:::</p>
<p>::: {#crawl .section}
[]{#std-command-crawl}</p>
<h5 id="crawlheaderlink"><a class="header" href="#crawlheaderlink">crawl<a href="#crawl" title="Permalink to this heading">¶</a>{.headerlink}</a></h5>
<ul>
<li>
<p>Syntax: [<code>scrapy</code>{.docutils .literal
.notranslate}]{.pre}<code> </code>{.docutils .literal
.notranslate}[<code>crawl</code>{.docutils .literal
.notranslate}]{.pre}<code> </code>{.docutils .literal
.notranslate}[<code>&lt;spider&gt;</code>{.docutils .literal .notranslate}]{.pre}</p>
</li>
<li>
<p>Requires project: <em>yes</em></p>
</li>
</ul>
<p>Start crawling using a spider.</p>
<p>Supported options:</p>
<ul>
<li>
<p>[<code>-h,</code>{.docutils .literal .notranslate}]{.pre}<code> </code>{.docutils .literal
.notranslate}[<code>--help</code>{.docutils .literal .notranslate}]{.pre}: show
a help message and exit</p>
</li>
<li>
<p>[<code>-a</code>{.docutils .literal .notranslate}]{.pre}<code> </code>{.docutils .literal
.notranslate}[<code>NAME=VALUE</code>{.docutils .literal .notranslate}]{.pre}:
set a spider argument (may be repeated)</p>
</li>
<li>
<p>[<code>--output</code>{.docutils .literal .notranslate}]{.pre}<code> </code>{.docutils
.literal .notranslate}[<code>FILE</code>{.docutils .literal
.notranslate}]{.pre} or [<code>-o</code>{.docutils .literal
.notranslate}]{.pre}<code> </code>{.docutils .literal
.notranslate}[<code>FILE</code>{.docutils .literal .notranslate}]{.pre}: append
scraped items to the end of FILE (use - for stdout), to define
format set a colon at the end of the output URI (i.e.
[<code>-o</code>{.docutils .literal .notranslate}]{.pre}<code> </code>{.docutils .literal
.notranslate}[<code>FILE:FORMAT</code>{.docutils .literal .notranslate}]{.pre})</p>
</li>
<li>
<p>[<code>--overwrite-output</code>{.docutils .literal
.notranslate}]{.pre}<code> </code>{.docutils .literal
.notranslate}[<code>FILE</code>{.docutils .literal .notranslate}]{.pre} or
[<code>-O</code>{.docutils .literal .notranslate}]{.pre}<code> </code>{.docutils .literal
.notranslate}[<code>FILE</code>{.docutils .literal .notranslate}]{.pre}: dump
scraped items into FILE, overwriting any existing file, to define
format set a colon at the end of the output URI (i.e.
[<code>-O</code>{.docutils .literal .notranslate}]{.pre}<code> </code>{.docutils .literal
.notranslate}[<code>FILE:FORMAT</code>{.docutils .literal .notranslate}]{.pre})</p>
</li>
<li>
<p>[<code>--output-format</code>{.docutils .literal
.notranslate}]{.pre}<code> </code>{.docutils .literal
.notranslate}[<code>FORMAT</code>{.docutils .literal .notranslate}]{.pre} or
[<code>-t</code>{.docutils .literal .notranslate}]{.pre}<code> </code>{.docutils .literal
.notranslate}[<code>FORMAT</code>{.docutils .literal .notranslate}]{.pre}:
deprecated way to define format to use for dumping items, does not
work in combination with [<code>-O</code>{.docutils .literal
.notranslate}]{.pre}</p>
</li>
</ul>
<p>Usage examples:</p>
<p>::: {.highlight-none .notranslate}
::: highlight
$ scrapy crawl myspider
[ ... myspider starts crawling ... ]</p>
<pre><code>$ scrapy crawl -o myfile:csv myspider
[ ... myspider starts crawling and appends the result to the file myfile in csv format ... ]

$ scrapy crawl -O myfile:json myspider
[ ... myspider starts crawling and saves the result in myfile in json format overwriting the original content... ]

$ scrapy crawl -o myfile -t csv myspider
[ ... myspider starts crawling and appends the result to the file myfile in csv format ... ]
</code></pre>
<p>:::
:::
:::</p>
<p>::: {#check .section}
[]{#std-command-check}</p>
<h5 id="checkheaderlink"><a class="header" href="#checkheaderlink">check<a href="#check" title="Permalink to this heading">¶</a>{.headerlink}</a></h5>
<ul>
<li>
<p>Syntax: [<code>scrapy</code>{.docutils .literal
.notranslate}]{.pre}<code> </code>{.docutils .literal
.notranslate}[<code>check</code>{.docutils .literal
.notranslate}]{.pre}<code> </code>{.docutils .literal
.notranslate}[<code>[-l]</code>{.docutils .literal
.notranslate}]{.pre}<code> </code>{.docutils .literal
.notranslate}[<code>&lt;spider&gt;</code>{.docutils .literal .notranslate}]{.pre}</p>
</li>
<li>
<p>Requires project: <em>yes</em></p>
</li>
</ul>
<p>Run contract checks.</p>
<p>Usage examples:</p>
<p>::: {.highlight-none .notranslate}
::: highlight
$ scrapy check -l
first_spider
* parse
* parse_item
second_spider
* parse
* parse_item</p>
<pre><code>$ scrapy check
[FAILED] first_spider:parse_item
&gt;&gt;&gt; 'RetailPricex' field is missing

[FAILED] first_spider:parse
&gt;&gt;&gt; Returned 92 requests, expected 0..4
</code></pre>
<p>:::
:::
:::</p>
<p>::: {#list .section}
[]{#std-command-list}</p>
<h5 id="listheaderlink"><a class="header" href="#listheaderlink">list<a href="#list" title="Permalink to this heading">¶</a>{.headerlink}</a></h5>
<ul>
<li>
<p>Syntax: [<code>scrapy</code>{.docutils .literal
.notranslate}]{.pre}<code> </code>{.docutils .literal
.notranslate}[<code>list</code>{.docutils .literal .notranslate}]{.pre}</p>
</li>
<li>
<p>Requires project: <em>yes</em></p>
</li>
</ul>
<p>List all available spiders in the current project. The output is one
spider per line.</p>
<p>Usage example:</p>
<p>::: {.highlight-none .notranslate}
::: highlight
$ scrapy list
spider1
spider2
:::
:::
:::</p>
<p>::: {#edit .section}
[]{#std-command-edit}</p>
<h5 id="editheaderlink"><a class="header" href="#editheaderlink">edit<a href="#edit" title="Permalink to this heading">¶</a>{.headerlink}</a></h5>
<ul>
<li>
<p>Syntax: [<code>scrapy</code>{.docutils .literal
.notranslate}]{.pre}<code> </code>{.docutils .literal
.notranslate}[<code>edit</code>{.docutils .literal
.notranslate}]{.pre}<code> </code>{.docutils .literal
.notranslate}[<code>&lt;spider&gt;</code>{.docutils .literal .notranslate}]{.pre}</p>
</li>
<li>
<p>Requires project: <em>yes</em></p>
</li>
</ul>
<p>Edit the given spider using the editor defined in the
[<code>EDITOR</code>{.docutils .literal .notranslate}]{.pre} environment variable
or (if unset) the <a href="index.html#std-setting-EDITOR">[<code>EDITOR</code>{.xref .std .std-setting .docutils .literal
.notranslate}]{.pre}</a>{.hoverxref .tooltip
.reference .internal} setting.</p>
<p>This command is provided only as a convenience shortcut for the most
common case, the developer is of course free to choose any tool or IDE
to write and debug spiders.</p>
<p>Usage example:</p>
<p>::: {.highlight-none .notranslate}
::: highlight
$ scrapy edit spider1
:::
:::
:::</p>
<p>::: {#fetch .section}
[]{#std-command-fetch}</p>
<h5 id="fetchheaderlink"><a class="header" href="#fetchheaderlink">fetch<a href="#fetch" title="Permalink to this heading">¶</a>{.headerlink}</a></h5>
<ul>
<li>
<p>Syntax: [<code>scrapy</code>{.docutils .literal
.notranslate}]{.pre}<code> </code>{.docutils .literal
.notranslate}[<code>fetch</code>{.docutils .literal
.notranslate}]{.pre}<code> </code>{.docutils .literal
.notranslate}[<code>&lt;url&gt;</code>{.docutils .literal .notranslate}]{.pre}</p>
</li>
<li>
<p>Requires project: <em>no</em></p>
</li>
</ul>
<p>Downloads the given URL using the Scrapy downloader and writes the
contents to standard output.</p>
<p>The interesting thing about this command is that it fetches the page how
the spider would download it. For example, if the spider has a
[<code>USER_AGENT</code>{.docutils .literal .notranslate}]{.pre} attribute which
overrides the User Agent, it will use that one.</p>
<p>So this command can be used to &quot;see&quot; how your spider would fetch a
certain page.</p>
<p>If used outside a project, no particular per-spider behaviour would be
applied and it will just use the default Scrapy downloader settings.</p>
<p>Supported options:</p>
<ul>
<li>
<p>[<code>--spider=SPIDER</code>{.docutils .literal .notranslate}]{.pre}: bypass
spider autodetection and force use of specific spider</p>
</li>
<li>
<p>[<code>--headers</code>{.docutils .literal .notranslate}]{.pre}: print the
response's HTTP headers instead of the response's body</p>
</li>
<li>
<p>[<code>--no-redirect</code>{.docutils .literal .notranslate}]{.pre}: do not
follow HTTP 3xx redirects (default is to follow them)</p>
</li>
</ul>
<p>Usage examples:</p>
<p>::: {.highlight-none .notranslate}
::: highlight
$ scrapy fetch --nolog http://www.example.com/some/page.html
[ ... html content here ... ]</p>
<pre><code>$ scrapy fetch --nolog --headers http://www.example.com/
{'Accept-Ranges': ['bytes'],
 'Age': ['1263   '],
 'Connection': ['close     '],
 'Content-Length': ['596'],
 'Content-Type': ['text/html; charset=UTF-8'],
 'Date': ['Wed, 18 Aug 2010 23:59:46 GMT'],
 'Etag': ['&quot;573c1-254-48c9c87349680&quot;'],
 'Last-Modified': ['Fri, 30 Jul 2010 15:30:18 GMT'],
 'Server': ['Apache/2.2.3 (CentOS)']}
</code></pre>
<p>:::
:::
:::</p>
<p>::: {#view .section}
[]{#std-command-view}</p>
<h5 id="viewheaderlink"><a class="header" href="#viewheaderlink">view<a href="#view" title="Permalink to this heading">¶</a>{.headerlink}</a></h5>
<ul>
<li>
<p>Syntax: [<code>scrapy</code>{.docutils .literal
.notranslate}]{.pre}<code> </code>{.docutils .literal
.notranslate}[<code>view</code>{.docutils .literal
.notranslate}]{.pre}<code> </code>{.docutils .literal
.notranslate}[<code>&lt;url&gt;</code>{.docutils .literal .notranslate}]{.pre}</p>
</li>
<li>
<p>Requires project: <em>no</em></p>
</li>
</ul>
<p>Opens the given URL in a browser, as your Scrapy spider would &quot;see&quot; it.
Sometimes spiders see pages differently from regular users, so this can
be used to check what the spider &quot;sees&quot; and confirm it's what you
expect.</p>
<p>Supported options:</p>
<ul>
<li>
<p>[<code>--spider=SPIDER</code>{.docutils .literal .notranslate}]{.pre}: bypass
spider autodetection and force use of specific spider</p>
</li>
<li>
<p>[<code>--no-redirect</code>{.docutils .literal .notranslate}]{.pre}: do not
follow HTTP 3xx redirects (default is to follow them)</p>
</li>
</ul>
<p>Usage example:</p>
<p>::: {.highlight-none .notranslate}
::: highlight
$ scrapy view http://www.example.com/some/page.html
[ ... browser starts ... ]
:::
:::
:::</p>
<p>::: {#shell .section}
[]{#std-command-shell}</p>
<h5 id="shellheaderlink"><a class="header" href="#shellheaderlink">shell<a href="#shell" title="Permalink to this heading">¶</a>{.headerlink}</a></h5>
<ul>
<li>
<p>Syntax: [<code>scrapy</code>{.docutils .literal
.notranslate}]{.pre}<code> </code>{.docutils .literal
.notranslate}[<code>shell</code>{.docutils .literal
.notranslate}]{.pre}<code> </code>{.docutils .literal
.notranslate}[<code>[url]</code>{.docutils .literal .notranslate}]{.pre}</p>
</li>
<li>
<p>Requires project: <em>no</em></p>
</li>
</ul>
<p>Starts the Scrapy shell for the given URL (if given) or empty if no URL
is given. Also supports UNIX-style local file paths, either relative
with [<code>./</code>{.docutils .literal .notranslate}]{.pre} or [<code>../</code>{.docutils
.literal .notranslate}]{.pre} prefixes or absolute file paths. See
<a href="index.html#topics-shell">[Scrapy shell]{.std .std-ref}</a>{.hoverxref
.tooltip .reference .internal} for more info.</p>
<p>Supported options:</p>
<ul>
<li>
<p>[<code>--spider=SPIDER</code>{.docutils .literal .notranslate}]{.pre}: bypass
spider autodetection and force use of specific spider</p>
</li>
<li>
<p>[<code>-c</code>{.docutils .literal .notranslate}]{.pre}<code> </code>{.docutils .literal
.notranslate}[<code>code</code>{.docutils .literal .notranslate}]{.pre}:
evaluate the code in the shell, print the result and exit</p>
</li>
<li>
<p>[<code>--no-redirect</code>{.docutils .literal .notranslate}]{.pre}: do not
follow HTTP 3xx redirects (default is to follow them); this only
affects the URL you may pass as argument on the command line; once
you are inside the shell, [<code>fetch(url)</code>{.docutils .literal
.notranslate}]{.pre} will still follow HTTP redirects by default.</p>
</li>
</ul>
<p>Usage example:</p>
<p>::: {.highlight-none .notranslate}
::: highlight
$ scrapy shell http://www.example.com/some/page.html
[ ... scrapy shell starts ... ]</p>
<pre><code>$ scrapy shell --nolog http://www.example.com/ -c '(response.status, response.url)'
(200, 'http://www.example.com/')

# shell follows HTTP redirects by default
$ scrapy shell --nolog http://httpbin.org/redirect-to?url=http%3A%2F%2Fexample.com%2F -c '(response.status, response.url)'
(200, 'http://example.com/')

# you can disable this with --no-redirect
# (only for the URL passed as command line argument)
$ scrapy shell --no-redirect --nolog http://httpbin.org/redirect-to?url=http%3A%2F%2Fexample.com%2F -c '(response.status, response.url)'
(302, 'http://httpbin.org/redirect-to?url=http%3A%2F%2Fexample.com%2F')
</code></pre>
<p>:::
:::
:::</p>
<p>::: {#parse .section}
[]{#std-command-parse}</p>
<h5 id="parseheaderlink"><a class="header" href="#parseheaderlink">parse<a href="#parse" title="Permalink to this heading">¶</a>{.headerlink}</a></h5>
<ul>
<li>
<p>Syntax: [<code>scrapy</code>{.docutils .literal
.notranslate}]{.pre}<code> </code>{.docutils .literal
.notranslate}[<code>parse</code>{.docutils .literal
.notranslate}]{.pre}<code> </code>{.docutils .literal
.notranslate}[<code>&lt;url&gt;</code>{.docutils .literal
.notranslate}]{.pre}<code> </code>{.docutils .literal
.notranslate}[<code>[options]</code>{.docutils .literal .notranslate}]{.pre}</p>
</li>
<li>
<p>Requires project: <em>yes</em></p>
</li>
</ul>
<p>Fetches the given URL and parses it with the spider that handles it,
using the method passed with the [<code>--callback</code>{.docutils .literal
.notranslate}]{.pre} option, or [<code>parse</code>{.docutils .literal
.notranslate}]{.pre} if not given.</p>
<p>Supported options:</p>
<ul>
<li>
<p>[<code>--spider=SPIDER</code>{.docutils .literal .notranslate}]{.pre}: bypass
spider autodetection and force use of specific spider</p>
</li>
<li>
<p>[<code>--a</code>{.docutils .literal .notranslate}]{.pre}<code> </code>{.docutils .literal
.notranslate}[<code>NAME=VALUE</code>{.docutils .literal .notranslate}]{.pre}:
set spider argument (may be repeated)</p>
</li>
<li>
<p>[<code>--callback</code>{.docutils .literal .notranslate}]{.pre} or
[<code>-c</code>{.docutils .literal .notranslate}]{.pre}: spider method to use
as callback for parsing the response</p>
</li>
<li>
<p>[<code>--meta</code>{.docutils .literal .notranslate}]{.pre} or [<code>-m</code>{.docutils
.literal .notranslate}]{.pre}: additional request meta that will be
passed to the callback request. This must be a valid json string.
Example: --meta='{&quot;foo&quot; : &quot;bar&quot;}'</p>
</li>
<li>
<p>[<code>--cbkwargs</code>{.docutils .literal .notranslate}]{.pre}: additional
keyword arguments that will be passed to the callback. This must be
a valid json string. Example: --cbkwargs='{&quot;foo&quot; : &quot;bar&quot;}'</p>
</li>
<li>
<p>[<code>--pipelines</code>{.docutils .literal .notranslate}]{.pre}: process
items through pipelines</p>
</li>
<li>
<p>[<code>--rules</code>{.docutils .literal .notranslate}]{.pre} or
[<code>-r</code>{.docutils .literal .notranslate}]{.pre}: use
<a href="index.html#scrapy.spiders.CrawlSpider" title="scrapy.spiders.CrawlSpider">[<code>CrawlSpider</code>{.xref .py .py-class .docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal} rules to discover the callback (i.e. spider method) to
use for parsing the response</p>
</li>
<li>
<p>[<code>--noitems</code>{.docutils .literal .notranslate}]{.pre}: don't show
scraped items</p>
</li>
<li>
<p>[<code>--nolinks</code>{.docutils .literal .notranslate}]{.pre}: don't show
extracted links</p>
</li>
<li>
<p>[<code>--nocolour</code>{.docutils .literal .notranslate}]{.pre}: avoid using
pygments to colorize the output</p>
</li>
<li>
<p>[<code>--depth</code>{.docutils .literal .notranslate}]{.pre} or
[<code>-d</code>{.docutils .literal .notranslate}]{.pre}: depth level for which
the requests should be followed recursively (default: 1)</p>
</li>
<li>
<p>[<code>--verbose</code>{.docutils .literal .notranslate}]{.pre} or
[<code>-v</code>{.docutils .literal .notranslate}]{.pre}: display information
for each depth level</p>
</li>
<li>
<p>[<code>--output</code>{.docutils .literal .notranslate}]{.pre} or
[<code>-o</code>{.docutils .literal .notranslate}]{.pre}: dump scraped items to
a file</p>
<p>::: versionadded
[New in version 2.3.]{.versionmodified .added}
:::</p>
</li>
</ul>
<p>Usage example:</p>
<p>::: {.highlight-none .notranslate}
::: highlight
$ scrapy parse http://www.example.com/ -c parse_item
[ ... scrapy log lines crawling example.com spider ... ]</p>
<pre><code>&gt;&gt;&gt; STATUS DEPTH LEVEL 1 &lt;&lt;&lt;
# Scraped Items  ------------------------------------------------------------
[{'name': 'Example item',
 'category': 'Furniture',
 'length': '12 cm'}]

# Requests  -----------------------------------------------------------------
[]
</code></pre>
<p>:::
:::
:::</p>
<p>::: {#settings .section}
[]{#std-command-settings}</p>
<h5 id="settingsheaderlink"><a class="header" href="#settingsheaderlink">settings<a href="#settings" title="Permalink to this heading">¶</a>{.headerlink}</a></h5>
<ul>
<li>
<p>Syntax: [<code>scrapy</code>{.docutils .literal
.notranslate}]{.pre}<code> </code>{.docutils .literal
.notranslate}[<code>settings</code>{.docutils .literal
.notranslate}]{.pre}<code> </code>{.docutils .literal
.notranslate}[<code>[options]</code>{.docutils .literal .notranslate}]{.pre}</p>
</li>
<li>
<p>Requires project: <em>no</em></p>
</li>
</ul>
<p>Get the value of a Scrapy setting.</p>
<p>If used inside a project it'll show the project setting value, otherwise
it'll show the default Scrapy value for that setting.</p>
<p>Example usage:</p>
<p>::: {.highlight-none .notranslate}
::: highlight
$ scrapy settings --get BOT_NAME
scrapybot
$ scrapy settings --get DOWNLOAD_DELAY
0
:::
:::
:::</p>
<p>::: {#runspider .section}
[]{#std-command-runspider}</p>
<h5 id="runspiderheaderlink"><a class="header" href="#runspiderheaderlink">runspider<a href="#runspider" title="Permalink to this heading">¶</a>{.headerlink}</a></h5>
<ul>
<li>
<p>Syntax: [<code>scrapy</code>{.docutils .literal
.notranslate}]{.pre}<code> </code>{.docutils .literal
.notranslate}[<code>runspider</code>{.docutils .literal
.notranslate}]{.pre}<code> </code>{.docutils .literal
.notranslate}[<code>&lt;spider_file.py&gt;</code>{.docutils .literal
.notranslate}]{.pre}</p>
</li>
<li>
<p>Requires project: <em>no</em></p>
</li>
</ul>
<p>Run a spider self-contained in a Python file, without having to create a
project.</p>
<p>Example usage:</p>
<p>::: {.highlight-none .notranslate}
::: highlight
$ scrapy runspider myspider.py
[ ... spider starts crawling ... ]
:::
:::
:::</p>
<p>::: {#version .section}
[]{#std-command-version}</p>
<h5 id="versionheaderlink"><a class="header" href="#versionheaderlink">version<a href="#version" title="Permalink to this heading">¶</a>{.headerlink}</a></h5>
<ul>
<li>
<p>Syntax: [<code>scrapy</code>{.docutils .literal
.notranslate}]{.pre}<code> </code>{.docutils .literal
.notranslate}[<code>version</code>{.docutils .literal
.notranslate}]{.pre}<code> </code>{.docutils .literal
.notranslate}[<code>[-v]</code>{.docutils .literal .notranslate}]{.pre}</p>
</li>
<li>
<p>Requires project: <em>no</em></p>
</li>
</ul>
<p>Prints the Scrapy version. If used with [<code>-v</code>{.docutils .literal
.notranslate}]{.pre} it also prints Python, Twisted and Platform info,
which is useful for bug reports.
:::</p>
<p>::: {#bench .section}
[]{#std-command-bench}</p>
<h5 id="benchheaderlink"><a class="header" href="#benchheaderlink">bench<a href="#bench" title="Permalink to this heading">¶</a>{.headerlink}</a></h5>
<ul>
<li>
<p>Syntax: [<code>scrapy</code>{.docutils .literal
.notranslate}]{.pre}<code> </code>{.docutils .literal
.notranslate}[<code>bench</code>{.docutils .literal .notranslate}]{.pre}</p>
</li>
<li>
<p>Requires project: <em>no</em></p>
</li>
</ul>
<p>Run a quick benchmark test. <a href="index.html#benchmarking">[Benchmarking]{.std
.std-ref}</a>{.hoverxref .tooltip .reference
.internal}.
:::
:::</p>
<p>::: {#custom-project-commands .section}</p>
<h4 id="custom-project-commandsheaderlink"><a class="header" href="#custom-project-commandsheaderlink">Custom project commands<a href="#custom-project-commands" title="Permalink to this heading">¶</a>{.headerlink}</a></h4>
<p>You can also add your custom project commands by using the
<a href="#std-setting-COMMANDS_MODULE">[<code>COMMANDS_MODULE</code>{.xref .std .std-setting .docutils .literal
.notranslate}]{.pre}</a>{.hoverxref .tooltip
.reference .internal} setting. See the Scrapy commands in
<a href="https://github.com/scrapy/scrapy/tree/master/scrapy/commands">scrapy/commands</a>{.reference
.external} for examples on how to implement your commands.</p>
<p>::: {#commands-module .section}
[]{#std-setting-COMMANDS_MODULE}</p>
<h5 id="commands_moduleheaderlink"><a class="header" href="#commands_moduleheaderlink">COMMANDS_MODULE<a href="#commands-module" title="Permalink to this heading">¶</a>{.headerlink}</a></h5>
<p>Default: [<code>''</code>{.docutils .literal .notranslate}]{.pre} (empty string)</p>
<p>A module to use for looking up custom Scrapy commands. This is used to
add custom commands for your Scrapy project.</p>
<p>Example:</p>
<p>::: {.highlight-python .notranslate}
::: highlight
COMMANDS_MODULE = &quot;mybot.commands&quot;
:::
:::
:::</p>
<p>::: {#register-commands-via-setup-py-entry-points .section}</p>
<h5 id="register-commands-via-setuppy-entry-pointsheaderlink"><a class="header" href="#register-commands-via-setuppy-entry-pointsheaderlink">Register commands via setup.py entry points<a href="#register-commands-via-setup-py-entry-points" title="Permalink to this heading">¶</a>{.headerlink}</a></h5>
<p>You can also add Scrapy commands from an external library by adding a
[<code>scrapy.commands</code>{.docutils .literal .notranslate}]{.pre} section in
the entry points of the library [<code>setup.py</code>{.docutils .literal
.notranslate}]{.pre} file.</p>
<p>The following example adds [<code>my_command</code>{.docutils .literal
.notranslate}]{.pre} command:</p>
<p>::: {.highlight-python .notranslate}
::: highlight
from setuptools import setup, find_packages</p>
<pre><code>setup(
    name=&quot;scrapy-mymodule&quot;,
    entry_points={
        &quot;scrapy.commands&quot;: [
            &quot;my_command=my_scrapy_module.commands:MyCommand&quot;,
        ],
    },
)
</code></pre>
<p>:::
:::
:::
:::
:::</p>
<p>[]{#document-topics/spiders}</p>
<p>::: {#spiders .section}
[]{#topics-spiders}</p>
<h3 id="spidersheaderlink"><a class="header" href="#spidersheaderlink">Spiders<a href="#spiders" title="Permalink to this heading">¶</a>{.headerlink}</a></h3>
<p>Spiders are classes which define how a certain site (or a group of
sites) will be scraped, including how to perform the crawl (i.e. follow
links) and how to extract structured data from their pages (i.e.
scraping items). In other words, Spiders are the place where you define
the custom behaviour for crawling and parsing pages for a particular
site (or, in some cases, a group of sites).</p>
<p>For spiders, the scraping cycle goes through something like this:</p>
<ol>
<li>
<p>You start by generating the initial Requests to crawl the first
URLs, and specify a callback function to be called with the response
downloaded from those requests.</p>
<p>The first requests to perform are obtained by calling the
<a href="#scrapy.Spider.start_requests" title="scrapy.Spider.start_requests">[<code>start_requests()</code>{.xref .py .py-meth .docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal} method which (by default) generates [<code>Request</code>{.xref .py
.py-class .docutils .literal .notranslate}]{.pre} for the URLs
specified in the <a href="#scrapy.Spider.start_urls" title="scrapy.Spider.start_urls">[<code>start_urls</code>{.xref .py .py-attr .docutils
.literal
.notranslate}]{.pre}</a>{.reference
.internal} and the <a href="#scrapy.Spider.parse" title="scrapy.Spider.parse">[<code>parse</code>{.xref .py .py-attr .docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal} method as callback function for the Requests.</p>
</li>
<li>
<p>In the callback function, you parse the response (web page) and
return <a href="index.html#topics-items">[item objects]{.std
.std-ref}</a>{.hoverxref .tooltip .reference
.internal}, [<code>Request</code>{.xref .py .py-class .docutils .literal
.notranslate}]{.pre} objects, or an iterable of these objects. Those
Requests will also contain a callback (maybe the same) and will then
be downloaded by Scrapy and then their response handled by the
specified callback.</p>
</li>
<li>
<p>In callback functions, you parse the page contents, typically using
<a href="index.html#topics-selectors">[Selectors]{.std .std-ref}</a>{.hoverxref
.tooltip .reference .internal} (but you can also use BeautifulSoup,
lxml or whatever mechanism you prefer) and generate items with the
parsed data.</p>
</li>
<li>
<p>Finally, the items returned from the spider will be typically
persisted to a database (in some <a href="index.html#topics-item-pipeline">[Item Pipeline]{.std
.std-ref}</a>{.hoverxref .tooltip
.reference .internal}) or written to a file using <a href="index.html#topics-feed-exports">[Feed
exports]{.std .std-ref}</a>{.hoverxref
.tooltip .reference .internal}.</p>
</li>
</ol>
<p>Even though this cycle applies (more or less) to any kind of spider,
there are different kinds of default spiders bundled into Scrapy for
different purposes. We will talk about those types here.</p>
<p>::: {#scrapy-spider .section}
[]{#topics-spiders-ref}</p>
<h4 id="scrapyspiderheaderlink"><a class="header" href="#scrapyspiderheaderlink">scrapy.Spider<a href="#scrapy-spider" title="Permalink to this heading">¶</a>{.headerlink}</a></h4>
<p><em>[class]{.pre}[ ]{.w}</em>[[scrapy.spiders.]{.pre}]{.sig-prename .descclassname}[[Spider]{.pre}]{.sig-name .descname}<a href="#scrapy.spiders.Spider" title="Permalink to this definition">¶</a>{.headerlink}</p>
<p>:</p>
<pre><code class="language-{=html}">&lt;!-- --&gt;
</code></pre>
<p><em>[class]{.pre}[ ]{.w}</em>[[scrapy.]{.pre}]{.sig-prename .descclassname}[[Spider]{.pre}]{.sig-name .descname}<a href="#scrapy.Spider" title="Permalink to this definition">¶</a>{.headerlink}</p>
<p>:   This is the simplest spider, and the one from which every other
spider must inherit (including spiders that come bundled with
Scrapy, as well as spiders that you write yourself). It doesn't
provide any special functionality. It just provides a default
<a href="#scrapy.Spider.start_requests" title="scrapy.Spider.start_requests">[<code>start_requests()</code>{.xref .py .py-meth .docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal} implementation which sends requests from the
<a href="#scrapy.Spider.start_urls" title="scrapy.Spider.start_urls">[<code>start_urls</code>{.xref .py .py-attr .docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal} spider attribute and calls the spider's method
[<code>parse</code>{.docutils .literal .notranslate}]{.pre} for each of the
resulting responses.</p>
<pre><code>[[name]{.pre}]{.sig-name .descname}[¶](#scrapy.Spider.name &quot;Permalink to this definition&quot;){.headerlink}

:   A string which defines the name for this spider. The spider name
    is how the spider is located (and instantiated) by Scrapy, so it
    must be unique. However, nothing prevents you from instantiating
    more than one instance of the same spider. This is the most
    important spider attribute and it's required.

    If the spider scrapes a single domain, a common practice is to
    name the spider after the domain, with or without the
    [TLD](https://en.wikipedia.org/wiki/Top-level_domain){.reference
    .external}. So, for example, a spider that crawls
    [`mywebsite.com`{.docutils .literal .notranslate}]{.pre} would
    often be called [`mywebsite`{.docutils .literal
    .notranslate}]{.pre}.

[[allowed_domains]{.pre}]{.sig-name .descname}[¶](#scrapy.Spider.allowed_domains &quot;Permalink to this definition&quot;){.headerlink}

:   An optional list of strings containing domains that this spider
    is allowed to crawl. Requests for URLs not belonging to the
    domain names specified in this list (or their subdomains) won't
    be followed if [[`OffsiteMiddleware`{.xref .py .py-class
    .docutils .literal
    .notranslate}]{.pre}](index.html#scrapy.spidermiddlewares.offsite.OffsiteMiddleware &quot;scrapy.spidermiddlewares.offsite.OffsiteMiddleware&quot;){.reference
    .internal} is enabled.

    Let's say your target url is
    [`https://www.example.com/1.html`{.docutils .literal
    .notranslate}]{.pre}, then add [`'example.com'`{.docutils
    .literal .notranslate}]{.pre} to the list.

[[start_urls]{.pre}]{.sig-name .descname}[¶](#scrapy.Spider.start_urls &quot;Permalink to this definition&quot;){.headerlink}

:   A list of URLs where the spider will begin to crawl from, when
    no particular URLs are specified. So, the first pages downloaded
    will be those listed here. The subsequent [`Request`{.xref .py
    .py-class .docutils .literal .notranslate}]{.pre} will be
    generated successively from data contained in the start URLs.

[[custom_settings]{.pre}]{.sig-name .descname}[¶](#scrapy.Spider.custom_settings &quot;Permalink to this definition&quot;){.headerlink}

:   A dictionary of settings that will be overridden from the
    project wide configuration when running this spider. It must be
    defined as a class attribute since the settings are updated
    before instantiation.

    For a list of available built-in settings see: [[Built-in
    settings reference]{.std
    .std-ref}](index.html#topics-settings-ref){.hoverxref .tooltip
    .reference .internal}.

[[crawler]{.pre}]{.sig-name .descname}[¶](#scrapy.Spider.crawler &quot;Permalink to this definition&quot;){.headerlink}

:   This attribute is set by the [[`from_crawler()`{.xref .py
    .py-meth .docutils .literal
    .notranslate}]{.pre}](index.html#from_crawler &quot;from_crawler&quot;){.reference
    .internal} class method after initializing the class, and links
    to the [[`Crawler`{.xref .py .py-class .docutils .literal
    .notranslate}]{.pre}](index.html#scrapy.crawler.Crawler &quot;scrapy.crawler.Crawler&quot;){.reference
    .internal} object to which this spider instance is bound.

    Crawlers encapsulate a lot of components in the project for
    their single entry access (such as extensions, middlewares,
    signals managers, etc). See [[Crawler API]{.std
    .std-ref}](index.html#topics-api-crawler){.hoverxref .tooltip
    .reference .internal} to know more about them.

[[settings]{.pre}]{.sig-name .descname}[¶](#scrapy.Spider.settings &quot;Permalink to this definition&quot;){.headerlink}

:   Configuration for running this spider. This is a
    [[`Settings`{.xref .py .py-class .docutils .literal
    .notranslate}]{.pre}](index.html#scrapy.settings.Settings &quot;scrapy.settings.Settings&quot;){.reference
    .internal} instance, see the [[Settings]{.std
    .std-ref}](index.html#topics-settings){.hoverxref .tooltip
    .reference .internal} topic for a detailed introduction on this
    subject.

[[logger]{.pre}]{.sig-name .descname}[¶](#scrapy.Spider.logger &quot;Permalink to this definition&quot;){.headerlink}

:   Python logger created with the Spider's [[`name`{.xref .py
    .py-attr .docutils .literal
    .notranslate}]{.pre}](#scrapy.Spider.name &quot;scrapy.Spider.name&quot;){.reference
    .internal}. You can use it to send log messages through it as
    described on [[Logging from Spiders]{.std
    .std-ref}](index.html#topics-logging-from-spiders){.hoverxref
    .tooltip .reference .internal}.

[[state]{.pre}]{.sig-name .descname}[¶](#scrapy.Spider.state &quot;Permalink to this definition&quot;){.headerlink}

:   A dict you can use to persist some spider state between batches.
    See [[Keeping persistent state between batches]{.std
    .std-ref}](index.html#topics-keeping-persistent-state-between-batches){.hoverxref
    .tooltip .reference .internal} to know more about it.

[[from_crawler]{.pre}]{.sig-name .descname}[(]{.sig-paren}*[[crawler]{.pre}]{.n}*, *[[\*]{.pre}]{.o}[[args]{.pre}]{.n}*, *[[\*\*]{.pre}]{.o}[[kwargs]{.pre}]{.n}*[)]{.sig-paren}[¶](#scrapy.Spider.from_crawler &quot;Permalink to this definition&quot;){.headerlink}

:   This is the class method used by Scrapy to create your spiders.

    You probably won't need to override this directly because the
    default implementation acts as a proxy to the
    [[`__init__()`{.xref .py .py-meth .docutils .literal
    .notranslate}]{.pre}](index.html#init__ &quot;__init__&quot;){.reference
    .internal} method, calling it with the given arguments
    [`args`{.docutils .literal .notranslate}]{.pre} and named
    arguments [`kwargs`{.docutils .literal .notranslate}]{.pre}.

    Nonetheless, this method sets the [[`crawler`{.xref .py .py-attr
    .docutils .literal
    .notranslate}]{.pre}](#scrapy.Spider.crawler &quot;scrapy.Spider.crawler&quot;){.reference
    .internal} and [[`settings`{.xref .py .py-attr .docutils
    .literal
    .notranslate}]{.pre}](#scrapy.Spider.settings &quot;scrapy.Spider.settings&quot;){.reference
    .internal} attributes in the new instance so they can be
    accessed later inside the spider's code.

    ::: versionchanged
    [Changed in version 2.11: ]{.versionmodified .changed}The
    settings in [`crawler.settings`{.docutils .literal
    .notranslate}]{.pre} can now be modified in this method, which
    is handy if you want to modify them based on arguments. As a
    consequence, these settings aren't the final values as they can
    be modified later by e.g. [[add-ons]{.std
    .std-ref}](index.html#topics-addons){.hoverxref .tooltip
    .reference .internal}. For the same reason, most of the
    [[`Crawler`{.xref .py .py-class .docutils .literal
    .notranslate}]{.pre}](index.html#scrapy.crawler.Crawler &quot;scrapy.crawler.Crawler&quot;){.reference
    .internal} attributes aren't initialized at this point.

    The final settings and the initialized [[`Crawler`{.xref .py
    .py-class .docutils .literal
    .notranslate}]{.pre}](index.html#scrapy.crawler.Crawler &quot;scrapy.crawler.Crawler&quot;){.reference
    .internal} attributes are available in the
    [[`start_requests()`{.xref .py .py-meth .docutils .literal
    .notranslate}]{.pre}](#scrapy.Spider.start_requests &quot;scrapy.Spider.start_requests&quot;){.reference
    .internal} method, handlers of the [[`engine_started`{.xref .std
    .std-signal .docutils .literal
    .notranslate}]{.pre}](index.html#std-signal-engine_started){.hoverxref
    .tooltip .reference .internal} signal and later.
    :::

    Parameters

    :   -   **crawler** ([[`Crawler`{.xref .py .py-class .docutils
            .literal
            .notranslate}]{.pre}](index.html#scrapy.crawler.Crawler &quot;scrapy.crawler.Crawler&quot;){.reference
            .internal} instance) -- crawler to which the spider will
            be bound

        -   **args**
            ([*list*](https://docs.python.org/3/library/stdtypes.html#list &quot;(in Python v3.12)&quot;){.reference
            .external}) -- arguments passed to the
            [[`__init__()`{.xref .py .py-meth .docutils .literal
            .notranslate}]{.pre}](index.html#init__ &quot;__init__&quot;){.reference
            .internal} method

        -   **kwargs**
            ([*dict*](https://docs.python.org/3/library/stdtypes.html#dict &quot;(in Python v3.12)&quot;){.reference
            .external}) -- keyword arguments passed to the
            [[`__init__()`{.xref .py .py-meth .docutils .literal
            .notranslate}]{.pre}](index.html#init__ &quot;__init__&quot;){.reference
            .internal} method

*[classmethod]{.pre}[ ]{.w}*[[update_settings]{.pre}]{.sig-name .descname}[(]{.sig-paren}*[[settings]{.pre}]{.n}*[)]{.sig-paren}[¶](#scrapy.Spider.update_settings &quot;Permalink to this definition&quot;){.headerlink}

:   The [`update_settings()`{.docutils .literal .notranslate}]{.pre}
    method is used to modify the spider's settings and is called
    during initialization of a spider instance.

    It takes a [[`Settings`{.xref .py .py-class .docutils .literal
    .notranslate}]{.pre}](index.html#scrapy.settings.Settings &quot;scrapy.settings.Settings&quot;){.reference
    .internal} object as a parameter and can add or update the
    spider's configuration values. This method is a class method,
    meaning that it is called on the [[`Spider`{.xref .py .py-class
    .docutils .literal
    .notranslate}]{.pre}](#scrapy.Spider &quot;scrapy.Spider&quot;){.reference
    .internal} class and allows all instances of the spider to share
    the same configuration.

    While per-spider settings can be set in
    [[`custom_settings`{.xref .py .py-attr .docutils .literal
    .notranslate}]{.pre}](#scrapy.Spider.custom_settings &quot;scrapy.Spider.custom_settings&quot;){.reference
    .internal}, using [`update_settings()`{.docutils .literal
    .notranslate}]{.pre} allows you to dynamically add, remove or
    change settings based on other settings, spider attributes or
    other factors and use setting priorities other than
    [`'spider'`{.docutils .literal .notranslate}]{.pre}. Also, it's
    easy to extend [`update_settings()`{.docutils .literal
    .notranslate}]{.pre} in a subclass by overriding it, while doing
    the same with [[`custom_settings`{.xref .py .py-attr .docutils
    .literal
    .notranslate}]{.pre}](#scrapy.Spider.custom_settings &quot;scrapy.Spider.custom_settings&quot;){.reference
    .internal} can be hard.

    For example, suppose a spider needs to modify [[`FEEDS`{.xref
    .std .std-setting .docutils .literal
    .notranslate}]{.pre}](index.html#std-setting-FEEDS){.hoverxref
    .tooltip .reference .internal}:

    ::: {.highlight-python .notranslate}
    ::: highlight
        import scrapy


        class MySpider(scrapy.Spider):
            name = &quot;myspider&quot;
            custom_feed = {
                &quot;/home/user/documents/items.json&quot;: {
                    &quot;format&quot;: &quot;json&quot;,
                    &quot;indent&quot;: 4,
                }
            }

            @classmethod
            def update_settings(cls, settings):
                super().update_settings(settings)
                settings.setdefault(&quot;FEEDS&quot;, {}).update(cls.custom_feed)
    :::
    :::

[[start_requests]{.pre}]{.sig-name .descname}[(]{.sig-paren}[)]{.sig-paren}[¶](#scrapy.Spider.start_requests &quot;Permalink to this definition&quot;){.headerlink}

:   This method must return an iterable with the first Requests to
    crawl for this spider. It is called by Scrapy when the spider is
    opened for scraping. Scrapy calls it only once, so it is safe to
    implement [[`start_requests()`{.xref .py .py-meth .docutils
    .literal
    .notranslate}]{.pre}](#scrapy.Spider.start_requests &quot;scrapy.Spider.start_requests&quot;){.reference
    .internal} as a generator.

    The default implementation generates [`Request(url,`{.docutils
    .literal .notranslate}]{.pre}` `{.docutils .literal
    .notranslate}[`dont_filter=True)`{.docutils .literal
    .notranslate}]{.pre} for each url in [[`start_urls`{.xref .py
    .py-attr .docutils .literal
    .notranslate}]{.pre}](#scrapy.Spider.start_urls &quot;scrapy.Spider.start_urls&quot;){.reference
    .internal}.

    If you want to change the Requests used to start scraping a
    domain, this is the method to override. For example, if you need
    to start by logging in using a POST request, you could do:

    ::: {.highlight-python .notranslate}
    ::: highlight
        import scrapy


        class MySpider(scrapy.Spider):
            name = &quot;myspider&quot;

            def start_requests(self):
                return [
                    scrapy.FormRequest(
                        &quot;http://www.example.com/login&quot;,
                        formdata={&quot;user&quot;: &quot;john&quot;, &quot;pass&quot;: &quot;secret&quot;},
                        callback=self.logged_in,
                    )
                ]

            def logged_in(self, response):
                # here you would extract links to follow and return Requests for
                # each of them, with another callback
                pass
    :::
    :::

[[parse]{.pre}]{.sig-name .descname}[(]{.sig-paren}*[[response]{.pre}]{.n}*[)]{.sig-paren}[¶](#scrapy.Spider.parse &quot;Permalink to this definition&quot;){.headerlink}

:   This is the default callback used by Scrapy to process
    downloaded responses, when their requests don't specify a
    callback.

    The [`parse`{.docutils .literal .notranslate}]{.pre} method is
    in charge of processing the response and returning scraped data
    and/or more URLs to follow. Other Requests callbacks have the
    same requirements as the [`Spider`{.xref .py .py-class .docutils
    .literal .notranslate}]{.pre} class.

    This method, as well as any other Request callback, must return
    a [`Request`{.xref .py .py-class .docutils .literal
    .notranslate}]{.pre} object, an [[item object]{.std
    .std-ref}](index.html#topics-items){.hoverxref .tooltip
    .reference .internal}, an iterable of [`Request`{.xref .py
    .py-class .docutils .literal .notranslate}]{.pre} objects and/or
    [[item objects]{.std
    .std-ref}](index.html#topics-items){.hoverxref .tooltip
    .reference .internal}, or [`None`{.docutils .literal
    .notranslate}]{.pre}.

    Parameters

    :   **response** ([[`Response`{.xref .py .py-class .docutils
        .literal
        .notranslate}]{.pre}](index.html#scrapy.http.Response &quot;scrapy.http.Response&quot;){.reference
        .internal}) -- the response to parse

[[log]{.pre}]{.sig-name .descname}[(]{.sig-paren}*[[message]{.pre}]{.n}*[\[]{.optional}, *[[level]{.pre}]{.n}*, *[[component]{.pre}]{.n}*[\]]{.optional}[)]{.sig-paren}[¶](#scrapy.Spider.log &quot;Permalink to this definition&quot;){.headerlink}

:   Wrapper that sends a log message through the Spider's
    [[`logger`{.xref .py .py-attr .docutils .literal
    .notranslate}]{.pre}](#scrapy.Spider.logger &quot;scrapy.Spider.logger&quot;){.reference
    .internal}, kept for backward compatibility. For more
    information see [[Logging from Spiders]{.std
    .std-ref}](index.html#topics-logging-from-spiders){.hoverxref
    .tooltip .reference .internal}.

[[closed]{.pre}]{.sig-name .descname}[(]{.sig-paren}*[[reason]{.pre}]{.n}*[)]{.sig-paren}[¶](#scrapy.Spider.closed &quot;Permalink to this definition&quot;){.headerlink}

:   Called when the spider closes. This method provides a shortcut
    to signals.connect() for the [[`spider_closed`{.xref .std
    .std-signal .docutils .literal
    .notranslate}]{.pre}](index.html#std-signal-spider_closed){.hoverxref
    .tooltip .reference .internal} signal.
</code></pre>
<p>Let's see an example:</p>
<p>::: {.highlight-python .notranslate}
::: highlight
import scrapy</p>
<pre><code>class MySpider(scrapy.Spider):
    name = &quot;example.com&quot;
    allowed_domains = [&quot;example.com&quot;]
    start_urls = [
        &quot;http://www.example.com/1.html&quot;,
        &quot;http://www.example.com/2.html&quot;,
        &quot;http://www.example.com/3.html&quot;,
    ]

    def parse(self, response):
        self.logger.info(&quot;A response from %s just arrived!&quot;, response.url)
</code></pre>
<p>:::
:::</p>
<p>Return multiple Requests and items from a single callback:</p>
<p>::: {.highlight-python .notranslate}
::: highlight
import scrapy</p>
<pre><code>class MySpider(scrapy.Spider):
    name = &quot;example.com&quot;
    allowed_domains = [&quot;example.com&quot;]
    start_urls = [
        &quot;http://www.example.com/1.html&quot;,
        &quot;http://www.example.com/2.html&quot;,
        &quot;http://www.example.com/3.html&quot;,
    ]

    def parse(self, response):
        for h3 in response.xpath(&quot;//h3&quot;).getall():
            yield {&quot;title&quot;: h3}

        for href in response.xpath(&quot;//a/@href&quot;).getall():
            yield scrapy.Request(response.urljoin(href), self.parse)
</code></pre>
<p>:::
:::</p>
<p>Instead of <a href="#scrapy.Spider.start_urls" title="scrapy.Spider.start_urls">[<code>start_urls</code>{.xref .py .py-attr .docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal} you can use <a href="#scrapy.Spider.start_requests" title="scrapy.Spider.start_requests">[<code>start_requests()</code>{.xref .py .py-meth .docutils
.literal
.notranslate}]{.pre}</a>{.reference
.internal} directly; to give data more structure you can use
[<code>Item</code>{.xref .py .py-class .docutils .literal .notranslate}]{.pre}
objects:</p>
<p>::: {.highlight-python .notranslate}
::: highlight
import scrapy
from myproject.items import MyItem</p>
<pre><code>class MySpider(scrapy.Spider):
    name = &quot;example.com&quot;
    allowed_domains = [&quot;example.com&quot;]

    def start_requests(self):
        yield scrapy.Request(&quot;http://www.example.com/1.html&quot;, self.parse)
        yield scrapy.Request(&quot;http://www.example.com/2.html&quot;, self.parse)
        yield scrapy.Request(&quot;http://www.example.com/3.html&quot;, self.parse)

    def parse(self, response):
        for h3 in response.xpath(&quot;//h3&quot;).getall():
            yield MyItem(title=h3)

        for href in response.xpath(&quot;//a/@href&quot;).getall():
            yield scrapy.Request(response.urljoin(href), self.parse)
</code></pre>
<p>:::
:::
:::</p>
<p>::: {#spider-arguments .section}
[]{#spiderargs}</p>
<h4 id="spider-argumentsheaderlink"><a class="header" href="#spider-argumentsheaderlink">Spider arguments<a href="#spider-arguments" title="Permalink to this heading">¶</a>{.headerlink}</a></h4>
<p>Spiders can receive arguments that modify their behaviour. Some common
uses for spider arguments are to define the start URLs or to restrict
the crawl to certain sections of the site, but they can be used to
configure any functionality of the spider.</p>
<p>Spider arguments are passed through the <a href="index.html#std-command-crawl">[<code>crawl</code>{.xref .std
.std-command .docutils .literal
.notranslate}]{.pre}</a>{.hoverxref .tooltip
.reference .internal} command using the [<code>-a</code>{.docutils .literal
.notranslate}]{.pre} option. For example:</p>
<p>::: {.highlight-default .notranslate}
::: highlight
scrapy crawl myspider -a category=electronics
:::
:::</p>
<p>Spiders can access arguments in their __init__ methods:</p>
<p>::: {.highlight-python .notranslate}
::: highlight
import scrapy</p>
<pre><code>class MySpider(scrapy.Spider):
    name = &quot;myspider&quot;

    def __init__(self, category=None, *args, **kwargs):
        super(MySpider, self).__init__(*args, **kwargs)
        self.start_urls = [f&quot;http://www.example.com/categories/{category}&quot;]
        # ...
</code></pre>
<p>:::
:::</p>
<p>The default __init__ method will take any spider arguments and copy
them to the spider as attributes. The above example can also be written
as follows:</p>
<p>::: {.highlight-python .notranslate}
::: highlight
import scrapy</p>
<pre><code>class MySpider(scrapy.Spider):
    name = &quot;myspider&quot;

    def start_requests(self):
        yield scrapy.Request(f&quot;http://www.example.com/categories/{self.category}&quot;)
</code></pre>
<p>:::
:::</p>
<p>If you are <a href="index.html#run-from-script">[running Scrapy from a script]{.std
.std-ref}</a>{.hoverxref .tooltip .reference
.internal}, you can specify spider arguments when calling
<a href="index.html#scrapy.crawler.CrawlerProcess.crawl" title="scrapy.crawler.CrawlerProcess.crawl">[<code>CrawlerProcess.crawl</code>{.xref .py .py-class .docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal} or <a href="index.html#scrapy.crawler.CrawlerRunner.crawl" title="scrapy.crawler.CrawlerRunner.crawl">[<code>CrawlerRunner.crawl</code>{.xref .py .py-class .docutils
.literal
.notranslate}]{.pre}</a>{.reference
.internal}:</p>
<p>::: {.highlight-python .notranslate}
::: highlight
process = CrawlerProcess()
process.crawl(MySpider, category=&quot;electronics&quot;)
:::
:::</p>
<p>Keep in mind that spider arguments are only strings. The spider will not
do any parsing on its own. If you were to set the
[<code>start_urls</code>{.docutils .literal .notranslate}]{.pre} attribute from the
command line, you would have to parse it on your own into a list using
something like <a href="https://docs.python.org/3/library/ast.html#ast.literal_eval" title="(in Python v3.12)">[<code>ast.literal_eval()</code>{.xref .py .py-func .docutils
.literal
.notranslate}]{.pre}</a>{.reference
.external} or <a href="https://docs.python.org/3/library/json.html#json.loads" title="(in Python v3.12)">[<code>json.loads()</code>{.xref .py .py-func .docutils .literal
.notranslate}]{.pre}</a>{.reference
.external} and then set it as an attribute. Otherwise, you would cause
iteration over a [<code>start_urls</code>{.docutils .literal .notranslate}]{.pre}
string (a very common python pitfall) resulting in each character being
seen as a separate url.</p>
<p>A valid use case is to set the http auth credentials used by
<a href="index.html#scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware" title="scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware">[<code>HttpAuthMiddleware</code>{.xref .py .py-class .docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal} or the user agent used by <a href="index.html#scrapy.downloadermiddlewares.useragent.UserAgentMiddleware" title="scrapy.downloadermiddlewares.useragent.UserAgentMiddleware">[<code>UserAgentMiddleware</code>{.xref .py
.py-class .docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal}:</p>
<p>::: {.highlight-default .notranslate}
::: highlight
scrapy crawl myspider -a http_user=myuser -a http_pass=mypassword -a user_agent=mybot
:::
:::</p>
<p>Spider arguments can also be passed through the Scrapyd
[<code>schedule.json</code>{.docutils .literal .notranslate}]{.pre} API. See
<a href="https://scrapyd.readthedocs.io/en/latest/">Scrapyd
documentation</a>{.reference
.external}.
:::</p>
<p>::: {#generic-spiders .section}
[]{#builtin-spiders}</p>
<h4 id="generic-spidersheaderlink"><a class="header" href="#generic-spidersheaderlink">Generic Spiders<a href="#generic-spiders" title="Permalink to this heading">¶</a>{.headerlink}</a></h4>
<p>Scrapy comes with some useful generic spiders that you can use to
subclass your spiders from. Their aim is to provide convenient
functionality for a few common scraping cases, like following all links
on a site based on certain rules, crawling from
<a href="https://www.sitemaps.org/index.html">Sitemaps</a>{.reference .external},
or parsing an XML/CSV feed.</p>
<p>For the examples used in the following spiders, we'll assume you have a
project with a [<code>TestItem</code>{.docutils .literal .notranslate}]{.pre}
declared in a [<code>myproject.items</code>{.docutils .literal .notranslate}]{.pre}
module:</p>
<p>::: {.highlight-python .notranslate}
::: highlight
import scrapy</p>
<pre><code>class TestItem(scrapy.Item):
    id = scrapy.Field()
    name = scrapy.Field()
    description = scrapy.Field()
</code></pre>
<p>:::
:::</p>
<p>::: {#crawlspider .section}</p>
<h5 id="crawlspiderheaderlink"><a class="header" href="#crawlspiderheaderlink">CrawlSpider<a href="#crawlspider" title="Permalink to this heading">¶</a>{.headerlink}</a></h5>
<p><em>[class]{.pre}[ ]{.w}</em>[[scrapy.spiders.]{.pre}]{.sig-prename .descclassname}[[CrawlSpider]{.pre}]{.sig-name .descname}<a href="_modules/scrapy/spiders/crawl.html#CrawlSpider">[[[source]]{.pre}]{.viewcode-link}</a>{.reference .internal}<a href="#scrapy.spiders.CrawlSpider" title="Permalink to this definition">¶</a>{.headerlink}</p>
<p>:   This is the most commonly used spider for crawling regular websites,
as it provides a convenient mechanism for following links by
defining a set of rules. It may not be the best suited for your
particular web sites or project, but it's generic enough for several
cases, so you can start from it and override it as needed for more
custom functionality, or just implement your own spider.</p>
<pre><code>Apart from the attributes inherited from Spider (that you must
specify), this class supports a new attribute:

[[rules]{.pre}]{.sig-name .descname}[¶](#scrapy.spiders.CrawlSpider.rules &quot;Permalink to this definition&quot;){.headerlink}

:   Which is a list of one (or more) [[`Rule`{.xref .py .py-class
    .docutils .literal
    .notranslate}]{.pre}](#scrapy.spiders.Rule &quot;scrapy.spiders.Rule&quot;){.reference
    .internal} objects. Each [[`Rule`{.xref .py .py-class .docutils
    .literal
    .notranslate}]{.pre}](#scrapy.spiders.Rule &quot;scrapy.spiders.Rule&quot;){.reference
    .internal} defines a certain behaviour for crawling the site.
    Rules objects are described below. If multiple rules match the
    same link, the first one will be used, according to the order
    they're defined in this attribute.

This spider also exposes an overridable method:

[[parse_start_url]{.pre}]{.sig-name .descname}[(]{.sig-paren}*[[response]{.pre}]{.n}*, *[[\*\*]{.pre}]{.o}[[kwargs]{.pre}]{.n}*[)]{.sig-paren}[[[\[source\]]{.pre}]{.viewcode-link}](_modules/scrapy/spiders/crawl.html#CrawlSpider.parse_start_url){.reference .internal}[¶](#scrapy.spiders.CrawlSpider.parse_start_url &quot;Permalink to this definition&quot;){.headerlink}

:   This method is called for each response produced for the URLs in
    the spider's [`start_urls`{.docutils .literal
    .notranslate}]{.pre} attribute. It allows to parse the initial
    responses and must return either an [[item object]{.std
    .std-ref}](index.html#topics-items){.hoverxref .tooltip
    .reference .internal}, a [`Request`{.xref .py .py-class
    .docutils .literal .notranslate}]{.pre} object, or an iterable
    containing any of them.
</code></pre>
<p>::: {#crawling-rules .section}</p>
<h6 id="crawling-rulesheaderlink"><a class="header" href="#crawling-rulesheaderlink">Crawling rules<a href="#crawling-rules" title="Permalink to this heading">¶</a>{.headerlink}</a></h6>
<p><em>[class]{.pre}[ ]{.w}</em>[[scrapy.spiders.]{.pre}]{.sig-prename .descclassname}[[Rule]{.pre}]{.sig-name .descname}[(]{.sig-paren}<em>[[link_extractor]{.pre}]{.n}[[=]{.pre}]{.o}[[None]{.pre}]{.default_value}</em>, <em>[[callback]{.pre}]{.n}[[=]{.pre}]{.o}[[None]{.pre}]{.default_value}</em>, <em>[[cb_kwargs]{.pre}]{.n}[[=]{.pre}]{.o}[[None]{.pre}]{.default_value}</em>, <em>[[follow]{.pre}]{.n}[[=]{.pre}]{.o}[[None]{.pre}]{.default_value}</em>, <em>[[process_links]{.pre}]{.n}[[=]{.pre}]{.o}[[None]{.pre}]{.default_value}</em>, <em>[[process_request]{.pre}]{.n}[[=]{.pre}]{.o}[[None]{.pre}]{.default_value}</em>, <em>[[errback]{.pre}]{.n}[[=]{.pre}]{.o}[[None]{.pre}]{.default_value}</em>[)]{.sig-paren}<a href="_modules/scrapy/spiders/crawl.html#Rule">[[[source]]{.pre}]{.viewcode-link}</a>{.reference .internal}<a href="#scrapy.spiders.Rule" title="Permalink to this definition">¶</a>{.headerlink}</p>
<p>:   [<code>link_extractor</code>{.docutils .literal .notranslate}]{.pre} is a
<a href="index.html#topics-link-extractors">[Link Extractor]{.std
.std-ref}</a>{.hoverxref .tooltip
.reference .internal} object which defines how links will be
extracted from each crawled page. Each produced link will be used to
generate a [<code>Request</code>{.xref .py .py-class .docutils .literal
.notranslate}]{.pre} object, which will contain the link's text in
its [<code>meta</code>{.docutils .literal .notranslate}]{.pre} dictionary
(under the [<code>link_text</code>{.docutils .literal .notranslate}]{.pre}
key). If omitted, a default link extractor created with no arguments
will be used, resulting in all links being extracted.</p>
<pre><code>[`callback`{.docutils .literal .notranslate}]{.pre} is a callable or
a string (in which case a method from the spider object with that
name will be used) to be called for each link extracted with the
specified link extractor. This callback receives a
[[`Response`{.xref .py .py-class .docutils .literal
.notranslate}]{.pre}](index.html#scrapy.http.Response &quot;scrapy.http.Response&quot;){.reference
.internal} as its first argument and must return either a single
instance or an iterable of [[item objects]{.std
.std-ref}](index.html#topics-items){.hoverxref .tooltip .reference
.internal} and/or [`Request`{.xref .py .py-class .docutils .literal
.notranslate}]{.pre} objects (or any subclass of them). As mentioned
above, the received [[`Response`{.xref .py .py-class .docutils
.literal
.notranslate}]{.pre}](index.html#scrapy.http.Response &quot;scrapy.http.Response&quot;){.reference
.internal} object will contain the text of the link that produced
the [`Request`{.xref .py .py-class .docutils .literal
.notranslate}]{.pre} in its [`meta`{.docutils .literal
.notranslate}]{.pre} dictionary (under the [`link_text`{.docutils
.literal .notranslate}]{.pre} key)

[`cb_kwargs`{.docutils .literal .notranslate}]{.pre} is a dict
containing the keyword arguments to be passed to the callback
function.

[`follow`{.docutils .literal .notranslate}]{.pre} is a boolean which
specifies if links should be followed from each response extracted
with this rule. If [`callback`{.docutils .literal
.notranslate}]{.pre} is None [`follow`{.docutils .literal
.notranslate}]{.pre} defaults to [`True`{.docutils .literal
.notranslate}]{.pre}, otherwise it defaults to [`False`{.docutils
.literal .notranslate}]{.pre}.

[`process_links`{.docutils .literal .notranslate}]{.pre} is a
callable, or a string (in which case a method from the spider object
with that name will be used) which will be called for each list of
links extracted from each response using the specified
[`link_extractor`{.docutils .literal .notranslate}]{.pre}. This is
mainly used for filtering purposes.

[`process_request`{.docutils .literal .notranslate}]{.pre} is a
callable (or a string, in which case a method from the spider object
with that name will be used) which will be called for every
[`Request`{.xref .py .py-class .docutils .literal
.notranslate}]{.pre} extracted by this rule. This callable should
take said request as first argument and the [[`Response`{.xref .py
.py-class .docutils .literal
.notranslate}]{.pre}](index.html#scrapy.http.Response &quot;scrapy.http.Response&quot;){.reference
.internal} from which the request originated as second argument. It
must return a [`Request`{.docutils .literal .notranslate}]{.pre}
object or [`None`{.docutils .literal .notranslate}]{.pre} (to filter
out the request).

[`errback`{.docutils .literal .notranslate}]{.pre} is a callable or
a string (in which case a method from the spider object with that
name will be used) to be called if any exception is raised while
processing a request generated by the rule. It receives a
[[`Twisted`{.xref .py .py-class .docutils .literal
.notranslate}]{.pre}` `{.xref .py .py-class .docutils .literal
.notranslate}[`Failure`{.xref .py .py-class .docutils .literal
.notranslate}]{.pre}](https://docs.twisted.org/en/stable/api/twisted.python.failure.Failure.html &quot;(in Twisted)&quot;){.reference
.external} instance as first parameter.

::: {.admonition .warning}
Warning

Because of its internal implementation, you must explicitly set
callbacks for new requests when writing [[`CrawlSpider`{.xref .py
.py-class .docutils .literal
.notranslate}]{.pre}](#scrapy.spiders.CrawlSpider &quot;scrapy.spiders.CrawlSpider&quot;){.reference
.internal}-based spiders; unexpected behaviour can occur otherwise.
:::

::: versionadded
[New in version 2.0: ]{.versionmodified .added}The *errback*
parameter.
:::
</code></pre>
<p>:::</p>
<p>::: {#crawlspider-example .section}</p>
<h6 id="crawlspider-exampleheaderlink"><a class="header" href="#crawlspider-exampleheaderlink">CrawlSpider example<a href="#crawlspider-example" title="Permalink to this heading">¶</a>{.headerlink}</a></h6>
<p>Let's now take a look at an example CrawlSpider with rules:</p>
<p>::: {.highlight-python .notranslate}
::: highlight
import scrapy
from scrapy.spiders import CrawlSpider, Rule
from scrapy.linkextractors import LinkExtractor</p>
<pre><code>class MySpider(CrawlSpider):
    name = &quot;example.com&quot;
    allowed_domains = [&quot;example.com&quot;]
    start_urls = [&quot;http://www.example.com&quot;]

    rules = (
        # Extract links matching 'category.php' (but not matching 'subsection.php')
        # and follow links from them (since no callback means follow=True by default).
        Rule(LinkExtractor(allow=(r&quot;category\.php&quot;,), deny=(r&quot;subsection\.php&quot;,))),
        # Extract links matching 'item.php' and parse them with the spider's method parse_item
        Rule(LinkExtractor(allow=(r&quot;item\.php&quot;,)), callback=&quot;parse_item&quot;),
    )

    def parse_item(self, response):
        self.logger.info(&quot;Hi, this is an item page! %s&quot;, response.url)
        item = scrapy.Item()
        item[&quot;id&quot;] = response.xpath('//td[@id=&quot;item_id&quot;]/text()').re(r&quot;ID: (\d+)&quot;)
        item[&quot;name&quot;] = response.xpath('//td[@id=&quot;item_name&quot;]/text()').get()
        item[&quot;description&quot;] = response.xpath(
            '//td[@id=&quot;item_description&quot;]/text()'
        ).get()
        item[&quot;link_text&quot;] = response.meta[&quot;link_text&quot;]
        url = response.xpath('//td[@id=&quot;additional_data&quot;]/@href').get()
        return response.follow(
            url, self.parse_additional_page, cb_kwargs=dict(item=item)
        )

    def parse_additional_page(self, response, item):
        item[&quot;additional_data&quot;] = response.xpath(
            '//p[@id=&quot;additional_data&quot;]/text()'
        ).get()
        return item
</code></pre>
<p>:::
:::</p>
<p>This spider would start crawling example.com's home page, collecting
category links, and item links, parsing the latter with the
[<code>parse_item</code>{.docutils .literal .notranslate}]{.pre} method. For each
item response, some data will be extracted from the HTML using XPath,
and an [<code>Item</code>{.xref .py .py-class .docutils .literal
.notranslate}]{.pre} will be filled with it.
:::
:::</p>
<p>::: {#xmlfeedspider .section}</p>
<h5 id="xmlfeedspiderheaderlink"><a class="header" href="#xmlfeedspiderheaderlink">XMLFeedSpider<a href="#xmlfeedspider" title="Permalink to this heading">¶</a>{.headerlink}</a></h5>
<p><em>[class]{.pre}[ ]{.w}</em>[[scrapy.spiders.]{.pre}]{.sig-prename .descclassname}[[XMLFeedSpider]{.pre}]{.sig-name .descname}<a href="_modules/scrapy/spiders/feed.html#XMLFeedSpider">[[[source]]{.pre}]{.viewcode-link}</a>{.reference .internal}<a href="#scrapy.spiders.XMLFeedSpider" title="Permalink to this definition">¶</a>{.headerlink}</p>
<p>:   XMLFeedSpider is designed for parsing XML feeds by iterating through
them by a certain node name. The iterator can be chosen from:
[<code>iternodes</code>{.docutils .literal .notranslate}]{.pre},
[<code>xml</code>{.docutils .literal .notranslate}]{.pre}, and
[<code>html</code>{.docutils .literal .notranslate}]{.pre}. It's recommended to
use the [<code>iternodes</code>{.docutils .literal .notranslate}]{.pre}
iterator for performance reasons, since the [<code>xml</code>{.docutils
.literal .notranslate}]{.pre} and [<code>html</code>{.docutils .literal
.notranslate}]{.pre} iterators generate the whole DOM at once in
order to parse it. However, using [<code>html</code>{.docutils .literal
.notranslate}]{.pre} as the iterator may be useful when parsing XML
with bad markup.</p>
<pre><code>To set the iterator and the tag name, you must define the following
class attributes:

[[iterator]{.pre}]{.sig-name .descname}[¶](#scrapy.spiders.XMLFeedSpider.iterator &quot;Permalink to this definition&quot;){.headerlink}

:   A string which defines the iterator to use. It can be either:

    &gt; &lt;div&gt;
    &gt;
    &gt; -   [`'iternodes'`{.docutils .literal .notranslate}]{.pre} - a
    &gt;     fast iterator based on regular expressions
    &gt;
    &gt; -   [`'html'`{.docutils .literal .notranslate}]{.pre} - an
    &gt;     iterator which uses [`Selector`{.xref .py .py-class
    &gt;     .docutils .literal .notranslate}]{.pre}. Keep in mind this
    &gt;     uses DOM parsing and must load all DOM in memory which
    &gt;     could be a problem for big feeds
    &gt;
    &gt; -   [`'xml'`{.docutils .literal .notranslate}]{.pre} - an
    &gt;     iterator which uses [`Selector`{.xref .py .py-class
    &gt;     .docutils .literal .notranslate}]{.pre}. Keep in mind this
    &gt;     uses DOM parsing and must load all DOM in memory which
    &gt;     could be a problem for big feeds
    &gt;
    &gt; &lt;/div&gt;

    It defaults to: [`'iternodes'`{.docutils .literal
    .notranslate}]{.pre}.

[[itertag]{.pre}]{.sig-name .descname}[¶](#scrapy.spiders.XMLFeedSpider.itertag &quot;Permalink to this definition&quot;){.headerlink}

:   A string with the name of the node (or element) to iterate in.
    Example:

    ::: {.highlight-default .notranslate}
    ::: highlight
        itertag = 'product'
    :::
    :::

[[namespaces]{.pre}]{.sig-name .descname}[¶](#scrapy.spiders.XMLFeedSpider.namespaces &quot;Permalink to this definition&quot;){.headerlink}

:   A list of [`(prefix,`{.docutils .literal
    .notranslate}]{.pre}` `{.docutils .literal
    .notranslate}[`uri)`{.docutils .literal .notranslate}]{.pre}
    tuples which define the namespaces available in that document
    that will be processed with this spider. The [`prefix`{.docutils
    .literal .notranslate}]{.pre} and [`uri`{.docutils .literal
    .notranslate}]{.pre} will be used to automatically register
    namespaces using the [`register_namespace()`{.xref .py .py-meth
    .docutils .literal .notranslate}]{.pre} method.

    You can then specify nodes with namespaces in the
    [[`itertag`{.xref .py .py-attr .docutils .literal
    .notranslate}]{.pre}](#scrapy.spiders.XMLFeedSpider.itertag &quot;scrapy.spiders.XMLFeedSpider.itertag&quot;){.reference
    .internal} attribute.

    Example:

    ::: {.highlight-default .notranslate}
    ::: highlight
        class YourSpider(XMLFeedSpider):

            namespaces = [('n', 'http://www.sitemaps.org/schemas/sitemap/0.9')]
            itertag = 'n:url'
            # ...
    :::
    :::

Apart from these new attributes, this spider has the following
overridable methods too:

[[adapt_response]{.pre}]{.sig-name .descname}[(]{.sig-paren}*[[response]{.pre}]{.n}*[)]{.sig-paren}[[[\[source\]]{.pre}]{.viewcode-link}](_modules/scrapy/spiders/feed.html#XMLFeedSpider.adapt_response){.reference .internal}[¶](#scrapy.spiders.XMLFeedSpider.adapt_response &quot;Permalink to this definition&quot;){.headerlink}

:   A method that receives the response as soon as it arrives from
    the spider middleware, before the spider starts parsing it. It
    can be used to modify the response body before parsing it. This
    method receives a response and also returns a response (it could
    be the same or another one).

[[parse_node]{.pre}]{.sig-name .descname}[(]{.sig-paren}*[[response]{.pre}]{.n}*, *[[selector]{.pre}]{.n}*[)]{.sig-paren}[[[\[source\]]{.pre}]{.viewcode-link}](_modules/scrapy/spiders/feed.html#XMLFeedSpider.parse_node){.reference .internal}[¶](#scrapy.spiders.XMLFeedSpider.parse_node &quot;Permalink to this definition&quot;){.headerlink}

:   This method is called for the nodes matching the provided tag
    name ([`itertag`{.docutils .literal .notranslate}]{.pre}).
    Receives the response and an [`Selector`{.xref .py .py-class
    .docutils .literal .notranslate}]{.pre} for each node.
    Overriding this method is mandatory. Otherwise, you spider won't
    work. This method must return an [[item object]{.std
    .std-ref}](index.html#topics-items){.hoverxref .tooltip
    .reference .internal}, a [`Request`{.xref .py .py-class
    .docutils .literal .notranslate}]{.pre} object, or an iterable
    containing any of them.

[[process_results]{.pre}]{.sig-name .descname}[(]{.sig-paren}*[[response]{.pre}]{.n}*, *[[results]{.pre}]{.n}*[)]{.sig-paren}[[[\[source\]]{.pre}]{.viewcode-link}](_modules/scrapy/spiders/feed.html#XMLFeedSpider.process_results){.reference .internal}[¶](#scrapy.spiders.XMLFeedSpider.process_results &quot;Permalink to this definition&quot;){.headerlink}

:   This method is called for each result (item or request) returned
    by the spider, and it's intended to perform any last time
    processing required before returning the results to the
    framework core, for example setting the item IDs. It receives a
    list of results and the response which originated those results.
    It must return a list of results (items or requests).

::: {.admonition .warning}
Warning

Because of its internal implementation, you must explicitly set
callbacks for new requests when writing [[`XMLFeedSpider`{.xref .py
.py-class .docutils .literal
.notranslate}]{.pre}](#scrapy.spiders.XMLFeedSpider &quot;scrapy.spiders.XMLFeedSpider&quot;){.reference
.internal}-based spiders; unexpected behaviour can occur otherwise.
:::
</code></pre>
<p>::: {#xmlfeedspider-example .section}</p>
<h6 id="xmlfeedspider-exampleheaderlink"><a class="header" href="#xmlfeedspider-exampleheaderlink">XMLFeedSpider example<a href="#xmlfeedspider-example" title="Permalink to this heading">¶</a>{.headerlink}</a></h6>
<p>These spiders are pretty easy to use, let's have a look at one example:</p>
<p>::: {.highlight-python .notranslate}
::: highlight
from scrapy.spiders import XMLFeedSpider
from myproject.items import TestItem</p>
<pre><code>class MySpider(XMLFeedSpider):
    name = &quot;example.com&quot;
    allowed_domains = [&quot;example.com&quot;]
    start_urls = [&quot;http://www.example.com/feed.xml&quot;]
    iterator = &quot;iternodes&quot;  # This is actually unnecessary, since it's the default value
    itertag = &quot;item&quot;

    def parse_node(self, response, node):
        self.logger.info(
            &quot;Hi, this is a &lt;%s&gt; node!: %s&quot;, self.itertag, &quot;&quot;.join(node.getall())
        )

        item = TestItem()
        item[&quot;id&quot;] = node.xpath(&quot;@id&quot;).get()
        item[&quot;name&quot;] = node.xpath(&quot;name&quot;).get()
        item[&quot;description&quot;] = node.xpath(&quot;description&quot;).get()
        return item
</code></pre>
<p>:::
:::</p>
<p>Basically what we did up there was to create a spider that downloads a
feed from the given [<code>start_urls</code>{.docutils .literal
.notranslate}]{.pre}, and then iterates through each of its
[<code>item</code>{.docutils .literal .notranslate}]{.pre} tags, prints them out,
and stores some random data in an [<code>Item</code>{.xref .py .py-class .docutils
.literal .notranslate}]{.pre}.
:::
:::</p>
<p>::: {#csvfeedspider .section}</p>
<h5 id="csvfeedspiderheaderlink"><a class="header" href="#csvfeedspiderheaderlink">CSVFeedSpider<a href="#csvfeedspider" title="Permalink to this heading">¶</a>{.headerlink}</a></h5>
<p><em>[class]{.pre}[ ]{.w}</em>[[scrapy.spiders.]{.pre}]{.sig-prename .descclassname}[[CSVFeedSpider]{.pre}]{.sig-name .descname}<a href="_modules/scrapy/spiders/feed.html#CSVFeedSpider">[[[source]]{.pre}]{.viewcode-link}</a>{.reference .internal}<a href="#scrapy.spiders.CSVFeedSpider" title="Permalink to this definition">¶</a>{.headerlink}</p>
<p>:   This spider is very similar to the XMLFeedSpider, except that it
iterates over rows, instead of nodes. The method that gets called in
each iteration is <a href="#scrapy.spiders.CSVFeedSpider.parse_row" title="scrapy.spiders.CSVFeedSpider.parse_row">[<code>parse_row()</code>{.xref .py .py-meth .docutils
.literal
.notranslate}]{.pre}</a>{.reference
.internal}.</p>
<pre><code>[[delimiter]{.pre}]{.sig-name .descname}[¶](#scrapy.spiders.CSVFeedSpider.delimiter &quot;Permalink to this definition&quot;){.headerlink}

:   A string with the separator character for each field in the CSV
    file Defaults to [`','`{.docutils .literal .notranslate}]{.pre}
    (comma).

[[quotechar]{.pre}]{.sig-name .descname}[¶](#scrapy.spiders.CSVFeedSpider.quotechar &quot;Permalink to this definition&quot;){.headerlink}

:   A string with the enclosure character for each field in the CSV
    file Defaults to [`'&quot;'`{.docutils .literal .notranslate}]{.pre}
    (quotation mark).

[[headers]{.pre}]{.sig-name .descname}[¶](#scrapy.spiders.CSVFeedSpider.headers &quot;Permalink to this definition&quot;){.headerlink}

:   A list of the column names in the CSV file.

[[parse_row]{.pre}]{.sig-name .descname}[(]{.sig-paren}*[[response]{.pre}]{.n}*, *[[row]{.pre}]{.n}*[)]{.sig-paren}[[[\[source\]]{.pre}]{.viewcode-link}](_modules/scrapy/spiders/feed.html#CSVFeedSpider.parse_row){.reference .internal}[¶](#scrapy.spiders.CSVFeedSpider.parse_row &quot;Permalink to this definition&quot;){.headerlink}

:   Receives a response and a dict (representing each row) with a
    key for each provided (or detected) header of the CSV file. This
    spider also gives the opportunity to override
    [`adapt_response`{.docutils .literal .notranslate}]{.pre} and
    [`process_results`{.docutils .literal .notranslate}]{.pre}
    methods for pre- and post-processing purposes.
</code></pre>
<p>::: {#csvfeedspider-example .section}</p>
<h6 id="csvfeedspider-exampleheaderlink"><a class="header" href="#csvfeedspider-exampleheaderlink">CSVFeedSpider example<a href="#csvfeedspider-example" title="Permalink to this heading">¶</a>{.headerlink}</a></h6>
<p>Let's see an example similar to the previous one, but using a
<a href="#scrapy.spiders.CSVFeedSpider" title="scrapy.spiders.CSVFeedSpider">[<code>CSVFeedSpider</code>{.xref .py .py-class .docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal}:</p>
<p>::: {.highlight-python .notranslate}
::: highlight
from scrapy.spiders import CSVFeedSpider
from myproject.items import TestItem</p>
<pre><code>class MySpider(CSVFeedSpider):
    name = &quot;example.com&quot;
    allowed_domains = [&quot;example.com&quot;]
    start_urls = [&quot;http://www.example.com/feed.csv&quot;]
    delimiter = &quot;;&quot;
    quotechar = &quot;'&quot;
    headers = [&quot;id&quot;, &quot;name&quot;, &quot;description&quot;]

    def parse_row(self, response, row):
        self.logger.info(&quot;Hi, this is a row!: %r&quot;, row)

        item = TestItem()
        item[&quot;id&quot;] = row[&quot;id&quot;]
        item[&quot;name&quot;] = row[&quot;name&quot;]
        item[&quot;description&quot;] = row[&quot;description&quot;]
        return item
</code></pre>
<p>:::
:::
:::
:::</p>
<p>::: {#sitemapspider .section}</p>
<h5 id="sitemapspiderheaderlink"><a class="header" href="#sitemapspiderheaderlink">SitemapSpider<a href="#sitemapspider" title="Permalink to this heading">¶</a>{.headerlink}</a></h5>
<p><em>[class]{.pre}[ ]{.w}</em>[[scrapy.spiders.]{.pre}]{.sig-prename .descclassname}[[SitemapSpider]{.pre}]{.sig-name .descname}<a href="_modules/scrapy/spiders/sitemap.html#SitemapSpider">[[[source]]{.pre}]{.viewcode-link}</a>{.reference .internal}<a href="#scrapy.spiders.SitemapSpider" title="Permalink to this definition">¶</a>{.headerlink}</p>
<p>:   SitemapSpider allows you to crawl a site by discovering the URLs
using <a href="https://www.sitemaps.org/index.html">Sitemaps</a>{.reference
.external}.</p>
<pre><code>It supports nested sitemaps and discovering sitemap urls from
[robots.txt](https://www.robotstxt.org/){.reference .external}.

[[sitemap_urls]{.pre}]{.sig-name .descname}[¶](#scrapy.spiders.SitemapSpider.sitemap_urls &quot;Permalink to this definition&quot;){.headerlink}

:   A list of urls pointing to the sitemaps whose urls you want to
    crawl.

    You can also point to a
    [robots.txt](https://www.robotstxt.org/){.reference .external}
    and it will be parsed to extract sitemap urls from it.

[[sitemap_rules]{.pre}]{.sig-name .descname}[¶](#scrapy.spiders.SitemapSpider.sitemap_rules &quot;Permalink to this definition&quot;){.headerlink}

:   A list of tuples [`(regex,`{.docutils .literal
    .notranslate}]{.pre}` `{.docutils .literal
    .notranslate}[`callback)`{.docutils .literal
    .notranslate}]{.pre} where:

    -   [`regex`{.docutils .literal .notranslate}]{.pre} is a
        regular expression to match urls extracted from sitemaps.
        [`regex`{.docutils .literal .notranslate}]{.pre} can be
        either a str or a compiled regex object.

    -   callback is the callback to use for processing the urls that
        match the regular expression. [`callback`{.docutils .literal
        .notranslate}]{.pre} can be a string (indicating the name of
        a spider method) or a callable.

    For example:

    ::: {.highlight-default .notranslate}
    ::: highlight
        sitemap_rules = [('/product/', 'parse_product')]
    :::
    :::

    Rules are applied in order, and only the first one that matches
    will be used.

    If you omit this attribute, all urls found in sitemaps will be
    processed with the [`parse`{.docutils .literal
    .notranslate}]{.pre} callback.

[[sitemap_follow]{.pre}]{.sig-name .descname}[¶](#scrapy.spiders.SitemapSpider.sitemap_follow &quot;Permalink to this definition&quot;){.headerlink}

:   A list of regexes of sitemap that should be followed. This is
    only for sites that use [Sitemap index
    files](https://www.sitemaps.org/protocol.html#index){.reference
    .external} that point to other sitemap files.

    By default, all sitemaps are followed.

[[sitemap_alternate_links]{.pre}]{.sig-name .descname}[¶](#scrapy.spiders.SitemapSpider.sitemap_alternate_links &quot;Permalink to this definition&quot;){.headerlink}

:   Specifies if alternate links for one [`url`{.docutils .literal
    .notranslate}]{.pre} should be followed. These are links for the
    same website in another language passed within the same
    [`url`{.docutils .literal .notranslate}]{.pre} block.

    For example:

    ::: {.highlight-default .notranslate}
    ::: highlight
        &lt;url&gt;
            &lt;loc&gt;http://example.com/&lt;/loc&gt;
            &lt;xhtml:link rel=&quot;alternate&quot; hreflang=&quot;de&quot; href=&quot;http://example.com/de&quot;/&gt;
        &lt;/url&gt;
    :::
    :::

    With [`sitemap_alternate_links`{.docutils .literal
    .notranslate}]{.pre} set, this would retrieve both URLs. With
    [`sitemap_alternate_links`{.docutils .literal
    .notranslate}]{.pre} disabled, only
    [`http://example.com/`{.docutils .literal .notranslate}]{.pre}
    would be retrieved.

    Default is [`sitemap_alternate_links`{.docutils .literal
    .notranslate}]{.pre} disabled.

[[sitemap_filter]{.pre}]{.sig-name .descname}[(]{.sig-paren}*[[entries]{.pre}]{.n}*[)]{.sig-paren}[[[\[source\]]{.pre}]{.viewcode-link}](_modules/scrapy/spiders/sitemap.html#SitemapSpider.sitemap_filter){.reference .internal}[¶](#scrapy.spiders.SitemapSpider.sitemap_filter &quot;Permalink to this definition&quot;){.headerlink}

:   This is a filter function that could be overridden to select
    sitemap entries based on their attributes.

    For example:

    ::: {.highlight-default .notranslate}
    ::: highlight
        &lt;url&gt;
            &lt;loc&gt;http://example.com/&lt;/loc&gt;
            &lt;lastmod&gt;2005-01-01&lt;/lastmod&gt;
        &lt;/url&gt;
    :::
    :::

    We can define a [`sitemap_filter`{.docutils .literal
    .notranslate}]{.pre} function to filter [`entries`{.docutils
    .literal .notranslate}]{.pre} by date:

    ::: {.highlight-python .notranslate}
    ::: highlight
        from datetime import datetime
        from scrapy.spiders import SitemapSpider


        class FilteredSitemapSpider(SitemapSpider):
            name = &quot;filtered_sitemap_spider&quot;
            allowed_domains = [&quot;example.com&quot;]
            sitemap_urls = [&quot;http://example.com/sitemap.xml&quot;]

            def sitemap_filter(self, entries):
                for entry in entries:
                    date_time = datetime.strptime(entry[&quot;lastmod&quot;], &quot;%Y-%m-%d&quot;)
                    if date_time.year &gt;= 2005:
                        yield entry
    :::
    :::

    This would retrieve only [`entries`{.docutils .literal
    .notranslate}]{.pre} modified on 2005 and the following years.

    Entries are dict objects extracted from the sitemap document.
    Usually, the key is the tag name and the value is the text
    inside it.

    It's important to notice that:

    -   as the loc attribute is required, entries without this tag
        are discarded

    -   alternate links are stored in a list with the key
        [`alternate`{.docutils .literal .notranslate}]{.pre} (see
        [`sitemap_alternate_links`{.docutils .literal
        .notranslate}]{.pre})

    -   namespaces are removed, so lxml tags named as
        [`{namespace}tagname`{.docutils .literal
        .notranslate}]{.pre} become only [`tagname`{.docutils
        .literal .notranslate}]{.pre}

    If you omit this method, all entries found in sitemaps will be
    processed, observing other attributes and their settings.
</code></pre>
<p>::: {#sitemapspider-examples .section}</p>
<h6 id="sitemapspider-examplesheaderlink"><a class="header" href="#sitemapspider-examplesheaderlink">SitemapSpider examples<a href="#sitemapspider-examples" title="Permalink to this heading">¶</a>{.headerlink}</a></h6>
<p>Simplest example: process all urls discovered through sitemaps using the
[<code>parse</code>{.docutils .literal .notranslate}]{.pre} callback:</p>
<p>::: {.highlight-python .notranslate}
::: highlight
from scrapy.spiders import SitemapSpider</p>
<pre><code>class MySpider(SitemapSpider):
    sitemap_urls = [&quot;http://www.example.com/sitemap.xml&quot;]

    def parse(self, response):
        pass  # ... scrape item here ...
</code></pre>
<p>:::
:::</p>
<p>Process some urls with certain callback and other urls with a different
callback:</p>
<p>::: {.highlight-python .notranslate}
::: highlight
from scrapy.spiders import SitemapSpider</p>
<pre><code>class MySpider(SitemapSpider):
    sitemap_urls = [&quot;http://www.example.com/sitemap.xml&quot;]
    sitemap_rules = [
        (&quot;/product/&quot;, &quot;parse_product&quot;),
        (&quot;/category/&quot;, &quot;parse_category&quot;),
    ]

    def parse_product(self, response):
        pass  # ... scrape product ...

    def parse_category(self, response):
        pass  # ... scrape category ...
</code></pre>
<p>:::
:::</p>
<p>Follow sitemaps defined in the
<a href="https://www.robotstxt.org/">robots.txt</a>{.reference .external} file and
only follow sitemaps whose url contains [<code>/sitemap_shop</code>{.docutils
.literal .notranslate}]{.pre}:</p>
<p>::: {.highlight-python .notranslate}
::: highlight
from scrapy.spiders import SitemapSpider</p>
<pre><code>class MySpider(SitemapSpider):
    sitemap_urls = [&quot;http://www.example.com/robots.txt&quot;]
    sitemap_rules = [
        (&quot;/shop/&quot;, &quot;parse_shop&quot;),
    ]
    sitemap_follow = [&quot;/sitemap_shops&quot;]

    def parse_shop(self, response):
        pass  # ... scrape shop here ...
</code></pre>
<p>:::
:::</p>
<p>Combine SitemapSpider with other sources of urls:</p>
<p>::: {.highlight-python .notranslate}
::: highlight
from scrapy.spiders import SitemapSpider</p>
<pre><code>class MySpider(SitemapSpider):
    sitemap_urls = [&quot;http://www.example.com/robots.txt&quot;]
    sitemap_rules = [
        (&quot;/shop/&quot;, &quot;parse_shop&quot;),
    ]

    other_urls = [&quot;http://www.example.com/about&quot;]

    def start_requests(self):
        requests = list(super(MySpider, self).start_requests())
        requests += [scrapy.Request(x, self.parse_other) for x in self.other_urls]
        return requests

    def parse_shop(self, response):
        pass  # ... scrape shop here ...

    def parse_other(self, response):
        pass  # ... scrape other here ...
</code></pre>
<p>:::
:::
:::
:::
:::
:::</p>
<p>[]{#document-topics/selectors}</p>
<p>::: {#selectors .section}
[]{#topics-selectors}</p>
<h3 id="selectorsheaderlink"><a class="header" href="#selectorsheaderlink">Selectors<a href="#selectors" title="Permalink to this heading">¶</a>{.headerlink}</a></h3>
<p>When you're scraping web pages, the most common task you need to perform
is to extract data from the HTML source. There are several libraries
available to achieve this, such as:</p>
<ul>
<li>
<p><a href="https://www.crummy.com/software/BeautifulSoup/">BeautifulSoup</a>{.reference
.external} is a very popular web scraping library among Python
programmers which constructs a Python object based on the structure
of the HTML code and also deals with bad markup reasonably well, but
it has one drawback: it's slow.</p>
</li>
<li>
<p><a href="https://lxml.de/">lxml</a>{.reference .external} is an XML parsing
library (which also parses HTML) with a pythonic API based on
<a href="https://docs.python.org/3/library/xml.etree.elementtree.html#module-xml.etree.ElementTree" title="(in Python v3.12)">[<code>ElementTree</code>{.xref .py .py-mod .docutils .literal
.notranslate}]{.pre}</a>{.reference
.external}. (lxml is not part of the Python standard library.)</p>
</li>
</ul>
<p>Scrapy comes with its own mechanism for extracting data. They're called
selectors because they &quot;select&quot; certain parts of the HTML document
specified either by <a href="https://www.w3.org/TR/xpath/all/">XPath</a>{.reference
.external} or <a href="https://www.w3.org/TR/selectors">CSS</a>{.reference
.external} expressions.</p>
<p><a href="https://www.w3.org/TR/xpath/all/">XPath</a>{.reference .external} is a
language for selecting nodes in XML documents, which can also be used
with HTML. <a href="https://www.w3.org/TR/selectors">CSS</a>{.reference .external}
is a language for applying styles to HTML documents. It defines
selectors to associate those styles with specific HTML elements.</p>
<p>::: {.admonition .note}
Note</p>
<p>Scrapy Selectors is a thin wrapper around
<a href="https://parsel.readthedocs.io/en/latest/">parsel</a>{.reference .external}
library; the purpose of this wrapper is to provide better integration
with Scrapy Response objects.</p>
<p><a href="https://parsel.readthedocs.io/en/latest/">parsel</a>{.reference .external}
is a stand-alone web scraping library which can be used without Scrapy.
It uses <a href="https://lxml.de/">lxml</a>{.reference .external} library under the
hood, and implements an easy API on top of lxml API. It means Scrapy
selectors are very similar in speed and parsing accuracy to lxml.
:::</p>
<p>::: {#using-selectors .section}</p>
<h4 id="using-selectorsheaderlink"><a class="header" href="#using-selectorsheaderlink">Using selectors<a href="#using-selectors" title="Permalink to this heading">¶</a>{.headerlink}</a></h4>
<p>::: {#constructing-selectors .section}</p>
<h5 id="constructing-selectorsheaderlink"><a class="header" href="#constructing-selectorsheaderlink">Constructing selectors<a href="#constructing-selectors" title="Permalink to this heading">¶</a>{.headerlink}</a></h5>
<p>Response objects expose a [<code>Selector</code>{.xref .py .py-class .docutils
.literal .notranslate}]{.pre} instance on [<code>.selector</code>{.docutils
.literal .notranslate}]{.pre} attribute:</p>
<p>::: {.highlight-pycon .notranslate}
::: highlight
&gt;&gt;&gt; response.selector.xpath(&quot;//span/text()&quot;).get()
'good'
:::
:::</p>
<p>Querying responses using XPath and CSS is so common that responses
include two more shortcuts: [<code>response.xpath()</code>{.docutils .literal
.notranslate}]{.pre} and [<code>response.css()</code>{.docutils .literal
.notranslate}]{.pre}:</p>
<p>::: {.highlight-pycon .notranslate}
::: highlight
&gt;&gt;&gt; response.xpath(&quot;//span/text()&quot;).get()
'good'
&gt;&gt;&gt; response.css(&quot;span::text&quot;).get()
'good'
:::
:::</p>
<p>Scrapy selectors are instances of [<code>Selector</code>{.xref .py .py-class
.docutils .literal .notranslate}]{.pre} class constructed by passing
either <a href="index.html#scrapy.http.TextResponse" title="scrapy.http.TextResponse">[<code>TextResponse</code>{.xref .py .py-class .docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal} object or markup as a string (in [<code>text</code>{.docutils .literal
.notranslate}]{.pre} argument).</p>
<p>Usually there is no need to construct Scrapy selectors manually:
[<code>response</code>{.docutils .literal .notranslate}]{.pre} object is available
in Spider callbacks, so in most cases it is more convenient to use
[<code>response.css()</code>{.docutils .literal .notranslate}]{.pre} and
[<code>response.xpath()</code>{.docutils .literal .notranslate}]{.pre} shortcuts.
By using [<code>response.selector</code>{.docutils .literal .notranslate}]{.pre} or
one of these shortcuts you can also ensure the response body is parsed
only once.</p>
<p>But if required, it is possible to use [<code>Selector</code>{.docutils .literal
.notranslate}]{.pre} directly. Constructing from text:</p>
<p>::: {.highlight-pycon .notranslate}
::: highlight
&gt;&gt;&gt; from scrapy.selector import Selector
&gt;&gt;&gt; body = &quot;<html><body><span>good</span></body></html>&quot;
&gt;&gt;&gt; Selector(text=body).xpath(&quot;//span/text()&quot;).get()
'good'
:::
:::</p>
<p>Constructing from response - <a href="index.html#scrapy.http.HtmlResponse" title="scrapy.http.HtmlResponse">[<code>HtmlResponse</code>{.xref .py .py-class
.docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal} is one of <a href="index.html#scrapy.http.TextResponse" title="scrapy.http.TextResponse">[<code>TextResponse</code>{.xref .py .py-class .docutils
.literal
.notranslate}]{.pre}</a>{.reference
.internal} subclasses:</p>
<p>::: {.highlight-pycon .notranslate}
::: highlight
&gt;&gt;&gt; from scrapy.selector import Selector
&gt;&gt;&gt; from scrapy.http import HtmlResponse
&gt;&gt;&gt; response = HtmlResponse(url=&quot;http://example.com&quot;, body=body, encoding=&quot;utf-8&quot;)
&gt;&gt;&gt; Selector(response=response).xpath(&quot;//span/text()&quot;).get()
'good'
:::
:::</p>
<p>[<code>Selector</code>{.docutils .literal .notranslate}]{.pre} automatically
chooses the best parsing rules (XML vs HTML) based on input type.
:::</p>
<p>::: {#id1 .section}</p>
<h5 id="using-selectorsheaderlink-1"><a class="header" href="#using-selectorsheaderlink-1">Using selectors<a href="#id1" title="Permalink to this heading">¶</a>{.headerlink}</a></h5>
<p>To explain how to use the selectors we'll use the [<code>Scrapy</code>{.docutils
.literal .notranslate}]{.pre}<code> </code>{.docutils .literal
.notranslate}[<code>shell</code>{.docutils .literal .notranslate}]{.pre} (which
provides interactive testing) and an example page located in the Scrapy
documentation server:</p>
<blockquote>
<div>
<p><a href="https://docs.scrapy.org/en/latest/_static/selectors-sample1.html">https://docs.scrapy.org/en/latest/_static/selectors-sample1.html</a>{.reference
.external}</p>
</div>
</blockquote>
<p>For the sake of completeness, here's its full HTML code:</p>
<p>::: {.highlight-html .notranslate}
::: highlight
<!DOCTYPE html></p>
<pre><code>&lt;html&gt;
  &lt;head&gt;
    &lt;base href='http://example.com/' /&gt;
    &lt;title&gt;Example website&lt;/title&gt;
  &lt;/head&gt;
  &lt;body&gt;
    &lt;div id='images'&gt;
      &lt;a href='image1.html'&gt;Name: My image 1 &lt;br /&gt;&lt;img src='image1_thumb.jpg' alt='image1'/&gt;&lt;/a&gt;
      &lt;a href='image2.html'&gt;Name: My image 2 &lt;br /&gt;&lt;img src='image2_thumb.jpg' alt='image2'/&gt;&lt;/a&gt;
      &lt;a href='image3.html'&gt;Name: My image 3 &lt;br /&gt;&lt;img src='image3_thumb.jpg' alt='image3'/&gt;&lt;/a&gt;
      &lt;a href='image4.html'&gt;Name: My image 4 &lt;br /&gt;&lt;img src='image4_thumb.jpg' alt='image4'/&gt;&lt;/a&gt;
      &lt;a href='image5.html'&gt;Name: My image 5 &lt;br /&gt;&lt;img src='image5_thumb.jpg' alt='image5'/&gt;&lt;/a&gt;
    &lt;/div&gt;
  &lt;/body&gt;
&lt;/html&gt;
</code></pre>
<p>:::
:::</p>
<p>First, let's open the shell:</p>
<p>::: {.highlight-sh .notranslate}
::: highlight
scrapy shell https://docs.scrapy.org/en/latest/_static/selectors-sample1.html
:::
:::</p>
<p>Then, after the shell loads, you'll have the response available as
[<code>response</code>{.docutils .literal .notranslate}]{.pre} shell variable, and
its attached selector in [<code>response.selector</code>{.docutils .literal
.notranslate}]{.pre} attribute.</p>
<p>Since we're dealing with HTML, the selector will automatically use an
HTML parser.</p>
<p>So, by looking at the <a href="#topics-selectors-htmlcode">[HTML code]{.std
.std-ref}</a>{.hoverxref .tooltip .reference
.internal} of that page, let's construct an XPath for selecting the text
inside the title tag:</p>
<p>::: {.highlight-pycon .notranslate}
::: highlight
&gt;&gt;&gt; response.xpath(&quot;//title/text()&quot;)
[<Selector query='//title/text()' data='Example website'>]
:::
:::</p>
<p>To actually extract the textual data, you must call the selector
[<code>.get()</code>{.docutils .literal .notranslate}]{.pre} or
[<code>.getall()</code>{.docutils .literal .notranslate}]{.pre} methods, as
follows:</p>
<p>::: {.highlight-pycon .notranslate}
::: highlight
&gt;&gt;&gt; response.xpath(&quot;//title/text()&quot;).getall()
['Example website']
&gt;&gt;&gt; response.xpath(&quot;//title/text()&quot;).get()
'Example website'
:::
:::</p>
<p>[<code>.get()</code>{.docutils .literal .notranslate}]{.pre} always returns a
single result; if there are several matches, content of a first match is
returned; if there are no matches, None is returned.
[<code>.getall()</code>{.docutils .literal .notranslate}]{.pre} returns a list with
all results.</p>
<p>Notice that CSS selectors can select text or attribute nodes using CSS3
pseudo-elements:</p>
<p>::: {.highlight-pycon .notranslate}
::: highlight
&gt;&gt;&gt; response.css(&quot;title::text&quot;).get()
'Example website'
:::
:::</p>
<p>As you can see, [<code>.xpath()</code>{.docutils .literal .notranslate}]{.pre} and
[<code>.css()</code>{.docutils .literal .notranslate}]{.pre} methods return a
<a href="#scrapy.selector.SelectorList" title="scrapy.selector.SelectorList">[<code>SelectorList</code>{.xref .py .py-class .docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal} instance, which is a list of new selectors. This API can be
used for quickly selecting nested data:</p>
<p>::: {.highlight-pycon .notranslate}
::: highlight
&gt;&gt;&gt; response.css(&quot;img&quot;).xpath(&quot;@src&quot;).getall()
['image1_thumb.jpg',
'image2_thumb.jpg',
'image3_thumb.jpg',
'image4_thumb.jpg',
'image5_thumb.jpg']
:::
:::</p>
<p>If you want to extract only the first matched element, you can call the
selector [<code>.get()</code>{.docutils .literal .notranslate}]{.pre} (or its alias
[<code>.extract_first()</code>{.docutils .literal .notranslate}]{.pre} commonly
used in previous Scrapy versions):</p>
<p>::: {.highlight-pycon .notranslate}
::: highlight
&gt;&gt;&gt; response.xpath('//div[@id=&quot;images&quot;]/a/text()').get()
'Name: My image 1 '
:::
:::</p>
<p>It returns [<code>None</code>{.docutils .literal .notranslate}]{.pre} if no element
was found:</p>
<p>::: {.highlight-pycon .notranslate}
::: highlight
&gt;&gt;&gt; response.xpath('//div[@id=&quot;not-exists&quot;]/text()').get() is None
True
:::
:::</p>
<p>A default return value can be provided as an argument, to be used
instead of [<code>None</code>{.docutils .literal .notranslate}]{.pre}:</p>
<p>::: {.highlight-pycon .notranslate}
::: highlight
&gt;&gt;&gt; response.xpath('//div[@id=&quot;not-exists&quot;]/text()').get(default=&quot;not-found&quot;)
'not-found'
:::
:::</p>
<p>Instead of using e.g. [<code>'@src'</code>{.docutils .literal .notranslate}]{.pre}
XPath it is possible to query for attributes using [<code>.attrib</code>{.docutils
.literal .notranslate}]{.pre} property of a [<code>Selector</code>{.xref .py
.py-class .docutils .literal .notranslate}]{.pre}:</p>
<p>::: {.highlight-pycon .notranslate}
::: highlight
&gt;&gt;&gt; [img.attrib[&quot;src&quot;] for img in response.css(&quot;img&quot;)]
['image1_thumb.jpg',
'image2_thumb.jpg',
'image3_thumb.jpg',
'image4_thumb.jpg',
'image5_thumb.jpg']
:::
:::</p>
<p>As a shortcut, [<code>.attrib</code>{.docutils .literal .notranslate}]{.pre} is
also available on SelectorList directly; it returns attributes for the
first matching element:</p>
<p>::: {.highlight-pycon .notranslate}
::: highlight
&gt;&gt;&gt; response.css(&quot;img&quot;).attrib[&quot;src&quot;]
'image1_thumb.jpg'
:::
:::</p>
<p>This is most useful when only a single result is expected, e.g. when
selecting by id, or selecting unique elements on a web page:</p>
<p>::: {.highlight-pycon .notranslate}
::: highlight
&gt;&gt;&gt; response.css(&quot;base&quot;).attrib[&quot;href&quot;]
'http://example.com/'
:::
:::</p>
<p>Now we're going to get the base URL and some image links:</p>
<p>::: {.highlight-pycon .notranslate}
::: highlight
&gt;&gt;&gt; response.xpath(&quot;//base/@href&quot;).get()
'http://example.com/'</p>
<pre><code>&gt;&gt;&gt; response.css(&quot;base::attr(href)&quot;).get()
'http://example.com/'

&gt;&gt;&gt; response.css(&quot;base&quot;).attrib[&quot;href&quot;]
'http://example.com/'

&gt;&gt;&gt; response.xpath('//a[contains(@href, &quot;image&quot;)]/@href').getall()
['image1.html',
'image2.html',
'image3.html',
'image4.html',
'image5.html']

&gt;&gt;&gt; response.css(&quot;a[href*=image]::attr(href)&quot;).getall()
['image1.html',
'image2.html',
'image3.html',
'image4.html',
'image5.html']

&gt;&gt;&gt; response.xpath('//a[contains(@href, &quot;image&quot;)]/img/@src').getall()
['image1_thumb.jpg',
'image2_thumb.jpg',
'image3_thumb.jpg',
'image4_thumb.jpg',
'image5_thumb.jpg']

&gt;&gt;&gt; response.css(&quot;a[href*=image] img::attr(src)&quot;).getall()
['image1_thumb.jpg',
'image2_thumb.jpg',
'image3_thumb.jpg',
'image4_thumb.jpg',
'image5_thumb.jpg']
</code></pre>
<p>:::
:::
:::</p>
<p>::: {#extensions-to-css-selectors .section}
[]{#topics-selectors-css-extensions}</p>
<h5 id="extensions-to-css-selectorsheaderlink"><a class="header" href="#extensions-to-css-selectorsheaderlink">Extensions to CSS Selectors<a href="#extensions-to-css-selectors" title="Permalink to this heading">¶</a>{.headerlink}</a></h5>
<p>Per W3C standards, <a href="https://www.w3.org/TR/selectors-3/#selectors">CSS
selectors</a>{.reference
.external} do not support selecting text nodes or attribute values. But
selecting these is so essential in a web scraping context that Scrapy
(parsel) implements a couple of <strong>non-standard pseudo-elements</strong>:</p>
<ul>
<li>
<p>to select text nodes, use [<code>::text</code>{.docutils .literal
.notranslate}]{.pre}</p>
</li>
<li>
<p>to select attribute values, use [<code>::attr(name)</code>{.docutils .literal
.notranslate}]{.pre} where <em>name</em> is the name of the attribute that
you want the value of</p>
</li>
</ul>
<p>::: {.admonition .warning}
Warning</p>
<p>These pseudo-elements are Scrapy-/Parsel-specific. They will most
probably not work with other libraries like
<a href="https://lxml.de/">lxml</a>{.reference .external} or
<a href="https://pypi.org/project/pyquery/">PyQuery</a>{.reference .external}.
:::</p>
<p>Examples:</p>
<ul>
<li>[<code>title::text</code>{.docutils .literal .notranslate}]{.pre} selects
children text nodes of a descendant [<code>&lt;title&gt;</code>{.docutils .literal
.notranslate}]{.pre} element:</li>
</ul>
<p>::: {.highlight-pycon .notranslate}
::: highlight
&gt;&gt;&gt; response.css(&quot;title::text&quot;).get()
'Example website'
:::
:::</p>
<ul>
<li>[<code>*::text</code>{.docutils .literal .notranslate}]{.pre} selects all
descendant text nodes of the current selector context:</li>
</ul>
<p>::: {.highlight-pycon .notranslate}
::: highlight
&gt;&gt;&gt; response.css(&quot;#images *::text&quot;).getall()
['\n   ',
'Name: My image 1 ',
'\n   ',
'Name: My image 2 ',
'\n   ',
'Name: My image 3 ',
'\n   ',
'Name: My image 4 ',
'\n   ',
'Name: My image 5 ',
'\n  ']
:::
:::</p>
<ul>
<li>[<code>foo::text</code>{.docutils .literal .notranslate}]{.pre} returns no
results if [<code>foo</code>{.docutils .literal .notranslate}]{.pre} element
exists, but contains no text (i.e. text is empty):</li>
</ul>
<p>::: {.highlight-pycon .notranslate}
::: highlight
&gt;&gt;&gt; response.css(&quot;img::text&quot;).getall()
[]</p>
<pre><code>This means ``.css('foo::text').get()`` could return None even if an element
exists. Use ``default=''`` if you always want a string:
</code></pre>
<p>:::
:::</p>
<p>::: {.highlight-pycon .notranslate}
::: highlight
&gt;&gt;&gt; response.css(&quot;img::text&quot;).get()
&gt;&gt;&gt; response.css(&quot;img::text&quot;).get(default=&quot;&quot;)
''
:::
:::</p>
<ul>
<li>[<code>a::attr(href)</code>{.docutils .literal .notranslate}]{.pre} selects the
<em>href</em> attribute value of descendant links:</li>
</ul>
<p>::: {.highlight-pycon .notranslate}
::: highlight
&gt;&gt;&gt; response.css(&quot;a::attr(href)&quot;).getall()
['image1.html',
'image2.html',
'image3.html',
'image4.html',
'image5.html']
:::
:::</p>
<p>::: {.admonition .note}
Note</p>
<p>See also: <a href="#selecting-attributes">[Selecting element attributes]{.std
.std-ref}</a>{.hoverxref .tooltip .reference
.internal}.
:::</p>
<p>::: {.admonition .note}
Note</p>
<p>You cannot chain these pseudo-elements. But in practice it would not
make much sense: text nodes do not have attributes, and attribute values
are string values already and do not have children nodes.
:::
:::</p>
<p>::: {#nesting-selectors .section}
[]{#topics-selectors-nesting-selectors}</p>
<h5 id="nesting-selectorsheaderlink"><a class="header" href="#nesting-selectorsheaderlink">Nesting selectors<a href="#nesting-selectors" title="Permalink to this heading">¶</a>{.headerlink}</a></h5>
<p>The selection methods ([<code>.xpath()</code>{.docutils .literal
.notranslate}]{.pre} or [<code>.css()</code>{.docutils .literal
.notranslate}]{.pre}) return a list of selectors of the same type, so
you can call the selection methods for those selectors too. Here's an
example:</p>
<p>::: {.highlight-pycon .notranslate}
::: highlight
&gt;&gt;&gt; links = response.xpath('//a[contains(@href, &quot;image&quot;)]')
&gt;&gt;&gt; links.getall()
['<a href="image1.html">Name: My image 1 <br><img src="image1_thumb.jpg" alt="image1"></a>',
'<a href="image2.html">Name: My image 2 <br><img src="image2_thumb.jpg" alt="image2"></a>',
'<a href="image3.html">Name: My image 3 <br><img src="image3_thumb.jpg" alt="image3"></a>',
'<a href="image4.html">Name: My image 4 <br><img src="image4_thumb.jpg" alt="image4"></a>',
'<a href="image5.html">Name: My image 5 <br><img src="image5_thumb.jpg" alt="image5"></a>']</p>
<pre><code>&gt;&gt;&gt; for index, link in enumerate(links):
...     href_xpath = link.xpath(&quot;@href&quot;).get()
...     img_xpath = link.xpath(&quot;img/@src&quot;).get()
...     print(f&quot;Link number {index} points to url {href_xpath!r} and image {img_xpath!r}&quot;)
...
Link number 0 points to url 'image1.html' and image 'image1_thumb.jpg'
Link number 1 points to url 'image2.html' and image 'image2_thumb.jpg'
Link number 2 points to url 'image3.html' and image 'image3_thumb.jpg'
Link number 3 points to url 'image4.html' and image 'image4_thumb.jpg'
Link number 4 points to url 'image5.html' and image 'image5_thumb.jpg'
</code></pre>
<p>:::
:::
:::</p>
<p>::: {#selecting-element-attributes .section}
[]{#selecting-attributes}</p>
<h5 id="selecting-element-attributesheaderlink"><a class="header" href="#selecting-element-attributesheaderlink">Selecting element attributes<a href="#selecting-element-attributes" title="Permalink to this heading">¶</a>{.headerlink}</a></h5>
<p>There are several ways to get a value of an attribute. First, one can
use XPath syntax:</p>
<p>::: {.highlight-pycon .notranslate}
::: highlight
&gt;&gt;&gt; response.xpath(&quot;//a/@href&quot;).getall()
['image1.html', 'image2.html', 'image3.html', 'image4.html', 'image5.html']
:::
:::</p>
<p>XPath syntax has a few advantages: it is a standard XPath feature, and
[<code>@attributes</code>{.docutils .literal .notranslate}]{.pre} can be used in
other parts of an XPath expression - e.g. it is possible to filter by
attribute value.</p>
<p>Scrapy also provides an extension to CSS selectors
([<code>::attr(...)</code>{.docutils .literal .notranslate}]{.pre}) which allows to
get attribute values:</p>
<p>::: {.highlight-pycon .notranslate}
::: highlight
&gt;&gt;&gt; response.css(&quot;a::attr(href)&quot;).getall()
['image1.html', 'image2.html', 'image3.html', 'image4.html', 'image5.html']
:::
:::</p>
<p>In addition to that, there is a [<code>.attrib</code>{.docutils .literal
.notranslate}]{.pre} property of Selector. You can use it if you prefer
to lookup attributes in Python code, without using XPaths or CSS
extensions:</p>
<p>::: {.highlight-pycon .notranslate}
::: highlight
&gt;&gt;&gt; [a.attrib[&quot;href&quot;] for a in response.css(&quot;a&quot;)]
['image1.html', 'image2.html', 'image3.html', 'image4.html', 'image5.html']
:::
:::</p>
<p>This property is also available on SelectorList; it returns a dictionary
with attributes of a first matching element. It is convenient to use
when a selector is expected to give a single result (e.g. when selecting
by element ID, or when selecting an unique element on a page):</p>
<p>::: {.highlight-pycon .notranslate}
::: highlight
&gt;&gt;&gt; response.css(&quot;base&quot;).attrib
{'href': 'http://example.com/'}
&gt;&gt;&gt; response.css(&quot;base&quot;).attrib[&quot;href&quot;]
'http://example.com/'
:::
:::</p>
<p>[<code>.attrib</code>{.docutils .literal .notranslate}]{.pre} property of an empty
SelectorList is empty:</p>
<p>::: {.highlight-pycon .notranslate}
::: highlight
&gt;&gt;&gt; response.css(&quot;foo&quot;).attrib
{}
:::
:::
:::</p>
<p>::: {#using-selectors-with-regular-expressions .section}</p>
<h5 id="using-selectors-with-regular-expressionsheaderlink"><a class="header" href="#using-selectors-with-regular-expressionsheaderlink">Using selectors with regular expressions<a href="#using-selectors-with-regular-expressions" title="Permalink to this heading">¶</a>{.headerlink}</a></h5>
<p>[<code>Selector</code>{.xref .py .py-class .docutils .literal .notranslate}]{.pre}
also has a [<code>.re()</code>{.docutils .literal .notranslate}]{.pre} method for
extracting data using regular expressions. However, unlike using
[<code>.xpath()</code>{.docutils .literal .notranslate}]{.pre} or
[<code>.css()</code>{.docutils .literal .notranslate}]{.pre} methods,
[<code>.re()</code>{.docutils .literal .notranslate}]{.pre} returns a list of
strings. So you can't construct nested [<code>.re()</code>{.docutils .literal
.notranslate}]{.pre} calls.</p>
<p>Here's an example used to extract image names from the <a href="#topics-selectors-htmlcode">[HTML code]{.std
.std-ref}</a>{.hoverxref .tooltip .reference
.internal} above:</p>
<p>::: {.highlight-pycon .notranslate}
::: highlight
&gt;&gt;&gt; response.xpath('//a[contains(@href, &quot;image&quot;)]/text()').re(r&quot;Name:\s*(.*)&quot;)
['My image 1 ',
'My image 2 ',
'My image 3 ',
'My image 4 ',
'My image 5 ']
:::
:::</p>
<p>There's an additional helper reciprocating [<code>.get()</code>{.docutils .literal
.notranslate}]{.pre} (and its alias [<code>.extract_first()</code>{.docutils
.literal .notranslate}]{.pre}) for [<code>.re()</code>{.docutils .literal
.notranslate}]{.pre}, named [<code>.re_first()</code>{.docutils .literal
.notranslate}]{.pre}. Use it to extract just the first matching string:</p>
<p>::: {.highlight-pycon .notranslate}
::: highlight
&gt;&gt;&gt; response.xpath('//a[contains(@href, &quot;image&quot;)]/text()').re_first(r&quot;Name:\s*(.*)&quot;)
'My image 1 '
:::
:::
:::</p>
<p>::: {#extract-and-extract-first .section}
[]{#old-extraction-api}</p>
<h5 id="extract-and-extract_firstheaderlink"><a class="header" href="#extract-and-extract_firstheaderlink">extract() and extract_first()<a href="#extract-and-extract-first" title="Permalink to this heading">¶</a>{.headerlink}</a></h5>
<p>If you're a long-time Scrapy user, you're probably familiar with
[<code>.extract()</code>{.docutils .literal .notranslate}]{.pre} and
[<code>.extract_first()</code>{.docutils .literal .notranslate}]{.pre} selector
methods. Many blog posts and tutorials are using them as well. These
methods are still supported by Scrapy, there are <strong>no plans</strong> to
deprecate them.</p>
<p>However, Scrapy usage docs are now written using [<code>.get()</code>{.docutils
.literal .notranslate}]{.pre} and [<code>.getall()</code>{.docutils .literal
.notranslate}]{.pre} methods. We feel that these new methods result in a
more concise and readable code.</p>
<p>The following examples show how these methods map to each other.</p>
<ol>
<li>[<code>SelectorList.get()</code>{.docutils .literal .notranslate}]{.pre} is the
same as [<code>SelectorList.extract_first()</code>{.docutils .literal
.notranslate}]{.pre}:</li>
</ol>
<p>::: {.highlight-pycon .notranslate}
::: highlight
&gt;&gt;&gt; response.css(&quot;a::attr(href)&quot;).get()
'image1.html'
&gt;&gt;&gt; response.css(&quot;a::attr(href)&quot;).extract_first()
'image1.html'
:::
:::</p>
<ol start="2">
<li>[<code>SelectorList.getall()</code>{.docutils .literal .notranslate}]{.pre} is
the same as [<code>SelectorList.extract()</code>{.docutils .literal
.notranslate}]{.pre}:</li>
</ol>
<p>::: {.highlight-pycon .notranslate}
::: highlight
&gt;&gt;&gt; response.css(&quot;a::attr(href)&quot;).getall()
['image1.html', 'image2.html', 'image3.html', 'image4.html', 'image5.html']
&gt;&gt;&gt; response.css(&quot;a::attr(href)&quot;).extract()
['image1.html', 'image2.html', 'image3.html', 'image4.html', 'image5.html']
:::
:::</p>
<ol start="3">
<li>[<code>Selector.get()</code>{.docutils .literal .notranslate}]{.pre} is the
same as [<code>Selector.extract()</code>{.docutils .literal
.notranslate}]{.pre}:</li>
</ol>
<p>::: {.highlight-pycon .notranslate}
::: highlight
&gt;&gt;&gt; response.css(&quot;a::attr(href)&quot;)[0].get()
'image1.html'
&gt;&gt;&gt; response.css(&quot;a::attr(href)&quot;)[0].extract()
'image1.html'
:::
:::</p>
<ol start="4">
<li>For consistency, there is also [<code>Selector.getall()</code>{.docutils
.literal .notranslate}]{.pre}, which returns a list:</li>
</ol>
<p>::: {.highlight-pycon .notranslate}
::: highlight
&gt;&gt;&gt; response.css(&quot;a::attr(href)&quot;)[0].getall()
['image1.html']
:::
:::</p>
<p>So, the main difference is that output of [<code>.get()</code>{.docutils .literal
.notranslate}]{.pre} and [<code>.getall()</code>{.docutils .literal
.notranslate}]{.pre} methods is more predictable: [<code>.get()</code>{.docutils
.literal .notranslate}]{.pre} always returns a single result,
[<code>.getall()</code>{.docutils .literal .notranslate}]{.pre} always returns a
list of all extracted results. With [<code>.extract()</code>{.docutils .literal
.notranslate}]{.pre} method it was not always obvious if a result is a
list or not; to get a single result either [<code>.extract()</code>{.docutils
.literal .notranslate}]{.pre} or [<code>.extract_first()</code>{.docutils .literal
.notranslate}]{.pre} should be called.
:::
:::</p>
<p>::: {#working-with-xpaths .section}
[]{#topics-selectors-xpaths}</p>
<h4 id="working-with-xpathsheaderlink"><a class="header" href="#working-with-xpathsheaderlink">Working with XPaths<a href="#working-with-xpaths" title="Permalink to this heading">¶</a>{.headerlink}</a></h4>
<p>Here are some tips which may help you to use XPath with Scrapy selectors
effectively. If you are not much familiar with XPath yet, you may want
to take a look first at this <a href="http://www.zvon.org/comp/r/tut-XPath_1.html">XPath
tutorial</a>{.reference
.external}.</p>
<p>::: {.admonition .note}
Note</p>
<p>Some of the tips are based on <a href="https://www.zyte.com/blog/xpath-tips-from-the-web-scraping-trenches/">this post from Zyte's
blog</a>{.reference
.external}.
:::</p>
<p>::: {#working-with-relative-xpaths .section}
[]{#topics-selectors-relative-xpaths}</p>
<h5 id="working-with-relative-xpathsheaderlink"><a class="header" href="#working-with-relative-xpathsheaderlink">Working with relative XPaths<a href="#working-with-relative-xpaths" title="Permalink to this heading">¶</a>{.headerlink}</a></h5>
<p>Keep in mind that if you are nesting selectors and use an XPath that
starts with [<code>/</code>{.docutils .literal .notranslate}]{.pre}, that XPath
will be absolute to the document and not relative to the
[<code>Selector</code>{.docutils .literal .notranslate}]{.pre} you're calling it
from.</p>
<p>For example, suppose you want to extract all [<code>&lt;p&gt;</code>{.docutils .literal
.notranslate}]{.pre} elements inside [<code>&lt;div&gt;</code>{.docutils .literal
.notranslate}]{.pre} elements. First, you would get all
[<code>&lt;div&gt;</code>{.docutils .literal .notranslate}]{.pre} elements:</p>
<p>::: {.highlight-pycon .notranslate}
::: highlight
&gt;&gt;&gt; divs = response.xpath(&quot;//div&quot;)
:::
:::</p>
<p>At first, you may be tempted to use the following approach, which is
wrong, as it actually extracts all [<code>&lt;p&gt;</code>{.docutils .literal
.notranslate}]{.pre} elements from the document, not only those inside
[<code>&lt;div&gt;</code>{.docutils .literal .notranslate}]{.pre} elements:</p>
<p>::: {.highlight-pycon .notranslate}
::: highlight
&gt;&gt;&gt; for p in divs.xpath(&quot;//p&quot;):  # this is wrong - gets all <p> from the whole document
...     print(p.get())
...
:::
:::</p>
<p>This is the proper way to do it (note the dot prefixing the
[<code>.//p</code>{.docutils .literal .notranslate}]{.pre} XPath):</p>
<p>::: {.highlight-pycon .notranslate}
::: highlight
&gt;&gt;&gt; for p in divs.xpath(&quot;.//p&quot;):  # extracts all <p> inside
...     print(p.get())
...
:::
:::</p>
<p>Another common case would be to extract all direct [<code>&lt;p&gt;</code>{.docutils
.literal .notranslate}]{.pre} children:</p>
<p>::: {.highlight-pycon .notranslate}
::: highlight
&gt;&gt;&gt; for p in divs.xpath(&quot;p&quot;):
...     print(p.get())
...
:::
:::</p>
<p>For more details about relative XPaths see the <a href="https://www.w3.org/TR/xpath/all/#location-paths">Location
Paths</a>{.reference
.external} section in the XPath specification.
:::</p>
<p>::: {#when-querying-by-class-consider-using-css .section}</p>
<h5 id="when-querying-by-class-consider-using-cssheaderlink"><a class="header" href="#when-querying-by-class-consider-using-cssheaderlink">When querying by class, consider using CSS<a href="#when-querying-by-class-consider-using-css" title="Permalink to this heading">¶</a>{.headerlink}</a></h5>
<p>Because an element can contain multiple CSS classes, the XPath way to
select elements by class is the rather verbose:</p>
<p>::: {.highlight-python .notranslate}
::: highlight
*[contains(concat(' ', normalize-space(@class), ' '), ' someclass ')]
:::
:::</p>
<p>If you use [<code>@class='someclass'</code>{.docutils .literal .notranslate}]{.pre}
you may end up missing elements that have other classes, and if you just
use [<code>contains(@class,</code>{.docutils .literal
.notranslate}]{.pre}<code> </code>{.docutils .literal
.notranslate}[<code>'someclass')</code>{.docutils .literal .notranslate}]{.pre} to
make up for that you may end up with more elements that you want, if
they have a different class name that shares the string
[<code>someclass</code>{.docutils .literal .notranslate}]{.pre}.</p>
<p>As it turns out, Scrapy selectors allow you to chain selectors, so most
of the time you can just select by class using CSS and then switch to
XPath when needed:</p>
<p>::: {.highlight-pycon .notranslate}
::: highlight
&gt;&gt;&gt; from scrapy import Selector
&gt;&gt;&gt; sel = Selector(
...     text='<div class="hero shout"><time datetime="2014-07-23 19:00">Special date</time></div>'
... )
&gt;&gt;&gt; sel.css(&quot;.shout&quot;).xpath(&quot;./time/@datetime&quot;).getall()
['2014-07-23 19:00']
:::
:::</p>
<p>This is cleaner than using the verbose XPath trick shown above. Just
remember to use the [<code>.</code>{.docutils .literal .notranslate}]{.pre} in the
XPath expressions that will follow.
:::</p>
<p>::: {#beware-of-the-difference-between-node-1-and-node-1 .section}</p>
<h5 id="beware-of-the-difference-between-node1-and-node1headerlink"><a class="header" href="#beware-of-the-difference-between-node1-and-node1headerlink">Beware of the difference between //node[1] and (//node)[1]<a href="#beware-of-the-difference-between-node-1-and-node-1" title="Permalink to this heading">¶</a>{.headerlink}</a></h5>
<p>[<code>//node[1]</code>{.docutils .literal .notranslate}]{.pre} selects all the
nodes occurring first under their respective parents.</p>
<p>[<code>(//node)[1]</code>{.docutils .literal .notranslate}]{.pre} selects all the
nodes in the document, and then gets only the first of them.</p>
<p>Example:</p>
<p>::: {.highlight-pycon .notranslate}
::: highlight
&gt;&gt;&gt; from scrapy import Selector
&gt;&gt;&gt; sel = Selector(
...     text=&quot;&quot;&quot;
...     <ul class="list">
...         <li>1</li>
...         <li>2</li>
...         <li>3</li>
...     </ul>
...     <ul class="list">
...         <li>4</li>
...         <li>5</li>
...         <li>6</li>
...     </ul>&quot;&quot;&quot;
... )
&gt;&gt;&gt; xp = lambda x: sel.xpath(x).getall()
:::
:::</p>
<p>This gets all first [<code>&lt;li&gt;</code>{.docutils .literal .notranslate}]{.pre}
elements under whatever it is its parent:</p>
<p>::: {.highlight-pycon .notranslate}
::: highlight
&gt;&gt;&gt; xp(&quot;//li[1]&quot;)
['<li>1</li>', '<li>4</li>']
:::
:::</p>
<p>And this gets the first [<code>&lt;li&gt;</code>{.docutils .literal .notranslate}]{.pre}
element in the whole document:</p>
<p>::: {.highlight-pycon .notranslate}
::: highlight
&gt;&gt;&gt; xp(&quot;(//li)[1]&quot;)
['<li>1</li>']
:::
:::</p>
<p>This gets all first [<code>&lt;li&gt;</code>{.docutils .literal .notranslate}]{.pre}
elements under an [<code>&lt;ul&gt;</code>{.docutils .literal .notranslate}]{.pre}
parent:</p>
<p>::: {.highlight-pycon .notranslate}
::: highlight
&gt;&gt;&gt; xp(&quot;//ul/li[1]&quot;)
['<li>1</li>', '<li>4</li>']
:::
:::</p>
<p>And this gets the first [<code>&lt;li&gt;</code>{.docutils .literal .notranslate}]{.pre}
element under an [<code>&lt;ul&gt;</code>{.docutils .literal .notranslate}]{.pre} parent
in the whole document:</p>
<p>::: {.highlight-pycon .notranslate}
::: highlight
&gt;&gt;&gt; xp(&quot;(//ul/li)[1]&quot;)
['<li>1</li>']
:::
:::
:::</p>
<p>::: {#using-text-nodes-in-a-condition .section}</p>
<h5 id="using-text-nodes-in-a-conditionheaderlink"><a class="header" href="#using-text-nodes-in-a-conditionheaderlink">Using text nodes in a condition<a href="#using-text-nodes-in-a-condition" title="Permalink to this heading">¶</a>{.headerlink}</a></h5>
<p>When you need to use the text content as argument to an <a href="https://www.w3.org/TR/xpath/all/#section-String-Functions">XPath string
function</a>{.reference
.external}, avoid using [<code>.//text()</code>{.docutils .literal
.notranslate}]{.pre} and use just [<code>.</code>{.docutils .literal
.notranslate}]{.pre} instead.</p>
<p>This is because the expression [<code>.//text()</code>{.docutils .literal
.notranslate}]{.pre} yields a collection of text elements -- a
<em>node-set</em>. And when a node-set is converted to a string, which happens
when it is passed as argument to a string function like
[<code>contains()</code>{.docutils .literal .notranslate}]{.pre} or
[<code>starts-with()</code>{.docutils .literal .notranslate}]{.pre}, it results in
the text for the first element only.</p>
<p>Example:</p>
<p>::: {.highlight-pycon .notranslate}
::: highlight
&gt;&gt;&gt; from scrapy import Selector
&gt;&gt;&gt; sel = Selector(
...     text='<a href="#">Click here to go to the <strong>Next Page</strong></a>'
... )
:::
:::</p>
<p>Converting a <em>node-set</em> to string:</p>
<p>::: {.highlight-pycon .notranslate}
::: highlight
&gt;&gt;&gt; sel.xpath(&quot;//a//text()&quot;).getall()  # take a peek at the node-set
['Click here to go to the ', 'Next Page']
&gt;&gt;&gt; sel.xpath(&quot;string(//a[1]//text())&quot;).getall()  # convert it to string
['Click here to go to the ']
:::
:::</p>
<p>A <em>node</em> converted to a string, however, puts together the text of
itself plus of all its descendants:</p>
<p>::: {.highlight-pycon .notranslate}
::: highlight
&gt;&gt;&gt; sel.xpath(&quot;//a[1]&quot;).getall()  # select the first node
['<a href="#">Click here to go to the <strong>Next Page</strong></a>']
&gt;&gt;&gt; sel.xpath(&quot;string(//a[1])&quot;).getall()  # convert it to string
['Click here to go to the Next Page']
:::
:::</p>
<p>So, using the [<code>.//text()</code>{.docutils .literal .notranslate}]{.pre}
node-set won't select anything in this case:</p>
<p>::: {.highlight-pycon .notranslate}
::: highlight
&gt;&gt;&gt; sel.xpath(&quot;//a[contains(.//text(), 'Next Page')]&quot;).getall()
[]
:::
:::</p>
<p>But using the [<code>.</code>{.docutils .literal .notranslate}]{.pre} to mean the
node, works:</p>
<p>::: {.highlight-pycon .notranslate}
::: highlight
&gt;&gt;&gt; sel.xpath(&quot;//a[contains(., 'Next Page')]&quot;).getall()
['<a href="#">Click here to go to the <strong>Next Page</strong></a>']
:::
:::
:::</p>
<p>::: {#variables-in-xpath-expressions .section}
[]{#topics-selectors-xpath-variables}</p>
<h5 id="variables-in-xpath-expressionsheaderlink"><a class="header" href="#variables-in-xpath-expressionsheaderlink">Variables in XPath expressions<a href="#variables-in-xpath-expressions" title="Permalink to this heading">¶</a>{.headerlink}</a></h5>
<p>XPath allows you to reference variables in your XPath expressions, using
the [<code>$somevariable</code>{.docutils .literal .notranslate}]{.pre} syntax.
This is somewhat similar to parameterized queries or prepared statements
in the SQL world where you replace some arguments in your queries with
placeholders like [<code>?</code>{.docutils .literal .notranslate}]{.pre}, which
are then substituted with values passed with the query.</p>
<p>Here's an example to match an element based on its &quot;id&quot; attribute value,
without hard-coding it (that was shown previously):</p>
<p>::: {.highlight-pycon .notranslate}
::: highlight
&gt;&gt;&gt; # <code>$val</code> used in the expression, a <code>val</code> argument needs to be passed
&gt;&gt;&gt; response.xpath(&quot;//div[@id=$val]/a/text()&quot;, val=&quot;images&quot;).get()
'Name: My image 1 '
:::
:::</p>
<p>Here's another example, to find the &quot;id&quot; attribute of a
[<code>&lt;div&gt;</code>{.docutils .literal .notranslate}]{.pre} tag containing five
[<code>&lt;a&gt;</code>{.docutils .literal .notranslate}]{.pre} children (here we pass
the value [<code>5</code>{.docutils .literal .notranslate}]{.pre} as an integer):</p>
<p>::: {.highlight-pycon .notranslate}
::: highlight
&gt;&gt;&gt; response.xpath(&quot;//div[count(a)=$cnt]/@id&quot;, cnt=5).get()
'images'
:::
:::</p>
<p>All variable references must have a binding value when calling
[<code>.xpath()</code>{.docutils .literal .notranslate}]{.pre} (otherwise you'll
get a [<code>ValueError:</code>{.docutils .literal
.notranslate}]{.pre}<code> </code>{.docutils .literal
.notranslate}[<code>XPath</code>{.docutils .literal
.notranslate}]{.pre}<code> </code>{.docutils .literal
.notranslate}[<code>error:</code>{.docutils .literal .notranslate}]{.pre}
exception). This is done by passing as many named arguments as
necessary.</p>
<p><a href="https://parsel.readthedocs.io/en/latest/">parsel</a>{.reference
.external}, the library powering Scrapy selectors, has more details and
examples on <a href="https://parsel.readthedocs.io/en/latest/usage.html#variables-in-xpath-expressions">XPath
variables</a>{.reference
.external}.
:::</p>
<p>::: {#removing-namespaces .section}
[]{#id2}</p>
<h5 id="removing-namespacesheaderlink"><a class="header" href="#removing-namespacesheaderlink">Removing namespaces<a href="#removing-namespaces" title="Permalink to this heading">¶</a>{.headerlink}</a></h5>
<p>When dealing with scraping projects, it is often quite convenient to get
rid of namespaces altogether and just work with element names, to write
more simple/convenient XPaths. You can use the
[<code>Selector.remove_namespaces()</code>{.xref .py .py-meth .docutils .literal
.notranslate}]{.pre} method for that.</p>
<p>Let's show an example that illustrates this with the Python Insider blog
atom feed.</p>
<p>First, we open the shell with the url we want to scrape:</p>
<p>::: {.highlight-sh .notranslate}
::: highlight
$ scrapy shell https://feeds.feedburner.com/PythonInsider
:::
:::</p>
<p>This is how the file starts:</p>
<p>::: {.highlight-sh .notranslate}
::: highlight
<?xml version="1.0" encoding="UTF-8"?>
&lt;?xml-stylesheet ...
<feed xmlns="http://www.w3.org/2005/Atom"
          xmlns:openSearch="http://a9.com/-/spec/opensearchrss/1.0/"
          xmlns:blogger="http://schemas.google.com/blogger/2008"
          xmlns:georss="http://www.georss.org/georss"
          xmlns:gd="http://schemas.google.com/g/2005"
          xmlns:thr="http://purl.org/syndication/thread/1.0"
          xmlns:feedburner="http://rssnamespace.org/feedburner/ext/1.0">
...
:::
:::</p>
<p>You can see several namespace declarations including a default
&quot;<a href="http://www.w3.org/2005/Atom">http://www.w3.org/2005/Atom</a>{.reference
.external}&quot; and another one using the &quot;gd:&quot; prefix for
&quot;<a href="http://schemas.google.com/g/2005">http://schemas.google.com/g/2005</a>{.reference
.external}&quot;.</p>
<p>Once in the shell we can try selecting all [<code>&lt;link&gt;</code>{.docutils .literal
.notranslate}]{.pre} objects and see that it doesn't work (because the
Atom XML namespace is obfuscating those nodes):</p>
<p>::: {.highlight-pycon .notranslate}
::: highlight
&gt;&gt;&gt; response.xpath(&quot;//link&quot;)
[]
:::
:::</p>
<p>But once we call the [<code>Selector.remove_namespaces()</code>{.xref .py .py-meth
.docutils .literal .notranslate}]{.pre} method, all nodes can be
accessed directly by their names:</p>
<p>::: {.highlight-pycon .notranslate}
::: highlight
&gt;&gt;&gt; response.selector.remove_namespaces()
&gt;&gt;&gt; response.xpath(&quot;//link&quot;)
[<Selector query='//link' data='<link rel="alternate" type="text/html" h'>,
<Selector query='//link' data='<link rel="next" type="application/atom+'>,
...
:::
:::</p>
<p>If you wonder why the namespace removal procedure isn't always called by
default instead of having to call it manually, this is because of two
reasons, which, in order of relevance, are:</p>
<ol>
<li>
<p>Removing namespaces requires to iterate and modify all nodes in the
document, which is a reasonably expensive operation to perform by
default for all documents crawled by Scrapy</p>
</li>
<li>
<p>There could be some cases where using namespaces is actually
required, in case some element names clash between namespaces. These
cases are very rare though.
:::</p>
</li>
</ol>
<p>::: {#using-exslt-extensions .section}</p>
<h5 id="using-exslt-extensionsheaderlink"><a class="header" href="#using-exslt-extensionsheaderlink">Using EXSLT extensions<a href="#using-exslt-extensions" title="Permalink to this heading">¶</a>{.headerlink}</a></h5>
<p>Being built atop <a href="https://lxml.de/">lxml</a>{.reference .external}, Scrapy
selectors support some <a href="http://exslt.org/">EXSLT</a>{.reference .external}
extensions and come with these pre-registered namespaces to use in XPath
expressions:</p>
<p>+-----+---------------------------------------+------------------------+
| pre | namespace                             | usage                  |
| fix |                                       |                        |
+=====+=======================================+========================+
| re  | http://exslt.org/regular-expressions  | [regular               |
|     |                                       | expressions](ht        |
|     |                                       | tp://exslt.org/regexp/ |
|     |                                       | index.html){.reference |
|     |                                       | .external}             |
+-----+---------------------------------------+------------------------+
| set | http://exslt.org/sets                 | [set                   |
|     |                                       | manipulation]          |
|     |                                       | (http://exslt.org/set/ |
|     |                                       | index.html){.reference |
|     |                                       | .external}             |
+-----+---------------------------------------+------------------------+</p>
<p>::: {#regular-expressions .section}</p>
<h6 id="regular-expressionsheaderlink"><a class="header" href="#regular-expressionsheaderlink">Regular expressions<a href="#regular-expressions" title="Permalink to this heading">¶</a>{.headerlink}</a></h6>
<p>The [<code>test()</code>{.docutils .literal .notranslate}]{.pre} function, for
example, can prove quite useful when XPath's [<code>starts-with()</code>{.docutils
.literal .notranslate}]{.pre} or [<code>contains()</code>{.docutils .literal
.notranslate}]{.pre} are not sufficient.</p>
<p>Example selecting links in list item with a &quot;class&quot; attribute ending
with a digit:</p>
<p>::: {.highlight-pycon .notranslate}
::: highlight
&gt;&gt;&gt; from scrapy import Selector
&gt;&gt;&gt; doc = &quot;&quot;&quot;
... <div>
...     <ul>
...         <li class="item-0"><a href="link1.html">first item</a></li>
...         <li class="item-1"><a href="link2.html">second item</a></li>
...         <li class="item-inactive"><a href="link3.html">third item</a></li>
...         <li class="item-1"><a href="link4.html">fourth item</a></li>
...         <li class="item-0"><a href="link5.html">fifth item</a></li>
...     </ul>
... </div>
... &quot;&quot;&quot;
&gt;&gt;&gt; sel = Selector(text=doc, type=&quot;html&quot;)
&gt;&gt;&gt; sel.xpath(&quot;//li//@href&quot;).getall()
['link1.html', 'link2.html', 'link3.html', 'link4.html', 'link5.html']
&gt;&gt;&gt; sel.xpath('//li[re:test(@class, &quot;item-\d$&quot;)]//@href').getall()
['link1.html', 'link2.html', 'link4.html', 'link5.html']
:::
:::</p>
<p>::: {.admonition .warning}
Warning</p>
<p>C library [<code>libxslt</code>{.docutils .literal .notranslate}]{.pre} doesn't
natively support EXSLT regular expressions so
<a href="https://lxml.de/">lxml</a>{.reference .external}'s implementation uses
hooks to Python's [<code>re</code>{.docutils .literal .notranslate}]{.pre} module.
Thus, using regexp functions in your XPath expressions may add a small
performance penalty.
:::
:::</p>
<p>::: {#set-operations .section}</p>
<h6 id="set-operationsheaderlink"><a class="header" href="#set-operationsheaderlink">Set operations<a href="#set-operations" title="Permalink to this heading">¶</a>{.headerlink}</a></h6>
<p>These can be handy for excluding parts of a document tree before
extracting text elements for example.</p>
<p>Example extracting microdata (sample content taken from
<a href="https://schema.org/Product">https://schema.org/Product</a>{.reference
.external}) with groups of itemscopes and corresponding itemprops:</p>
<p>::: {.highlight-pycon .notranslate}
::: highlight
&gt;&gt;&gt; doc = &quot;&quot;&quot;
... <div itemscope itemtype="http://schema.org/Product">
...   <span itemprop="name">Kenmore White 17&quot; Microwave</span>
...   <img src="kenmore-microwave-17in.jpg" alt='Kenmore 17" Microwave' />
...   &lt;div itemprop=&quot;aggregateRating&quot;
...     itemscope itemtype=&quot;http://schema.org/AggregateRating&quot;&gt;
...    Rated <span itemprop="ratingValue">3.5</span>/5
...    based on <span itemprop="reviewCount">11</span> customer reviews
...   </div>
...   <div itemprop="offers" itemscope itemtype="http://schema.org/Offer">
...     <span itemprop="price">$55.00</span>
...     <link itemprop="availability" href="http://schema.org/InStock" />In stock
...   </div>
...   Product description:
...   <span itemprop="description">0.7 cubic feet countertop microwave.
...   Has six preset cooking categories and convenience features like
...   Add-A-Minute and Child Lock.</span>
...   Customer reviews:
...   <div itemprop="review" itemscope itemtype="http://schema.org/Review">
...     <span itemprop="name">Not a happy camper</span> -
...     by <span itemprop="author">Ellie</span>,
...     <meta itemprop="datePublished" content="2011-04-01">April 1, 2011
...     <div itemprop="reviewRating" itemscope itemtype="http://schema.org/Rating">
...       <meta itemprop="worstRating" content = "1">
...       <span itemprop="ratingValue">1</span>/
...       <span itemprop="bestRating">5</span>stars
...     </div>
...     <span itemprop="description">The lamp burned out and now I have to replace
...     it. </span>
...   </div>
...   <div itemprop="review" itemscope itemtype="http://schema.org/Review">
...     <span itemprop="name">Value purchase</span> -
...     by <span itemprop="author">Lucas</span>,
...     <meta itemprop="datePublished" content="2011-03-25">March 25, 2011
...     <div itemprop="reviewRating" itemscope itemtype="http://schema.org/Rating">
...       <meta itemprop="worstRating" content = "1"/>
...       <span itemprop="ratingValue">4</span>/
...       <span itemprop="bestRating">5</span>stars
...     </div>
...     <span itemprop="description">Great microwave for the price. It is small and
...     fits in my apartment.</span>
...   </div>
...   ...
... </div>
... &quot;&quot;&quot;
&gt;&gt;&gt; sel = Selector(text=doc, type=&quot;html&quot;)
&gt;&gt;&gt; for scope in sel.xpath(&quot;//div[@itemscope]&quot;):
...     print(&quot;current scope:&quot;, scope.xpath(&quot;@itemtype&quot;).getall())
...     props = scope.xpath(
...         &quot;&quot;&quot;
...                 set:difference(./descendant::<em>/@itemprop,
...                                .//</em>[@itemscope]/*/@itemprop)&quot;&quot;&quot;
...     )
...     print(f&quot;    properties: {props.getall()}&quot;)
...     print(&quot;&quot;)
...</p>
<pre><code>current scope: ['http://schema.org/Product']
    properties: ['name', 'aggregateRating', 'offers', 'description', 'review', 'review']

current scope: ['http://schema.org/AggregateRating']
    properties: ['ratingValue', 'reviewCount']

current scope: ['http://schema.org/Offer']
    properties: ['price', 'availability']

current scope: ['http://schema.org/Review']
    properties: ['name', 'author', 'datePublished', 'reviewRating', 'description']

current scope: ['http://schema.org/Rating']
    properties: ['worstRating', 'ratingValue', 'bestRating']

current scope: ['http://schema.org/Review']
    properties: ['name', 'author', 'datePublished', 'reviewRating', 'description']

current scope: ['http://schema.org/Rating']
    properties: ['worstRating', 'ratingValue', 'bestRating']
</code></pre>
<p>:::
:::</p>
<p>Here we first iterate over [<code>itemscope</code>{.docutils .literal
.notranslate}]{.pre} elements, and for each one, we look for all
[<code>itemprops</code>{.docutils .literal .notranslate}]{.pre} elements and
exclude those that are themselves inside another [<code>itemscope</code>{.docutils
.literal .notranslate}]{.pre}.
:::
:::</p>
<p>::: {#other-xpath-extensions .section}</p>
<h5 id="other-xpath-extensionsheaderlink"><a class="header" href="#other-xpath-extensionsheaderlink">Other XPath extensions<a href="#other-xpath-extensions" title="Permalink to this heading">¶</a>{.headerlink}</a></h5>
<p>Scrapy selectors also provide a sorely missed XPath extension function
[<code>has-class</code>{.docutils .literal .notranslate}]{.pre} that returns
[<code>True</code>{.docutils .literal .notranslate}]{.pre} for nodes that have all
of the specified HTML classes.</p>
<p>For the following HTML:</p>
<p>::: {.highlight-pycon .notranslate}
::: highlight
&gt;&gt;&gt; from scrapy.http import HtmlResponse
&gt;&gt;&gt; response = HtmlResponse(
...     url=&quot;http://example.com&quot;,
...     body=&quot;&quot;&quot;
... <html>
...     <body>
...         <p class="foo bar-baz">First</p>
...         <p class="foo">Second</p>
...         <p class="bar">Third</p>
...         <p>Fourth</p>
...     </body>
... </html>
... &quot;&quot;&quot;,
...     encoding=&quot;utf-8&quot;,
... )
:::
:::</p>
<p>You can use it like this:</p>
<p>::: {.highlight-pycon .notranslate}
::: highlight
&gt;&gt;&gt; response.xpath('//p[has-class(&quot;foo&quot;)]')
[<Selector query='//p[has-class("foo")]' data='<p class="foo bar-baz">First</p>'>,
<Selector query='//p[has-class("foo")]' data='<p class="foo">Second</p>'>]
&gt;&gt;&gt; response.xpath('//p[has-class(&quot;foo&quot;, &quot;bar-baz&quot;)]')
[<Selector query='//p[has-class("foo", "bar-baz")]' data='<p class="foo bar-baz">First</p>'>]
&gt;&gt;&gt; response.xpath('//p[has-class(&quot;foo&quot;, &quot;bar&quot;)]')
[]
:::
:::</p>
<p>So XPath [<code>//p[has-class(&quot;foo&quot;,</code>{.docutils .literal
.notranslate}]{.pre}<code> </code>{.docutils .literal
.notranslate}[<code>&quot;bar-baz&quot;)]</code>{.docutils .literal .notranslate}]{.pre} is
roughly equivalent to CSS [<code>p.foo.bar-baz</code>{.docutils .literal
.notranslate}]{.pre}. Please note, that it is slower in most of the
cases, because it's a pure-Python function that's invoked for every node
in question whereas the CSS lookup is translated into XPath and thus
runs more efficiently, so performance-wise its uses are limited to
situations that are not easily described with CSS selectors.</p>
<p>Parsel also simplifies adding your own XPath extensions.</p>
<p>[[parsel.xpathfuncs.]{.pre}]{.sig-prename .descclassname}[[set_xpathfunc]{.pre}]{.sig-name .descname}[(]{.sig-paren}<em>[[fname]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[<a href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.12)">[str]{.pre}</a>{.reference .external}]{.n}</em>, <em>[[func]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[<a href="https://docs.python.org/3/library/typing.html#typing.Optional" title="(in Python v3.12)">[Optional]{.pre}</a>{.reference .external}[[[]{.pre}]{.p}<a href="https://docs.python.org/3/library/typing.html#typing.Callable" title="(in Python v3.12)">[Callable]{.pre}</a>{.reference .external}[[]]{.pre}]{.p}]{.n}</em>[)]{.sig-paren} [[→]{.sig-return-icon} [<a href="https://docs.python.org/3/library/constants.html#None" title="(in Python v3.12)">[None]{.pre}</a>{.reference .external}]{.sig-return-typehint}]{.sig-return}<a href="_modules/parsel/xpathfuncs.html#set_xpathfunc">[[[source]]{.pre}]{.viewcode-link}</a>{.reference .internal}<a href="#parsel.xpathfuncs.set_xpathfunc" title="Permalink to this definition">¶</a>{.headerlink}</p>
<p>:   Register a custom extension function to use in XPath expressions.</p>
<pre><code>The function [`func`{.docutils .literal .notranslate}]{.pre}
registered under [`fname`{.docutils .literal .notranslate}]{.pre}
identifier will be called for every matching node, being passed a
[`context`{.docutils .literal .notranslate}]{.pre} parameter as well
as any parameters passed from the corresponding XPath expression.

If [`func`{.docutils .literal .notranslate}]{.pre} is
[`None`{.docutils .literal .notranslate}]{.pre}, the extension
function will be removed.

See more [in lxml
documentation](https://lxml.de/extensions.html#xpath-extension-functions){.reference
.external}.
</code></pre>
<p>:::
:::</p>
<p>::: {#module-scrapy.selector .section}
[]{#built-in-selectors-reference}[]{#topics-selectors-ref}</p>
<h4 id="built-in-selectors-referenceheaderlink"><a class="header" href="#built-in-selectors-referenceheaderlink">Built-in Selectors reference<a href="#module-scrapy.selector" title="Permalink to this heading">¶</a>{.headerlink}</a></h4>
<p>::: {#selector-objects .section}</p>
<h5 id="selector-objectsheaderlink"><a class="header" href="#selector-objectsheaderlink">Selector objects<a href="#selector-objects" title="Permalink to this heading">¶</a>{.headerlink}</a></h5>
<p><em>[class]{.pre}[ ]{.w}</em>[[scrapy.selector.]{.pre}]{.sig-prename .descclassname}[[Selector]{.pre}]{.sig-name .descname}[(]{.sig-paren}<em>[[*]{.pre}]{.o}[[args]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[<a href="https://docs.python.org/3/library/typing.html#typing.Any" title="(in Python v3.12)">[Any]{.pre}</a>{.reference .external}]{.n}</em>, <em>[[**]{.pre}]{.o}[[kwargs]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[<a href="https://docs.python.org/3/library/typing.html#typing.Any" title="(in Python v3.12)">[Any]{.pre}</a>{.reference .external}]{.n}</em>[)]{.sig-paren}<a href="_modules/scrapy/selector/unified.html#Selector">[[[source]]{.pre}]{.viewcode-link}</a>{.reference .internal}<a href="#scrapy.selector.Selector" title="Permalink to this definition">¶</a>{.headerlink}</p>
<p>:   An instance of <a href="#scrapy.selector.Selector" title="scrapy.selector.Selector">[<code>Selector</code>{.xref .py .py-class .docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal} is a wrapper over response to select certain parts of its
content.</p>
<pre><code>[`response`{.docutils .literal .notranslate}]{.pre} is an
[[`HtmlResponse`{.xref .py .py-class .docutils .literal
.notranslate}]{.pre}](index.html#scrapy.http.HtmlResponse &quot;scrapy.http.HtmlResponse&quot;){.reference
.internal} or an [[`XmlResponse`{.xref .py .py-class .docutils
.literal
.notranslate}]{.pre}](index.html#scrapy.http.XmlResponse &quot;scrapy.http.XmlResponse&quot;){.reference
.internal} object that will be used for selecting and extracting
data.

[`text`{.docutils .literal .notranslate}]{.pre} is a unicode string
or utf-8 encoded text for cases when a [`response`{.docutils
.literal .notranslate}]{.pre} isn't available. Using
[`text`{.docutils .literal .notranslate}]{.pre} and
[`response`{.docutils .literal .notranslate}]{.pre} together is
undefined behavior.

[`type`{.docutils .literal .notranslate}]{.pre} defines the selector
type, it can be [`&quot;html&quot;`{.docutils .literal .notranslate}]{.pre},
[`&quot;xml&quot;`{.docutils .literal .notranslate}]{.pre} or
[`None`{.docutils .literal .notranslate}]{.pre} (default).

If [`type`{.docutils .literal .notranslate}]{.pre} is
[`None`{.docutils .literal .notranslate}]{.pre}, the selector
automatically chooses the best type based on [`response`{.docutils
.literal .notranslate}]{.pre} type (see below), or defaults to
[`&quot;html&quot;`{.docutils .literal .notranslate}]{.pre} in case it is used
together with [`text`{.docutils .literal .notranslate}]{.pre}.

If [`type`{.docutils .literal .notranslate}]{.pre} is
[`None`{.docutils .literal .notranslate}]{.pre} and a
[`response`{.docutils .literal .notranslate}]{.pre} is passed, the
selector type is inferred from the response type as follows:

-   [`&quot;html&quot;`{.docutils .literal .notranslate}]{.pre} for
    [[`HtmlResponse`{.xref .py .py-class .docutils .literal
    .notranslate}]{.pre}](index.html#scrapy.http.HtmlResponse &quot;scrapy.http.HtmlResponse&quot;){.reference
    .internal} type

-   [`&quot;xml&quot;`{.docutils .literal .notranslate}]{.pre} for
    [[`XmlResponse`{.xref .py .py-class .docutils .literal
    .notranslate}]{.pre}](index.html#scrapy.http.XmlResponse &quot;scrapy.http.XmlResponse&quot;){.reference
    .internal} type

-   [`&quot;html&quot;`{.docutils .literal .notranslate}]{.pre} for anything
    else

Otherwise, if [`type`{.docutils .literal .notranslate}]{.pre} is
set, the selector type will be forced and no detection will occur.

[[xpath]{.pre}]{.sig-name .descname}[(]{.sig-paren}*[[query]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[[[str]{.pre}](https://docs.python.org/3/library/stdtypes.html#str &quot;(in Python v3.12)&quot;){.reference .external}]{.n}*, *[[namespaces]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[[[Optional]{.pre}](https://docs.python.org/3/library/typing.html#typing.Optional &quot;(in Python v3.12)&quot;){.reference .external}[[\[]{.pre}]{.p}[[Mapping]{.pre}](https://docs.python.org/3/library/typing.html#typing.Mapping &quot;(in Python v3.12)&quot;){.reference .external}[[\[]{.pre}]{.p}[[str]{.pre}](https://docs.python.org/3/library/stdtypes.html#str &quot;(in Python v3.12)&quot;){.reference .external}[[,]{.pre}]{.p}[ ]{.w}[[str]{.pre}](https://docs.python.org/3/library/stdtypes.html#str &quot;(in Python v3.12)&quot;){.reference .external}[[\]]{.pre}]{.p}[[\]]{.pre}]{.p}]{.n}[ ]{.w}[[=]{.pre}]{.o}[ ]{.w}[[None]{.pre}]{.default_value}*, *[[\*\*]{.pre}]{.o}[[kwargs]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[[[Any]{.pre}](https://docs.python.org/3/library/typing.html#typing.Any &quot;(in Python v3.12)&quot;){.reference .external}]{.n}*[)]{.sig-paren} [[→]{.sig-return-icon} [[SelectorList]{.pre}[[\[]{.pre}]{.p}[\_SelectorType]{.pre}[[\]]{.pre}]{.p}]{.sig-return-typehint}]{.sig-return}[[[\[source\]]{.pre}]{.viewcode-link}](_modules/parsel/selector.html#Selector.xpath){.reference .internal}[¶](#scrapy.selector.Selector.xpath &quot;Permalink to this definition&quot;){.headerlink}

:   Find nodes matching the xpath [`query`{.docutils .literal
    .notranslate}]{.pre} and return the result as a
    [[`SelectorList`{.xref .py .py-class .docutils .literal
    .notranslate}]{.pre}](#scrapy.selector.SelectorList &quot;scrapy.selector.SelectorList&quot;){.reference
    .internal} instance with all elements flattened. List elements
    implement [[`Selector`{.xref .py .py-class .docutils .literal
    .notranslate}]{.pre}](#scrapy.selector.Selector &quot;scrapy.selector.Selector&quot;){.reference
    .internal} interface too.

    [`query`{.docutils .literal .notranslate}]{.pre} is a string
    containing the XPATH query to apply.

    [`namespaces`{.docutils .literal .notranslate}]{.pre} is an
    optional [`prefix:`{.docutils .literal
    .notranslate}]{.pre}` `{.docutils .literal
    .notranslate}[`namespace-uri`{.docutils .literal
    .notranslate}]{.pre} mapping (dict) for additional prefixes to
    those registered with [`register_namespace(prefix,`{.docutils
    .literal .notranslate}]{.pre}` `{.docutils .literal
    .notranslate}[`uri)`{.docutils .literal .notranslate}]{.pre}.
    Contrary to [`register_namespace()`{.docutils .literal
    .notranslate}]{.pre}, these prefixes are not saved for future
    calls.

    Any additional named arguments can be used to pass values for
    XPath variables in the XPath expression, e.g.:

    ::: {.highlight-python .notranslate}
    ::: highlight
        selector.xpath('//a[href=$url]', url=&quot;http://www.example.com&quot;)
    :::
    :::

    ::: {.admonition .note}
    Note

    For convenience, this method can be called as
    [`response.xpath()`{.docutils .literal .notranslate}]{.pre}
    :::

[[css]{.pre}]{.sig-name .descname}[(]{.sig-paren}*[[query]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[[[str]{.pre}](https://docs.python.org/3/library/stdtypes.html#str &quot;(in Python v3.12)&quot;){.reference .external}]{.n}*[)]{.sig-paren} [[→]{.sig-return-icon} [[SelectorList]{.pre}[[\[]{.pre}]{.p}[\_SelectorType]{.pre}[[\]]{.pre}]{.p}]{.sig-return-typehint}]{.sig-return}[[[\[source\]]{.pre}]{.viewcode-link}](_modules/parsel/selector.html#Selector.css){.reference .internal}[¶](#scrapy.selector.Selector.css &quot;Permalink to this definition&quot;){.headerlink}

:   Apply the given CSS selector and return a [[`SelectorList`{.xref
    .py .py-class .docutils .literal
    .notranslate}]{.pre}](#scrapy.selector.SelectorList &quot;scrapy.selector.SelectorList&quot;){.reference
    .internal} instance.

    [`query`{.docutils .literal .notranslate}]{.pre} is a string
    containing the CSS selector to apply.

    In the background, CSS queries are translated into XPath queries
    using
    [cssselect](https://pypi.python.org/pypi/cssselect/){.reference
    .external} library and run [`.xpath()`{.docutils .literal
    .notranslate}]{.pre} method.

    ::: {.admonition .note}
    Note

    For convenience, this method can be called as
    [`response.css()`{.docutils .literal .notranslate}]{.pre}
    :::

[[get]{.pre}]{.sig-name .descname}[(]{.sig-paren}[)]{.sig-paren} [[→]{.sig-return-icon} [[[Any]{.pre}](https://docs.python.org/3/library/typing.html#typing.Any &quot;(in Python v3.12)&quot;){.reference .external}]{.sig-return-typehint}]{.sig-return}[[[\[source\]]{.pre}]{.viewcode-link}](_modules/parsel/selector.html#Selector.get){.reference .internal}[¶](#scrapy.selector.Selector.get &quot;Permalink to this definition&quot;){.headerlink}

:   Serialize and return the matched nodes in a single string.
    Percent encoded content is unquoted.

    See also: [[extract() and extract_first()]{.std
    .std-ref}](#old-extraction-api){.hoverxref .tooltip .reference
    .internal}

[[attrib]{.pre}]{.sig-name .descname}[¶](#scrapy.selector.Selector.attrib &quot;Permalink to this definition&quot;){.headerlink}

:   Return the attributes dictionary for underlying element.

    See also: [[Selecting element attributes]{.std
    .std-ref}](#selecting-attributes){.hoverxref .tooltip .reference
    .internal}.

[[re]{.pre}]{.sig-name .descname}[(]{.sig-paren}*[[regex]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[[[Union]{.pre}](https://docs.python.org/3/library/typing.html#typing.Union &quot;(in Python v3.12)&quot;){.reference .external}[[\[]{.pre}]{.p}[[str]{.pre}](https://docs.python.org/3/library/stdtypes.html#str &quot;(in Python v3.12)&quot;){.reference .external}[[,]{.pre}]{.p}[ ]{.w}[[Pattern]{.pre}](https://docs.python.org/3/library/typing.html#typing.Pattern &quot;(in Python v3.12)&quot;){.reference .external}[[\[]{.pre}]{.p}[[str]{.pre}](https://docs.python.org/3/library/stdtypes.html#str &quot;(in Python v3.12)&quot;){.reference .external}[[\]]{.pre}]{.p}[[\]]{.pre}]{.p}]{.n}*, *[[replace_entities]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[[[bool]{.pre}](https://docs.python.org/3/library/functions.html#bool &quot;(in Python v3.12)&quot;){.reference .external}]{.n}[ ]{.w}[[=]{.pre}]{.o}[ ]{.w}[[True]{.pre}]{.default_value}*[)]{.sig-paren} [[→]{.sig-return-icon} [[[List]{.pre}](https://docs.python.org/3/library/typing.html#typing.List &quot;(in Python v3.12)&quot;){.reference .external}[[\[]{.pre}]{.p}[[str]{.pre}](https://docs.python.org/3/library/stdtypes.html#str &quot;(in Python v3.12)&quot;){.reference .external}[[\]]{.pre}]{.p}]{.sig-return-typehint}]{.sig-return}[[[\[source\]]{.pre}]{.viewcode-link}](_modules/parsel/selector.html#Selector.re){.reference .internal}[¶](#scrapy.selector.Selector.re &quot;Permalink to this definition&quot;){.headerlink}

:   Apply the given regex and return a list of strings with the
    matches.

    [`regex`{.docutils .literal .notranslate}]{.pre} can be either a
    compiled regular expression or a string which will be compiled
    to a regular expression using [`re.compile(regex)`{.docutils
    .literal .notranslate}]{.pre}.

    By default, character entity references are replaced by their
    corresponding character (except for [`&amp;amp;`{.docutils .literal
    .notranslate}]{.pre} and [`&amp;lt;`{.docutils .literal
    .notranslate}]{.pre}). Passing [`replace_entities`{.docutils
    .literal .notranslate}]{.pre} as [`False`{.docutils .literal
    .notranslate}]{.pre} switches off these replacements.

[[re_first]{.pre}]{.sig-name .descname}[(]{.sig-paren}*[[regex]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[[[Union]{.pre}](https://docs.python.org/3/library/typing.html#typing.Union &quot;(in Python v3.12)&quot;){.reference .external}[[\[]{.pre}]{.p}[[str]{.pre}](https://docs.python.org/3/library/stdtypes.html#str &quot;(in Python v3.12)&quot;){.reference .external}[[,]{.pre}]{.p}[ ]{.w}[[Pattern]{.pre}](https://docs.python.org/3/library/typing.html#typing.Pattern &quot;(in Python v3.12)&quot;){.reference .external}[[\[]{.pre}]{.p}[[str]{.pre}](https://docs.python.org/3/library/stdtypes.html#str &quot;(in Python v3.12)&quot;){.reference .external}[[\]]{.pre}]{.p}[[\]]{.pre}]{.p}]{.n}*, *[[default]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[[[None]{.pre}](https://docs.python.org/3/library/constants.html#None &quot;(in Python v3.12)&quot;){.reference .external}]{.n}[ ]{.w}[[=]{.pre}]{.o}[ ]{.w}[[None]{.pre}]{.default_value}*, *[[replace_entities]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[[[bool]{.pre}](https://docs.python.org/3/library/functions.html#bool &quot;(in Python v3.12)&quot;){.reference .external}]{.n}[ ]{.w}[[=]{.pre}]{.o}[ ]{.w}[[True]{.pre}]{.default_value}*[)]{.sig-paren} [[→]{.sig-return-icon} [[[Optional]{.pre}](https://docs.python.org/3/library/typing.html#typing.Optional &quot;(in Python v3.12)&quot;){.reference .external}[[\[]{.pre}]{.p}[[str]{.pre}](https://docs.python.org/3/library/stdtypes.html#str &quot;(in Python v3.12)&quot;){.reference .external}[[\]]{.pre}]{.p}]{.sig-return-typehint}]{.sig-return}[[[\[source\]]{.pre}]{.viewcode-link}](_modules/parsel/selector.html#Selector.re_first){.reference .internal}[¶](#scrapy.selector.Selector.re_first &quot;Permalink to this definition&quot;){.headerlink}\
[[re_first]{.pre}]{.sig-name .descname}[(]{.sig-paren}*[[regex]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[[[Union]{.pre}](https://docs.python.org/3/library/typing.html#typing.Union &quot;(in Python v3.12)&quot;){.reference .external}[[\[]{.pre}]{.p}[[str]{.pre}](https://docs.python.org/3/library/stdtypes.html#str &quot;(in Python v3.12)&quot;){.reference .external}[[,]{.pre}]{.p}[ ]{.w}[[Pattern]{.pre}](https://docs.python.org/3/library/typing.html#typing.Pattern &quot;(in Python v3.12)&quot;){.reference .external}[[\[]{.pre}]{.p}[[str]{.pre}](https://docs.python.org/3/library/stdtypes.html#str &quot;(in Python v3.12)&quot;){.reference .external}[[\]]{.pre}]{.p}[[\]]{.pre}]{.p}]{.n}*, *[[default]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[[[str]{.pre}](https://docs.python.org/3/library/stdtypes.html#str &quot;(in Python v3.12)&quot;){.reference .external}]{.n}*, *[[replace_entities]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[[[bool]{.pre}](https://docs.python.org/3/library/functions.html#bool &quot;(in Python v3.12)&quot;){.reference .external}]{.n}[ ]{.w}[[=]{.pre}]{.o}[ ]{.w}[[True]{.pre}]{.default_value}*[)]{.sig-paren} [[→]{.sig-return-icon} [[[str]{.pre}](https://docs.python.org/3/library/stdtypes.html#str &quot;(in Python v3.12)&quot;){.reference .external}]{.sig-return-typehint}]{.sig-return}

:   Apply the given regex and return the first string which matches.
    If there is no match, return the default value
    ([`None`{.docutils .literal .notranslate}]{.pre} if the argument
    is not provided).

    By default, character entity references are replaced by their
    corresponding character (except for [`&amp;amp;`{.docutils .literal
    .notranslate}]{.pre} and [`&amp;lt;`{.docutils .literal
    .notranslate}]{.pre}). Passing [`replace_entities`{.docutils
    .literal .notranslate}]{.pre} as [`False`{.docutils .literal
    .notranslate}]{.pre} switches off these replacements.

[[register_namespace]{.pre}]{.sig-name .descname}[(]{.sig-paren}*[[prefix]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[[[str]{.pre}](https://docs.python.org/3/library/stdtypes.html#str &quot;(in Python v3.12)&quot;){.reference .external}]{.n}*, *[[uri]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[[[str]{.pre}](https://docs.python.org/3/library/stdtypes.html#str &quot;(in Python v3.12)&quot;){.reference .external}]{.n}*[)]{.sig-paren} [[→]{.sig-return-icon} [[[None]{.pre}](https://docs.python.org/3/library/constants.html#None &quot;(in Python v3.12)&quot;){.reference .external}]{.sig-return-typehint}]{.sig-return}[[[\[source\]]{.pre}]{.viewcode-link}](_modules/parsel/selector.html#Selector.register_namespace){.reference .internal}[¶](#scrapy.selector.Selector.register_namespace &quot;Permalink to this definition&quot;){.headerlink}

:   Register the given namespace to be used in this
    [[`Selector`{.xref .py .py-class .docutils .literal
    .notranslate}]{.pre}](#scrapy.selector.Selector &quot;scrapy.selector.Selector&quot;){.reference
    .internal}. Without registering namespaces you can't select or
    extract data from non-standard namespaces. See [[Selector
    examples on XML response]{.std
    .std-ref}](#selector-examples-xml){.hoverxref .tooltip
    .reference .internal}.

[[remove_namespaces]{.pre}]{.sig-name .descname}[(]{.sig-paren}[)]{.sig-paren} [[→]{.sig-return-icon} [[[None]{.pre}](https://docs.python.org/3/library/constants.html#None &quot;(in Python v3.12)&quot;){.reference .external}]{.sig-return-typehint}]{.sig-return}[[[\[source\]]{.pre}]{.viewcode-link}](_modules/parsel/selector.html#Selector.remove_namespaces){.reference .internal}[¶](#scrapy.selector.Selector.remove_namespaces &quot;Permalink to this definition&quot;){.headerlink}

:   Remove all namespaces, allowing to traverse the document using
    namespace-less xpaths. See [[Removing namespaces]{.std
    .std-ref}](#removing-namespaces){.hoverxref .tooltip .reference
    .internal}.

[[\_\_bool\_\_]{.pre}]{.sig-name .descname}[(]{.sig-paren}[)]{.sig-paren} [[→]{.sig-return-icon} [[[bool]{.pre}](https://docs.python.org/3/library/functions.html#bool &quot;(in Python v3.12)&quot;){.reference .external}]{.sig-return-typehint}]{.sig-return}[[[\[source\]]{.pre}]{.viewcode-link}](_modules/parsel/selector.html#Selector.__bool__){.reference .internal}[¶](#scrapy.selector.Selector.__bool__ &quot;Permalink to this definition&quot;){.headerlink}

:   Return [`True`{.docutils .literal .notranslate}]{.pre} if there
    is any real content selected or [`False`{.docutils .literal
    .notranslate}]{.pre} otherwise. In other words, the boolean
    value of a [[`Selector`{.xref .py .py-class .docutils .literal
    .notranslate}]{.pre}](#scrapy.selector.Selector &quot;scrapy.selector.Selector&quot;){.reference
    .internal} is given by the contents it selects.

[[getall]{.pre}]{.sig-name .descname}[(]{.sig-paren}[)]{.sig-paren} [[→]{.sig-return-icon} [[[List]{.pre}](https://docs.python.org/3/library/typing.html#typing.List &quot;(in Python v3.12)&quot;){.reference .external}[[\[]{.pre}]{.p}[[str]{.pre}](https://docs.python.org/3/library/stdtypes.html#str &quot;(in Python v3.12)&quot;){.reference .external}[[\]]{.pre}]{.p}]{.sig-return-typehint}]{.sig-return}[[[\[source\]]{.pre}]{.viewcode-link}](_modules/parsel/selector.html#Selector.getall){.reference .internal}[¶](#scrapy.selector.Selector.getall &quot;Permalink to this definition&quot;){.headerlink}

:   Serialize and return the matched node in a 1-element list of
    strings.

    This method is added to Selector for consistency; it is more
    useful with SelectorList. See also: [[extract() and
    extract_first()]{.std .std-ref}](#old-extraction-api){.hoverxref
    .tooltip .reference .internal}
</code></pre>
<p>:::</p>
<p>::: {#selectorlist-objects .section}</p>
<h5 id="selectorlist-objectsheaderlink"><a class="header" href="#selectorlist-objectsheaderlink">SelectorList objects<a href="#selectorlist-objects" title="Permalink to this heading">¶</a>{.headerlink}</a></h5>
<p><em>[class]{.pre}[ ]{.w}</em>[[scrapy.selector.]{.pre}]{.sig-prename .descclassname}[[SelectorList]{.pre}]{.sig-name .descname}[(]{.sig-paren}<em>[[iterable]{.pre}]{.n}[[=]{.pre}]{.o}[[()]{.pre}]{.default_value}</em>, <em>[[/]{.pre}]{.o}</em>[)]{.sig-paren}<a href="_modules/scrapy/selector/unified.html#SelectorList">[[[source]]{.pre}]{.viewcode-link}</a>{.reference .internal}<a href="#scrapy.selector.SelectorList" title="Permalink to this definition">¶</a>{.headerlink}</p>
<p>:   The <a href="#scrapy.selector.SelectorList" title="scrapy.selector.SelectorList">[<code>SelectorList</code>{.xref .py .py-class .docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal} class is a subclass of the builtin [<code>list</code>{.docutils
.literal .notranslate}]{.pre} class, which provides a few additional
methods.</p>
<pre><code>[[xpath]{.pre}]{.sig-name .descname}[(]{.sig-paren}*[[xpath]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[[[str]{.pre}](https://docs.python.org/3/library/stdtypes.html#str &quot;(in Python v3.12)&quot;){.reference .external}]{.n}*, *[[namespaces]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[[[Optional]{.pre}](https://docs.python.org/3/library/typing.html#typing.Optional &quot;(in Python v3.12)&quot;){.reference .external}[[\[]{.pre}]{.p}[[Mapping]{.pre}](https://docs.python.org/3/library/typing.html#typing.Mapping &quot;(in Python v3.12)&quot;){.reference .external}[[\[]{.pre}]{.p}[[str]{.pre}](https://docs.python.org/3/library/stdtypes.html#str &quot;(in Python v3.12)&quot;){.reference .external}[[,]{.pre}]{.p}[ ]{.w}[[str]{.pre}](https://docs.python.org/3/library/stdtypes.html#str &quot;(in Python v3.12)&quot;){.reference .external}[[\]]{.pre}]{.p}[[\]]{.pre}]{.p}]{.n}[ ]{.w}[[=]{.pre}]{.o}[ ]{.w}[[None]{.pre}]{.default_value}*, *[[\*\*]{.pre}]{.o}[[kwargs]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[[[Any]{.pre}](https://docs.python.org/3/library/typing.html#typing.Any &quot;(in Python v3.12)&quot;){.reference .external}]{.n}*[)]{.sig-paren} [[→]{.sig-return-icon} [[SelectorList]{.pre}[[\[]{.pre}]{.p}[\_SelectorType]{.pre}[[\]]{.pre}]{.p}]{.sig-return-typehint}]{.sig-return}[[[\[source\]]{.pre}]{.viewcode-link}](_modules/parsel/selector.html#SelectorList.xpath){.reference .internal}[¶](#scrapy.selector.SelectorList.xpath &quot;Permalink to this definition&quot;){.headerlink}

:   Call the [`.xpath()`{.docutils .literal .notranslate}]{.pre}
    method for each element in this list and return their results
    flattened as another [[`SelectorList`{.xref .py .py-class
    .docutils .literal
    .notranslate}]{.pre}](#scrapy.selector.SelectorList &quot;scrapy.selector.SelectorList&quot;){.reference
    .internal}.

    [`xpath`{.docutils .literal .notranslate}]{.pre} is the same
    argument as the one in [[`Selector.xpath()`{.xref .py .py-meth
    .docutils .literal
    .notranslate}]{.pre}](#scrapy.selector.Selector.xpath &quot;scrapy.selector.Selector.xpath&quot;){.reference
    .internal}

    [`namespaces`{.docutils .literal .notranslate}]{.pre} is an
    optional [`prefix:`{.docutils .literal
    .notranslate}]{.pre}` `{.docutils .literal
    .notranslate}[`namespace-uri`{.docutils .literal
    .notranslate}]{.pre} mapping (dict) for additional prefixes to
    those registered with [`register_namespace(prefix,`{.docutils
    .literal .notranslate}]{.pre}` `{.docutils .literal
    .notranslate}[`uri)`{.docutils .literal .notranslate}]{.pre}.
    Contrary to [`register_namespace()`{.docutils .literal
    .notranslate}]{.pre}, these prefixes are not saved for future
    calls.

    Any additional named arguments can be used to pass values for
    XPath variables in the XPath expression, e.g.:

    ::: {.highlight-python .notranslate}
    ::: highlight
        selector.xpath('//a[href=$url]', url=&quot;http://www.example.com&quot;)
    :::
    :::

[[css]{.pre}]{.sig-name .descname}[(]{.sig-paren}*[[query]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[[[str]{.pre}](https://docs.python.org/3/library/stdtypes.html#str &quot;(in Python v3.12)&quot;){.reference .external}]{.n}*[)]{.sig-paren} [[→]{.sig-return-icon} [[SelectorList]{.pre}[[\[]{.pre}]{.p}[\_SelectorType]{.pre}[[\]]{.pre}]{.p}]{.sig-return-typehint}]{.sig-return}[[[\[source\]]{.pre}]{.viewcode-link}](_modules/parsel/selector.html#SelectorList.css){.reference .internal}[¶](#scrapy.selector.SelectorList.css &quot;Permalink to this definition&quot;){.headerlink}

:   Call the [`.css()`{.docutils .literal .notranslate}]{.pre}
    method for each element in this list and return their results
    flattened as another [[`SelectorList`{.xref .py .py-class
    .docutils .literal
    .notranslate}]{.pre}](#scrapy.selector.SelectorList &quot;scrapy.selector.SelectorList&quot;){.reference
    .internal}.

    [`query`{.docutils .literal .notranslate}]{.pre} is the same
    argument as the one in [[`Selector.css()`{.xref .py .py-meth
    .docutils .literal
    .notranslate}]{.pre}](#scrapy.selector.Selector.css &quot;scrapy.selector.Selector.css&quot;){.reference
    .internal}

[[getall]{.pre}]{.sig-name .descname}[(]{.sig-paren}[)]{.sig-paren} [[→]{.sig-return-icon} [[[List]{.pre}](https://docs.python.org/3/library/typing.html#typing.List &quot;(in Python v3.12)&quot;){.reference .external}[[\[]{.pre}]{.p}[[str]{.pre}](https://docs.python.org/3/library/stdtypes.html#str &quot;(in Python v3.12)&quot;){.reference .external}[[\]]{.pre}]{.p}]{.sig-return-typehint}]{.sig-return}[[[\[source\]]{.pre}]{.viewcode-link}](_modules/parsel/selector.html#SelectorList.getall){.reference .internal}[¶](#scrapy.selector.SelectorList.getall &quot;Permalink to this definition&quot;){.headerlink}

:   Call the [`.get()`{.docutils .literal .notranslate}]{.pre}
    method for each element is this list and return their results
    flattened, as a list of strings.

    See also: [[extract() and extract_first()]{.std
    .std-ref}](#old-extraction-api){.hoverxref .tooltip .reference
    .internal}

[[get]{.pre}]{.sig-name .descname}[(]{.sig-paren}*[[default]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[[[None]{.pre}](https://docs.python.org/3/library/constants.html#None &quot;(in Python v3.12)&quot;){.reference .external}]{.n}[ ]{.w}[[=]{.pre}]{.o}[ ]{.w}[[None]{.pre}]{.default_value}*[)]{.sig-paren} [[→]{.sig-return-icon} [[[Optional]{.pre}](https://docs.python.org/3/library/typing.html#typing.Optional &quot;(in Python v3.12)&quot;){.reference .external}[[\[]{.pre}]{.p}[[str]{.pre}](https://docs.python.org/3/library/stdtypes.html#str &quot;(in Python v3.12)&quot;){.reference .external}[[\]]{.pre}]{.p}]{.sig-return-typehint}]{.sig-return}[[[\[source\]]{.pre}]{.viewcode-link}](_modules/parsel/selector.html#SelectorList.get){.reference .internal}[¶](#scrapy.selector.SelectorList.get &quot;Permalink to this definition&quot;){.headerlink}\
[[get]{.pre}]{.sig-name .descname}[(]{.sig-paren}*[[default]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[[[str]{.pre}](https://docs.python.org/3/library/stdtypes.html#str &quot;(in Python v3.12)&quot;){.reference .external}]{.n}*[)]{.sig-paren} [[→]{.sig-return-icon} [[[str]{.pre}](https://docs.python.org/3/library/stdtypes.html#str &quot;(in Python v3.12)&quot;){.reference .external}]{.sig-return-typehint}]{.sig-return}

:   Return the result of [`.get()`{.docutils .literal
    .notranslate}]{.pre} for the first element in this list. If the
    list is empty, return the default value.

    See also: [[extract() and extract_first()]{.std
    .std-ref}](#old-extraction-api){.hoverxref .tooltip .reference
    .internal}

[[re]{.pre}]{.sig-name .descname}[(]{.sig-paren}*[[regex]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[[[Union]{.pre}](https://docs.python.org/3/library/typing.html#typing.Union &quot;(in Python v3.12)&quot;){.reference .external}[[\[]{.pre}]{.p}[[str]{.pre}](https://docs.python.org/3/library/stdtypes.html#str &quot;(in Python v3.12)&quot;){.reference .external}[[,]{.pre}]{.p}[ ]{.w}[[Pattern]{.pre}](https://docs.python.org/3/library/typing.html#typing.Pattern &quot;(in Python v3.12)&quot;){.reference .external}[[\[]{.pre}]{.p}[[str]{.pre}](https://docs.python.org/3/library/stdtypes.html#str &quot;(in Python v3.12)&quot;){.reference .external}[[\]]{.pre}]{.p}[[\]]{.pre}]{.p}]{.n}*, *[[replace_entities]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[[[bool]{.pre}](https://docs.python.org/3/library/functions.html#bool &quot;(in Python v3.12)&quot;){.reference .external}]{.n}[ ]{.w}[[=]{.pre}]{.o}[ ]{.w}[[True]{.pre}]{.default_value}*[)]{.sig-paren} [[→]{.sig-return-icon} [[[List]{.pre}](https://docs.python.org/3/library/typing.html#typing.List &quot;(in Python v3.12)&quot;){.reference .external}[[\[]{.pre}]{.p}[[str]{.pre}](https://docs.python.org/3/library/stdtypes.html#str &quot;(in Python v3.12)&quot;){.reference .external}[[\]]{.pre}]{.p}]{.sig-return-typehint}]{.sig-return}[[[\[source\]]{.pre}]{.viewcode-link}](_modules/parsel/selector.html#SelectorList.re){.reference .internal}[¶](#scrapy.selector.SelectorList.re &quot;Permalink to this definition&quot;){.headerlink}

:   Call the [`.re()`{.docutils .literal .notranslate}]{.pre} method
    for each element in this list and return their results
    flattened, as a list of strings.

    By default, character entity references are replaced by their
    corresponding character (except for [`&amp;amp;`{.docutils .literal
    .notranslate}]{.pre} and [`&amp;lt;`{.docutils .literal
    .notranslate}]{.pre}. Passing [`replace_entities`{.docutils
    .literal .notranslate}]{.pre} as [`False`{.docutils .literal
    .notranslate}]{.pre} switches off these replacements.

[[re_first]{.pre}]{.sig-name .descname}[(]{.sig-paren}*[[regex]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[[[Union]{.pre}](https://docs.python.org/3/library/typing.html#typing.Union &quot;(in Python v3.12)&quot;){.reference .external}[[\[]{.pre}]{.p}[[str]{.pre}](https://docs.python.org/3/library/stdtypes.html#str &quot;(in Python v3.12)&quot;){.reference .external}[[,]{.pre}]{.p}[ ]{.w}[[Pattern]{.pre}](https://docs.python.org/3/library/typing.html#typing.Pattern &quot;(in Python v3.12)&quot;){.reference .external}[[\[]{.pre}]{.p}[[str]{.pre}](https://docs.python.org/3/library/stdtypes.html#str &quot;(in Python v3.12)&quot;){.reference .external}[[\]]{.pre}]{.p}[[\]]{.pre}]{.p}]{.n}*, *[[default]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[[[None]{.pre}](https://docs.python.org/3/library/constants.html#None &quot;(in Python v3.12)&quot;){.reference .external}]{.n}[ ]{.w}[[=]{.pre}]{.o}[ ]{.w}[[None]{.pre}]{.default_value}*, *[[replace_entities]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[[[bool]{.pre}](https://docs.python.org/3/library/functions.html#bool &quot;(in Python v3.12)&quot;){.reference .external}]{.n}[ ]{.w}[[=]{.pre}]{.o}[ ]{.w}[[True]{.pre}]{.default_value}*[)]{.sig-paren} [[→]{.sig-return-icon} [[[Optional]{.pre}](https://docs.python.org/3/library/typing.html#typing.Optional &quot;(in Python v3.12)&quot;){.reference .external}[[\[]{.pre}]{.p}[[str]{.pre}](https://docs.python.org/3/library/stdtypes.html#str &quot;(in Python v3.12)&quot;){.reference .external}[[\]]{.pre}]{.p}]{.sig-return-typehint}]{.sig-return}[[[\[source\]]{.pre}]{.viewcode-link}](_modules/parsel/selector.html#SelectorList.re_first){.reference .internal}[¶](#scrapy.selector.SelectorList.re_first &quot;Permalink to this definition&quot;){.headerlink}\
[[re_first]{.pre}]{.sig-name .descname}[(]{.sig-paren}*[[regex]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[[[Union]{.pre}](https://docs.python.org/3/library/typing.html#typing.Union &quot;(in Python v3.12)&quot;){.reference .external}[[\[]{.pre}]{.p}[[str]{.pre}](https://docs.python.org/3/library/stdtypes.html#str &quot;(in Python v3.12)&quot;){.reference .external}[[,]{.pre}]{.p}[ ]{.w}[[Pattern]{.pre}](https://docs.python.org/3/library/typing.html#typing.Pattern &quot;(in Python v3.12)&quot;){.reference .external}[[\[]{.pre}]{.p}[[str]{.pre}](https://docs.python.org/3/library/stdtypes.html#str &quot;(in Python v3.12)&quot;){.reference .external}[[\]]{.pre}]{.p}[[\]]{.pre}]{.p}]{.n}*, *[[default]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[[[str]{.pre}](https://docs.python.org/3/library/stdtypes.html#str &quot;(in Python v3.12)&quot;){.reference .external}]{.n}*, *[[replace_entities]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[[[bool]{.pre}](https://docs.python.org/3/library/functions.html#bool &quot;(in Python v3.12)&quot;){.reference .external}]{.n}[ ]{.w}[[=]{.pre}]{.o}[ ]{.w}[[True]{.pre}]{.default_value}*[)]{.sig-paren} [[→]{.sig-return-icon} [[[str]{.pre}](https://docs.python.org/3/library/stdtypes.html#str &quot;(in Python v3.12)&quot;){.reference .external}]{.sig-return-typehint}]{.sig-return}

:   Call the [`.re()`{.docutils .literal .notranslate}]{.pre} method
    for the first element in this list and return the result in an
    string. If the list is empty or the regex doesn't match
    anything, return the default value ([`None`{.docutils .literal
    .notranslate}]{.pre} if the argument is not provided).

    By default, character entity references are replaced by their
    corresponding character (except for [`&amp;amp;`{.docutils .literal
    .notranslate}]{.pre} and [`&amp;lt;`{.docutils .literal
    .notranslate}]{.pre}. Passing [`replace_entities`{.docutils
    .literal .notranslate}]{.pre} as [`False`{.docutils .literal
    .notranslate}]{.pre} switches off these replacements.

[[attrib]{.pre}]{.sig-name .descname}[¶](#scrapy.selector.SelectorList.attrib &quot;Permalink to this definition&quot;){.headerlink}

:   Return the attributes dictionary for the first element. If the
    list is empty, return an empty dict.

    See also: [[Selecting element attributes]{.std
    .std-ref}](#selecting-attributes){.hoverxref .tooltip .reference
    .internal}.
</code></pre>
<p>:::
:::</p>
<p>::: {#examples .section}
[]{#selector-examples}</p>
<h4 id="examplesheaderlink-1"><a class="header" href="#examplesheaderlink-1">Examples<a href="#examples" title="Permalink to this heading">¶</a>{.headerlink}</a></h4>
<p>::: {#selector-examples-on-html-response .section}
[]{#selector-examples-html}</p>
<h5 id="selector-examples-on-html-responseheaderlink"><a class="header" href="#selector-examples-on-html-responseheaderlink">Selector examples on HTML response<a href="#selector-examples-on-html-response" title="Permalink to this heading">¶</a>{.headerlink}</a></h5>
<p>Here are some <a href="#scrapy.selector.Selector" title="scrapy.selector.Selector">[<code>Selector</code>{.xref .py .py-class .docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal} examples to illustrate several concepts. In all cases, we
assume there is already a <a href="#scrapy.selector.Selector" title="scrapy.selector.Selector">[<code>Selector</code>{.xref .py .py-class .docutils
.literal
.notranslate}]{.pre}</a>{.reference
.internal} instantiated with a <a href="index.html#scrapy.http.HtmlResponse" title="scrapy.http.HtmlResponse">[<code>HtmlResponse</code>{.xref .py .py-class
.docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal} object like this:</p>
<p>::: {.highlight-python .notranslate}
::: highlight
sel = Selector(html_response)
:::
:::</p>
<ol>
<li>
<p>Select all [<code>&lt;h1&gt;</code>{.docutils .literal .notranslate}]{.pre} elements
from an HTML response body, returning a list of <a href="#scrapy.selector.Selector" title="scrapy.selector.Selector">[<code>Selector</code>{.xref
.py .py-class .docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal} objects (i.e. a <a href="#scrapy.selector.SelectorList" title="scrapy.selector.SelectorList">[<code>SelectorList</code>{.xref .py .py-class
.docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal} object):</p>
<p>::: {.highlight-python .notranslate}
::: highlight
sel.xpath(&quot;//h1&quot;)
:::
:::</p>
</li>
<li>
<p>Extract the text of all [<code>&lt;h1&gt;</code>{.docutils .literal
.notranslate}]{.pre} elements from an HTML response body, returning
a list of strings:</p>
<p>::: {.highlight-python .notranslate}
::: highlight
sel.xpath(&quot;//h1&quot;).getall()  # this includes the h1 tag
sel.xpath(&quot;//h1/text()&quot;).getall()  # this excludes the h1 tag
:::
:::</p>
</li>
<li>
<p>Iterate over all [<code>&lt;p&gt;</code>{.docutils .literal .notranslate}]{.pre} tags
and print their class attribute:</p>
<p>::: {.highlight-python .notranslate}
::: highlight
for node in sel.xpath(&quot;//p&quot;):
print(node.attrib[&quot;class&quot;])
:::
:::
:::</p>
</li>
</ol>
<p>::: {#selector-examples-on-xml-response .section}
[]{#selector-examples-xml}</p>
<h5 id="selector-examples-on-xml-responseheaderlink"><a class="header" href="#selector-examples-on-xml-responseheaderlink">Selector examples on XML response<a href="#selector-examples-on-xml-response" title="Permalink to this heading">¶</a>{.headerlink}</a></h5>
<p>Here are some examples to illustrate concepts for <a href="#scrapy.selector.Selector" title="scrapy.selector.Selector">[<code>Selector</code>{.xref .py
.py-class .docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal} objects instantiated with an <a href="index.html#scrapy.http.XmlResponse" title="scrapy.http.XmlResponse">[<code>XmlResponse</code>{.xref .py
.py-class .docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal} object:</p>
<p>::: {.highlight-python .notranslate}
::: highlight
sel = Selector(xml_response)
:::
:::</p>
<ol>
<li>
<p>Select all [<code>&lt;product&gt;</code>{.docutils .literal .notranslate}]{.pre}
elements from an XML response body, returning a list of
<a href="#scrapy.selector.Selector" title="scrapy.selector.Selector">[<code>Selector</code>{.xref .py .py-class .docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal} objects (i.e. a <a href="#scrapy.selector.SelectorList" title="scrapy.selector.SelectorList">[<code>SelectorList</code>{.xref .py .py-class
.docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal} object):</p>
<p>::: {.highlight-python .notranslate}
::: highlight
sel.xpath(&quot;//product&quot;)
:::
:::</p>
</li>
<li>
<p>Extract all prices from a <a href="https://support.google.com/merchants/answer/160589?hl=en&amp;ref_topic=2473799">Google Base XML
feed</a>{.reference
.external} which requires registering a namespace:</p>
<p>::: {.highlight-python .notranslate}
::: highlight
sel.register_namespace(&quot;g&quot;, &quot;http://base.google.com/ns/1.0&quot;)
sel.xpath(&quot;//g:price&quot;).getall()
:::
:::
:::
:::
:::</p>
</li>
</ol>
<p>[]{#document-topics/items}</p>
<p>::: {#module-scrapy.item .section}
[]{#items}[]{#topics-items}</p>
<h3 id="itemsheaderlink"><a class="header" href="#itemsheaderlink">Items<a href="#module-scrapy.item" title="Permalink to this heading">¶</a>{.headerlink}</a></h3>
<p>The main goal in scraping is to extract structured data from
unstructured sources, typically, web pages. <a href="index.html#topics-spiders">[Spiders]{.std
.std-ref}</a>{.hoverxref .tooltip .reference
.internal} may return the extracted data as items, Python objects that
define key-value pairs.</p>
<p>Scrapy supports <a href="#item-types">[multiple types of items]{.std
.std-ref}</a>{.hoverxref .tooltip .reference .internal}. When
you create an item, you may use whichever type of item you want. When
you write code that receives an item, your code should <a href="#supporting-item-types">[work for any
item type]{.std .std-ref}</a>{.hoverxref .tooltip
.reference .internal}.</p>
<p>::: {#item-types .section}
[]{#id1}</p>
<h4 id="item-typesheaderlink"><a class="header" href="#item-typesheaderlink">Item Types<a href="#item-types" title="Permalink to this heading">¶</a>{.headerlink}</a></h4>
<p>Scrapy supports the following types of items, via the
<a href="https://github.com/scrapy/itemadapter">itemadapter</a>{.reference
.external} library: <a href="#dict-items">[dictionaries]{.std
.std-ref}</a>{.hoverxref .tooltip .reference .internal},
<a href="#item-objects">[Item objects]{.std .std-ref}</a>{.hoverxref .tooltip
.reference .internal}, <a href="#dataclass-items">[dataclass objects]{.std
.std-ref}</a>{.hoverxref .tooltip .reference .internal},
and <a href="#attrs-items">[attrs objects]{.std .std-ref}</a>{.hoverxref .tooltip
.reference .internal}.</p>
<p>::: {#dictionaries .section}
[]{#dict-items}</p>
<h5 id="dictionariesheaderlink"><a class="header" href="#dictionariesheaderlink">Dictionaries<a href="#dictionaries" title="Permalink to this heading">¶</a>{.headerlink}</a></h5>
<p>As an item type, <a href="https://docs.python.org/3/library/stdtypes.html#dict" title="(in Python v3.12)">[<code>dict</code>{.xref .py .py-class .docutils .literal
.notranslate}]{.pre}</a>{.reference
.external} is convenient and familiar.
:::</p>
<p>::: {#item-objects .section}
[]{#id2}</p>
<h5 id="item-objectsheaderlink"><a class="header" href="#item-objectsheaderlink">Item objects<a href="#item-objects" title="Permalink to this heading">¶</a>{.headerlink}</a></h5>
<p>[<code>Item</code>{.xref .py .py-class .docutils .literal .notranslate}]{.pre}
provides a <a href="https://docs.python.org/3/library/stdtypes.html#dict" title="(in Python v3.12)">[<code>dict</code>{.xref .py .py-class .docutils .literal
.notranslate}]{.pre}</a>{.reference
.external}-like API plus additional features that make it the most
feature-complete item type:</p>
<p><em>[class]{.pre}[ ]{.w}</em>[[scrapy.item.]{.pre}]{.sig-prename .descclassname}[[Item]{.pre}]{.sig-name .descname}[(]{.sig-paren}[[]{.optional}<em>[[arg]{.pre}]{.n}</em>[]]{.optional}[)]{.sig-paren}<a href="#scrapy.item.scrapy.item.Item" title="Permalink to this definition">¶</a>{.headerlink}</p>
<p>:</p>
<pre><code class="language-{=html}">&lt;!-- --&gt;
</code></pre>
<p><em>[class]{.pre}[ ]{.w}</em>[[scrapy.]{.pre}]{.sig-prename .descclassname}[[Item]{.pre}]{.sig-name .descname}[(]{.sig-paren}[[]{.optional}<em>[[arg]{.pre}]{.n}</em>[]]{.optional}[)]{.sig-paren}<a href="#scrapy.item.scrapy.Item" title="Permalink to this definition">¶</a>{.headerlink}</p>
<p>:   [<code>Item</code>{.xref .py .py-class .docutils .literal .notranslate}]{.pre}
objects replicate the standard <a href="https://docs.python.org/3/library/stdtypes.html#dict" title="(in Python v3.12)">[<code>dict</code>{.xref .py .py-class
.docutils .literal
.notranslate}]{.pre}</a>{.reference
.external} API, including its [<code>__init__</code>{.docutils .literal
.notranslate}]{.pre} method.</p>
<pre><code>[`Item`{.xref .py .py-class .docutils .literal .notranslate}]{.pre}
allows defining field names, so that:

-   [[`KeyError`{.xref .py .py-class .docutils .literal
    .notranslate}]{.pre}](https://docs.python.org/3/library/exceptions.html#KeyError &quot;(in Python v3.12)&quot;){.reference
    .external} is raised when using undefined field names (i.e.
    prevents typos going unnoticed)

-   [[Item exporters]{.std
    .std-ref}](index.html#topics-exporters){.hoverxref .tooltip
    .reference .internal} can export all fields by default even if
    the first scraped object does not have values for all of them

[`Item`{.xref .py .py-class .docutils .literal .notranslate}]{.pre}
also allows defining field metadata, which can be used to
[[customize serialization]{.std
.std-ref}](index.html#topics-exporters-field-serialization){.hoverxref
.tooltip .reference .internal}.

[`trackref`{.xref .py .py-mod .docutils .literal
.notranslate}]{.pre} tracks [`Item`{.xref .py .py-class .docutils
.literal .notranslate}]{.pre} objects to help find memory leaks (see
[[Debugging memory leaks with trackref]{.std
.std-ref}](index.html#topics-leaks-trackrefs){.hoverxref .tooltip
.reference .internal}).

[`Item`{.xref .py .py-class .docutils .literal .notranslate}]{.pre}
objects also provide the following additional API members:

[[Item.]{.pre}]{.sig-prename .descclassname}[[copy]{.pre}]{.sig-name .descname}[(]{.sig-paren}[)]{.sig-paren}[¶](#scrapy.scrapy.Item.Item.copy &quot;Permalink to this definition&quot;){.headerlink}

:   

[[Item.]{.pre}]{.sig-prename .descclassname}[[deepcopy]{.pre}]{.sig-name .descname}[(]{.sig-paren}[)]{.sig-paren}[¶](#scrapy.scrapy.Item.Item.deepcopy &quot;Permalink to this definition&quot;){.headerlink}

:   Return a [[`deepcopy()`{.xref .py .py-func .docutils .literal
    .notranslate}]{.pre}](https://docs.python.org/3/library/copy.html#copy.deepcopy &quot;(in Python v3.12)&quot;){.reference
    .external} of this item.

[[fields]{.pre}]{.sig-name .descname}[¶](#scrapy.item.scrapy.Item.fields &quot;Permalink to this definition&quot;){.headerlink}

:   A dictionary containing *all declared fields* for this Item, not
    only those populated. The keys are the field names and the
    values are the [`Field`{.xref .py .py-class .docutils .literal
    .notranslate}]{.pre} objects used in the [[Item
    declaration]{.std .std-ref}](#topics-items-declaring){.hoverxref
    .tooltip .reference .internal}.
</code></pre>
<p>Example:</p>
<p>::: {.highlight-python .notranslate}
::: highlight
from scrapy.item import Item, Field</p>
<pre><code>class CustomItem(Item):
    one_field = Field()
    another_field = Field()
</code></pre>
<p>:::
:::
:::</p>
<p>::: {#dataclass-objects .section}
[]{#dataclass-items}</p>
<h5 id="dataclass-objectsheaderlink"><a class="header" href="#dataclass-objectsheaderlink">Dataclass objects<a href="#dataclass-objects" title="Permalink to this heading">¶</a>{.headerlink}</a></h5>
<p>::: versionadded
[New in version 2.2.]{.versionmodified .added}
:::</p>
<p><a href="https://docs.python.org/3/library/dataclasses.html#dataclasses.dataclass" title="(in Python v3.12)">[<code>dataclass()</code>{.xref .py .py-func .docutils .literal
.notranslate}]{.pre}</a>{.reference
.external} allows defining item classes with field names, so that <a href="index.html#topics-exporters">[item
exporters]{.std .std-ref}</a>{.hoverxref
.tooltip .reference .internal} can export all fields by default even if
the first scraped object does not have values for all of them.</p>
<p>Additionally, [<code>dataclass</code>{.docutils .literal .notranslate}]{.pre} items
also allow to:</p>
<ul>
<li>
<p>define the type and default value of each defined field.</p>
</li>
<li>
<p>define custom field metadata through <a href="https://docs.python.org/3/library/dataclasses.html#dataclasses.field" title="(in Python v3.12)">[<code>dataclasses.field()</code>{.xref
.py .py-func .docutils .literal
.notranslate}]{.pre}</a>{.reference
.external}, which can be used to <a href="index.html#topics-exporters-field-serialization">[customize serialization]{.std
.std-ref}</a>{.hoverxref
.tooltip .reference .internal}.</p>
</li>
</ul>
<p>Example:</p>
<p>::: {.highlight-python .notranslate}
::: highlight
from dataclasses import dataclass</p>
<pre><code>@dataclass
class CustomItem:
    one_field: str
    another_field: int
</code></pre>
<p>:::
:::</p>
<p>::: {.admonition .note}
Note</p>
<p>Field types are not enforced at run time.
:::
:::</p>
<p>::: {#attr-s-objects .section}
[]{#attrs-items}</p>
<h5 id="attrs-objectsheaderlink"><a class="header" href="#attrs-objectsheaderlink">attr.s objects<a href="#attr-s-objects" title="Permalink to this heading">¶</a>{.headerlink}</a></h5>
<p>::: versionadded
[New in version 2.2.]{.versionmodified .added}
:::</p>
<p><a href="https://www.attrs.org/en/stable/api-attr.html#attr.s" title="(in attrs v23.1)">[<code>attr.s()</code>{.xref .py .py-func .docutils .literal
.notranslate}]{.pre}</a>{.reference
.external} allows defining item classes with field names, so that <a href="index.html#topics-exporters">[item
exporters]{.std .std-ref}</a>{.hoverxref
.tooltip .reference .internal} can export all fields by default even if
the first scraped object does not have values for all of them.</p>
<p>Additionally, [<code>attr.s</code>{.docutils .literal .notranslate}]{.pre} items
also allow to:</p>
<ul>
<li>
<p>define the type and default value of each defined field.</p>
</li>
<li>
<p>define custom field <a href="https://www.attrs.org/en/stable/examples.html#metadata" title="(in attrs v23.1)">[metadata]{.xref .std
.std-ref}</a>{.reference
.external}, which can be used to <a href="index.html#topics-exporters-field-serialization">[customize serialization]{.std
.std-ref}</a>{.hoverxref
.tooltip .reference .internal}.</p>
</li>
</ul>
<p>In order to use this type, the <a href="https://www.attrs.org/en/stable/index.html" title="(in attrs v23.1)">[attrs package]{.xref .std
.std-doc}</a>{.reference
.external} needs to be installed.</p>
<p>Example:</p>
<p>::: {.highlight-python .notranslate}
::: highlight
import attr</p>
<pre><code>@attr.s
class CustomItem:
    one_field = attr.ib()
    another_field = attr.ib()
</code></pre>
<p>:::
:::
:::
:::</p>
<p>::: {#working-with-item-objects .section}</p>
<h4 id="working-with-item-objectsheaderlink"><a class="header" href="#working-with-item-objectsheaderlink">Working with Item objects<a href="#working-with-item-objects" title="Permalink to this heading">¶</a>{.headerlink}</a></h4>
<p>::: {#declaring-item-subclasses .section}
[]{#topics-items-declaring}</p>
<h5 id="declaring-item-subclassesheaderlink"><a class="header" href="#declaring-item-subclassesheaderlink">Declaring Item subclasses<a href="#declaring-item-subclasses" title="Permalink to this heading">¶</a>{.headerlink}</a></h5>
<p>Item subclasses are declared using a simple class definition syntax and
[<code>Field</code>{.xref .py .py-class .docutils .literal .notranslate}]{.pre}
objects. Here is an example:</p>
<p>::: {.highlight-python .notranslate}
::: highlight
import scrapy</p>
<pre><code>class Product(scrapy.Item):
    name = scrapy.Field()
    price = scrapy.Field()
    stock = scrapy.Field()
    tags = scrapy.Field()
    last_updated = scrapy.Field(serializer=str)
</code></pre>
<p>:::
:::</p>
<p>::: {.admonition .note}
Note</p>
<p>Those familiar with <a href="https://www.djangoproject.com/">Django</a>{.reference
.external} will notice that Scrapy Items are declared similar to <a href="https://docs.djangoproject.com/en/dev/topics/db/models/">Django
Models</a>{.reference
.external}, except that Scrapy Items are much simpler as there is no
concept of different field types.
:::
:::</p>
<p>::: {#declaring-fields .section}
[]{#topics-items-fields}</p>
<h5 id="declaring-fieldsheaderlink"><a class="header" href="#declaring-fieldsheaderlink">Declaring fields<a href="#declaring-fields" title="Permalink to this heading">¶</a>{.headerlink}</a></h5>
<p>[<code>Field</code>{.xref .py .py-class .docutils .literal .notranslate}]{.pre}
objects are used to specify metadata for each field. For example, the
serializer function for the [<code>last_updated</code>{.docutils .literal
.notranslate}]{.pre} field illustrated in the example above.</p>
<p>You can specify any kind of metadata for each field. There is no
restriction on the values accepted by [<code>Field</code>{.xref .py .py-class
.docutils .literal .notranslate}]{.pre} objects. For this same reason,
there is no reference list of all available metadata keys. Each key
defined in [<code>Field</code>{.xref .py .py-class .docutils .literal
.notranslate}]{.pre} objects could be used by a different component, and
only those components know about it. You can also define and use any
other [<code>Field</code>{.xref .py .py-class .docutils .literal
.notranslate}]{.pre} key in your project too, for your own needs. The
main goal of [<code>Field</code>{.xref .py .py-class .docutils .literal
.notranslate}]{.pre} objects is to provide a way to define all field
metadata in one place. Typically, those components whose behaviour
depends on each field use certain field keys to configure that
behaviour. You must refer to their documentation to see which metadata
keys are used by each component.</p>
<p>It's important to note that the [<code>Field</code>{.xref .py .py-class .docutils
.literal .notranslate}]{.pre} objects used to declare the item do not
stay assigned as class attributes. Instead, they can be accessed through
the [<code>Item.fields</code>{.xref .py .py-attr .docutils .literal
.notranslate}]{.pre} attribute.</p>
<p><em>[class]{.pre}[ ]{.w}</em>[[scrapy.item.]{.pre}]{.sig-prename .descclassname}[[Field]{.pre}]{.sig-name .descname}[(]{.sig-paren}[[]{.optional}<em>[[arg]{.pre}]{.n}</em>[]]{.optional}[)]{.sig-paren}<a href="#scrapy.item.scrapy.item.Field" title="Permalink to this definition">¶</a>{.headerlink}</p>
<p>:</p>
<pre><code class="language-{=html}">&lt;!-- --&gt;
</code></pre>
<p><em>[class]{.pre}[ ]{.w}</em>[[scrapy.]{.pre}]{.sig-prename .descclassname}[[Field]{.pre}]{.sig-name .descname}[(]{.sig-paren}[[]{.optional}<em>[[arg]{.pre}]{.n}</em>[]]{.optional}[)]{.sig-paren}<a href="#scrapy.item.scrapy.Field" title="Permalink to this definition">¶</a>{.headerlink}</p>
<p>:   The [<code>Field</code>{.xref .py .py-class .docutils .literal
.notranslate}]{.pre} class is just an alias to the built-in
<a href="https://docs.python.org/3/library/stdtypes.html#dict" title="(in Python v3.12)">[<code>dict</code>{.xref .py .py-class .docutils .literal
.notranslate}]{.pre}</a>{.reference
.external} class and doesn't provide any extra functionality or
attributes. In other words, [<code>Field</code>{.xref .py .py-class .docutils
.literal .notranslate}]{.pre} objects are plain-old Python dicts. A
separate class is used to support the <a href="#topics-items-declaring">[item declaration
syntax]{.std .std-ref}</a>{.hoverxref .tooltip
.reference .internal} based on class attributes.</p>
<p>::: {.admonition .note}
Note</p>
<p>Field metadata can also be declared for [<code>dataclass</code>{.docutils .literal
.notranslate}]{.pre} and [<code>attrs</code>{.docutils .literal
.notranslate}]{.pre} items. Please refer to the documentation for
<a href="https://docs.python.org/3/library/dataclasses.html#dataclasses.field">dataclasses.field</a>{.reference
.external} and
<a href="https://www.attrs.org/en/stable/api.html#attr.ib">attr.ib</a>{.reference
.external} for additional information.
:::
:::</p>
<p>::: {#id3 .section}</p>
<h5 id="working-with-item-objectsheaderlink-1"><a class="header" href="#working-with-item-objectsheaderlink-1">Working with Item objects<a href="#id3" title="Permalink to this heading">¶</a>{.headerlink}</a></h5>
<p>Here are some examples of common tasks performed with items, using the
[<code>Product</code>{.docutils .literal .notranslate}]{.pre} item <a href="#topics-items-declaring">[declared
above]{.std .std-ref}</a>{.hoverxref .tooltip
.reference .internal}. You will notice the API is very similar to the
<a href="https://docs.python.org/3/library/stdtypes.html#dict" title="(in Python v3.12)">[<code>dict</code>{.xref .py .py-class .docutils .literal
.notranslate}]{.pre}</a>{.reference
.external} API.</p>
<p>::: {#creating-items .section}</p>
<h6 id="creating-itemsheaderlink"><a class="header" href="#creating-itemsheaderlink">Creating items<a href="#creating-items" title="Permalink to this heading">¶</a>{.headerlink}</a></h6>
<p>::: {.highlight-pycon .notranslate}
::: highlight
&gt;&gt;&gt; product = Product(name=&quot;Desktop PC&quot;, price=1000)
&gt;&gt;&gt; print(product)
Product(name='Desktop PC', price=1000)
:::
:::
:::</p>
<p>::: {#getting-field-values .section}</p>
<h6 id="getting-field-valuesheaderlink"><a class="header" href="#getting-field-valuesheaderlink">Getting field values<a href="#getting-field-values" title="Permalink to this heading">¶</a>{.headerlink}</a></h6>
<p>::: {.highlight-pycon .notranslate}
::: highlight
&gt;&gt;&gt; product[&quot;name&quot;]
Desktop PC
&gt;&gt;&gt; product.get(&quot;name&quot;)
Desktop PC</p>
<pre><code>&gt;&gt;&gt; product[&quot;price&quot;]
1000

&gt;&gt;&gt; product[&quot;last_updated&quot;]
Traceback (most recent call last):
    ...
KeyError: 'last_updated'

&gt;&gt;&gt; product.get(&quot;last_updated&quot;, &quot;not set&quot;)
not set

&gt;&gt;&gt; product[&quot;lala&quot;]  # getting unknown field
Traceback (most recent call last):
    ...
KeyError: 'lala'

&gt;&gt;&gt; product.get(&quot;lala&quot;, &quot;unknown field&quot;)
'unknown field'

&gt;&gt;&gt; &quot;name&quot; in product  # is name field populated?
True

&gt;&gt;&gt; &quot;last_updated&quot; in product  # is last_updated populated?
False

&gt;&gt;&gt; &quot;last_updated&quot; in product.fields  # is last_updated a declared field?
True

&gt;&gt;&gt; &quot;lala&quot; in product.fields  # is lala a declared field?
False
</code></pre>
<p>:::
:::
:::</p>
<p>::: {#setting-field-values .section}</p>
<h6 id="setting-field-valuesheaderlink"><a class="header" href="#setting-field-valuesheaderlink">Setting field values<a href="#setting-field-values" title="Permalink to this heading">¶</a>{.headerlink}</a></h6>
<p>::: {.highlight-pycon .notranslate}
::: highlight
&gt;&gt;&gt; product[&quot;last_updated&quot;] = &quot;today&quot;
&gt;&gt;&gt; product[&quot;last_updated&quot;]
today</p>
<pre><code>&gt;&gt;&gt; product[&quot;lala&quot;] = &quot;test&quot;  # setting unknown field
Traceback (most recent call last):
    ...
KeyError: 'Product does not support field: lala'
</code></pre>
<p>:::
:::
:::</p>
<p>::: {#accessing-all-populated-values .section}</p>
<h6 id="accessing-all-populated-valuesheaderlink"><a class="header" href="#accessing-all-populated-valuesheaderlink">Accessing all populated values<a href="#accessing-all-populated-values" title="Permalink to this heading">¶</a>{.headerlink}</a></h6>
<p>To access all populated values, just use the typical <a href="https://docs.python.org/3/library/stdtypes.html#dict" title="(in Python v3.12)">[<code>dict</code>{.xref .py
.py-class .docutils .literal
.notranslate}]{.pre}</a>{.reference
.external} API:</p>
<p>::: {.highlight-pycon .notranslate}
::: highlight
&gt;&gt;&gt; product.keys()
['price', 'name']</p>
<pre><code>&gt;&gt;&gt; product.items()
[('price', 1000), ('name', 'Desktop PC')]
</code></pre>
<p>:::
:::
:::</p>
<p>::: {#copying-items .section}
[]{#id4}</p>
<h6 id="copying-itemsheaderlink"><a class="header" href="#copying-itemsheaderlink">Copying items<a href="#copying-items" title="Permalink to this heading">¶</a>{.headerlink}</a></h6>
<p>To copy an item, you must first decide whether you want a shallow copy
or a deep copy.</p>
<p>If your item contains <a href="https://docs.python.org/3/glossary.html#term-mutable" title="(in Python v3.12)">[mutable]{.xref .std
.std-term}</a>{.reference
.external} values like lists or dictionaries, a shallow copy will keep
references to the same mutable values across all different copies.</p>
<p>For example, if you have an item with a list of tags, and you create a
shallow copy of that item, both the original item and the copy have the
same list of tags. Adding a tag to the list of one of the items will add
the tag to the other item as well.</p>
<p>If that is not the desired behavior, use a deep copy instead.</p>
<p>See <a href="https://docs.python.org/3/library/copy.html#module-copy" title="(in Python v3.12)">[<code>copy</code>{.xref .py .py-mod .docutils .literal
.notranslate}]{.pre}</a>{.reference
.external} for more information.</p>
<p>To create a shallow copy of an item, you can either call [<code>copy()</code>{.xref
.py .py-meth .docutils .literal .notranslate}]{.pre} on an existing item
([<code>product2</code>{.docutils .literal .notranslate}]{.pre}<code> </code>{.docutils
.literal .notranslate}[<code>=</code>{.docutils .literal
.notranslate}]{.pre}<code> </code>{.docutils .literal
.notranslate}[<code>product.copy()</code>{.docutils .literal .notranslate}]{.pre})
or instantiate your item class from an existing item
([<code>product2</code>{.docutils .literal .notranslate}]{.pre}<code> </code>{.docutils
.literal .notranslate}[<code>=</code>{.docutils .literal
.notranslate}]{.pre}<code> </code>{.docutils .literal
.notranslate}[<code>Product(product)</code>{.docutils .literal
.notranslate}]{.pre}).</p>
<p>To create a deep copy, call [<code>deepcopy()</code>{.xref .py .py-meth .docutils
.literal .notranslate}]{.pre} instead ([<code>product2</code>{.docutils .literal
.notranslate}]{.pre}<code> </code>{.docutils .literal .notranslate}[<code>=</code>{.docutils
.literal .notranslate}]{.pre}<code> </code>{.docutils .literal
.notranslate}[<code>product.deepcopy()</code>{.docutils .literal
.notranslate}]{.pre}).
:::</p>
<p>::: {#other-common-tasks .section}</p>
<h6 id="other-common-tasksheaderlink"><a class="header" href="#other-common-tasksheaderlink">Other common tasks<a href="#other-common-tasks" title="Permalink to this heading">¶</a>{.headerlink}</a></h6>
<p>Creating dicts from items:</p>
<p>::: {.highlight-pycon .notranslate}
::: highlight
&gt;&gt;&gt; dict(product)  # create a dict from all populated values
{'price': 1000, 'name': 'Desktop PC'}</p>
<pre><code>Creating items from dicts:

&gt;&gt;&gt; Product({&quot;name&quot;: &quot;Laptop PC&quot;, &quot;price&quot;: 1500})
Product(price=1500, name='Laptop PC')

&gt;&gt;&gt; Product({&quot;name&quot;: &quot;Laptop PC&quot;, &quot;lala&quot;: 1500})  # warning: unknown field in dict
Traceback (most recent call last):
    ...
KeyError: 'Product does not support field: lala'
</code></pre>
<p>:::
:::
:::
:::</p>
<p>::: {#extending-item-subclasses .section}</p>
<h5 id="extending-item-subclassesheaderlink"><a class="header" href="#extending-item-subclassesheaderlink">Extending Item subclasses<a href="#extending-item-subclasses" title="Permalink to this heading">¶</a>{.headerlink}</a></h5>
<p>You can extend Items (to add more fields or to change some metadata for
some fields) by declaring a subclass of your original Item.</p>
<p>For example:</p>
<p>::: {.highlight-python .notranslate}
::: highlight
class DiscountedProduct(Product):
discount_percent = scrapy.Field(serializer=str)
discount_expiration_date = scrapy.Field()
:::
:::</p>
<p>You can also extend field metadata by using the previous field metadata
and appending more values, or changing existing values, like this:</p>
<p>::: {.highlight-python .notranslate}
::: highlight
class SpecificProduct(Product):
name = scrapy.Field(Product.fields[&quot;name&quot;], serializer=my_serializer)
:::
:::</p>
<p>That adds (or replaces) the [<code>serializer</code>{.docutils .literal
.notranslate}]{.pre} metadata key for the [<code>name</code>{.docutils .literal
.notranslate}]{.pre} field, keeping all the previously existing metadata
values.
:::
:::</p>
<p>::: {#supporting-all-item-types .section}
[]{#supporting-item-types}</p>
<h4 id="supporting-all-item-typesheaderlink"><a class="header" href="#supporting-all-item-typesheaderlink">Supporting All Item Types<a href="#supporting-all-item-types" title="Permalink to this heading">¶</a>{.headerlink}</a></h4>
<p>In code that receives an item, such as methods of <a href="index.html#topics-item-pipeline">[item pipelines]{.std
.std-ref}</a>{.hoverxref .tooltip
.reference .internal} or <a href="index.html#topics-spider-middleware">[spider middlewares]{.std
.std-ref}</a>{.hoverxref .tooltip
.reference .internal}, it is a good practice to use the
<a href="#itemadapter.ItemAdapter" title="itemadapter.ItemAdapter">[<code>ItemAdapter</code>{.xref .py .py-class .docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal} class and the <a href="#itemadapter.is_item" title="itemadapter.is_item">[<code>is_item()</code>{.xref .py .py-func .docutils
.literal
.notranslate}]{.pre}</a>{.reference
.internal} function to write code that works for any <a href="#item-types">[supported item
type]{.std .std-ref}</a>{.hoverxref .tooltip .reference
.internal}:</p>
<p><em>[class]{.pre}[ ]{.w}</em>[[itemadapter.]{.pre}]{.sig-prename .descclassname}[[ItemAdapter]{.pre}]{.sig-name .descname}[(]{.sig-paren}<em>[[item]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[<a href="https://docs.python.org/3/library/typing.html#typing.Any" title="(in Python v3.12)">[Any]{.pre}</a>{.reference .external}]{.n}</em>[)]{.sig-paren}<a href="_modules/itemadapter/adapter.html#ItemAdapter">[[[source]]{.pre}]{.viewcode-link}</a>{.reference .internal}<a href="#itemadapter.ItemAdapter" title="Permalink to this definition">¶</a>{.headerlink}</p>
<p>:   Wrapper class to interact with data container objects. It provides a
common interface to extract and set data without having to take the
object's type into account.</p>
<pre><code class="language-{=html}">&lt;!-- --&gt;
</code></pre>
<p>[[itemadapter.]{.pre}]{.sig-prename .descclassname}[[is_item]{.pre}]{.sig-name .descname}[(]{.sig-paren}<em>[[obj]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[<a href="https://docs.python.org/3/library/typing.html#typing.Any" title="(in Python v3.12)">[Any]{.pre}</a>{.reference .external}]{.n}</em>[)]{.sig-paren} [[→]{.sig-return-icon} [<a href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.12)">[bool]{.pre}</a>{.reference .external}]{.sig-return-typehint}]{.sig-return}<a href="_modules/itemadapter/utils.html#is_item">[[[source]]{.pre}]{.viewcode-link}</a>{.reference .internal}<a href="#itemadapter.is_item" title="Permalink to this definition">¶</a>{.headerlink}</p>
<p>:   Return True if the given object belongs to one of the supported
types, False otherwise.</p>
<pre><code>Alias for ItemAdapter.is_item
</code></pre>
<p>:::</p>
<p>::: {#other-classes-related-to-items .section}</p>
<h4 id="other-classes-related-to-itemsheaderlink"><a class="header" href="#other-classes-related-to-itemsheaderlink">Other classes related to items<a href="#other-classes-related-to-items" title="Permalink to this heading">¶</a>{.headerlink}</a></h4>
<p><em>[class]{.pre}[ ]{.w}</em>[[scrapy.item.]{.pre}]{.sig-prename .descclassname}[[ItemMeta]{.pre}]{.sig-name .descname}[(]{.sig-paren}<em>[[class_name]{.pre}]{.n}</em>, <em>[[bases]{.pre}]{.n}</em>, <em>[[attrs]{.pre}]{.n}</em>[)]{.sig-paren}<a href="_modules/scrapy/item.html#ItemMeta">[[[source]]{.pre}]{.viewcode-link}</a>{.reference .internal}<a href="#scrapy.item.ItemMeta" title="Permalink to this definition">¶</a>{.headerlink}</p>
<p>:   <a href="https://realpython.com/python-metaclasses">Metaclass</a>{.reference
.external} of [<code>Item</code>{.xref .py .py-class .docutils .literal
.notranslate}]{.pre} that handles field definitions.
:::
:::</p>
<p>[]{#document-topics/loaders}</p>
<p>::: {#module-scrapy.loader .section}
[]{#item-loaders}[]{#topics-loaders}</p>
<h3 id="item-loadersheaderlink"><a class="header" href="#item-loadersheaderlink">Item Loaders<a href="#module-scrapy.loader" title="Permalink to this heading">¶</a>{.headerlink}</a></h3>
<p>Item Loaders provide a convenient mechanism for populating scraped
<a href="index.html#topics-items">[items]{.std .std-ref}</a>{.hoverxref .tooltip
.reference .internal}. Even though items can be populated directly, Item
Loaders provide a much more convenient API for populating them from a
scraping process, by automating some common tasks like parsing the raw
extracted data before assigning it.</p>
<p>In other words, <a href="index.html#topics-items">[items]{.std
.std-ref}</a>{.hoverxref .tooltip .reference
.internal} provide the <em>container</em> of scraped data, while Item Loaders
provide the mechanism for <em>populating</em> that container.</p>
<p>Item Loaders are designed to provide a flexible, efficient and easy
mechanism for extending and overriding different field parsing rules,
either by spider, or by source format (HTML, XML, etc) without becoming
a nightmare to maintain.</p>
<p>::: {.admonition .note}
Note</p>
<p>Item Loaders are an extension of the
<a href="https://itemloaders.readthedocs.io/en/latest/">itemloaders</a>{.reference
.external} library that make it easier to work with Scrapy by adding
support for <a href="index.html#topics-request-response">[responses]{.std
.std-ref}</a>{.hoverxref .tooltip
.reference .internal}.
:::</p>
<p>::: {#using-item-loaders-to-populate-items .section}</p>
<h4 id="using-item-loaders-to-populate-itemsheaderlink"><a class="header" href="#using-item-loaders-to-populate-itemsheaderlink">Using Item Loaders to populate items<a href="#using-item-loaders-to-populate-items" title="Permalink to this heading">¶</a>{.headerlink}</a></h4>
<p>To use an Item Loader, you must first instantiate it. You can either
instantiate it with an <a href="index.html#topics-items">[item object]{.std
.std-ref}</a>{.hoverxref .tooltip .reference
.internal} or without one, in which case an <a href="index.html#topics-items">[item object]{.std
.std-ref}</a>{.hoverxref .tooltip .reference
.internal} is automatically created in the Item Loader
[<code>__init__</code>{.docutils .literal .notranslate}]{.pre} method using the
<a href="index.html#topics-items">[item]{.std .std-ref}</a>{.hoverxref .tooltip
.reference .internal} class specified in the
<a href="#scrapy.loader.ItemLoader.default_item_class" title="scrapy.loader.ItemLoader.default_item_class">[<code>ItemLoader.default_item_class</code>{.xref .py .py-attr .docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal} attribute.</p>
<p>Then, you start collecting values into the Item Loader, typically using
<a href="index.html#topics-selectors">[Selectors]{.std .std-ref}</a>{.hoverxref
.tooltip .reference .internal}. You can add more than one value to the
same item field; the Item Loader will know how to &quot;join&quot; those values
later using a proper processing function.</p>
<p>::: {.admonition .note}
Note</p>
<p>Collected data is internally stored as lists, allowing to add several
values to the same field. If an [<code>item</code>{.docutils .literal
.notranslate}]{.pre} argument is passed when creating a loader, each of
the item's values will be stored as-is if it's already an iterable, or
wrapped with a list if it's a single value.
:::</p>
<p>Here is a typical Item Loader usage in a <a href="index.html#topics-spiders">[Spider]{.std
.std-ref}</a>{.hoverxref .tooltip .reference
.internal}, using the <a href="index.html#topics-items-declaring">[Product item]{.std
.std-ref}</a>{.hoverxref .tooltip
.reference .internal} declared in the <a href="index.html#topics-items">[Items chapter]{.std
.std-ref}</a>{.hoverxref .tooltip .reference
.internal}:</p>
<p>::: {.highlight-python .notranslate}
::: highlight
from scrapy.loader import ItemLoader
from myproject.items import Product</p>
<pre><code>def parse(self, response):
    l = ItemLoader(item=Product(), response=response)
    l.add_xpath(&quot;name&quot;, '//div[@class=&quot;product_name&quot;]')
    l.add_xpath(&quot;name&quot;, '//div[@class=&quot;product_title&quot;]')
    l.add_xpath(&quot;price&quot;, '//p[@id=&quot;price&quot;]')
    l.add_css(&quot;stock&quot;, &quot;p#stock&quot;)
    l.add_value(&quot;last_updated&quot;, &quot;today&quot;)  # you can also use literal values
    return l.load_item()
</code></pre>
<p>:::
:::</p>
<p>By quickly looking at that code, we can see the [<code>name</code>{.docutils
.literal .notranslate}]{.pre} field is being extracted from two
different XPath locations in the page:</p>
<ol>
<li>
<p>[<code>//div[@class=&quot;product_name&quot;]</code>{.docutils .literal
.notranslate}]{.pre}</p>
</li>
<li>
<p>[<code>//div[@class=&quot;product_title&quot;]</code>{.docutils .literal
.notranslate}]{.pre}</p>
</li>
</ol>
<p>In other words, data is being collected by extracting it from two XPath
locations, using the <a href="#scrapy.loader.ItemLoader.add_xpath" title="scrapy.loader.ItemLoader.add_xpath">[<code>add_xpath()</code>{.xref .py .py-meth .docutils
.literal
.notranslate}]{.pre}</a>{.reference
.internal} method. This is the data that will be assigned to the
[<code>name</code>{.docutils .literal .notranslate}]{.pre} field later.</p>
<p>Afterwards, similar calls are used for [<code>price</code>{.docutils .literal
.notranslate}]{.pre} and [<code>stock</code>{.docutils .literal
.notranslate}]{.pre} fields (the latter using a CSS selector with the
<a href="#scrapy.loader.ItemLoader.add_css" title="scrapy.loader.ItemLoader.add_css">[<code>add_css()</code>{.xref .py .py-meth .docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal} method), and finally the [<code>last_update</code>{.docutils .literal
.notranslate}]{.pre} field is populated directly with a literal value
([<code>today</code>{.docutils .literal .notranslate}]{.pre}) using a different
method: <a href="#scrapy.loader.ItemLoader.add_value" title="scrapy.loader.ItemLoader.add_value">[<code>add_value()</code>{.xref .py .py-meth .docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal}.</p>
<p>Finally, when all data is collected, the
<a href="#scrapy.loader.ItemLoader.load_item" title="scrapy.loader.ItemLoader.load_item">[<code>ItemLoader.load_item()</code>{.xref .py .py-meth .docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal} method is called which actually returns the item populated
with the data previously extracted and collected with the
<a href="#scrapy.loader.ItemLoader.add_xpath" title="scrapy.loader.ItemLoader.add_xpath">[<code>add_xpath()</code>{.xref .py .py-meth .docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal}, <a href="#scrapy.loader.ItemLoader.add_css" title="scrapy.loader.ItemLoader.add_css">[<code>add_css()</code>{.xref .py .py-meth .docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal}, and <a href="#scrapy.loader.ItemLoader.add_value" title="scrapy.loader.ItemLoader.add_value">[<code>add_value()</code>{.xref .py .py-meth .docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal} calls.
:::</p>
<p>::: {#working-with-dataclass-items .section}
[]{#topics-loaders-dataclass}</p>
<h4 id="working-with-dataclass-itemsheaderlink"><a class="header" href="#working-with-dataclass-itemsheaderlink">Working with dataclass items<a href="#working-with-dataclass-items" title="Permalink to this heading">¶</a>{.headerlink}</a></h4>
<p>By default, <a href="index.html#dataclass-items">[dataclass items]{.std
.std-ref}</a>{.hoverxref .tooltip .reference
.internal} require all fields to be passed when created. This could be
an issue when using dataclass items with item loaders: unless a
pre-populated item is passed to the loader, fields will be populated
incrementally using the loader's <a href="#scrapy.loader.ItemLoader.add_xpath" title="scrapy.loader.ItemLoader.add_xpath">[<code>add_xpath()</code>{.xref .py .py-meth
.docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal}, <a href="#scrapy.loader.ItemLoader.add_css" title="scrapy.loader.ItemLoader.add_css">[<code>add_css()</code>{.xref .py .py-meth .docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal} and <a href="#scrapy.loader.ItemLoader.add_value" title="scrapy.loader.ItemLoader.add_value">[<code>add_value()</code>{.xref .py .py-meth .docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal} methods.</p>
<p>One approach to overcome this is to define items using the
<a href="https://docs.python.org/3/library/dataclasses.html#dataclasses.field" title="(in Python v3.12)">[<code>field()</code>{.xref .py .py-func .docutils .literal
.notranslate}]{.pre}</a>{.reference
.external} function, with a [<code>default</code>{.docutils .literal
.notranslate}]{.pre} argument:</p>
<p>::: {.highlight-python .notranslate}
::: highlight
from dataclasses import dataclass, field
from typing import Optional</p>
<pre><code>@dataclass
class InventoryItem:
    name: Optional[str] = field(default=None)
    price: Optional[float] = field(default=None)
    stock: Optional[int] = field(default=None)
</code></pre>
<p>:::
:::
:::</p>
<p>::: {#input-and-output-processors .section}
[]{#topics-loaders-processors}</p>
<h4 id="input-and-output-processorsheaderlink"><a class="header" href="#input-and-output-processorsheaderlink">Input and Output processors<a href="#input-and-output-processors" title="Permalink to this heading">¶</a>{.headerlink}</a></h4>
<p>An Item Loader contains one input processor and one output processor for
each (item) field. The input processor processes the extracted data as
soon as it's received (through the <a href="#scrapy.loader.ItemLoader.add_xpath" title="scrapy.loader.ItemLoader.add_xpath">[<code>add_xpath()</code>{.xref .py .py-meth
.docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal}, <a href="#scrapy.loader.ItemLoader.add_css" title="scrapy.loader.ItemLoader.add_css">[<code>add_css()</code>{.xref .py .py-meth .docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal} or <a href="#scrapy.loader.ItemLoader.add_value" title="scrapy.loader.ItemLoader.add_value">[<code>add_value()</code>{.xref .py .py-meth .docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal} methods) and the result of the input processor is collected
and kept inside the ItemLoader. After collecting all data, the
<a href="#scrapy.loader.ItemLoader.load_item" title="scrapy.loader.ItemLoader.load_item">[<code>ItemLoader.load_item()</code>{.xref .py .py-meth .docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal} method is called to populate and get the populated <a href="index.html#topics-items">[item
object]{.std .std-ref}</a>{.hoverxref .tooltip
.reference .internal}. That's when the output processor is called with
the data previously collected (and processed using the input processor).
The result of the output processor is the final value that gets assigned
to the item.</p>
<p>Let's see an example to illustrate how the input and output processors
are called for a particular field (the same applies for any other
field):</p>
<p>::: {.highlight-python .notranslate}
::: highlight
l = ItemLoader(Product(), some_selector)
l.add_xpath(&quot;name&quot;, xpath1)  # (1)
l.add_xpath(&quot;name&quot;, xpath2)  # (2)
l.add_css(&quot;name&quot;, css)  # (3)
l.add_value(&quot;name&quot;, &quot;test&quot;)  # (4)
return l.load_item()  # (5)
:::
:::</p>
<p>So what happens is:</p>
<ol>
<li>
<p>Data from [<code>xpath1</code>{.docutils .literal .notranslate}]{.pre} is
extracted, and passed through the <em>input processor</em> of the
[<code>name</code>{.docutils .literal .notranslate}]{.pre} field. The result of
the input processor is collected and kept in the Item Loader (but
not yet assigned to the item).</p>
</li>
<li>
<p>Data from [<code>xpath2</code>{.docutils .literal .notranslate}]{.pre} is
extracted, and passed through the same <em>input processor</em> used in
(1). The result of the input processor is appended to the data
collected in (1) (if any).</p>
</li>
<li>
<p>This case is similar to the previous ones, except that the data is
extracted from the [<code>css</code>{.docutils .literal .notranslate}]{.pre}
CSS selector, and passed through the same <em>input processor</em> used
in (1) and (2). The result of the input processor is appended to the
data collected in (1) and (2) (if any).</p>
</li>
<li>
<p>This case is also similar to the previous ones, except that the
value to be collected is assigned directly, instead of being
extracted from a XPath expression or a CSS selector. However, the
value is still passed through the input processors. In this case,
since the value is not iterable it is converted to an iterable of a
single element before passing it to the input processor, because
input processor always receive iterables.</p>
</li>
<li>
<p>The data collected in steps (1), (2), (3) and (4) is passed through
the <em>output processor</em> of the [<code>name</code>{.docutils .literal
.notranslate}]{.pre} field. The result of the output processor is
the value assigned to the [<code>name</code>{.docutils .literal
.notranslate}]{.pre} field in the item.</p>
</li>
</ol>
<p>It's worth noticing that processors are just callable objects, which are
called with the data to be parsed, and return a parsed value. So you can
use any function as input or output processor. The only requirement is
that they must accept one (and only one) positional argument, which will
be an iterable.</p>
<p>::: versionchanged
[Changed in version 2.0: ]{.versionmodified .changed}Processors no
longer need to be methods.
:::</p>
<p>::: {.admonition .note}
Note</p>
<p>Both input and output processors must receive an iterable as their first
argument. The output of those functions can be anything. The result of
input processors will be appended to an internal list (in the Loader)
containing the collected values (for that field). The result of the
output processors is the value that will be finally assigned to the
item.
:::</p>
<p>The other thing you need to keep in mind is that the values returned by
input processors are collected internally (in lists) and then passed to
output processors to populate the fields.</p>
<p>Last, but not least,
<a href="https://itemloaders.readthedocs.io/en/latest/">itemloaders</a>{.reference
.external} comes with some <a href="https://itemloaders.readthedocs.io/en/latest/built-in-processors.html#built-in-processors" title="(in itemloaders)">[commonly used processors]{.xref .std
.std-ref}</a>{.reference
.external} built-in for convenience.
:::</p>
<p>::: {#declaring-item-loaders .section}</p>
<h4 id="declaring-item-loadersheaderlink"><a class="header" href="#declaring-item-loadersheaderlink">Declaring Item Loaders<a href="#declaring-item-loaders" title="Permalink to this heading">¶</a>{.headerlink}</a></h4>
<p>Item Loaders are declared using a class definition syntax. Here is an
example:</p>
<p>::: {.highlight-python .notranslate}
::: highlight
from itemloaders.processors import TakeFirst, MapCompose, Join
from scrapy.loader import ItemLoader</p>
<pre><code>class ProductLoader(ItemLoader):
    default_output_processor = TakeFirst()

    name_in = MapCompose(str.title)
    name_out = Join()

    price_in = MapCompose(str.strip)

    # ...
</code></pre>
<p>:::
:::</p>
<p>As you can see, input processors are declared using the [<code>_in</code>{.docutils
.literal .notranslate}]{.pre} suffix while output processors are
declared using the [<code>_out</code>{.docutils .literal .notranslate}]{.pre}
suffix. And you can also declare a default input/output processors using
the <a href="#scrapy.loader.ItemLoader.default_input_processor" title="scrapy.loader.ItemLoader.default_input_processor">[<code>ItemLoader.default_input_processor</code>{.xref .py .py-attr .docutils
.literal
.notranslate}]{.pre}</a>{.reference
.internal} and <a href="#scrapy.loader.ItemLoader.default_output_processor" title="scrapy.loader.ItemLoader.default_output_processor">[<code>ItemLoader.default_output_processor</code>{.xref .py
.py-attr .docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal} attributes.
:::</p>
<p>::: {#declaring-input-and-output-processors .section}
[]{#topics-loaders-processors-declaring}</p>
<h4 id="declaring-input-and-output-processorsheaderlink"><a class="header" href="#declaring-input-and-output-processorsheaderlink">Declaring Input and Output Processors<a href="#declaring-input-and-output-processors" title="Permalink to this heading">¶</a>{.headerlink}</a></h4>
<p>As seen in the previous section, input and output processors can be
declared in the Item Loader definition, and it's very common to declare
input processors this way. However, there is one more place where you
can specify the input and output processors to use: in the <a href="index.html#topics-items-fields">[Item
Field]{.std .std-ref}</a>{.hoverxref
.tooltip .reference .internal} metadata. Here is an example:</p>
<p>::: {.highlight-python .notranslate}
::: highlight
import scrapy
from itemloaders.processors import Join, MapCompose, TakeFirst
from w3lib.html import remove_tags</p>
<pre><code>def filter_price(value):
    if value.isdigit():
        return value


class Product(scrapy.Item):
    name = scrapy.Field(
        input_processor=MapCompose(remove_tags),
        output_processor=Join(),
    )
    price = scrapy.Field(
        input_processor=MapCompose(remove_tags, filter_price),
        output_processor=TakeFirst(),
    )
</code></pre>
<p>:::
:::</p>
<p>::: {.highlight-pycon .notranslate}
::: highlight
&gt;&gt;&gt; from scrapy.loader import ItemLoader
&gt;&gt;&gt; il = ItemLoader(item=Product())
&gt;&gt;&gt; il.add_value(&quot;name&quot;, [&quot;Welcome to my&quot;, &quot;<strong>website</strong>&quot;])
&gt;&gt;&gt; il.add_value(&quot;price&quot;, [&quot;€&quot;, &quot;<span>1000</span>&quot;])
&gt;&gt;&gt; il.load_item()
{'name': 'Welcome to my website', 'price': '1000'}
:::
:::</p>
<p>The precedence order, for both input and output processors, is as
follows:</p>
<ol>
<li>
<p>Item Loader field-specific attributes: [<code>field_in</code>{.docutils
.literal .notranslate}]{.pre} and [<code>field_out</code>{.docutils .literal
.notranslate}]{.pre} (most precedence)</p>
</li>
<li>
<p>Field metadata ([<code>input_processor</code>{.docutils .literal
.notranslate}]{.pre} and [<code>output_processor</code>{.docutils .literal
.notranslate}]{.pre} key)</p>
</li>
<li>
<p>Item Loader defaults: <a href="#scrapy.loader.ItemLoader.default_input_processor" title="scrapy.loader.ItemLoader.default_input_processor">[<code>ItemLoader.default_input_processor()</code>{.xref
.py .py-meth .docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal} and <a href="#scrapy.loader.ItemLoader.default_output_processor" title="scrapy.loader.ItemLoader.default_output_processor">[<code>ItemLoader.default_output_processor()</code>{.xref .py
.py-meth .docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal} (least precedence)</p>
</li>
</ol>
<p>See also: <a href="#topics-loaders-extending">[Reusing and extending Item Loaders]{.std
.std-ref}</a>{.hoverxref .tooltip .reference
.internal}.
:::</p>
<p>::: {#item-loader-context .section}
[]{#topics-loaders-context}</p>
<h4 id="item-loader-contextheaderlink"><a class="header" href="#item-loader-contextheaderlink">Item Loader Context<a href="#item-loader-context" title="Permalink to this heading">¶</a>{.headerlink}</a></h4>
<p>The Item Loader Context is a dict of arbitrary key/values which is
shared among all input and output processors in the Item Loader. It can
be passed when declaring, instantiating or using Item Loader. They are
used to modify the behaviour of the input/output processors.</p>
<p>For example, suppose you have a function [<code>parse_length</code>{.docutils
.literal .notranslate}]{.pre} which receives a text value and extracts a
length from it:</p>
<p>::: {.highlight-python .notranslate}
::: highlight
def parse_length(text, loader_context):
unit = loader_context.get(&quot;unit&quot;, &quot;m&quot;)
# ... length parsing code goes here ...
return parsed_length
:::
:::</p>
<p>By accepting a [<code>loader_context</code>{.docutils .literal .notranslate}]{.pre}
argument the function is explicitly telling the Item Loader that it's
able to receive an Item Loader context, so the Item Loader passes the
currently active context when calling it, and the processor function
([<code>parse_length</code>{.docutils .literal .notranslate}]{.pre} in this case)
can thus use them.</p>
<p>There are several ways to modify Item Loader context values:</p>
<ol>
<li>
<p>By modifying the currently active Item Loader context
(<a href="#scrapy.loader.ItemLoader.context" title="scrapy.loader.ItemLoader.context">[<code>context</code>{.xref .py .py-attr .docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal} attribute):</p>
<p>::: {.highlight-python .notranslate}
::: highlight
loader = ItemLoader(product)
loader.context[&quot;unit&quot;] = &quot;cm&quot;
:::
:::</p>
</li>
<li>
<p>On Item Loader instantiation (the keyword arguments of Item Loader
[<code>__init__</code>{.docutils .literal .notranslate}]{.pre} method are
stored in the Item Loader context):</p>
<p>::: {.highlight-python .notranslate}
::: highlight
loader = ItemLoader(product, unit=&quot;cm&quot;)
:::
:::</p>
</li>
<li>
<p>On Item Loader declaration, for those input/output processors that
support instantiating them with an Item Loader context.
[<code>MapCompose</code>{.xref .py .py-class .docutils .literal
.notranslate}]{.pre} is one of them:</p>
<p>::: {.highlight-python .notranslate}
::: highlight
class ProductLoader(ItemLoader):
length_out = MapCompose(parse_length, unit=&quot;cm&quot;)
:::
:::
:::</p>
</li>
</ol>
<p>::: {#itemloader-objects .section}</p>
<h4 id="itemloader-objectsheaderlink"><a class="header" href="#itemloader-objectsheaderlink">ItemLoader objects<a href="#itemloader-objects" title="Permalink to this heading">¶</a>{.headerlink}</a></h4>
<p><em>[class]{.pre}[ ]{.w}</em>[[scrapy.loader.]{.pre}]{.sig-prename .descclassname}[[ItemLoader]{.pre}]{.sig-name .descname}[(]{.sig-paren}<em>[[item]{.pre}]{.n}[[=]{.pre}]{.o}[[None]{.pre}]{.default_value}</em>, <em>[[selector]{.pre}]{.n}[[=]{.pre}]{.o}[[None]{.pre}]{.default_value}</em>, <em>[[response]{.pre}]{.n}[[=]{.pre}]{.o}[[None]{.pre}]{.default_value}</em>, <em>[[parent]{.pre}]{.n}[[=]{.pre}]{.o}[[None]{.pre}]{.default_value}</em>, <em>[[**]{.pre}]{.o}[[context]{.pre}]{.n}</em>[)]{.sig-paren}<a href="_modules/scrapy/loader.html#ItemLoader">[[[source]]{.pre}]{.viewcode-link}</a>{.reference .internal}<a href="#scrapy.loader.ItemLoader" title="Permalink to this definition">¶</a>{.headerlink}</p>
<p>:   A user-friendly abstraction to populate an <a href="index.html#topics-items">[item]{.std
.std-ref}</a>{.hoverxref .tooltip .reference
.internal} with data by applying <a href="#topics-loaders-processors">[field processors]{.std
.std-ref}</a>{.hoverxref .tooltip
.reference .internal} to scraped data. When instantiated with a
[<code>selector</code>{.docutils .literal .notranslate}]{.pre} or a
[<code>response</code>{.docutils .literal .notranslate}]{.pre} it supports data
extraction from web pages using <a href="index.html#topics-selectors">[selectors]{.std
.std-ref}</a>{.hoverxref .tooltip
.reference .internal}.</p>
<pre><code>Parameters

:   -   **item**
        ([*scrapy.item.Item*](index.html#scrapy.item.scrapy.item.Item &quot;scrapy.item.scrapy.item.Item&quot;){.reference
        .internal}) -- The item instance to populate using
        subsequent calls to [[`add_xpath()`{.xref .py .py-meth
        .docutils .literal
        .notranslate}]{.pre}](#scrapy.loader.ItemLoader.add_xpath &quot;scrapy.loader.ItemLoader.add_xpath&quot;){.reference
        .internal}, [[`add_css()`{.xref .py .py-meth .docutils
        .literal
        .notranslate}]{.pre}](#scrapy.loader.ItemLoader.add_css &quot;scrapy.loader.ItemLoader.add_css&quot;){.reference
        .internal}, or [[`add_value()`{.xref .py .py-meth .docutils
        .literal
        .notranslate}]{.pre}](#scrapy.loader.ItemLoader.add_value &quot;scrapy.loader.ItemLoader.add_value&quot;){.reference
        .internal}.

    -   **selector** ([[`Selector`{.xref .py .py-class .docutils
        .literal
        .notranslate}]{.pre}](index.html#scrapy.selector.Selector &quot;scrapy.selector.Selector&quot;){.reference
        .internal} object) -- The selector to extract data from,
        when using the [[`add_xpath()`{.xref .py .py-meth .docutils
        .literal
        .notranslate}]{.pre}](#scrapy.loader.ItemLoader.add_xpath &quot;scrapy.loader.ItemLoader.add_xpath&quot;){.reference
        .internal}, [[`add_css()`{.xref .py .py-meth .docutils
        .literal
        .notranslate}]{.pre}](#scrapy.loader.ItemLoader.add_css &quot;scrapy.loader.ItemLoader.add_css&quot;){.reference
        .internal}, [[`replace_xpath()`{.xref .py .py-meth .docutils
        .literal
        .notranslate}]{.pre}](#scrapy.loader.ItemLoader.replace_xpath &quot;scrapy.loader.ItemLoader.replace_xpath&quot;){.reference
        .internal}, or [[`replace_css()`{.xref .py .py-meth
        .docutils .literal
        .notranslate}]{.pre}](#scrapy.loader.ItemLoader.replace_css &quot;scrapy.loader.ItemLoader.replace_css&quot;){.reference
        .internal} method.

    -   **response** ([[`Response`{.xref .py .py-class .docutils
        .literal
        .notranslate}]{.pre}](index.html#scrapy.http.Response &quot;scrapy.http.Response&quot;){.reference
        .internal} object) -- The response used to construct the
        selector using the [[`default_selector_class`{.xref .py
        .py-attr .docutils .literal
        .notranslate}]{.pre}](#scrapy.loader.ItemLoader.default_selector_class &quot;scrapy.loader.ItemLoader.default_selector_class&quot;){.reference
        .internal}, unless the selector argument is given, in which
        case this argument is ignored.

If no item is given, one is instantiated automatically using the
class in [[`default_item_class`{.xref .py .py-attr .docutils
.literal
.notranslate}]{.pre}](#scrapy.loader.ItemLoader.default_item_class &quot;scrapy.loader.ItemLoader.default_item_class&quot;){.reference
.internal}.

The item, selector, response and remaining keyword arguments are
assigned to the Loader context (accessible through the
[[`context`{.xref .py .py-attr .docutils .literal
.notranslate}]{.pre}](#scrapy.loader.ItemLoader.context &quot;scrapy.loader.ItemLoader.context&quot;){.reference
.internal} attribute).

[[item]{.pre}]{.sig-name .descname}[¶](#scrapy.loader.ItemLoader.item &quot;Permalink to this definition&quot;){.headerlink}

:   The item object being parsed by this Item Loader. This is mostly
    used as a property so, when attempting to override this value,
    you may want to check out [[`default_item_class`{.xref .py
    .py-attr .docutils .literal
    .notranslate}]{.pre}](#scrapy.loader.ItemLoader.default_item_class &quot;scrapy.loader.ItemLoader.default_item_class&quot;){.reference
    .internal} first.

[[context]{.pre}]{.sig-name .descname}[¶](#scrapy.loader.ItemLoader.context &quot;Permalink to this definition&quot;){.headerlink}

:   The currently active [[Context]{.xref .std
    .std-ref}](https://itemloaders.readthedocs.io/en/latest/loaders-context.html#loaders-context &quot;(in itemloaders)&quot;){.reference
    .external} of this Item Loader.

[[default_item_class]{.pre}]{.sig-name .descname}[¶](#scrapy.loader.ItemLoader.default_item_class &quot;Permalink to this definition&quot;){.headerlink}

:   An [[item]{.std .std-ref}](index.html#topics-items){.hoverxref
    .tooltip .reference .internal} class (or factory), used to
    instantiate items when not given in the [`__init__`{.docutils
    .literal .notranslate}]{.pre} method.

[[default_input_processor]{.pre}]{.sig-name .descname}[¶](#scrapy.loader.ItemLoader.default_input_processor &quot;Permalink to this definition&quot;){.headerlink}

:   The default input processor to use for those fields which don't
    specify one.

[[default_output_processor]{.pre}]{.sig-name .descname}[¶](#scrapy.loader.ItemLoader.default_output_processor &quot;Permalink to this definition&quot;){.headerlink}

:   The default output processor to use for those fields which don't
    specify one.

[[default_selector_class]{.pre}]{.sig-name .descname}[¶](#scrapy.loader.ItemLoader.default_selector_class &quot;Permalink to this definition&quot;){.headerlink}

:   The class used to construct the [[`selector`{.xref .py .py-attr
    .docutils .literal
    .notranslate}]{.pre}](#scrapy.loader.ItemLoader.selector &quot;scrapy.loader.ItemLoader.selector&quot;){.reference
    .internal} of this [[`ItemLoader`{.xref .py .py-class .docutils
    .literal
    .notranslate}]{.pre}](#scrapy.loader.ItemLoader &quot;scrapy.loader.ItemLoader&quot;){.reference
    .internal}, if only a response is given in the
    [`__init__`{.docutils .literal .notranslate}]{.pre} method. If a
    selector is given in the [`__init__`{.docutils .literal
    .notranslate}]{.pre} method this attribute is ignored. This
    attribute is sometimes overridden in subclasses.

[[selector]{.pre}]{.sig-name .descname}[¶](#scrapy.loader.ItemLoader.selector &quot;Permalink to this definition&quot;){.headerlink}

:   The [[`Selector`{.xref .py .py-class .docutils .literal
    .notranslate}]{.pre}](index.html#scrapy.selector.Selector &quot;scrapy.selector.Selector&quot;){.reference
    .internal} object to extract data from. It's either the selector
    given in the [`__init__`{.docutils .literal .notranslate}]{.pre}
    method or one created from the response given in the
    [`__init__`{.docutils .literal .notranslate}]{.pre} method using
    the [[`default_selector_class`{.xref .py .py-attr .docutils
    .literal
    .notranslate}]{.pre}](#scrapy.loader.ItemLoader.default_selector_class &quot;scrapy.loader.ItemLoader.default_selector_class&quot;){.reference
    .internal}. This attribute is meant to be read-only.

[[add_css]{.pre}]{.sig-name .descname}[(]{.sig-paren}*[[field_name]{.pre}]{.n}*, *[[css]{.pre}]{.n}*, *[[\*]{.pre}]{.o}[[processors]{.pre}]{.n}*, *[[re]{.pre}]{.n}[[=]{.pre}]{.o}[[None]{.pre}]{.default_value}*, *[[\*\*]{.pre}]{.o}[[kw]{.pre}]{.n}*[)]{.sig-paren}[[[\[source\]]{.pre}]{.viewcode-link}](_modules/itemloaders.html#ItemLoader.add_css){.reference .internal}[¶](#scrapy.loader.ItemLoader.add_css &quot;Permalink to this definition&quot;){.headerlink}

:   Similar to [[`ItemLoader.add_value()`{.xref .py .py-meth
    .docutils .literal
    .notranslate}]{.pre}](#scrapy.loader.ItemLoader.add_value &quot;scrapy.loader.ItemLoader.add_value&quot;){.reference
    .internal} but receives a CSS selector instead of a value, which
    is used to extract a list of unicode strings from the selector
    associated with this [[`ItemLoader`{.xref .py .py-class
    .docutils .literal
    .notranslate}]{.pre}](#scrapy.loader.ItemLoader &quot;scrapy.loader.ItemLoader&quot;){.reference
    .internal}.

    See [[`get_css()`{.xref .py .py-meth .docutils .literal
    .notranslate}]{.pre}](#scrapy.loader.ItemLoader.get_css &quot;scrapy.loader.ItemLoader.get_css&quot;){.reference
    .internal} for [`kwargs`{.docutils .literal
    .notranslate}]{.pre}.

    Parameters

    :   **css**
        ([*str*](https://docs.python.org/3/library/stdtypes.html#str &quot;(in Python v3.12)&quot;){.reference
        .external}) -- the CSS selector to extract data from

    Examples:

    ::: {.highlight-default .notranslate}
    ::: highlight
        # HTML snippet: &lt;p class=&quot;product-name&quot;&gt;Color TV&lt;/p&gt;
        loader.add_css('name', 'p.product-name')
        # HTML snippet: &lt;p id=&quot;price&quot;&gt;the price is $1200&lt;/p&gt;
        loader.add_css('price', 'p#price', re='the price is (.*)')
    :::
    :::

[[add_jmes]{.pre}]{.sig-name .descname}[(]{.sig-paren}*[[field_name]{.pre}]{.n}*, *[[jmes]{.pre}]{.n}*, *[[\*]{.pre}]{.o}[[processors]{.pre}]{.n}*, *[[re]{.pre}]{.n}[[=]{.pre}]{.o}[[None]{.pre}]{.default_value}*, *[[\*\*]{.pre}]{.o}[[kw]{.pre}]{.n}*[)]{.sig-paren}[[[\[source\]]{.pre}]{.viewcode-link}](_modules/itemloaders.html#ItemLoader.add_jmes){.reference .internal}[¶](#scrapy.loader.ItemLoader.add_jmes &quot;Permalink to this definition&quot;){.headerlink}

:   Similar to [[`ItemLoader.add_value()`{.xref .py .py-meth
    .docutils .literal
    .notranslate}]{.pre}](#scrapy.loader.ItemLoader.add_value &quot;scrapy.loader.ItemLoader.add_value&quot;){.reference
    .internal} but receives a JMESPath selector instead of a value,
    which is used to extract a list of unicode strings from the
    selector associated with this [[`ItemLoader`{.xref .py .py-class
    .docutils .literal
    .notranslate}]{.pre}](#scrapy.loader.ItemLoader &quot;scrapy.loader.ItemLoader&quot;){.reference
    .internal}.

    See [[`get_jmes()`{.xref .py .py-meth .docutils .literal
    .notranslate}]{.pre}](#scrapy.loader.ItemLoader.get_jmes &quot;scrapy.loader.ItemLoader.get_jmes&quot;){.reference
    .internal} for [`kwargs`{.docutils .literal
    .notranslate}]{.pre}.

    Parameters

    :   **jmes**
        ([*str*](https://docs.python.org/3/library/stdtypes.html#str &quot;(in Python v3.12)&quot;){.reference
        .external}) -- the JMESPath selector to extract data from

    Examples:

    ::: {.highlight-default .notranslate}
    ::: highlight
        # HTML snippet: {&quot;name&quot;: &quot;Color TV&quot;}
        loader.add_jmes('name')
        # HTML snippet: {&quot;price&quot;: the price is $1200&quot;}
        loader.add_jmes('price', TakeFirst(), re='the price is (.*)')
    :::
    :::

[[add_value]{.pre}]{.sig-name .descname}[(]{.sig-paren}*[[field_name]{.pre}]{.n}*, *[[value]{.pre}]{.n}*, *[[\*]{.pre}]{.o}[[processors]{.pre}]{.n}*, *[[re]{.pre}]{.n}[[=]{.pre}]{.o}[[None]{.pre}]{.default_value}*, *[[\*\*]{.pre}]{.o}[[kw]{.pre}]{.n}*[)]{.sig-paren}[[[\[source\]]{.pre}]{.viewcode-link}](_modules/itemloaders.html#ItemLoader.add_value){.reference .internal}[¶](#scrapy.loader.ItemLoader.add_value &quot;Permalink to this definition&quot;){.headerlink}

:   Process and then add the given [`value`{.docutils .literal
    .notranslate}]{.pre} for the given field.

    The value is first passed through [[`get_value()`{.xref .py
    .py-meth .docutils .literal
    .notranslate}]{.pre}](#scrapy.loader.ItemLoader.get_value &quot;scrapy.loader.ItemLoader.get_value&quot;){.reference
    .internal} by giving the [`processors`{.docutils .literal
    .notranslate}]{.pre} and [`kwargs`{.docutils .literal
    .notranslate}]{.pre}, and then passed through the [[field input
    processor]{.xref .std
    .std-ref}](https://itemloaders.readthedocs.io/en/latest/processors.html#processors &quot;(in itemloaders)&quot;){.reference
    .external} and its result appended to the data collected for
    that field. If the field already contains collected data, the
    new data is added.

    The given [`field_name`{.docutils .literal .notranslate}]{.pre}
    can be [`None`{.docutils .literal .notranslate}]{.pre}, in which
    case values for multiple fields may be added. And the processed
    value should be a dict with field_name mapped to values.

    Examples:

    ::: {.highlight-default .notranslate}
    ::: highlight
        loader.add_value('name', 'Color TV')
        loader.add_value('colours', ['white', 'blue'])
        loader.add_value('length', '100')
        loader.add_value('name', 'name: foo', TakeFirst(), re='name: (.+)')
        loader.add_value(None, {'name': 'foo', 'sex': 'male'})
    :::
    :::

[[add_xpath]{.pre}]{.sig-name .descname}[(]{.sig-paren}*[[field_name]{.pre}]{.n}*, *[[xpath]{.pre}]{.n}*, *[[\*]{.pre}]{.o}[[processors]{.pre}]{.n}*, *[[re]{.pre}]{.n}[[=]{.pre}]{.o}[[None]{.pre}]{.default_value}*, *[[\*\*]{.pre}]{.o}[[kw]{.pre}]{.n}*[)]{.sig-paren}[[[\[source\]]{.pre}]{.viewcode-link}](_modules/itemloaders.html#ItemLoader.add_xpath){.reference .internal}[¶](#scrapy.loader.ItemLoader.add_xpath &quot;Permalink to this definition&quot;){.headerlink}

:   Similar to [[`ItemLoader.add_value()`{.xref .py .py-meth
    .docutils .literal
    .notranslate}]{.pre}](#scrapy.loader.ItemLoader.add_value &quot;scrapy.loader.ItemLoader.add_value&quot;){.reference
    .internal} but receives an XPath instead of a value, which is
    used to extract a list of strings from the selector associated
    with this [[`ItemLoader`{.xref .py .py-class .docutils .literal
    .notranslate}]{.pre}](#scrapy.loader.ItemLoader &quot;scrapy.loader.ItemLoader&quot;){.reference
    .internal}.

    See [[`get_xpath()`{.xref .py .py-meth .docutils .literal
    .notranslate}]{.pre}](#scrapy.loader.ItemLoader.get_xpath &quot;scrapy.loader.ItemLoader.get_xpath&quot;){.reference
    .internal} for [`kwargs`{.docutils .literal
    .notranslate}]{.pre}.

    Parameters

    :   **xpath**
        ([*str*](https://docs.python.org/3/library/stdtypes.html#str &quot;(in Python v3.12)&quot;){.reference
        .external}) -- the XPath to extract data from

    Examples:

    ::: {.highlight-default .notranslate}
    ::: highlight
        # HTML snippet: &lt;p class=&quot;product-name&quot;&gt;Color TV&lt;/p&gt;
        loader.add_xpath('name', '//p[@class=&quot;product-name&quot;]')
        # HTML snippet: &lt;p id=&quot;price&quot;&gt;the price is $1200&lt;/p&gt;
        loader.add_xpath('price', '//p[@id=&quot;price&quot;]', re='the price is (.*)')
    :::
    :::

[[get_collected_values]{.pre}]{.sig-name .descname}[(]{.sig-paren}*[[field_name]{.pre}]{.n}*[)]{.sig-paren}[[[\[source\]]{.pre}]{.viewcode-link}](_modules/itemloaders.html#ItemLoader.get_collected_values){.reference .internal}[¶](#scrapy.loader.ItemLoader.get_collected_values &quot;Permalink to this definition&quot;){.headerlink}

:   Return the collected values for the given field.

[[get_css]{.pre}]{.sig-name .descname}[(]{.sig-paren}*[[css]{.pre}]{.n}*, *[[\*]{.pre}]{.o}[[processors]{.pre}]{.n}*, *[[re]{.pre}]{.n}[[=]{.pre}]{.o}[[None]{.pre}]{.default_value}*, *[[\*\*]{.pre}]{.o}[[kw]{.pre}]{.n}*[)]{.sig-paren}[[[\[source\]]{.pre}]{.viewcode-link}](_modules/itemloaders.html#ItemLoader.get_css){.reference .internal}[¶](#scrapy.loader.ItemLoader.get_css &quot;Permalink to this definition&quot;){.headerlink}

:   Similar to [[`ItemLoader.get_value()`{.xref .py .py-meth
    .docutils .literal
    .notranslate}]{.pre}](#scrapy.loader.ItemLoader.get_value &quot;scrapy.loader.ItemLoader.get_value&quot;){.reference
    .internal} but receives a CSS selector instead of a value, which
    is used to extract a list of unicode strings from the selector
    associated with this [[`ItemLoader`{.xref .py .py-class
    .docutils .literal
    .notranslate}]{.pre}](#scrapy.loader.ItemLoader &quot;scrapy.loader.ItemLoader&quot;){.reference
    .internal}.

    Parameters

    :   -   **css**
            ([*str*](https://docs.python.org/3/library/stdtypes.html#str &quot;(in Python v3.12)&quot;){.reference
            .external}) -- the CSS selector to extract data from

        -   **re**
            ([*str*](https://docs.python.org/3/library/stdtypes.html#str &quot;(in Python v3.12)&quot;){.reference
            .external} *or*
            [*Pattern*](https://docs.python.org/3/library/typing.html#typing.Pattern &quot;(in Python v3.12)&quot;){.reference
            .external}) -- a regular expression to use for
            extracting data from the selected CSS region

    Examples:

    ::: {.highlight-default .notranslate}
    ::: highlight
        # HTML snippet: &lt;p class=&quot;product-name&quot;&gt;Color TV&lt;/p&gt;
        loader.get_css('p.product-name')
        # HTML snippet: &lt;p id=&quot;price&quot;&gt;the price is $1200&lt;/p&gt;
        loader.get_css('p#price', TakeFirst(), re='the price is (.*)')
    :::
    :::

[[get_jmes]{.pre}]{.sig-name .descname}[(]{.sig-paren}*[[jmes]{.pre}]{.n}*, *[[\*]{.pre}]{.o}[[processors]{.pre}]{.n}*, *[[re]{.pre}]{.n}[[=]{.pre}]{.o}[[None]{.pre}]{.default_value}*, *[[\*\*]{.pre}]{.o}[[kw]{.pre}]{.n}*[)]{.sig-paren}[[[\[source\]]{.pre}]{.viewcode-link}](_modules/itemloaders.html#ItemLoader.get_jmes){.reference .internal}[¶](#scrapy.loader.ItemLoader.get_jmes &quot;Permalink to this definition&quot;){.headerlink}

:   Similar to [[`ItemLoader.get_value()`{.xref .py .py-meth
    .docutils .literal
    .notranslate}]{.pre}](#scrapy.loader.ItemLoader.get_value &quot;scrapy.loader.ItemLoader.get_value&quot;){.reference
    .internal} but receives a JMESPath selector instead of a value,
    which is used to extract a list of unicode strings from the
    selector associated with this [[`ItemLoader`{.xref .py .py-class
    .docutils .literal
    .notranslate}]{.pre}](#scrapy.loader.ItemLoader &quot;scrapy.loader.ItemLoader&quot;){.reference
    .internal}.

    Parameters

    :   -   **jmes**
            ([*str*](https://docs.python.org/3/library/stdtypes.html#str &quot;(in Python v3.12)&quot;){.reference
            .external}) -- the JMESPath selector to extract data
            from

        -   **re**
            ([*str*](https://docs.python.org/3/library/stdtypes.html#str &quot;(in Python v3.12)&quot;){.reference
            .external} *or*
            [*Pattern*](https://docs.python.org/3/library/typing.html#typing.Pattern &quot;(in Python v3.12)&quot;){.reference
            .external}) -- a regular expression to use for
            extracting data from the selected JMESPath

    Examples:

    ::: {.highlight-default .notranslate}
    ::: highlight
        # HTML snippet: {&quot;name&quot;: &quot;Color TV&quot;}
        loader.get_jmes('name')
        # HTML snippet: {&quot;price&quot;: the price is $1200&quot;}
        loader.get_jmes('price', TakeFirst(), re='the price is (.*)')
    :::
    :::

[[get_output_value]{.pre}]{.sig-name .descname}[(]{.sig-paren}*[[field_name]{.pre}]{.n}*[)]{.sig-paren}[[[\[source\]]{.pre}]{.viewcode-link}](_modules/itemloaders.html#ItemLoader.get_output_value){.reference .internal}[¶](#scrapy.loader.ItemLoader.get_output_value &quot;Permalink to this definition&quot;){.headerlink}

:   Return the collected values parsed using the output processor,
    for the given field. This method doesn't populate or modify the
    item at all.

[[get_value]{.pre}]{.sig-name .descname}[(]{.sig-paren}*[[value]{.pre}]{.n}*, *[[\*]{.pre}]{.o}[[processors]{.pre}]{.n}*, *[[re]{.pre}]{.n}[[=]{.pre}]{.o}[[None]{.pre}]{.default_value}*, *[[\*\*]{.pre}]{.o}[[kw]{.pre}]{.n}*[)]{.sig-paren}[[[\[source\]]{.pre}]{.viewcode-link}](_modules/itemloaders.html#ItemLoader.get_value){.reference .internal}[¶](#scrapy.loader.ItemLoader.get_value &quot;Permalink to this definition&quot;){.headerlink}

:   Process the given [`value`{.docutils .literal
    .notranslate}]{.pre} by the given [`processors`{.docutils
    .literal .notranslate}]{.pre} and keyword arguments.

    Available keyword arguments:

    Parameters

    :   **re**
        ([*str*](https://docs.python.org/3/library/stdtypes.html#str &quot;(in Python v3.12)&quot;){.reference
        .external} *or*
        [*Pattern*](https://docs.python.org/3/library/typing.html#typing.Pattern &quot;(in Python v3.12)&quot;){.reference
        .external}) -- a regular expression to use for extracting
        data from the given value using [`extract_regex()`{.xref .py
        .py-func .docutils .literal .notranslate}]{.pre} method,
        applied before processors

    Examples:

    ::: {.doctest .highlight-default .notranslate}
    ::: highlight
        &gt;&gt;&gt; from itemloaders import ItemLoader
        &gt;&gt;&gt; from itemloaders.processors import TakeFirst
        &gt;&gt;&gt; loader = ItemLoader()
        &gt;&gt;&gt; loader.get_value('name: foo', TakeFirst(), str.upper, re='name: (.+)')
        'FOO'
    :::
    :::

[[get_xpath]{.pre}]{.sig-name .descname}[(]{.sig-paren}*[[xpath]{.pre}]{.n}*, *[[\*]{.pre}]{.o}[[processors]{.pre}]{.n}*, *[[re]{.pre}]{.n}[[=]{.pre}]{.o}[[None]{.pre}]{.default_value}*, *[[\*\*]{.pre}]{.o}[[kw]{.pre}]{.n}*[)]{.sig-paren}[[[\[source\]]{.pre}]{.viewcode-link}](_modules/itemloaders.html#ItemLoader.get_xpath){.reference .internal}[¶](#scrapy.loader.ItemLoader.get_xpath &quot;Permalink to this definition&quot;){.headerlink}

:   Similar to [[`ItemLoader.get_value()`{.xref .py .py-meth
    .docutils .literal
    .notranslate}]{.pre}](#scrapy.loader.ItemLoader.get_value &quot;scrapy.loader.ItemLoader.get_value&quot;){.reference
    .internal} but receives an XPath instead of a value, which is
    used to extract a list of unicode strings from the selector
    associated with this [[`ItemLoader`{.xref .py .py-class
    .docutils .literal
    .notranslate}]{.pre}](#scrapy.loader.ItemLoader &quot;scrapy.loader.ItemLoader&quot;){.reference
    .internal}.

    Parameters

    :   -   **xpath**
            ([*str*](https://docs.python.org/3/library/stdtypes.html#str &quot;(in Python v3.12)&quot;){.reference
            .external}) -- the XPath to extract data from

        -   **re**
            ([*str*](https://docs.python.org/3/library/stdtypes.html#str &quot;(in Python v3.12)&quot;){.reference
            .external} *or*
            [*Pattern*](https://docs.python.org/3/library/typing.html#typing.Pattern &quot;(in Python v3.12)&quot;){.reference
            .external}) -- a regular expression to use for
            extracting data from the selected XPath region

    Examples:

    ::: {.highlight-default .notranslate}
    ::: highlight
        # HTML snippet: &lt;p class=&quot;product-name&quot;&gt;Color TV&lt;/p&gt;
        loader.get_xpath('//p[@class=&quot;product-name&quot;]')
        # HTML snippet: &lt;p id=&quot;price&quot;&gt;the price is $1200&lt;/p&gt;
        loader.get_xpath('//p[@id=&quot;price&quot;]', TakeFirst(), re='the price is (.*)')
    :::
    :::

[[load_item]{.pre}]{.sig-name .descname}[(]{.sig-paren}[)]{.sig-paren}[[[\[source\]]{.pre}]{.viewcode-link}](_modules/itemloaders.html#ItemLoader.load_item){.reference .internal}[¶](#scrapy.loader.ItemLoader.load_item &quot;Permalink to this definition&quot;){.headerlink}

:   Populate the item with the data collected so far, and return it.
    The data collected is first passed through the [[output
    processors]{.xref .std
    .std-ref}](https://itemloaders.readthedocs.io/en/latest/processors.html#processors &quot;(in itemloaders)&quot;){.reference
    .external} to get the final value to assign to each item field.

[[nested_css]{.pre}]{.sig-name .descname}[(]{.sig-paren}*[[css]{.pre}]{.n}*, *[[\*\*]{.pre}]{.o}[[context]{.pre}]{.n}*[)]{.sig-paren}[[[\[source\]]{.pre}]{.viewcode-link}](_modules/itemloaders.html#ItemLoader.nested_css){.reference .internal}[¶](#scrapy.loader.ItemLoader.nested_css &quot;Permalink to this definition&quot;){.headerlink}

:   Create a nested loader with a css selector. The supplied
    selector is applied relative to selector associated with this
    [[`ItemLoader`{.xref .py .py-class .docutils .literal
    .notranslate}]{.pre}](#scrapy.loader.ItemLoader &quot;scrapy.loader.ItemLoader&quot;){.reference
    .internal}. The nested loader shares the item with the parent
    [[`ItemLoader`{.xref .py .py-class .docutils .literal
    .notranslate}]{.pre}](#scrapy.loader.ItemLoader &quot;scrapy.loader.ItemLoader&quot;){.reference
    .internal} so calls to [[`add_xpath()`{.xref .py .py-meth
    .docutils .literal
    .notranslate}]{.pre}](#scrapy.loader.ItemLoader.add_xpath &quot;scrapy.loader.ItemLoader.add_xpath&quot;){.reference
    .internal}, [[`add_value()`{.xref .py .py-meth .docutils
    .literal
    .notranslate}]{.pre}](#scrapy.loader.ItemLoader.add_value &quot;scrapy.loader.ItemLoader.add_value&quot;){.reference
    .internal}, [[`replace_value()`{.xref .py .py-meth .docutils
    .literal
    .notranslate}]{.pre}](#scrapy.loader.ItemLoader.replace_value &quot;scrapy.loader.ItemLoader.replace_value&quot;){.reference
    .internal}, etc. will behave as expected.

[[nested_xpath]{.pre}]{.sig-name .descname}[(]{.sig-paren}*[[xpath]{.pre}]{.n}*, *[[\*\*]{.pre}]{.o}[[context]{.pre}]{.n}*[)]{.sig-paren}[[[\[source\]]{.pre}]{.viewcode-link}](_modules/itemloaders.html#ItemLoader.nested_xpath){.reference .internal}[¶](#scrapy.loader.ItemLoader.nested_xpath &quot;Permalink to this definition&quot;){.headerlink}

:   Create a nested loader with an xpath selector. The supplied
    selector is applied relative to selector associated with this
    [[`ItemLoader`{.xref .py .py-class .docutils .literal
    .notranslate}]{.pre}](#scrapy.loader.ItemLoader &quot;scrapy.loader.ItemLoader&quot;){.reference
    .internal}. The nested loader shares the item with the parent
    [[`ItemLoader`{.xref .py .py-class .docutils .literal
    .notranslate}]{.pre}](#scrapy.loader.ItemLoader &quot;scrapy.loader.ItemLoader&quot;){.reference
    .internal} so calls to [[`add_xpath()`{.xref .py .py-meth
    .docutils .literal
    .notranslate}]{.pre}](#scrapy.loader.ItemLoader.add_xpath &quot;scrapy.loader.ItemLoader.add_xpath&quot;){.reference
    .internal}, [[`add_value()`{.xref .py .py-meth .docutils
    .literal
    .notranslate}]{.pre}](#scrapy.loader.ItemLoader.add_value &quot;scrapy.loader.ItemLoader.add_value&quot;){.reference
    .internal}, [[`replace_value()`{.xref .py .py-meth .docutils
    .literal
    .notranslate}]{.pre}](#scrapy.loader.ItemLoader.replace_value &quot;scrapy.loader.ItemLoader.replace_value&quot;){.reference
    .internal}, etc. will behave as expected.

[[replace_css]{.pre}]{.sig-name .descname}[(]{.sig-paren}*[[field_name]{.pre}]{.n}*, *[[css]{.pre}]{.n}*, *[[\*]{.pre}]{.o}[[processors]{.pre}]{.n}*, *[[re]{.pre}]{.n}[[=]{.pre}]{.o}[[None]{.pre}]{.default_value}*, *[[\*\*]{.pre}]{.o}[[kw]{.pre}]{.n}*[)]{.sig-paren}[[[\[source\]]{.pre}]{.viewcode-link}](_modules/itemloaders.html#ItemLoader.replace_css){.reference .internal}[¶](#scrapy.loader.ItemLoader.replace_css &quot;Permalink to this definition&quot;){.headerlink}

:   Similar to [[`add_css()`{.xref .py .py-meth .docutils .literal
    .notranslate}]{.pre}](#scrapy.loader.ItemLoader.add_css &quot;scrapy.loader.ItemLoader.add_css&quot;){.reference
    .internal} but replaces collected data instead of adding it.

[[replace_jmes]{.pre}]{.sig-name .descname}[(]{.sig-paren}*[[field_name]{.pre}]{.n}*, *[[jmes]{.pre}]{.n}*, *[[\*]{.pre}]{.o}[[processors]{.pre}]{.n}*, *[[re]{.pre}]{.n}[[=]{.pre}]{.o}[[None]{.pre}]{.default_value}*, *[[\*\*]{.pre}]{.o}[[kw]{.pre}]{.n}*[)]{.sig-paren}[[[\[source\]]{.pre}]{.viewcode-link}](_modules/itemloaders.html#ItemLoader.replace_jmes){.reference .internal}[¶](#scrapy.loader.ItemLoader.replace_jmes &quot;Permalink to this definition&quot;){.headerlink}

:   Similar to [[`add_jmes()`{.xref .py .py-meth .docutils .literal
    .notranslate}]{.pre}](#scrapy.loader.ItemLoader.add_jmes &quot;scrapy.loader.ItemLoader.add_jmes&quot;){.reference
    .internal} but replaces collected data instead of adding it.

[[replace_value]{.pre}]{.sig-name .descname}[(]{.sig-paren}*[[field_name]{.pre}]{.n}*, *[[value]{.pre}]{.n}*, *[[\*]{.pre}]{.o}[[processors]{.pre}]{.n}*, *[[re]{.pre}]{.n}[[=]{.pre}]{.o}[[None]{.pre}]{.default_value}*, *[[\*\*]{.pre}]{.o}[[kw]{.pre}]{.n}*[)]{.sig-paren}[[[\[source\]]{.pre}]{.viewcode-link}](_modules/itemloaders.html#ItemLoader.replace_value){.reference .internal}[¶](#scrapy.loader.ItemLoader.replace_value &quot;Permalink to this definition&quot;){.headerlink}

:   Similar to [[`add_value()`{.xref .py .py-meth .docutils .literal
    .notranslate}]{.pre}](#scrapy.loader.ItemLoader.add_value &quot;scrapy.loader.ItemLoader.add_value&quot;){.reference
    .internal} but replaces the collected data with the new value
    instead of adding it.

[[replace_xpath]{.pre}]{.sig-name .descname}[(]{.sig-paren}*[[field_name]{.pre}]{.n}*, *[[xpath]{.pre}]{.n}*, *[[\*]{.pre}]{.o}[[processors]{.pre}]{.n}*, *[[re]{.pre}]{.n}[[=]{.pre}]{.o}[[None]{.pre}]{.default_value}*, *[[\*\*]{.pre}]{.o}[[kw]{.pre}]{.n}*[)]{.sig-paren}[[[\[source\]]{.pre}]{.viewcode-link}](_modules/itemloaders.html#ItemLoader.replace_xpath){.reference .internal}[¶](#scrapy.loader.ItemLoader.replace_xpath &quot;Permalink to this definition&quot;){.headerlink}

:   Similar to [[`add_xpath()`{.xref .py .py-meth .docutils .literal
    .notranslate}]{.pre}](#scrapy.loader.ItemLoader.add_xpath &quot;scrapy.loader.ItemLoader.add_xpath&quot;){.reference
    .internal} but replaces collected data instead of adding it.
</code></pre>
<p>:::</p>
<p>::: {#nested-loaders .section}
[]{#topics-loaders-nested}</p>
<h4 id="nested-loadersheaderlink"><a class="header" href="#nested-loadersheaderlink">Nested Loaders<a href="#nested-loaders" title="Permalink to this heading">¶</a>{.headerlink}</a></h4>
<p>When parsing related values from a subsection of a document, it can be
useful to create nested loaders. Imagine you're extracting details from
a footer of a page that looks something like:</p>
<p>Example:</p>
<p>::: {.highlight-default .notranslate}
::: highlight
<footer>
<a class="social" href="https://facebook.com/whatever">Like Us</a>
<a class="social" href="https://twitter.com/whatever">Follow Us</a>
<a class="email" href="mailto:whatever@example.com">Email Us</a>
</footer>
:::
:::</p>
<p>Without nested loaders, you need to specify the full xpath (or css) for
each value that you wish to extract.</p>
<p>Example:</p>
<p>::: {.highlight-python .notranslate}
::: highlight
loader = ItemLoader(item=Item())
# load stuff not in the footer
loader.add_xpath(&quot;social&quot;, '//footer/a[@class = &quot;social&quot;]/@href')
loader.add_xpath(&quot;email&quot;, '//footer/a[@class = &quot;email&quot;]/@href')
loader.load_item()
:::
:::</p>
<p>Instead, you can create a nested loader with the footer selector and add
values relative to the footer. The functionality is the same but you
avoid repeating the footer selector.</p>
<p>Example:</p>
<p>::: {.highlight-python .notranslate}
::: highlight
loader = ItemLoader(item=Item())
# load stuff not in the footer
footer_loader = loader.nested_xpath(&quot;//footer&quot;)
footer_loader.add_xpath(&quot;social&quot;, 'a[@class = &quot;social&quot;]/@href')
footer_loader.add_xpath(&quot;email&quot;, 'a[@class = &quot;email&quot;]/@href')
# no need to call footer_loader.load_item()
loader.load_item()
:::
:::</p>
<p>You can nest loaders arbitrarily and they work with either xpath or css
selectors. As a general guideline, use nested loaders when they make
your code simpler but do not go overboard with nesting or your parser
can become difficult to read.
:::</p>
<p>::: {#reusing-and-extending-item-loaders .section}
[]{#topics-loaders-extending}</p>
<h4 id="reusing-and-extending-item-loadersheaderlink"><a class="header" href="#reusing-and-extending-item-loadersheaderlink">Reusing and extending Item Loaders<a href="#reusing-and-extending-item-loaders" title="Permalink to this heading">¶</a>{.headerlink}</a></h4>
<p>As your project grows bigger and acquires more and more spiders,
maintenance becomes a fundamental problem, especially when you have to
deal with many different parsing rules for each spider, having a lot of
exceptions, but also wanting to reuse the common processors.</p>
<p>Item Loaders are designed to ease the maintenance burden of parsing
rules, without losing flexibility and, at the same time, providing a
convenient mechanism for extending and overriding them. For this reason
Item Loaders support traditional Python class inheritance for dealing
with differences of specific spiders (or groups of spiders).</p>
<p>Suppose, for example, that some particular site encloses their product
names in three dashes (e.g. [<code>---Plasma</code>{.docutils .literal
.notranslate}]{.pre}<code> </code>{.docutils .literal
.notranslate}[<code>TV---</code>{.docutils .literal .notranslate}]{.pre}) and you
don't want to end up scraping those dashes in the final product names.</p>
<p>Here's how you can remove those dashes by reusing and extending the
default Product Item Loader ([<code>ProductLoader</code>{.docutils .literal
.notranslate}]{.pre}):</p>
<p>::: {.highlight-python .notranslate}
::: highlight
from itemloaders.processors import MapCompose
from myproject.ItemLoaders import ProductLoader</p>
<pre><code>def strip_dashes(x):
    return x.strip(&quot;-&quot;)


class SiteSpecificLoader(ProductLoader):
    name_in = MapCompose(strip_dashes, ProductLoader.name_in)
</code></pre>
<p>:::
:::</p>
<p>Another case where extending Item Loaders can be very helpful is when
you have multiple source formats, for example XML and HTML. In the XML
version you may want to remove [<code>CDATA</code>{.docutils .literal
.notranslate}]{.pre} occurrences. Here's an example of how to do it:</p>
<p>::: {.highlight-python .notranslate}
::: highlight
from itemloaders.processors import MapCompose
from myproject.ItemLoaders import ProductLoader
from myproject.utils.xml import remove_cdata</p>
<pre><code>class XmlProductLoader(ProductLoader):
    name_in = MapCompose(remove_cdata, ProductLoader.name_in)
</code></pre>
<p>:::
:::</p>
<p>And that's how you typically extend input processors.</p>
<p>As for output processors, it is more common to declare them in the field
metadata, as they usually depend only on the field and not on each
specific site parsing rule (as input processors do). See also:
<a href="#topics-loaders-processors-declaring">[Declaring Input and Output Processors]{.std
.std-ref}</a>{.hoverxref .tooltip
.reference .internal}.</p>
<p>There are many other possible ways to extend, inherit and override your
Item Loaders, and different Item Loaders hierarchies may fit better for
different projects. Scrapy only provides the mechanism; it doesn't
impose any specific organization of your Loaders collection - that's up
to you and your project's needs.
:::
:::</p>
<p>[]{#document-topics/shell}</p>
<p>::: {#scrapy-shell .section}
[]{#topics-shell}</p>
<h3 id="scrapy-shellheaderlink"><a class="header" href="#scrapy-shellheaderlink">Scrapy shell<a href="#scrapy-shell" title="Permalink to this heading">¶</a>{.headerlink}</a></h3>
<p>The Scrapy shell is an interactive shell where you can try and debug
your scraping code very quickly, without having to run the spider. It's
meant to be used for testing data extraction code, but you can actually
use it for testing any kind of code as it is also a regular Python
shell.</p>
<p>The shell is used for testing XPath or CSS expressions and see how they
work and what data they extract from the web pages you're trying to
scrape. It allows you to interactively test your expressions while
you're writing your spider, without having to run the spider to test
every change.</p>
<p>Once you get familiarized with the Scrapy shell, you'll see that it's an
invaluable tool for developing and debugging your spiders.</p>
<p>::: {#configuring-the-shell .section}</p>
<h4 id="configuring-the-shellheaderlink"><a class="header" href="#configuring-the-shellheaderlink">Configuring the shell<a href="#configuring-the-shell" title="Permalink to this heading">¶</a>{.headerlink}</a></h4>
<p>If you have <a href="https://ipython.org/">IPython</a>{.reference .external}
installed, the Scrapy shell will use it (instead of the standard Python
console). The <a href="https://ipython.org/">IPython</a>{.reference .external}
console is much more powerful and provides smart auto-completion and
colorized output, among other things.</p>
<p>We highly recommend you install
<a href="https://ipython.org/">IPython</a>{.reference .external}, specially if
you're working on Unix systems (where
<a href="https://ipython.org/">IPython</a>{.reference .external} excels). See the
<a href="https://ipython.org/install.html">IPython installation
guide</a>{.reference .external} for more
info.</p>
<p>Scrapy also has support for
<a href="https://bpython-interpreter.org/">bpython</a>{.reference .external}, and
will try to use it where <a href="https://ipython.org/">IPython</a>{.reference
.external} is unavailable.</p>
<p>Through Scrapy's settings you can configure it to use any one of
[<code>ipython</code>{.docutils .literal .notranslate}]{.pre}, [<code>bpython</code>{.docutils
.literal .notranslate}]{.pre} or the standard [<code>python</code>{.docutils
.literal .notranslate}]{.pre} shell, regardless of which are installed.
This is done by setting the [<code>SCRAPY_PYTHON_SHELL</code>{.docutils .literal
.notranslate}]{.pre} environment variable; or by defining it in your
<a href="index.html#topics-config-settings">[scrapy.cfg]{.std
.std-ref}</a>{.hoverxref .tooltip
.reference .internal}:</p>
<p>::: {.highlight-default .notranslate}
::: highlight
[settings]
shell = bpython
:::
:::
:::</p>
<p>::: {#launch-the-shell .section}</p>
<h4 id="launch-the-shellheaderlink"><a class="header" href="#launch-the-shellheaderlink">Launch the shell<a href="#launch-the-shell" title="Permalink to this heading">¶</a>{.headerlink}</a></h4>
<p>To launch the Scrapy shell you can use the <a href="index.html#std-command-shell">[<code>shell</code>{.xref .std
.std-command .docutils .literal
.notranslate}]{.pre}</a>{.hoverxref .tooltip
.reference .internal} command like this:</p>
<p>::: {.highlight-default .notranslate}
::: highlight
scrapy shell <url>
:::
:::</p>
<p>Where the [<code>&lt;url&gt;</code>{.docutils .literal .notranslate}]{.pre} is the URL
you want to scrape.</p>
<p><a href="index.html#std-command-shell">[<code>shell</code>{.xref .std .std-command .docutils .literal
.notranslate}]{.pre}</a>{.hoverxref .tooltip
.reference .internal} also works for local files. This can be handy if
you want to play around with a local copy of a web page. <a href="index.html#std-command-shell">[<code>shell</code>{.xref
.std .std-command .docutils .literal
.notranslate}]{.pre}</a>{.hoverxref .tooltip
.reference .internal} understands the following syntaxes for local
files:</p>
<p>::: {.highlight-default .notranslate}
::: highlight
# UNIX-style
scrapy shell ./path/to/file.html
scrapy shell ../other/path/to/file.html
scrapy shell /absolute/path/to/file.html</p>
<pre><code># File URI
scrapy shell file:///absolute/path/to/file.html
</code></pre>
<p>:::
:::</p>
<p>::: {.admonition .note}
Note</p>
<p>When using relative file paths, be explicit and prepend them with
[<code>./</code>{.docutils .literal .notranslate}]{.pre} (or [<code>../</code>{.docutils
.literal .notranslate}]{.pre} when relevant). [<code>scrapy</code>{.docutils
.literal .notranslate}]{.pre}<code> </code>{.docutils .literal
.notranslate}[<code>shell</code>{.docutils .literal
.notranslate}]{.pre}<code> </code>{.docutils .literal
.notranslate}[<code>index.html</code>{.docutils .literal .notranslate}]{.pre} will
not work as one might expect (and this is by design, not a bug).</p>
<p>Because <a href="index.html#std-command-shell">[<code>shell</code>{.xref .std .std-command .docutils .literal
.notranslate}]{.pre}</a>{.hoverxref .tooltip
.reference .internal} favors HTTP URLs over File URIs, and
[<code>index.html</code>{.docutils .literal .notranslate}]{.pre} being
syntactically similar to [<code>example.com</code>{.docutils .literal
.notranslate}]{.pre}, <a href="index.html#std-command-shell">[<code>shell</code>{.xref .std .std-command .docutils
.literal .notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal} will treat [<code>index.html</code>{.docutils
.literal .notranslate}]{.pre} as a domain name and trigger a DNS lookup
error:</p>
<p>::: {.highlight-default .notranslate}
::: highlight
$ scrapy shell index.html
[ ... scrapy shell starts ... ]
[ ... traceback ... ]
twisted.internet.error.DNSLookupError: DNS lookup failed:
address 'index.html' not found: [Errno -5] No address associated with hostname.
:::
:::</p>
<p><a href="index.html#std-command-shell">[<code>shell</code>{.xref .std .std-command .docutils .literal
.notranslate}]{.pre}</a>{.hoverxref .tooltip
.reference .internal} will not test beforehand if a file called
[<code>index.html</code>{.docutils .literal .notranslate}]{.pre} exists in the
current directory. Again, be explicit.
:::
:::</p>
<p>::: {#using-the-shell .section}</p>
<h4 id="using-the-shellheaderlink"><a class="header" href="#using-the-shellheaderlink">Using the shell<a href="#using-the-shell" title="Permalink to this heading">¶</a>{.headerlink}</a></h4>
<p>The Scrapy shell is just a regular Python console (or
<a href="https://ipython.org/">IPython</a>{.reference .external} console if you
have it available) which provides some additional shortcut functions for
convenience.</p>
<p>::: {#available-shortcuts .section}</p>
<h5 id="available-shortcutsheaderlink"><a class="header" href="#available-shortcutsheaderlink">Available Shortcuts<a href="#available-shortcuts" title="Permalink to this heading">¶</a>{.headerlink}</a></h5>
<ul>
<li>
<p>[<code>shelp()</code>{.docutils .literal .notranslate}]{.pre} - print a help
with the list of available objects and shortcuts</p>
</li>
<li>
<p>[<code>fetch(url[,</code>{.docutils .literal .notranslate}]{.pre}<code> </code>{.docutils
.literal .notranslate}[<code>redirect=True])</code>{.docutils .literal
.notranslate}]{.pre} - fetch a new response from the given URL and
update all related objects accordingly. You can optionally ask for
HTTP 3xx redirections to not be followed by passing
[<code>redirect=False</code>{.docutils .literal .notranslate}]{.pre}</p>
</li>
<li>
<p>[<code>fetch(request)</code>{.docutils .literal .notranslate}]{.pre} - fetch a
new response from the given request and update all related objects
accordingly.</p>
</li>
<li>
<p>[<code>view(response)</code>{.docutils .literal .notranslate}]{.pre} - open the
given response in your local web browser, for inspection. This will
add a <a href="https://developer.mozilla.org/en-US/docs/Web/HTML/Element/base">&lt;base&gt;
tag</a>{.reference
.external} to the response body in order for external links (such as
images and style sheets) to display properly. Note, however, that
this will create a temporary file in your computer, which won't be
removed automatically.
:::</p>
</li>
</ul>
<p>::: {#available-scrapy-objects .section}</p>
<h5 id="available-scrapy-objectsheaderlink"><a class="header" href="#available-scrapy-objectsheaderlink">Available Scrapy objects<a href="#available-scrapy-objects" title="Permalink to this heading">¶</a>{.headerlink}</a></h5>
<p>The Scrapy shell automatically creates some convenient objects from the
downloaded page, like the <a href="index.html#scrapy.http.Response" title="scrapy.http.Response">[<code>Response</code>{.xref .py .py-class .docutils
.literal
.notranslate}]{.pre}</a>{.reference
.internal} object and the [<code>Selector</code>{.xref .py .py-class .docutils
.literal .notranslate}]{.pre} objects (for both HTML and XML content).</p>
<p>Those objects are:</p>
<ul>
<li>
<p>[<code>crawler</code>{.docutils .literal .notranslate}]{.pre} - the current
<a href="index.html#scrapy.crawler.Crawler" title="scrapy.crawler.Crawler">[<code>Crawler</code>{.xref .py .py-class .docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal} object.</p>
</li>
<li>
<p>[<code>spider</code>{.docutils .literal .notranslate}]{.pre} - the Spider which
is known to handle the URL, or a <a href="index.html#scrapy.Spider" title="scrapy.Spider">[<code>Spider</code>{.xref .py .py-class
.docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal} object if there is no spider found for the current URL</p>
</li>
<li>
<p>[<code>request</code>{.docutils .literal .notranslate}]{.pre} - a
[<code>Request</code>{.xref .py .py-class .docutils .literal
.notranslate}]{.pre} object of the last fetched page. You can modify
this request using [<code>replace()</code>{.xref .py .py-meth .docutils
.literal .notranslate}]{.pre} or fetch a new request (without
leaving the shell) using the [<code>fetch</code>{.docutils .literal
.notranslate}]{.pre} shortcut.</p>
</li>
<li>
<p>[<code>response</code>{.docutils .literal .notranslate}]{.pre} - a
<a href="index.html#scrapy.http.Response" title="scrapy.http.Response">[<code>Response</code>{.xref .py .py-class .docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal} object containing the last fetched page</p>
</li>
<li>
<p>[<code>settings</code>{.docutils .literal .notranslate}]{.pre} - the current
<a href="index.html#topics-settings">[Scrapy settings]{.std
.std-ref}</a>{.hoverxref .tooltip
.reference .internal}
:::
:::</p>
</li>
</ul>
<p>::: {#example-of-shell-session .section}</p>
<h4 id="example-of-shell-sessionheaderlink"><a class="header" href="#example-of-shell-sessionheaderlink">Example of shell session<a href="#example-of-shell-session" title="Permalink to this heading">¶</a>{.headerlink}</a></h4>
<p>Here's an example of a typical shell session where we start by scraping
the <a href="https://scrapy.org">https://scrapy.org</a>{.reference .external} page,
and then proceed to scrape the
<a href="https://old.reddit.com/">https://old.reddit.com/</a>{.reference .external}
page. Finally, we modify the (Reddit) request method to POST and
re-fetch it getting an error. We end the session by typing Ctrl-D (in
Unix systems) or Ctrl-Z in Windows.</p>
<p>Keep in mind that the data extracted here may not be the same when you
try it, as those pages are not static and could have changed by the time
you test this. The only purpose of this example is to get you
familiarized with how the Scrapy shell works.</p>
<p>First, we launch the shell:</p>
<p>::: {.highlight-default .notranslate}
::: highlight
scrapy shell 'https://scrapy.org' --nolog
:::
:::</p>
<p>::: {.admonition .note}
Note</p>
<p>Remember to always enclose URLs in quotes when running the Scrapy shell
from the command line, otherwise URLs containing arguments (i.e. the
[<code>&amp;</code>{.docutils .literal .notranslate}]{.pre} character) will not work.</p>
<p>On Windows, use double quotes instead:</p>
<p>::: {.highlight-default .notranslate}
::: highlight
scrapy shell &quot;https://scrapy.org&quot; --nolog
:::
:::
:::</p>
<p>Then, the shell fetches the URL (using the Scrapy downloader) and prints
the list of available objects and useful shortcuts (you'll notice that
these lines all start with the [<code>[s]</code>{.docutils .literal
.notranslate}]{.pre} prefix):</p>
<p>::: {.highlight-default .notranslate}
::: highlight
[s] Available Scrapy objects:
[s]   scrapy     scrapy module (contains scrapy.Request, scrapy.Selector, etc)
[s]   crawler    &lt;scrapy.crawler.Crawler object at 0x7f07395dd690&gt;
[s]   item       {}
[s]   request    &lt;GET https://scrapy.org&gt;
[s]   response   &lt;200 https://scrapy.org/&gt;
[s]   settings   &lt;scrapy.settings.Settings object at 0x7f07395dd710&gt;
[s]   spider     &lt;DefaultSpider 'default' at 0x7f0735891690&gt;
[s] Useful shortcuts:
[s]   fetch(url[, redirect=True]) Fetch URL and update local objects (by default, redirects are followed)
[s]   fetch(req)                  Fetch a scrapy.Request and update local objects
[s]   shelp()           Shell help (print this help)
[s]   view(response)    View response in a browser</p>
<pre><code>&gt;&gt;&gt;
</code></pre>
<p>:::
:::</p>
<p>After that, we can start playing with the objects:</p>
<p>::: {.highlight-pycon .notranslate}
::: highlight
&gt;&gt;&gt; response.xpath(&quot;//title/text()&quot;).get()
'Scrapy | A Fast and Powerful Scraping and Web Crawling Framework'</p>
<pre><code>&gt;&gt;&gt; fetch(&quot;https://old.reddit.com/&quot;)

&gt;&gt;&gt; response.xpath(&quot;//title/text()&quot;).get()
'reddit: the front page of the internet'

&gt;&gt;&gt; request = request.replace(method=&quot;POST&quot;)

&gt;&gt;&gt; fetch(request)

&gt;&gt;&gt; response.status
404

&gt;&gt;&gt; from pprint import pprint

&gt;&gt;&gt; pprint(response.headers)
{'Accept-Ranges': ['bytes'],
'Cache-Control': ['max-age=0, must-revalidate'],
'Content-Type': ['text/html; charset=UTF-8'],
'Date': ['Thu, 08 Dec 2016 16:21:19 GMT'],
'Server': ['snooserv'],
'Set-Cookie': ['loid=KqNLou0V9SKMX4qb4n; Domain=reddit.com; Max-Age=63071999; Path=/; expires=Sat, 08-Dec-2018 16:21:19 GMT; secure',
                'loidcreated=2016-12-08T16%3A21%3A19.445Z; Domain=reddit.com; Max-Age=63071999; Path=/; expires=Sat, 08-Dec-2018 16:21:19 GMT; secure',
                'loid=vi0ZVe4NkxNWdlH7r7; Domain=reddit.com; Max-Age=63071999; Path=/; expires=Sat, 08-Dec-2018 16:21:19 GMT; secure',
                'loidcreated=2016-12-08T16%3A21%3A19.459Z; Domain=reddit.com; Max-Age=63071999; Path=/; expires=Sat, 08-Dec-2018 16:21:19 GMT; secure'],
'Vary': ['accept-encoding'],
'Via': ['1.1 varnish'],
'X-Cache': ['MISS'],
'X-Cache-Hits': ['0'],
'X-Content-Type-Options': ['nosniff'],
'X-Frame-Options': ['SAMEORIGIN'],
'X-Moose': ['majestic'],
'X-Served-By': ['cache-cdg8730-CDG'],
'X-Timer': ['S1481214079.394283,VS0,VE159'],
'X-Ua-Compatible': ['IE=edge'],
'X-Xss-Protection': ['1; mode=block']}
</code></pre>
<p>:::
:::
:::</p>
<p>::: {#invoking-the-shell-from-spiders-to-inspect-responses .section}
[]{#topics-shell-inspect-response}</p>
<h4 id="invoking-the-shell-from-spiders-to-inspect-responsesheaderlink"><a class="header" href="#invoking-the-shell-from-spiders-to-inspect-responsesheaderlink">Invoking the shell from spiders to inspect responses<a href="#invoking-the-shell-from-spiders-to-inspect-responses" title="Permalink to this heading">¶</a>{.headerlink}</a></h4>
<p>Sometimes you want to inspect the responses that are being processed in
a certain point of your spider, if only to check that response you
expect is getting there.</p>
<p>This can be achieved by using the
[<code>scrapy.shell.inspect_response</code>{.docutils .literal .notranslate}]{.pre}
function.</p>
<p>Here's an example of how you would call it from your spider:</p>
<p>::: {.highlight-python .notranslate}
::: highlight
import scrapy</p>
<pre><code>class MySpider(scrapy.Spider):
    name = &quot;myspider&quot;
    start_urls = [
        &quot;http://example.com&quot;,
        &quot;http://example.org&quot;,
        &quot;http://example.net&quot;,
    ]

    def parse(self, response):
        # We want to inspect one specific response.
        if &quot;.org&quot; in response.url:
            from scrapy.shell import inspect_response

            inspect_response(response, self)

        # Rest of parsing code.
</code></pre>
<p>:::
:::</p>
<p>When you run the spider, you will get something similar to this:</p>
<p>::: {.highlight-default .notranslate}
::: highlight
2014-01-23 17:48:31-0400 [scrapy.core.engine] DEBUG: Crawled (200) &lt;GET http://example.com&gt; (referer: None)
2014-01-23 17:48:31-0400 [scrapy.core.engine] DEBUG: Crawled (200) &lt;GET http://example.org&gt; (referer: None)
[s] Available Scrapy objects:
[s]   crawler    &lt;scrapy.crawler.Crawler object at 0x1e16b50&gt;
...</p>
<pre><code>&gt;&gt;&gt; response.url
'http://example.org'
</code></pre>
<p>:::
:::</p>
<p>Then, you can check if the extraction code is working:</p>
<p>::: {.highlight-pycon .notranslate}
::: highlight
&gt;&gt;&gt; response.xpath('//h1[@class=&quot;fn&quot;]')
[]
:::
:::</p>
<p>Nope, it doesn't. So you can open the response in your web browser and
see if it's the response you were expecting:</p>
<p>::: {.highlight-pycon .notranslate}
::: highlight
&gt;&gt;&gt; view(response)
True
:::
:::</p>
<p>Finally you hit Ctrl-D (or Ctrl-Z in Windows) to exit the shell and
resume the crawling:</p>
<p>::: {.highlight-default .notranslate}
::: highlight
&gt;&gt;&gt; ^D
2014-01-23 17:50:03-0400 [scrapy.core.engine] DEBUG: Crawled (200) &lt;GET http://example.net&gt; (referer: None)
...
:::
:::</p>
<p>Note that you can't use the [<code>fetch</code>{.docutils .literal
.notranslate}]{.pre} shortcut here since the Scrapy engine is blocked by
the shell. However, after you leave the shell, the spider will continue
crawling where it stopped, as shown above.
:::
:::</p>
<p>[]{#document-topics/item-pipeline}</p>
<p>::: {#item-pipeline .section}
[]{#topics-item-pipeline}</p>
<h3 id="item-pipelineheaderlink"><a class="header" href="#item-pipelineheaderlink">Item Pipeline<a href="#item-pipeline" title="Permalink to this heading">¶</a>{.headerlink}</a></h3>
<p>After an item has been scraped by a spider, it is sent to the Item
Pipeline which processes it through several components that are executed
sequentially.</p>
<p>Each item pipeline component (sometimes referred as just &quot;Item
Pipeline&quot;) is a Python class that implements a simple method. They
receive an item and perform an action over it, also deciding if the item
should continue through the pipeline or be dropped and no longer
processed.</p>
<p>Typical uses of item pipelines are:</p>
<ul>
<li>
<p>cleansing HTML data</p>
</li>
<li>
<p>validating scraped data (checking that the items contain certain
fields)</p>
</li>
<li>
<p>checking for duplicates (and dropping them)</p>
</li>
<li>
<p>storing the scraped item in a database</p>
</li>
</ul>
<p>::: {#writing-your-own-item-pipeline .section}</p>
<h4 id="writing-your-own-item-pipelineheaderlink"><a class="header" href="#writing-your-own-item-pipelineheaderlink">Writing your own item pipeline<a href="#writing-your-own-item-pipeline" title="Permalink to this heading">¶</a>{.headerlink}</a></h4>
<p>Each item pipeline component is a Python class that must implement the
following method:</p>
<p>[[process_item]{.pre}]{.sig-name .descname}[(]{.sig-paren}<em>[[self]{.pre}]{.n}</em>, <em>[[item]{.pre}]{.n}</em>, <em>[[spider]{.pre}]{.n}</em>[)]{.sig-paren}<a href="#process_item" title="Permalink to this definition">¶</a>{.headerlink}</p>
<p>:   This method is called for every item pipeline component.</p>
<pre><code>item is an [[item object]{.std
.std-ref}](index.html#item-types){.hoverxref .tooltip .reference
.internal}, see [[Supporting All Item Types]{.std
.std-ref}](index.html#supporting-item-types){.hoverxref .tooltip
.reference .internal}.

[[`process_item()`{.xref .py .py-meth .docutils .literal
.notranslate}]{.pre}](#process_item &quot;process_item&quot;){.reference
.internal} must either: return an [[item object]{.std
.std-ref}](index.html#item-types){.hoverxref .tooltip .reference
.internal}, return a [[`Deferred`{.xref .py .py-class .docutils
.literal
.notranslate}]{.pre}](https://docs.twisted.org/en/stable/api/twisted.internet.defer.Deferred.html &quot;(in Twisted)&quot;){.reference
.external} or raise a [[`DropItem`{.xref .py .py-exc .docutils
.literal
.notranslate}]{.pre}](index.html#scrapy.exceptions.DropItem &quot;scrapy.exceptions.DropItem&quot;){.reference
.internal} exception.

Dropped items are no longer processed by further pipeline
components.

Parameters

:   -   **item** ([[item object]{.std
        .std-ref}](index.html#item-types){.hoverxref .tooltip
        .reference .internal}) -- the scraped item

    -   **spider** ([[`Spider`{.xref .py .py-class .docutils
        .literal
        .notranslate}]{.pre}](index.html#scrapy.Spider &quot;scrapy.Spider&quot;){.reference
        .internal} object) -- the spider which scraped the item
</code></pre>
<p>Additionally, they may also implement the following methods:</p>
<p>[[open_spider]{.pre}]{.sig-name .descname}[(]{.sig-paren}<em>[[self]{.pre}]{.n}</em>, <em>[[spider]{.pre}]{.n}</em>[)]{.sig-paren}<a href="#open_spider" title="Permalink to this definition">¶</a>{.headerlink}</p>
<p>:   This method is called when the spider is opened.</p>
<pre><code>Parameters

:   **spider** ([[`Spider`{.xref .py .py-class .docutils .literal
    .notranslate}]{.pre}](index.html#scrapy.Spider &quot;scrapy.Spider&quot;){.reference
    .internal} object) -- the spider which was opened
</code></pre>
<pre><code class="language-{=html}">&lt;!-- --&gt;
</code></pre>
<p>[[close_spider]{.pre}]{.sig-name .descname}[(]{.sig-paren}<em>[[self]{.pre}]{.n}</em>, <em>[[spider]{.pre}]{.n}</em>[)]{.sig-paren}<a href="#close_spider" title="Permalink to this definition">¶</a>{.headerlink}</p>
<p>:   This method is called when the spider is closed.</p>
<pre><code>Parameters

:   **spider** ([[`Spider`{.xref .py .py-class .docutils .literal
    .notranslate}]{.pre}](index.html#scrapy.Spider &quot;scrapy.Spider&quot;){.reference
    .internal} object) -- the spider which was closed
</code></pre>
<pre><code class="language-{=html}">&lt;!-- --&gt;
</code></pre>
<p><em>[classmethod]{.pre}[ ]{.w}</em>[[from_crawler]{.pre}]{.sig-name .descname}[(]{.sig-paren}<em>[[cls]{.pre}]{.n}</em>, <em>[[crawler]{.pre}]{.n}</em>[)]{.sig-paren}<a href="#from_crawler" title="Permalink to this definition">¶</a>{.headerlink}</p>
<p>:   If present, this class method is called to create a pipeline
instance from a <a href="index.html#scrapy.crawler.Crawler" title="scrapy.crawler.Crawler">[<code>Crawler</code>{.xref .py .py-class .docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal}. It must return a new instance of the pipeline. Crawler
object provides access to all Scrapy core components like settings
and signals; it is a way for pipeline to access them and hook its
functionality into Scrapy.</p>
<pre><code>Parameters

:   **crawler** ([[`Crawler`{.xref .py .py-class .docutils .literal
    .notranslate}]{.pre}](index.html#scrapy.crawler.Crawler &quot;scrapy.crawler.Crawler&quot;){.reference
    .internal} object) -- crawler that uses this pipeline
</code></pre>
<p>:::</p>
<p>::: {#item-pipeline-example .section}</p>
<h4 id="item-pipeline-exampleheaderlink"><a class="header" href="#item-pipeline-exampleheaderlink">Item pipeline example<a href="#item-pipeline-example" title="Permalink to this heading">¶</a>{.headerlink}</a></h4>
<p>::: {#price-validation-and-dropping-items-with-no-prices .section}</p>
<h5 id="price-validation-and-dropping-items-with-no-pricesheaderlink"><a class="header" href="#price-validation-and-dropping-items-with-no-pricesheaderlink">Price validation and dropping items with no prices<a href="#price-validation-and-dropping-items-with-no-prices" title="Permalink to this heading">¶</a>{.headerlink}</a></h5>
<p>Let's take a look at the following hypothetical pipeline that adjusts
the [<code>price</code>{.docutils .literal .notranslate}]{.pre} attribute for those
items that do not include VAT ([<code>price_excludes_vat</code>{.docutils .literal
.notranslate}]{.pre} attribute), and drops those items which don't
contain a price:</p>
<p>::: {.highlight-python .notranslate}
::: highlight
from itemadapter import ItemAdapter
from scrapy.exceptions import DropItem</p>
<pre><code>class PricePipeline:
    vat_factor = 1.15

    def process_item(self, item, spider):
        adapter = ItemAdapter(item)
        if adapter.get(&quot;price&quot;):
            if adapter.get(&quot;price_excludes_vat&quot;):
                adapter[&quot;price&quot;] = adapter[&quot;price&quot;] * self.vat_factor
            return item
        else:
            raise DropItem(f&quot;Missing price in {item}&quot;)
</code></pre>
<p>:::
:::
:::</p>
<p>::: {#write-items-to-a-json-lines-file .section}</p>
<h5 id="write-items-to-a-json-lines-fileheaderlink"><a class="header" href="#write-items-to-a-json-lines-fileheaderlink">Write items to a JSON lines file<a href="#write-items-to-a-json-lines-file" title="Permalink to this heading">¶</a>{.headerlink}</a></h5>
<p>The following pipeline stores all scraped items (from all spiders) into
a single [<code>items.jsonl</code>{.docutils .literal .notranslate}]{.pre} file,
containing one item per line serialized in JSON format:</p>
<p>::: {.highlight-python .notranslate}
::: highlight
import json</p>
<pre><code>from itemadapter import ItemAdapter


class JsonWriterPipeline:
    def open_spider(self, spider):
        self.file = open(&quot;items.jsonl&quot;, &quot;w&quot;)

    def close_spider(self, spider):
        self.file.close()

    def process_item(self, item, spider):
        line = json.dumps(ItemAdapter(item).asdict()) + &quot;\n&quot;
        self.file.write(line)
        return item
</code></pre>
<p>:::
:::</p>
<p>::: {.admonition .note}
Note</p>
<p>The purpose of JsonWriterPipeline is just to introduce how to write item
pipelines. If you really want to store all scraped items into a JSON
file you should use the <a href="index.html#topics-feed-exports">[Feed exports]{.std
.std-ref}</a>{.hoverxref .tooltip
.reference .internal}.
:::
:::</p>
<p>::: {#write-items-to-mongodb .section}</p>
<h5 id="write-items-to-mongodbheaderlink"><a class="header" href="#write-items-to-mongodbheaderlink">Write items to MongoDB<a href="#write-items-to-mongodb" title="Permalink to this heading">¶</a>{.headerlink}</a></h5>
<p>In this example we'll write items to
<a href="https://www.mongodb.com/">MongoDB</a>{.reference .external} using
<a href="https://api.mongodb.com/python/current/">pymongo</a>{.reference
.external}. MongoDB address and database name are specified in Scrapy
settings; MongoDB collection is named after item class.</p>
<p>The main point of this example is to show how to use
<a href="#from_crawler" title="from_crawler">[<code>from_crawler()</code>{.xref .py .py-meth .docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal} method and how to clean up the resources properly.</p>
<p>::: {.highlight-python .notranslate}
::: highlight
import pymongo
from itemadapter import ItemAdapter</p>
<pre><code>class MongoPipeline:
    collection_name = &quot;scrapy_items&quot;

    def __init__(self, mongo_uri, mongo_db):
        self.mongo_uri = mongo_uri
        self.mongo_db = mongo_db

    @classmethod
    def from_crawler(cls, crawler):
        return cls(
            mongo_uri=crawler.settings.get(&quot;MONGO_URI&quot;),
            mongo_db=crawler.settings.get(&quot;MONGO_DATABASE&quot;, &quot;items&quot;),
        )

    def open_spider(self, spider):
        self.client = pymongo.MongoClient(self.mongo_uri)
        self.db = self.client[self.mongo_db]

    def close_spider(self, spider):
        self.client.close()

    def process_item(self, item, spider):
        self.db[self.collection_name].insert_one(ItemAdapter(item).asdict())
        return item
</code></pre>
<p>:::
:::
:::</p>
<p>::: {#take-screenshot-of-item .section}
[]{#screenshotpipeline}</p>
<h5 id="take-screenshot-of-itemheaderlink"><a class="header" href="#take-screenshot-of-itemheaderlink">Take screenshot of item<a href="#take-screenshot-of-item" title="Permalink to this heading">¶</a>{.headerlink}</a></h5>
<p>This example demonstrates how to use <a href="index.html#document-topics/coroutines">[coroutine
syntax]{.doc}</a>{.reference
.internal} in the <a href="#process_item" title="process_item">[<code>process_item()</code>{.xref .py .py-meth .docutils
.literal .notranslate}]{.pre}</a>{.reference
.internal} method.</p>
<p>This item pipeline makes a request to a locally-running instance of
<a href="https://splash.readthedocs.io/en/stable/">Splash</a>{.reference .external}
to render a screenshot of the item URL. After the request response is
downloaded, the item pipeline saves the screenshot to a file and adds
the filename to the item.</p>
<p>::: {.highlight-python .notranslate}
::: highlight
import hashlib
from pathlib import Path
from urllib.parse import quote</p>
<pre><code>import scrapy
from itemadapter import ItemAdapter
from scrapy.http.request import NO_CALLBACK
from scrapy.utils.defer import maybe_deferred_to_future


class ScreenshotPipeline:
    &quot;&quot;&quot;Pipeline that uses Splash to render screenshot of
    every Scrapy item.&quot;&quot;&quot;

    SPLASH_URL = &quot;http://localhost:8050/render.png?url={}&quot;

    async def process_item(self, item, spider):
        adapter = ItemAdapter(item)
        encoded_item_url = quote(adapter[&quot;url&quot;])
        screenshot_url = self.SPLASH_URL.format(encoded_item_url)
        request = scrapy.Request(screenshot_url, callback=NO_CALLBACK)
        response = await maybe_deferred_to_future(
            spider.crawler.engine.download(request)
        )

        if response.status != 200:
            # Error happened, return item.
            return item

        # Save screenshot to file, filename will be hash of url.
        url = adapter[&quot;url&quot;]
        url_hash = hashlib.md5(url.encode(&quot;utf8&quot;)).hexdigest()
        filename = f&quot;{url_hash}.png&quot;
        Path(filename).write_bytes(response.body)

        # Store filename in item.
        adapter[&quot;screenshot_filename&quot;] = filename
        return item
</code></pre>
<p>:::
:::
:::</p>
<p>::: {#duplicates-filter .section}</p>
<h5 id="duplicates-filterheaderlink"><a class="header" href="#duplicates-filterheaderlink">Duplicates filter<a href="#duplicates-filter" title="Permalink to this heading">¶</a>{.headerlink}</a></h5>
<p>A filter that looks for duplicate items, and drops those items that were
already processed. Let's say that our items have a unique id, but our
spider returns multiples items with the same id:</p>
<p>::: {.highlight-python .notranslate}
::: highlight
from itemadapter import ItemAdapter
from scrapy.exceptions import DropItem</p>
<pre><code>class DuplicatesPipeline:
    def __init__(self):
        self.ids_seen = set()

    def process_item(self, item, spider):
        adapter = ItemAdapter(item)
        if adapter[&quot;id&quot;] in self.ids_seen:
            raise DropItem(f&quot;Duplicate item found: {item!r}&quot;)
        else:
            self.ids_seen.add(adapter[&quot;id&quot;])
            return item
</code></pre>
<p>:::
:::
:::
:::</p>
<p>::: {#activating-an-item-pipeline-component .section}</p>
<h4 id="activating-an-item-pipeline-componentheaderlink"><a class="header" href="#activating-an-item-pipeline-componentheaderlink">Activating an Item Pipeline component<a href="#activating-an-item-pipeline-component" title="Permalink to this heading">¶</a>{.headerlink}</a></h4>
<p>To activate an Item Pipeline component you must add its class to the
<a href="index.html#std-setting-ITEM_PIPELINES">[<code>ITEM_PIPELINES</code>{.xref .std .std-setting .docutils .literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal} setting, like in the following example:</p>
<p>::: {.highlight-python .notranslate}
::: highlight
ITEM_PIPELINES = {
&quot;myproject.pipelines.PricePipeline&quot;: 300,
&quot;myproject.pipelines.JsonWriterPipeline&quot;: 800,
}
:::
:::</p>
<p>The integer values you assign to classes in this setting determine the
order in which they run: items go through from lower valued to higher
valued classes. It's customary to define these numbers in the 0-1000
range.
:::
:::</p>
<p>[]{#document-topics/feed-exports}</p>
<p>::: {#feed-exports .section}
[]{#topics-feed-exports}</p>
<h3 id="feed-exportsheaderlink"><a class="header" href="#feed-exportsheaderlink">Feed exports<a href="#feed-exports" title="Permalink to this heading">¶</a>{.headerlink}</a></h3>
<p>One of the most frequently required features when implementing scrapers
is being able to store the scraped data properly and, quite often, that
means generating an &quot;export file&quot; with the scraped data (commonly called
&quot;export feed&quot;) to be consumed by other systems.</p>
<p>Scrapy provides this functionality out of the box with the Feed Exports,
which allows you to generate feeds with the scraped items, using
multiple serialization formats and storage backends.</p>
<p>::: {#serialization-formats .section}
[]{#topics-feed-format}</p>
<h4 id="serialization-formatsheaderlink"><a class="header" href="#serialization-formatsheaderlink">Serialization formats<a href="#serialization-formats" title="Permalink to this heading">¶</a>{.headerlink}</a></h4>
<p>For serializing the scraped data, the feed exports use the <a href="index.html#topics-exporters">[Item
exporters]{.std .std-ref}</a>{.hoverxref
.tooltip .reference .internal}. These formats are supported out of the
box:</p>
<ul>
<li>
<p><a href="#topics-feed-format-json">[JSON]{.std .std-ref}</a>{.hoverxref
.tooltip .reference .internal}</p>
</li>
<li>
<p><a href="#topics-feed-format-jsonlines">[JSON lines]{.std
.std-ref}</a>{.hoverxref .tooltip
.reference .internal}</p>
</li>
<li>
<p><a href="#topics-feed-format-csv">[CSV]{.std .std-ref}</a>{.hoverxref .tooltip
.reference .internal}</p>
</li>
<li>
<p><a href="#topics-feed-format-xml">[XML]{.std .std-ref}</a>{.hoverxref .tooltip
.reference .internal}</p>
</li>
</ul>
<p>But you can also extend the supported format through the
<a href="#std-setting-FEED_EXPORTERS">[<code>FEED_EXPORTERS</code>{.xref .std .std-setting .docutils .literal
.notranslate}]{.pre}</a>{.hoverxref .tooltip
.reference .internal} setting.</p>
<p>::: {#json .section}
[]{#topics-feed-format-json}</p>
<h5 id="jsonheaderlink"><a class="header" href="#jsonheaderlink">JSON<a href="#json" title="Permalink to this heading">¶</a>{.headerlink}</a></h5>
<ul>
<li>
<p>Value for the [<code>format</code>{.docutils .literal .notranslate}]{.pre} key
in the <a href="#std-setting-FEEDS">[<code>FEEDS</code>{.xref .std .std-setting .docutils .literal
.notranslate}]{.pre}</a>{.hoverxref .tooltip
.reference .internal} setting: [<code>json</code>{.docutils .literal
.notranslate}]{.pre}</p>
</li>
<li>
<p>Exporter used: <a href="index.html#scrapy.exporters.JsonItemExporter" title="scrapy.exporters.JsonItemExporter">[<code>JsonItemExporter</code>{.xref .py .py-class .docutils
.literal
.notranslate}]{.pre}</a>{.reference
.internal}</p>
</li>
<li>
<p>See <a href="index.html#json-with-large-data">[this warning]{.std
.std-ref}</a>{.hoverxref .tooltip
.reference .internal} if you're using JSON with large feeds.
:::</p>
</li>
</ul>
<p>::: {#json-lines .section}
[]{#topics-feed-format-jsonlines}</p>
<h5 id="json-linesheaderlink"><a class="header" href="#json-linesheaderlink">JSON lines<a href="#json-lines" title="Permalink to this heading">¶</a>{.headerlink}</a></h5>
<ul>
<li>
<p>Value for the [<code>format</code>{.docutils .literal .notranslate}]{.pre} key
in the <a href="#std-setting-FEEDS">[<code>FEEDS</code>{.xref .std .std-setting .docutils .literal
.notranslate}]{.pre}</a>{.hoverxref .tooltip
.reference .internal} setting: [<code>jsonlines</code>{.docutils .literal
.notranslate}]{.pre}</p>
</li>
<li>
<p>Exporter used: <a href="index.html#scrapy.exporters.JsonLinesItemExporter" title="scrapy.exporters.JsonLinesItemExporter">[<code>JsonLinesItemExporter</code>{.xref .py .py-class
.docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal}
:::</p>
</li>
</ul>
<p>::: {#csv .section}
[]{#topics-feed-format-csv}</p>
<h5 id="csvheaderlink"><a class="header" href="#csvheaderlink">CSV<a href="#csv" title="Permalink to this heading">¶</a>{.headerlink}</a></h5>
<ul>
<li>
<p>Value for the [<code>format</code>{.docutils .literal .notranslate}]{.pre} key
in the <a href="#std-setting-FEEDS">[<code>FEEDS</code>{.xref .std .std-setting .docutils .literal
.notranslate}]{.pre}</a>{.hoverxref .tooltip
.reference .internal} setting: [<code>csv</code>{.docutils .literal
.notranslate}]{.pre}</p>
</li>
<li>
<p>Exporter used: <a href="index.html#scrapy.exporters.CsvItemExporter" title="scrapy.exporters.CsvItemExporter">[<code>CsvItemExporter</code>{.xref .py .py-class .docutils
.literal
.notranslate}]{.pre}</a>{.reference
.internal}</p>
</li>
<li>
<p>To specify columns to export, their order and their column names,
use <a href="#std-setting-FEED_EXPORT_FIELDS">[<code>FEED_EXPORT_FIELDS</code>{.xref .std .std-setting .docutils
.literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal}. Other feed exporters can also use
this option, but it is important for CSV because unlike many other
export formats CSV uses a fixed header.
:::</p>
</li>
</ul>
<p>::: {#xml .section}
[]{#topics-feed-format-xml}</p>
<h5 id="xmlheaderlink"><a class="header" href="#xmlheaderlink">XML<a href="#xml" title="Permalink to this heading">¶</a>{.headerlink}</a></h5>
<ul>
<li>
<p>Value for the [<code>format</code>{.docutils .literal .notranslate}]{.pre} key
in the <a href="#std-setting-FEEDS">[<code>FEEDS</code>{.xref .std .std-setting .docutils .literal
.notranslate}]{.pre}</a>{.hoverxref .tooltip
.reference .internal} setting: [<code>xml</code>{.docutils .literal
.notranslate}]{.pre}</p>
</li>
<li>
<p>Exporter used: <a href="index.html#scrapy.exporters.XmlItemExporter" title="scrapy.exporters.XmlItemExporter">[<code>XmlItemExporter</code>{.xref .py .py-class .docutils
.literal
.notranslate}]{.pre}</a>{.reference
.internal}
:::</p>
</li>
</ul>
<p>::: {#pickle .section}
[]{#topics-feed-format-pickle}</p>
<h5 id="pickleheaderlink"><a class="header" href="#pickleheaderlink">Pickle<a href="#pickle" title="Permalink to this heading">¶</a>{.headerlink}</a></h5>
<ul>
<li>
<p>Value for the [<code>format</code>{.docutils .literal .notranslate}]{.pre} key
in the <a href="#std-setting-FEEDS">[<code>FEEDS</code>{.xref .std .std-setting .docutils .literal
.notranslate}]{.pre}</a>{.hoverxref .tooltip
.reference .internal} setting: [<code>pickle</code>{.docutils .literal
.notranslate}]{.pre}</p>
</li>
<li>
<p>Exporter used: <a href="index.html#scrapy.exporters.PickleItemExporter" title="scrapy.exporters.PickleItemExporter">[<code>PickleItemExporter</code>{.xref .py .py-class .docutils
.literal
.notranslate}]{.pre}</a>{.reference
.internal}
:::</p>
</li>
</ul>
<p>::: {#marshal .section}
[]{#topics-feed-format-marshal}</p>
<h5 id="marshalheaderlink"><a class="header" href="#marshalheaderlink">Marshal<a href="#marshal" title="Permalink to this heading">¶</a>{.headerlink}</a></h5>
<ul>
<li>
<p>Value for the [<code>format</code>{.docutils .literal .notranslate}]{.pre} key
in the <a href="#std-setting-FEEDS">[<code>FEEDS</code>{.xref .std .std-setting .docutils .literal
.notranslate}]{.pre}</a>{.hoverxref .tooltip
.reference .internal} setting: [<code>marshal</code>{.docutils .literal
.notranslate}]{.pre}</p>
</li>
<li>
<p>Exporter used: <a href="index.html#scrapy.exporters.MarshalItemExporter" title="scrapy.exporters.MarshalItemExporter">[<code>MarshalItemExporter</code>{.xref .py .py-class .docutils
.literal
.notranslate}]{.pre}</a>{.reference
.internal}
:::
:::</p>
</li>
</ul>
<p>::: {#storages .section}
[]{#topics-feed-storage}</p>
<h4 id="storagesheaderlink"><a class="header" href="#storagesheaderlink">Storages<a href="#storages" title="Permalink to this heading">¶</a>{.headerlink}</a></h4>
<p>When using the feed exports you define where to store the feed using one
or multiple
<a href="https://en.wikipedia.org/wiki/Uniform_Resource_Identifier">URIs</a>{.reference
.external} (through the <a href="#std-setting-FEEDS">[<code>FEEDS</code>{.xref .std .std-setting .docutils
.literal .notranslate}]{.pre}</a>{.hoverxref .tooltip
.reference .internal} setting). The feed exports supports multiple
storage backend types which are defined by the URI scheme.</p>
<p>The storages backends supported out of the box are:</p>
<ul>
<li>
<p><a href="#topics-feed-storage-fs">[Local filesystem]{.std
.std-ref}</a>{.hoverxref .tooltip .reference
.internal}</p>
</li>
<li>
<p><a href="#topics-feed-storage-ftp">[FTP]{.std .std-ref}</a>{.hoverxref .tooltip
.reference .internal}</p>
</li>
<li>
<p><a href="#topics-feed-storage-s3">[S3]{.std .std-ref}</a>{.hoverxref .tooltip
.reference .internal} (requires
<a href="https://github.com/boto/boto3">boto3</a>{.reference .external})</p>
</li>
<li>
<p><a href="#topics-feed-storage-gcs">[Google Cloud Storage (GCS)]{.std
.std-ref}</a>{.hoverxref .tooltip .reference
.internal} (requires
<a href="https://cloud.google.com/storage/docs/reference/libraries#client-libraries-install-python">google-cloud-storage</a>{.reference
.external})</p>
</li>
<li>
<p><a href="#topics-feed-storage-stdout">[Standard output]{.std
.std-ref}</a>{.hoverxref .tooltip
.reference .internal}</p>
</li>
</ul>
<p>Some storage backends may be unavailable if the required external
libraries are not available. For example, the S3 backend is only
available if the <a href="https://github.com/boto/boto3">boto3</a>{.reference
.external} library is installed.
:::</p>
<p>::: {#storage-uri-parameters .section}
[]{#topics-feed-uri-params}</p>
<h4 id="storage-uri-parametersheaderlink"><a class="header" href="#storage-uri-parametersheaderlink">Storage URI parameters<a href="#storage-uri-parameters" title="Permalink to this heading">¶</a>{.headerlink}</a></h4>
<p>The storage URI can also contain parameters that get replaced when the
feed is being created. These parameters are:</p>
<ul>
<li>
<p>[<code>%(time)s</code>{.docutils .literal .notranslate}]{.pre} - gets replaced
by a timestamp when the feed is being created</p>
</li>
<li>
<p>[<code>%(name)s</code>{.docutils .literal .notranslate}]{.pre} - gets replaced
by the spider name</p>
</li>
</ul>
<p>Any other named parameter gets replaced by the spider attribute of the
same name. For example, [<code>%(site_id)s</code>{.docutils .literal
.notranslate}]{.pre} would get replaced by the
[<code>spider.site_id</code>{.docutils .literal .notranslate}]{.pre} attribute the
moment the feed is being created.</p>
<p>Here are some examples to illustrate:</p>
<ul>
<li>
<p>Store in FTP using one directory per spider:</p>
<ul>
<li>[<code>ftp://user:password@ftp.example.com/scraping/feeds/%(name)s/%(time)s.json</code>{.docutils
.literal .notranslate}]{.pre}</li>
</ul>
</li>
<li>
<p>Store in S3 using one directory per spider:</p>
<ul>
<li>[<code>s3://mybucket/scraping/feeds/%(name)s/%(time)s.json</code>{.docutils
.literal .notranslate}]{.pre}</li>
</ul>
</li>
</ul>
<p>::: {.admonition .note}
Note</p>
<p><a href="index.html#spiderargs">[Spider arguments]{.std .std-ref}</a>{.hoverxref
.tooltip .reference .internal} become spider attributes, hence they can
also be used as storage URI parameters.
:::
:::</p>
<p>::: {#storage-backends .section}
[]{#topics-feed-storage-backends}</p>
<h4 id="storage-backendsheaderlink"><a class="header" href="#storage-backendsheaderlink">Storage backends<a href="#storage-backends" title="Permalink to this heading">¶</a>{.headerlink}</a></h4>
<p>::: {#local-filesystem .section}
[]{#topics-feed-storage-fs}</p>
<h5 id="local-filesystemheaderlink"><a class="header" href="#local-filesystemheaderlink">Local filesystem<a href="#local-filesystem" title="Permalink to this heading">¶</a>{.headerlink}</a></h5>
<p>The feeds are stored in the local filesystem.</p>
<ul>
<li>
<p>URI scheme: [<code>file</code>{.docutils .literal .notranslate}]{.pre}</p>
</li>
<li>
<p>Example URI: [<code>file:///tmp/export.csv</code>{.docutils .literal
.notranslate}]{.pre}</p>
</li>
<li>
<p>Required external libraries: none</p>
</li>
</ul>
<p>Note that for the local filesystem storage (only) you can omit the
scheme if you specify an absolute path like [<code>/tmp/export.csv</code>{.docutils
.literal .notranslate}]{.pre} (Unix systems only). Alternatively you can
also use a <a href="https://docs.python.org/3/library/pathlib.html#pathlib.Path" title="(in Python v3.12)">[<code>pathlib.Path</code>{.xref .py .py-class .docutils .literal
.notranslate}]{.pre}</a>{.reference
.external} object.
:::</p>
<p>::: {#ftp .section}
[]{#topics-feed-storage-ftp}</p>
<h5 id="ftpheaderlink"><a class="header" href="#ftpheaderlink">FTP<a href="#ftp" title="Permalink to this heading">¶</a>{.headerlink}</a></h5>
<p>The feeds are stored in a FTP server.</p>
<ul>
<li>
<p>URI scheme: [<code>ftp</code>{.docutils .literal .notranslate}]{.pre}</p>
</li>
<li>
<p>Example URI:
[<code>ftp://user:pass@ftp.example.com/path/to/export.csv</code>{.docutils
.literal .notranslate}]{.pre}</p>
</li>
<li>
<p>Required external libraries: none</p>
</li>
</ul>
<p>FTP supports two different connection modes: <a href="https://stackoverflow.com/a/1699163">active or
passive</a>{.reference .external}.
Scrapy uses the passive connection mode by default. To use the active
connection mode instead, set the <a href="#std-setting-FEED_STORAGE_FTP_ACTIVE">[<code>FEED_STORAGE_FTP_ACTIVE</code>{.xref .std
.std-setting .docutils .literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal} setting to [<code>True</code>{.docutils .literal
.notranslate}]{.pre}.</p>
<p>The default value for the [<code>overwrite</code>{.docutils .literal
.notranslate}]{.pre} key in the <a href="#std-setting-FEEDS">[<code>FEEDS</code>{.xref .std .std-setting
.docutils .literal .notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal} for this storage backend is:
[<code>True</code>{.docutils .literal .notranslate}]{.pre}.</p>
<p>::: {.admonition .caution}
Caution</p>
<p>The value [<code>True</code>{.docutils .literal .notranslate}]{.pre} in
[<code>overwrite</code>{.docutils .literal .notranslate}]{.pre} will cause you to
lose the previous version of your data.
:::</p>
<p>This storage backend uses <a href="#delayed-file-delivery">[delayed file delivery]{.std
.std-ref}</a>{.hoverxref .tooltip .reference
.internal}.
:::</p>
<p>::: {#s3 .section}
[]{#topics-feed-storage-s3}</p>
<h5 id="s3headerlink"><a class="header" href="#s3headerlink">S3<a href="#s3" title="Permalink to this heading">¶</a>{.headerlink}</a></h5>
<p>The feeds are stored on <a href="https://aws.amazon.com/s3/">Amazon
S3</a>{.reference .external}.</p>
<ul>
<li>
<p>URI scheme: [<code>s3</code>{.docutils .literal .notranslate}]{.pre}</p>
</li>
<li>
<p>Example URIs:</p>
<ul>
<li>
<p>[<code>s3://mybucket/path/to/export.csv</code>{.docutils .literal
.notranslate}]{.pre}</p>
</li>
<li>
<p>[<code>s3://aws_key:aws_secret@mybucket/path/to/export.csv</code>{.docutils
.literal .notranslate}]{.pre}</p>
</li>
</ul>
</li>
<li>
<p>Required external libraries:
<a href="https://github.com/boto/boto3">boto3</a>{.reference .external} &gt;=
1.20.0</p>
</li>
</ul>
<p>The AWS credentials can be passed as user/password in the URI, or they
can be passed through the following settings:</p>
<ul>
<li>
<p><a href="index.html#std-setting-AWS_ACCESS_KEY_ID">[<code>AWS_ACCESS_KEY_ID</code>{.xref .std .std-setting .docutils .literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal}</p>
</li>
<li>
<p><a href="index.html#std-setting-AWS_SECRET_ACCESS_KEY">[<code>AWS_SECRET_ACCESS_KEY</code>{.xref .std .std-setting .docutils .literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal}</p>
</li>
<li>
<p><a href="index.html#std-setting-AWS_SESSION_TOKEN">[<code>AWS_SESSION_TOKEN</code>{.xref .std .std-setting .docutils .literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal} (only needed for <a href="https://docs.aws.amazon.com/general/latest/gr/aws-sec-cred-types.html#temporary-access-keys">temporary security
credentials</a>{.reference
.external})</p>
</li>
</ul>
<p>You can also define a custom ACL, custom endpoint, and region name for
exported feeds using these settings:</p>
<ul>
<li>
<p><a href="#std-setting-FEED_STORAGE_S3_ACL">[<code>FEED_STORAGE_S3_ACL</code>{.xref .std .std-setting .docutils .literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal}</p>
</li>
<li>
<p><a href="index.html#std-setting-AWS_ENDPOINT_URL">[<code>AWS_ENDPOINT_URL</code>{.xref .std .std-setting .docutils .literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal}</p>
</li>
<li>
<p><a href="index.html#std-setting-AWS_REGION_NAME">[<code>AWS_REGION_NAME</code>{.xref .std .std-setting .docutils .literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal}</p>
</li>
</ul>
<p>The default value for the [<code>overwrite</code>{.docutils .literal
.notranslate}]{.pre} key in the <a href="#std-setting-FEEDS">[<code>FEEDS</code>{.xref .std .std-setting
.docutils .literal .notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal} for this storage backend is:
[<code>True</code>{.docutils .literal .notranslate}]{.pre}.</p>
<p>::: {.admonition .caution}
Caution</p>
<p>The value [<code>True</code>{.docutils .literal .notranslate}]{.pre} in
[<code>overwrite</code>{.docutils .literal .notranslate}]{.pre} will cause you to
lose the previous version of your data.
:::</p>
<p>This storage backend uses <a href="#delayed-file-delivery">[delayed file delivery]{.std
.std-ref}</a>{.hoverxref .tooltip .reference
.internal}.
:::</p>
<p>::: {#google-cloud-storage-gcs .section}
[]{#topics-feed-storage-gcs}</p>
<h5 id="google-cloud-storage-gcsheaderlink"><a class="header" href="#google-cloud-storage-gcsheaderlink">Google Cloud Storage (GCS)<a href="#google-cloud-storage-gcs" title="Permalink to this heading">¶</a>{.headerlink}</a></h5>
<p>::: versionadded
[New in version 2.3.]{.versionmodified .added}
:::</p>
<p>The feeds are stored on <a href="https://cloud.google.com/storage/">Google Cloud
Storage</a>{.reference .external}.</p>
<ul>
<li>
<p>URI scheme: [<code>gs</code>{.docutils .literal .notranslate}]{.pre}</p>
</li>
<li>
<p>Example URIs:</p>
<ul>
<li>[<code>gs://mybucket/path/to/export.csv</code>{.docutils .literal
.notranslate}]{.pre}</li>
</ul>
</li>
<li>
<p>Required external libraries:
<a href="https://cloud.google.com/storage/docs/reference/libraries#client-libraries-install-python">google-cloud-storage</a>{.reference
.external}.</p>
</li>
</ul>
<p>For more information about authentication, please refer to <a href="https://cloud.google.com/docs/authentication/production">Google Cloud
documentation</a>{.reference
.external}.</p>
<p>You can set a <em>Project ID</em> and <em>Access Control List (ACL)</em> through the
following settings:</p>
<ul>
<li>
<p><a href="index.html#std-setting-FEED_STORAGE_GCS_ACL">[<code>FEED_STORAGE_GCS_ACL</code>{.xref .std .std-setting .docutils .literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal}</p>
</li>
<li>
<p><a href="index.html#std-setting-GCS_PROJECT_ID">[<code>GCS_PROJECT_ID</code>{.xref .std .std-setting .docutils .literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal}</p>
</li>
</ul>
<p>The default value for the [<code>overwrite</code>{.docutils .literal
.notranslate}]{.pre} key in the <a href="#std-setting-FEEDS">[<code>FEEDS</code>{.xref .std .std-setting
.docutils .literal .notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal} for this storage backend is:
[<code>True</code>{.docutils .literal .notranslate}]{.pre}.</p>
<p>::: {.admonition .caution}
Caution</p>
<p>The value [<code>True</code>{.docutils .literal .notranslate}]{.pre} in
[<code>overwrite</code>{.docutils .literal .notranslate}]{.pre} will cause you to
lose the previous version of your data.
:::</p>
<p>This storage backend uses <a href="#delayed-file-delivery">[delayed file delivery]{.std
.std-ref}</a>{.hoverxref .tooltip .reference
.internal}.
:::</p>
<p>::: {#standard-output .section}
[]{#topics-feed-storage-stdout}</p>
<h5 id="standard-outputheaderlink"><a class="header" href="#standard-outputheaderlink">Standard output<a href="#standard-output" title="Permalink to this heading">¶</a>{.headerlink}</a></h5>
<p>The feeds are written to the standard output of the Scrapy process.</p>
<ul>
<li>
<p>URI scheme: [<code>stdout</code>{.docutils .literal .notranslate}]{.pre}</p>
</li>
<li>
<p>Example URI: [<code>stdout:</code>{.docutils .literal .notranslate}]{.pre}</p>
</li>
<li>
<p>Required external libraries: none
:::</p>
</li>
</ul>
<p>::: {#delayed-file-delivery .section}
[]{#id1}</p>
<h5 id="delayed-file-deliveryheaderlink"><a class="header" href="#delayed-file-deliveryheaderlink">Delayed file delivery<a href="#delayed-file-delivery" title="Permalink to this heading">¶</a>{.headerlink}</a></h5>
<p>As indicated above, some of the described storage backends use delayed
file delivery.</p>
<p>These storage backends do not upload items to the feed URI as those
items are scraped. Instead, Scrapy writes items into a temporary local
file, and only once all the file contents have been written (i.e. at the
end of the crawl) is that file uploaded to the feed URI.</p>
<p>If you want item delivery to start earlier when using one of these
storage backends, use <a href="#std-setting-FEED_EXPORT_BATCH_ITEM_COUNT">[<code>FEED_EXPORT_BATCH_ITEM_COUNT</code>{.xref .std
.std-setting .docutils .literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal} to split the output items in multiple
files, with the specified maximum item count per file. That way, as soon
as a file reaches the maximum item count, that file is delivered to the
feed URI, allowing item delivery to start way before the end of the
crawl.
:::
:::</p>
<p>::: {#item-filtering .section}
[]{#item-filter}</p>
<h4 id="item-filteringheaderlink"><a class="header" href="#item-filteringheaderlink">Item filtering<a href="#item-filtering" title="Permalink to this heading">¶</a>{.headerlink}</a></h4>
<p>::: versionadded
[New in version 2.6.0.]{.versionmodified .added}
:::</p>
<p>You can filter items that you want to allow for a particular feed by
using the [<code>item_classes</code>{.docutils .literal .notranslate}]{.pre} option
in <a href="#feed-options">[feeds options]{.std .std-ref}</a>{.hoverxref .tooltip
.reference .internal}. Only items of the specified types will be added
to the feed.</p>
<p>The [<code>item_classes</code>{.docutils .literal .notranslate}]{.pre} option is
implemented by the <a href="#scrapy.extensions.feedexport.ItemFilter" title="scrapy.extensions.feedexport.ItemFilter">[<code>ItemFilter</code>{.xref .py .py-class .docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal} class, which is the default value of the
[<code>item_filter</code>{.docutils .literal .notranslate}]{.pre} <a href="#feed-options">[feed
option]{.std .std-ref}</a>{.hoverxref .tooltip .reference
.internal}.</p>
<p>You can create your own custom filtering class by implementing
<a href="#scrapy.extensions.feedexport.ItemFilter" title="scrapy.extensions.feedexport.ItemFilter">[<code>ItemFilter</code>{.xref .py .py-class .docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal}'s method [<code>accepts</code>{.docutils .literal .notranslate}]{.pre}
and taking [<code>feed_options</code>{.docutils .literal .notranslate}]{.pre} as an
argument.</p>
<p>For instance:</p>
<p>::: {.highlight-python .notranslate}
::: highlight
class MyCustomFilter:
def <strong>init</strong>(self, feed_options):
self.feed_options = feed_options</p>
<pre><code>    def accepts(self, item):
        if &quot;field1&quot; in item and item[&quot;field1&quot;] == &quot;expected_data&quot;:
            return True
        return False
</code></pre>
<p>:::
:::</p>
<p>You can assign your custom filtering class to the
[<code>item_filter</code>{.docutils .literal .notranslate}]{.pre} <a href="#feed-options">[option of a
feed]{.std .std-ref}</a>{.hoverxref .tooltip .reference
.internal}. See <a href="#std-setting-FEEDS">[<code>FEEDS</code>{.xref .std .std-setting .docutils .literal
.notranslate}]{.pre}</a>{.hoverxref .tooltip .reference
.internal} for examples.</p>
<p>::: {#itemfilter .section}</p>
<h5 id="itemfilterheaderlink"><a class="header" href="#itemfilterheaderlink">ItemFilter<a href="#itemfilter" title="Permalink to this heading">¶</a>{.headerlink}</a></h5>
<p><em>[class]{.pre}[ ]{.w}</em>[[scrapy.extensions.feedexport.]{.pre}]{.sig-prename .descclassname}[[ItemFilter]{.pre}]{.sig-name .descname}[(]{.sig-paren}<em>[[feed_options]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[<a href="https://docs.python.org/3/library/typing.html#typing.Optional" title="(in Python v3.12)">[Optional]{.pre}</a>{.reference .external}[[[]{.pre}]{.p}<a href="https://docs.python.org/3/library/stdtypes.html#dict" title="(in Python v3.12)">[dict]{.pre}</a>{.reference .external}[[]]{.pre}]{.p}]{.n}</em>[)]{.sig-paren}<a href="_modules/scrapy/extensions/feedexport.html#ItemFilter">[[[source]]{.pre}]{.viewcode-link}</a>{.reference .internal}<a href="#scrapy.extensions.feedexport.ItemFilter" title="Permalink to this definition">¶</a>{.headerlink}</p>
<p>:   This will be used by FeedExporter to decide if an item should be
allowed to be exported to a particular feed.</p>
<pre><code>Parameters

:   **feed_options**
    ([*dict*](https://docs.python.org/3/library/stdtypes.html#dict &quot;(in Python v3.12)&quot;){.reference
    .external}) -- feed specific options passed from FeedExporter

[[accepts]{.pre}]{.sig-name .descname}[(]{.sig-paren}*[[item]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[[[Any]{.pre}](https://docs.python.org/3/library/typing.html#typing.Any &quot;(in Python v3.12)&quot;){.reference .external}]{.n}*[)]{.sig-paren} [[→]{.sig-return-icon} [[[bool]{.pre}](https://docs.python.org/3/library/functions.html#bool &quot;(in Python v3.12)&quot;){.reference .external}]{.sig-return-typehint}]{.sig-return}[[[\[source\]]{.pre}]{.viewcode-link}](_modules/scrapy/extensions/feedexport.html#ItemFilter.accepts){.reference .internal}[¶](#scrapy.extensions.feedexport.ItemFilter.accepts &quot;Permalink to this definition&quot;){.headerlink}

:   Return [`True`{.docutils .literal .notranslate}]{.pre} if item
    should be exported or [`False`{.docutils .literal
    .notranslate}]{.pre} otherwise.

    Parameters

    :   **item** ([[Scrapy items]{.std
        .std-ref}](index.html#topics-items){.hoverxref .tooltip
        .reference .internal}) -- scraped item which user wants to
        check if is acceptable

    Returns

    :   True if accepted, False otherwise

    Return type

    :   [bool](https://docs.python.org/3/library/functions.html#bool &quot;(in Python v3.12)&quot;){.reference
        .external}
</code></pre>
<p>:::
:::</p>
<p>::: {#post-processing .section}
[]{#id2}</p>
<h4 id="post-processingheaderlink"><a class="header" href="#post-processingheaderlink">Post-Processing<a href="#post-processing" title="Permalink to this heading">¶</a>{.headerlink}</a></h4>
<p>::: versionadded
[New in version 2.6.0.]{.versionmodified .added}
:::</p>
<p>Scrapy provides an option to activate plugins to post-process feeds
before they are exported to feed storages. In addition to using
<a href="#builtin-plugins">[builtin plugins]{.std .std-ref}</a>{.hoverxref .tooltip
.reference .internal}, you can create your own <a href="#custom-plugins">[plugins]{.std
.std-ref}</a>{.hoverxref .tooltip .reference .internal}.</p>
<p>These plugins can be activated through the [<code>postprocessing</code>{.docutils
.literal .notranslate}]{.pre} option of a feed. The option must be
passed a list of post-processing plugins in the order you want the feed
to be processed. These plugins can be declared either as an import
string or with the imported class of the plugin. Parameters to plugins
can be passed through the feed options. See <a href="#feed-options">[feed options]{.std
.std-ref}</a>{.hoverxref .tooltip .reference .internal} for
examples.</p>
<p>::: {#built-in-plugins .section}
[]{#builtin-plugins}</p>
<h5 id="built-in-pluginsheaderlink"><a class="header" href="#built-in-pluginsheaderlink">Built-in Plugins<a href="#built-in-plugins" title="Permalink to this heading">¶</a>{.headerlink}</a></h5>
<p><em>[class]{.pre}[ ]{.w}</em>[[scrapy.extensions.postprocessing.]{.pre}]{.sig-prename .descclassname}[[GzipPlugin]{.pre}]{.sig-name .descname}[(]{.sig-paren}<em>[[file]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[<a href="https://docs.python.org/3/library/typing.html#typing.BinaryIO" title="(in Python v3.12)">[BinaryIO]{.pre}</a>{.reference .external}]{.n}</em>, <em>[[feed_options]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[<a href="https://docs.python.org/3/library/typing.html#typing.Dict" title="(in Python v3.12)">[Dict]{.pre}</a>{.reference .external}[[[]{.pre}]{.p}<a href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.12)">[str]{.pre}</a>{.reference .external}[[,]{.pre}]{.p}[ ]{.w}<a href="https://docs.python.org/3/library/typing.html#typing.Any" title="(in Python v3.12)">[Any]{.pre}</a>{.reference .external}[[]]{.pre}]{.p}]{.n}</em>[)]{.sig-paren}<a href="_modules/scrapy/extensions/postprocessing.html#GzipPlugin">[[[source]]{.pre}]{.viewcode-link}</a>{.reference .internal}<a href="#scrapy.extensions.postprocessing.GzipPlugin" title="Permalink to this definition">¶</a>{.headerlink}</p>
<p>:   Compresses received data using
<a href="https://en.wikipedia.org/wiki/Gzip">gzip</a>{.reference .external}.</p>
<pre><code>Accepted [`feed_options`{.docutils .literal .notranslate}]{.pre}
parameters:

-   gzip_compresslevel

-   gzip_mtime

-   gzip_filename

See [[`gzip.GzipFile`{.xref .py .py-class .docutils .literal
.notranslate}]{.pre}](https://docs.python.org/3/library/gzip.html#gzip.GzipFile &quot;(in Python v3.12)&quot;){.reference
.external} for more info about parameters.
</code></pre>
<pre><code class="language-{=html}">&lt;!-- --&gt;
</code></pre>
<p><em>[class]{.pre}[ ]{.w}</em>[[scrapy.extensions.postprocessing.]{.pre}]{.sig-prename .descclassname}[[LZMAPlugin]{.pre}]{.sig-name .descname}[(]{.sig-paren}<em>[[file]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[<a href="https://docs.python.org/3/library/typing.html#typing.BinaryIO" title="(in Python v3.12)">[BinaryIO]{.pre}</a>{.reference .external}]{.n}</em>, <em>[[feed_options]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[<a href="https://docs.python.org/3/library/typing.html#typing.Dict" title="(in Python v3.12)">[Dict]{.pre}</a>{.reference .external}[[[]{.pre}]{.p}<a href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.12)">[str]{.pre}</a>{.reference .external}[[,]{.pre}]{.p}[ ]{.w}<a href="https://docs.python.org/3/library/typing.html#typing.Any" title="(in Python v3.12)">[Any]{.pre}</a>{.reference .external}[[]]{.pre}]{.p}]{.n}</em>[)]{.sig-paren}<a href="_modules/scrapy/extensions/postprocessing.html#LZMAPlugin">[[[source]]{.pre}]{.viewcode-link}</a>{.reference .internal}<a href="#scrapy.extensions.postprocessing.LZMAPlugin" title="Permalink to this definition">¶</a>{.headerlink}</p>
<p>:   Compresses received data using
<a href="https://en.wikipedia.org/wiki/Lempel%E2%80%93Ziv%E2%80%93Markov_chain_algorithm">lzma</a>{.reference
.external}.</p>
<pre><code>Accepted [`feed_options`{.docutils .literal .notranslate}]{.pre}
parameters:

-   lzma_format

-   lzma_check

-   lzma_preset

-   lzma_filters

::: {.admonition .note}
Note

[`lzma_filters`{.docutils .literal .notranslate}]{.pre} cannot be
used in pypy version 7.3.1 and older.
:::

See [[`lzma.LZMAFile`{.xref .py .py-class .docutils .literal
.notranslate}]{.pre}](https://docs.python.org/3/library/lzma.html#lzma.LZMAFile &quot;(in Python v3.12)&quot;){.reference
.external} for more info about parameters.
</code></pre>
<pre><code class="language-{=html}">&lt;!-- --&gt;
</code></pre>
<p><em>[class]{.pre}[ ]{.w}</em>[[scrapy.extensions.postprocessing.]{.pre}]{.sig-prename .descclassname}[[Bz2Plugin]{.pre}]{.sig-name .descname}[(]{.sig-paren}<em>[[file]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[<a href="https://docs.python.org/3/library/typing.html#typing.BinaryIO" title="(in Python v3.12)">[BinaryIO]{.pre}</a>{.reference .external}]{.n}</em>, <em>[[feed_options]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[<a href="https://docs.python.org/3/library/typing.html#typing.Dict" title="(in Python v3.12)">[Dict]{.pre}</a>{.reference .external}[[[]{.pre}]{.p}<a href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.12)">[str]{.pre}</a>{.reference .external}[[,]{.pre}]{.p}[ ]{.w}<a href="https://docs.python.org/3/library/typing.html#typing.Any" title="(in Python v3.12)">[Any]{.pre}</a>{.reference .external}[[]]{.pre}]{.p}]{.n}</em>[)]{.sig-paren}<a href="_modules/scrapy/extensions/postprocessing.html#Bz2Plugin">[[[source]]{.pre}]{.viewcode-link}</a>{.reference .internal}<a href="#scrapy.extensions.postprocessing.Bz2Plugin" title="Permalink to this definition">¶</a>{.headerlink}</p>
<p>:   Compresses received data using
<a href="https://en.wikipedia.org/wiki/Bzip2">bz2</a>{.reference .external}.</p>
<pre><code>Accepted [`feed_options`{.docutils .literal .notranslate}]{.pre}
parameters:

-   bz2_compresslevel

See [[`bz2.BZ2File`{.xref .py .py-class .docutils .literal
.notranslate}]{.pre}](https://docs.python.org/3/library/bz2.html#bz2.BZ2File &quot;(in Python v3.12)&quot;){.reference
.external} for more info about parameters.
</code></pre>
<p>:::</p>
<p>::: {#custom-plugins .section}
[]{#id3}</p>
<h5 id="custom-pluginsheaderlink"><a class="header" href="#custom-pluginsheaderlink">Custom Plugins<a href="#custom-plugins" title="Permalink to this heading">¶</a>{.headerlink}</a></h5>
<p>Each plugin is a class that must implement the following methods:</p>
<p>[[__init__]{.pre}]{.sig-name .descname}[(]{.sig-paren}<em>[[self]{.pre}]{.n}</em>, <em>[[file]{.pre}]{.n}</em>, <em>[[feed_options]{.pre}]{.n}</em>[)]{.sig-paren}<a href="#init__" title="Permalink to this definition">¶</a>{.headerlink}</p>
<p>:   Initialize the plugin.</p>
<pre><code>Parameters

:   -   **file** -- file-like object having at least the write, tell
        and close methods implemented

    -   **feed_options** ([[`dict`{.xref .py .py-class .docutils
        .literal
        .notranslate}]{.pre}](https://docs.python.org/3/library/stdtypes.html#dict &quot;(in Python v3.12)&quot;){.reference
        .external}) -- feed-specific [[options]{.std
        .std-ref}](#feed-options){.hoverxref .tooltip .reference
        .internal}
</code></pre>
<pre><code class="language-{=html}">&lt;!-- --&gt;
</code></pre>
<p>[[write]{.pre}]{.sig-name .descname}[(]{.sig-paren}<em>[[self]{.pre}]{.n}</em>, <em>[[data]{.pre}]{.n}</em>[)]{.sig-paren}<a href="#write" title="Permalink to this definition">¶</a>{.headerlink}</p>
<p>:   Process and write data (<a href="https://docs.python.org/3/library/stdtypes.html#bytes" title="(in Python v3.12)">[<code>bytes</code>{.xref .py .py-class .docutils
.literal
.notranslate}]{.pre}</a>{.reference
.external} or <a href="https://docs.python.org/3/library/stdtypes.html#memoryview" title="(in Python v3.12)">[<code>memoryview</code>{.xref .py .py-class .docutils .literal
.notranslate}]{.pre}</a>{.reference
.external}) into the plugin's target file. It must return number of
bytes written.</p>
<pre><code class="language-{=html}">&lt;!-- --&gt;
</code></pre>
<p>[[close]{.pre}]{.sig-name .descname}[(]{.sig-paren}<em>[[self]{.pre}]{.n}</em>[)]{.sig-paren}<a href="#close" title="Permalink to this definition">¶</a>{.headerlink}</p>
<p>:   Close the target file object.</p>
<p>To pass a parameter to your plugin, use <a href="#feed-options">[feed options]{.std
.std-ref}</a>{.hoverxref .tooltip .reference .internal}. You
can then access those parameters from the [<code>__init__</code>{.docutils .literal
.notranslate}]{.pre} method of your plugin.
:::
:::</p>
<p>::: {#settings .section}</p>
<h4 id="settingsheaderlink-1"><a class="header" href="#settingsheaderlink-1">Settings<a href="#settings" title="Permalink to this heading">¶</a>{.headerlink}</a></h4>
<p>These are the settings used for configuring the feed exports:</p>
<ul>
<li>
<p><a href="#std-setting-FEEDS">[<code>FEEDS</code>{.xref .std .std-setting .docutils .literal
.notranslate}]{.pre}</a>{.hoverxref .tooltip
.reference .internal} (mandatory)</p>
</li>
<li>
<p><a href="#std-setting-FEED_EXPORT_ENCODING">[<code>FEED_EXPORT_ENCODING</code>{.xref .std .std-setting .docutils .literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal}</p>
</li>
<li>
<p><a href="#std-setting-FEED_STORE_EMPTY">[<code>FEED_STORE_EMPTY</code>{.xref .std .std-setting .docutils .literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal}</p>
</li>
<li>
<p><a href="#std-setting-FEED_EXPORT_FIELDS">[<code>FEED_EXPORT_FIELDS</code>{.xref .std .std-setting .docutils .literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal}</p>
</li>
<li>
<p><a href="#std-setting-FEED_EXPORT_INDENT">[<code>FEED_EXPORT_INDENT</code>{.xref .std .std-setting .docutils .literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal}</p>
</li>
<li>
<p><a href="#std-setting-FEED_STORAGES">[<code>FEED_STORAGES</code>{.xref .std .std-setting .docutils .literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal}</p>
</li>
<li>
<p><a href="#std-setting-FEED_STORAGE_FTP_ACTIVE">[<code>FEED_STORAGE_FTP_ACTIVE</code>{.xref .std .std-setting .docutils
.literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal}</p>
</li>
<li>
<p><a href="#std-setting-FEED_STORAGE_S3_ACL">[<code>FEED_STORAGE_S3_ACL</code>{.xref .std .std-setting .docutils .literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal}</p>
</li>
<li>
<p><a href="#std-setting-FEED_EXPORTERS">[<code>FEED_EXPORTERS</code>{.xref .std .std-setting .docutils .literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal}</p>
</li>
<li>
<p><a href="#std-setting-FEED_EXPORT_BATCH_ITEM_COUNT">[<code>FEED_EXPORT_BATCH_ITEM_COUNT</code>{.xref .std .std-setting .docutils
.literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal}</p>
</li>
</ul>
<p>::: {#feeds .section}
[]{#std-setting-FEEDS}</p>
<h5 id="feedsheaderlink"><a class="header" href="#feedsheaderlink">FEEDS<a href="#feeds" title="Permalink to this heading">¶</a>{.headerlink}</a></h5>
<p>::: versionadded
[New in version 2.1.]{.versionmodified .added}
:::</p>
<p>Default: [<code>{}</code>{.docutils .literal .notranslate}]{.pre}</p>
<p>A dictionary in which every key is a feed URI (or a
<a href="https://docs.python.org/3/library/pathlib.html#pathlib.Path" title="(in Python v3.12)">[<code>pathlib.Path</code>{.xref .py .py-class .docutils .literal
.notranslate}]{.pre}</a>{.reference
.external} object) and each value is a nested dictionary containing
configuration parameters for the specific feed.</p>
<p>This setting is required for enabling the feed export feature.</p>
<p>See <a href="#topics-feed-storage-backends">[Storage backends]{.std
.std-ref}</a>{.hoverxref .tooltip .reference
.internal} for supported URI schemes.</p>
<p>For instance:</p>
<p>::: {.highlight-default .notranslate}
::: highlight
{
'items.json': {
'format': 'json',
'encoding': 'utf8',
'store_empty': False,
'item_classes': [MyItemClass1, 'myproject.items.MyItemClass2'],
'fields': None,
'indent': 4,
'item_export_kwargs': {
'export_empty_fields': True,
},
},
'/home/user/documents/items.xml': {
'format': 'xml',
'fields': ['name', 'price'],
'item_filter': MyCustomFilter1,
'encoding': 'latin1',
'indent': 8,
},
pathlib.Path('items.csv.gz'): {
'format': 'csv',
'fields': ['price', 'name'],
'item_filter': 'myproject.filters.MyCustomFilter2',
'postprocessing': [MyPlugin1, 'scrapy.extensions.postprocessing.GzipPlugin'],
'gzip_compresslevel': 5,
},
}
:::
:::</p>
<p>The following is a list of the accepted keys and the setting that is
used as a fallback value if that key is not provided for a specific feed
definition:</p>
<ul>
<li>
<p>[<code>format</code>{.docutils .literal .notranslate}]{.pre}: the
<a href="#topics-feed-format">[serialization format]{.std
.std-ref}</a>{.hoverxref .tooltip .reference
.internal}.</p>
<p>This setting is mandatory, there is no fallback value.</p>
</li>
<li>
<p>[<code>batch_item_count</code>{.docutils .literal .notranslate}]{.pre}: falls
back to <a href="#std-setting-FEED_EXPORT_BATCH_ITEM_COUNT">[<code>FEED_EXPORT_BATCH_ITEM_COUNT</code>{.xref .std .std-setting
.docutils .literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal}.</p>
<p>::: versionadded
[New in version 2.3.0.]{.versionmodified .added}
:::</p>
</li>
<li>
<p>[<code>encoding</code>{.docutils .literal .notranslate}]{.pre}: falls back to
<a href="#std-setting-FEED_EXPORT_ENCODING">[<code>FEED_EXPORT_ENCODING</code>{.xref .std .std-setting .docutils .literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal}.</p>
</li>
<li>
<p>[<code>fields</code>{.docutils .literal .notranslate}]{.pre}: falls back to
<a href="#std-setting-FEED_EXPORT_FIELDS">[<code>FEED_EXPORT_FIELDS</code>{.xref .std .std-setting .docutils .literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal}.</p>
</li>
<li>
<p>[<code>item_classes</code>{.docutils .literal .notranslate}]{.pre}: list of
<a href="index.html#topics-items">[item classes]{.std .std-ref}</a>{.hoverxref
.tooltip .reference .internal} to export.</p>
<p>If undefined or empty, all items are exported.</p>
<p>::: versionadded
[New in version 2.6.0.]{.versionmodified .added}
:::</p>
</li>
<li>
<p>[<code>item_filter</code>{.docutils .literal .notranslate}]{.pre}: a <a href="#item-filter">[filter
class]{.std .std-ref}</a>{.hoverxref .tooltip .reference
.internal} to filter items to export.</p>
<p><a href="#scrapy.extensions.feedexport.ItemFilter" title="scrapy.extensions.feedexport.ItemFilter">[<code>ItemFilter</code>{.xref .py .py-class .docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal} is used be default.</p>
<p>::: versionadded
[New in version 2.6.0.]{.versionmodified .added}
:::</p>
</li>
<li>
<p>[<code>indent</code>{.docutils .literal .notranslate}]{.pre}: falls back to
<a href="#std-setting-FEED_EXPORT_INDENT">[<code>FEED_EXPORT_INDENT</code>{.xref .std .std-setting .docutils .literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal}.</p>
</li>
<li>
<p>[<code>item_export_kwargs</code>{.docutils .literal .notranslate}]{.pre}:
<a href="https://docs.python.org/3/library/stdtypes.html#dict" title="(in Python v3.12)">[<code>dict</code>{.xref .py .py-class .docutils .literal
.notranslate}]{.pre}</a>{.reference
.external} with keyword arguments for the corresponding <a href="index.html#topics-exporters">[item
exporter class]{.std
.std-ref}</a>{.hoverxref .tooltip
.reference .internal}.</p>
<p>::: versionadded
[New in version 2.4.0.]{.versionmodified .added}
:::</p>
</li>
<li>
<p>[<code>overwrite</code>{.docutils .literal .notranslate}]{.pre}: whether to
overwrite the file if it already exists ([<code>True</code>{.docutils .literal
.notranslate}]{.pre}) or append to its content ([<code>False</code>{.docutils
.literal .notranslate}]{.pre}).</p>
<p>The default value depends on the <a href="#topics-feed-storage-backends">[storage backend]{.std
.std-ref}</a>{.hoverxref .tooltip
.reference .internal}:</p>
<ul>
<li>
<p><a href="#topics-feed-storage-fs">[Local filesystem]{.std
.std-ref}</a>{.hoverxref .tooltip
.reference .internal}: [<code>False</code>{.docutils .literal
.notranslate}]{.pre}</p>
</li>
<li>
<p><a href="#topics-feed-storage-ftp">[FTP]{.std .std-ref}</a>{.hoverxref
.tooltip .reference .internal}: [<code>True</code>{.docutils .literal
.notranslate}]{.pre}</p>
<p>::: {.admonition .note}
Note</p>
<p>Some FTP servers may not support appending to files (the
[<code>APPE</code>{.docutils .literal .notranslate}]{.pre} FTP command).
:::</p>
</li>
<li>
<p><a href="#topics-feed-storage-s3">[S3]{.std .std-ref}</a>{.hoverxref
.tooltip .reference .internal}: [<code>True</code>{.docutils .literal
.notranslate}]{.pre} (appending <a href="https://forums.aws.amazon.com/message.jspa?messageID=540395">is not
supported</a>{.reference
.external})</p>
</li>
<li>
<p><a href="#topics-feed-storage-gcs">[Google Cloud Storage (GCS)]{.std
.std-ref}</a>{.hoverxref .tooltip
.reference .internal}: [<code>True</code>{.docutils .literal
.notranslate}]{.pre} (appending is not supported)</p>
</li>
<li>
<p><a href="#topics-feed-storage-stdout">[Standard output]{.std
.std-ref}</a>{.hoverxref .tooltip
.reference .internal}: [<code>False</code>{.docutils .literal
.notranslate}]{.pre} (overwriting is not supported)</p>
</li>
</ul>
<p>::: versionadded
[New in version 2.4.0.]{.versionmodified .added}
:::</p>
</li>
<li>
<p>[<code>store_empty</code>{.docutils .literal .notranslate}]{.pre}: falls back
to <a href="#std-setting-FEED_STORE_EMPTY">[<code>FEED_STORE_EMPTY</code>{.xref .std .std-setting .docutils .literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal}.</p>
</li>
<li>
<p>[<code>uri_params</code>{.docutils .literal .notranslate}]{.pre}: falls back to
<a href="#std-setting-FEED_URI_PARAMS">[<code>FEED_URI_PARAMS</code>{.xref .std .std-setting .docutils .literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal}.</p>
</li>
<li>
<p>[<code>postprocessing</code>{.docutils .literal .notranslate}]{.pre}: list of
<a href="#post-processing">[plugins]{.std .std-ref}</a>{.hoverxref .tooltip
.reference .internal} to use for post-processing.</p>
<p>The plugins will be used in the order of the list passed.</p>
<p>::: versionadded
[New in version 2.6.0.]{.versionmodified .added}
:::
:::</p>
</li>
</ul>
<p>::: {#feed-export-encoding .section}
[]{#std-setting-FEED_EXPORT_ENCODING}</p>
<h5 id="feed_export_encodingheaderlink"><a class="header" href="#feed_export_encodingheaderlink">FEED_EXPORT_ENCODING<a href="#feed-export-encoding" title="Permalink to this heading">¶</a>{.headerlink}</a></h5>
<p>Default: [<code>None</code>{.docutils .literal .notranslate}]{.pre}</p>
<p>The encoding to be used for the feed.</p>
<p>If unset or set to [<code>None</code>{.docutils .literal .notranslate}]{.pre}
(default) it uses UTF-8 for everything except JSON output, which uses
safe numeric encoding ([<code>\uXXXX</code>{.docutils .literal .notranslate}]{.pre}
sequences) for historic reasons.</p>
<p>Use [<code>utf-8</code>{.docutils .literal .notranslate}]{.pre} if you want UTF-8
for JSON too.</p>
<p>::: versionchanged
[Changed in version 2.8: ]{.versionmodified .changed}The
<a href="index.html#std-command-startproject">[<code>startproject</code>{.xref .std .std-command .docutils .literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal} command now sets this setting to
[<code>utf-8</code>{.docutils .literal .notranslate}]{.pre} in the generated
[<code>settings.py</code>{.docutils .literal .notranslate}]{.pre} file.
:::
:::</p>
<p>::: {#feed-export-fields .section}
[]{#std-setting-FEED_EXPORT_FIELDS}</p>
<h5 id="feed_export_fieldsheaderlink"><a class="header" href="#feed_export_fieldsheaderlink">FEED_EXPORT_FIELDS<a href="#feed-export-fields" title="Permalink to this heading">¶</a>{.headerlink}</a></h5>
<p>Default: [<code>None</code>{.docutils .literal .notranslate}]{.pre}</p>
<p>Use the [<code>FEED_EXPORT_FIELDS</code>{.docutils .literal .notranslate}]{.pre}
setting to define the fields to export, their order and their output
names. See <a href="index.html#scrapy.exporters.BaseItemExporter.fields_to_export" title="scrapy.exporters.BaseItemExporter.fields_to_export">[<code>BaseItemExporter.fields_to_export</code>{.xref .py .py-attr
.docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal} for more information.
:::</p>
<p>::: {#feed-export-indent .section}
[]{#std-setting-FEED_EXPORT_INDENT}</p>
<h5 id="feed_export_indentheaderlink"><a class="header" href="#feed_export_indentheaderlink">FEED_EXPORT_INDENT<a href="#feed-export-indent" title="Permalink to this heading">¶</a>{.headerlink}</a></h5>
<p>Default: [<code>0</code>{.docutils .literal .notranslate}]{.pre}</p>
<p>Amount of spaces used to indent the output on each level. If
[<code>FEED_EXPORT_INDENT</code>{.docutils .literal .notranslate}]{.pre} is a
non-negative integer, then array elements and object members will be
pretty-printed with that indent level. An indent level of [<code>0</code>{.docutils
.literal .notranslate}]{.pre} (the default), or negative, will put each
item on a new line. [<code>None</code>{.docutils .literal .notranslate}]{.pre}
selects the most compact representation.</p>
<p>Currently implemented only by <a href="index.html#scrapy.exporters.JsonItemExporter" title="scrapy.exporters.JsonItemExporter">[<code>JsonItemExporter</code>{.xref .py .py-class
.docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal} and <a href="index.html#scrapy.exporters.XmlItemExporter" title="scrapy.exporters.XmlItemExporter">[<code>XmlItemExporter</code>{.xref .py .py-class .docutils
.literal
.notranslate}]{.pre}</a>{.reference
.internal}, i.e. when you are exporting to [<code>.json</code>{.docutils .literal
.notranslate}]{.pre} or [<code>.xml</code>{.docutils .literal .notranslate}]{.pre}.
:::</p>
<p>::: {#feed-store-empty .section}
[]{#std-setting-FEED_STORE_EMPTY}</p>
<h5 id="feed_store_emptyheaderlink"><a class="header" href="#feed_store_emptyheaderlink">FEED_STORE_EMPTY<a href="#feed-store-empty" title="Permalink to this heading">¶</a>{.headerlink}</a></h5>
<p>Default: [<code>True</code>{.docutils .literal .notranslate}]{.pre}</p>
<p>Whether to export empty feeds (i.e. feeds with no items). If
[<code>False</code>{.docutils .literal .notranslate}]{.pre}, and there are no items
to export, no new files are created and existing files are not modified,
even if the <a href="#feed-options">[overwrite feed option]{.std
.std-ref}</a>{.hoverxref .tooltip .reference .internal} is
enabled.
:::</p>
<p>::: {#feed-storages .section}
[]{#std-setting-FEED_STORAGES}</p>
<h5 id="feed_storagesheaderlink"><a class="header" href="#feed_storagesheaderlink">FEED_STORAGES<a href="#feed-storages" title="Permalink to this heading">¶</a>{.headerlink}</a></h5>
<p>Default: [<code>{}</code>{.docutils .literal .notranslate}]{.pre}</p>
<p>A dict containing additional feed storage backends supported by your
project. The keys are URI schemes and the values are paths to storage
classes.
:::</p>
<p>::: {#feed-storage-ftp-active .section}
[]{#std-setting-FEED_STORAGE_FTP_ACTIVE}</p>
<h5 id="feed_storage_ftp_activeheaderlink"><a class="header" href="#feed_storage_ftp_activeheaderlink">FEED_STORAGE_FTP_ACTIVE<a href="#feed-storage-ftp-active" title="Permalink to this heading">¶</a>{.headerlink}</a></h5>
<p>Default: [<code>False</code>{.docutils .literal .notranslate}]{.pre}</p>
<p>Whether to use the active connection mode when exporting feeds to an FTP
server ([<code>True</code>{.docutils .literal .notranslate}]{.pre}) or use the
passive connection mode instead ([<code>False</code>{.docutils .literal
.notranslate}]{.pre}, default).</p>
<p>For information about FTP connection modes, see <a href="https://stackoverflow.com/a/1699163">What is the difference
between active and passive
FTP?</a>{.reference .external}.
:::</p>
<p>::: {#feed-storage-s3-acl .section}
[]{#std-setting-FEED_STORAGE_S3_ACL}</p>
<h5 id="feed_storage_s3_aclheaderlink"><a class="header" href="#feed_storage_s3_aclheaderlink">FEED_STORAGE_S3_ACL<a href="#feed-storage-s3-acl" title="Permalink to this heading">¶</a>{.headerlink}</a></h5>
<p>Default: [<code>''</code>{.docutils .literal .notranslate}]{.pre} (empty string)</p>
<p>A string containing a custom ACL for feeds exported to Amazon S3 by your
project.</p>
<p>For a complete list of available values, access the <a href="https://docs.aws.amazon.com/AmazonS3/latest/dev/acl-overview.html#canned-acl">Canned
ACL</a>{.reference
.external} section on Amazon S3 docs.
:::</p>
<p>::: {#feed-storages-base .section}
[]{#std-setting-FEED_STORAGES_BASE}</p>
<h5 id="feed_storages_baseheaderlink"><a class="header" href="#feed_storages_baseheaderlink">FEED_STORAGES_BASE<a href="#feed-storages-base" title="Permalink to this heading">¶</a>{.headerlink}</a></h5>
<p>Default:</p>
<p>::: {.highlight-python .notranslate}
::: highlight
{
&quot;&quot;: &quot;scrapy.extensions.feedexport.FileFeedStorage&quot;,
&quot;file&quot;: &quot;scrapy.extensions.feedexport.FileFeedStorage&quot;,
&quot;stdout&quot;: &quot;scrapy.extensions.feedexport.StdoutFeedStorage&quot;,
&quot;s3&quot;: &quot;scrapy.extensions.feedexport.S3FeedStorage&quot;,
&quot;ftp&quot;: &quot;scrapy.extensions.feedexport.FTPFeedStorage&quot;,
}
:::
:::</p>
<p>A dict containing the built-in feed storage backends supported by
Scrapy. You can disable any of these backends by assigning
[<code>None</code>{.docutils .literal .notranslate}]{.pre} to their URI scheme in
<a href="#std-setting-FEED_STORAGES">[<code>FEED_STORAGES</code>{.xref .std .std-setting .docutils .literal
.notranslate}]{.pre}</a>{.hoverxref .tooltip
.reference .internal}. E.g., to disable the built-in FTP storage backend
(without replacement), place this in your [<code>settings.py</code>{.docutils
.literal .notranslate}]{.pre}:</p>
<p>::: {.highlight-python .notranslate}
::: highlight
FEED_STORAGES = {
&quot;ftp&quot;: None,
}
:::
:::
:::</p>
<p>::: {#feed-exporters .section}
[]{#std-setting-FEED_EXPORTERS}</p>
<h5 id="feed_exportersheaderlink"><a class="header" href="#feed_exportersheaderlink">FEED_EXPORTERS<a href="#feed-exporters" title="Permalink to this heading">¶</a>{.headerlink}</a></h5>
<p>Default: [<code>{}</code>{.docutils .literal .notranslate}]{.pre}</p>
<p>A dict containing additional exporters supported by your project. The
keys are serialization formats and the values are paths to <a href="index.html#topics-exporters">[Item
exporter]{.std .std-ref}</a>{.hoverxref
.tooltip .reference .internal} classes.
:::</p>
<p>::: {#feed-exporters-base .section}
[]{#std-setting-FEED_EXPORTERS_BASE}</p>
<h5 id="feed_exporters_baseheaderlink"><a class="header" href="#feed_exporters_baseheaderlink">FEED_EXPORTERS_BASE<a href="#feed-exporters-base" title="Permalink to this heading">¶</a>{.headerlink}</a></h5>
<p>Default:</p>
<p>::: {.highlight-python .notranslate}
::: highlight
{
&quot;json&quot;: &quot;scrapy.exporters.JsonItemExporter&quot;,
&quot;jsonlines&quot;: &quot;scrapy.exporters.JsonLinesItemExporter&quot;,
&quot;jsonl&quot;: &quot;scrapy.exporters.JsonLinesItemExporter&quot;,
&quot;jl&quot;: &quot;scrapy.exporters.JsonLinesItemExporter&quot;,
&quot;csv&quot;: &quot;scrapy.exporters.CsvItemExporter&quot;,
&quot;xml&quot;: &quot;scrapy.exporters.XmlItemExporter&quot;,
&quot;marshal&quot;: &quot;scrapy.exporters.MarshalItemExporter&quot;,
&quot;pickle&quot;: &quot;scrapy.exporters.PickleItemExporter&quot;,
}
:::
:::</p>
<p>A dict containing the built-in feed exporters supported by Scrapy. You
can disable any of these exporters by assigning [<code>None</code>{.docutils
.literal .notranslate}]{.pre} to their serialization format in
<a href="#std-setting-FEED_EXPORTERS">[<code>FEED_EXPORTERS</code>{.xref .std .std-setting .docutils .literal
.notranslate}]{.pre}</a>{.hoverxref .tooltip
.reference .internal}. E.g., to disable the built-in CSV exporter
(without replacement), place this in your [<code>settings.py</code>{.docutils
.literal .notranslate}]{.pre}:</p>
<p>::: {.highlight-python .notranslate}
::: highlight
FEED_EXPORTERS = {
&quot;csv&quot;: None,
}
:::
:::
:::</p>
<p>::: {#feed-export-batch-item-count .section}
[]{#std-setting-FEED_EXPORT_BATCH_ITEM_COUNT}</p>
<h5 id="feed_export_batch_item_countheaderlink"><a class="header" href="#feed_export_batch_item_countheaderlink">FEED_EXPORT_BATCH_ITEM_COUNT<a href="#feed-export-batch-item-count" title="Permalink to this heading">¶</a>{.headerlink}</a></h5>
<p>::: versionadded
[New in version 2.3.0.]{.versionmodified .added}
:::</p>
<p>Default: [<code>0</code>{.docutils .literal .notranslate}]{.pre}</p>
<p>If assigned an integer number higher than [<code>0</code>{.docutils .literal
.notranslate}]{.pre}, Scrapy generates multiple output files storing up
to the specified number of items in each output file.</p>
<p>When generating multiple output files, you must use at least one of the
following placeholders in the feed URI to indicate how the different
output file names are generated:</p>
<ul>
<li>
<p>[<code>%(batch_time)s</code>{.docutils .literal .notranslate}]{.pre} - gets
replaced by a timestamp when the feed is being created (e.g.
[<code>2020-03-28T14-45-08.237134</code>{.docutils .literal
.notranslate}]{.pre})</p>
</li>
<li>
<p>[<code>%(batch_id)d</code>{.docutils .literal .notranslate}]{.pre} - gets
replaced by the 1-based sequence number of the batch.</p>
<p>Use <a href="https://docs.python.org/3/library/stdtypes.html#old-string-formatting" title="(in Python v3.12)">[printf-style string formatting]{.xref .std
.std-ref}</a>{.reference
.external} to alter the number format. For example, to make the
batch ID a 5-digit number by introducing leading zeroes as needed,
use [<code>%(batch_id)05d</code>{.docutils .literal .notranslate}]{.pre} (e.g.
[<code>3</code>{.docutils .literal .notranslate}]{.pre} becomes
[<code>00003</code>{.docutils .literal .notranslate}]{.pre}, [<code>123</code>{.docutils
.literal .notranslate}]{.pre} becomes [<code>00123</code>{.docutils .literal
.notranslate}]{.pre}).</p>
</li>
</ul>
<p>For instance, if your settings include:</p>
<p>::: {.highlight-python .notranslate}
::: highlight
FEED_EXPORT_BATCH_ITEM_COUNT = 100
:::
:::</p>
<p>And your <a href="index.html#std-command-crawl">[<code>crawl</code>{.xref .std .std-command .docutils .literal
.notranslate}]{.pre}</a>{.hoverxref .tooltip
.reference .internal} command line is:</p>
<p>::: {.highlight-default .notranslate}
::: highlight
scrapy crawl spidername -o &quot;dirname/%(batch_id)d-filename%(batch_time)s.json&quot;
:::
:::</p>
<p>The command line above can generate a directory tree like:</p>
<p>::: {.highlight-default .notranslate}
::: highlight
-&gt;projectname
--&gt;dirname
---&gt;1-filename2020-03-28T14-45-08.237134.json
---&gt;2-filename2020-03-28T14-45-09.148903.json
---&gt;3-filename2020-03-28T14-45-10.046092.json
:::
:::</p>
<p>Where the first and second files contain exactly 100 items. The last one
contains 100 items or fewer.
:::</p>
<p>::: {#feed-uri-params .section}
[]{#std-setting-FEED_URI_PARAMS}</p>
<h5 id="feed_uri_paramsheaderlink"><a class="header" href="#feed_uri_paramsheaderlink">FEED_URI_PARAMS<a href="#feed-uri-params" title="Permalink to this heading">¶</a>{.headerlink}</a></h5>
<p>Default: [<code>None</code>{.docutils .literal .notranslate}]{.pre}</p>
<p>A string with the import path of a function to set the parameters to
apply with <a href="https://docs.python.org/3/library/stdtypes.html#old-string-formatting" title="(in Python v3.12)">[printf-style string formatting]{.xref .std
.std-ref}</a>{.reference
.external} to the feed URI.</p>
<p>The function signature should be as follows:</p>
<p>[[scrapy.extensions.feedexport.]{.pre}]{.sig-prename .descclassname}[[uri_params]{.pre}]{.sig-name .descname}[(]{.sig-paren}<em>[[params]{.pre}]{.n}</em>, <em>[[spider]{.pre}]{.n}</em>[)]{.sig-paren}<a href="#scrapy.extensions.feedexport.uri_params" title="Permalink to this definition">¶</a>{.headerlink}</p>
<p>:   Return a <a href="https://docs.python.org/3/library/stdtypes.html#dict" title="(in Python v3.12)">[<code>dict</code>{.xref .py .py-class .docutils .literal
.notranslate}]{.pre}</a>{.reference
.external} of key-value pairs to apply to the feed URI using
<a href="https://docs.python.org/3/library/stdtypes.html#old-string-formatting" title="(in Python v3.12)">[printf-style string formatting]{.xref .std
.std-ref}</a>{.reference
.external}.</p>
<pre><code>Parameters

:   -   **params**
        ([*dict*](https://docs.python.org/3/library/stdtypes.html#dict &quot;(in Python v3.12)&quot;){.reference
        .external}) --

        default key-value pairs

        Specifically:

        -   [`batch_id`{.docutils .literal .notranslate}]{.pre}: ID
            of the file batch. See
            [[`FEED_EXPORT_BATCH_ITEM_COUNT`{.xref .std .std-setting
            .docutils .literal
            .notranslate}]{.pre}](#std-setting-FEED_EXPORT_BATCH_ITEM_COUNT){.hoverxref
            .tooltip .reference .internal}.

            If [[`FEED_EXPORT_BATCH_ITEM_COUNT`{.xref .std
            .std-setting .docutils .literal
            .notranslate}]{.pre}](#std-setting-FEED_EXPORT_BATCH_ITEM_COUNT){.hoverxref
            .tooltip .reference .internal} is [`0`{.docutils
            .literal .notranslate}]{.pre}, [`batch_id`{.docutils
            .literal .notranslate}]{.pre} is always [`1`{.docutils
            .literal .notranslate}]{.pre}.

            ::: versionadded
            [New in version 2.3.0.]{.versionmodified .added}
            :::

        -   [`batch_time`{.docutils .literal .notranslate}]{.pre}:
            UTC date and time, in ISO format with [`:`{.docutils
            .literal .notranslate}]{.pre} replaced with
            [`-`{.docutils .literal .notranslate}]{.pre}.

            See [[`FEED_EXPORT_BATCH_ITEM_COUNT`{.xref .std
            .std-setting .docutils .literal
            .notranslate}]{.pre}](#std-setting-FEED_EXPORT_BATCH_ITEM_COUNT){.hoverxref
            .tooltip .reference .internal}.

            ::: versionadded
            [New in version 2.3.0.]{.versionmodified .added}
            :::

        -   [`time`{.docutils .literal .notranslate}]{.pre}:
            [`batch_time`{.docutils .literal .notranslate}]{.pre},
            with microseconds set to [`0`{.docutils .literal
            .notranslate}]{.pre}.

    -   **spider**
        ([*scrapy.Spider*](index.html#scrapy.Spider &quot;scrapy.Spider&quot;){.reference
        .internal}) -- source spider of the feed items

::: {.admonition .caution}
Caution

The function should return a new dictionary, modifying the received
[`params`{.docutils .literal .notranslate}]{.pre} in-place is
deprecated.
:::
</code></pre>
<p>For example, to include the <a href="index.html#scrapy.Spider.name" title="scrapy.Spider.name">[<code>name</code>{.xref .py .py-attr .docutils
.literal
.notranslate}]{.pre}</a>{.reference
.internal} of the source spider in the feed URI:</p>
<ol>
<li>
<p>Define the following function somewhere in your project:</p>
<p>::: {.highlight-python .notranslate}
::: highlight
# myproject/utils.py
def uri_params(params, spider):
return {**params, &quot;spider_name&quot;: spider.name}
:::
:::</p>
</li>
<li>
<p>Point <a href="#std-setting-FEED_URI_PARAMS">[<code>FEED_URI_PARAMS</code>{.xref .std .std-setting .docutils .literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal} to that function in your settings:</p>
<p>::: {.highlight-python .notranslate}
::: highlight
# myproject/settings.py
FEED_URI_PARAMS = &quot;myproject.utils.uri_params&quot;
:::
:::</p>
</li>
<li>
<p>Use [<code>%(spider_name)s</code>{.docutils .literal .notranslate}]{.pre} in
your feed URI:</p>
<p>::: {.highlight-default .notranslate}
::: highlight
scrapy crawl &lt;spider_name&gt; -o &quot;%(spider_name)s.jsonl&quot;
:::
:::
:::
:::
:::</p>
</li>
</ol>
<p>[]{#document-topics/request-response}</p>
<p>::: {#module-scrapy.http .section}
[]{#requests-and-responses}[]{#topics-request-response}</p>
<h3 id="requests-and-responsesheaderlink"><a class="header" href="#requests-and-responsesheaderlink">Requests and Responses<a href="#module-scrapy.http" title="Permalink to this heading">¶</a>{.headerlink}</a></h3>
<p>Scrapy uses <a href="#scrapy.http.Request" title="scrapy.http.Request">[<code>Request</code>{.xref .py .py-class .docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal} and <a href="#scrapy.http.Response" title="scrapy.http.Response">[<code>Response</code>{.xref .py .py-class .docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal} objects for crawling web sites.</p>
<p>Typically, <a href="#scrapy.http.Request" title="scrapy.http.Request">[<code>Request</code>{.xref .py .py-class .docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal} objects are generated in the spiders and pass across the
system until they reach the Downloader, which executes the request and
returns a <a href="#scrapy.http.Response" title="scrapy.http.Response">[<code>Response</code>{.xref .py .py-class .docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal} object which travels back to the spider that issued the
request.</p>
<p>Both <a href="#scrapy.http.Request" title="scrapy.http.Request">[<code>Request</code>{.xref .py .py-class .docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal} and <a href="#scrapy.http.Response" title="scrapy.http.Response">[<code>Response</code>{.xref .py .py-class .docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal} classes have subclasses which add functionality not required
in the base classes. These are described below in <a href="#topics-request-response-ref-request-subclasses">[Request
subclasses]{.std
.std-ref}</a>{.hoverxref
.tooltip .reference .internal} and <a href="#topics-request-response-ref-response-subclasses">[Response subclasses]{.std
.std-ref}</a>{.hoverxref
.tooltip .reference .internal}.</p>
<p>::: {#request-objects .section}</p>
<h4 id="request-objectsheaderlink"><a class="header" href="#request-objectsheaderlink">Request objects<a href="#request-objects" title="Permalink to this heading">¶</a>{.headerlink}</a></h4>
<p><em>[class]{.pre}[ ]{.w}</em>[[scrapy.http.]{.pre}]{.sig-prename .descclassname}[[Request]{.pre}]{.sig-name .descname}[(]{.sig-paren}<em>[[*]{.pre}]{.o}[[args]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[<a href="https://docs.python.org/3/library/typing.html#typing.Any" title="(in Python v3.12)">[Any]{.pre}</a>{.reference .external}]{.n}</em>, <em>[[**]{.pre}]{.o}[[kwargs]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[<a href="https://docs.python.org/3/library/typing.html#typing.Any" title="(in Python v3.12)">[Any]{.pre}</a>{.reference .external}]{.n}</em>[)]{.sig-paren}<a href="_modules/scrapy/http/request.html#Request">[[[source]]{.pre}]{.viewcode-link}</a>{.reference .internal}<a href="#scrapy.http.Request" title="Permalink to this definition">¶</a>{.headerlink}</p>
<p>:   Represents an HTTP request, which is usually generated in a Spider
and executed by the Downloader, thus generating a <a href="#scrapy.http.Response" title="scrapy.http.Response">[<code>Response</code>{.xref
.py .py-class .docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal}.</p>
<pre><code>Parameters

:   -   **url**
        ([*str*](https://docs.python.org/3/library/stdtypes.html#str &quot;(in Python v3.12)&quot;){.reference
        .external}) --

        the URL of this request

        If the URL is invalid, a [[`ValueError`{.xref .py .py-exc
        .docutils .literal
        .notranslate}]{.pre}](https://docs.python.org/3/library/exceptions.html#ValueError &quot;(in Python v3.12)&quot;){.reference
        .external} exception is raised.

    -   **callback**
        ([*collections.abc.Callable*](https://docs.python.org/3/library/collections.abc.html#collections.abc.Callable &quot;(in Python v3.12)&quot;){.reference
        .external}) --

        the function that will be called with the response of this
        request (once it's downloaded) as its first parameter.

        In addition to a function, the following values are
        supported:

        -   [`None`{.docutils .literal .notranslate}]{.pre}
            (default), which indicates that the spider's
            [[`parse()`{.xref .py .py-meth .docutils .literal
            .notranslate}]{.pre}](index.html#scrapy.Spider.parse &quot;scrapy.Spider.parse&quot;){.reference
            .internal} method must be used.

        -   [[`NO_CALLBACK()`{.xref .py .py-func .docutils .literal
            .notranslate}]{.pre}](#scrapy.http.request.NO_CALLBACK &quot;scrapy.http.request.NO_CALLBACK&quot;){.reference
            .internal}

        For more information, see [[Passing additional data to
        callback functions]{.std
        .std-ref}](#topics-request-response-ref-request-callback-arguments){.hoverxref
        .tooltip .reference .internal}.

        ::: {.admonition .note}
        Note

        If exceptions are raised during processing,
        [`errback`{.docutils .literal .notranslate}]{.pre} is called
        instead.
        :::

    -   **method**
        ([*str*](https://docs.python.org/3/library/stdtypes.html#str &quot;(in Python v3.12)&quot;){.reference
        .external}) -- the HTTP method of this request. Defaults to
        [`'GET'`{.docutils .literal .notranslate}]{.pre}.

    -   **meta**
        ([*dict*](https://docs.python.org/3/library/stdtypes.html#dict &quot;(in Python v3.12)&quot;){.reference
        .external}) -- the initial values for the
        [[`Request.meta`{.xref .py .py-attr .docutils .literal
        .notranslate}]{.pre}](#scrapy.http.Request.meta &quot;scrapy.http.Request.meta&quot;){.reference
        .internal} attribute. If given, the dict passed in this
        parameter will be shallow copied.

    -   **body**
        ([*bytes*](https://docs.python.org/3/library/stdtypes.html#bytes &quot;(in Python v3.12)&quot;){.reference
        .external} *or*
        [*str*](https://docs.python.org/3/library/stdtypes.html#str &quot;(in Python v3.12)&quot;){.reference
        .external}) -- the request body. If a string is passed, then
        it's encoded as bytes using the [`encoding`{.docutils
        .literal .notranslate}]{.pre} passed (which defaults to
        [`utf-8`{.docutils .literal .notranslate}]{.pre}). If
        [`body`{.docutils .literal .notranslate}]{.pre} is not
        given, an empty bytes object is stored. Regardless of the
        type of this argument, the final value stored will be a
        bytes object (never a string or [`None`{.docutils .literal
        .notranslate}]{.pre}).

    -   **headers**
        ([*dict*](https://docs.python.org/3/library/stdtypes.html#dict &quot;(in Python v3.12)&quot;){.reference
        .external}) --

        the headers of this request. The dict values can be strings
        (for single valued headers) or lists (for multi-valued
        headers). If [`None`{.docutils .literal .notranslate}]{.pre}
        is passed as value, the HTTP header will not be sent at all.

        &gt; &lt;div&gt;
        &gt;
        &gt; ::: {.admonition .caution}
        &gt; Caution
        &gt;
        &gt; Cookies set via the [`Cookie`{.docutils .literal
        &gt; .notranslate}]{.pre} header are not considered by the
        &gt; [[CookiesMiddleware]{.std
        &gt; .std-ref}](index.html#cookies-mw){.hoverxref .tooltip
        &gt; .reference .internal}. If you need to set cookies for a
        &gt; request, use the [`Request.cookies`{.xref .py .py-class
        &gt; .docutils .literal .notranslate}]{.pre} parameter. This is
        &gt; a known current limitation that is being worked on.
        &gt; :::
        &gt;
        &gt; &lt;/div&gt;

    -   **cookies**
        ([*dict*](https://docs.python.org/3/library/stdtypes.html#dict &quot;(in Python v3.12)&quot;){.reference
        .external} *or*
        [*list*](https://docs.python.org/3/library/stdtypes.html#list &quot;(in Python v3.12)&quot;){.reference
        .external}) --

        the request cookies. These can be sent in two forms.

        1.  Using a dict:

        ::: {.highlight-python .notranslate}
        ::: highlight
            request_with_cookies = Request(
                url=&quot;http://www.example.com&quot;,
                cookies={&quot;currency&quot;: &quot;USD&quot;, &quot;country&quot;: &quot;UY&quot;},
            )
        :::
        :::

        2.  Using a list of dicts:

        ::: {.highlight-python .notranslate}
        ::: highlight
            request_with_cookies = Request(
                url=&quot;http://www.example.com&quot;,
                cookies=[
                    {
                        &quot;name&quot;: &quot;currency&quot;,
                        &quot;value&quot;: &quot;USD&quot;,
                        &quot;domain&quot;: &quot;example.com&quot;,
                        &quot;path&quot;: &quot;/currency&quot;,
                    },
                ],
            )
        :::
        :::

        The latter form allows for customizing the
        [`domain`{.docutils .literal .notranslate}]{.pre} and
        [`path`{.docutils .literal .notranslate}]{.pre} attributes
        of the cookie. This is only useful if the cookies are saved
        for later requests.

        []{#std-reqmeta-dont_merge_cookies .target}

        When some site returns cookies (in a response) those are
        stored in the cookies for that domain and will be sent again
        in future requests. That's the typical behaviour of any
        regular web browser.

        Note that setting the [[`dont_merge_cookies`{.xref .std
        .std-reqmeta .docutils .literal
        .notranslate}]{.pre}](#std-reqmeta-dont_merge_cookies){.hoverxref
        .tooltip .reference .internal} key to [`True`{.docutils
        .literal .notranslate}]{.pre} in [`request.meta`{.xref .py
        .py-attr .docutils .literal .notranslate}]{.pre} causes
        custom cookies to be ignored.

        For more info see [[CookiesMiddleware]{.std
        .std-ref}](index.html#cookies-mw){.hoverxref .tooltip
        .reference .internal}.

        ::: {.admonition .caution}
        Caution

        Cookies set via the [`Cookie`{.docutils .literal
        .notranslate}]{.pre} header are not considered by the
        [[CookiesMiddleware]{.std
        .std-ref}](index.html#cookies-mw){.hoverxref .tooltip
        .reference .internal}. If you need to set cookies for a
        request, use the [`Request.cookies`{.xref .py .py-class
        .docutils .literal .notranslate}]{.pre} parameter. This is a
        known current limitation that is being worked on.
        :::

        ::: versionadded
        [New in version 2.6.0: ]{.versionmodified .added}Cookie
        values that are [[`bool`{.xref .py .py-class .docutils
        .literal
        .notranslate}]{.pre}](https://docs.python.org/3/library/functions.html#bool &quot;(in Python v3.12)&quot;){.reference
        .external}, [[`float`{.xref .py .py-class .docutils .literal
        .notranslate}]{.pre}](https://docs.python.org/3/library/functions.html#float &quot;(in Python v3.12)&quot;){.reference
        .external} or [[`int`{.xref .py .py-class .docutils .literal
        .notranslate}]{.pre}](https://docs.python.org/3/library/functions.html#int &quot;(in Python v3.12)&quot;){.reference
        .external} are casted to [[`str`{.xref .py .py-class
        .docutils .literal
        .notranslate}]{.pre}](https://docs.python.org/3/library/stdtypes.html#str &quot;(in Python v3.12)&quot;){.reference
        .external}.
        :::

    -   **encoding**
        ([*str*](https://docs.python.org/3/library/stdtypes.html#str &quot;(in Python v3.12)&quot;){.reference
        .external}) -- the encoding of this request (defaults to
        [`'utf-8'`{.docutils .literal .notranslate}]{.pre}). This
        encoding will be used to percent-encode the URL and to
        convert the body to bytes (if given as a string).

    -   **priority**
        ([*int*](https://docs.python.org/3/library/functions.html#int &quot;(in Python v3.12)&quot;){.reference
        .external}) -- the priority of this request (defaults to
        [`0`{.docutils .literal .notranslate}]{.pre}). The priority
        is used by the scheduler to define the order used to process
        requests. Requests with a higher priority value will execute
        earlier. Negative values are allowed in order to indicate
        relatively low-priority.

    -   **dont_filter**
        ([*bool*](https://docs.python.org/3/library/functions.html#bool &quot;(in Python v3.12)&quot;){.reference
        .external}) -- indicates that this request should not be
        filtered by the scheduler. This is used when you want to
        perform an identical request multiple times, to ignore the
        duplicates filter. Use it with care, or you will get into
        crawling loops. Default to [`False`{.docutils .literal
        .notranslate}]{.pre}.

    -   **errback**
        ([*collections.abc.Callable*](https://docs.python.org/3/library/collections.abc.html#collections.abc.Callable &quot;(in Python v3.12)&quot;){.reference
        .external}) --

        a function that will be called if any exception was raised
        while processing the request. This includes pages that
        failed with 404 HTTP errors and such. It receives a
        [[`Failure`{.xref .py .py-exc .docutils .literal
        .notranslate}]{.pre}](https://docs.twisted.org/en/stable/api/twisted.python.failure.Failure.html &quot;(in Twisted)&quot;){.reference
        .external} as first parameter. For more information, see
        [[Using errbacks to catch exceptions in request
        processing]{.std
        .std-ref}](#topics-request-response-ref-errbacks){.hoverxref
        .tooltip .reference .internal} below.

        ::: versionchanged
        [Changed in version 2.0: ]{.versionmodified .changed}The
        *callback* parameter is no longer required when the
        *errback* parameter is specified.
        :::

    -   **flags**
        ([*list*](https://docs.python.org/3/library/stdtypes.html#list &quot;(in Python v3.12)&quot;){.reference
        .external}) -- Flags sent to the request, can be used for
        logging or similar purposes.

    -   **cb_kwargs**
        ([*dict*](https://docs.python.org/3/library/stdtypes.html#dict &quot;(in Python v3.12)&quot;){.reference
        .external}) -- A dict with arbitrary data that will be
        passed as keyword arguments to the Request's callback.

[[url]{.pre}]{.sig-name .descname}[¶](#scrapy.http.Request.url &quot;Permalink to this definition&quot;){.headerlink}

:   A string containing the URL of this request. Keep in mind that
    this attribute contains the escaped URL, so it can differ from
    the URL passed in the [`__init__`{.docutils .literal
    .notranslate}]{.pre} method.

    This attribute is read-only. To change the URL of a Request use
    [[`replace()`{.xref .py .py-meth .docutils .literal
    .notranslate}]{.pre}](#scrapy.http.Request.replace &quot;scrapy.http.Request.replace&quot;){.reference
    .internal}.

[[method]{.pre}]{.sig-name .descname}[¶](#scrapy.http.Request.method &quot;Permalink to this definition&quot;){.headerlink}

:   A string representing the HTTP method in the request. This is
    guaranteed to be uppercase. Example: [`&quot;GET&quot;`{.docutils .literal
    .notranslate}]{.pre}, [`&quot;POST&quot;`{.docutils .literal
    .notranslate}]{.pre}, [`&quot;PUT&quot;`{.docutils .literal
    .notranslate}]{.pre}, etc

[[headers]{.pre}]{.sig-name .descname}[¶](#scrapy.http.Request.headers &quot;Permalink to this definition&quot;){.headerlink}

:   A dictionary-like object which contains the request headers.

[[body]{.pre}]{.sig-name .descname}[¶](#scrapy.http.Request.body &quot;Permalink to this definition&quot;){.headerlink}

:   The request body as bytes.

    This attribute is read-only. To change the body of a Request use
    [[`replace()`{.xref .py .py-meth .docutils .literal
    .notranslate}]{.pre}](#scrapy.http.Request.replace &quot;scrapy.http.Request.replace&quot;){.reference
    .internal}.

[[meta]{.pre}]{.sig-name .descname}*[ ]{.w}[[=]{.pre}]{.p}[ ]{.w}[{}]{.pre}*[¶](#scrapy.http.Request.meta &quot;Permalink to this definition&quot;){.headerlink}

:   &lt;div&gt;
    &gt;
    &gt; A dictionary of arbitrary metadata for the request.
    &gt;
    &gt; You may extend request metadata as you see fit.
    &gt;
    &gt; Request metadata can also be accessed through the
    &gt; [[`meta`{.xref .py .py-attr .docutils .literal
    &gt; .notranslate}]{.pre}](#scrapy.http.Response.meta &quot;scrapy.http.Response.meta&quot;){.reference
    &gt; .internal} attribute of a response.
    &gt;
    &gt; To pass data from one spider callback to another, consider
    &gt; using [[`cb_kwargs`{.xref .py .py-attr .docutils .literal
    &gt; .notranslate}]{.pre}](#scrapy.http.Request.cb_kwargs &quot;scrapy.http.Request.cb_kwargs&quot;){.reference
    &gt; .internal} instead. However, request metadata may be the right
    &gt; choice in certain scenarios, such as to maintain some
    &gt; debugging data across all follow-up requests (e.g. the source
    &gt; URL).
    &gt;
    &gt; A common use of request metadata is to define request-specific
    &gt; parameters for Scrapy components (extensions, middlewares,
    &gt; etc.). For example, if you set [`dont_retry`{.docutils
    &gt; .literal .notranslate}]{.pre} to [`True`{.docutils .literal
    &gt; .notranslate}]{.pre}, [[`RetryMiddleware`{.xref .py .py-class
    &gt; .docutils .literal
    &gt; .notranslate}]{.pre}](index.html#scrapy.downloadermiddlewares.retry.RetryMiddleware &quot;scrapy.downloadermiddlewares.retry.RetryMiddleware&quot;){.reference
    &gt; .internal} will never retry that request, even if it fails.
    &gt; See [[Request.meta special keys]{.std
    &gt; .std-ref}](#topics-request-meta){.hoverxref .tooltip
    &gt; .reference .internal}.
    &gt;
    &gt; You may also use request metadata in your custom Scrapy
    &gt; components, for example, to keep request state information
    &gt; relevant to your component. For example,
    &gt; [[`RetryMiddleware`{.xref .py .py-class .docutils .literal
    &gt; .notranslate}]{.pre}](index.html#scrapy.downloadermiddlewares.retry.RetryMiddleware &quot;scrapy.downloadermiddlewares.retry.RetryMiddleware&quot;){.reference
    &gt; .internal} uses the [`retry_times`{.docutils .literal
    &gt; .notranslate}]{.pre} metadata key to keep track of how many
    &gt; times a request has been retried so far.
    &gt;
    &gt; Copying all the metadata of a previous request into a new,
    &gt; follow-up request in a spider callback is a bad practice,
    &gt; because request metadata may include metadata set by Scrapy
    &gt; components that is not meant to be copied into other requests.
    &gt; For example, copying the [`retry_times`{.docutils .literal
    &gt; .notranslate}]{.pre} metadata key into follow-up requests can
    &gt; lower the amount of retries allowed for those follow-up
    &gt; requests.
    &gt;
    &gt; You should only copy all request metadata from one request to
    &gt; another if the new request is meant to replace the old
    &gt; request, as is often the case when returning a request from a
    &gt; [[downloader middleware]{.std
    &gt; .std-ref}](index.html#topics-downloader-middleware){.hoverxref
    &gt; .tooltip .reference .internal} method.
    &gt;
    &gt; Also mind that the [[`copy()`{.xref .py .py-meth .docutils
    &gt; .literal
    &gt; .notranslate}]{.pre}](#scrapy.http.Request.copy &quot;scrapy.http.Request.copy&quot;){.reference
    &gt; .internal} and [[`replace()`{.xref .py .py-meth .docutils
    &gt; .literal
    &gt; .notranslate}]{.pre}](#scrapy.http.Request.replace &quot;scrapy.http.Request.replace&quot;){.reference
    &gt; .internal} request methods [[shallow-copy]{.xref .std
    &gt; .std-doc}](https://docs.python.org/3/library/copy.html &quot;(in Python v3.12)&quot;){.reference
    &gt; .external} request metadata.
    &gt;
    &gt; &lt;/div&gt;

[[cb_kwargs]{.pre}]{.sig-name .descname}[¶](#scrapy.http.Request.cb_kwargs &quot;Permalink to this definition&quot;){.headerlink}

:   A dictionary that contains arbitrary metadata for this request.
    Its contents will be passed to the Request's callback as keyword
    arguments. It is empty for new Requests, which means by default
    callbacks only get a [[`Response`{.xref .py .py-class .docutils
    .literal
    .notranslate}]{.pre}](#scrapy.http.Response &quot;scrapy.http.Response&quot;){.reference
    .internal} object as argument.

    This dict is [[shallow copied]{.xref .std
    .std-doc}](https://docs.python.org/3/library/copy.html &quot;(in Python v3.12)&quot;){.reference
    .external} when the request is cloned using the
    [`copy()`{.docutils .literal .notranslate}]{.pre} or
    [`replace()`{.docutils .literal .notranslate}]{.pre} methods,
    and can also be accessed, in your spider, from the
    [`response.cb_kwargs`{.docutils .literal .notranslate}]{.pre}
    attribute.

    In case of a failure to process the request, this dict can be
    accessed as [`failure.request.cb_kwargs`{.docutils .literal
    .notranslate}]{.pre} in the request's errback. For more
    information, see [[Accessing additional data in errback
    functions]{.std .std-ref}](#errback-cb-kwargs){.hoverxref
    .tooltip .reference .internal}.

[[attributes]{.pre}]{.sig-name .descname}*[[:]{.pre}]{.p}[ ]{.w}[[Tuple]{.pre}](https://docs.python.org/3/library/typing.html#typing.Tuple &quot;(in Python v3.12)&quot;){.reference .external}[[\[]{.pre}]{.p}[[str]{.pre}](https://docs.python.org/3/library/stdtypes.html#str &quot;(in Python v3.12)&quot;){.reference .external}[[,]{.pre}]{.p}[ ]{.w}[[\...]{.pre}]{.p}[[\]]{.pre}]{.p}[ ]{.w}[[=]{.pre}]{.p}[ ]{.w}[(\'url\',]{.pre} [\'callback\',]{.pre} [\'method\',]{.pre} [\'headers\',]{.pre} [\'body\',]{.pre} [\'cookies\',]{.pre} [\'meta\',]{.pre} [\'encoding\',]{.pre} [\'priority\',]{.pre} [\'dont_filter\',]{.pre} [\'errback\',]{.pre} [\'flags\',]{.pre} [\'cb_kwargs\')]{.pre}*[¶](#scrapy.http.Request.attributes &quot;Permalink to this definition&quot;){.headerlink}

:   A tuple of [[`str`{.xref .py .py-class .docutils .literal
    .notranslate}]{.pre}](https://docs.python.org/3/library/stdtypes.html#str &quot;(in Python v3.12)&quot;){.reference
    .external} objects containing the name of all public attributes
    of the class that are also keyword parameters of the
    [`__init__`{.docutils .literal .notranslate}]{.pre} method.

    Currently used by [[`Request.replace()`{.xref .py .py-meth
    .docutils .literal
    .notranslate}]{.pre}](#scrapy.http.Request.replace &quot;scrapy.http.Request.replace&quot;){.reference
    .internal}, [[`Request.to_dict()`{.xref .py .py-meth .docutils
    .literal
    .notranslate}]{.pre}](#scrapy.http.Request.to_dict &quot;scrapy.http.Request.to_dict&quot;){.reference
    .internal} and [[`request_from_dict()`{.xref .py .py-func
    .docutils .literal
    .notranslate}]{.pre}](#scrapy.utils.request.request_from_dict &quot;scrapy.utils.request.request_from_dict&quot;){.reference
    .internal}.

[[copy]{.pre}]{.sig-name .descname}[(]{.sig-paren}[)]{.sig-paren}[[[\[source\]]{.pre}]{.viewcode-link}](_modules/scrapy/http/request.html#Request.copy){.reference .internal}[¶](#scrapy.http.Request.copy &quot;Permalink to this definition&quot;){.headerlink}

:   Return a new Request which is a copy of this Request. See also:
    [[Passing additional data to callback functions]{.std
    .std-ref}](#topics-request-response-ref-request-callback-arguments){.hoverxref
    .tooltip .reference .internal}.

[[replace]{.pre}]{.sig-name .descname}[(]{.sig-paren}[\[]{.optional}*[[url]{.pre}]{.n}*, *[[method]{.pre}]{.n}*, *[[headers]{.pre}]{.n}*, *[[body]{.pre}]{.n}*, *[[cookies]{.pre}]{.n}*, *[[meta]{.pre}]{.n}*, *[[flags]{.pre}]{.n}*, *[[encoding]{.pre}]{.n}*, *[[priority]{.pre}]{.n}*, *[[dont_filter]{.pre}]{.n}*, *[[callback]{.pre}]{.n}*, *[[errback]{.pre}]{.n}*, *[[cb_kwargs]{.pre}]{.n}*[\]]{.optional}[)]{.sig-paren}[[[\[source\]]{.pre}]{.viewcode-link}](_modules/scrapy/http/request.html#Request.replace){.reference .internal}[¶](#scrapy.http.Request.replace &quot;Permalink to this definition&quot;){.headerlink}

:   Return a Request object with the same members, except for those
    members given new values by whichever keyword arguments are
    specified. The [[`Request.cb_kwargs`{.xref .py .py-attr
    .docutils .literal
    .notranslate}]{.pre}](#scrapy.http.Request.cb_kwargs &quot;scrapy.http.Request.cb_kwargs&quot;){.reference
    .internal} and [[`Request.meta`{.xref .py .py-attr .docutils
    .literal
    .notranslate}]{.pre}](#scrapy.http.Request.meta &quot;scrapy.http.Request.meta&quot;){.reference
    .internal} attributes are shallow copied by default (unless new
    values are given as arguments). See also [[Passing additional
    data to callback functions]{.std
    .std-ref}](#topics-request-response-ref-request-callback-arguments){.hoverxref
    .tooltip .reference .internal}.

*[classmethod]{.pre}[ ]{.w}*[[from_curl]{.pre}]{.sig-name .descname}[(]{.sig-paren}*[[curl_command]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[[[str]{.pre}](https://docs.python.org/3/library/stdtypes.html#str &quot;(in Python v3.12)&quot;){.reference .external}]{.n}*, *[[ignore_unknown_options]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[[[bool]{.pre}](https://docs.python.org/3/library/functions.html#bool &quot;(in Python v3.12)&quot;){.reference .external}]{.n}[ ]{.w}[[=]{.pre}]{.o}[ ]{.w}[[True]{.pre}]{.default_value}*, *[[\*\*]{.pre}]{.o}[[kwargs]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[[[Any]{.pre}](https://docs.python.org/3/library/typing.html#typing.Any &quot;(in Python v3.12)&quot;){.reference .external}]{.n}*[)]{.sig-paren} [[→]{.sig-return-icon} [[RequestTypeVar]{.pre}]{.sig-return-typehint}]{.sig-return}[[[\[source\]]{.pre}]{.viewcode-link}](_modules/scrapy/http/request.html#Request.from_curl){.reference .internal}[¶](#scrapy.http.Request.from_curl &quot;Permalink to this definition&quot;){.headerlink}

:   Create a Request object from a string containing a
    [cURL](https://curl.haxx.se/){.reference .external} command. It
    populates the HTTP method, the URL, the headers, the cookies and
    the body. It accepts the same arguments as the [[`Request`{.xref
    .py .py-class .docutils .literal
    .notranslate}]{.pre}](#scrapy.http.Request &quot;scrapy.http.Request&quot;){.reference
    .internal} class, taking preference and overriding the values of
    the same arguments contained in the cURL command.

    Unrecognized options are ignored by default. To raise an error
    when finding unknown options call this method by passing
    [`ignore_unknown_options=False`{.docutils .literal
    .notranslate}]{.pre}.

    ::: {.admonition .caution}
    Caution

    Using [[`from_curl()`{.xref .py .py-meth .docutils .literal
    .notranslate}]{.pre}](#scrapy.http.Request.from_curl &quot;scrapy.http.Request.from_curl&quot;){.reference
    .internal} from [[`Request`{.xref .py .py-class .docutils
    .literal
    .notranslate}]{.pre}](#scrapy.http.Request &quot;scrapy.http.Request&quot;){.reference
    .internal} subclasses, such as [[`JsonRequest`{.xref .py
    .py-class .docutils .literal
    .notranslate}]{.pre}](#scrapy.http.JsonRequest &quot;scrapy.http.JsonRequest&quot;){.reference
    .internal}, or [`XmlRpcRequest`{.xref .py .py-class .docutils
    .literal .notranslate}]{.pre}, as well as having [[downloader
    middlewares]{.std
    .std-ref}](index.html#topics-downloader-middleware){.hoverxref
    .tooltip .reference .internal} and [[spider middlewares]{.std
    .std-ref}](index.html#topics-spider-middleware){.hoverxref
    .tooltip .reference .internal} enabled, such as
    [[`DefaultHeadersMiddleware`{.xref .py .py-class .docutils
    .literal
    .notranslate}]{.pre}](index.html#scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware &quot;scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware&quot;){.reference
    .internal}, [[`UserAgentMiddleware`{.xref .py .py-class
    .docutils .literal
    .notranslate}]{.pre}](index.html#scrapy.downloadermiddlewares.useragent.UserAgentMiddleware &quot;scrapy.downloadermiddlewares.useragent.UserAgentMiddleware&quot;){.reference
    .internal}, or [[`HttpCompressionMiddleware`{.xref .py .py-class
    .docutils .literal
    .notranslate}]{.pre}](index.html#scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware &quot;scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware&quot;){.reference
    .internal}, may modify the [[`Request`{.xref .py .py-class
    .docutils .literal
    .notranslate}]{.pre}](#scrapy.http.Request &quot;scrapy.http.Request&quot;){.reference
    .internal} object.
    :::

    To translate a cURL command into a Scrapy request, you may use
    [curl2scrapy](https://michael-shub.github.io/curl2scrapy/){.reference
    .external}.

[[to_dict]{.pre}]{.sig-name .descname}[(]{.sig-paren}*[[\*]{.pre}]{.o}*, *[[spider]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[[[Optional]{.pre}](https://docs.python.org/3/library/typing.html#typing.Optional &quot;(in Python v3.12)&quot;){.reference .external}[[\[]{.pre}]{.p}[[Spider]{.pre}](index.html#scrapy.spiders.Spider &quot;scrapy.spiders.Spider&quot;){.reference .internal}[[\]]{.pre}]{.p}]{.n}[ ]{.w}[[=]{.pre}]{.o}[ ]{.w}[[None]{.pre}]{.default_value}*[)]{.sig-paren} [[→]{.sig-return-icon} [[[Dict]{.pre}](https://docs.python.org/3/library/typing.html#typing.Dict &quot;(in Python v3.12)&quot;){.reference .external}[[\[]{.pre}]{.p}[[str]{.pre}](https://docs.python.org/3/library/stdtypes.html#str &quot;(in Python v3.12)&quot;){.reference .external}[[,]{.pre}]{.p}[ ]{.w}[[Any]{.pre}](https://docs.python.org/3/library/typing.html#typing.Any &quot;(in Python v3.12)&quot;){.reference .external}[[\]]{.pre}]{.p}]{.sig-return-typehint}]{.sig-return}[[[\[source\]]{.pre}]{.viewcode-link}](_modules/scrapy/http/request.html#Request.to_dict){.reference .internal}[¶](#scrapy.http.Request.to_dict &quot;Permalink to this definition&quot;){.headerlink}

:   Return a dictionary containing the Request's data.

    Use [[`request_from_dict()`{.xref .py .py-func .docutils
    .literal
    .notranslate}]{.pre}](#scrapy.utils.request.request_from_dict &quot;scrapy.utils.request.request_from_dict&quot;){.reference
    .internal} to convert back into a [`Request`{.xref .py .py-class
    .docutils .literal .notranslate}]{.pre} object.

    If a spider is given, this method will try to find out the name
    of the spider methods used as callback and errback and include
    them in the output dict, raising an exception if they cannot be
    found.
</code></pre>
<p>::: {#other-functions-related-to-requests .section}</p>
<h5 id="other-functions-related-to-requestsheaderlink"><a class="header" href="#other-functions-related-to-requestsheaderlink">Other functions related to requests<a href="#other-functions-related-to-requests" title="Permalink to this heading">¶</a>{.headerlink}</a></h5>
<p>[[scrapy.http.request.]{.pre}]{.sig-prename .descclassname}[[NO_CALLBACK]{.pre}]{.sig-name .descname}[(]{.sig-paren}<em>[[*]{.pre}]{.o}[[args]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[<a href="https://docs.python.org/3/library/typing.html#typing.Any" title="(in Python v3.12)">[Any]{.pre}</a>{.reference .external}]{.n}</em>, <em>[[**]{.pre}]{.o}[[kwargs]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[<a href="https://docs.python.org/3/library/typing.html#typing.Any" title="(in Python v3.12)">[Any]{.pre}</a>{.reference .external}]{.n}</em>[)]{.sig-paren} [[→]{.sig-return-icon} [<a href="https://docs.python.org/3/library/typing.html#typing.NoReturn" title="(in Python v3.12)">[NoReturn]{.pre}</a>{.reference .external}]{.sig-return-typehint}]{.sig-return}<a href="_modules/scrapy/http/request.html#NO_CALLBACK">[[[source]]{.pre}]{.viewcode-link}</a>{.reference .internal}<a href="#scrapy.http.request.NO_CALLBACK" title="Permalink to this definition">¶</a>{.headerlink}</p>
<p>:   When assigned to the [<code>callback</code>{.docutils .literal
.notranslate}]{.pre} parameter of <a href="#scrapy.http.Request" title="scrapy.http.Request">[<code>Request</code>{.xref .py .py-class
.docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal}, it indicates that the request is not meant to have a
spider callback at all.</p>
<pre><code>For example:

::: {.highlight-python .notranslate}
::: highlight
    Request(&quot;https://example.com&quot;, callback=NO_CALLBACK)
:::
:::

This value should be used by [[components]{.std
.std-ref}](index.html#topics-components){.hoverxref .tooltip
.reference .internal} that create and handle their own requests,
e.g. through [`scrapy.core.engine.ExecutionEngine.download()`{.xref
.py .py-meth .docutils .literal .notranslate}]{.pre}, so that
downloader middlewares handling such requests can treat them
differently from requests intended for the [[`parse()`{.xref .py
.py-meth .docutils .literal
.notranslate}]{.pre}](index.html#scrapy.Spider.parse &quot;scrapy.Spider.parse&quot;){.reference
.internal} callback.
</code></pre>
<pre><code class="language-{=html}">&lt;!-- --&gt;
</code></pre>
<p>[[scrapy.utils.request.]{.pre}]{.sig-prename .descclassname}[[request_from_dict]{.pre}]{.sig-name .descname}[(]{.sig-paren}<em>[[d]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[<a href="https://docs.python.org/3/library/stdtypes.html#dict" title="(in Python v3.12)">[dict]{.pre}</a>{.reference .external}]{.n}</em>, <em>[[*]{.pre}]{.o}</em>, <em>[[spider]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[<a href="https://docs.python.org/3/library/typing.html#typing.Optional" title="(in Python v3.12)">[Optional]{.pre}</a>{.reference .external}[[[]{.pre}]{.p}<a href="index.html#scrapy.spiders.Spider" title="scrapy.spiders.Spider">[Spider]{.pre}</a>{.reference .internal}[[]]{.pre}]{.p}]{.n}[ ]{.w}[[=]{.pre}]{.o}[ ]{.w}[[None]{.pre}]{.default_value}</em>[)]{.sig-paren} [[→]{.sig-return-icon} [<a href="index.html#scrapy.http.Request" title="scrapy.http.request.Request">[Request]{.pre}</a>{.reference .internal}]{.sig-return-typehint}]{.sig-return}<a href="_modules/scrapy/utils/request.html#request_from_dict">[[[source]]{.pre}]{.viewcode-link}</a>{.reference .internal}<a href="#scrapy.utils.request.request_from_dict" title="Permalink to this definition">¶</a>{.headerlink}</p>
<p>:   Create a [<code>Request</code>{.xref .py .py-class .docutils .literal
.notranslate}]{.pre} object from a dict.</p>
<pre><code>If a spider is given, it will try to resolve the callbacks looking
at the spider for methods with the same name.
</code></pre>
<p>:::</p>
<p>::: {#passing-additional-data-to-callback-functions .section}
[]{#topics-request-response-ref-request-callback-arguments}</p>
<h5 id="passing-additional-data-to-callback-functionsheaderlink"><a class="header" href="#passing-additional-data-to-callback-functionsheaderlink">Passing additional data to callback functions<a href="#passing-additional-data-to-callback-functions" title="Permalink to this heading">¶</a>{.headerlink}</a></h5>
<p>The callback of a request is a function that will be called when the
response of that request is downloaded. The callback function will be
called with the downloaded <a href="#scrapy.http.Response" title="scrapy.http.Response">[<code>Response</code>{.xref .py .py-class .docutils
.literal
.notranslate}]{.pre}</a>{.reference
.internal} object as its first argument.</p>
<p>Example:</p>
<p>::: {.highlight-python .notranslate}
::: highlight
def parse_page1(self, response):
return scrapy.Request(
&quot;http://www.example.com/some_page.html&quot;, callback=self.parse_page2
)</p>
<pre><code>def parse_page2(self, response):
    # this would log http://www.example.com/some_page.html
    self.logger.info(&quot;Visited %s&quot;, response.url)
</code></pre>
<p>:::
:::</p>
<p>In some cases you may be interested in passing arguments to those
callback functions so you can receive the arguments later, in the second
callback. The following example shows how to achieve this by using the
<a href="#scrapy.http.Request.cb_kwargs" title="scrapy.http.Request.cb_kwargs">[<code>Request.cb_kwargs</code>{.xref .py .py-attr .docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal} attribute:</p>
<p>::: {.highlight-python .notranslate}
::: highlight
def parse(self, response):
request = scrapy.Request(
&quot;http://www.example.com/index.html&quot;,
callback=self.parse_page2,
cb_kwargs=dict(main_url=response.url),
)
request.cb_kwargs[&quot;foo&quot;] = &quot;bar&quot;  # add more arguments for the callback
yield request</p>
<pre><code>def parse_page2(self, response, main_url, foo):
    yield dict(
        main_url=main_url,
        other_url=response.url,
        foo=foo,
    )
</code></pre>
<p>:::
:::</p>
<p>::: {.admonition .caution}
Caution</p>
<p><a href="#scrapy.http.Request.cb_kwargs" title="scrapy.http.Request.cb_kwargs">[<code>Request.cb_kwargs</code>{.xref .py .py-attr .docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal} was introduced in version [<code>1.7</code>{.docutils .literal
.notranslate}]{.pre}. Prior to that, using <a href="#scrapy.http.Request.meta" title="scrapy.http.Request.meta">[<code>Request.meta</code>{.xref .py
.py-attr .docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal} was recommended for passing information around callbacks.
After [<code>1.7</code>{.docutils .literal .notranslate}]{.pre},
<a href="#scrapy.http.Request.cb_kwargs" title="scrapy.http.Request.cb_kwargs">[<code>Request.cb_kwargs</code>{.xref .py .py-attr .docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal} became the preferred way for handling user information,
leaving <a href="#scrapy.http.Request.meta" title="scrapy.http.Request.meta">[<code>Request.meta</code>{.xref .py .py-attr .docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal} for communication with components like middlewares and
extensions.
:::
:::</p>
<p>::: {#using-errbacks-to-catch-exceptions-in-request-processing .section}
[]{#topics-request-response-ref-errbacks}</p>
<h5 id="using-errbacks-to-catch-exceptions-in-request-processingheaderlink"><a class="header" href="#using-errbacks-to-catch-exceptions-in-request-processingheaderlink">Using errbacks to catch exceptions in request processing<a href="#using-errbacks-to-catch-exceptions-in-request-processing" title="Permalink to this heading">¶</a>{.headerlink}</a></h5>
<p>The errback of a request is a function that will be called when an
exception is raise while processing it.</p>
<p>It receives a <a href="https://docs.twisted.org/en/stable/api/twisted.python.failure.Failure.html" title="(in Twisted)">[<code>Failure</code>{.xref .py .py-exc .docutils .literal
.notranslate}]{.pre}</a>{.reference
.external} as first parameter and can be used to track connection
establishment timeouts, DNS errors etc.</p>
<p>Here's an example spider logging all errors and catching some specific
errors if needed:</p>
<p>::: {.highlight-python .notranslate}
::: highlight
import scrapy</p>
<pre><code>from scrapy.spidermiddlewares.httperror import HttpError
from twisted.internet.error import DNSLookupError
from twisted.internet.error import TimeoutError, TCPTimedOutError


class ErrbackSpider(scrapy.Spider):
    name = &quot;errback_example&quot;
    start_urls = [
        &quot;http://www.httpbin.org/&quot;,  # HTTP 200 expected
        &quot;http://www.httpbin.org/status/404&quot;,  # Not found error
        &quot;http://www.httpbin.org/status/500&quot;,  # server issue
        &quot;http://www.httpbin.org:12345/&quot;,  # non-responding host, timeout expected
        &quot;https://example.invalid/&quot;,  # DNS error expected
    ]

    def start_requests(self):
        for u in self.start_urls:
            yield scrapy.Request(
                u,
                callback=self.parse_httpbin,
                errback=self.errback_httpbin,
                dont_filter=True,
            )

    def parse_httpbin(self, response):
        self.logger.info(&quot;Got successful response from {}&quot;.format(response.url))
        # do something useful here...

    def errback_httpbin(self, failure):
        # log all failures
        self.logger.error(repr(failure))

        # in case you want to do something special for some errors,
        # you may need the failure's type:

        if failure.check(HttpError):
            # these exceptions come from HttpError spider middleware
            # you can get the non-200 response
            response = failure.value.response
            self.logger.error(&quot;HttpError on %s&quot;, response.url)

        elif failure.check(DNSLookupError):
            # this is the original request
            request = failure.request
            self.logger.error(&quot;DNSLookupError on %s&quot;, request.url)

        elif failure.check(TimeoutError, TCPTimedOutError):
            request = failure.request
            self.logger.error(&quot;TimeoutError on %s&quot;, request.url)
</code></pre>
<p>:::
:::
:::</p>
<p>::: {#accessing-additional-data-in-errback-functions .section}
[]{#errback-cb-kwargs}</p>
<h5 id="accessing-additional-data-in-errback-functionsheaderlink"><a class="header" href="#accessing-additional-data-in-errback-functionsheaderlink">Accessing additional data in errback functions<a href="#accessing-additional-data-in-errback-functions" title="Permalink to this heading">¶</a>{.headerlink}</a></h5>
<p>In case of a failure to process the request, you may be interested in
accessing arguments to the callback functions so you can process further
based on the arguments in the errback. The following example shows how
to achieve this by using [<code>Failure.request.cb_kwargs</code>{.docutils .literal
.notranslate}]{.pre}:</p>
<p>::: {.highlight-python .notranslate}
::: highlight
def parse(self, response):
request = scrapy.Request(
&quot;http://www.example.com/index.html&quot;,
callback=self.parse_page2,
errback=self.errback_page2,
cb_kwargs=dict(main_url=response.url),
)
yield request</p>
<pre><code>def parse_page2(self, response, main_url):
    pass


def errback_page2(self, failure):
    yield dict(
        main_url=failure.request.cb_kwargs[&quot;main_url&quot;],
    )
</code></pre>
<p>:::
:::
:::</p>
<p>::: {#request-fingerprints .section}
[]{#id1}</p>
<h5 id="request-fingerprintsheaderlink"><a class="header" href="#request-fingerprintsheaderlink">Request fingerprints<a href="#request-fingerprints" title="Permalink to this heading">¶</a>{.headerlink}</a></h5>
<p>There are some aspects of scraping, such as filtering out duplicate
requests (see <a href="index.html#std-setting-DUPEFILTER_CLASS">[<code>DUPEFILTER_CLASS</code>{.xref .std .std-setting .docutils
.literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal}) or caching responses (see
<a href="index.html#std-setting-HTTPCACHE_POLICY">[<code>HTTPCACHE_POLICY</code>{.xref .std .std-setting .docutils .literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal}), where you need the ability to generate
a short, unique identifier from a <a href="#scrapy.http.Request" title="scrapy.http.Request">[<code>Request</code>{.xref .py .py-class
.docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal} object: a request fingerprint.</p>
<p>You often do not need to worry about request fingerprints, the default
request fingerprinter works for most projects.</p>
<p>However, there is no universal way to generate a unique identifier from
a request, because different situations require comparing requests
differently. For example, sometimes you may need to compare URLs
case-insensitively, include URL fragments, exclude certain URL query
parameters, include some or all headers, etc.</p>
<p>To change how request fingerprints are built for your requests, use the
<a href="#std-setting-REQUEST_FINGERPRINTER_CLASS">[<code>REQUEST_FINGERPRINTER_CLASS</code>{.xref .std .std-setting .docutils
.literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal} setting.</p>
<p>::: {#request-fingerprinter-class .section}
[]{#std-setting-REQUEST_FINGERPRINTER_CLASS}</p>
<h6 id="request_fingerprinter_classheaderlink"><a class="header" href="#request_fingerprinter_classheaderlink">REQUEST_FINGERPRINTER_CLASS<a href="#request-fingerprinter-class" title="Permalink to this heading">¶</a>{.headerlink}</a></h6>
<p>::: versionadded
[New in version 2.7.]{.versionmodified .added}
:::</p>
<p>Default: <a href="#scrapy.utils.request.RequestFingerprinter" title="scrapy.utils.request.RequestFingerprinter">[<code>scrapy.utils.request.RequestFingerprinter</code>{.xref .py
.py-class .docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal}</p>
<p>A <a href="#custom-request-fingerprinter">[request fingerprinter class]{.std
.std-ref}</a>{.hoverxref .tooltip .reference
.internal} or its import path.</p>
<p><em>[class]{.pre}[ ]{.w}</em>[[scrapy.utils.request.]{.pre}]{.sig-prename .descclassname}[[RequestFingerprinter]{.pre}]{.sig-name .descname}[(]{.sig-paren}<em>[[crawler]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[<a href="https://docs.python.org/3/library/typing.html#typing.Optional" title="(in Python v3.12)">[Optional]{.pre}</a>{.reference .external}[[[]{.pre}]{.p}<a href="index.html#scrapy.crawler.Crawler" title="scrapy.crawler.Crawler">[Crawler]{.pre}</a>{.reference .internal}[[]]{.pre}]{.p}]{.n}[ ]{.w}[[=]{.pre}]{.o}[ ]{.w}[[None]{.pre}]{.default_value}</em>[)]{.sig-paren}<a href="_modules/scrapy/utils/request.html#RequestFingerprinter">[[[source]]{.pre}]{.viewcode-link}</a>{.reference .internal}<a href="#scrapy.utils.request.RequestFingerprinter" title="Permalink to this definition">¶</a>{.headerlink}</p>
<p>:   Default fingerprinter.</p>
<pre><code>It takes into account a canonical version
([[`w3lib.url.canonicalize_url()`{.xref .py .py-func .docutils
.literal
.notranslate}]{.pre}](https://w3lib.readthedocs.io/en/latest/w3lib.html#w3lib.url.canonicalize_url &quot;(in w3lib v2.1)&quot;){.reference
.external}) of [[`request.url`{.xref .py .py-attr .docutils .literal
.notranslate}]{.pre}](#scrapy.http.Request.url &quot;scrapy.http.Request.url&quot;){.reference
.internal} and the values of [[`request.method`{.xref .py .py-attr
.docutils .literal
.notranslate}]{.pre}](#scrapy.http.Request.method &quot;scrapy.http.Request.method&quot;){.reference
.internal} and [[`request.body`{.xref .py .py-attr .docutils
.literal
.notranslate}]{.pre}](#scrapy.http.Request.body &quot;scrapy.http.Request.body&quot;){.reference
.internal}. It then generates an
[SHA1](https://en.wikipedia.org/wiki/SHA-1){.reference .external}
hash.

::: {.admonition .seealso}
See also

[[`REQUEST_FINGERPRINTER_IMPLEMENTATION`{.xref .std .std-setting
.docutils .literal
.notranslate}]{.pre}](#std-setting-REQUEST_FINGERPRINTER_IMPLEMENTATION){.hoverxref
.tooltip .reference .internal}.
:::
</code></pre>
<p>:::</p>
<p>::: {#request-fingerprinter-implementation .section}
[]{#std-setting-REQUEST_FINGERPRINTER_IMPLEMENTATION}</p>
<h6 id="request_fingerprinter_implementationheaderlink"><a class="header" href="#request_fingerprinter_implementationheaderlink">REQUEST_FINGERPRINTER_IMPLEMENTATION<a href="#request-fingerprinter-implementation" title="Permalink to this heading">¶</a>{.headerlink}</a></h6>
<p>::: versionadded
[New in version 2.7.]{.versionmodified .added}
:::</p>
<p>Default: [<code>'2.6'</code>{.docutils .literal .notranslate}]{.pre}</p>
<p>Determines which request fingerprinting algorithm is used by the default
request fingerprinter class (see <a href="#std-setting-REQUEST_FINGERPRINTER_CLASS">[<code>REQUEST_FINGERPRINTER_CLASS</code>{.xref
.std .std-setting .docutils .literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal}).</p>
<p>Possible values are:</p>
<ul>
<li>
<p>[<code>'2.6'</code>{.docutils .literal .notranslate}]{.pre} (default)</p>
<p>This implementation uses the same request fingerprinting algorithm
as Scrapy 2.6 and earlier versions.</p>
<p>Even though this is the default value for backward compatibility
reasons, it is a deprecated value.</p>
</li>
<li>
<p>[<code>'2.7'</code>{.docutils .literal .notranslate}]{.pre}</p>
<p>This implementation was introduced in Scrapy 2.7 to fix an issue of
the previous implementation.</p>
<p>New projects should use this value. The <a href="index.html#std-command-startproject">[<code>startproject</code>{.xref .std
.std-command .docutils .literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal} command sets this value in the
generated [<code>settings.py</code>{.docutils .literal .notranslate}]{.pre}
file.</p>
</li>
</ul>
<p>If you are using the default value ([<code>'2.6'</code>{.docutils .literal
.notranslate}]{.pre}) for this setting, and you are using Scrapy
components where changing the request fingerprinting algorithm would
cause undesired results, you need to carefully decide when to change the
value of this setting, or switch the
<a href="#std-setting-REQUEST_FINGERPRINTER_CLASS">[<code>REQUEST_FINGERPRINTER_CLASS</code>{.xref .std .std-setting .docutils
.literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal} setting to a custom request fingerprinter
class that implements the 2.6 request fingerprinting algorithm and does
not log this warning ( <a href="#request-fingerprinter">[Writing your own request fingerprinter]{.std
.std-ref}</a>{.hoverxref .tooltip .reference
.internal} includes an example implementation of such a class).</p>
<p>Scenarios where changing the request fingerprinting algorithm may cause
undesired results include, for example, using the HTTP cache middleware
(see <a href="index.html#scrapy.downloadermiddlewares.httpcache.HttpCacheMiddleware" title="scrapy.downloadermiddlewares.httpcache.HttpCacheMiddleware">[<code>HttpCacheMiddleware</code>{.xref .py .py-class .docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal}). Changing the request fingerprinting algorithm would
invalidate the current cache, requiring you to redownload all requests
again.</p>
<p>Otherwise, set <a href="#std-setting-REQUEST_FINGERPRINTER_IMPLEMENTATION">[<code>REQUEST_FINGERPRINTER_IMPLEMENTATION</code>{.xref .std
.std-setting .docutils .literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal} to [<code>'2.7'</code>{.docutils .literal
.notranslate}]{.pre} in your settings to switch already to the request
fingerprinting implementation that will be the only request
fingerprinting implementation available in a future version of Scrapy,
and remove the deprecation warning triggered by using the default value
([<code>'2.6'</code>{.docutils .literal .notranslate}]{.pre}).
:::</p>
<p>::: {#writing-your-own-request-fingerprinter .section}
[]{#custom-request-fingerprinter}[]{#request-fingerprinter}</p>
<h6 id="writing-your-own-request-fingerprinterheaderlink"><a class="header" href="#writing-your-own-request-fingerprinterheaderlink">Writing your own request fingerprinter<a href="#writing-your-own-request-fingerprinter" title="Permalink to this heading">¶</a>{.headerlink}</a></h6>
<p>A request fingerprinter is a class that must implement the following
method:</p>
<p>[[fingerprint]{.pre}]{.sig-name .descname}[(]{.sig-paren}<em>[[self]{.pre}]{.n}</em>, <em>[[request]{.pre}]{.n}</em>[)]{.sig-paren}<a href="#fingerprint" title="Permalink to this definition">¶</a>{.headerlink}</p>
<p>:   Return a <a href="https://docs.python.org/3/library/stdtypes.html#bytes" title="(in Python v3.12)">[<code>bytes</code>{.xref .py .py-class .docutils .literal
.notranslate}]{.pre}</a>{.reference
.external} object that uniquely identifies <em>request</em>.</p>
<pre><code>See also [[Request fingerprint restrictions]{.std
.std-ref}](#request-fingerprint-restrictions){.hoverxref .tooltip
.reference .internal}.

Parameters

:   **request**
    ([*scrapy.http.Request*](index.html#scrapy.http.Request &quot;scrapy.http.Request&quot;){.reference
    .internal}) -- request to fingerprint
</code></pre>
<p>Additionally, it may also implement the following methods:</p>
<p><em>[classmethod]{.pre}[ ]{.w}</em>[[from_crawler]{.pre}]{.sig-name .descname}[(]{.sig-paren}<em>[[cls]{.pre}]{.n}</em>, <em>[[crawler]{.pre}]{.n}</em>[)]{.sig-paren}</p>
<p>:   If present, this class method is called to create a request
fingerprinter instance from a <a href="index.html#scrapy.crawler.Crawler" title="scrapy.crawler.Crawler">[<code>Crawler</code>{.xref .py .py-class
.docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal} object. It must return a new instance of the request
fingerprinter.</p>
<pre><code>*crawler* provides access to all Scrapy core components like
settings and signals; it is a way for the request fingerprinter to
access them and hook its functionality into Scrapy.

Parameters

:   **crawler** ([[`Crawler`{.xref .py .py-class .docutils .literal
    .notranslate}]{.pre}](index.html#scrapy.crawler.Crawler &quot;scrapy.crawler.Crawler&quot;){.reference
    .internal} object) -- crawler that uses this request
    fingerprinter
</code></pre>
<pre><code class="language-{=html}">&lt;!-- --&gt;
</code></pre>
<p><em>[classmethod]{.pre}[ ]{.w}</em>[[from_settings]{.pre}]{.sig-name .descname}[(]{.sig-paren}<em>[[cls]{.pre}]{.n}</em>, <em>[[settings]{.pre}]{.n}</em>[)]{.sig-paren}<a href="#from_settings" title="Permalink to this definition">¶</a>{.headerlink}</p>
<p>:   If present, and [<code>from_crawler</code>{.docutils .literal
.notranslate}]{.pre} is not defined, this class method is called to
create a request fingerprinter instance from a <a href="index.html#scrapy.settings.Settings" title="scrapy.settings.Settings">[<code>Settings</code>{.xref
.py .py-class .docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal} object. It must return a new instance of the request
fingerprinter.</p>
<p>The <a href="#fingerprint" title="fingerprint">[<code>fingerprint()</code>{.xref .py .py-meth .docutils .literal
.notranslate}]{.pre}</a>{.reference .internal}
method of the default request fingerprinter,
<a href="#scrapy.utils.request.RequestFingerprinter" title="scrapy.utils.request.RequestFingerprinter">[<code>scrapy.utils.request.RequestFingerprinter</code>{.xref .py .py-class
.docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal}, uses <a href="#scrapy.utils.request.fingerprint" title="scrapy.utils.request.fingerprint">[<code>scrapy.utils.request.fingerprint()</code>{.xref .py
.py-func .docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal} with its default parameters. For some common use cases you
can use <a href="#scrapy.utils.request.fingerprint" title="scrapy.utils.request.fingerprint">[<code>scrapy.utils.request.fingerprint()</code>{.xref .py .py-func
.docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal} as well in your <a href="#fingerprint" title="fingerprint">[<code>fingerprint()</code>{.xref .py .py-meth
.docutils .literal
.notranslate}]{.pre}</a>{.reference .internal}
method implementation:</p>
<p>[[scrapy.utils.request.]{.pre}]{.sig-prename .descclassname}[[fingerprint]{.pre}]{.sig-name .descname}[(]{.sig-paren}<em>[[request]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[<a href="index.html#scrapy.http.Request" title="scrapy.http.request.Request">[Request]{.pre}</a>{.reference .internal}]{.n}</em>, <em>[[*]{.pre}]{.o}</em>, <em>[[include_headers]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[<a href="https://docs.python.org/3/library/typing.html#typing.Optional" title="(in Python v3.12)">[Optional]{.pre}</a>{.reference .external}[[[]{.pre}]{.p}<a href="https://docs.python.org/3/library/typing.html#typing.Iterable" title="(in Python v3.12)">[Iterable]{.pre}</a>{.reference .external}[[[]{.pre}]{.p}<a href="https://docs.python.org/3/library/typing.html#typing.Union" title="(in Python v3.12)">[Union]{.pre}</a>{.reference .external}[[[]{.pre}]{.p}<a href="https://docs.python.org/3/library/stdtypes.html#bytes" title="(in Python v3.12)">[bytes]{.pre}</a>{.reference .external}[[,]{.pre}]{.p}[ ]{.w}<a href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.12)">[str]{.pre}</a>{.reference .external}[[]]{.pre}]{.p}[[]]{.pre}]{.p}[[]]{.pre}]{.p}]{.n}[ ]{.w}[[=]{.pre}]{.o}[ ]{.w}[[None]{.pre}]{.default_value}</em>, <em>[[keep_fragments]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[<a href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.12)">[bool]{.pre}</a>{.reference .external}]{.n}[ ]{.w}[[=]{.pre}]{.o}[ ]{.w}[[False]{.pre}]{.default_value}</em>[)]{.sig-paren} [[→]{.sig-return-icon} [<a href="https://docs.python.org/3/library/stdtypes.html#bytes" title="(in Python v3.12)">[bytes]{.pre}</a>{.reference .external}]{.sig-return-typehint}]{.sig-return}<a href="_modules/scrapy/utils/request.html#fingerprint">[[[source]]{.pre}]{.viewcode-link}</a>{.reference .internal}<a href="#scrapy.utils.request.fingerprint" title="Permalink to this definition">¶</a>{.headerlink}</p>
<p>:   Return the request fingerprint.</p>
<pre><code>The request fingerprint is a hash that uniquely identifies the
resource the request points to. For example, take the following two
urls:

[http://www.example.com/query?id=111&amp;cat=222](http://www.example.com/query?id=111&amp;cat=222){.reference
.external}
[http://www.example.com/query?cat=222&amp;id=111](http://www.example.com/query?cat=222&amp;id=111){.reference
.external}

Even though those are two different URLs both point to the same
resource and are equivalent (i.e. they should return the same
response).

Another example are cookies used to store session ids. Suppose the
following page is only accessible to authenticated users:

[http://www.example.com/members/offers.html](http://www.example.com/members/offers.html){.reference
.external}

Lots of sites use a cookie to store the session id, which adds a
random component to the HTTP Request and thus should be ignored when
calculating the fingerprint.

For this reason, request headers are ignored by default when
calculating the fingerprint. If you want to include specific headers
use the include_headers argument, which is a list of Request headers
to include.

Also, servers usually ignore fragments in urls when handling
requests, so they are also ignored by default when calculating the
fingerprint. If you want to include them, set the keep_fragments
argument to True (for instance when handling requests with a
headless browser).
</code></pre>
<p>For example, to take the value of a request header named
[<code>X-ID</code>{.docutils .literal .notranslate}]{.pre} into account:</p>
<p>::: {.highlight-python .notranslate}
::: highlight
# my_project/settings.py
REQUEST_FINGERPRINTER_CLASS = &quot;my_project.utils.RequestFingerprinter&quot;</p>
<pre><code># my_project/utils.py
from scrapy.utils.request import fingerprint


class RequestFingerprinter:
    def fingerprint(self, request):
        return fingerprint(request, include_headers=[&quot;X-ID&quot;])
</code></pre>
<p>:::
:::</p>
<p>You can also write your own fingerprinting logic from scratch.</p>
<p>However, if you do not use <a href="#scrapy.utils.request.fingerprint" title="scrapy.utils.request.fingerprint">[<code>scrapy.utils.request.fingerprint()</code>{.xref
.py .py-func .docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal}, make sure you use <a href="https://docs.python.org/3/library/weakref.html#weakref.WeakKeyDictionary" title="(in Python v3.12)">[<code>WeakKeyDictionary</code>{.xref .py .py-class
.docutils .literal
.notranslate}]{.pre}</a>{.reference
.external} to cache request fingerprints:</p>
<ul>
<li>
<p>Caching saves CPU by ensuring that fingerprints are calculated only
once per request, and not once per Scrapy component that needs the
fingerprint of a request.</p>
</li>
<li>
<p>Using <a href="https://docs.python.org/3/library/weakref.html#weakref.WeakKeyDictionary" title="(in Python v3.12)">[<code>WeakKeyDictionary</code>{.xref .py .py-class .docutils .literal
.notranslate}]{.pre}</a>{.reference
.external} saves memory by ensuring that request objects do not stay
in memory forever just because you have references to them in your
cache dictionary.</p>
</li>
</ul>
<p>For example, to take into account only the URL of a request, without any
prior URL canonicalization or taking the request method or body into
account:</p>
<p>::: {.highlight-python .notranslate}
::: highlight
from hashlib import sha1
from weakref import WeakKeyDictionary</p>
<pre><code>from scrapy.utils.python import to_bytes


class RequestFingerprinter:
    cache = WeakKeyDictionary()

    def fingerprint(self, request):
        if request not in self.cache:
            fp = sha1()
            fp.update(to_bytes(request.url))
            self.cache[request] = fp.digest()
        return self.cache[request]
</code></pre>
<p>:::
:::</p>
<p>If you need to be able to override the request fingerprinting for
arbitrary requests from your spider callbacks, you may implement a
request fingerprinter that reads fingerprints from
<a href="#scrapy.http.Request.meta" title="scrapy.http.Request.meta">[<code>request.meta</code>{.xref .py .py-attr .docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal} when available, and then falls back to
<a href="#scrapy.utils.request.fingerprint" title="scrapy.utils.request.fingerprint">[<code>scrapy.utils.request.fingerprint()</code>{.xref .py .py-func .docutils
.literal
.notranslate}]{.pre}</a>{.reference
.internal}. For example:</p>
<p>::: {.highlight-python .notranslate}
::: highlight
from scrapy.utils.request import fingerprint</p>
<pre><code>class RequestFingerprinter:
    def fingerprint(self, request):
        if &quot;fingerprint&quot; in request.meta:
            return request.meta[&quot;fingerprint&quot;]
        return fingerprint(request)
</code></pre>
<p>:::
:::</p>
<p>If you need to reproduce the same fingerprinting algorithm as Scrapy 2.6
without using the deprecated [<code>'2.6'</code>{.docutils .literal
.notranslate}]{.pre} value of the
<a href="#std-setting-REQUEST_FINGERPRINTER_IMPLEMENTATION">[<code>REQUEST_FINGERPRINTER_IMPLEMENTATION</code>{.xref .std .std-setting
.docutils .literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal} setting, use the following request
fingerprinter:</p>
<p>::: {.highlight-python .notranslate}
::: highlight
from hashlib import sha1
from weakref import WeakKeyDictionary</p>
<pre><code>from scrapy.utils.python import to_bytes
from w3lib.url import canonicalize_url


class RequestFingerprinter:
    cache = WeakKeyDictionary()

    def fingerprint(self, request):
        if request not in self.cache:
            fp = sha1()
            fp.update(to_bytes(request.method))
            fp.update(to_bytes(canonicalize_url(request.url)))
            fp.update(request.body or b&quot;&quot;)
            self.cache[request] = fp.digest()
        return self.cache[request]
</code></pre>
<p>:::
:::
:::</p>
<p>::: {#request-fingerprint-restrictions .section}
[]{#id2}</p>
<h6 id="request-fingerprint-restrictionsheaderlink"><a class="header" href="#request-fingerprint-restrictionsheaderlink">Request fingerprint restrictions<a href="#request-fingerprint-restrictions" title="Permalink to this heading">¶</a>{.headerlink}</a></h6>
<p>Scrapy components that use request fingerprints may impose additional
restrictions on the format of the fingerprints that your <a href="#custom-request-fingerprinter">[request
fingerprinter]{.std .std-ref}</a>{.hoverxref
.tooltip .reference .internal} generates.</p>
<p>The following built-in Scrapy components have such restrictions:</p>
<ul>
<li>
<p><a href="index.html#scrapy.extensions.httpcache.FilesystemCacheStorage" title="scrapy.extensions.httpcache.FilesystemCacheStorage">[<code>scrapy.extensions.httpcache.FilesystemCacheStorage</code>{.xref .py
.py-class .docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal} (default value of <a href="index.html#std-setting-HTTPCACHE_STORAGE">[<code>HTTPCACHE_STORAGE</code>{.xref .std
.std-setting .docutils .literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal})</p>
<p>Request fingerprints must be at least 1 byte long.</p>
<p>Path and filename length limits of the file system of
<a href="index.html#std-setting-HTTPCACHE_DIR">[<code>HTTPCACHE_DIR</code>{.xref .std .std-setting .docutils .literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal} also apply. Inside
<a href="index.html#std-setting-HTTPCACHE_DIR">[<code>HTTPCACHE_DIR</code>{.xref .std .std-setting .docutils .literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal}, the following directory structure is
created:</p>
<ul>
<li>
<p>[<code>Spider.name</code>{.xref .py .py-attr .docutils .literal
.notranslate}]{.pre}</p>
<ul>
<li>
<p>first byte of a request fingerprint as hexadecimal</p>
<ul>
<li>
<p>fingerprint as hexadecimal</p>
<ul>
<li>filenames up to 16 characters long</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
<p>For example, if a request fingerprint is made of 20 bytes (default),
<a href="index.html#std-setting-HTTPCACHE_DIR">[<code>HTTPCACHE_DIR</code>{.xref .std .std-setting .docutils .literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal} is
[<code>'/home/user/project/.scrapy/httpcache'</code>{.docutils .literal
.notranslate}]{.pre}, and the name of your spider is
[<code>'my_spider'</code>{.docutils .literal .notranslate}]{.pre} your file
system must support a file path like:</p>
<p>::: {.highlight-default .notranslate}
::: highlight
/home/user/project/.scrapy/httpcache/my_spider/01/0123456789abcdef0123456789abcdef01234567/response_headers
:::
:::</p>
</li>
<li>
<p><a href="index.html#scrapy.extensions.httpcache.DbmCacheStorage" title="scrapy.extensions.httpcache.DbmCacheStorage">[<code>scrapy.extensions.httpcache.DbmCacheStorage</code>{.xref .py .py-class
.docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal}</p>
<p>The underlying DBM implementation must support keys as long as twice
the number of bytes of a request fingerprint, plus 5. For example,
if a request fingerprint is made of 20 bytes (default),
45-character-long keys must be supported.
:::
:::
:::</p>
</li>
</ul>
<p>::: {#request-meta-special-keys .section}
[]{#topics-request-meta}</p>
<h4 id="requestmeta-special-keysheaderlink"><a class="header" href="#requestmeta-special-keysheaderlink">Request.meta special keys<a href="#request-meta-special-keys" title="Permalink to this heading">¶</a>{.headerlink}</a></h4>
<p>The <a href="#scrapy.http.Request.meta" title="scrapy.http.Request.meta">[<code>Request.meta</code>{.xref .py .py-attr .docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal} attribute can contain any arbitrary data, but there are some
special keys recognized by Scrapy and its built-in extensions.</p>
<p>Those are:</p>
<ul>
<li>
<p><a href="#std-reqmeta-bindaddress">[<code>bindaddress</code>{.xref .std .std-reqmeta .docutils .literal
.notranslate}]{.pre}</a>{.hoverxref .tooltip
.reference .internal}</p>
</li>
<li>
<p><a href="index.html#std-reqmeta-cookiejar">[<code>cookiejar</code>{.xref .std .std-reqmeta .docutils .literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal}</p>
</li>
<li>
<p><a href="index.html#std-reqmeta-dont_cache">[<code>dont_cache</code>{.xref .std .std-reqmeta .docutils .literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal}</p>
</li>
<li>
<p><a href="#std-reqmeta-dont_merge_cookies">[<code>dont_merge_cookies</code>{.xref .std .std-reqmeta .docutils .literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal}</p>
</li>
<li>
<p><a href="index.html#std-reqmeta-dont_obey_robotstxt">[<code>dont_obey_robotstxt</code>{.xref .std .std-reqmeta .docutils .literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal}</p>
</li>
<li>
<p><a href="index.html#std-reqmeta-dont_redirect">[<code>dont_redirect</code>{.xref .std .std-reqmeta .docutils .literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal}</p>
</li>
<li>
<p><a href="index.html#std-reqmeta-dont_retry">[<code>dont_retry</code>{.xref .std .std-reqmeta .docutils .literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal}</p>
</li>
<li>
<p><a href="#std-reqmeta-download_fail_on_dataloss">[<code>download_fail_on_dataloss</code>{.xref .std .std-reqmeta .docutils
.literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal}</p>
</li>
<li>
<p><a href="#std-reqmeta-download_latency">[<code>download_latency</code>{.xref .std .std-reqmeta .docutils .literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal}</p>
</li>
<li>
<p><a href="index.html#std-reqmeta-download_maxsize">[<code>download_maxsize</code>{.xref .std .std-reqmeta .docutils .literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal}</p>
</li>
<li>
<p><a href="#std-reqmeta-download_timeout">[<code>download_timeout</code>{.xref .std .std-reqmeta .docutils .literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal}</p>
</li>
<li>
<p>[<code>ftp_password</code>{.docutils .literal .notranslate}]{.pre} (See
<a href="index.html#std-setting-FTP_PASSWORD">[<code>FTP_PASSWORD</code>{.xref .std .std-setting .docutils .literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal} for more info)</p>
</li>
<li>
<p>[<code>ftp_user</code>{.docutils .literal .notranslate}]{.pre} (See
<a href="index.html#std-setting-FTP_USER">[<code>FTP_USER</code>{.xref .std .std-setting .docutils .literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal} for more info)</p>
</li>
<li>
<p><a href="index.html#std-reqmeta-handle_httpstatus_all">[<code>handle_httpstatus_all</code>{.xref .std .std-reqmeta .docutils .literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal}</p>
</li>
<li>
<p><a href="index.html#std-reqmeta-handle_httpstatus_list">[<code>handle_httpstatus_list</code>{.xref .std .std-reqmeta .docutils
.literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal}</p>
</li>
<li>
<p><a href="#std-reqmeta-max_retry_times">[<code>max_retry_times</code>{.xref .std .std-reqmeta .docutils .literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal}</p>
</li>
<li>
<p><a href="index.html#std-reqmeta-proxy">[<code>proxy</code>{.xref .std .std-reqmeta .docutils .literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal}</p>
</li>
<li>
<p><a href="index.html#std-reqmeta-redirect_reasons">[<code>redirect_reasons</code>{.xref .std .std-reqmeta .docutils .literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal}</p>
</li>
<li>
<p><a href="index.html#std-reqmeta-redirect_urls">[<code>redirect_urls</code>{.xref .std .std-reqmeta .docutils .literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal}</p>
</li>
<li>
<p><a href="index.html#std-reqmeta-referrer_policy">[<code>referrer_policy</code>{.xref .std .std-reqmeta .docutils .literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal}</p>
</li>
</ul>
<p>::: {#bindaddress .section}
[]{#std-reqmeta-bindaddress}</p>
<h5 id="bindaddressheaderlink"><a class="header" href="#bindaddressheaderlink">bindaddress<a href="#bindaddress" title="Permalink to this heading">¶</a>{.headerlink}</a></h5>
<p>The IP of the outgoing IP address to use for the performing the request.
:::</p>
<p>::: {#download-timeout .section}
[]{#std-reqmeta-download_timeout}</p>
<h5 id="download_timeoutheaderlink"><a class="header" href="#download_timeoutheaderlink">download_timeout<a href="#download-timeout" title="Permalink to this heading">¶</a>{.headerlink}</a></h5>
<p>The amount of time (in secs) that the downloader will wait before timing
out. See also: <a href="index.html#std-setting-DOWNLOAD_TIMEOUT">[<code>DOWNLOAD_TIMEOUT</code>{.xref .std .std-setting .docutils
.literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal}.
:::</p>
<p>::: {#download-latency .section}
[]{#std-reqmeta-download_latency}</p>
<h5 id="download_latencyheaderlink"><a class="header" href="#download_latencyheaderlink">download_latency<a href="#download-latency" title="Permalink to this heading">¶</a>{.headerlink}</a></h5>
<p>The amount of time spent to fetch the response, since the request has
been started, i.e. HTTP message sent over the network. This meta key
only becomes available when the response has been downloaded. While most
other meta keys are used to control Scrapy behavior, this one is
supposed to be read-only.
:::</p>
<p>::: {#download-fail-on-dataloss .section}
[]{#std-reqmeta-download_fail_on_dataloss}</p>
<h5 id="download_fail_on_datalossheaderlink"><a class="header" href="#download_fail_on_datalossheaderlink">download_fail_on_dataloss<a href="#download-fail-on-dataloss" title="Permalink to this heading">¶</a>{.headerlink}</a></h5>
<p>Whether or not to fail on broken responses. See:
<a href="index.html#std-setting-DOWNLOAD_FAIL_ON_DATALOSS">[<code>DOWNLOAD_FAIL_ON_DATALOSS</code>{.xref .std .std-setting .docutils .literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal}.
:::</p>
<p>::: {#max-retry-times .section}
[]{#std-reqmeta-max_retry_times}</p>
<h5 id="max_retry_timesheaderlink"><a class="header" href="#max_retry_timesheaderlink">max_retry_times<a href="#max-retry-times" title="Permalink to this heading">¶</a>{.headerlink}</a></h5>
<p>The meta key is used set retry times per request. When initialized, the
<a href="#std-reqmeta-max_retry_times">[<code>max_retry_times</code>{.xref .std .std-reqmeta .docutils .literal
.notranslate}]{.pre}</a>{.hoverxref .tooltip
.reference .internal} meta key takes higher precedence over the
<a href="index.html#std-setting-RETRY_TIMES">[<code>RETRY_TIMES</code>{.xref .std .std-setting .docutils .literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal} setting.
:::
:::</p>
<p>::: {#stopping-the-download-of-a-response .section}
[]{#topics-stop-response-download}</p>
<h4 id="stopping-the-download-of-a-responseheaderlink"><a class="header" href="#stopping-the-download-of-a-responseheaderlink">Stopping the download of a Response<a href="#stopping-the-download-of-a-response" title="Permalink to this heading">¶</a>{.headerlink}</a></h4>
<p>Raising a <a href="index.html#scrapy.exceptions.StopDownload" title="scrapy.exceptions.StopDownload">[<code>StopDownload</code>{.xref .py .py-exc .docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal} exception from a handler for the <a href="index.html#scrapy.signals.bytes_received" title="scrapy.signals.bytes_received">[<code>bytes_received</code>{.xref .py
.py-class .docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal} or <a href="index.html#scrapy.signals.headers_received" title="scrapy.signals.headers_received">[<code>headers_received</code>{.xref .py .py-class .docutils
.literal
.notranslate}]{.pre}</a>{.reference
.internal} signals will stop the download of a given response. See the
following example:</p>
<p>::: {.highlight-python .notranslate}
::: highlight
import scrapy</p>
<pre><code>class StopSpider(scrapy.Spider):
    name = &quot;stop&quot;
    start_urls = [&quot;https://docs.scrapy.org/en/latest/&quot;]

    @classmethod
    def from_crawler(cls, crawler):
        spider = super().from_crawler(crawler)
        crawler.signals.connect(
            spider.on_bytes_received, signal=scrapy.signals.bytes_received
        )
        return spider

    def parse(self, response):
        # 'last_chars' show that the full response was not downloaded
        yield {&quot;len&quot;: len(response.text), &quot;last_chars&quot;: response.text[-40:]}

    def on_bytes_received(self, data, request, spider):
        raise scrapy.exceptions.StopDownload(fail=False)
</code></pre>
<p>:::
:::</p>
<p>which produces the following output:</p>
<p>::: {.highlight-default .notranslate}
::: highlight
2020-05-19 17:26:12 [scrapy.core.engine] INFO: Spider opened
2020-05-19 17:26:12 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2020-05-19 17:26:13 [scrapy.core.downloader.handlers.http11] DEBUG: Download stopped for &lt;GET https://docs.scrapy.org/en/latest/&gt; from signal handler StopSpider.on_bytes_received
2020-05-19 17:26:13 [scrapy.core.engine] DEBUG: Crawled (200) &lt;GET https://docs.scrapy.org/en/latest/&gt; (referer: None) ['download_stopped']
2020-05-19 17:26:13 [scrapy.core.scraper] DEBUG: Scraped from &lt;200 https://docs.scrapy.org/en/latest/&gt;
{'len': 279, 'last_chars': 'dth, initial-scale=1.0&quot;&gt;\n  \n  <title>Scr'}
2020-05-19 17:26:13 [scrapy.core.engine] INFO: Closing spider (finished)
:::
:::</p>
<p>By default, resulting responses are handled by their corresponding
errbacks. To call their callback instead, like in this example, pass
[<code>fail=False</code>{.docutils .literal .notranslate}]{.pre} to the
<a href="index.html#scrapy.exceptions.StopDownload" title="scrapy.exceptions.StopDownload">[<code>StopDownload</code>{.xref .py .py-exc .docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal} exception.
:::</p>
<p>::: {#request-subclasses .section}
[]{#topics-request-response-ref-request-subclasses}</p>
<h4 id="request-subclassesheaderlink"><a class="header" href="#request-subclassesheaderlink">Request subclasses<a href="#request-subclasses" title="Permalink to this heading">¶</a>{.headerlink}</a></h4>
<p>Here is the list of built-in <a href="#scrapy.http.Request" title="scrapy.http.Request">[<code>Request</code>{.xref .py .py-class .docutils
.literal
.notranslate}]{.pre}</a>{.reference
.internal} subclasses. You can also subclass it to implement your own
custom functionality.</p>
<p>::: {#formrequest-objects .section}</p>
<h5 id="formrequest-objectsheaderlink"><a class="header" href="#formrequest-objectsheaderlink">FormRequest objects<a href="#formrequest-objects" title="Permalink to this heading">¶</a>{.headerlink}</a></h5>
<p>The FormRequest class extends the base <a href="#scrapy.http.Request" title="scrapy.http.Request">[<code>Request</code>{.xref .py .py-class
.docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal} with functionality for dealing with HTML forms. It uses
<a href="https://lxml.de/lxmlhtml.html#forms">lxml.html forms</a>{.reference
.external} to pre-populate form fields with form data from
<a href="#scrapy.http.Response" title="scrapy.http.Response">[<code>Response</code>{.xref .py .py-class .docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal} objects.</p>
<p><em>[class]{.pre}[ ]{.w}</em>[[scrapy.http.request.form.]{.pre}]{.sig-prename .descclassname}[[FormRequest]{.pre}]{.sig-name .descname}<a href="#scrapy.http.scrapy.http.request.form.FormRequest" title="Permalink to this definition">¶</a>{.headerlink}</p>
<p>:</p>
<pre><code class="language-{=html}">&lt;!-- --&gt;
</code></pre>
<p><em>[class]{.pre}[ ]{.w}</em>[[scrapy.http.]{.pre}]{.sig-prename .descclassname}[[FormRequest]{.pre}]{.sig-name .descname}<a href="#scrapy.http.scrapy.http.FormRequest" title="Permalink to this definition">¶</a>{.headerlink}</p>
<p>:</p>
<pre><code class="language-{=html}">&lt;!-- --&gt;
</code></pre>
<p><em>[class]{.pre}[ ]{.w}</em>[[scrapy.]{.pre}]{.sig-prename .descclassname}[[FormRequest]{.pre}]{.sig-name .descname}[(]{.sig-paren}<em>[[url]{.pre}]{.n}</em>[[]{.optional}, <em>[[formdata]{.pre}]{.n}</em>, <em>[[...]{.pre}]{.n}</em>[]]{.optional}[)]{.sig-paren}<a href="#scrapy.http.scrapy.FormRequest" title="Permalink to this definition">¶</a>{.headerlink}</p>
<p>:   The [<code>FormRequest</code>{.xref .py .py-class .docutils .literal
.notranslate}]{.pre} class adds a new keyword parameter to the
[<code>__init__</code>{.docutils .literal .notranslate}]{.pre} method. The
remaining arguments are the same as for the <a href="#scrapy.http.Request" title="scrapy.http.Request">[<code>Request</code>{.xref .py
.py-class .docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal} class and are not documented here.</p>
<pre><code>Parameters

:   **formdata**
    ([*dict*](https://docs.python.org/3/library/stdtypes.html#dict &quot;(in Python v3.12)&quot;){.reference
    .external} *or*
    [*collections.abc.Iterable*](https://docs.python.org/3/library/collections.abc.html#collections.abc.Iterable &quot;(in Python v3.12)&quot;){.reference
    .external}) -- is a dictionary (or iterable of (key, value)
    tuples) containing HTML Form data which will be url-encoded and
    assigned to the body of the request.

The [`FormRequest`{.xref .py .py-class .docutils .literal
.notranslate}]{.pre} objects support the following class method in
addition to the standard [[`Request`{.xref .py .py-class .docutils
.literal
.notranslate}]{.pre}](#scrapy.http.Request &quot;scrapy.http.Request&quot;){.reference
.internal} methods:

*[classmethod]{.pre}[ ]{.w}*[[FormRequest.]{.pre}]{.sig-prename .descclassname}[[from_response]{.pre}]{.sig-name .descname}[(]{.sig-paren}*[[response]{.pre}]{.n}*[\[]{.optional}, *[[formname=None]{.pre}]{.n}*, *[[formid=None]{.pre}]{.n}*, *[[formnumber=0]{.pre}]{.n}*, *[[formdata=None]{.pre}]{.n}*, *[[formxpath=None]{.pre}]{.n}*, *[[formcss=None]{.pre}]{.n}*, *[[clickdata=None]{.pre}]{.n}*, *[[dont_click=False]{.pre}]{.n}*, *[[\...]{.pre}]{.n}*[\]]{.optional}[)]{.sig-paren}[¶](#scrapy.http.scrapy.FormRequest.FormRequest.from_response &quot;Permalink to this definition&quot;){.headerlink}

:   Returns a new [`FormRequest`{.xref .py .py-class .docutils
    .literal .notranslate}]{.pre} object with its form field values
    pre-populated with those found in the HTML [`&lt;form&gt;`{.docutils
    .literal .notranslate}]{.pre} element contained in the given
    response. For an example see [[Using FormRequest.from_response()
    to simulate a user login]{.std
    .std-ref}](#topics-request-response-ref-request-userlogin){.hoverxref
    .tooltip .reference .internal}.

    The policy is to automatically simulate a click, by default, on
    any form control that looks clickable, like a
    [`&lt;input`{.docutils .literal .notranslate}]{.pre}` `{.docutils
    .literal .notranslate}[`type=&quot;submit&quot;&gt;`{.docutils .literal
    .notranslate}]{.pre}. Even though this is quite convenient, and
    often the desired behaviour, sometimes it can cause problems
    which could be hard to debug. For example, when working with
    forms that are filled and/or submitted using javascript, the
    default [`from_response()`{.xref .py .py-meth .docutils .literal
    .notranslate}]{.pre} behaviour may not be the most appropriate.
    To disable this behaviour you can set the
    [`dont_click`{.docutils .literal .notranslate}]{.pre} argument
    to [`True`{.docutils .literal .notranslate}]{.pre}. Also, if you
    want to change the control clicked (instead of disabling it) you
    can also use the [`clickdata`{.docutils .literal
    .notranslate}]{.pre} argument.

    ::: {.admonition .caution}
    Caution

    Using this method with select elements which have leading or
    trailing whitespace in the option values will not work due to a
    [bug in
    lxml](https://bugs.launchpad.net/lxml/+bug/1665241){.reference
    .external}, which should be fixed in lxml 3.8 and above.
    :::

    Parameters

    :   -   **response** ([[`Response`{.xref .py .py-class .docutils
            .literal
            .notranslate}]{.pre}](#scrapy.http.Response &quot;scrapy.http.Response&quot;){.reference
            .internal} object) -- the response containing a HTML
            form which will be used to pre-populate the form fields

        -   **formname**
            ([*str*](https://docs.python.org/3/library/stdtypes.html#str &quot;(in Python v3.12)&quot;){.reference
            .external}) -- if given, the form with name attribute
            set to this value will be used.

        -   **formid**
            ([*str*](https://docs.python.org/3/library/stdtypes.html#str &quot;(in Python v3.12)&quot;){.reference
            .external}) -- if given, the form with id attribute set
            to this value will be used.

        -   **formxpath**
            ([*str*](https://docs.python.org/3/library/stdtypes.html#str &quot;(in Python v3.12)&quot;){.reference
            .external}) -- if given, the first form that matches the
            xpath will be used.

        -   **formcss**
            ([*str*](https://docs.python.org/3/library/stdtypes.html#str &quot;(in Python v3.12)&quot;){.reference
            .external}) -- if given, the first form that matches the
            css selector will be used.

        -   **formnumber**
            ([*int*](https://docs.python.org/3/library/functions.html#int &quot;(in Python v3.12)&quot;){.reference
            .external}) -- the number of form to use, when the
            response contains multiple forms. The first one (and
            also the default) is [`0`{.docutils .literal
            .notranslate}]{.pre}.

        -   **formdata**
            ([*dict*](https://docs.python.org/3/library/stdtypes.html#dict &quot;(in Python v3.12)&quot;){.reference
            .external}) -- fields to override in the form data. If a
            field was already present in the response
            [`&lt;form&gt;`{.docutils .literal .notranslate}]{.pre}
            element, its value is overridden by the one passed in
            this parameter. If a value passed in this parameter is
            [`None`{.docutils .literal .notranslate}]{.pre}, the
            field will not be included in the request, even if it
            was present in the response [`&lt;form&gt;`{.docutils .literal
            .notranslate}]{.pre} element.

        -   **clickdata**
            ([*dict*](https://docs.python.org/3/library/stdtypes.html#dict &quot;(in Python v3.12)&quot;){.reference
            .external}) -- attributes to lookup the control clicked.
            If it's not given, the form data will be submitted
            simulating a click on the first clickable element. In
            addition to html attributes, the control can be
            identified by its zero-based index relative to other
            submittable inputs inside the form, via the
            [`nr`{.docutils .literal .notranslate}]{.pre} attribute.

        -   **dont_click**
            ([*bool*](https://docs.python.org/3/library/functions.html#bool &quot;(in Python v3.12)&quot;){.reference
            .external}) -- If True, the form data will be submitted
            without clicking in any element.

    The other parameters of this class method are passed directly to
    the [`FormRequest`{.xref .py .py-class .docutils .literal
    .notranslate}]{.pre} [`__init__`{.docutils .literal
    .notranslate}]{.pre} method.
</code></pre>
<p>:::</p>
<p>::: {#request-usage-examples .section}</p>
<h5 id="request-usage-examplesheaderlink"><a class="header" href="#request-usage-examplesheaderlink">Request usage examples<a href="#request-usage-examples" title="Permalink to this heading">¶</a>{.headerlink}</a></h5>
<p>::: {#using-formrequest-to-send-data-via-http-post .section}</p>
<h6 id="using-formrequest-to-send-data-via-http-postheaderlink"><a class="header" href="#using-formrequest-to-send-data-via-http-postheaderlink">Using FormRequest to send data via HTTP POST<a href="#using-formrequest-to-send-data-via-http-post" title="Permalink to this heading">¶</a>{.headerlink}</a></h6>
<p>If you want to simulate a HTML Form POST in your spider and send a
couple of key-value fields, you can return a [<code>FormRequest</code>{.xref .py
.py-class .docutils .literal .notranslate}]{.pre} object (from your
spider) like this:</p>
<p>::: {.highlight-python .notranslate}
::: highlight
return [
FormRequest(
url=&quot;http://www.example.com/post/action&quot;,
formdata={&quot;name&quot;: &quot;John Doe&quot;, &quot;age&quot;: &quot;27&quot;},
callback=self.after_post,
)
]
:::
:::
:::</p>
<p>::: {#using-formrequest-from-response-to-simulate-a-user-login .section}
[]{#topics-request-response-ref-request-userlogin}</p>
<h6 id="using-formrequestfrom_response-to-simulate-a-user-loginheaderlink"><a class="header" href="#using-formrequestfrom_response-to-simulate-a-user-loginheaderlink">Using FormRequest.from_response() to simulate a user login<a href="#using-formrequest-from-response-to-simulate-a-user-login" title="Permalink to this heading">¶</a>{.headerlink}</a></h6>
<p>It is usual for web sites to provide pre-populated form fields through
[<code>&lt;input</code>{.docutils .literal .notranslate}]{.pre}<code> </code>{.docutils .literal
.notranslate}[<code>type=&quot;hidden&quot;&gt;</code>{.docutils .literal .notranslate}]{.pre}
elements, such as session related data or authentication tokens (for
login pages). When scraping, you'll want these fields to be
automatically pre-populated and only override a couple of them, such as
the user name and password. You can use the
[<code>FormRequest.from_response()</code>{.xref .py .py-meth .docutils .literal
.notranslate}]{.pre} method for this job. Here's an example spider which
uses it:</p>
<p>::: {.highlight-python .notranslate}
::: highlight
import scrapy</p>
<pre><code>def authentication_failed(response):
    # TODO: Check the contents of the response and return True if it failed
    # or False if it succeeded.
    pass


class LoginSpider(scrapy.Spider):
    name = &quot;example.com&quot;
    start_urls = [&quot;http://www.example.com/users/login.php&quot;]

    def parse(self, response):
        return scrapy.FormRequest.from_response(
            response,
            formdata={&quot;username&quot;: &quot;john&quot;, &quot;password&quot;: &quot;secret&quot;},
            callback=self.after_login,
        )

    def after_login(self, response):
        if authentication_failed(response):
            self.logger.error(&quot;Login failed&quot;)
            return

        # continue scraping with authenticated session...
</code></pre>
<p>:::
:::
:::
:::</p>
<p>::: {#jsonrequest .section}</p>
<h5 id="jsonrequestheaderlink"><a class="header" href="#jsonrequestheaderlink">JsonRequest<a href="#jsonrequest" title="Permalink to this heading">¶</a>{.headerlink}</a></h5>
<p>The JsonRequest class extends the base <a href="#scrapy.http.Request" title="scrapy.http.Request">[<code>Request</code>{.xref .py .py-class
.docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal} class with functionality for dealing with JSON requests.</p>
<p><em>[class]{.pre}[ ]{.w}</em>[[scrapy.http.]{.pre}]{.sig-prename .descclassname}[[JsonRequest]{.pre}]{.sig-name .descname}[(]{.sig-paren}<em>[[url]{.pre}]{.n}</em>[[]{.optional}, <em>[[...]{.pre} [data]{.pre}]{.n}</em>, <em>[[dumps_kwargs]{.pre}]{.n}</em>[]]{.optional}[)]{.sig-paren}<a href="_modules/scrapy/http/request/json_request.html#JsonRequest">[[[source]]{.pre}]{.viewcode-link}</a>{.reference .internal}<a href="#scrapy.http.JsonRequest" title="Permalink to this definition">¶</a>{.headerlink}</p>
<p>:   The <a href="#scrapy.http.JsonRequest" title="scrapy.http.JsonRequest">[<code>JsonRequest</code>{.xref .py .py-class .docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal} class adds two new keyword parameters to the
[<code>__init__</code>{.docutils .literal .notranslate}]{.pre} method. The
remaining arguments are the same as for the <a href="#scrapy.http.Request" title="scrapy.http.Request">[<code>Request</code>{.xref .py
.py-class .docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal} class and are not documented here.</p>
<pre><code>Using the [[`JsonRequest`{.xref .py .py-class .docutils .literal
.notranslate}]{.pre}](#scrapy.http.JsonRequest &quot;scrapy.http.JsonRequest&quot;){.reference
.internal} will set the [`Content-Type`{.docutils .literal
.notranslate}]{.pre} header to [`application/json`{.docutils
.literal .notranslate}]{.pre} and [`Accept`{.docutils .literal
.notranslate}]{.pre} header to [`application/json,`{.docutils
.literal .notranslate}]{.pre}` `{.docutils .literal
.notranslate}[`text/javascript,`{.docutils .literal
.notranslate}]{.pre}` `{.docutils .literal
.notranslate}[`*/*;`{.docutils .literal
.notranslate}]{.pre}` `{.docutils .literal
.notranslate}[`q=0.01`{.docutils .literal .notranslate}]{.pre}

Parameters

:   -   **data**
        ([*object*](https://docs.python.org/3/library/functions.html#object &quot;(in Python v3.12)&quot;){.reference
        .external}) -- is any JSON serializable object that needs to
        be JSON encoded and assigned to body. if
        [[`Request.body`{.xref .py .py-attr .docutils .literal
        .notranslate}]{.pre}](#scrapy.http.Request.body &quot;scrapy.http.Request.body&quot;){.reference
        .internal} argument is provided this parameter will be
        ignored. if [[`Request.body`{.xref .py .py-attr .docutils
        .literal
        .notranslate}]{.pre}](#scrapy.http.Request.body &quot;scrapy.http.Request.body&quot;){.reference
        .internal} argument is not provided and data argument is
        provided [[`Request.method`{.xref .py .py-attr .docutils
        .literal
        .notranslate}]{.pre}](#scrapy.http.Request.method &quot;scrapy.http.Request.method&quot;){.reference
        .internal} will be set to [`'POST'`{.docutils .literal
        .notranslate}]{.pre} automatically.

    -   **dumps_kwargs**
        ([*dict*](https://docs.python.org/3/library/stdtypes.html#dict &quot;(in Python v3.12)&quot;){.reference
        .external}) -- Parameters that will be passed to underlying
        [[`json.dumps()`{.xref .py .py-func .docutils .literal
        .notranslate}]{.pre}](https://docs.python.org/3/library/json.html#json.dumps &quot;(in Python v3.12)&quot;){.reference
        .external} method which is used to serialize data into JSON
        format.

[[attributes]{.pre}]{.sig-name .descname}*[[:]{.pre}]{.p}[ ]{.w}[[Tuple]{.pre}](https://docs.python.org/3/library/typing.html#typing.Tuple &quot;(in Python v3.12)&quot;){.reference .external}[[\[]{.pre}]{.p}[[str]{.pre}](https://docs.python.org/3/library/stdtypes.html#str &quot;(in Python v3.12)&quot;){.reference .external}[[,]{.pre}]{.p}[ ]{.w}[[\...]{.pre}]{.p}[[\]]{.pre}]{.p}[ ]{.w}[[=]{.pre}]{.p}[ ]{.w}[(\'url\',]{.pre} [\'callback\',]{.pre} [\'method\',]{.pre} [\'headers\',]{.pre} [\'body\',]{.pre} [\'cookies\',]{.pre} [\'meta\',]{.pre} [\'encoding\',]{.pre} [\'priority\',]{.pre} [\'dont_filter\',]{.pre} [\'errback\',]{.pre} [\'flags\',]{.pre} [\'cb_kwargs\',]{.pre} [\'dumps_kwargs\')]{.pre}*[¶](#scrapy.http.JsonRequest.attributes &quot;Permalink to this definition&quot;){.headerlink}

:   A tuple of [[`str`{.xref .py .py-class .docutils .literal
    .notranslate}]{.pre}](https://docs.python.org/3/library/stdtypes.html#str &quot;(in Python v3.12)&quot;){.reference
    .external} objects containing the name of all public attributes
    of the class that are also keyword parameters of the
    [`__init__`{.docutils .literal .notranslate}]{.pre} method.

    Currently used by [[`Request.replace()`{.xref .py .py-meth
    .docutils .literal
    .notranslate}]{.pre}](#scrapy.http.Request.replace &quot;scrapy.http.Request.replace&quot;){.reference
    .internal}, [[`Request.to_dict()`{.xref .py .py-meth .docutils
    .literal
    .notranslate}]{.pre}](#scrapy.http.Request.to_dict &quot;scrapy.http.Request.to_dict&quot;){.reference
    .internal} and [[`request_from_dict()`{.xref .py .py-func
    .docutils .literal
    .notranslate}]{.pre}](#scrapy.utils.request.request_from_dict &quot;scrapy.utils.request.request_from_dict&quot;){.reference
    .internal}.
</code></pre>
<p>:::</p>
<p>::: {#jsonrequest-usage-example .section}</p>
<h5 id="jsonrequest-usage-exampleheaderlink"><a class="header" href="#jsonrequest-usage-exampleheaderlink">JsonRequest usage example<a href="#jsonrequest-usage-example" title="Permalink to this heading">¶</a>{.headerlink}</a></h5>
<p>Sending a JSON POST request with a JSON payload:</p>
<p>::: {.highlight-python .notranslate}
::: highlight
data = {
&quot;name1&quot;: &quot;value1&quot;,
&quot;name2&quot;: &quot;value2&quot;,
}
yield JsonRequest(url=&quot;http://www.example.com/post/action&quot;, data=data)
:::
:::
:::
:::</p>
<p>::: {#response-objects .section}</p>
<h4 id="response-objectsheaderlink"><a class="header" href="#response-objectsheaderlink">Response objects<a href="#response-objects" title="Permalink to this heading">¶</a>{.headerlink}</a></h4>
<p><em>[class]{.pre}[ ]{.w}</em>[[scrapy.http.]{.pre}]{.sig-prename .descclassname}[[Response]{.pre}]{.sig-name .descname}[(]{.sig-paren}<em>[[*]{.pre}]{.o}[[args]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[<a href="https://docs.python.org/3/library/typing.html#typing.Any" title="(in Python v3.12)">[Any]{.pre}</a>{.reference .external}]{.n}</em>, <em>[[**]{.pre}]{.o}[[kwargs]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[<a href="https://docs.python.org/3/library/typing.html#typing.Any" title="(in Python v3.12)">[Any]{.pre}</a>{.reference .external}]{.n}</em>[)]{.sig-paren}<a href="_modules/scrapy/http/response.html#Response">[[[source]]{.pre}]{.viewcode-link}</a>{.reference .internal}<a href="#scrapy.http.Response" title="Permalink to this definition">¶</a>{.headerlink}</p>
<p>:   An object that represents an HTTP response, which is usually
downloaded (by the Downloader) and fed to the Spiders for
processing.</p>
<pre><code>Parameters

:   -   **url**
        ([*str*](https://docs.python.org/3/library/stdtypes.html#str &quot;(in Python v3.12)&quot;){.reference
        .external}) -- the URL of this response

    -   **status**
        ([*int*](https://docs.python.org/3/library/functions.html#int &quot;(in Python v3.12)&quot;){.reference
        .external}) -- the HTTP status of the response. Defaults to
        [`200`{.docutils .literal .notranslate}]{.pre}.

    -   **headers**
        ([*dict*](https://docs.python.org/3/library/stdtypes.html#dict &quot;(in Python v3.12)&quot;){.reference
        .external}) -- the headers of this response. The dict values
        can be strings (for single valued headers) or lists (for
        multi-valued headers).

    -   **body**
        ([*bytes*](https://docs.python.org/3/library/stdtypes.html#bytes &quot;(in Python v3.12)&quot;){.reference
        .external}) -- the response body. To access the decoded text
        as a string, use [`response.text`{.docutils .literal
        .notranslate}]{.pre} from an encoding-aware [[Response
        subclass]{.std
        .std-ref}](#topics-request-response-ref-response-subclasses){.hoverxref
        .tooltip .reference .internal}, such as
        [[`TextResponse`{.xref .py .py-class .docutils .literal
        .notranslate}]{.pre}](#scrapy.http.TextResponse &quot;scrapy.http.TextResponse&quot;){.reference
        .internal}.

    -   **flags**
        ([*list*](https://docs.python.org/3/library/stdtypes.html#list &quot;(in Python v3.12)&quot;){.reference
        .external}) -- is a list containing the initial values for
        the [[`Response.flags`{.xref .py .py-attr .docutils .literal
        .notranslate}]{.pre}](#scrapy.http.Response.flags &quot;scrapy.http.Response.flags&quot;){.reference
        .internal} attribute. If given, the list will be shallow
        copied.

    -   **request** (*scrapy.Request*) -- the initial value of the
        [[`Response.request`{.xref .py .py-attr .docutils .literal
        .notranslate}]{.pre}](#scrapy.http.Response.request &quot;scrapy.http.Response.request&quot;){.reference
        .internal} attribute. This represents the [[`Request`{.xref
        .py .py-class .docutils .literal
        .notranslate}]{.pre}](#scrapy.http.Request &quot;scrapy.http.Request&quot;){.reference
        .internal} that generated this response.

    -   **certificate**
        ([*twisted.internet.ssl.Certificate*](https://docs.twisted.org/en/stable/api/twisted.internet.ssl.Certificate.html &quot;(in Twisted)&quot;){.reference
        .external}) -- an object representing the server's SSL
        certificate.

    -   **ip_address** ([[`ipaddress.IPv4Address`{.xref .py
        .py-class .docutils .literal
        .notranslate}]{.pre}](https://docs.python.org/3/library/ipaddress.html#ipaddress.IPv4Address &quot;(in Python v3.12)&quot;){.reference
        .external} or [[`ipaddress.IPv6Address`{.xref .py .py-class
        .docutils .literal
        .notranslate}]{.pre}](https://docs.python.org/3/library/ipaddress.html#ipaddress.IPv6Address &quot;(in Python v3.12)&quot;){.reference
        .external}) -- The IP address of the server from which the
        Response originated.

    -   **protocol** ([[`str`{.xref .py .py-class .docutils .literal
        .notranslate}]{.pre}](https://docs.python.org/3/library/stdtypes.html#str &quot;(in Python v3.12)&quot;){.reference
        .external}) -- The protocol that was used to download the
        response. For instance: &quot;HTTP/1.0&quot;, &quot;HTTP/1.1&quot;, &quot;h2&quot;

::: versionadded
[New in version 2.0.0: ]{.versionmodified .added}The
[`certificate`{.docutils .literal .notranslate}]{.pre} parameter.
:::

::: versionadded
[New in version 2.1.0: ]{.versionmodified .added}The
[`ip_address`{.docutils .literal .notranslate}]{.pre} parameter.
:::

::: versionadded
[New in version 2.5.0: ]{.versionmodified .added}The
[`protocol`{.docutils .literal .notranslate}]{.pre} parameter.
:::

[[url]{.pre}]{.sig-name .descname}[¶](#scrapy.http.Response.url &quot;Permalink to this definition&quot;){.headerlink}

:   A string containing the URL of the response.

    This attribute is read-only. To change the URL of a Response use
    [[`replace()`{.xref .py .py-meth .docutils .literal
    .notranslate}]{.pre}](#scrapy.http.Response.replace &quot;scrapy.http.Response.replace&quot;){.reference
    .internal}.

[[status]{.pre}]{.sig-name .descname}[¶](#scrapy.http.Response.status &quot;Permalink to this definition&quot;){.headerlink}

:   An integer representing the HTTP status of the response.
    Example: [`200`{.docutils .literal .notranslate}]{.pre},
    [`404`{.docutils .literal .notranslate}]{.pre}.

[[headers]{.pre}]{.sig-name .descname}[¶](#scrapy.http.Response.headers &quot;Permalink to this definition&quot;){.headerlink}

:   A dictionary-like object which contains the response headers.
    Values can be accessed using [`get()`{.xref .py .py-meth
    .docutils .literal .notranslate}]{.pre} to return the first
    header value with the specified name or [`getlist()`{.xref .py
    .py-meth .docutils .literal .notranslate}]{.pre} to return all
    header values with the specified name. For example, this call
    will give you all cookies in the headers:

    ::: {.highlight-default .notranslate}
    ::: highlight
        response.headers.getlist('Set-Cookie')
    :::
    :::

[[body]{.pre}]{.sig-name .descname}[¶](#scrapy.http.Response.body &quot;Permalink to this definition&quot;){.headerlink}

:   The response body as bytes.

    If you want the body as a string, use
    [[`TextResponse.text`{.xref .py .py-attr .docutils .literal
    .notranslate}]{.pre}](#scrapy.http.TextResponse.text &quot;scrapy.http.TextResponse.text&quot;){.reference
    .internal} (only available in [[`TextResponse`{.xref .py
    .py-class .docutils .literal
    .notranslate}]{.pre}](#scrapy.http.TextResponse &quot;scrapy.http.TextResponse&quot;){.reference
    .internal} and subclasses).

    This attribute is read-only. To change the body of a Response
    use [[`replace()`{.xref .py .py-meth .docutils .literal
    .notranslate}]{.pre}](#scrapy.http.Response.replace &quot;scrapy.http.Response.replace&quot;){.reference
    .internal}.

[[request]{.pre}]{.sig-name .descname}[¶](#scrapy.http.Response.request &quot;Permalink to this definition&quot;){.headerlink}

:   The [[`Request`{.xref .py .py-class .docutils .literal
    .notranslate}]{.pre}](#scrapy.http.Request &quot;scrapy.http.Request&quot;){.reference
    .internal} object that generated this response. This attribute
    is assigned in the Scrapy engine, after the response and the
    request have passed through all [[Downloader Middlewares]{.std
    .std-ref}](index.html#topics-downloader-middleware){.hoverxref
    .tooltip .reference .internal}. In particular, this means that:

    -   HTTP redirections will create a new request from the request
        before redirection. It has the majority of the same metadata
        and original request attributes and gets assigned to the
        redirected response instead of the propagation of the
        original request.

    -   Response.request.url doesn't always equal Response.url

    -   This attribute is only available in the spider code, and in
        the [[Spider Middlewares]{.std
        .std-ref}](index.html#topics-spider-middleware){.hoverxref
        .tooltip .reference .internal}, but not in Downloader
        Middlewares (although you have the Request available there
        by other means) and handlers of the
        [[`response_downloaded`{.xref .std .std-signal .docutils
        .literal
        .notranslate}]{.pre}](index.html#std-signal-response_downloaded){.hoverxref
        .tooltip .reference .internal} signal.

[[meta]{.pre}]{.sig-name .descname}[¶](#scrapy.http.Response.meta &quot;Permalink to this definition&quot;){.headerlink}

:   A shortcut to the [[`Request.meta`{.xref .py .py-attr .docutils
    .literal
    .notranslate}]{.pre}](#scrapy.http.Request.meta &quot;scrapy.http.Request.meta&quot;){.reference
    .internal} attribute of the [[`Response.request`{.xref .py
    .py-attr .docutils .literal
    .notranslate}]{.pre}](#scrapy.http.Response.request &quot;scrapy.http.Response.request&quot;){.reference
    .internal} object (i.e. [`self.request.meta`{.docutils .literal
    .notranslate}]{.pre}).

    Unlike the [[`Response.request`{.xref .py .py-attr .docutils
    .literal
    .notranslate}]{.pre}](#scrapy.http.Response.request &quot;scrapy.http.Response.request&quot;){.reference
    .internal} attribute, the [[`Response.meta`{.xref .py .py-attr
    .docutils .literal
    .notranslate}]{.pre}](#scrapy.http.Response.meta &quot;scrapy.http.Response.meta&quot;){.reference
    .internal} attribute is propagated along redirects and retries,
    so you will get the original [[`Request.meta`{.xref .py .py-attr
    .docutils .literal
    .notranslate}]{.pre}](#scrapy.http.Request.meta &quot;scrapy.http.Request.meta&quot;){.reference
    .internal} sent from your spider.

    ::: {.admonition .seealso}
    See also

    [[`Request.meta`{.xref .py .py-attr .docutils .literal
    .notranslate}]{.pre}](#scrapy.http.Request.meta &quot;scrapy.http.Request.meta&quot;){.reference
    .internal} attribute
    :::

[[cb_kwargs]{.pre}]{.sig-name .descname}[¶](#scrapy.http.Response.cb_kwargs &quot;Permalink to this definition&quot;){.headerlink}

:   ::: versionadded
    [New in version 2.0.]{.versionmodified .added}
    :::

    A shortcut to the [[`Request.cb_kwargs`{.xref .py .py-attr
    .docutils .literal
    .notranslate}]{.pre}](#scrapy.http.Request.cb_kwargs &quot;scrapy.http.Request.cb_kwargs&quot;){.reference
    .internal} attribute of the [[`Response.request`{.xref .py
    .py-attr .docutils .literal
    .notranslate}]{.pre}](#scrapy.http.Response.request &quot;scrapy.http.Response.request&quot;){.reference
    .internal} object (i.e. [`self.request.cb_kwargs`{.docutils
    .literal .notranslate}]{.pre}).

    Unlike the [[`Response.request`{.xref .py .py-attr .docutils
    .literal
    .notranslate}]{.pre}](#scrapy.http.Response.request &quot;scrapy.http.Response.request&quot;){.reference
    .internal} attribute, the [[`Response.cb_kwargs`{.xref .py
    .py-attr .docutils .literal
    .notranslate}]{.pre}](#scrapy.http.Response.cb_kwargs &quot;scrapy.http.Response.cb_kwargs&quot;){.reference
    .internal} attribute is propagated along redirects and retries,
    so you will get the original [[`Request.cb_kwargs`{.xref .py
    .py-attr .docutils .literal
    .notranslate}]{.pre}](#scrapy.http.Request.cb_kwargs &quot;scrapy.http.Request.cb_kwargs&quot;){.reference
    .internal} sent from your spider.

    ::: {.admonition .seealso}
    See also

    [[`Request.cb_kwargs`{.xref .py .py-attr .docutils .literal
    .notranslate}]{.pre}](#scrapy.http.Request.cb_kwargs &quot;scrapy.http.Request.cb_kwargs&quot;){.reference
    .internal} attribute
    :::

[[flags]{.pre}]{.sig-name .descname}[¶](#scrapy.http.Response.flags &quot;Permalink to this definition&quot;){.headerlink}

:   A list that contains flags for this response. Flags are labels
    used for tagging Responses. For example: [`'cached'`{.docutils
    .literal .notranslate}]{.pre}, [`'redirected`{.docutils .literal
    .notranslate}]{.pre}', etc. And they're shown on the string
    representation of the Response (\_\_str\_\_ method) which is
    used by the engine for logging.

[[certificate]{.pre}]{.sig-name .descname}[¶](#scrapy.http.Response.certificate &quot;Permalink to this definition&quot;){.headerlink}

:   ::: versionadded
    [New in version 2.0.0.]{.versionmodified .added}
    :::

    A [[`twisted.internet.ssl.Certificate`{.xref .py .py-class
    .docutils .literal
    .notranslate}]{.pre}](https://docs.twisted.org/en/stable/api/twisted.internet.ssl.Certificate.html &quot;(in Twisted)&quot;){.reference
    .external} object representing the server's SSL certificate.

    Only populated for [`https`{.docutils .literal
    .notranslate}]{.pre} responses, [`None`{.docutils .literal
    .notranslate}]{.pre} otherwise.

[[ip_address]{.pre}]{.sig-name .descname}[¶](#scrapy.http.Response.ip_address &quot;Permalink to this definition&quot;){.headerlink}

:   ::: versionadded
    [New in version 2.1.0.]{.versionmodified .added}
    :::

    The IP address of the server from which the Response originated.

    This attribute is currently only populated by the HTTP 1.1
    download handler, i.e. for [`http(s)`{.docutils .literal
    .notranslate}]{.pre} responses. For other handlers,
    [[`ip_address`{.xref .py .py-attr .docutils .literal
    .notranslate}]{.pre}](#scrapy.http.Response.ip_address &quot;scrapy.http.Response.ip_address&quot;){.reference
    .internal} is always [`None`{.docutils .literal
    .notranslate}]{.pre}.

[[protocol]{.pre}]{.sig-name .descname}[¶](#scrapy.http.Response.protocol &quot;Permalink to this definition&quot;){.headerlink}

:   ::: versionadded
    [New in version 2.5.0.]{.versionmodified .added}
    :::

    The protocol that was used to download the response. For
    instance: &quot;HTTP/1.0&quot;, &quot;HTTP/1.1&quot;

    This attribute is currently only populated by the HTTP download
    handlers, i.e. for [`http(s)`{.docutils .literal
    .notranslate}]{.pre} responses. For other handlers,
    [[`protocol`{.xref .py .py-attr .docutils .literal
    .notranslate}]{.pre}](#scrapy.http.Response.protocol &quot;scrapy.http.Response.protocol&quot;){.reference
    .internal} is always [`None`{.docutils .literal
    .notranslate}]{.pre}.

[[attributes]{.pre}]{.sig-name .descname}*[[:]{.pre}]{.p}[ ]{.w}[[Tuple]{.pre}](https://docs.python.org/3/library/typing.html#typing.Tuple &quot;(in Python v3.12)&quot;){.reference .external}[[\[]{.pre}]{.p}[[str]{.pre}](https://docs.python.org/3/library/stdtypes.html#str &quot;(in Python v3.12)&quot;){.reference .external}[[,]{.pre}]{.p}[ ]{.w}[[\...]{.pre}]{.p}[[\]]{.pre}]{.p}[ ]{.w}[[=]{.pre}]{.p}[ ]{.w}[(\'url\',]{.pre} [\'status\',]{.pre} [\'headers\',]{.pre} [\'body\',]{.pre} [\'flags\',]{.pre} [\'request\',]{.pre} [\'certificate\',]{.pre} [\'ip_address\',]{.pre} [\'protocol\')]{.pre}*[¶](#scrapy.http.Response.attributes &quot;Permalink to this definition&quot;){.headerlink}

:   A tuple of [[`str`{.xref .py .py-class .docutils .literal
    .notranslate}]{.pre}](https://docs.python.org/3/library/stdtypes.html#str &quot;(in Python v3.12)&quot;){.reference
    .external} objects containing the name of all public attributes
    of the class that are also keyword parameters of the
    [`__init__`{.docutils .literal .notranslate}]{.pre} method.

    Currently used by [[`Response.replace()`{.xref .py .py-meth
    .docutils .literal
    .notranslate}]{.pre}](#scrapy.http.Response.replace &quot;scrapy.http.Response.replace&quot;){.reference
    .internal}.

[[copy]{.pre}]{.sig-name .descname}[(]{.sig-paren}[)]{.sig-paren}[[[\[source\]]{.pre}]{.viewcode-link}](_modules/scrapy/http/response.html#Response.copy){.reference .internal}[¶](#scrapy.http.Response.copy &quot;Permalink to this definition&quot;){.headerlink}

:   Returns a new Response which is a copy of this Response.

[[replace]{.pre}]{.sig-name .descname}[(]{.sig-paren}[\[]{.optional}*[[url]{.pre}]{.n}*, *[[status]{.pre}]{.n}*, *[[headers]{.pre}]{.n}*, *[[body]{.pre}]{.n}*, *[[request]{.pre}]{.n}*, *[[flags]{.pre}]{.n}*, *[[cls]{.pre}]{.n}*[\]]{.optional}[)]{.sig-paren}[[[\[source\]]{.pre}]{.viewcode-link}](_modules/scrapy/http/response.html#Response.replace){.reference .internal}[¶](#scrapy.http.Response.replace &quot;Permalink to this definition&quot;){.headerlink}

:   Returns a Response object with the same members, except for
    those members given new values by whichever keyword arguments
    are specified. The attribute [[`Response.meta`{.xref .py
    .py-attr .docutils .literal
    .notranslate}]{.pre}](#scrapy.http.Response.meta &quot;scrapy.http.Response.meta&quot;){.reference
    .internal} is copied by default.

[[urljoin]{.pre}]{.sig-name .descname}[(]{.sig-paren}*[[url]{.pre}]{.n}*[)]{.sig-paren}[[[\[source\]]{.pre}]{.viewcode-link}](_modules/scrapy/http/response.html#Response.urljoin){.reference .internal}[¶](#scrapy.http.Response.urljoin &quot;Permalink to this definition&quot;){.headerlink}

:   Constructs an absolute url by combining the Response's
    [[`url`{.xref .py .py-attr .docutils .literal
    .notranslate}]{.pre}](#scrapy.http.Response.url &quot;scrapy.http.Response.url&quot;){.reference
    .internal} with a possible relative url.

    This is a wrapper over [[`urljoin()`{.xref .py .py-func
    .docutils .literal
    .notranslate}]{.pre}](https://docs.python.org/3/library/urllib.parse.html#urllib.parse.urljoin &quot;(in Python v3.12)&quot;){.reference
    .external}, it's merely an alias for making this call:

    ::: {.highlight-default .notranslate}
    ::: highlight
        urllib.parse.urljoin(response.url, url)
    :::
    :::

[[follow]{.pre}]{.sig-name .descname}[(]{.sig-paren}*[[url]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[[[Union]{.pre}](https://docs.python.org/3/library/typing.html#typing.Union &quot;(in Python v3.12)&quot;){.reference .external}[[\[]{.pre}]{.p}[[str]{.pre}](https://docs.python.org/3/library/stdtypes.html#str &quot;(in Python v3.12)&quot;){.reference .external}[[,]{.pre}]{.p}[ ]{.w}[[Link]{.pre}](index.html#scrapy.link.Link &quot;scrapy.link.Link&quot;){.reference .internal}[[\]]{.pre}]{.p}]{.n}*, *[[callback]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[[[Optional]{.pre}](https://docs.python.org/3/library/typing.html#typing.Optional &quot;(in Python v3.12)&quot;){.reference .external}[[\[]{.pre}]{.p}[[Callable]{.pre}](https://docs.python.org/3/library/typing.html#typing.Callable &quot;(in Python v3.12)&quot;){.reference .external}[[\]]{.pre}]{.p}]{.n}[ ]{.w}[[=]{.pre}]{.o}[ ]{.w}[[None]{.pre}]{.default_value}*, *[[method]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[[[str]{.pre}](https://docs.python.org/3/library/stdtypes.html#str &quot;(in Python v3.12)&quot;){.reference .external}]{.n}[ ]{.w}[[=]{.pre}]{.o}[ ]{.w}[[\'GET\']{.pre}]{.default_value}*, *[[headers]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[[[Optional]{.pre}](https://docs.python.org/3/library/typing.html#typing.Optional &quot;(in Python v3.12)&quot;){.reference .external}[[\[]{.pre}]{.p}[[Union]{.pre}](https://docs.python.org/3/library/typing.html#typing.Union &quot;(in Python v3.12)&quot;){.reference .external}[[\[]{.pre}]{.p}[[Mapping]{.pre}](https://docs.python.org/3/library/typing.html#typing.Mapping &quot;(in Python v3.12)&quot;){.reference .external}[[\[]{.pre}]{.p}[AnyStr]{.pre}[[,]{.pre}]{.p}[ ]{.w}[[Any]{.pre}](https://docs.python.org/3/library/typing.html#typing.Any &quot;(in Python v3.12)&quot;){.reference .external}[[\]]{.pre}]{.p}[[,]{.pre}]{.p}[ ]{.w}[[Iterable]{.pre}](https://docs.python.org/3/library/typing.html#typing.Iterable &quot;(in Python v3.12)&quot;){.reference .external}[[\[]{.pre}]{.p}[[Tuple]{.pre}](https://docs.python.org/3/library/typing.html#typing.Tuple &quot;(in Python v3.12)&quot;){.reference .external}[[\[]{.pre}]{.p}[AnyStr]{.pre}[[,]{.pre}]{.p}[ ]{.w}[[Any]{.pre}](https://docs.python.org/3/library/typing.html#typing.Any &quot;(in Python v3.12)&quot;){.reference .external}[[\]]{.pre}]{.p}[[\]]{.pre}]{.p}[[\]]{.pre}]{.p}[[\]]{.pre}]{.p}]{.n}[ ]{.w}[[=]{.pre}]{.o}[ ]{.w}[[None]{.pre}]{.default_value}*, *[[body]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[[[Optional]{.pre}](https://docs.python.org/3/library/typing.html#typing.Optional &quot;(in Python v3.12)&quot;){.reference .external}[[\[]{.pre}]{.p}[[Union]{.pre}](https://docs.python.org/3/library/typing.html#typing.Union &quot;(in Python v3.12)&quot;){.reference .external}[[\[]{.pre}]{.p}[[bytes]{.pre}](https://docs.python.org/3/library/stdtypes.html#bytes &quot;(in Python v3.12)&quot;){.reference .external}[[,]{.pre}]{.p}[ ]{.w}[[str]{.pre}](https://docs.python.org/3/library/stdtypes.html#str &quot;(in Python v3.12)&quot;){.reference .external}[[\]]{.pre}]{.p}[[\]]{.pre}]{.p}]{.n}[ ]{.w}[[=]{.pre}]{.o}[ ]{.w}[[None]{.pre}]{.default_value}*, *[[cookies]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[[[Optional]{.pre}](https://docs.python.org/3/library/typing.html#typing.Optional &quot;(in Python v3.12)&quot;){.reference .external}[[\[]{.pre}]{.p}[[Union]{.pre}](https://docs.python.org/3/library/typing.html#typing.Union &quot;(in Python v3.12)&quot;){.reference .external}[[\[]{.pre}]{.p}[[dict]{.pre}](https://docs.python.org/3/library/stdtypes.html#dict &quot;(in Python v3.12)&quot;){.reference .external}[[,]{.pre}]{.p}[ ]{.w}[[List]{.pre}](https://docs.python.org/3/library/typing.html#typing.List &quot;(in Python v3.12)&quot;){.reference .external}[[\[]{.pre}]{.p}[[dict]{.pre}](https://docs.python.org/3/library/stdtypes.html#dict &quot;(in Python v3.12)&quot;){.reference .external}[[\]]{.pre}]{.p}[[\]]{.pre}]{.p}[[\]]{.pre}]{.p}]{.n}[ ]{.w}[[=]{.pre}]{.o}[ ]{.w}[[None]{.pre}]{.default_value}*, *[[meta]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[[[Optional]{.pre}](https://docs.python.org/3/library/typing.html#typing.Optional &quot;(in Python v3.12)&quot;){.reference .external}[[\[]{.pre}]{.p}[[Dict]{.pre}](https://docs.python.org/3/library/typing.html#typing.Dict &quot;(in Python v3.12)&quot;){.reference .external}[[\[]{.pre}]{.p}[[str]{.pre}](https://docs.python.org/3/library/stdtypes.html#str &quot;(in Python v3.12)&quot;){.reference .external}[[,]{.pre}]{.p}[ ]{.w}[[Any]{.pre}](https://docs.python.org/3/library/typing.html#typing.Any &quot;(in Python v3.12)&quot;){.reference .external}[[\]]{.pre}]{.p}[[\]]{.pre}]{.p}]{.n}[ ]{.w}[[=]{.pre}]{.o}[ ]{.w}[[None]{.pre}]{.default_value}*, *[[encoding]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[[[Optional]{.pre}](https://docs.python.org/3/library/typing.html#typing.Optional &quot;(in Python v3.12)&quot;){.reference .external}[[\[]{.pre}]{.p}[[str]{.pre}](https://docs.python.org/3/library/stdtypes.html#str &quot;(in Python v3.12)&quot;){.reference .external}[[\]]{.pre}]{.p}]{.n}[ ]{.w}[[=]{.pre}]{.o}[ ]{.w}[[\'utf-8\']{.pre}]{.default_value}*, *[[priority]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[[[int]{.pre}](https://docs.python.org/3/library/functions.html#int &quot;(in Python v3.12)&quot;){.reference .external}]{.n}[ ]{.w}[[=]{.pre}]{.o}[ ]{.w}[[0]{.pre}]{.default_value}*, *[[dont_filter]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[[[bool]{.pre}](https://docs.python.org/3/library/functions.html#bool &quot;(in Python v3.12)&quot;){.reference .external}]{.n}[ ]{.w}[[=]{.pre}]{.o}[ ]{.w}[[False]{.pre}]{.default_value}*, *[[errback]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[[[Optional]{.pre}](https://docs.python.org/3/library/typing.html#typing.Optional &quot;(in Python v3.12)&quot;){.reference .external}[[\[]{.pre}]{.p}[[Callable]{.pre}](https://docs.python.org/3/library/typing.html#typing.Callable &quot;(in Python v3.12)&quot;){.reference .external}[[\]]{.pre}]{.p}]{.n}[ ]{.w}[[=]{.pre}]{.o}[ ]{.w}[[None]{.pre}]{.default_value}*, *[[cb_kwargs]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[[[Optional]{.pre}](https://docs.python.org/3/library/typing.html#typing.Optional &quot;(in Python v3.12)&quot;){.reference .external}[[\[]{.pre}]{.p}[[Dict]{.pre}](https://docs.python.org/3/library/typing.html#typing.Dict &quot;(in Python v3.12)&quot;){.reference .external}[[\[]{.pre}]{.p}[[str]{.pre}](https://docs.python.org/3/library/stdtypes.html#str &quot;(in Python v3.12)&quot;){.reference .external}[[,]{.pre}]{.p}[ ]{.w}[[Any]{.pre}](https://docs.python.org/3/library/typing.html#typing.Any &quot;(in Python v3.12)&quot;){.reference .external}[[\]]{.pre}]{.p}[[\]]{.pre}]{.p}]{.n}[ ]{.w}[[=]{.pre}]{.o}[ ]{.w}[[None]{.pre}]{.default_value}*, *[[flags]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[[[Optional]{.pre}](https://docs.python.org/3/library/typing.html#typing.Optional &quot;(in Python v3.12)&quot;){.reference .external}[[\[]{.pre}]{.p}[[List]{.pre}](https://docs.python.org/3/library/typing.html#typing.List &quot;(in Python v3.12)&quot;){.reference .external}[[\[]{.pre}]{.p}[[str]{.pre}](https://docs.python.org/3/library/stdtypes.html#str &quot;(in Python v3.12)&quot;){.reference .external}[[\]]{.pre}]{.p}[[\]]{.pre}]{.p}]{.n}[ ]{.w}[[=]{.pre}]{.o}[ ]{.w}[[None]{.pre}]{.default_value}*[)]{.sig-paren} [[→]{.sig-return-icon} [[[Request]{.pre}](index.html#scrapy.http.Request &quot;scrapy.http.request.Request&quot;){.reference .internal}]{.sig-return-typehint}]{.sig-return}[[[\[source\]]{.pre}]{.viewcode-link}](_modules/scrapy/http/response.html#Response.follow){.reference .internal}[¶](#scrapy.http.Response.follow &quot;Permalink to this definition&quot;){.headerlink}

:   Return a [[`Request`{.xref .py .py-class .docutils .literal
    .notranslate}]{.pre}](#scrapy.http.Request &quot;scrapy.http.Request&quot;){.reference
    .internal} instance to follow a link [`url`{.docutils .literal
    .notranslate}]{.pre}. It accepts the same arguments as
    [`Request.__init__`{.docutils .literal .notranslate}]{.pre}
    method, but [`url`{.docutils .literal .notranslate}]{.pre} can
    be a relative URL or a [`scrapy.link.Link`{.docutils .literal
    .notranslate}]{.pre} object, not only an absolute URL.

    [[`TextResponse`{.xref .py .py-class .docutils .literal
    .notranslate}]{.pre}](#scrapy.http.TextResponse &quot;scrapy.http.TextResponse&quot;){.reference
    .internal} provides a [[`follow()`{.xref .py .py-meth .docutils
    .literal
    .notranslate}]{.pre}](#scrapy.http.TextResponse.follow &quot;scrapy.http.TextResponse.follow&quot;){.reference
    .internal} method which supports selectors in addition to
    absolute/relative URLs and Link objects.

    ::: versionadded
    [New in version 2.0: ]{.versionmodified .added}The *flags*
    parameter.
    :::

[[follow_all]{.pre}]{.sig-name .descname}[(]{.sig-paren}*[[urls]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[[[Iterable]{.pre}](https://docs.python.org/3/library/typing.html#typing.Iterable &quot;(in Python v3.12)&quot;){.reference .external}[[\[]{.pre}]{.p}[[Union]{.pre}](https://docs.python.org/3/library/typing.html#typing.Union &quot;(in Python v3.12)&quot;){.reference .external}[[\[]{.pre}]{.p}[[str]{.pre}](https://docs.python.org/3/library/stdtypes.html#str &quot;(in Python v3.12)&quot;){.reference .external}[[,]{.pre}]{.p}[ ]{.w}[[Link]{.pre}](index.html#scrapy.link.Link &quot;scrapy.link.Link&quot;){.reference .internal}[[\]]{.pre}]{.p}[[\]]{.pre}]{.p}]{.n}*, *[[callback]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[[[Optional]{.pre}](https://docs.python.org/3/library/typing.html#typing.Optional &quot;(in Python v3.12)&quot;){.reference .external}[[\[]{.pre}]{.p}[[Callable]{.pre}](https://docs.python.org/3/library/typing.html#typing.Callable &quot;(in Python v3.12)&quot;){.reference .external}[[\]]{.pre}]{.p}]{.n}[ ]{.w}[[=]{.pre}]{.o}[ ]{.w}[[None]{.pre}]{.default_value}*, *[[method]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[[[str]{.pre}](https://docs.python.org/3/library/stdtypes.html#str &quot;(in Python v3.12)&quot;){.reference .external}]{.n}[ ]{.w}[[=]{.pre}]{.o}[ ]{.w}[[\'GET\']{.pre}]{.default_value}*, *[[headers]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[[[Optional]{.pre}](https://docs.python.org/3/library/typing.html#typing.Optional &quot;(in Python v3.12)&quot;){.reference .external}[[\[]{.pre}]{.p}[[Union]{.pre}](https://docs.python.org/3/library/typing.html#typing.Union &quot;(in Python v3.12)&quot;){.reference .external}[[\[]{.pre}]{.p}[[Mapping]{.pre}](https://docs.python.org/3/library/typing.html#typing.Mapping &quot;(in Python v3.12)&quot;){.reference .external}[[\[]{.pre}]{.p}[AnyStr]{.pre}[[,]{.pre}]{.p}[ ]{.w}[[Any]{.pre}](https://docs.python.org/3/library/typing.html#typing.Any &quot;(in Python v3.12)&quot;){.reference .external}[[\]]{.pre}]{.p}[[,]{.pre}]{.p}[ ]{.w}[[Iterable]{.pre}](https://docs.python.org/3/library/typing.html#typing.Iterable &quot;(in Python v3.12)&quot;){.reference .external}[[\[]{.pre}]{.p}[[Tuple]{.pre}](https://docs.python.org/3/library/typing.html#typing.Tuple &quot;(in Python v3.12)&quot;){.reference .external}[[\[]{.pre}]{.p}[AnyStr]{.pre}[[,]{.pre}]{.p}[ ]{.w}[[Any]{.pre}](https://docs.python.org/3/library/typing.html#typing.Any &quot;(in Python v3.12)&quot;){.reference .external}[[\]]{.pre}]{.p}[[\]]{.pre}]{.p}[[\]]{.pre}]{.p}[[\]]{.pre}]{.p}]{.n}[ ]{.w}[[=]{.pre}]{.o}[ ]{.w}[[None]{.pre}]{.default_value}*, *[[body]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[[[Optional]{.pre}](https://docs.python.org/3/library/typing.html#typing.Optional &quot;(in Python v3.12)&quot;){.reference .external}[[\[]{.pre}]{.p}[[Union]{.pre}](https://docs.python.org/3/library/typing.html#typing.Union &quot;(in Python v3.12)&quot;){.reference .external}[[\[]{.pre}]{.p}[[bytes]{.pre}](https://docs.python.org/3/library/stdtypes.html#bytes &quot;(in Python v3.12)&quot;){.reference .external}[[,]{.pre}]{.p}[ ]{.w}[[str]{.pre}](https://docs.python.org/3/library/stdtypes.html#str &quot;(in Python v3.12)&quot;){.reference .external}[[\]]{.pre}]{.p}[[\]]{.pre}]{.p}]{.n}[ ]{.w}[[=]{.pre}]{.o}[ ]{.w}[[None]{.pre}]{.default_value}*, *[[cookies]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[[[Optional]{.pre}](https://docs.python.org/3/library/typing.html#typing.Optional &quot;(in Python v3.12)&quot;){.reference .external}[[\[]{.pre}]{.p}[[Union]{.pre}](https://docs.python.org/3/library/typing.html#typing.Union &quot;(in Python v3.12)&quot;){.reference .external}[[\[]{.pre}]{.p}[[dict]{.pre}](https://docs.python.org/3/library/stdtypes.html#dict &quot;(in Python v3.12)&quot;){.reference .external}[[,]{.pre}]{.p}[ ]{.w}[[List]{.pre}](https://docs.python.org/3/library/typing.html#typing.List &quot;(in Python v3.12)&quot;){.reference .external}[[\[]{.pre}]{.p}[[dict]{.pre}](https://docs.python.org/3/library/stdtypes.html#dict &quot;(in Python v3.12)&quot;){.reference .external}[[\]]{.pre}]{.p}[[\]]{.pre}]{.p}[[\]]{.pre}]{.p}]{.n}[ ]{.w}[[=]{.pre}]{.o}[ ]{.w}[[None]{.pre}]{.default_value}*, *[[meta]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[[[Optional]{.pre}](https://docs.python.org/3/library/typing.html#typing.Optional &quot;(in Python v3.12)&quot;){.reference .external}[[\[]{.pre}]{.p}[[Dict]{.pre}](https://docs.python.org/3/library/typing.html#typing.Dict &quot;(in Python v3.12)&quot;){.reference .external}[[\[]{.pre}]{.p}[[str]{.pre}](https://docs.python.org/3/library/stdtypes.html#str &quot;(in Python v3.12)&quot;){.reference .external}[[,]{.pre}]{.p}[ ]{.w}[[Any]{.pre}](https://docs.python.org/3/library/typing.html#typing.Any &quot;(in Python v3.12)&quot;){.reference .external}[[\]]{.pre}]{.p}[[\]]{.pre}]{.p}]{.n}[ ]{.w}[[=]{.pre}]{.o}[ ]{.w}[[None]{.pre}]{.default_value}*, *[[encoding]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[[[Optional]{.pre}](https://docs.python.org/3/library/typing.html#typing.Optional &quot;(in Python v3.12)&quot;){.reference .external}[[\[]{.pre}]{.p}[[str]{.pre}](https://docs.python.org/3/library/stdtypes.html#str &quot;(in Python v3.12)&quot;){.reference .external}[[\]]{.pre}]{.p}]{.n}[ ]{.w}[[=]{.pre}]{.o}[ ]{.w}[[\'utf-8\']{.pre}]{.default_value}*, *[[priority]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[[[int]{.pre}](https://docs.python.org/3/library/functions.html#int &quot;(in Python v3.12)&quot;){.reference .external}]{.n}[ ]{.w}[[=]{.pre}]{.o}[ ]{.w}[[0]{.pre}]{.default_value}*, *[[dont_filter]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[[[bool]{.pre}](https://docs.python.org/3/library/functions.html#bool &quot;(in Python v3.12)&quot;){.reference .external}]{.n}[ ]{.w}[[=]{.pre}]{.o}[ ]{.w}[[False]{.pre}]{.default_value}*, *[[errback]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[[[Optional]{.pre}](https://docs.python.org/3/library/typing.html#typing.Optional &quot;(in Python v3.12)&quot;){.reference .external}[[\[]{.pre}]{.p}[[Callable]{.pre}](https://docs.python.org/3/library/typing.html#typing.Callable &quot;(in Python v3.12)&quot;){.reference .external}[[\]]{.pre}]{.p}]{.n}[ ]{.w}[[=]{.pre}]{.o}[ ]{.w}[[None]{.pre}]{.default_value}*, *[[cb_kwargs]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[[[Optional]{.pre}](https://docs.python.org/3/library/typing.html#typing.Optional &quot;(in Python v3.12)&quot;){.reference .external}[[\[]{.pre}]{.p}[[Dict]{.pre}](https://docs.python.org/3/library/typing.html#typing.Dict &quot;(in Python v3.12)&quot;){.reference .external}[[\[]{.pre}]{.p}[[str]{.pre}](https://docs.python.org/3/library/stdtypes.html#str &quot;(in Python v3.12)&quot;){.reference .external}[[,]{.pre}]{.p}[ ]{.w}[[Any]{.pre}](https://docs.python.org/3/library/typing.html#typing.Any &quot;(in Python v3.12)&quot;){.reference .external}[[\]]{.pre}]{.p}[[\]]{.pre}]{.p}]{.n}[ ]{.w}[[=]{.pre}]{.o}[ ]{.w}[[None]{.pre}]{.default_value}*, *[[flags]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[[[Optional]{.pre}](https://docs.python.org/3/library/typing.html#typing.Optional &quot;(in Python v3.12)&quot;){.reference .external}[[\[]{.pre}]{.p}[[List]{.pre}](https://docs.python.org/3/library/typing.html#typing.List &quot;(in Python v3.12)&quot;){.reference .external}[[\[]{.pre}]{.p}[[str]{.pre}](https://docs.python.org/3/library/stdtypes.html#str &quot;(in Python v3.12)&quot;){.reference .external}[[\]]{.pre}]{.p}[[\]]{.pre}]{.p}]{.n}[ ]{.w}[[=]{.pre}]{.o}[ ]{.w}[[None]{.pre}]{.default_value}*[)]{.sig-paren} [[→]{.sig-return-icon} [[[Generator]{.pre}](https://docs.python.org/3/library/typing.html#typing.Generator &quot;(in Python v3.12)&quot;){.reference .external}[[\[]{.pre}]{.p}[[Request]{.pre}](index.html#scrapy.http.Request &quot;scrapy.http.request.Request&quot;){.reference .internal}[[,]{.pre}]{.p}[ ]{.w}[[None]{.pre}](https://docs.python.org/3/library/constants.html#None &quot;(in Python v3.12)&quot;){.reference .external}[[,]{.pre}]{.p}[ ]{.w}[[None]{.pre}](https://docs.python.org/3/library/constants.html#None &quot;(in Python v3.12)&quot;){.reference .external}[[\]]{.pre}]{.p}]{.sig-return-typehint}]{.sig-return}[[[\[source\]]{.pre}]{.viewcode-link}](_modules/scrapy/http/response.html#Response.follow_all){.reference .internal}[¶](#scrapy.http.Response.follow_all &quot;Permalink to this definition&quot;){.headerlink}

:   ::: versionadded
    [New in version 2.0.]{.versionmodified .added}
    :::

    Return an iterable of [[`Request`{.xref .py .py-class .docutils
    .literal
    .notranslate}]{.pre}](#scrapy.http.Request &quot;scrapy.http.Request&quot;){.reference
    .internal} instances to follow all links in [`urls`{.docutils
    .literal .notranslate}]{.pre}. It accepts the same arguments as
    [`Request.__init__`{.docutils .literal .notranslate}]{.pre}
    method, but elements of [`urls`{.docutils .literal
    .notranslate}]{.pre} can be relative URLs or [[`Link`{.xref .py
    .py-class .docutils .literal
    .notranslate}]{.pre}](index.html#scrapy.link.Link &quot;scrapy.link.Link&quot;){.reference
    .internal} objects, not only absolute URLs.

    [[`TextResponse`{.xref .py .py-class .docutils .literal
    .notranslate}]{.pre}](#scrapy.http.TextResponse &quot;scrapy.http.TextResponse&quot;){.reference
    .internal} provides a [[`follow_all()`{.xref .py .py-meth
    .docutils .literal
    .notranslate}]{.pre}](#scrapy.http.TextResponse.follow_all &quot;scrapy.http.TextResponse.follow_all&quot;){.reference
    .internal} method which supports selectors in addition to
    absolute/relative URLs and Link objects.
</code></pre>
<p>:::</p>
<p>::: {#response-subclasses .section}
[]{#topics-request-response-ref-response-subclasses}</p>
<h4 id="response-subclassesheaderlink"><a class="header" href="#response-subclassesheaderlink">Response subclasses<a href="#response-subclasses" title="Permalink to this heading">¶</a>{.headerlink}</a></h4>
<p>Here is the list of available built-in Response subclasses. You can also
subclass the Response class to implement your own functionality.</p>
<p>::: {#textresponse-objects .section}</p>
<h5 id="textresponse-objectsheaderlink"><a class="header" href="#textresponse-objectsheaderlink">TextResponse objects<a href="#textresponse-objects" title="Permalink to this heading">¶</a>{.headerlink}</a></h5>
<p><em>[class]{.pre}[ ]{.w}</em>[[scrapy.http.]{.pre}]{.sig-prename .descclassname}[[TextResponse]{.pre}]{.sig-name .descname}[(]{.sig-paren}<em>[[url]{.pre}]{.n}</em>[[]{.optional}, <em>[[encoding]{.pre}]{.n}</em>[[]{.optional}, <em>[[...]{.pre}]{.n}</em>[]]{.optional}[]]{.optional}[)]{.sig-paren}<a href="_modules/scrapy/http/response/text.html#TextResponse">[[[source]]{.pre}]{.viewcode-link}</a>{.reference .internal}<a href="#scrapy.http.TextResponse" title="Permalink to this definition">¶</a>{.headerlink}</p>
<p>:   <a href="#scrapy.http.TextResponse" title="scrapy.http.TextResponse">[<code>TextResponse</code>{.xref .py .py-class .docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal} objects adds encoding capabilities to the base
<a href="#scrapy.http.Response" title="scrapy.http.Response">[<code>Response</code>{.xref .py .py-class .docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal} class, which is meant to be used only for binary data,
such as images, sounds or any media file.</p>
<pre><code>[[`TextResponse`{.xref .py .py-class .docutils .literal
.notranslate}]{.pre}](#scrapy.http.TextResponse &quot;scrapy.http.TextResponse&quot;){.reference
.internal} objects support a new [`__init__`{.docutils .literal
.notranslate}]{.pre} method argument, in addition to the base
[[`Response`{.xref .py .py-class .docutils .literal
.notranslate}]{.pre}](#scrapy.http.Response &quot;scrapy.http.Response&quot;){.reference
.internal} objects. The remaining functionality is the same as for
the [[`Response`{.xref .py .py-class .docutils .literal
.notranslate}]{.pre}](#scrapy.http.Response &quot;scrapy.http.Response&quot;){.reference
.internal} class and is not documented here.

Parameters

:   **encoding**
    ([*str*](https://docs.python.org/3/library/stdtypes.html#str &quot;(in Python v3.12)&quot;){.reference
    .external}) -- is a string which contains the encoding to use
    for this response. If you create a [[`TextResponse`{.xref .py
    .py-class .docutils .literal
    .notranslate}]{.pre}](#scrapy.http.TextResponse &quot;scrapy.http.TextResponse&quot;){.reference
    .internal} object with a string as body, it will be converted to
    bytes encoded using this encoding. If *encoding* is
    [`None`{.docutils .literal .notranslate}]{.pre} (default), the
    encoding will be looked up in the response headers and body
    instead.

[[`TextResponse`{.xref .py .py-class .docutils .literal
.notranslate}]{.pre}](#scrapy.http.TextResponse &quot;scrapy.http.TextResponse&quot;){.reference
.internal} objects support the following attributes in addition to
the standard [[`Response`{.xref .py .py-class .docutils .literal
.notranslate}]{.pre}](#scrapy.http.Response &quot;scrapy.http.Response&quot;){.reference
.internal} ones:

[[text]{.pre}]{.sig-name .descname}[¶](#scrapy.http.TextResponse.text &quot;Permalink to this definition&quot;){.headerlink}

:   Response body, as a string.

    The same as [`response.body.decode(response.encoding)`{.docutils
    .literal .notranslate}]{.pre}, but the result is cached after
    the first call, so you can access [`response.text`{.docutils
    .literal .notranslate}]{.pre} multiple times without extra
    overhead.

    ::: {.admonition .note}
    Note

    [`str(response.body)`{.docutils .literal .notranslate}]{.pre} is
    not a correct way to convert the response body into a string:

    ::: {.highlight-pycon .notranslate}
    ::: highlight
        &gt;&gt;&gt; str(b&quot;body&quot;)
        &quot;b'body'&quot;
    :::
    :::
    :::

[[encoding]{.pre}]{.sig-name .descname}[¶](#scrapy.http.TextResponse.encoding &quot;Permalink to this definition&quot;){.headerlink}

:   A string with the encoding of this response. The encoding is
    resolved by trying the following mechanisms, in order:

    1.  the encoding passed in the [`__init__`{.docutils .literal
        .notranslate}]{.pre} method [`encoding`{.docutils .literal
        .notranslate}]{.pre} argument

    2.  the encoding declared in the Content-Type HTTP header. If
        this encoding is not valid (i.e. unknown), it is ignored and
        the next resolution mechanism is tried.

    3.  the encoding declared in the response body. The TextResponse
        class doesn't provide any special functionality for this.
        However, the [[`HtmlResponse`{.xref .py .py-class .docutils
        .literal
        .notranslate}]{.pre}](#scrapy.http.HtmlResponse &quot;scrapy.http.HtmlResponse&quot;){.reference
        .internal} and [[`XmlResponse`{.xref .py .py-class .docutils
        .literal
        .notranslate}]{.pre}](#scrapy.http.XmlResponse &quot;scrapy.http.XmlResponse&quot;){.reference
        .internal} classes do.

    4.  the encoding inferred by looking at the response body. This
        is the more fragile method but also the last one tried.

[[selector]{.pre}]{.sig-name .descname}[¶](#scrapy.http.TextResponse.selector &quot;Permalink to this definition&quot;){.headerlink}

:   A [`Selector`{.xref .py .py-class .docutils .literal
    .notranslate}]{.pre} instance using the response as target. The
    selector is lazily instantiated on first access.

[[attributes]{.pre}]{.sig-name .descname}*[[:]{.pre}]{.p}[ ]{.w}[Tuple]{.pre}[[\[]{.pre}]{.p}[[str]{.pre}](https://docs.python.org/3/library/stdtypes.html#str &quot;(in Python v3.12)&quot;){.reference .external}[[,]{.pre}]{.p}[ ]{.w}[[\...]{.pre}]{.p}[[\]]{.pre}]{.p}[ ]{.w}[[=]{.pre}]{.p}[ ]{.w}[(\'url\',]{.pre} [\'status\',]{.pre} [\'headers\',]{.pre} [\'body\',]{.pre} [\'flags\',]{.pre} [\'request\',]{.pre} [\'certificate\',]{.pre} [\'ip_address\',]{.pre} [\'protocol\',]{.pre} [\'encoding\')]{.pre}*[¶](#scrapy.http.TextResponse.attributes &quot;Permalink to this definition&quot;){.headerlink}

:   A tuple of [[`str`{.xref .py .py-class .docutils .literal
    .notranslate}]{.pre}](https://docs.python.org/3/library/stdtypes.html#str &quot;(in Python v3.12)&quot;){.reference
    .external} objects containing the name of all public attributes
    of the class that are also keyword parameters of the
    [`__init__`{.docutils .literal .notranslate}]{.pre} method.

    Currently used by [[`Response.replace()`{.xref .py .py-meth
    .docutils .literal
    .notranslate}]{.pre}](#scrapy.http.Response.replace &quot;scrapy.http.Response.replace&quot;){.reference
    .internal}.

[[`TextResponse`{.xref .py .py-class .docutils .literal
.notranslate}]{.pre}](#scrapy.http.TextResponse &quot;scrapy.http.TextResponse&quot;){.reference
.internal} objects support the following methods in addition to the
standard [[`Response`{.xref .py .py-class .docutils .literal
.notranslate}]{.pre}](#scrapy.http.Response &quot;scrapy.http.Response&quot;){.reference
.internal} ones:

[[jmespath]{.pre}]{.sig-name .descname}[(]{.sig-paren}*[[query]{.pre}]{.n}*[)]{.sig-paren}[[[\[source\]]{.pre}]{.viewcode-link}](_modules/scrapy/http/response/text.html#TextResponse.jmespath){.reference .internal}[¶](#scrapy.http.TextResponse.jmespath &quot;Permalink to this definition&quot;){.headerlink}

:   A shortcut to [`TextResponse.selector.jmespath(query)`{.docutils
    .literal .notranslate}]{.pre}:

    ::: {.highlight-default .notranslate}
    ::: highlight
        response.jmespath('object.[*]')
    :::
    :::

[[xpath]{.pre}]{.sig-name .descname}[(]{.sig-paren}*[[query]{.pre}]{.n}*[)]{.sig-paren}[[[\[source\]]{.pre}]{.viewcode-link}](_modules/scrapy/http/response/text.html#TextResponse.xpath){.reference .internal}[¶](#scrapy.http.TextResponse.xpath &quot;Permalink to this definition&quot;){.headerlink}

:   A shortcut to [`TextResponse.selector.xpath(query)`{.docutils
    .literal .notranslate}]{.pre}:

    ::: {.highlight-default .notranslate}
    ::: highlight
        response.xpath('//p')
    :::
    :::

[[css]{.pre}]{.sig-name .descname}[(]{.sig-paren}*[[query]{.pre}]{.n}*[)]{.sig-paren}[[[\[source\]]{.pre}]{.viewcode-link}](_modules/scrapy/http/response/text.html#TextResponse.css){.reference .internal}[¶](#scrapy.http.TextResponse.css &quot;Permalink to this definition&quot;){.headerlink}

:   A shortcut to [`TextResponse.selector.css(query)`{.docutils
    .literal .notranslate}]{.pre}:

    ::: {.highlight-default .notranslate}
    ::: highlight
        response.css('p')
    :::
    :::

[[follow]{.pre}]{.sig-name .descname}[(]{.sig-paren}*[[url]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[[[Union]{.pre}](https://docs.python.org/3/library/typing.html#typing.Union &quot;(in Python v3.12)&quot;){.reference .external}[[\[]{.pre}]{.p}[[str]{.pre}](https://docs.python.org/3/library/stdtypes.html#str &quot;(in Python v3.12)&quot;){.reference .external}[[,]{.pre}]{.p}[ ]{.w}[[Link]{.pre}](index.html#scrapy.link.Link &quot;scrapy.link.Link&quot;){.reference .internal}[[,]{.pre}]{.p}[ ]{.w}[Selector]{.pre}[[\]]{.pre}]{.p}]{.n}*, *[[callback]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[[[Optional]{.pre}](https://docs.python.org/3/library/typing.html#typing.Optional &quot;(in Python v3.12)&quot;){.reference .external}[[\[]{.pre}]{.p}[[Callable]{.pre}](https://docs.python.org/3/library/typing.html#typing.Callable &quot;(in Python v3.12)&quot;){.reference .external}[[\]]{.pre}]{.p}]{.n}[ ]{.w}[[=]{.pre}]{.o}[ ]{.w}[[None]{.pre}]{.default_value}*, *[[method]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[[[str]{.pre}](https://docs.python.org/3/library/stdtypes.html#str &quot;(in Python v3.12)&quot;){.reference .external}]{.n}[ ]{.w}[[=]{.pre}]{.o}[ ]{.w}[[\'GET\']{.pre}]{.default_value}*, *[[headers]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[[[Optional]{.pre}](https://docs.python.org/3/library/typing.html#typing.Optional &quot;(in Python v3.12)&quot;){.reference .external}[[\[]{.pre}]{.p}[[Union]{.pre}](https://docs.python.org/3/library/typing.html#typing.Union &quot;(in Python v3.12)&quot;){.reference .external}[[\[]{.pre}]{.p}[[Mapping]{.pre}](https://docs.python.org/3/library/typing.html#typing.Mapping &quot;(in Python v3.12)&quot;){.reference .external}[[\[]{.pre}]{.p}[AnyStr]{.pre}[[,]{.pre}]{.p}[ ]{.w}[[Any]{.pre}](https://docs.python.org/3/library/typing.html#typing.Any &quot;(in Python v3.12)&quot;){.reference .external}[[\]]{.pre}]{.p}[[,]{.pre}]{.p}[ ]{.w}[[Iterable]{.pre}](https://docs.python.org/3/library/typing.html#typing.Iterable &quot;(in Python v3.12)&quot;){.reference .external}[[\[]{.pre}]{.p}[[Tuple]{.pre}](https://docs.python.org/3/library/typing.html#typing.Tuple &quot;(in Python v3.12)&quot;){.reference .external}[[\[]{.pre}]{.p}[AnyStr]{.pre}[[,]{.pre}]{.p}[ ]{.w}[[Any]{.pre}](https://docs.python.org/3/library/typing.html#typing.Any &quot;(in Python v3.12)&quot;){.reference .external}[[\]]{.pre}]{.p}[[\]]{.pre}]{.p}[[\]]{.pre}]{.p}[[\]]{.pre}]{.p}]{.n}[ ]{.w}[[=]{.pre}]{.o}[ ]{.w}[[None]{.pre}]{.default_value}*, *[[body]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[[[Optional]{.pre}](https://docs.python.org/3/library/typing.html#typing.Optional &quot;(in Python v3.12)&quot;){.reference .external}[[\[]{.pre}]{.p}[[Union]{.pre}](https://docs.python.org/3/library/typing.html#typing.Union &quot;(in Python v3.12)&quot;){.reference .external}[[\[]{.pre}]{.p}[[bytes]{.pre}](https://docs.python.org/3/library/stdtypes.html#bytes &quot;(in Python v3.12)&quot;){.reference .external}[[,]{.pre}]{.p}[ ]{.w}[[str]{.pre}](https://docs.python.org/3/library/stdtypes.html#str &quot;(in Python v3.12)&quot;){.reference .external}[[\]]{.pre}]{.p}[[\]]{.pre}]{.p}]{.n}[ ]{.w}[[=]{.pre}]{.o}[ ]{.w}[[None]{.pre}]{.default_value}*, *[[cookies]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[[[Optional]{.pre}](https://docs.python.org/3/library/typing.html#typing.Optional &quot;(in Python v3.12)&quot;){.reference .external}[[\[]{.pre}]{.p}[[Union]{.pre}](https://docs.python.org/3/library/typing.html#typing.Union &quot;(in Python v3.12)&quot;){.reference .external}[[\[]{.pre}]{.p}[[dict]{.pre}](https://docs.python.org/3/library/stdtypes.html#dict &quot;(in Python v3.12)&quot;){.reference .external}[[,]{.pre}]{.p}[ ]{.w}[[List]{.pre}](https://docs.python.org/3/library/typing.html#typing.List &quot;(in Python v3.12)&quot;){.reference .external}[[\[]{.pre}]{.p}[[dict]{.pre}](https://docs.python.org/3/library/stdtypes.html#dict &quot;(in Python v3.12)&quot;){.reference .external}[[\]]{.pre}]{.p}[[\]]{.pre}]{.p}[[\]]{.pre}]{.p}]{.n}[ ]{.w}[[=]{.pre}]{.o}[ ]{.w}[[None]{.pre}]{.default_value}*, *[[meta]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[[[Optional]{.pre}](https://docs.python.org/3/library/typing.html#typing.Optional &quot;(in Python v3.12)&quot;){.reference .external}[[\[]{.pre}]{.p}[[Dict]{.pre}](https://docs.python.org/3/library/typing.html#typing.Dict &quot;(in Python v3.12)&quot;){.reference .external}[[\[]{.pre}]{.p}[[str]{.pre}](https://docs.python.org/3/library/stdtypes.html#str &quot;(in Python v3.12)&quot;){.reference .external}[[,]{.pre}]{.p}[ ]{.w}[[Any]{.pre}](https://docs.python.org/3/library/typing.html#typing.Any &quot;(in Python v3.12)&quot;){.reference .external}[[\]]{.pre}]{.p}[[\]]{.pre}]{.p}]{.n}[ ]{.w}[[=]{.pre}]{.o}[ ]{.w}[[None]{.pre}]{.default_value}*, *[[encoding]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[[[Optional]{.pre}](https://docs.python.org/3/library/typing.html#typing.Optional &quot;(in Python v3.12)&quot;){.reference .external}[[\[]{.pre}]{.p}[[str]{.pre}](https://docs.python.org/3/library/stdtypes.html#str &quot;(in Python v3.12)&quot;){.reference .external}[[\]]{.pre}]{.p}]{.n}[ ]{.w}[[=]{.pre}]{.o}[ ]{.w}[[None]{.pre}]{.default_value}*, *[[priority]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[[[int]{.pre}](https://docs.python.org/3/library/functions.html#int &quot;(in Python v3.12)&quot;){.reference .external}]{.n}[ ]{.w}[[=]{.pre}]{.o}[ ]{.w}[[0]{.pre}]{.default_value}*, *[[dont_filter]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[[[bool]{.pre}](https://docs.python.org/3/library/functions.html#bool &quot;(in Python v3.12)&quot;){.reference .external}]{.n}[ ]{.w}[[=]{.pre}]{.o}[ ]{.w}[[False]{.pre}]{.default_value}*, *[[errback]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[[[Optional]{.pre}](https://docs.python.org/3/library/typing.html#typing.Optional &quot;(in Python v3.12)&quot;){.reference .external}[[\[]{.pre}]{.p}[[Callable]{.pre}](https://docs.python.org/3/library/typing.html#typing.Callable &quot;(in Python v3.12)&quot;){.reference .external}[[\]]{.pre}]{.p}]{.n}[ ]{.w}[[=]{.pre}]{.o}[ ]{.w}[[None]{.pre}]{.default_value}*, *[[cb_kwargs]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[[[Optional]{.pre}](https://docs.python.org/3/library/typing.html#typing.Optional &quot;(in Python v3.12)&quot;){.reference .external}[[\[]{.pre}]{.p}[[Dict]{.pre}](https://docs.python.org/3/library/typing.html#typing.Dict &quot;(in Python v3.12)&quot;){.reference .external}[[\[]{.pre}]{.p}[[str]{.pre}](https://docs.python.org/3/library/stdtypes.html#str &quot;(in Python v3.12)&quot;){.reference .external}[[,]{.pre}]{.p}[ ]{.w}[[Any]{.pre}](https://docs.python.org/3/library/typing.html#typing.Any &quot;(in Python v3.12)&quot;){.reference .external}[[\]]{.pre}]{.p}[[\]]{.pre}]{.p}]{.n}[ ]{.w}[[=]{.pre}]{.o}[ ]{.w}[[None]{.pre}]{.default_value}*, *[[flags]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[[[Optional]{.pre}](https://docs.python.org/3/library/typing.html#typing.Optional &quot;(in Python v3.12)&quot;){.reference .external}[[\[]{.pre}]{.p}[[List]{.pre}](https://docs.python.org/3/library/typing.html#typing.List &quot;(in Python v3.12)&quot;){.reference .external}[[\[]{.pre}]{.p}[[str]{.pre}](https://docs.python.org/3/library/stdtypes.html#str &quot;(in Python v3.12)&quot;){.reference .external}[[\]]{.pre}]{.p}[[\]]{.pre}]{.p}]{.n}[ ]{.w}[[=]{.pre}]{.o}[ ]{.w}[[None]{.pre}]{.default_value}*[)]{.sig-paren} [[→]{.sig-return-icon} [[[Request]{.pre}](index.html#scrapy.http.Request &quot;scrapy.http.request.Request&quot;){.reference .internal}]{.sig-return-typehint}]{.sig-return}[[[\[source\]]{.pre}]{.viewcode-link}](_modules/scrapy/http/response/text.html#TextResponse.follow){.reference .internal}[¶](#scrapy.http.TextResponse.follow &quot;Permalink to this definition&quot;){.headerlink}

:   Return a [[`Request`{.xref .py .py-class .docutils .literal
    .notranslate}]{.pre}](#scrapy.http.Request &quot;scrapy.http.Request&quot;){.reference
    .internal} instance to follow a link [`url`{.docutils .literal
    .notranslate}]{.pre}. It accepts the same arguments as
    [`Request.__init__`{.docutils .literal .notranslate}]{.pre}
    method, but [`url`{.docutils .literal .notranslate}]{.pre} can
    be not only an absolute URL, but also

    -   a relative URL

    -   a [[`Link`{.xref .py .py-class .docutils .literal
        .notranslate}]{.pre}](index.html#scrapy.link.Link &quot;scrapy.link.Link&quot;){.reference
        .internal} object, e.g. the result of [[Link
        Extractors]{.std
        .std-ref}](index.html#topics-link-extractors){.hoverxref
        .tooltip .reference .internal}

    -   a [[`Selector`{.xref .py .py-class .docutils .literal
        .notranslate}]{.pre}](index.html#scrapy.selector.Selector &quot;scrapy.selector.Selector&quot;){.reference
        .internal} object for a [`&lt;link&gt;`{.docutils .literal
        .notranslate}]{.pre} or [`&lt;a&gt;`{.docutils .literal
        .notranslate}]{.pre} element, e.g.
        [`response.css('a.my_link')[0]`{.docutils .literal
        .notranslate}]{.pre}

    -   an attribute [[`Selector`{.xref .py .py-class .docutils
        .literal
        .notranslate}]{.pre}](index.html#scrapy.selector.Selector &quot;scrapy.selector.Selector&quot;){.reference
        .internal} (not SelectorList), e.g.
        [`response.css('a::attr(href)')[0]`{.docutils .literal
        .notranslate}]{.pre} or
        [`response.xpath('//img/@src')[0]`{.docutils .literal
        .notranslate}]{.pre}

    See [[A shortcut for creating Requests]{.std
    .std-ref}](index.html#response-follow-example){.hoverxref
    .tooltip .reference .internal} for usage examples.

[[follow_all]{.pre}]{.sig-name .descname}[(]{.sig-paren}*[[urls]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[[[Optional]{.pre}](https://docs.python.org/3/library/typing.html#typing.Optional &quot;(in Python v3.12)&quot;){.reference .external}[[\[]{.pre}]{.p}[[Union]{.pre}](https://docs.python.org/3/library/typing.html#typing.Union &quot;(in Python v3.12)&quot;){.reference .external}[[\[]{.pre}]{.p}[[Iterable]{.pre}](https://docs.python.org/3/library/typing.html#typing.Iterable &quot;(in Python v3.12)&quot;){.reference .external}[[\[]{.pre}]{.p}[[Union]{.pre}](https://docs.python.org/3/library/typing.html#typing.Union &quot;(in Python v3.12)&quot;){.reference .external}[[\[]{.pre}]{.p}[[str]{.pre}](https://docs.python.org/3/library/stdtypes.html#str &quot;(in Python v3.12)&quot;){.reference .external}[[,]{.pre}]{.p}[ ]{.w}[[Link]{.pre}](index.html#scrapy.link.Link &quot;scrapy.link.Link&quot;){.reference .internal}[[\]]{.pre}]{.p}[[\]]{.pre}]{.p}[[,]{.pre}]{.p}[ ]{.w}[SelectorList]{.pre}[[\]]{.pre}]{.p}[[\]]{.pre}]{.p}]{.n}[ ]{.w}[[=]{.pre}]{.o}[ ]{.w}[[None]{.pre}]{.default_value}*, *[[callback]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[[[Optional]{.pre}](https://docs.python.org/3/library/typing.html#typing.Optional &quot;(in Python v3.12)&quot;){.reference .external}[[\[]{.pre}]{.p}[[Callable]{.pre}](https://docs.python.org/3/library/typing.html#typing.Callable &quot;(in Python v3.12)&quot;){.reference .external}[[\]]{.pre}]{.p}]{.n}[ ]{.w}[[=]{.pre}]{.o}[ ]{.w}[[None]{.pre}]{.default_value}*, *[[method]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[[[str]{.pre}](https://docs.python.org/3/library/stdtypes.html#str &quot;(in Python v3.12)&quot;){.reference .external}]{.n}[ ]{.w}[[=]{.pre}]{.o}[ ]{.w}[[\'GET\']{.pre}]{.default_value}*, *[[headers]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[[[Optional]{.pre}](https://docs.python.org/3/library/typing.html#typing.Optional &quot;(in Python v3.12)&quot;){.reference .external}[[\[]{.pre}]{.p}[[Union]{.pre}](https://docs.python.org/3/library/typing.html#typing.Union &quot;(in Python v3.12)&quot;){.reference .external}[[\[]{.pre}]{.p}[[Mapping]{.pre}](https://docs.python.org/3/library/typing.html#typing.Mapping &quot;(in Python v3.12)&quot;){.reference .external}[[\[]{.pre}]{.p}[AnyStr]{.pre}[[,]{.pre}]{.p}[ ]{.w}[[Any]{.pre}](https://docs.python.org/3/library/typing.html#typing.Any &quot;(in Python v3.12)&quot;){.reference .external}[[\]]{.pre}]{.p}[[,]{.pre}]{.p}[ ]{.w}[[Iterable]{.pre}](https://docs.python.org/3/library/typing.html#typing.Iterable &quot;(in Python v3.12)&quot;){.reference .external}[[\[]{.pre}]{.p}[[Tuple]{.pre}](https://docs.python.org/3/library/typing.html#typing.Tuple &quot;(in Python v3.12)&quot;){.reference .external}[[\[]{.pre}]{.p}[AnyStr]{.pre}[[,]{.pre}]{.p}[ ]{.w}[[Any]{.pre}](https://docs.python.org/3/library/typing.html#typing.Any &quot;(in Python v3.12)&quot;){.reference .external}[[\]]{.pre}]{.p}[[\]]{.pre}]{.p}[[\]]{.pre}]{.p}[[\]]{.pre}]{.p}]{.n}[ ]{.w}[[=]{.pre}]{.o}[ ]{.w}[[None]{.pre}]{.default_value}*, *[[body]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[[[Optional]{.pre}](https://docs.python.org/3/library/typing.html#typing.Optional &quot;(in Python v3.12)&quot;){.reference .external}[[\[]{.pre}]{.p}[[Union]{.pre}](https://docs.python.org/3/library/typing.html#typing.Union &quot;(in Python v3.12)&quot;){.reference .external}[[\[]{.pre}]{.p}[[bytes]{.pre}](https://docs.python.org/3/library/stdtypes.html#bytes &quot;(in Python v3.12)&quot;){.reference .external}[[,]{.pre}]{.p}[ ]{.w}[[str]{.pre}](https://docs.python.org/3/library/stdtypes.html#str &quot;(in Python v3.12)&quot;){.reference .external}[[\]]{.pre}]{.p}[[\]]{.pre}]{.p}]{.n}[ ]{.w}[[=]{.pre}]{.o}[ ]{.w}[[None]{.pre}]{.default_value}*, *[[cookies]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[[[Optional]{.pre}](https://docs.python.org/3/library/typing.html#typing.Optional &quot;(in Python v3.12)&quot;){.reference .external}[[\[]{.pre}]{.p}[[Union]{.pre}](https://docs.python.org/3/library/typing.html#typing.Union &quot;(in Python v3.12)&quot;){.reference .external}[[\[]{.pre}]{.p}[[dict]{.pre}](https://docs.python.org/3/library/stdtypes.html#dict &quot;(in Python v3.12)&quot;){.reference .external}[[,]{.pre}]{.p}[ ]{.w}[[List]{.pre}](https://docs.python.org/3/library/typing.html#typing.List &quot;(in Python v3.12)&quot;){.reference .external}[[\[]{.pre}]{.p}[[dict]{.pre}](https://docs.python.org/3/library/stdtypes.html#dict &quot;(in Python v3.12)&quot;){.reference .external}[[\]]{.pre}]{.p}[[\]]{.pre}]{.p}[[\]]{.pre}]{.p}]{.n}[ ]{.w}[[=]{.pre}]{.o}[ ]{.w}[[None]{.pre}]{.default_value}*, *[[meta]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[[[Optional]{.pre}](https://docs.python.org/3/library/typing.html#typing.Optional &quot;(in Python v3.12)&quot;){.reference .external}[[\[]{.pre}]{.p}[[Dict]{.pre}](https://docs.python.org/3/library/typing.html#typing.Dict &quot;(in Python v3.12)&quot;){.reference .external}[[\[]{.pre}]{.p}[[str]{.pre}](https://docs.python.org/3/library/stdtypes.html#str &quot;(in Python v3.12)&quot;){.reference .external}[[,]{.pre}]{.p}[ ]{.w}[[Any]{.pre}](https://docs.python.org/3/library/typing.html#typing.Any &quot;(in Python v3.12)&quot;){.reference .external}[[\]]{.pre}]{.p}[[\]]{.pre}]{.p}]{.n}[ ]{.w}[[=]{.pre}]{.o}[ ]{.w}[[None]{.pre}]{.default_value}*, *[[encoding]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[[[Optional]{.pre}](https://docs.python.org/3/library/typing.html#typing.Optional &quot;(in Python v3.12)&quot;){.reference .external}[[\[]{.pre}]{.p}[[str]{.pre}](https://docs.python.org/3/library/stdtypes.html#str &quot;(in Python v3.12)&quot;){.reference .external}[[\]]{.pre}]{.p}]{.n}[ ]{.w}[[=]{.pre}]{.o}[ ]{.w}[[None]{.pre}]{.default_value}*, *[[priority]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[[[int]{.pre}](https://docs.python.org/3/library/functions.html#int &quot;(in Python v3.12)&quot;){.reference .external}]{.n}[ ]{.w}[[=]{.pre}]{.o}[ ]{.w}[[0]{.pre}]{.default_value}*, *[[dont_filter]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[[[bool]{.pre}](https://docs.python.org/3/library/functions.html#bool &quot;(in Python v3.12)&quot;){.reference .external}]{.n}[ ]{.w}[[=]{.pre}]{.o}[ ]{.w}[[False]{.pre}]{.default_value}*, *[[errback]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[[[Optional]{.pre}](https://docs.python.org/3/library/typing.html#typing.Optional &quot;(in Python v3.12)&quot;){.reference .external}[[\[]{.pre}]{.p}[[Callable]{.pre}](https://docs.python.org/3/library/typing.html#typing.Callable &quot;(in Python v3.12)&quot;){.reference .external}[[\]]{.pre}]{.p}]{.n}[ ]{.w}[[=]{.pre}]{.o}[ ]{.w}[[None]{.pre}]{.default_value}*, *[[cb_kwargs]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[[[Optional]{.pre}](https://docs.python.org/3/library/typing.html#typing.Optional &quot;(in Python v3.12)&quot;){.reference .external}[[\[]{.pre}]{.p}[[Dict]{.pre}](https://docs.python.org/3/library/typing.html#typing.Dict &quot;(in Python v3.12)&quot;){.reference .external}[[\[]{.pre}]{.p}[[str]{.pre}](https://docs.python.org/3/library/stdtypes.html#str &quot;(in Python v3.12)&quot;){.reference .external}[[,]{.pre}]{.p}[ ]{.w}[[Any]{.pre}](https://docs.python.org/3/library/typing.html#typing.Any &quot;(in Python v3.12)&quot;){.reference .external}[[\]]{.pre}]{.p}[[\]]{.pre}]{.p}]{.n}[ ]{.w}[[=]{.pre}]{.o}[ ]{.w}[[None]{.pre}]{.default_value}*, *[[flags]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[[[Optional]{.pre}](https://docs.python.org/3/library/typing.html#typing.Optional &quot;(in Python v3.12)&quot;){.reference .external}[[\[]{.pre}]{.p}[[List]{.pre}](https://docs.python.org/3/library/typing.html#typing.List &quot;(in Python v3.12)&quot;){.reference .external}[[\[]{.pre}]{.p}[[str]{.pre}](https://docs.python.org/3/library/stdtypes.html#str &quot;(in Python v3.12)&quot;){.reference .external}[[\]]{.pre}]{.p}[[\]]{.pre}]{.p}]{.n}[ ]{.w}[[=]{.pre}]{.o}[ ]{.w}[[None]{.pre}]{.default_value}*, *[[css]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[[[Optional]{.pre}](https://docs.python.org/3/library/typing.html#typing.Optional &quot;(in Python v3.12)&quot;){.reference .external}[[\[]{.pre}]{.p}[[str]{.pre}](https://docs.python.org/3/library/stdtypes.html#str &quot;(in Python v3.12)&quot;){.reference .external}[[\]]{.pre}]{.p}]{.n}[ ]{.w}[[=]{.pre}]{.o}[ ]{.w}[[None]{.pre}]{.default_value}*, *[[xpath]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[[[Optional]{.pre}](https://docs.python.org/3/library/typing.html#typing.Optional &quot;(in Python v3.12)&quot;){.reference .external}[[\[]{.pre}]{.p}[[str]{.pre}](https://docs.python.org/3/library/stdtypes.html#str &quot;(in Python v3.12)&quot;){.reference .external}[[\]]{.pre}]{.p}]{.n}[ ]{.w}[[=]{.pre}]{.o}[ ]{.w}[[None]{.pre}]{.default_value}*[)]{.sig-paren} [[→]{.sig-return-icon} [[[Generator]{.pre}](https://docs.python.org/3/library/typing.html#typing.Generator &quot;(in Python v3.12)&quot;){.reference .external}[[\[]{.pre}]{.p}[[Request]{.pre}](index.html#scrapy.http.Request &quot;scrapy.http.request.Request&quot;){.reference .internal}[[,]{.pre}]{.p}[ ]{.w}[[None]{.pre}](https://docs.python.org/3/library/constants.html#None &quot;(in Python v3.12)&quot;){.reference .external}[[,]{.pre}]{.p}[ ]{.w}[[None]{.pre}](https://docs.python.org/3/library/constants.html#None &quot;(in Python v3.12)&quot;){.reference .external}[[\]]{.pre}]{.p}]{.sig-return-typehint}]{.sig-return}[[[\[source\]]{.pre}]{.viewcode-link}](_modules/scrapy/http/response/text.html#TextResponse.follow_all){.reference .internal}[¶](#scrapy.http.TextResponse.follow_all &quot;Permalink to this definition&quot;){.headerlink}

:   A generator that produces [[`Request`{.xref .py .py-class
    .docutils .literal
    .notranslate}]{.pre}](#scrapy.http.Request &quot;scrapy.http.Request&quot;){.reference
    .internal} instances to follow all links in [`urls`{.docutils
    .literal .notranslate}]{.pre}. It accepts the same arguments as
    the [[`Request`{.xref .py .py-class .docutils .literal
    .notranslate}]{.pre}](#scrapy.http.Request &quot;scrapy.http.Request&quot;){.reference
    .internal}'s [`__init__`{.docutils .literal .notranslate}]{.pre}
    method, except that each [`urls`{.docutils .literal
    .notranslate}]{.pre} element does not need to be an absolute
    URL, it can be any of the following:

    -   a relative URL

    -   a [[`Link`{.xref .py .py-class .docutils .literal
        .notranslate}]{.pre}](index.html#scrapy.link.Link &quot;scrapy.link.Link&quot;){.reference
        .internal} object, e.g. the result of [[Link
        Extractors]{.std
        .std-ref}](index.html#topics-link-extractors){.hoverxref
        .tooltip .reference .internal}

    -   a [[`Selector`{.xref .py .py-class .docutils .literal
        .notranslate}]{.pre}](index.html#scrapy.selector.Selector &quot;scrapy.selector.Selector&quot;){.reference
        .internal} object for a [`&lt;link&gt;`{.docutils .literal
        .notranslate}]{.pre} or [`&lt;a&gt;`{.docutils .literal
        .notranslate}]{.pre} element, e.g.
        [`response.css('a.my_link')[0]`{.docutils .literal
        .notranslate}]{.pre}

    -   an attribute [[`Selector`{.xref .py .py-class .docutils
        .literal
        .notranslate}]{.pre}](index.html#scrapy.selector.Selector &quot;scrapy.selector.Selector&quot;){.reference
        .internal} (not SelectorList), e.g.
        [`response.css('a::attr(href)')[0]`{.docutils .literal
        .notranslate}]{.pre} or
        [`response.xpath('//img/@src')[0]`{.docutils .literal
        .notranslate}]{.pre}

    In addition, [`css`{.docutils .literal .notranslate}]{.pre} and
    [`xpath`{.docutils .literal .notranslate}]{.pre} arguments are
    accepted to perform the link extraction within the
    [`follow_all`{.docutils .literal .notranslate}]{.pre} method
    (only one of [`urls`{.docutils .literal .notranslate}]{.pre},
    [`css`{.docutils .literal .notranslate}]{.pre} and
    [`xpath`{.docutils .literal .notranslate}]{.pre} is accepted).

    Note that when passing a [`SelectorList`{.docutils .literal
    .notranslate}]{.pre} as argument for the [`urls`{.docutils
    .literal .notranslate}]{.pre} parameter or using the
    [`css`{.docutils .literal .notranslate}]{.pre} or
    [`xpath`{.docutils .literal .notranslate}]{.pre} parameters,
    this method will not produce requests for selectors from which
    links cannot be obtained (for instance, anchor tags without an
    [`href`{.docutils .literal .notranslate}]{.pre} attribute)

[[json]{.pre}]{.sig-name .descname}[(]{.sig-paren}[)]{.sig-paren}[[[\[source\]]{.pre}]{.viewcode-link}](_modules/scrapy/http/response/text.html#TextResponse.json){.reference .internal}[¶](#scrapy.http.TextResponse.json &quot;Permalink to this definition&quot;){.headerlink}

:   ::: versionadded
    [New in version 2.2.]{.versionmodified .added}
    :::

    Deserialize a JSON document to a Python object.

    Returns a Python object from deserialized JSON document. The
    result is cached after the first call.

[[urljoin]{.pre}]{.sig-name .descname}[(]{.sig-paren}*[[url]{.pre}]{.n}*[)]{.sig-paren}[[[\[source\]]{.pre}]{.viewcode-link}](_modules/scrapy/http/response/text.html#TextResponse.urljoin){.reference .internal}[¶](#scrapy.http.TextResponse.urljoin &quot;Permalink to this definition&quot;){.headerlink}

:   Constructs an absolute url by combining the Response's base url
    with a possible relative url. The base url shall be extracted
    from the [`&lt;base&gt;`{.docutils .literal .notranslate}]{.pre} tag,
    or just the Response's [`url`{.xref .py .py-attr .docutils
    .literal .notranslate}]{.pre} if there is no such tag.
</code></pre>
<p>:::</p>
<p>::: {#htmlresponse-objects .section}</p>
<h5 id="htmlresponse-objectsheaderlink"><a class="header" href="#htmlresponse-objectsheaderlink">HtmlResponse objects<a href="#htmlresponse-objects" title="Permalink to this heading">¶</a>{.headerlink}</a></h5>
<p><em>[class]{.pre}[ ]{.w}</em>[[scrapy.http.]{.pre}]{.sig-prename .descclassname}[[HtmlResponse]{.pre}]{.sig-name .descname}[(]{.sig-paren}<em>[[url]{.pre}]{.n}</em>[[]{.optional}, <em>[[...]{.pre}]{.n}</em>[]]{.optional}[)]{.sig-paren}<a href="_modules/scrapy/http/response/html.html#HtmlResponse">[[[source]]{.pre}]{.viewcode-link}</a>{.reference .internal}<a href="#scrapy.http.HtmlResponse" title="Permalink to this definition">¶</a>{.headerlink}</p>
<p>:   The <a href="#scrapy.http.HtmlResponse" title="scrapy.http.HtmlResponse">[<code>HtmlResponse</code>{.xref .py .py-class .docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal} class is a subclass of <a href="#scrapy.http.TextResponse" title="scrapy.http.TextResponse">[<code>TextResponse</code>{.xref .py
.py-class .docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal} which adds encoding auto-discovering support by looking
into the HTML <a href="https://www.w3schools.com/TAGS/att_meta_http_equiv.asp">meta
http-equiv</a>{.reference
.external} attribute. See <a href="#scrapy.http.TextResponse.encoding" title="scrapy.http.TextResponse.encoding">[<code>TextResponse.encoding</code>{.xref .py
.py-attr .docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal}.
:::</p>
<p>::: {#xmlresponse-objects .section}</p>
<h5 id="xmlresponse-objectsheaderlink"><a class="header" href="#xmlresponse-objectsheaderlink">XmlResponse objects<a href="#xmlresponse-objects" title="Permalink to this heading">¶</a>{.headerlink}</a></h5>
<p><em>[class]{.pre}[ ]{.w}</em>[[scrapy.http.]{.pre}]{.sig-prename .descclassname}[[XmlResponse]{.pre}]{.sig-name .descname}[(]{.sig-paren}<em>[[url]{.pre}]{.n}</em>[[]{.optional}, <em>[[...]{.pre}]{.n}</em>[]]{.optional}[)]{.sig-paren}<a href="_modules/scrapy/http/response/xml.html#XmlResponse">[[[source]]{.pre}]{.viewcode-link}</a>{.reference .internal}<a href="#scrapy.http.XmlResponse" title="Permalink to this definition">¶</a>{.headerlink}</p>
<p>:   The <a href="#scrapy.http.XmlResponse" title="scrapy.http.XmlResponse">[<code>XmlResponse</code>{.xref .py .py-class .docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal} class is a subclass of <a href="#scrapy.http.TextResponse" title="scrapy.http.TextResponse">[<code>TextResponse</code>{.xref .py
.py-class .docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal} which adds encoding auto-discovering support by looking
into the XML declaration line. See <a href="#scrapy.http.TextResponse.encoding" title="scrapy.http.TextResponse.encoding">[<code>TextResponse.encoding</code>{.xref
.py .py-attr .docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal}.
:::
:::
:::</p>
<p>[]{#document-topics/link-extractors}</p>
<p>::: {#link-extractors .section}
[]{#topics-link-extractors}</p>
<h3 id="link-extractorsheaderlink"><a class="header" href="#link-extractorsheaderlink">Link Extractors<a href="#link-extractors" title="Permalink to this heading">¶</a>{.headerlink}</a></h3>
<p>A link extractor is an object that extracts links from responses.</p>
<p>The [<code>__init__</code>{.docutils .literal .notranslate}]{.pre} method of
<a href="#scrapy.linkextractors.lxmlhtml.LxmlLinkExtractor" title="scrapy.linkextractors.lxmlhtml.LxmlLinkExtractor">[<code>LxmlLinkExtractor</code>{.xref .py .py-class .docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal} takes settings that determine which links may be extracted.
<a href="#scrapy.linkextractors.lxmlhtml.LxmlLinkExtractor.extract_links" title="scrapy.linkextractors.lxmlhtml.LxmlLinkExtractor.extract_links">[<code>LxmlLinkExtractor.extract_links</code>{.xref .py .py-class .docutils
.literal
.notranslate}]{.pre}</a>{.reference
.internal} returns a list of matching <a href="#scrapy.link.Link" title="scrapy.link.Link">[<code>Link</code>{.xref .py .py-class
.docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal} objects from a <a href="index.html#scrapy.http.Response" title="scrapy.http.Response">[<code>Response</code>{.xref .py .py-class .docutils
.literal
.notranslate}]{.pre}</a>{.reference
.internal} object.</p>
<p>Link extractors are used in <a href="index.html#scrapy.spiders.CrawlSpider" title="scrapy.spiders.CrawlSpider">[<code>CrawlSpider</code>{.xref .py .py-class
.docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal} spiders through a set of <a href="index.html#scrapy.spiders.Rule" title="scrapy.spiders.Rule">[<code>Rule</code>{.xref .py .py-class
.docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal} objects.</p>
<p>You can also use link extractors in regular spiders. For example, you
can instantiate <a href="#scrapy.linkextractors.lxmlhtml.LxmlLinkExtractor" title="scrapy.linkextractors.lxmlhtml.LxmlLinkExtractor">[<code>LinkExtractor</code>{.xref .py .py-class .docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal} into a class variable in your spider, and use it from your
spider callbacks:</p>
<p>::: {.highlight-python .notranslate}
::: highlight
def parse(self, response):
for link in self.link_extractor.extract_links(response):
yield Request(link.url, callback=self.parse)
:::
:::</p>
<p>::: {#module-scrapy.linkextractors .section}
[]{#link-extractor-reference}[]{#topics-link-extractors-ref}</p>
<h4 id="link-extractor-referenceheaderlink"><a class="header" href="#link-extractor-referenceheaderlink">Link extractor reference<a href="#module-scrapy.linkextractors" title="Permalink to this heading">¶</a>{.headerlink}</a></h4>
<p>The link extractor class is
<a href="#scrapy.linkextractors.lxmlhtml.LxmlLinkExtractor" title="scrapy.linkextractors.lxmlhtml.LxmlLinkExtractor">[<code>scrapy.linkextractors.lxmlhtml.LxmlLinkExtractor</code>{.xref .py .py-class
.docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal}. For convenience it can also be imported as
[<code>scrapy.linkextractors.LinkExtractor</code>{.docutils .literal
.notranslate}]{.pre}:</p>
<p>::: {.highlight-default .notranslate}
::: highlight
from scrapy.linkextractors import LinkExtractor
:::
:::</p>
<p>::: {#module-scrapy.linkextractors.lxmlhtml .section}
[]{#lxmllinkextractor}</p>
<h5 id="lxmllinkextractorheaderlink"><a class="header" href="#lxmllinkextractorheaderlink">LxmlLinkExtractor<a href="#module-scrapy.linkextractors.lxmlhtml" title="Permalink to this heading">¶</a>{.headerlink}</a></h5>
<p><em>[class]{.pre}[ ]{.w}</em>[[scrapy.linkextractors.lxmlhtml.]{.pre}]{.sig-prename .descclassname}[[LxmlLinkExtractor]{.pre}]{.sig-name .descname}[(]{.sig-paren}<em>[[allow]{.pre}]{.n}[[=]{.pre}]{.o}[[()]{.pre}]{.default_value}</em>, <em>[[deny]{.pre}]{.n}[[=]{.pre}]{.o}[[()]{.pre}]{.default_value}</em>, <em>[[allow_domains]{.pre}]{.n}[[=]{.pre}]{.o}[[()]{.pre}]{.default_value}</em>, <em>[[deny_domains]{.pre}]{.n}[[=]{.pre}]{.o}[[()]{.pre}]{.default_value}</em>, <em>[[deny_extensions]{.pre}]{.n}[[=]{.pre}]{.o}[[None]{.pre}]{.default_value}</em>, <em>[[restrict_xpaths]{.pre}]{.n}[[=]{.pre}]{.o}[[()]{.pre}]{.default_value}</em>, <em>[[restrict_css]{.pre}]{.n}[[=]{.pre}]{.o}[[()]{.pre}]{.default_value}</em>, <em>[[tags]{.pre}]{.n}[[=]{.pre}]{.o}[[('a',]{.pre} ['area')]{.pre}]{.default_value}</em>, <em>[[attrs]{.pre}]{.n}[[=]{.pre}]{.o}[[('href',)]{.pre}]{.default_value}</em>, <em>[[canonicalize]{.pre}]{.n}[[=]{.pre}]{.o}[[False]{.pre}]{.default_value}</em>, <em>[[unique]{.pre}]{.n}[[=]{.pre}]{.o}[[True]{.pre}]{.default_value}</em>, <em>[[process_value]{.pre}]{.n}[[=]{.pre}]{.o}[[None]{.pre}]{.default_value}</em>, <em>[[strip]{.pre}]{.n}[[=]{.pre}]{.o}[[True]{.pre}]{.default_value}</em>[)]{.sig-paren}<a href="_modules/scrapy/linkextractors/lxmlhtml.html#LxmlLinkExtractor">[[[source]]{.pre}]{.viewcode-link}</a>{.reference .internal}<a href="#scrapy.linkextractors.lxmlhtml.LxmlLinkExtractor" title="Permalink to this definition">¶</a>{.headerlink}</p>
<p>:   LxmlLinkExtractor is the recommended link extractor with handy
filtering options. It is implemented using lxml's robust HTMLParser.</p>
<pre><code>Parameters

:   -   **allow**
        ([*str*](https://docs.python.org/3/library/stdtypes.html#str &quot;(in Python v3.12)&quot;){.reference
        .external} *or*
        [*list*](https://docs.python.org/3/library/stdtypes.html#list &quot;(in Python v3.12)&quot;){.reference
        .external}) -- a single regular expression (or list of
        regular expressions) that the (absolute) urls must match in
        order to be extracted. If not given (or empty), it will
        match all links.

    -   **deny**
        ([*str*](https://docs.python.org/3/library/stdtypes.html#str &quot;(in Python v3.12)&quot;){.reference
        .external} *or*
        [*list*](https://docs.python.org/3/library/stdtypes.html#list &quot;(in Python v3.12)&quot;){.reference
        .external}) -- a single regular expression (or list of
        regular expressions) that the (absolute) urls must match in
        order to be excluded (i.e. not extracted). It has precedence
        over the [`allow`{.docutils .literal .notranslate}]{.pre}
        parameter. If not given (or empty) it won't exclude any
        links.

    -   **allow_domains**
        ([*str*](https://docs.python.org/3/library/stdtypes.html#str &quot;(in Python v3.12)&quot;){.reference
        .external} *or*
        [*list*](https://docs.python.org/3/library/stdtypes.html#list &quot;(in Python v3.12)&quot;){.reference
        .external}) -- a single value or a list of string containing
        domains which will be considered for extracting the links

    -   **deny_domains**
        ([*str*](https://docs.python.org/3/library/stdtypes.html#str &quot;(in Python v3.12)&quot;){.reference
        .external} *or*
        [*list*](https://docs.python.org/3/library/stdtypes.html#list &quot;(in Python v3.12)&quot;){.reference
        .external}) -- a single value or a list of strings
        containing domains which won't be considered for extracting
        the links

    -   **deny_extensions**
        ([*list*](https://docs.python.org/3/library/stdtypes.html#list &quot;(in Python v3.12)&quot;){.reference
        .external}) --

        a single value or list of strings containing extensions that
        should be ignored when extracting links. If not given, it
        will default to
        [`scrapy.linkextractors.IGNORED_EXTENSIONS`{.xref .py
        .py-data .docutils .literal .notranslate}]{.pre}.

        ::: versionchanged
        [Changed in version 2.0: ]{.versionmodified
        .changed}[`IGNORED_EXTENSIONS`{.xref .py .py-data .docutils
        .literal .notranslate}]{.pre} now includes [`7z`{.docutils
        .literal .notranslate}]{.pre}, [`7zip`{.docutils .literal
        .notranslate}]{.pre}, [`apk`{.docutils .literal
        .notranslate}]{.pre}, [`bz2`{.docutils .literal
        .notranslate}]{.pre}, [`cdr`{.docutils .literal
        .notranslate}]{.pre}, [`dmg`{.docutils .literal
        .notranslate}]{.pre}, [`ico`{.docutils .literal
        .notranslate}]{.pre}, [`iso`{.docutils .literal
        .notranslate}]{.pre}, [`tar`{.docutils .literal
        .notranslate}]{.pre}, [`tar.gz`{.docutils .literal
        .notranslate}]{.pre}, [`webm`{.docutils .literal
        .notranslate}]{.pre}, and [`xz`{.docutils .literal
        .notranslate}]{.pre}.
        :::

    -   **restrict_xpaths**
        ([*str*](https://docs.python.org/3/library/stdtypes.html#str &quot;(in Python v3.12)&quot;){.reference
        .external} *or*
        [*list*](https://docs.python.org/3/library/stdtypes.html#list &quot;(in Python v3.12)&quot;){.reference
        .external}) -- is an XPath (or list of XPath's) which
        defines regions inside the response where links should be
        extracted from. If given, only the text selected by those
        XPath will be scanned for links. See examples below.

    -   **restrict_css**
        ([*str*](https://docs.python.org/3/library/stdtypes.html#str &quot;(in Python v3.12)&quot;){.reference
        .external} *or*
        [*list*](https://docs.python.org/3/library/stdtypes.html#list &quot;(in Python v3.12)&quot;){.reference
        .external}) -- a CSS selector (or list of selectors) which
        defines regions inside the response where links should be
        extracted from. Has the same behaviour as
        [`restrict_xpaths`{.docutils .literal .notranslate}]{.pre}.

    -   **restrict_text**
        ([*str*](https://docs.python.org/3/library/stdtypes.html#str &quot;(in Python v3.12)&quot;){.reference
        .external} *or*
        [*list*](https://docs.python.org/3/library/stdtypes.html#list &quot;(in Python v3.12)&quot;){.reference
        .external}) -- a single regular expression (or list of
        regular expressions) that the link's text must match in
        order to be extracted. If not given (or empty), it will
        match all links. If a list of regular expressions is given,
        the link will be extracted if it matches at least one.

    -   **tags**
        ([*str*](https://docs.python.org/3/library/stdtypes.html#str &quot;(in Python v3.12)&quot;){.reference
        .external} *or*
        [*list*](https://docs.python.org/3/library/stdtypes.html#list &quot;(in Python v3.12)&quot;){.reference
        .external}) -- a tag or a list of tags to consider when
        extracting links. Defaults to [`('a',`{.docutils .literal
        .notranslate}]{.pre}` `{.docutils .literal
        .notranslate}[`'area')`{.docutils .literal
        .notranslate}]{.pre}.

    -   **attrs**
        ([*list*](https://docs.python.org/3/library/stdtypes.html#list &quot;(in Python v3.12)&quot;){.reference
        .external}) -- an attribute or list of attributes which
        should be considered when looking for links to extract (only
        for those tags specified in the [`tags`{.docutils .literal
        .notranslate}]{.pre} parameter). Defaults to
        [`('href',)`{.docutils .literal .notranslate}]{.pre}

    -   **canonicalize**
        ([*bool*](https://docs.python.org/3/library/functions.html#bool &quot;(in Python v3.12)&quot;){.reference
        .external}) -- canonicalize each extracted url (using
        w3lib.url.canonicalize_url). Defaults to [`False`{.docutils
        .literal .notranslate}]{.pre}. Note that canonicalize_url is
        meant for duplicate checking; it can change the URL visible
        at server side, so the response can be different for
        requests with canonicalized and raw URLs. If you're using
        LinkExtractor to follow links it is more robust to keep the
        default [`canonicalize=False`{.docutils .literal
        .notranslate}]{.pre}.

    -   **unique**
        ([*bool*](https://docs.python.org/3/library/functions.html#bool &quot;(in Python v3.12)&quot;){.reference
        .external}) -- whether duplicate filtering should be applied
        to extracted links.

    -   **process_value**
        ([*collections.abc.Callable*](https://docs.python.org/3/library/collections.abc.html#collections.abc.Callable &quot;(in Python v3.12)&quot;){.reference
        .external}) --

        a function which receives each value extracted from the tag
        and attributes scanned and can modify the value and return a
        new one, or return [`None`{.docutils .literal
        .notranslate}]{.pre} to ignore the link altogether. If not
        given, [`process_value`{.docutils .literal
        .notranslate}]{.pre} defaults to [`lambda`{.docutils
        .literal .notranslate}]{.pre}` `{.docutils .literal
        .notranslate}[`x:`{.docutils .literal
        .notranslate}]{.pre}` `{.docutils .literal
        .notranslate}[`x`{.docutils .literal .notranslate}]{.pre}.

        For example, to extract links from this code:

        ::: {.highlight-html .notranslate}
        ::: highlight
            &lt;a href=&quot;javascript:goToPage('../other/page.html'); return false&quot;&gt;Link text&lt;/a&gt;
        :::
        :::

        You can use the following function in
        [`process_value`{.docutils .literal .notranslate}]{.pre}:

        ::: {.highlight-python .notranslate}
        ::: highlight
            def process_value(value):
                m = re.search(r&quot;javascript:goToPage\('(.*?)'&quot;, value)
                if m:
                    return m.group(1)
        :::
        :::

    -   **strip**
        ([*bool*](https://docs.python.org/3/library/functions.html#bool &quot;(in Python v3.12)&quot;){.reference
        .external}) -- whether to strip whitespaces from extracted
        attributes. According to HTML5 standard, leading and
        trailing whitespaces must be stripped from [`href`{.docutils
        .literal .notranslate}]{.pre} attributes of [`&lt;a&gt;`{.docutils
        .literal .notranslate}]{.pre}, [`&lt;area&gt;`{.docutils .literal
        .notranslate}]{.pre} and many other elements,
        [`src`{.docutils .literal .notranslate}]{.pre} attribute of
        [`&lt;img&gt;`{.docutils .literal .notranslate}]{.pre},
        [`&lt;iframe&gt;`{.docutils .literal .notranslate}]{.pre}
        elements, etc., so LinkExtractor strips space chars by
        default. Set [`strip=False`{.docutils .literal
        .notranslate}]{.pre} to turn it off (e.g. if you're
        extracting urls from elements or attributes which allow
        leading/trailing whitespaces).

[[extract_links]{.pre}]{.sig-name .descname}[(]{.sig-paren}*[[response]{.pre}]{.n}*[)]{.sig-paren}[[[\[source\]]{.pre}]{.viewcode-link}](_modules/scrapy/linkextractors/lxmlhtml.html#LxmlLinkExtractor.extract_links){.reference .internal}[¶](#scrapy.linkextractors.lxmlhtml.LxmlLinkExtractor.extract_links &quot;Permalink to this definition&quot;){.headerlink}

:   Returns a list of [[`Link`{.xref .py .py-class .docutils
    .literal
    .notranslate}]{.pre}](#scrapy.link.Link &quot;scrapy.link.Link&quot;){.reference
    .internal} objects from the specified [[`response`{.xref .py
    .py-class .docutils .literal
    .notranslate}]{.pre}](index.html#scrapy.http.Response &quot;scrapy.http.Response&quot;){.reference
    .internal}.

    Only links that match the settings passed to the
    [`__init__`{.docutils .literal .notranslate}]{.pre} method of
    the link extractor are returned.

    Duplicate links are omitted if the [`unique`{.docutils .literal
    .notranslate}]{.pre} attribute is set to [`True`{.docutils
    .literal .notranslate}]{.pre}, otherwise they are returned.
</code></pre>
<p>:::</p>
<p>::: {#module-scrapy.link .section}
[]{#link}</p>
<h5 id="linkheaderlink"><a class="header" href="#linkheaderlink">Link<a href="#module-scrapy.link" title="Permalink to this heading">¶</a>{.headerlink}</a></h5>
<p><em>[class]{.pre}[ ]{.w}</em>[[scrapy.link.]{.pre}]{.sig-prename .descclassname}[[Link]{.pre}]{.sig-name .descname}[(]{.sig-paren}<em>[[url]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[<a href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.12)">[str]{.pre}</a>{.reference .external}]{.n}</em>, <em>[[text]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[<a href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.12)">[str]{.pre}</a>{.reference .external}]{.n}[ ]{.w}[[=]{.pre}]{.o}[ ]{.w}[['']{.pre}]{.default_value}</em>, <em>[[fragment]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[<a href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.12)">[str]{.pre}</a>{.reference .external}]{.n}[ ]{.w}[[=]{.pre}]{.o}[ ]{.w}[['']{.pre}]{.default_value}</em>, <em>[[nofollow]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[<a href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.12)">[bool]{.pre}</a>{.reference .external}]{.n}[ ]{.w}[[=]{.pre}]{.o}[ ]{.w}[[False]{.pre}]{.default_value}</em>[)]{.sig-paren}<a href="_modules/scrapy/link.html#Link">[[[source]]{.pre}]{.viewcode-link}</a>{.reference .internal}<a href="#scrapy.link.Link" title="Permalink to this definition">¶</a>{.headerlink}</p>
<p>:   Link objects represent an extracted link by the LinkExtractor.</p>
<pre><code>Using the anchor tag sample below to illustrate the parameters:

::: {.highlight-python .notranslate}
::: highlight
    &lt;a href=&quot;https://example.com/nofollow.html#foo&quot; rel=&quot;nofollow&quot;&gt;Dont follow this one&lt;/a&gt;
:::
:::

Parameters

:   -   **url** -- the absolute url being linked to in the anchor
        tag. From the sample, this is
        [`https://example.com/nofollow.html`{.docutils .literal
        .notranslate}]{.pre}.

    -   **text** -- the text in the anchor tag. From the sample,
        this is [`Dont`{.docutils .literal
        .notranslate}]{.pre}` `{.docutils .literal
        .notranslate}[`follow`{.docutils .literal
        .notranslate}]{.pre}` `{.docutils .literal
        .notranslate}[`this`{.docutils .literal
        .notranslate}]{.pre}` `{.docutils .literal
        .notranslate}[`one`{.docutils .literal .notranslate}]{.pre}.

    -   **fragment** -- the part of the url after the hash symbol.
        From the sample, this is [`foo`{.docutils .literal
        .notranslate}]{.pre}.

    -   **nofollow** -- an indication of the presence or absence of
        a nofollow value in the [`rel`{.docutils .literal
        .notranslate}]{.pre} attribute of the anchor tag.
</code></pre>
<p>:::
:::
:::</p>
<p>[]{#document-topics/settings}</p>
<p>::: {#settings .section}
[]{#topics-settings}</p>
<h3 id="settingsheaderlink-2"><a class="header" href="#settingsheaderlink-2">Settings<a href="#settings" title="Permalink to this heading">¶</a>{.headerlink}</a></h3>
<p>The Scrapy settings allows you to customize the behaviour of all Scrapy
components, including the core, extensions, pipelines and spiders
themselves.</p>
<p>The infrastructure of the settings provides a global namespace of
key-value mappings that the code can use to pull configuration values
from. The settings can be populated through different mechanisms, which
are described below.</p>
<p>The settings are also the mechanism for selecting the currently active
Scrapy project (in case you have many).</p>
<p>For a list of available built-in settings see: <a href="#topics-settings-ref">[Built-in settings
reference]{.std .std-ref}</a>{.hoverxref .tooltip
.reference .internal}.</p>
<p>::: {#designating-the-settings .section}
[]{#topics-settings-module-envvar}</p>
<h4 id="designating-the-settingsheaderlink"><a class="header" href="#designating-the-settingsheaderlink">Designating the settings<a href="#designating-the-settings" title="Permalink to this heading">¶</a>{.headerlink}</a></h4>
<p>When you use Scrapy, you have to tell it which settings you're using.
You can do this by using an environment variable,
[<code>SCRAPY_SETTINGS_MODULE</code>{.docutils .literal .notranslate}]{.pre}.</p>
<p>The value of [<code>SCRAPY_SETTINGS_MODULE</code>{.docutils .literal
.notranslate}]{.pre} should be in Python path syntax, e.g.
[<code>myproject.settings</code>{.docutils .literal .notranslate}]{.pre}. Note that
the settings module should be on the Python <a href="https://docs.python.org/3/tutorial/modules.html#tut-searchpath" title="(in Python v3.12)">[import search path]{.xref
.std
.std-ref}</a>{.reference
.external}.
:::</p>
<p>::: {#populating-the-settings .section}
[]{#populating-settings}</p>
<h4 id="populating-the-settingsheaderlink"><a class="header" href="#populating-the-settingsheaderlink">Populating the settings<a href="#populating-the-settings" title="Permalink to this heading">¶</a>{.headerlink}</a></h4>
<p>Settings can be populated using different mechanisms, each of which
having a different precedence. Here is the list of them in decreasing
order of precedence:</p>
<blockquote>
<div>
<ol>
<li>
<p>Command line options (most precedence)</p>
</li>
<li>
<p>Settings per-spider</p>
</li>
<li>
<p>Project settings module</p>
</li>
<li>
<p>Settings set by add-ons</p>
</li>
<li>
<p>Default settings per-command</p>
</li>
<li>
<p>Default global settings (less precedence)</p>
</li>
</ol>
</div>
</blockquote>
<p>The population of these settings sources is taken care of internally,
but a manual handling is possible using API calls. See the <a href="index.html#topics-api-settings">[Settings
API]{.std .std-ref}</a>{.hoverxref .tooltip
.reference .internal} topic for reference.</p>
<p>These mechanisms are described in more detail below.</p>
<p>::: {#command-line-options .section}</p>
<h5 id="1-command-line-optionsheaderlink"><a class="header" href="#1-command-line-optionsheaderlink">1. Command line options<a href="#command-line-options" title="Permalink to this heading">¶</a>{.headerlink}</a></h5>
<p>Arguments provided by the command line are the ones that take most
precedence, overriding any other options. You can explicitly override
one (or more) settings using the [<code>-s</code>{.docutils .literal
.notranslate}]{.pre} (or [<code>--set</code>{.docutils .literal
.notranslate}]{.pre}) command line option.</p>
<p>Example:</p>
<p>::: {.highlight-sh .notranslate}
::: highlight
scrapy crawl myspider -s LOG_FILE=scrapy.log
:::
:::
:::</p>
<p>::: {#settings-per-spider .section}</p>
<h5 id="2-settings-per-spiderheaderlink"><a class="header" href="#2-settings-per-spiderheaderlink">2. Settings per-spider<a href="#settings-per-spider" title="Permalink to this heading">¶</a>{.headerlink}</a></h5>
<p>Spiders (See the <a href="index.html#topics-spiders">[Spiders]{.std
.std-ref}</a>{.hoverxref .tooltip .reference
.internal} chapter for reference) can define their own settings that
will take precedence and override the project ones. One way to do so is
by setting their <a href="index.html#scrapy.Spider.custom_settings" title="scrapy.Spider.custom_settings">[<code>custom_settings</code>{.xref .py .py-attr .docutils
.literal
.notranslate}]{.pre}</a>{.reference
.internal} attribute:</p>
<p>::: {.highlight-python .notranslate}
::: highlight
import scrapy</p>
<pre><code>class MySpider(scrapy.Spider):
    name = &quot;myspider&quot;

    custom_settings = {
        &quot;SOME_SETTING&quot;: &quot;some value&quot;,
    }
</code></pre>
<p>:::
:::</p>
<p>It's often better to implement <a href="index.html#scrapy.Spider.update_settings" title="scrapy.Spider.update_settings">[<code>update_settings()</code>{.xref .py .py-meth
.docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal} instead, and settings set there should use the &quot;spider&quot;
priority explicitly:</p>
<p>::: {.highlight-python .notranslate}
::: highlight
import scrapy</p>
<pre><code>class MySpider(scrapy.Spider):
    name = &quot;myspider&quot;

    @classmethod
    def update_settings(cls, settings):
        super().update_settings(settings)
        settings.set(&quot;SOME_SETTING&quot;, &quot;some value&quot;, priority=&quot;spider&quot;)
</code></pre>
<p>:::
:::</p>
<p>::: versionadded
[New in version 2.11.]{.versionmodified .added}
:::</p>
<p>It's also possible to modify the settings in the
<a href="index.html#scrapy.Spider.from_crawler" title="scrapy.Spider.from_crawler">[<code>from_crawler()</code>{.xref .py .py-meth .docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal} method, e.g. based on <a href="index.html#spiderargs">[spider arguments]{.std
.std-ref}</a>{.hoverxref .tooltip .reference
.internal} or other logic:</p>
<p>::: {.highlight-python .notranslate}
::: highlight
import scrapy</p>
<pre><code>class MySpider(scrapy.Spider):
    name = &quot;myspider&quot;

    @classmethod
    def from_crawler(cls, crawler, *args, **kwargs):
        spider = super().from_crawler(crawler, *args, **kwargs)
        if &quot;some_argument&quot; in kwargs:
            spider.settings.set(
                &quot;SOME_SETTING&quot;, kwargs[&quot;some_argument&quot;], priority=&quot;spider&quot;
            )
        return spider
</code></pre>
<p>:::
:::
:::</p>
<p>::: {#project-settings-module .section}</p>
<h5 id="3-project-settings-moduleheaderlink"><a class="header" href="#3-project-settings-moduleheaderlink">3. Project settings module<a href="#project-settings-module" title="Permalink to this heading">¶</a>{.headerlink}</a></h5>
<p>The project settings module is the standard configuration file for your
Scrapy project, it's where most of your custom settings will be
populated. For a standard Scrapy project, this means you'll be adding or
changing the settings in the [<code>settings.py</code>{.docutils .literal
.notranslate}]{.pre} file created for your project.
:::</p>
<p>::: {#settings-set-by-add-ons .section}</p>
<h5 id="4-settings-set-by-add-onsheaderlink"><a class="header" href="#4-settings-set-by-add-onsheaderlink">4. Settings set by add-ons<a href="#settings-set-by-add-ons" title="Permalink to this heading">¶</a>{.headerlink}</a></h5>
<p><a href="index.html#topics-addons">[Add-ons]{.std .std-ref}</a>{.hoverxref .tooltip
.reference .internal} can modify settings. They should do this with this
priority, though this is not enforced.
:::</p>
<p>::: {#default-settings-per-command .section}</p>
<h5 id="5-default-settings-per-commandheaderlink"><a class="header" href="#5-default-settings-per-commandheaderlink">5. Default settings per-command<a href="#default-settings-per-command" title="Permalink to this heading">¶</a>{.headerlink}</a></h5>
<p>Each <a href="index.html#document-topics/commands">[Scrapy
tool]{.doc}</a>{.reference .internal}
command can have its own default settings, which override the global
default settings. Those custom command settings are specified in the
[<code>default_settings</code>{.docutils .literal .notranslate}]{.pre} attribute of
the command class.
:::</p>
<p>::: {#default-global-settings .section}</p>
<h5 id="6-default-global-settingsheaderlink"><a class="header" href="#6-default-global-settingsheaderlink">6. Default global settings<a href="#default-global-settings" title="Permalink to this heading">¶</a>{.headerlink}</a></h5>
<p>The global defaults are located in the
[<code>scrapy.settings.default_settings</code>{.docutils .literal
.notranslate}]{.pre} module and documented in the <a href="#topics-settings-ref">[Built-in settings
reference]{.std .std-ref}</a>{.hoverxref .tooltip
.reference .internal} section.
:::
:::</p>
<p>::: {#compatibility-with-pickle .section}</p>
<h4 id="compatibility-with-pickleheaderlink"><a class="header" href="#compatibility-with-pickleheaderlink">Compatibility with pickle<a href="#compatibility-with-pickle" title="Permalink to this heading">¶</a>{.headerlink}</a></h4>
<p>Setting values must be <a href="https://docs.python.org/3/library/pickle.html#pickle-picklable" title="(in Python v3.12)">[picklable]{.xref .std
.std-ref}</a>{.reference
.external}.
:::</p>
<p>::: {#import-paths-and-classes .section}</p>
<h4 id="import-paths-and-classesheaderlink"><a class="header" href="#import-paths-and-classesheaderlink">Import paths and classes<a href="#import-paths-and-classes" title="Permalink to this heading">¶</a>{.headerlink}</a></h4>
<p>::: versionadded
[New in version 2.4.0.]{.versionmodified .added}
:::</p>
<p>When a setting references a callable object to be imported by Scrapy,
such as a class or a function, there are two different ways you can
specify that object:</p>
<ul>
<li>
<p>As a string containing the import path of that object</p>
</li>
<li>
<p>As the object itself</p>
</li>
</ul>
<p>For example:</p>
<p>::: {.highlight-python .notranslate}
::: highlight
from mybot.pipelines.validate import ValidateMyItem</p>
<pre><code>ITEM_PIPELINES = {
    # passing the classname...
    ValidateMyItem: 300,
    # ...equals passing the class path
    &quot;mybot.pipelines.validate.ValidateMyItem&quot;: 300,
}
</code></pre>
<p>:::
:::</p>
<p>::: {.admonition .note}
Note</p>
<p>Passing non-callable objects is not supported.
:::
:::</p>
<p>::: {#how-to-access-settings .section}</p>
<h4 id="how-to-access-settingsheaderlink"><a class="header" href="#how-to-access-settingsheaderlink">How to access settings<a href="#how-to-access-settings" title="Permalink to this heading">¶</a>{.headerlink}</a></h4>
<p>In a spider, the settings are available through
[<code>self.settings</code>{.docutils .literal .notranslate}]{.pre}:</p>
<p>::: {.highlight-python .notranslate}
::: highlight
class MySpider(scrapy.Spider):
name = &quot;myspider&quot;
start_urls = [&quot;http://example.com&quot;]</p>
<pre><code>    def parse(self, response):
        print(f&quot;Existing settings: {self.settings.attributes.keys()}&quot;)
</code></pre>
<p>:::
:::</p>
<p>::: {.admonition .note}
Note</p>
<p>The [<code>settings</code>{.docutils .literal .notranslate}]{.pre} attribute is set
in the base Spider class after the spider is initialized. If you want to
use the settings before the initialization (e.g., in your spider's
[<code>__init__()</code>{.docutils .literal .notranslate}]{.pre} method), you'll
need to override the <a href="index.html#scrapy.Spider.from_crawler" title="scrapy.Spider.from_crawler">[<code>from_crawler()</code>{.xref .py .py-meth .docutils
.literal
.notranslate}]{.pre}</a>{.reference
.internal} method.
:::</p>
<p>Settings can be accessed through the
<a href="index.html#scrapy.crawler.Crawler.settings" title="scrapy.crawler.Crawler.settings">[<code>scrapy.crawler.Crawler.settings</code>{.xref .py .py-attr .docutils
.literal
.notranslate}]{.pre}</a>{.reference
.internal} attribute of the Crawler that is passed to
[<code>from_crawler</code>{.docutils .literal .notranslate}]{.pre} method in
extensions, middlewares and item pipelines:</p>
<p>::: {.highlight-python .notranslate}
::: highlight
class MyExtension:
def <strong>init</strong>(self, log_is_enabled=False):
if log_is_enabled:
print(&quot;log is enabled!&quot;)</p>
<pre><code>    @classmethod
    def from_crawler(cls, crawler):
        settings = crawler.settings
        return cls(settings.getbool(&quot;LOG_ENABLED&quot;))
</code></pre>
<p>:::
:::</p>
<p>The settings object can be used like a dict (e.g.,
[<code>settings['LOG_ENABLED']</code>{.docutils .literal .notranslate}]{.pre}), but
it's usually preferred to extract the setting in the format you need it
to avoid type errors, using one of the methods provided by the
<a href="index.html#scrapy.settings.Settings" title="scrapy.settings.Settings">[<code>Settings</code>{.xref .py .py-class .docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal} API.
:::</p>
<p>::: {#rationale-for-setting-names .section}</p>
<h4 id="rationale-for-setting-namesheaderlink"><a class="header" href="#rationale-for-setting-namesheaderlink">Rationale for setting names<a href="#rationale-for-setting-names" title="Permalink to this heading">¶</a>{.headerlink}</a></h4>
<p>Setting names are usually prefixed with the component that they
configure. For example, proper setting names for a fictional robots.txt
extension would be [<code>ROBOTSTXT_ENABLED</code>{.docutils .literal
.notranslate}]{.pre}, [<code>ROBOTSTXT_OBEY</code>{.docutils .literal
.notranslate}]{.pre}, [<code>ROBOTSTXT_CACHEDIR</code>{.docutils .literal
.notranslate}]{.pre}, etc.
:::</p>
<p>::: {#built-in-settings-reference .section}
[]{#topics-settings-ref}</p>
<h4 id="built-in-settings-referenceheaderlink"><a class="header" href="#built-in-settings-referenceheaderlink">Built-in settings reference<a href="#built-in-settings-reference" title="Permalink to this heading">¶</a>{.headerlink}</a></h4>
<p>Here's a list of all available Scrapy settings, in alphabetical order,
along with their default values and the scope where they apply.</p>
<p>The scope, where available, shows where the setting is being used, if
it's tied to any particular component. In that case the module of that
component will be shown, typically an extension, middleware or pipeline.
It also means that the component must be enabled in order for the
setting to have any effect.</p>
<p>::: {#addons .section}
[]{#std-setting-ADDONS}</p>
<h5 id="addonsheaderlink"><a class="header" href="#addonsheaderlink">ADDONS<a href="#addons" title="Permalink to this heading">¶</a>{.headerlink}</a></h5>
<p>Default: [<code>{}</code>{.docutils .literal .notranslate}]{.pre}</p>
<p>A dict containing paths to the add-ons enabled in your project and their
priorities. For more information, see <a href="index.html#topics-addons">[Add-ons]{.std
.std-ref}</a>{.hoverxref .tooltip .reference
.internal}.
:::</p>
<p>::: {#aws-access-key-id .section}
[]{#std-setting-AWS_ACCESS_KEY_ID}</p>
<h5 id="aws_access_key_idheaderlink"><a class="header" href="#aws_access_key_idheaderlink">AWS_ACCESS_KEY_ID<a href="#aws-access-key-id" title="Permalink to this heading">¶</a>{.headerlink}</a></h5>
<p>Default: [<code>None</code>{.docutils .literal .notranslate}]{.pre}</p>
<p>The AWS access key used by code that requires access to <a href="https://aws.amazon.com/">Amazon Web
services</a>{.reference .external}, such as the
<a href="index.html#topics-feed-storage-s3">[S3 feed storage backend]{.std
.std-ref}</a>{.hoverxref .tooltip
.reference .internal}.
:::</p>
<p>::: {#aws-secret-access-key .section}
[]{#std-setting-AWS_SECRET_ACCESS_KEY}</p>
<h5 id="aws_secret_access_keyheaderlink"><a class="header" href="#aws_secret_access_keyheaderlink">AWS_SECRET_ACCESS_KEY<a href="#aws-secret-access-key" title="Permalink to this heading">¶</a>{.headerlink}</a></h5>
<p>Default: [<code>None</code>{.docutils .literal .notranslate}]{.pre}</p>
<p>The AWS secret key used by code that requires access to <a href="https://aws.amazon.com/">Amazon Web
services</a>{.reference .external}, such as the
<a href="index.html#topics-feed-storage-s3">[S3 feed storage backend]{.std
.std-ref}</a>{.hoverxref .tooltip
.reference .internal}.
:::</p>
<p>::: {#aws-session-token .section}
[]{#std-setting-AWS_SESSION_TOKEN}</p>
<h5 id="aws_session_tokenheaderlink"><a class="header" href="#aws_session_tokenheaderlink">AWS_SESSION_TOKEN<a href="#aws-session-token" title="Permalink to this heading">¶</a>{.headerlink}</a></h5>
<p>Default: [<code>None</code>{.docutils .literal .notranslate}]{.pre}</p>
<p>The AWS security token used by code that requires access to <a href="https://aws.amazon.com/">Amazon Web
services</a>{.reference .external}, such as the
<a href="index.html#topics-feed-storage-s3">[S3 feed storage backend]{.std
.std-ref}</a>{.hoverxref .tooltip
.reference .internal}, when using <a href="https://docs.aws.amazon.com/general/latest/gr/aws-sec-cred-types.html#temporary-access-keys">temporary security
credentials</a>{.reference
.external}.
:::</p>
<p>::: {#aws-endpoint-url .section}
[]{#std-setting-AWS_ENDPOINT_URL}</p>
<h5 id="aws_endpoint_urlheaderlink"><a class="header" href="#aws_endpoint_urlheaderlink">AWS_ENDPOINT_URL<a href="#aws-endpoint-url" title="Permalink to this heading">¶</a>{.headerlink}</a></h5>
<p>Default: [<code>None</code>{.docutils .literal .notranslate}]{.pre}</p>
<p>Endpoint URL used for S3-like storage, for example Minio or s3.scality.
:::</p>
<p>::: {#aws-use-ssl .section}
[]{#std-setting-AWS_USE_SSL}</p>
<h5 id="aws_use_sslheaderlink"><a class="header" href="#aws_use_sslheaderlink">AWS_USE_SSL<a href="#aws-use-ssl" title="Permalink to this heading">¶</a>{.headerlink}</a></h5>
<p>Default: [<code>None</code>{.docutils .literal .notranslate}]{.pre}</p>
<p>Use this option if you want to disable SSL connection for communication
with S3 or S3-like storage. By default SSL will be used.
:::</p>
<p>::: {#aws-verify .section}
[]{#std-setting-AWS_VERIFY}</p>
<h5 id="aws_verifyheaderlink"><a class="header" href="#aws_verifyheaderlink">AWS_VERIFY<a href="#aws-verify" title="Permalink to this heading">¶</a>{.headerlink}</a></h5>
<p>Default: [<code>None</code>{.docutils .literal .notranslate}]{.pre}</p>
<p>Verify SSL connection between Scrapy and S3 or S3-like storage. By
default SSL verification will occur.
:::</p>
<p>::: {#aws-region-name .section}
[]{#std-setting-AWS_REGION_NAME}</p>
<h5 id="aws_region_nameheaderlink"><a class="header" href="#aws_region_nameheaderlink">AWS_REGION_NAME<a href="#aws-region-name" title="Permalink to this heading">¶</a>{.headerlink}</a></h5>
<p>Default: [<code>None</code>{.docutils .literal .notranslate}]{.pre}</p>
<p>The name of the region associated with the AWS client.
:::</p>
<p>::: {#asyncio-event-loop .section}
[]{#std-setting-ASYNCIO_EVENT_LOOP}</p>
<h5 id="asyncio_event_loopheaderlink"><a class="header" href="#asyncio_event_loopheaderlink">ASYNCIO_EVENT_LOOP<a href="#asyncio-event-loop" title="Permalink to this heading">¶</a>{.headerlink}</a></h5>
<p>Default: [<code>None</code>{.docutils .literal .notranslate}]{.pre}</p>
<p>Import path of a given [<code>asyncio</code>{.docutils .literal
.notranslate}]{.pre} event loop class.</p>
<p>If the asyncio reactor is enabled (see <a href="#std-setting-TWISTED_REACTOR">[<code>TWISTED_REACTOR</code>{.xref .std
.std-setting .docutils .literal
.notranslate}]{.pre}</a>{.hoverxref .tooltip
.reference .internal}) this setting can be used to specify the asyncio
event loop to be used with it. Set the setting to the import path of the
desired asyncio event loop class. If the setting is set to
[<code>None</code>{.docutils .literal .notranslate}]{.pre} the default asyncio
event loop will be used.</p>
<p>If you are installing the asyncio reactor manually using the
<a href="#scrapy.utils.reactor.install_reactor" title="scrapy.utils.reactor.install_reactor">[<code>install_reactor()</code>{.xref .py .py-func .docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal} function, you can use the [<code>event_loop_path</code>{.docutils
.literal .notranslate}]{.pre} parameter to indicate the import path of
the event loop class to be used.</p>
<p>Note that the event loop class must inherit from
<a href="https://docs.python.org/3/library/asyncio-eventloop.html#asyncio.AbstractEventLoop" title="(in Python v3.12)">[<code>asyncio.AbstractEventLoop</code>{.xref .py .py-class .docutils .literal
.notranslate}]{.pre}</a>{.reference
.external}.</p>
<p>::: {.admonition .caution}
Caution</p>
<p>Please be aware that, when using a non-default event loop (either
defined via <a href="#std-setting-ASYNCIO_EVENT_LOOP">[<code>ASYNCIO_EVENT_LOOP</code>{.xref .std .std-setting .docutils
.literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal} or installed with
<a href="#scrapy.utils.reactor.install_reactor" title="scrapy.utils.reactor.install_reactor">[<code>install_reactor()</code>{.xref .py .py-func .docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal}), Scrapy will call <a href="https://docs.python.org/3/library/asyncio-eventloop.html#asyncio.set_event_loop" title="(in Python v3.12)">[<code>asyncio.set_event_loop()</code>{.xref .py
.py-func .docutils .literal
.notranslate}]{.pre}</a>{.reference
.external}, which will set the specified event loop as the current loop
for the current OS thread.
:::
:::</p>
<p>::: {#bot-name .section}
[]{#std-setting-BOT_NAME}</p>
<h5 id="bot_nameheaderlink"><a class="header" href="#bot_nameheaderlink">BOT_NAME<a href="#bot-name" title="Permalink to this heading">¶</a>{.headerlink}</a></h5>
<p>Default: [<code>'scrapybot'</code>{.docutils .literal .notranslate}]{.pre}</p>
<p>The name of the bot implemented by this Scrapy project (also known as
the project name). This name will be used for the logging too.</p>
<p>It's automatically populated with your project name when you create your
project with the <a href="index.html#std-command-startproject">[<code>startproject</code>{.xref .std .std-command .docutils
.literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal} command.
:::</p>
<p>::: {#concurrent-items .section}
[]{#std-setting-CONCURRENT_ITEMS}</p>
<h5 id="concurrent_itemsheaderlink"><a class="header" href="#concurrent_itemsheaderlink">CONCURRENT_ITEMS<a href="#concurrent-items" title="Permalink to this heading">¶</a>{.headerlink}</a></h5>
<p>Default: [<code>100</code>{.docutils .literal .notranslate}]{.pre}</p>
<p>Maximum number of concurrent items (per response) to process in parallel
in <a href="index.html#topics-item-pipeline">[item pipelines]{.std
.std-ref}</a>{.hoverxref .tooltip
.reference .internal}.
:::</p>
<p>::: {#concurrent-requests .section}
[]{#std-setting-CONCURRENT_REQUESTS}</p>
<h5 id="concurrent_requestsheaderlink"><a class="header" href="#concurrent_requestsheaderlink">CONCURRENT_REQUESTS<a href="#concurrent-requests" title="Permalink to this heading">¶</a>{.headerlink}</a></h5>
<p>Default: [<code>16</code>{.docutils .literal .notranslate}]{.pre}</p>
<p>The maximum number of concurrent (i.e. simultaneous) requests that will
be performed by the Scrapy downloader.
:::</p>
<p>::: {#concurrent-requests-per-domain .section}
[]{#std-setting-CONCURRENT_REQUESTS_PER_DOMAIN}</p>
<h5 id="concurrent_requests_per_domainheaderlink"><a class="header" href="#concurrent_requests_per_domainheaderlink">CONCURRENT_REQUESTS_PER_DOMAIN<a href="#concurrent-requests-per-domain" title="Permalink to this heading">¶</a>{.headerlink}</a></h5>
<p>Default: [<code>8</code>{.docutils .literal .notranslate}]{.pre}</p>
<p>The maximum number of concurrent (i.e. simultaneous) requests that will
be performed to any single domain.</p>
<p>See also: <a href="index.html#topics-autothrottle">[AutoThrottle extension]{.std
.std-ref}</a>{.hoverxref .tooltip
.reference .internal} and its <a href="index.html#std-setting-AUTOTHROTTLE_TARGET_CONCURRENCY">[<code>AUTOTHROTTLE_TARGET_CONCURRENCY</code>{.xref
.std .std-setting .docutils .literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal} option.
:::</p>
<p>::: {#concurrent-requests-per-ip .section}
[]{#std-setting-CONCURRENT_REQUESTS_PER_IP}</p>
<h5 id="concurrent_requests_per_ipheaderlink"><a class="header" href="#concurrent_requests_per_ipheaderlink">CONCURRENT_REQUESTS_PER_IP<a href="#concurrent-requests-per-ip" title="Permalink to this heading">¶</a>{.headerlink}</a></h5>
<p>Default: [<code>0</code>{.docutils .literal .notranslate}]{.pre}</p>
<p>The maximum number of concurrent (i.e. simultaneous) requests that will
be performed to any single IP. If non-zero, the
<a href="#std-setting-CONCURRENT_REQUESTS_PER_DOMAIN">[<code>CONCURRENT_REQUESTS_PER_DOMAIN</code>{.xref .std .std-setting .docutils
.literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal} setting is ignored, and this one is used
instead. In other words, concurrency limits will be applied per IP, not
per domain.</p>
<p>This setting also affects <a href="#std-setting-DOWNLOAD_DELAY">[<code>DOWNLOAD_DELAY</code>{.xref .std .std-setting
.docutils .literal
.notranslate}]{.pre}</a>{.hoverxref .tooltip
.reference .internal} and <a href="index.html#topics-autothrottle">[AutoThrottle extension]{.std
.std-ref}</a>{.hoverxref .tooltip
.reference .internal}: if <a href="#std-setting-CONCURRENT_REQUESTS_PER_IP">[<code>CONCURRENT_REQUESTS_PER_IP</code>{.xref .std
.std-setting .docutils .literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal} is non-zero, download delay is enforced
per IP, not per domain.
:::</p>
<p>::: {#default-item-class .section}
[]{#std-setting-DEFAULT_ITEM_CLASS}</p>
<h5 id="default_item_classheaderlink"><a class="header" href="#default_item_classheaderlink">DEFAULT_ITEM_CLASS<a href="#default-item-class" title="Permalink to this heading">¶</a>{.headerlink}</a></h5>
<p>Default: [<code>'scrapy.Item'</code>{.docutils .literal .notranslate}]{.pre}</p>
<p>The default class that will be used for instantiating items in the <a href="index.html#topics-shell">[the
Scrapy shell]{.std .std-ref}</a>{.hoverxref
.tooltip .reference .internal}.
:::</p>
<p>::: {#default-request-headers .section}
[]{#std-setting-DEFAULT_REQUEST_HEADERS}</p>
<h5 id="default_request_headersheaderlink"><a class="header" href="#default_request_headersheaderlink">DEFAULT_REQUEST_HEADERS<a href="#default-request-headers" title="Permalink to this heading">¶</a>{.headerlink}</a></h5>
<p>Default:</p>
<p>::: {.highlight-python .notranslate}
::: highlight
{
&quot;Accept&quot;: &quot;text/html,application/xhtml+xml,application/xml;q=0.9,<em>/</em>;q=0.8&quot;,
&quot;Accept-Language&quot;: &quot;en&quot;,
}
:::
:::</p>
<p>The default headers used for Scrapy HTTP Requests. They're populated in
the <a href="index.html#scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware" title="scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware">[<code>DefaultHeadersMiddleware</code>{.xref .py .py-class .docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal}.</p>
<p>::: {.admonition .caution}
Caution</p>
<p>Cookies set via the [<code>Cookie</code>{.docutils .literal .notranslate}]{.pre}
header are not considered by the <a href="index.html#cookies-mw">[CookiesMiddleware]{.std
.std-ref}</a>{.hoverxref .tooltip .reference
.internal}. If you need to set cookies for a request, use the
[<code>Request.cookies</code>{.xref .py .py-class .docutils .literal
.notranslate}]{.pre} parameter. This is a known current limitation that
is being worked on.
:::
:::</p>
<p>::: {#depth-limit .section}
[]{#std-setting-DEPTH_LIMIT}</p>
<h5 id="depth_limitheaderlink"><a class="header" href="#depth_limitheaderlink">DEPTH_LIMIT<a href="#depth-limit" title="Permalink to this heading">¶</a>{.headerlink}</a></h5>
<p>Default: [<code>0</code>{.docutils .literal .notranslate}]{.pre}</p>
<p>Scope: [<code>scrapy.spidermiddlewares.depth.DepthMiddleware</code>{.docutils
.literal .notranslate}]{.pre}</p>
<p>The maximum depth that will be allowed to crawl for any site. If zero,
no limit will be imposed.
:::</p>
<p>::: {#depth-priority .section}
[]{#std-setting-DEPTH_PRIORITY}</p>
<h5 id="depth_priorityheaderlink"><a class="header" href="#depth_priorityheaderlink">DEPTH_PRIORITY<a href="#depth-priority" title="Permalink to this heading">¶</a>{.headerlink}</a></h5>
<p>Default: [<code>0</code>{.docutils .literal .notranslate}]{.pre}</p>
<p>Scope: [<code>scrapy.spidermiddlewares.depth.DepthMiddleware</code>{.docutils
.literal .notranslate}]{.pre}</p>
<p>An integer that is used to adjust the [<code>priority</code>{.xref .py .py-attr
.docutils .literal .notranslate}]{.pre} of a [<code>Request</code>{.xref .py
.py-class .docutils .literal .notranslate}]{.pre} based on its depth.</p>
<p>The priority of a request is adjusted as follows:</p>
<p>::: {.highlight-python .notranslate}
::: highlight
request.priority = request.priority - (depth * DEPTH_PRIORITY)
:::
:::</p>
<p>As depth increases, positive values of [<code>DEPTH_PRIORITY</code>{.docutils
.literal .notranslate}]{.pre} decrease request priority (BFO), while
negative values increase request priority (DFO). See also <a href="index.html#faq-bfo-dfo">[Does Scrapy
crawl in breadth-first or depth-first order?]{.std
.std-ref}</a>{.hoverxref .tooltip .reference
.internal}.</p>
<p>::: {.admonition .note}
Note</p>
<p>This setting adjusts priority <strong>in the opposite way</strong> compared to other
priority settings <a href="#std-setting-REDIRECT_PRIORITY_ADJUST">[<code>REDIRECT_PRIORITY_ADJUST</code>{.xref .std .std-setting
.docutils .literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal} and <a href="index.html#std-setting-RETRY_PRIORITY_ADJUST">[<code>RETRY_PRIORITY_ADJUST</code>{.xref .std
.std-setting .docutils .literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal}.
:::
:::</p>
<p>::: {#depth-stats-verbose .section}
[]{#std-setting-DEPTH_STATS_VERBOSE}</p>
<h5 id="depth_stats_verboseheaderlink"><a class="header" href="#depth_stats_verboseheaderlink">DEPTH_STATS_VERBOSE<a href="#depth-stats-verbose" title="Permalink to this heading">¶</a>{.headerlink}</a></h5>
<p>Default: [<code>False</code>{.docutils .literal .notranslate}]{.pre}</p>
<p>Scope: [<code>scrapy.spidermiddlewares.depth.DepthMiddleware</code>{.docutils
.literal .notranslate}]{.pre}</p>
<p>Whether to collect verbose depth stats. If this is enabled, the number
of requests for each depth is collected in the stats.
:::</p>
<p>::: {#dnscache-enabled .section}
[]{#std-setting-DNSCACHE_ENABLED}</p>
<h5 id="dnscache_enabledheaderlink"><a class="header" href="#dnscache_enabledheaderlink">DNSCACHE_ENABLED<a href="#dnscache-enabled" title="Permalink to this heading">¶</a>{.headerlink}</a></h5>
<p>Default: [<code>True</code>{.docutils .literal .notranslate}]{.pre}</p>
<p>Whether to enable DNS in-memory cache.
:::</p>
<p>::: {#dnscache-size .section}
[]{#std-setting-DNSCACHE_SIZE}</p>
<h5 id="dnscache_sizeheaderlink"><a class="header" href="#dnscache_sizeheaderlink">DNSCACHE_SIZE<a href="#dnscache-size" title="Permalink to this heading">¶</a>{.headerlink}</a></h5>
<p>Default: [<code>10000</code>{.docutils .literal .notranslate}]{.pre}</p>
<p>DNS in-memory cache size.
:::</p>
<p>::: {#dns-resolver .section}
[]{#std-setting-DNS_RESOLVER}</p>
<h5 id="dns_resolverheaderlink"><a class="header" href="#dns_resolverheaderlink">DNS_RESOLVER<a href="#dns-resolver" title="Permalink to this heading">¶</a>{.headerlink}</a></h5>
<p>::: versionadded
[New in version 2.0.]{.versionmodified .added}
:::</p>
<p>Default: [<code>'scrapy.resolver.CachingThreadedResolver'</code>{.docutils .literal
.notranslate}]{.pre}</p>
<p>The class to be used to resolve DNS names. The default
[<code>scrapy.resolver.CachingThreadedResolver</code>{.docutils .literal
.notranslate}]{.pre} supports specifying a timeout for DNS requests via
the <a href="#std-setting-DNS_TIMEOUT">[<code>DNS_TIMEOUT</code>{.xref .std .std-setting .docutils .literal
.notranslate}]{.pre}</a>{.hoverxref .tooltip
.reference .internal} setting, but works only with IPv4 addresses.
Scrapy provides an alternative resolver,
[<code>scrapy.resolver.CachingHostnameResolver</code>{.docutils .literal
.notranslate}]{.pre}, which supports IPv4/IPv6 addresses but does not
take the <a href="#std-setting-DNS_TIMEOUT">[<code>DNS_TIMEOUT</code>{.xref .std .std-setting .docutils .literal
.notranslate}]{.pre}</a>{.hoverxref .tooltip
.reference .internal} setting into account.
:::</p>
<p>::: {#dns-timeout .section}
[]{#std-setting-DNS_TIMEOUT}</p>
<h5 id="dns_timeoutheaderlink"><a class="header" href="#dns_timeoutheaderlink">DNS_TIMEOUT<a href="#dns-timeout" title="Permalink to this heading">¶</a>{.headerlink}</a></h5>
<p>Default: [<code>60</code>{.docutils .literal .notranslate}]{.pre}</p>
<p>Timeout for processing of DNS queries in seconds. Float is supported.
:::</p>
<p>::: {#downloader .section}
[]{#std-setting-DOWNLOADER}</p>
<h5 id="downloaderheaderlink"><a class="header" href="#downloaderheaderlink">DOWNLOADER<a href="#downloader" title="Permalink to this heading">¶</a>{.headerlink}</a></h5>
<p>Default: [<code>'scrapy.core.downloader.Downloader'</code>{.docutils .literal
.notranslate}]{.pre}</p>
<p>The downloader to use for crawling.
:::</p>
<p>::: {#downloader-httpclientfactory .section}
[]{#std-setting-DOWNLOADER_HTTPCLIENTFACTORY}</p>
<h5 id="downloader_httpclientfactoryheaderlink"><a class="header" href="#downloader_httpclientfactoryheaderlink">DOWNLOADER_HTTPCLIENTFACTORY<a href="#downloader-httpclientfactory" title="Permalink to this heading">¶</a>{.headerlink}</a></h5>
<p>Default:
[<code>'scrapy.core.downloader.webclient.ScrapyHTTPClientFactory'</code>{.docutils
.literal .notranslate}]{.pre}</p>
<p>Defines a Twisted [<code>protocol.ClientFactory</code>{.docutils .literal
.notranslate}]{.pre} class to use for HTTP/1.0 connections (for
[<code>HTTP10DownloadHandler</code>{.docutils .literal .notranslate}]{.pre}).</p>
<p>::: {.admonition .note}
Note</p>
<p>HTTP/1.0 is rarely used nowadays so you can safely ignore this setting,
unless you really want to use HTTP/1.0 and override
<a href="#std-setting-DOWNLOAD_HANDLERS">[<code>DOWNLOAD_HANDLERS</code>{.xref .std .std-setting .docutils .literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal} for [<code>http(s)</code>{.docutils .literal
.notranslate}]{.pre} scheme accordingly, i.e. to
[<code>'scrapy.core.downloader.handlers.http.HTTP10DownloadHandler'</code>{.docutils
.literal .notranslate}]{.pre}.
:::
:::</p>
<p>::: {#downloader-clientcontextfactory .section}
[]{#std-setting-DOWNLOADER_CLIENTCONTEXTFACTORY}</p>
<h5 id="downloader_clientcontextfactoryheaderlink"><a class="header" href="#downloader_clientcontextfactoryheaderlink">DOWNLOADER_CLIENTCONTEXTFACTORY<a href="#downloader-clientcontextfactory" title="Permalink to this heading">¶</a>{.headerlink}</a></h5>
<p>Default:
[<code>'scrapy.core.downloader.contextfactory.ScrapyClientContextFactory'</code>{.docutils
.literal .notranslate}]{.pre}</p>
<p>Represents the classpath to the ContextFactory to use.</p>
<p>Here, &quot;ContextFactory&quot; is a Twisted term for SSL/TLS contexts, defining
the TLS/SSL protocol version to use, whether to do certificate
verification, or even enable client-side authentication (and various
other things).</p>
<p>::: {.admonition .note}
Note</p>
<p>Scrapy default context factory <strong>does NOT perform remote server
certificate verification</strong>. This is usually fine for web scraping.</p>
<p>If you do need remote server certificate verification enabled, Scrapy
also has another context factory class that you can set,
[<code>'scrapy.core.downloader.contextfactory.BrowserLikeContextFactory'</code>{.docutils
.literal .notranslate}]{.pre}, which uses the platform's certificates to
validate remote endpoints.
:::</p>
<p>If you do use a custom ContextFactory, make sure its
[<code>__init__</code>{.docutils .literal .notranslate}]{.pre} method accepts a
[<code>method</code>{.docutils .literal .notranslate}]{.pre} parameter (this is the
[<code>OpenSSL.SSL</code>{.docutils .literal .notranslate}]{.pre} method mapping
<a href="#std-setting-DOWNLOADER_CLIENT_TLS_METHOD">[<code>DOWNLOADER_CLIENT_TLS_METHOD</code>{.xref .std .std-setting .docutils
.literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal}), a [<code>tls_verbose_logging</code>{.docutils
.literal .notranslate}]{.pre} parameter ([<code>bool</code>{.docutils .literal
.notranslate}]{.pre}) and a [<code>tls_ciphers</code>{.docutils .literal
.notranslate}]{.pre} parameter (see
<a href="#std-setting-DOWNLOADER_CLIENT_TLS_CIPHERS">[<code>DOWNLOADER_CLIENT_TLS_CIPHERS</code>{.xref .std .std-setting .docutils
.literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal}).
:::</p>
<p>::: {#downloader-client-tls-ciphers .section}
[]{#std-setting-DOWNLOADER_CLIENT_TLS_CIPHERS}</p>
<h5 id="downloader_client_tls_ciphersheaderlink"><a class="header" href="#downloader_client_tls_ciphersheaderlink">DOWNLOADER_CLIENT_TLS_CIPHERS<a href="#downloader-client-tls-ciphers" title="Permalink to this heading">¶</a>{.headerlink}</a></h5>
<p>Default: [<code>'DEFAULT'</code>{.docutils .literal .notranslate}]{.pre}</p>
<p>Use this setting to customize the TLS/SSL ciphers used by the default
HTTP/1.1 downloader.</p>
<p>The setting should contain a string in the <a href="https://www.openssl.org/docs/manmaster/man1/openssl-ciphers.html#CIPHER-LIST-FORMAT">OpenSSL cipher list
format</a>{.reference
.external}, these ciphers will be used as client ciphers. Changing this
setting may be necessary to access certain HTTPS websites: for example,
you may need to use [<code>'DEFAULT:!DH'</code>{.docutils .literal
.notranslate}]{.pre} for a website with weak DH parameters or enable a
specific cipher that is not included in [<code>DEFAULT</code>{.docutils .literal
.notranslate}]{.pre} if a website requires it.
:::</p>
<p>::: {#downloader-client-tls-method .section}
[]{#std-setting-DOWNLOADER_CLIENT_TLS_METHOD}</p>
<h5 id="downloader_client_tls_methodheaderlink"><a class="header" href="#downloader_client_tls_methodheaderlink">DOWNLOADER_CLIENT_TLS_METHOD<a href="#downloader-client-tls-method" title="Permalink to this heading">¶</a>{.headerlink}</a></h5>
<p>Default: [<code>'TLS'</code>{.docutils .literal .notranslate}]{.pre}</p>
<p>Use this setting to customize the TLS/SSL method used by the default
HTTP/1.1 downloader.</p>
<p>This setting must be one of these string values:</p>
<ul>
<li>
<p>[<code>'TLS'</code>{.docutils .literal .notranslate}]{.pre}: maps to OpenSSL's
[<code>TLS_method()</code>{.docutils .literal .notranslate}]{.pre} (a.k.a
[<code>SSLv23_method()</code>{.docutils .literal .notranslate}]{.pre}), which
allows protocol negotiation, starting from the highest supported by
the platform; <strong>default, recommended</strong></p>
</li>
<li>
<p>[<code>'TLSv1.0'</code>{.docutils .literal .notranslate}]{.pre}: this value
forces HTTPS connections to use TLS version 1.0 ; set this if you
want the behavior of Scrapy&lt;1.1</p>
</li>
<li>
<p>[<code>'TLSv1.1'</code>{.docutils .literal .notranslate}]{.pre}: forces TLS
version 1.1</p>
</li>
<li>
<p>[<code>'TLSv1.2'</code>{.docutils .literal .notranslate}]{.pre}: forces TLS
version 1.2
:::</p>
</li>
</ul>
<p>::: {#downloader-client-tls-verbose-logging .section}
[]{#std-setting-DOWNLOADER_CLIENT_TLS_VERBOSE_LOGGING}</p>
<h5 id="downloader_client_tls_verbose_loggingheaderlink"><a class="header" href="#downloader_client_tls_verbose_loggingheaderlink">DOWNLOADER_CLIENT_TLS_VERBOSE_LOGGING<a href="#downloader-client-tls-verbose-logging" title="Permalink to this heading">¶</a>{.headerlink}</a></h5>
<p>Default: [<code>False</code>{.docutils .literal .notranslate}]{.pre}</p>
<p>Setting this to [<code>True</code>{.docutils .literal .notranslate}]{.pre} will
enable DEBUG level messages about TLS connection parameters after
establishing HTTPS connections. The kind of information logged depends
on the versions of OpenSSL and pyOpenSSL.</p>
<p>This setting is only used for the default
<a href="#std-setting-DOWNLOADER_CLIENTCONTEXTFACTORY">[<code>DOWNLOADER_CLIENTCONTEXTFACTORY</code>{.xref .std .std-setting .docutils
.literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal}.
:::</p>
<p>::: {#downloader-middlewares .section}
[]{#std-setting-DOWNLOADER_MIDDLEWARES}</p>
<h5 id="downloader_middlewaresheaderlink"><a class="header" href="#downloader_middlewaresheaderlink">DOWNLOADER_MIDDLEWARES<a href="#downloader-middlewares" title="Permalink to this heading">¶</a>{.headerlink}</a></h5>
<p>Default:: [<code>{}</code>{.docutils .literal .notranslate}]{.pre}</p>
<p>A dict containing the downloader middlewares enabled in your project,
and their orders. For more info see <a href="index.html#topics-downloader-middleware-setting">[Activating a downloader
middleware]{.std
.std-ref}</a>{.hoverxref
.tooltip .reference .internal}.
:::</p>
<p>::: {#downloader-middlewares-base .section}
[]{#std-setting-DOWNLOADER_MIDDLEWARES_BASE}</p>
<h5 id="downloader_middlewares_baseheaderlink"><a class="header" href="#downloader_middlewares_baseheaderlink">DOWNLOADER_MIDDLEWARES_BASE<a href="#downloader-middlewares-base" title="Permalink to this heading">¶</a>{.headerlink}</a></h5>
<p>Default:</p>
<p>::: {.highlight-python .notranslate}
::: highlight
{
&quot;scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware&quot;: 100,
&quot;scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware&quot;: 300,
&quot;scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware&quot;: 350,
&quot;scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware&quot;: 400,
&quot;scrapy.downloadermiddlewares.useragent.UserAgentMiddleware&quot;: 500,
&quot;scrapy.downloadermiddlewares.retry.RetryMiddleware&quot;: 550,
&quot;scrapy.downloadermiddlewares.ajaxcrawl.AjaxCrawlMiddleware&quot;: 560,
&quot;scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware&quot;: 580,
&quot;scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware&quot;: 590,
&quot;scrapy.downloadermiddlewares.redirect.RedirectMiddleware&quot;: 600,
&quot;scrapy.downloadermiddlewares.cookies.CookiesMiddleware&quot;: 700,
&quot;scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware&quot;: 750,
&quot;scrapy.downloadermiddlewares.stats.DownloaderStats&quot;: 850,
&quot;scrapy.downloadermiddlewares.httpcache.HttpCacheMiddleware&quot;: 900,
}
:::
:::</p>
<p>A dict containing the downloader middlewares enabled by default in
Scrapy. Low orders are closer to the engine, high orders are closer to
the downloader. You should never modify this setting in your project,
modify <a href="#std-setting-DOWNLOADER_MIDDLEWARES">[<code>DOWNLOADER_MIDDLEWARES</code>{.xref .std .std-setting .docutils
.literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal} instead. For more info see <a href="index.html#topics-downloader-middleware-setting">[Activating a
downloader middleware]{.std
.std-ref}</a>{.hoverxref
.tooltip .reference .internal}.
:::</p>
<p>::: {#downloader-stats .section}
[]{#std-setting-DOWNLOADER_STATS}</p>
<h5 id="downloader_statsheaderlink"><a class="header" href="#downloader_statsheaderlink">DOWNLOADER_STATS<a href="#downloader-stats" title="Permalink to this heading">¶</a>{.headerlink}</a></h5>
<p>Default: [<code>True</code>{.docutils .literal .notranslate}]{.pre}</p>
<p>Whether to enable downloader stats collection.
:::</p>
<p>::: {#download-delay .section}
[]{#std-setting-DOWNLOAD_DELAY}</p>
<h5 id="download_delayheaderlink"><a class="header" href="#download_delayheaderlink">DOWNLOAD_DELAY<a href="#download-delay" title="Permalink to this heading">¶</a>{.headerlink}</a></h5>
<p>Default: [<code>0</code>{.docutils .literal .notranslate}]{.pre}</p>
<p>Minimum seconds to wait between 2 consecutive requests to the same
domain.</p>
<p>Use <a href="#std-setting-DOWNLOAD_DELAY">[<code>DOWNLOAD_DELAY</code>{.xref .std .std-setting .docutils .literal
.notranslate}]{.pre}</a>{.hoverxref .tooltip
.reference .internal} to throttle your crawling speed, to avoid hitting
servers too hard.</p>
<p>Decimal numbers are supported. For example, to send a maximum of 4
requests every 10 seconds:</p>
<p>::: {.highlight-python .notranslate}
::: highlight
DOWNLOAD_DELAY = 2.5
:::
:::</p>
<p>This setting is also affected by the <a href="#std-setting-RANDOMIZE_DOWNLOAD_DELAY">[<code>RANDOMIZE_DOWNLOAD_DELAY</code>{.xref
.std .std-setting .docutils .literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal} setting, which is enabled by default.</p>
<p>When <a href="#std-setting-CONCURRENT_REQUESTS_PER_IP">[<code>CONCURRENT_REQUESTS_PER_IP</code>{.xref .std .std-setting .docutils
.literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal} is non-zero, delays are enforced per IP
address instead of per domain.</p>
<p>Note that <a href="#std-setting-DOWNLOAD_DELAY">[<code>DOWNLOAD_DELAY</code>{.xref .std .std-setting .docutils .literal
.notranslate}]{.pre}</a>{.hoverxref .tooltip
.reference .internal} can lower the effective per-domain concurrency
below <a href="#std-setting-CONCURRENT_REQUESTS_PER_DOMAIN">[<code>CONCURRENT_REQUESTS_PER_DOMAIN</code>{.xref .std .std-setting
.docutils .literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal}. If the response time of a domain is
lower than <a href="#std-setting-DOWNLOAD_DELAY">[<code>DOWNLOAD_DELAY</code>{.xref .std .std-setting .docutils .literal
.notranslate}]{.pre}</a>{.hoverxref .tooltip
.reference .internal}, the effective concurrency for that domain is 1.
When testing throttling configurations, it usually makes sense to lower
<a href="#std-setting-CONCURRENT_REQUESTS_PER_DOMAIN">[<code>CONCURRENT_REQUESTS_PER_DOMAIN</code>{.xref .std .std-setting .docutils
.literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal} first, and only increase
<a href="#std-setting-DOWNLOAD_DELAY">[<code>DOWNLOAD_DELAY</code>{.xref .std .std-setting .docutils .literal
.notranslate}]{.pre}</a>{.hoverxref .tooltip
.reference .internal} once <a href="#std-setting-CONCURRENT_REQUESTS_PER_DOMAIN">[<code>CONCURRENT_REQUESTS_PER_DOMAIN</code>{.xref .std
.std-setting .docutils .literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal} is 1 but a higher throttling is desired.</p>
<p>::: {#spider-download-delay-attribute .admonition .note}
Note</p>
<p>This delay can be set per spider using [<code>download_delay</code>{.xref .py
.py-attr .docutils .literal .notranslate}]{.pre} spider attribute.
:::</p>
<p>It is also possible to change this setting per domain, although it
requires non-trivial code. See the implementation of the
<a href="index.html#topics-autothrottle">[AutoThrottle]{.std
.std-ref}</a>{.hoverxref .tooltip
.reference .internal} extension for an example.
:::</p>
<p>::: {#download-handlers .section}
[]{#std-setting-DOWNLOAD_HANDLERS}</p>
<h5 id="download_handlersheaderlink"><a class="header" href="#download_handlersheaderlink">DOWNLOAD_HANDLERS<a href="#download-handlers" title="Permalink to this heading">¶</a>{.headerlink}</a></h5>
<p>Default: [<code>{}</code>{.docutils .literal .notranslate}]{.pre}</p>
<p>A dict containing the request downloader handlers enabled in your
project. See <a href="#std-setting-DOWNLOAD_HANDLERS_BASE">[<code>DOWNLOAD_HANDLERS_BASE</code>{.xref .std .std-setting
.docutils .literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal} for example format.
:::</p>
<p>::: {#download-handlers-base .section}
[]{#std-setting-DOWNLOAD_HANDLERS_BASE}</p>
<h5 id="download_handlers_baseheaderlink"><a class="header" href="#download_handlers_baseheaderlink">DOWNLOAD_HANDLERS_BASE<a href="#download-handlers-base" title="Permalink to this heading">¶</a>{.headerlink}</a></h5>
<p>Default:</p>
<p>::: {.highlight-python .notranslate}
::: highlight
{
&quot;data&quot;: &quot;scrapy.core.downloader.handlers.datauri.DataURIDownloadHandler&quot;,
&quot;file&quot;: &quot;scrapy.core.downloader.handlers.file.FileDownloadHandler&quot;,
&quot;http&quot;: &quot;scrapy.core.downloader.handlers.http.HTTPDownloadHandler&quot;,
&quot;https&quot;: &quot;scrapy.core.downloader.handlers.http.HTTPDownloadHandler&quot;,
&quot;s3&quot;: &quot;scrapy.core.downloader.handlers.s3.S3DownloadHandler&quot;,
&quot;ftp&quot;: &quot;scrapy.core.downloader.handlers.ftp.FTPDownloadHandler&quot;,
}
:::
:::</p>
<p>A dict containing the request download handlers enabled by default in
Scrapy. You should never modify this setting in your project, modify
<a href="#std-setting-DOWNLOAD_HANDLERS">[<code>DOWNLOAD_HANDLERS</code>{.xref .std .std-setting .docutils .literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal} instead.</p>
<p>You can disable any of these download handlers by assigning
[<code>None</code>{.docutils .literal .notranslate}]{.pre} to their URI scheme in
<a href="#std-setting-DOWNLOAD_HANDLERS">[<code>DOWNLOAD_HANDLERS</code>{.xref .std .std-setting .docutils .literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal}. E.g., to disable the built-in FTP
handler (without replacement), place this in your
[<code>settings.py</code>{.docutils .literal .notranslate}]{.pre}:</p>
<p>::: {.highlight-python .notranslate}
::: highlight
DOWNLOAD_HANDLERS = {
&quot;ftp&quot;: None,
}
:::
:::</p>
<p>The default HTTPS handler uses HTTP/1.1. To use HTTP/2:</p>
<ol>
<li>
<p>Install [<code>Twisted[http2]&gt;=17.9.0</code>{.docutils .literal
.notranslate}]{.pre} to install the packages required to enable
HTTP/2 support in Twisted.</p>
</li>
<li>
<p>Update <a href="#std-setting-DOWNLOAD_HANDLERS">[<code>DOWNLOAD_HANDLERS</code>{.xref .std .std-setting .docutils
.literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal} as follows:</p>
<p>::: {.highlight-python .notranslate}
::: highlight
DOWNLOAD_HANDLERS = {
&quot;https&quot;: &quot;scrapy.core.downloader.handlers.http2.H2DownloadHandler&quot;,
}
:::
:::</p>
</li>
</ol>
<p>::: {.admonition .warning}
Warning</p>
<p>HTTP/2 support in Scrapy is experimental, and not yet recommended for
production environments. Future Scrapy versions may introduce related
changes without a deprecation period or warning.
:::</p>
<p>::: {.admonition .note}
Note</p>
<p>Known limitations of the current HTTP/2 implementation of Scrapy
include:</p>
<ul>
<li>
<p>No support for HTTP/2 Cleartext (h2c), since no major browser
supports HTTP/2 unencrypted (refer <a href="https://http2.github.io/faq/#does-http2-require-encryption">http2
faq</a>{.reference
.external}).</p>
</li>
<li>
<p>No setting to specify a maximum <a href="https://tools.ietf.org/html/rfc7540#section-4.2">frame
size</a>{.reference
.external} larger than the default value, 16384. Connections to
servers that send a larger frame will fail.</p>
</li>
<li>
<p>No support for <a href="https://tools.ietf.org/html/rfc7540#section-8.2">server
pushes</a>{.reference
.external}, which are ignored.</p>
</li>
<li>
<p>No support for the <a href="index.html#std-signal-bytes_received">[<code>bytes_received</code>{.xref .std .std-signal
.docutils .literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal} and <a href="index.html#std-signal-headers_received">[<code>headers_received</code>{.xref .std
.std-signal .docutils .literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal} signals.
:::
:::</p>
</li>
</ul>
<p>::: {#download-slots .section}
[]{#std-setting-DOWNLOAD_SLOTS}</p>
<h5 id="download_slotsheaderlink"><a class="header" href="#download_slotsheaderlink">DOWNLOAD_SLOTS<a href="#download-slots" title="Permalink to this heading">¶</a>{.headerlink}</a></h5>
<p>Default: [<code>{}</code>{.docutils .literal .notranslate}]{.pre}</p>
<p>Allows to define concurrency/delay parameters on per slot (domain)
basis:</p>
<blockquote>
<div>
<p>::: {.highlight-python .notranslate}
::: highlight
DOWNLOAD_SLOTS = {
&quot;quotes.toscrape.com&quot;: {&quot;concurrency&quot;: 1, &quot;delay&quot;: 2, &quot;randomize_delay&quot;: False},
&quot;books.toscrape.com&quot;: {&quot;delay&quot;: 3, &quot;randomize_delay&quot;: False},
}
:::
:::</p>
</div>
</blockquote>
<p>::: {.admonition .note}
Note</p>
<p>For other downloader slots default settings values will be used:</p>
<ul>
<li>
<p><a href="#std-setting-DOWNLOAD_DELAY">[<code>DOWNLOAD_DELAY</code>{.xref .std .std-setting .docutils .literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal}: [<code>delay</code>{.docutils .literal
.notranslate}]{.pre}</p>
</li>
<li>
<p><a href="#std-setting-CONCURRENT_REQUESTS_PER_DOMAIN">[<code>CONCURRENT_REQUESTS_PER_DOMAIN</code>{.xref .std .std-setting .docutils
.literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal}: [<code>concurrency</code>{.docutils .literal
.notranslate}]{.pre}</p>
</li>
<li>
<p><a href="#std-setting-RANDOMIZE_DOWNLOAD_DELAY">[<code>RANDOMIZE_DOWNLOAD_DELAY</code>{.xref .std .std-setting .docutils
.literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal}: [<code>randomize_delay</code>{.docutils
.literal .notranslate}]{.pre}
:::
:::</p>
</li>
</ul>
<p>::: {#download-timeout .section}
[]{#std-setting-DOWNLOAD_TIMEOUT}</p>
<h5 id="download_timeoutheaderlink-1"><a class="header" href="#download_timeoutheaderlink-1">DOWNLOAD_TIMEOUT<a href="#download-timeout" title="Permalink to this heading">¶</a>{.headerlink}</a></h5>
<p>Default: [<code>180</code>{.docutils .literal .notranslate}]{.pre}</p>
<p>The amount of time (in secs) that the downloader will wait before timing
out.</p>
<p>::: {.admonition .note}
Note</p>
<p>This timeout can be set per spider using [<code>download_timeout</code>{.xref .py
.py-attr .docutils .literal .notranslate}]{.pre} spider attribute and
per-request using <a href="index.html#std-reqmeta-download_timeout">[<code>download_timeout</code>{.xref .std .std-reqmeta .docutils
.literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal} Request.meta key.
:::
:::</p>
<p>::: {#download-maxsize .section}
[]{#std-setting-DOWNLOAD_MAXSIZE}</p>
<h5 id="download_maxsizeheaderlink"><a class="header" href="#download_maxsizeheaderlink">DOWNLOAD_MAXSIZE<a href="#download-maxsize" title="Permalink to this heading">¶</a>{.headerlink}</a></h5>
<p>Default: [<code>1073741824</code>{.docutils .literal .notranslate}]{.pre} (1024MB)</p>
<p>The maximum response size (in bytes) that downloader will download.</p>
<p>If you want to disable it set to 0.</p>
<p>::: {#std-reqmeta-download_maxsize .admonition .note}
Note</p>
<p>This size can be set per spider using [<code>download_maxsize</code>{.xref .py
.py-attr .docutils .literal .notranslate}]{.pre} spider attribute and
per-request using <a href="#std-reqmeta-download_maxsize">[<code>download_maxsize</code>{.xref .std .std-reqmeta .docutils
.literal .notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal} Request.meta key.
:::
:::</p>
<p>::: {#download-warnsize .section}
[]{#std-setting-DOWNLOAD_WARNSIZE}</p>
<h5 id="download_warnsizeheaderlink"><a class="header" href="#download_warnsizeheaderlink">DOWNLOAD_WARNSIZE<a href="#download-warnsize" title="Permalink to this heading">¶</a>{.headerlink}</a></h5>
<p>Default: [<code>33554432</code>{.docutils .literal .notranslate}]{.pre} (32MB)</p>
<p>The response size (in bytes) that downloader will start to warn.</p>
<p>If you want to disable it set to 0.</p>
<p>::: {.admonition .note}
Note</p>
<p>This size can be set per spider using [<code>download_warnsize</code>{.xref .py
.py-attr .docutils .literal .notranslate}]{.pre} spider attribute and
per-request using [<code>download_warnsize</code>{.xref .std .std-reqmeta .docutils
.literal .notranslate}]{.pre} Request.meta key.
:::
:::</p>
<p>::: {#download-fail-on-dataloss .section}
[]{#std-setting-DOWNLOAD_FAIL_ON_DATALOSS}</p>
<h5 id="download_fail_on_datalossheaderlink-1"><a class="header" href="#download_fail_on_datalossheaderlink-1">DOWNLOAD_FAIL_ON_DATALOSS<a href="#download-fail-on-dataloss" title="Permalink to this heading">¶</a>{.headerlink}</a></h5>
<p>Default: [<code>True</code>{.docutils .literal .notranslate}]{.pre}</p>
<p>Whether or not to fail on broken responses, that is, declared
[<code>Content-Length</code>{.docutils .literal .notranslate}]{.pre} does not match
content sent by the server or chunked response was not properly finish.
If [<code>True</code>{.docutils .literal .notranslate}]{.pre}, these responses
raise a [<code>ResponseFailed([_DataLoss])</code>{.docutils .literal
.notranslate}]{.pre} error. If [<code>False</code>{.docutils .literal
.notranslate}]{.pre}, these responses are passed through and the flag
[<code>dataloss</code>{.docutils .literal .notranslate}]{.pre} is added to the
response, i.e.: [<code>'dataloss'</code>{.docutils .literal
.notranslate}]{.pre}<code> </code>{.docutils .literal .notranslate}[<code>in</code>{.docutils
.literal .notranslate}]{.pre}<code> </code>{.docutils .literal
.notranslate}[<code>response.flags</code>{.docutils .literal .notranslate}]{.pre}
is [<code>True</code>{.docutils .literal .notranslate}]{.pre}.</p>
<p>Optionally, this can be set per-request basis by using the
<a href="index.html#std-reqmeta-download_fail_on_dataloss">[<code>download_fail_on_dataloss</code>{.xref .std .std-reqmeta .docutils .literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal} Request.meta key to [<code>False</code>{.docutils
.literal .notranslate}]{.pre}.</p>
<p>::: {.admonition .note}
Note</p>
<p>A broken response, or data loss error, may happen under several
circumstances, from server misconfiguration to network errors to data
corruption. It is up to the user to decide if it makes sense to process
broken responses considering they may contain partial or incomplete
content. If <a href="index.html#std-setting-RETRY_ENABLED">[<code>RETRY_ENABLED</code>{.xref .std .std-setting .docutils .literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal} is [<code>True</code>{.docutils .literal
.notranslate}]{.pre} and this setting is set to [<code>True</code>{.docutils
.literal .notranslate}]{.pre}, the
[<code>ResponseFailed([_DataLoss])</code>{.docutils .literal .notranslate}]{.pre}
failure will be retried as usual.
:::</p>
<p>::: {.admonition .warning}
Warning</p>
<p>This setting is ignored by the [<code>H2DownloadHandler</code>{.xref .py .py-class
.docutils .literal .notranslate}]{.pre} download handler (see
<a href="#std-setting-DOWNLOAD_HANDLERS">[<code>DOWNLOAD_HANDLERS</code>{.xref .std .std-setting .docutils .literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal}). In case of a data loss error, the
corresponding HTTP/2 connection may be corrupted, affecting other
requests that use the same connection; hence, a
[<code>ResponseFailed([InvalidBodyLengthError])</code>{.docutils .literal
.notranslate}]{.pre} failure is always raised for every request that was
using that connection.
:::
:::</p>
<p>::: {#dupefilter-class .section}
[]{#std-setting-DUPEFILTER_CLASS}</p>
<h5 id="dupefilter_classheaderlink"><a class="header" href="#dupefilter_classheaderlink">DUPEFILTER_CLASS<a href="#dupefilter-class" title="Permalink to this heading">¶</a>{.headerlink}</a></h5>
<p>Default: [<code>'scrapy.dupefilters.RFPDupeFilter'</code>{.docutils .literal
.notranslate}]{.pre}</p>
<p>The class used to detect and filter duplicate requests.</p>
<p>The default ([<code>RFPDupeFilter</code>{.docutils .literal .notranslate}]{.pre})
filters based on the <a href="index.html#std-setting-REQUEST_FINGERPRINTER_CLASS">[<code>REQUEST_FINGERPRINTER_CLASS</code>{.xref .std
.std-setting .docutils .literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal} setting.</p>
<p>You can disable filtering of duplicate requests by setting
<a href="#std-setting-DUPEFILTER_CLASS">[<code>DUPEFILTER_CLASS</code>{.xref .std .std-setting .docutils .literal
.notranslate}]{.pre}</a>{.hoverxref .tooltip
.reference .internal} to
[<code>'scrapy.dupefilters.BaseDupeFilter'</code>{.docutils .literal
.notranslate}]{.pre}. Be very careful about this however, because you
can get into crawling loops. It's usually a better idea to set the
[<code>dont_filter</code>{.docutils .literal .notranslate}]{.pre} parameter to
[<code>True</code>{.docutils .literal .notranslate}]{.pre} on the specific
[<code>Request</code>{.xref .py .py-class .docutils .literal .notranslate}]{.pre}
that should not be filtered.
:::</p>
<p>::: {#dupefilter-debug .section}
[]{#std-setting-DUPEFILTER_DEBUG}</p>
<h5 id="dupefilter_debugheaderlink"><a class="header" href="#dupefilter_debugheaderlink">DUPEFILTER_DEBUG<a href="#dupefilter-debug" title="Permalink to this heading">¶</a>{.headerlink}</a></h5>
<p>Default: [<code>False</code>{.docutils .literal .notranslate}]{.pre}</p>
<p>By default, [<code>RFPDupeFilter</code>{.docutils .literal .notranslate}]{.pre}
only logs the first duplicate request. Setting
<a href="#std-setting-DUPEFILTER_DEBUG">[<code>DUPEFILTER_DEBUG</code>{.xref .std .std-setting .docutils .literal
.notranslate}]{.pre}</a>{.hoverxref .tooltip
.reference .internal} to [<code>True</code>{.docutils .literal .notranslate}]{.pre}
will make it log all duplicate requests.
:::</p>
<p>::: {#editor .section}
[]{#std-setting-EDITOR}</p>
<h5 id="editorheaderlink"><a class="header" href="#editorheaderlink">EDITOR<a href="#editor" title="Permalink to this heading">¶</a>{.headerlink}</a></h5>
<p>Default: [<code>vi</code>{.docutils .literal .notranslate}]{.pre} (on Unix systems)
or the IDLE editor (on Windows)</p>
<p>The editor to use for editing spiders with the <a href="index.html#std-command-edit">[<code>edit</code>{.xref .std
.std-command .docutils .literal
.notranslate}]{.pre}</a>{.hoverxref .tooltip
.reference .internal} command. Additionally, if the [<code>EDITOR</code>{.docutils
.literal .notranslate}]{.pre} environment variable is set, the
<a href="index.html#std-command-edit">[<code>edit</code>{.xref .std .std-command .docutils .literal
.notranslate}]{.pre}</a>{.hoverxref .tooltip
.reference .internal} command will prefer it over the default setting.
:::</p>
<p>::: {#extensions .section}
[]{#std-setting-EXTENSIONS}</p>
<h5 id="extensionsheaderlink"><a class="header" href="#extensionsheaderlink">EXTENSIONS<a href="#extensions" title="Permalink to this heading">¶</a>{.headerlink}</a></h5>
<p>Default:: [<code>{}</code>{.docutils .literal .notranslate}]{.pre}</p>
<p>A dict containing the extensions enabled in your project, and their
orders.
:::</p>
<p>::: {#extensions-base .section}
[]{#std-setting-EXTENSIONS_BASE}</p>
<h5 id="extensions_baseheaderlink"><a class="header" href="#extensions_baseheaderlink">EXTENSIONS_BASE<a href="#extensions-base" title="Permalink to this heading">¶</a>{.headerlink}</a></h5>
<p>Default:</p>
<p>::: {.highlight-python .notranslate}
::: highlight
{
&quot;scrapy.extensions.corestats.CoreStats&quot;: 0,
&quot;scrapy.extensions.telnet.TelnetConsole&quot;: 0,
&quot;scrapy.extensions.memusage.MemoryUsage&quot;: 0,
&quot;scrapy.extensions.memdebug.MemoryDebugger&quot;: 0,
&quot;scrapy.extensions.closespider.CloseSpider&quot;: 0,
&quot;scrapy.extensions.feedexport.FeedExporter&quot;: 0,
&quot;scrapy.extensions.logstats.LogStats&quot;: 0,
&quot;scrapy.extensions.spiderstate.SpiderState&quot;: 0,
&quot;scrapy.extensions.throttle.AutoThrottle&quot;: 0,
}
:::
:::</p>
<p>A dict containing the extensions available by default in Scrapy, and
their orders. This setting contains all stable built-in extensions. Keep
in mind that some of them need to be enabled through a setting.</p>
<p>For more information See the <a href="index.html#topics-extensions">[extensions user guide]{.std
.std-ref}</a>{.hoverxref .tooltip .reference
.internal} and the <a href="index.html#topics-extensions-ref">[list of available extensions]{.std
.std-ref}</a>{.hoverxref .tooltip
.reference .internal}.
:::</p>
<p>::: {#feed-tempdir .section}
[]{#std-setting-FEED_TEMPDIR}</p>
<h5 id="feed_tempdirheaderlink"><a class="header" href="#feed_tempdirheaderlink">FEED_TEMPDIR<a href="#feed-tempdir" title="Permalink to this heading">¶</a>{.headerlink}</a></h5>
<p>The Feed Temp dir allows you to set a custom folder to save crawler
temporary files before uploading with <a href="index.html#topics-feed-storage-ftp">[FTP feed storage]{.std
.std-ref}</a>{.hoverxref .tooltip
.reference .internal} and <a href="index.html#topics-feed-storage-s3">[Amazon S3]{.std
.std-ref}</a>{.hoverxref .tooltip
.reference .internal}.
:::</p>
<p>::: {#feed-storage-gcs-acl .section}
[]{#std-setting-FEED_STORAGE_GCS_ACL}</p>
<h5 id="feed_storage_gcs_aclheaderlink"><a class="header" href="#feed_storage_gcs_aclheaderlink">FEED_STORAGE_GCS_ACL<a href="#feed-storage-gcs-acl" title="Permalink to this heading">¶</a>{.headerlink}</a></h5>
<p>The Access Control List (ACL) used when storing items to <a href="index.html#topics-feed-storage-gcs">[Google Cloud
Storage]{.std .std-ref}</a>{.hoverxref
.tooltip .reference .internal}. For more information on how to set this
value, please refer to the column <em>JSON API</em> in <a href="https://cloud.google.com/storage/docs/access-control/lists">Google Cloud
documentation</a>{.reference
.external}.
:::</p>
<p>::: {#ftp-passive-mode .section}
[]{#std-setting-FTP_PASSIVE_MODE}</p>
<h5 id="ftp_passive_modeheaderlink"><a class="header" href="#ftp_passive_modeheaderlink">FTP_PASSIVE_MODE<a href="#ftp-passive-mode" title="Permalink to this heading">¶</a>{.headerlink}</a></h5>
<p>Default: [<code>True</code>{.docutils .literal .notranslate}]{.pre}</p>
<p>Whether or not to use passive mode when initiating FTP transfers.</p>
<p>[]{#std-reqmeta-ftp_password .target}
:::</p>
<p>::: {#ftp-password .section}
[]{#std-setting-FTP_PASSWORD}</p>
<h5 id="ftp_passwordheaderlink"><a class="header" href="#ftp_passwordheaderlink">FTP_PASSWORD<a href="#ftp-password" title="Permalink to this heading">¶</a>{.headerlink}</a></h5>
<p>Default: [<code>&quot;guest&quot;</code>{.docutils .literal .notranslate}]{.pre}</p>
<p>The password to use for FTP connections when there is no
[<code>&quot;ftp_password&quot;</code>{.docutils .literal .notranslate}]{.pre} in
[<code>Request</code>{.docutils .literal .notranslate}]{.pre} meta.</p>
<p>::: {.admonition .note}
Note</p>
<p>Paraphrasing <a href="https://tools.ietf.org/html/rfc1635">RFC 1635</a>{.reference
.external}, although it is common to use either the password &quot;guest&quot; or
one's e-mail address for anonymous FTP, some FTP servers explicitly ask
for the user's e-mail address and will not allow login with the &quot;guest&quot;
password.
:::</p>
<p>[]{#std-reqmeta-ftp_user .target}
:::</p>
<p>::: {#ftp-user .section}
[]{#std-setting-FTP_USER}</p>
<h5 id="ftp_userheaderlink"><a class="header" href="#ftp_userheaderlink">FTP_USER<a href="#ftp-user" title="Permalink to this heading">¶</a>{.headerlink}</a></h5>
<p>Default: [<code>&quot;anonymous&quot;</code>{.docutils .literal .notranslate}]{.pre}</p>
<p>The username to use for FTP connections when there is no
[<code>&quot;ftp_user&quot;</code>{.docutils .literal .notranslate}]{.pre} in
[<code>Request</code>{.docutils .literal .notranslate}]{.pre} meta.
:::</p>
<p>::: {#gcs-project-id .section}
[]{#std-setting-GCS_PROJECT_ID}</p>
<h5 id="gcs_project_idheaderlink"><a class="header" href="#gcs_project_idheaderlink">GCS_PROJECT_ID<a href="#gcs-project-id" title="Permalink to this heading">¶</a>{.headerlink}</a></h5>
<p>Default: [<code>None</code>{.docutils .literal .notranslate}]{.pre}</p>
<p>The Project ID that will be used when storing data on <a href="https://cloud.google.com/storage/">Google Cloud
Storage</a>{.reference .external}.
:::</p>
<p>::: {#item-pipelines .section}
[]{#std-setting-ITEM_PIPELINES}</p>
<h5 id="item_pipelinesheaderlink"><a class="header" href="#item_pipelinesheaderlink">ITEM_PIPELINES<a href="#item-pipelines" title="Permalink to this heading">¶</a>{.headerlink}</a></h5>
<p>Default: [<code>{}</code>{.docutils .literal .notranslate}]{.pre}</p>
<p>A dict containing the item pipelines to use, and their orders. Order
values are arbitrary, but it is customary to define them in the 0-1000
range. Lower orders process before higher orders.</p>
<p>Example:</p>
<p>::: {.highlight-python .notranslate}
::: highlight
ITEM_PIPELINES = {
&quot;mybot.pipelines.validate.ValidateMyItem&quot;: 300,
&quot;mybot.pipelines.validate.StoreMyItem&quot;: 800,
}
:::
:::
:::</p>
<p>::: {#item-pipelines-base .section}
[]{#std-setting-ITEM_PIPELINES_BASE}</p>
<h5 id="item_pipelines_baseheaderlink"><a class="header" href="#item_pipelines_baseheaderlink">ITEM_PIPELINES_BASE<a href="#item-pipelines-base" title="Permalink to this heading">¶</a>{.headerlink}</a></h5>
<p>Default: [<code>{}</code>{.docutils .literal .notranslate}]{.pre}</p>
<p>A dict containing the pipelines enabled by default in Scrapy. You should
never modify this setting in your project, modify
<a href="#std-setting-ITEM_PIPELINES">[<code>ITEM_PIPELINES</code>{.xref .std .std-setting .docutils .literal
.notranslate}]{.pre}</a>{.hoverxref .tooltip
.reference .internal} instead.
:::</p>
<p>::: {#jobdir .section}
[]{#std-setting-JOBDIR}</p>
<h5 id="jobdirheaderlink"><a class="header" href="#jobdirheaderlink">JOBDIR<a href="#jobdir" title="Permalink to this heading">¶</a>{.headerlink}</a></h5>
<p>Default: [<code>None</code>{.docutils .literal .notranslate}]{.pre}</p>
<p>A string indicating the directory for storing the state of a crawl when
<a href="index.html#topics-jobs">[pausing and resuming crawls]{.std
.std-ref}</a>{.hoverxref .tooltip .reference
.internal}.
:::</p>
<p>::: {#log-enabled .section}
[]{#std-setting-LOG_ENABLED}</p>
<h5 id="log_enabledheaderlink"><a class="header" href="#log_enabledheaderlink">LOG_ENABLED<a href="#log-enabled" title="Permalink to this heading">¶</a>{.headerlink}</a></h5>
<p>Default: [<code>True</code>{.docutils .literal .notranslate}]{.pre}</p>
<p>Whether to enable logging.
:::</p>
<p>::: {#log-encoding .section}
[]{#std-setting-LOG_ENCODING}</p>
<h5 id="log_encodingheaderlink"><a class="header" href="#log_encodingheaderlink">LOG_ENCODING<a href="#log-encoding" title="Permalink to this heading">¶</a>{.headerlink}</a></h5>
<p>Default: [<code>'utf-8'</code>{.docutils .literal .notranslate}]{.pre}</p>
<p>The encoding to use for logging.
:::</p>
<p>::: {#log-file .section}
[]{#std-setting-LOG_FILE}</p>
<h5 id="log_fileheaderlink"><a class="header" href="#log_fileheaderlink">LOG_FILE<a href="#log-file" title="Permalink to this heading">¶</a>{.headerlink}</a></h5>
<p>Default: [<code>None</code>{.docutils .literal .notranslate}]{.pre}</p>
<p>File name to use for logging output. If [<code>None</code>{.docutils .literal
.notranslate}]{.pre}, standard error will be used.
:::</p>
<p>::: {#log-file-append .section}
[]{#std-setting-LOG_FILE_APPEND}</p>
<h5 id="log_file_appendheaderlink"><a class="header" href="#log_file_appendheaderlink">LOG_FILE_APPEND<a href="#log-file-append" title="Permalink to this heading">¶</a>{.headerlink}</a></h5>
<p>Default: [<code>True</code>{.docutils .literal .notranslate}]{.pre}</p>
<p>If [<code>False</code>{.docutils .literal .notranslate}]{.pre}, the log file
specified with <a href="#std-setting-LOG_FILE">[<code>LOG_FILE</code>{.xref .std .std-setting .docutils .literal
.notranslate}]{.pre}</a>{.hoverxref .tooltip
.reference .internal} will be overwritten (discarding the output from
previous runs, if any).
:::</p>
<p>::: {#log-format .section}
[]{#std-setting-LOG_FORMAT}</p>
<h5 id="log_formatheaderlink"><a class="header" href="#log_formatheaderlink">LOG_FORMAT<a href="#log-format" title="Permalink to this heading">¶</a>{.headerlink}</a></h5>
<p>Default: [<code>'%(asctime)s</code>{.docutils .literal
.notranslate}]{.pre}<code> </code>{.docutils .literal
.notranslate}[<code>[%(name)s]</code>{.docutils .literal
.notranslate}]{.pre}<code> </code>{.docutils .literal
.notranslate}[<code>%(levelname)s:</code>{.docutils .literal
.notranslate}]{.pre}<code> </code>{.docutils .literal
.notranslate}[<code>%(message)s'</code>{.docutils .literal .notranslate}]{.pre}</p>
<p>String for formatting log messages. Refer to the <a href="https://docs.python.org/3/library/logging.html#logrecord-attributes" title="(in Python v3.12)">[Python logging
documentation]{.xref .std
.std-ref}</a>{.reference
.external} for the whole list of available placeholders.
:::</p>
<p>::: {#log-dateformat .section}
[]{#std-setting-LOG_DATEFORMAT}</p>
<h5 id="log_dateformatheaderlink"><a class="header" href="#log_dateformatheaderlink">LOG_DATEFORMAT<a href="#log-dateformat" title="Permalink to this heading">¶</a>{.headerlink}</a></h5>
<p>Default: [<code>'%Y-%m-%d</code>{.docutils .literal
.notranslate}]{.pre}<code> </code>{.docutils .literal
.notranslate}[<code>%H:%M:%S'</code>{.docutils .literal .notranslate}]{.pre}</p>
<p>String for formatting date/time, expansion of the
[<code>%(asctime)s</code>{.docutils .literal .notranslate}]{.pre} placeholder in
<a href="#std-setting-LOG_FORMAT">[<code>LOG_FORMAT</code>{.xref .std .std-setting .docutils .literal
.notranslate}]{.pre}</a>{.hoverxref .tooltip
.reference .internal}. Refer to the <a href="https://docs.python.org/3/library/datetime.html#strftime-strptime-behavior" title="(in Python v3.12)">[Python datetime
documentation]{.xref .std
.std-ref}</a>{.reference
.external} for the whole list of available directives.
:::</p>
<p>::: {#log-formatter .section}
[]{#std-setting-LOG_FORMATTER}</p>
<h5 id="log_formatterheaderlink"><a class="header" href="#log_formatterheaderlink">LOG_FORMATTER<a href="#log-formatter" title="Permalink to this heading">¶</a>{.headerlink}</a></h5>
<p>Default: <a href="index.html#scrapy.logformatter.LogFormatter" title="scrapy.logformatter.LogFormatter">[<code>scrapy.logformatter.LogFormatter</code>{.xref .py .py-class
.docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal}</p>
<p>The class to use for <a href="index.html#custom-log-formats">[formatting log messages]{.std
.std-ref}</a>{.hoverxref .tooltip .reference
.internal} for different actions.
:::</p>
<p>::: {#log-level .section}
[]{#std-setting-LOG_LEVEL}</p>
<h5 id="log_levelheaderlink"><a class="header" href="#log_levelheaderlink">LOG_LEVEL<a href="#log-level" title="Permalink to this heading">¶</a>{.headerlink}</a></h5>
<p>Default: [<code>'DEBUG'</code>{.docutils .literal .notranslate}]{.pre}</p>
<p>Minimum level to log. Available levels are: CRITICAL, ERROR, WARNING,
INFO, DEBUG. For more info see <a href="index.html#topics-logging">[Logging]{.std
.std-ref}</a>{.hoverxref .tooltip .reference
.internal}.
:::</p>
<p>::: {#log-stdout .section}
[]{#std-setting-LOG_STDOUT}</p>
<h5 id="log_stdoutheaderlink"><a class="header" href="#log_stdoutheaderlink">LOG_STDOUT<a href="#log-stdout" title="Permalink to this heading">¶</a>{.headerlink}</a></h5>
<p>Default: [<code>False</code>{.docutils .literal .notranslate}]{.pre}</p>
<p>If [<code>True</code>{.docutils .literal .notranslate}]{.pre}, all standard output
(and error) of your process will be redirected to the log. For example
if you [<code>print('hello')</code>{.docutils .literal .notranslate}]{.pre} it will
appear in the Scrapy log.
:::</p>
<p>::: {#log-short-names .section}
[]{#std-setting-LOG_SHORT_NAMES}</p>
<h5 id="log_short_namesheaderlink"><a class="header" href="#log_short_namesheaderlink">LOG_SHORT_NAMES<a href="#log-short-names" title="Permalink to this heading">¶</a>{.headerlink}</a></h5>
<p>Default: [<code>False</code>{.docutils .literal .notranslate}]{.pre}</p>
<p>If [<code>True</code>{.docutils .literal .notranslate}]{.pre}, the logs will just
contain the root path. If it is set to [<code>False</code>{.docutils .literal
.notranslate}]{.pre} then it displays the component responsible for the
log output
:::</p>
<p>::: {#logstats-interval .section}
[]{#std-setting-LOGSTATS_INTERVAL}</p>
<h5 id="logstats_intervalheaderlink"><a class="header" href="#logstats_intervalheaderlink">LOGSTATS_INTERVAL<a href="#logstats-interval" title="Permalink to this heading">¶</a>{.headerlink}</a></h5>
<p>Default: [<code>60.0</code>{.docutils .literal .notranslate}]{.pre}</p>
<p>The interval (in seconds) between each logging printout of the stats by
<a href="index.html#scrapy.extensions.logstats.LogStats" title="scrapy.extensions.logstats.LogStats">[<code>LogStats</code>{.xref .py .py-class .docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal}.
:::</p>
<p>::: {#memdebug-enabled .section}
[]{#std-setting-MEMDEBUG_ENABLED}</p>
<h5 id="memdebug_enabledheaderlink"><a class="header" href="#memdebug_enabledheaderlink">MEMDEBUG_ENABLED<a href="#memdebug-enabled" title="Permalink to this heading">¶</a>{.headerlink}</a></h5>
<p>Default: [<code>False</code>{.docutils .literal .notranslate}]{.pre}</p>
<p>Whether to enable memory debugging.
:::</p>
<p>::: {#memdebug-notify .section}
[]{#std-setting-MEMDEBUG_NOTIFY}</p>
<h5 id="memdebug_notifyheaderlink"><a class="header" href="#memdebug_notifyheaderlink">MEMDEBUG_NOTIFY<a href="#memdebug-notify" title="Permalink to this heading">¶</a>{.headerlink}</a></h5>
<p>Default: [<code>[]</code>{.docutils .literal .notranslate}]{.pre}</p>
<p>When memory debugging is enabled a memory report will be sent to the
specified addresses if this setting is not empty, otherwise the report
will be written to the log.</p>
<p>Example:</p>
<p>::: {.highlight-python .notranslate}
::: highlight
MEMDEBUG_NOTIFY = ['user@example.com']
:::
:::
:::</p>
<p>::: {#memusage-enabled .section}
[]{#std-setting-MEMUSAGE_ENABLED}</p>
<h5 id="memusage_enabledheaderlink"><a class="header" href="#memusage_enabledheaderlink">MEMUSAGE_ENABLED<a href="#memusage-enabled" title="Permalink to this heading">¶</a>{.headerlink}</a></h5>
<p>Default: [<code>True</code>{.docutils .literal .notranslate}]{.pre}</p>
<p>Scope: [<code>scrapy.extensions.memusage</code>{.docutils .literal
.notranslate}]{.pre}</p>
<p>Whether to enable the memory usage extension. This extension keeps track
of a peak memory used by the process (it writes it to stats). It can
also optionally shutdown the Scrapy process when it exceeds a memory
limit (see <a href="#std-setting-MEMUSAGE_LIMIT_MB">[<code>MEMUSAGE_LIMIT_MB</code>{.xref .std .std-setting .docutils
.literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal}), and notify by email when that happened
(see <a href="#std-setting-MEMUSAGE_NOTIFY_MAIL">[<code>MEMUSAGE_NOTIFY_MAIL</code>{.xref .std .std-setting .docutils .literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal}).</p>
<p>See <a href="index.html#topics-extensions-ref-memusage">[Memory usage extension]{.std
.std-ref}</a>{.hoverxref
.tooltip .reference .internal}.
:::</p>
<p>::: {#memusage-limit-mb .section}
[]{#std-setting-MEMUSAGE_LIMIT_MB}</p>
<h5 id="memusage_limit_mbheaderlink"><a class="header" href="#memusage_limit_mbheaderlink">MEMUSAGE_LIMIT_MB<a href="#memusage-limit-mb" title="Permalink to this heading">¶</a>{.headerlink}</a></h5>
<p>Default: [<code>0</code>{.docutils .literal .notranslate}]{.pre}</p>
<p>Scope: [<code>scrapy.extensions.memusage</code>{.docutils .literal
.notranslate}]{.pre}</p>
<p>The maximum amount of memory to allow (in megabytes) before shutting
down Scrapy (if MEMUSAGE_ENABLED is True). If zero, no check will be
performed.</p>
<p>See <a href="index.html#topics-extensions-ref-memusage">[Memory usage extension]{.std
.std-ref}</a>{.hoverxref
.tooltip .reference .internal}.
:::</p>
<p>::: {#memusage-check-interval-seconds .section}
[]{#std-setting-MEMUSAGE_CHECK_INTERVAL_SECONDS}</p>
<h5 id="memusage_check_interval_secondsheaderlink"><a class="header" href="#memusage_check_interval_secondsheaderlink">MEMUSAGE_CHECK_INTERVAL_SECONDS<a href="#memusage-check-interval-seconds" title="Permalink to this heading">¶</a>{.headerlink}</a></h5>
<p>Default: [<code>60.0</code>{.docutils .literal .notranslate}]{.pre}</p>
<p>Scope: [<code>scrapy.extensions.memusage</code>{.docutils .literal
.notranslate}]{.pre}</p>
<p>The <a href="index.html#topics-extensions-ref-memusage">[Memory usage extension]{.std
.std-ref}</a>{.hoverxref
.tooltip .reference .internal} checks the current memory usage, versus
the limits set by <a href="#std-setting-MEMUSAGE_LIMIT_MB">[<code>MEMUSAGE_LIMIT_MB</code>{.xref .std .std-setting
.docutils .literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal} and <a href="#std-setting-MEMUSAGE_WARNING_MB">[<code>MEMUSAGE_WARNING_MB</code>{.xref .std
.std-setting .docutils .literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal}, at fixed time intervals.</p>
<p>This sets the length of these intervals, in seconds.</p>
<p>See <a href="index.html#topics-extensions-ref-memusage">[Memory usage extension]{.std
.std-ref}</a>{.hoverxref
.tooltip .reference .internal}.
:::</p>
<p>::: {#memusage-notify-mail .section}
[]{#std-setting-MEMUSAGE_NOTIFY_MAIL}</p>
<h5 id="memusage_notify_mailheaderlink"><a class="header" href="#memusage_notify_mailheaderlink">MEMUSAGE_NOTIFY_MAIL<a href="#memusage-notify-mail" title="Permalink to this heading">¶</a>{.headerlink}</a></h5>
<p>Default: [<code>False</code>{.docutils .literal .notranslate}]{.pre}</p>
<p>Scope: [<code>scrapy.extensions.memusage</code>{.docutils .literal
.notranslate}]{.pre}</p>
<p>A list of emails to notify if the memory limit has been reached.</p>
<p>Example:</p>
<p>::: {.highlight-python .notranslate}
::: highlight
MEMUSAGE_NOTIFY_MAIL = ['user@example.com']
:::
:::</p>
<p>See <a href="index.html#topics-extensions-ref-memusage">[Memory usage extension]{.std
.std-ref}</a>{.hoverxref
.tooltip .reference .internal}.
:::</p>
<p>::: {#memusage-warning-mb .section}
[]{#std-setting-MEMUSAGE_WARNING_MB}</p>
<h5 id="memusage_warning_mbheaderlink"><a class="header" href="#memusage_warning_mbheaderlink">MEMUSAGE_WARNING_MB<a href="#memusage-warning-mb" title="Permalink to this heading">¶</a>{.headerlink}</a></h5>
<p>Default: [<code>0</code>{.docutils .literal .notranslate}]{.pre}</p>
<p>Scope: [<code>scrapy.extensions.memusage</code>{.docutils .literal
.notranslate}]{.pre}</p>
<p>The maximum amount of memory to allow (in megabytes) before sending a
warning email notifying about it. If zero, no warning will be produced.
:::</p>
<p>::: {#newspider-module .section}
[]{#std-setting-NEWSPIDER_MODULE}</p>
<h5 id="newspider_moduleheaderlink"><a class="header" href="#newspider_moduleheaderlink">NEWSPIDER_MODULE<a href="#newspider-module" title="Permalink to this heading">¶</a>{.headerlink}</a></h5>
<p>Default: [<code>''</code>{.docutils .literal .notranslate}]{.pre}</p>
<p>Module where to create new spiders using the <a href="index.html#std-command-genspider">[<code>genspider</code>{.xref .std
.std-command .docutils .literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal} command.</p>
<p>Example:</p>
<p>::: {.highlight-python .notranslate}
::: highlight
NEWSPIDER_MODULE = 'mybot.spiders_dev'
:::
:::
:::</p>
<p>::: {#randomize-download-delay .section}
[]{#std-setting-RANDOMIZE_DOWNLOAD_DELAY}</p>
<h5 id="randomize_download_delayheaderlink"><a class="header" href="#randomize_download_delayheaderlink">RANDOMIZE_DOWNLOAD_DELAY<a href="#randomize-download-delay" title="Permalink to this heading">¶</a>{.headerlink}</a></h5>
<p>Default: [<code>True</code>{.docutils .literal .notranslate}]{.pre}</p>
<p>If enabled, Scrapy will wait a random amount of time (between 0.5 *
<a href="#std-setting-DOWNLOAD_DELAY">[<code>DOWNLOAD_DELAY</code>{.xref .std .std-setting .docutils .literal
.notranslate}]{.pre}</a>{.hoverxref .tooltip
.reference .internal} and 1.5 * <a href="#std-setting-DOWNLOAD_DELAY">[<code>DOWNLOAD_DELAY</code>{.xref .std
.std-setting .docutils .literal
.notranslate}]{.pre}</a>{.hoverxref .tooltip
.reference .internal}) while fetching requests from the same website.</p>
<p>This randomization decreases the chance of the crawler being detected
(and subsequently blocked) by sites which analyze requests looking for
statistically significant similarities in the time between their
requests.</p>
<p>The randomization policy is the same used by
<a href="https://www.gnu.org/software/wget/manual/wget.html">wget</a>{.reference
.external} [<code>--random-wait</code>{.docutils .literal .notranslate}]{.pre}
option.</p>
<p>If <a href="#std-setting-DOWNLOAD_DELAY">[<code>DOWNLOAD_DELAY</code>{.xref .std .std-setting .docutils .literal
.notranslate}]{.pre}</a>{.hoverxref .tooltip
.reference .internal} is zero (default) this option has no effect.
:::</p>
<p>::: {#reactor-threadpool-maxsize .section}
[]{#std-setting-REACTOR_THREADPOOL_MAXSIZE}</p>
<h5 id="reactor_threadpool_maxsizeheaderlink"><a class="header" href="#reactor_threadpool_maxsizeheaderlink">REACTOR_THREADPOOL_MAXSIZE<a href="#reactor-threadpool-maxsize" title="Permalink to this heading">¶</a>{.headerlink}</a></h5>
<p>Default: [<code>10</code>{.docutils .literal .notranslate}]{.pre}</p>
<p>The maximum limit for Twisted Reactor thread pool size. This is common
multi-purpose thread pool used by various Scrapy components. Threaded
DNS Resolver, BlockingFeedStorage, S3FilesStore just to name a few.
Increase this value if you're experiencing problems with insufficient
blocking IO.
:::</p>
<p>::: {#redirect-priority-adjust .section}
[]{#std-setting-REDIRECT_PRIORITY_ADJUST}</p>
<h5 id="redirect_priority_adjustheaderlink"><a class="header" href="#redirect_priority_adjustheaderlink">REDIRECT_PRIORITY_ADJUST<a href="#redirect-priority-adjust" title="Permalink to this heading">¶</a>{.headerlink}</a></h5>
<p>Default: [<code>+2</code>{.docutils .literal .notranslate}]{.pre}</p>
<p>Scope:
[<code>scrapy.downloadermiddlewares.redirect.RedirectMiddleware</code>{.docutils
.literal .notranslate}]{.pre}</p>
<p>Adjust redirect request priority relative to original request:</p>
<ul>
<li>
<p><strong>a positive priority adjust (default) means higher priority.</strong></p>
</li>
<li>
<p>a negative priority adjust means lower priority.
:::</p>
</li>
</ul>
<p>::: {#robotstxt-obey .section}
[]{#std-setting-ROBOTSTXT_OBEY}</p>
<h5 id="robotstxt_obeyheaderlink"><a class="header" href="#robotstxt_obeyheaderlink">ROBOTSTXT_OBEY<a href="#robotstxt-obey" title="Permalink to this heading">¶</a>{.headerlink}</a></h5>
<p>Default: [<code>False</code>{.docutils .literal .notranslate}]{.pre}</p>
<p>Scope: [<code>scrapy.downloadermiddlewares.robotstxt</code>{.docutils .literal
.notranslate}]{.pre}</p>
<p>If enabled, Scrapy will respect robots.txt policies. For more
information see <a href="index.html#topics-dlmw-robots">[RobotsTxtMiddleware]{.std
.std-ref}</a>{.hoverxref .tooltip .reference
.internal}.</p>
<p>::: {.admonition .note}
Note</p>
<p>While the default value is [<code>False</code>{.docutils .literal
.notranslate}]{.pre} for historical reasons, this option is enabled by
default in settings.py file generated by [<code>scrapy</code>{.docutils .literal
.notranslate}]{.pre}<code> </code>{.docutils .literal
.notranslate}[<code>startproject</code>{.docutils .literal .notranslate}]{.pre}
command.
:::
:::</p>
<p>::: {#robotstxt-parser .section}
[]{#std-setting-ROBOTSTXT_PARSER}</p>
<h5 id="robotstxt_parserheaderlink"><a class="header" href="#robotstxt_parserheaderlink">ROBOTSTXT_PARSER<a href="#robotstxt-parser" title="Permalink to this heading">¶</a>{.headerlink}</a></h5>
<p>Default: [<code>'scrapy.robotstxt.ProtegoRobotParser'</code>{.docutils .literal
.notranslate}]{.pre}</p>
<p>The parser backend to use for parsing [<code>robots.txt</code>{.docutils .literal
.notranslate}]{.pre} files. For more information see
<a href="index.html#topics-dlmw-robots">[RobotsTxtMiddleware]{.std
.std-ref}</a>{.hoverxref .tooltip .reference
.internal}.</p>
<p>::: {#robotstxt-user-agent .section}
[]{#std-setting-ROBOTSTXT_USER_AGENT}</p>
<h6 id="robotstxt_user_agentheaderlink"><a class="header" href="#robotstxt_user_agentheaderlink">ROBOTSTXT_USER_AGENT<a href="#robotstxt-user-agent" title="Permalink to this heading">¶</a>{.headerlink}</a></h6>
<p>Default: [<code>None</code>{.docutils .literal .notranslate}]{.pre}</p>
<p>The user agent string to use for matching in the robots.txt file. If
[<code>None</code>{.docutils .literal .notranslate}]{.pre}, the User-Agent header
you are sending with the request or the <a href="#std-setting-USER_AGENT">[<code>USER_AGENT</code>{.xref .std
.std-setting .docutils .literal
.notranslate}]{.pre}</a>{.hoverxref .tooltip
.reference .internal} setting (in that order) will be used for
determining the user agent to use in the robots.txt file.
:::
:::</p>
<p>::: {#scheduler .section}
[]{#std-setting-SCHEDULER}</p>
<h5 id="schedulerheaderlink"><a class="header" href="#schedulerheaderlink">SCHEDULER<a href="#scheduler" title="Permalink to this heading">¶</a>{.headerlink}</a></h5>
<p>Default: [<code>'scrapy.core.scheduler.Scheduler'</code>{.docutils .literal
.notranslate}]{.pre}</p>
<p>The scheduler class to be used for crawling. See the <a href="index.html#topics-scheduler">[Scheduler]{.std
.std-ref}</a>{.hoverxref .tooltip .reference
.internal} topic for details.
:::</p>
<p>::: {#scheduler-debug .section}
[]{#std-setting-SCHEDULER_DEBUG}</p>
<h5 id="scheduler_debugheaderlink"><a class="header" href="#scheduler_debugheaderlink">SCHEDULER_DEBUG<a href="#scheduler-debug" title="Permalink to this heading">¶</a>{.headerlink}</a></h5>
<p>Default: [<code>False</code>{.docutils .literal .notranslate}]{.pre}</p>
<p>Setting to [<code>True</code>{.docutils .literal .notranslate}]{.pre} will log
debug information about the requests scheduler. This currently logs
(only once) if the requests cannot be serialized to disk. Stats counter
([<code>scheduler/unserializable</code>{.docutils .literal .notranslate}]{.pre})
tracks the number of times this happens.</p>
<p>Example entry in logs:</p>
<p>::: {.highlight-python .notranslate}
::: highlight
1956-01-31 00:00:00+0800 [scrapy.core.scheduler] ERROR: Unable to serialize request:
&lt;GET http://example.com&gt; - reason: cannot serialize &lt;Request at 0x9a7c7ec&gt;
(type Request)&gt; - no more unserializable requests will be logged
(see 'scheduler/unserializable' stats counter)
:::
:::
:::</p>
<p>::: {#scheduler-disk-queue .section}
[]{#std-setting-SCHEDULER_DISK_QUEUE}</p>
<h5 id="scheduler_disk_queueheaderlink"><a class="header" href="#scheduler_disk_queueheaderlink">SCHEDULER_DISK_QUEUE<a href="#scheduler-disk-queue" title="Permalink to this heading">¶</a>{.headerlink}</a></h5>
<p>Default: [<code>'scrapy.squeues.PickleLifoDiskQueue'</code>{.docutils .literal
.notranslate}]{.pre}</p>
<p>Type of disk queue that will be used by scheduler. Other available types
are [<code>scrapy.squeues.PickleFifoDiskQueue</code>{.docutils .literal
.notranslate}]{.pre}, [<code>scrapy.squeues.MarshalFifoDiskQueue</code>{.docutils
.literal .notranslate}]{.pre},
[<code>scrapy.squeues.MarshalLifoDiskQueue</code>{.docutils .literal
.notranslate}]{.pre}.
:::</p>
<p>::: {#scheduler-memory-queue .section}
[]{#std-setting-SCHEDULER_MEMORY_QUEUE}</p>
<h5 id="scheduler_memory_queueheaderlink"><a class="header" href="#scheduler_memory_queueheaderlink">SCHEDULER_MEMORY_QUEUE<a href="#scheduler-memory-queue" title="Permalink to this heading">¶</a>{.headerlink}</a></h5>
<p>Default: [<code>'scrapy.squeues.LifoMemoryQueue'</code>{.docutils .literal
.notranslate}]{.pre}</p>
<p>Type of in-memory queue used by scheduler. Other available type is:
[<code>scrapy.squeues.FifoMemoryQueue</code>{.docutils .literal
.notranslate}]{.pre}.
:::</p>
<p>::: {#scheduler-priority-queue .section}
[]{#std-setting-SCHEDULER_PRIORITY_QUEUE}</p>
<h5 id="scheduler_priority_queueheaderlink"><a class="header" href="#scheduler_priority_queueheaderlink">SCHEDULER_PRIORITY_QUEUE<a href="#scheduler-priority-queue" title="Permalink to this heading">¶</a>{.headerlink}</a></h5>
<p>Default: [<code>'scrapy.pqueues.ScrapyPriorityQueue'</code>{.docutils .literal
.notranslate}]{.pre}</p>
<p>Type of priority queue used by the scheduler. Another available type is
[<code>scrapy.pqueues.DownloaderAwarePriorityQueue</code>{.docutils .literal
.notranslate}]{.pre}.
[<code>scrapy.pqueues.DownloaderAwarePriorityQueue</code>{.docutils .literal
.notranslate}]{.pre} works better than
[<code>scrapy.pqueues.ScrapyPriorityQueue</code>{.docutils .literal
.notranslate}]{.pre} when you crawl many different domains in parallel.
But currently [<code>scrapy.pqueues.DownloaderAwarePriorityQueue</code>{.docutils
.literal .notranslate}]{.pre} does not work together with
<a href="#std-setting-CONCURRENT_REQUESTS_PER_IP">[<code>CONCURRENT_REQUESTS_PER_IP</code>{.xref .std .std-setting .docutils
.literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal}.
:::</p>
<p>::: {#scraper-slot-max-active-size .section}
[]{#std-setting-SCRAPER_SLOT_MAX_ACTIVE_SIZE}</p>
<h5 id="scraper_slot_max_active_sizeheaderlink"><a class="header" href="#scraper_slot_max_active_sizeheaderlink">SCRAPER_SLOT_MAX_ACTIVE_SIZE<a href="#scraper-slot-max-active-size" title="Permalink to this heading">¶</a>{.headerlink}</a></h5>
<p>::: versionadded
[New in version 2.0.]{.versionmodified .added}
:::</p>
<p>Default: [<code>5_000_000</code>{.docutils .literal .notranslate}]{.pre}</p>
<p>Soft limit (in bytes) for response data being processed.</p>
<p>While the sum of the sizes of all responses being processed is above
this value, Scrapy does not process new requests.
:::</p>
<p>::: {#spider-contracts .section}
[]{#std-setting-SPIDER_CONTRACTS}</p>
<h5 id="spider_contractsheaderlink"><a class="header" href="#spider_contractsheaderlink">SPIDER_CONTRACTS<a href="#spider-contracts" title="Permalink to this heading">¶</a>{.headerlink}</a></h5>
<p>Default:: [<code>{}</code>{.docutils .literal .notranslate}]{.pre}</p>
<p>A dict containing the spider contracts enabled in your project, used for
testing spiders. For more info see <a href="index.html#topics-contracts">[Spiders Contracts]{.std
.std-ref}</a>{.hoverxref .tooltip .reference
.internal}.
:::</p>
<p>::: {#spider-contracts-base .section}
[]{#std-setting-SPIDER_CONTRACTS_BASE}</p>
<h5 id="spider_contracts_baseheaderlink"><a class="header" href="#spider_contracts_baseheaderlink">SPIDER_CONTRACTS_BASE<a href="#spider-contracts-base" title="Permalink to this heading">¶</a>{.headerlink}</a></h5>
<p>Default:</p>
<p>::: {.highlight-python .notranslate}
::: highlight
{
&quot;scrapy.contracts.default.UrlContract&quot;: 1,
&quot;scrapy.contracts.default.ReturnsContract&quot;: 2,
&quot;scrapy.contracts.default.ScrapesContract&quot;: 3,
}
:::
:::</p>
<p>A dict containing the Scrapy contracts enabled by default in Scrapy. You
should never modify this setting in your project, modify
<a href="#std-setting-SPIDER_CONTRACTS">[<code>SPIDER_CONTRACTS</code>{.xref .std .std-setting .docutils .literal
.notranslate}]{.pre}</a>{.hoverxref .tooltip
.reference .internal} instead. For more info see <a href="index.html#topics-contracts">[Spiders
Contracts]{.std .std-ref}</a>{.hoverxref
.tooltip .reference .internal}.</p>
<p>You can disable any of these contracts by assigning [<code>None</code>{.docutils
.literal .notranslate}]{.pre} to their class path in
<a href="#std-setting-SPIDER_CONTRACTS">[<code>SPIDER_CONTRACTS</code>{.xref .std .std-setting .docutils .literal
.notranslate}]{.pre}</a>{.hoverxref .tooltip
.reference .internal}. E.g., to disable the built-in
[<code>ScrapesContract</code>{.docutils .literal .notranslate}]{.pre}, place this
in your [<code>settings.py</code>{.docutils .literal .notranslate}]{.pre}:</p>
<p>::: {.highlight-python .notranslate}
::: highlight
SPIDER_CONTRACTS = {
&quot;scrapy.contracts.default.ScrapesContract&quot;: None,
}
:::
:::
:::</p>
<p>::: {#spider-loader-class .section}
[]{#std-setting-SPIDER_LOADER_CLASS}</p>
<h5 id="spider_loader_classheaderlink"><a class="header" href="#spider_loader_classheaderlink">SPIDER_LOADER_CLASS<a href="#spider-loader-class" title="Permalink to this heading">¶</a>{.headerlink}</a></h5>
<p>Default: [<code>'scrapy.spiderloader.SpiderLoader'</code>{.docutils .literal
.notranslate}]{.pre}</p>
<p>The class that will be used for loading spiders, which must implement
the <a href="index.html#topics-api-spiderloader">[SpiderLoader API]{.std
.std-ref}</a>{.hoverxref .tooltip
.reference .internal}.
:::</p>
<p>::: {#spider-loader-warn-only .section}
[]{#std-setting-SPIDER_LOADER_WARN_ONLY}</p>
<h5 id="spider_loader_warn_onlyheaderlink"><a class="header" href="#spider_loader_warn_onlyheaderlink">SPIDER_LOADER_WARN_ONLY<a href="#spider-loader-warn-only" title="Permalink to this heading">¶</a>{.headerlink}</a></h5>
<p>Default: [<code>False</code>{.docutils .literal .notranslate}]{.pre}</p>
<p>By default, when Scrapy tries to import spider classes from
<a href="#std-setting-SPIDER_MODULES">[<code>SPIDER_MODULES</code>{.xref .std .std-setting .docutils .literal
.notranslate}]{.pre}</a>{.hoverxref .tooltip
.reference .internal}, it will fail loudly if there is any
[<code>ImportError</code>{.docutils .literal .notranslate}]{.pre} exception. But
you can choose to silence this exception and turn it into a simple
warning by setting [<code>SPIDER_LOADER_WARN_ONLY</code>{.docutils .literal
.notranslate}]{.pre}<code> </code>{.docutils .literal .notranslate}[<code>=</code>{.docutils
.literal .notranslate}]{.pre}<code> </code>{.docutils .literal
.notranslate}[<code>True</code>{.docutils .literal .notranslate}]{.pre}.</p>
<p>::: {.admonition .note}
Note</p>
<p>Some <a href="index.html#topics-commands">[scrapy commands]{.std
.std-ref}</a>{.hoverxref .tooltip .reference
.internal} run with this setting to [<code>True</code>{.docutils .literal
.notranslate}]{.pre} already (i.e. they will only issue a warning and
will not fail) since they do not actually need to load spider classes to
work: <a href="index.html#std-command-runspider">[<code>scrapy</code>{.xref .std .std-command .docutils .literal
.notranslate}]{.pre}<code> </code>{.xref .std .std-command .docutils .literal
.notranslate}[<code>runspider</code>{.xref .std .std-command .docutils .literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal}, <a href="index.html#std-command-settings">[<code>scrapy</code>{.xref .std .std-command
.docutils .literal .notranslate}]{.pre}<code> </code>{.xref .std .std-command
.docutils .literal .notranslate}[<code>settings</code>{.xref .std .std-command
.docutils .literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal}, <a href="index.html#std-command-startproject">[<code>scrapy</code>{.xref .std .std-command
.docutils .literal .notranslate}]{.pre}<code> </code>{.xref .std .std-command
.docutils .literal .notranslate}[<code>startproject</code>{.xref .std .std-command
.docutils .literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal}, <a href="index.html#std-command-version">[<code>scrapy</code>{.xref .std .std-command
.docutils .literal .notranslate}]{.pre}<code> </code>{.xref .std .std-command
.docutils .literal .notranslate}[<code>version</code>{.xref .std .std-command
.docutils .literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal}.
:::
:::</p>
<p>::: {#spider-middlewares .section}
[]{#std-setting-SPIDER_MIDDLEWARES}</p>
<h5 id="spider_middlewaresheaderlink"><a class="header" href="#spider_middlewaresheaderlink">SPIDER_MIDDLEWARES<a href="#spider-middlewares" title="Permalink to this heading">¶</a>{.headerlink}</a></h5>
<p>Default:: [<code>{}</code>{.docutils .literal .notranslate}]{.pre}</p>
<p>A dict containing the spider middlewares enabled in your project, and
their orders. For more info see <a href="index.html#topics-spider-middleware-setting">[Activating a spider middleware]{.std
.std-ref}</a>{.hoverxref
.tooltip .reference .internal}.
:::</p>
<p>::: {#spider-middlewares-base .section}
[]{#std-setting-SPIDER_MIDDLEWARES_BASE}</p>
<h5 id="spider_middlewares_baseheaderlink"><a class="header" href="#spider_middlewares_baseheaderlink">SPIDER_MIDDLEWARES_BASE<a href="#spider-middlewares-base" title="Permalink to this heading">¶</a>{.headerlink}</a></h5>
<p>Default:</p>
<p>::: {.highlight-python .notranslate}
::: highlight
{
&quot;scrapy.spidermiddlewares.httperror.HttpErrorMiddleware&quot;: 50,
&quot;scrapy.spidermiddlewares.offsite.OffsiteMiddleware&quot;: 500,
&quot;scrapy.spidermiddlewares.referer.RefererMiddleware&quot;: 700,
&quot;scrapy.spidermiddlewares.urllength.UrlLengthMiddleware&quot;: 800,
&quot;scrapy.spidermiddlewares.depth.DepthMiddleware&quot;: 900,
}
:::
:::</p>
<p>A dict containing the spider middlewares enabled by default in Scrapy,
and their orders. Low orders are closer to the engine, high orders are
closer to the spider. For more info see <a href="index.html#topics-spider-middleware-setting">[Activating a spider
middleware]{.std
.std-ref}</a>{.hoverxref
.tooltip .reference .internal}.
:::</p>
<p>::: {#spider-modules .section}
[]{#std-setting-SPIDER_MODULES}</p>
<h5 id="spider_modulesheaderlink"><a class="header" href="#spider_modulesheaderlink">SPIDER_MODULES<a href="#spider-modules" title="Permalink to this heading">¶</a>{.headerlink}</a></h5>
<p>Default: [<code>[]</code>{.docutils .literal .notranslate}]{.pre}</p>
<p>A list of modules where Scrapy will look for spiders.</p>
<p>Example:</p>
<p>::: {.highlight-python .notranslate}
::: highlight
SPIDER_MODULES = [&quot;mybot.spiders_prod&quot;, &quot;mybot.spiders_dev&quot;]
:::
:::
:::</p>
<p>::: {#stats-class .section}
[]{#std-setting-STATS_CLASS}</p>
<h5 id="stats_classheaderlink"><a class="header" href="#stats_classheaderlink">STATS_CLASS<a href="#stats-class" title="Permalink to this heading">¶</a>{.headerlink}</a></h5>
<p>Default: [<code>'scrapy.statscollectors.MemoryStatsCollector'</code>{.docutils
.literal .notranslate}]{.pre}</p>
<p>The class to use for collecting stats, who must implement the <a href="index.html#topics-api-stats">[Stats
Collector API]{.std .std-ref}</a>{.hoverxref
.tooltip .reference .internal}.
:::</p>
<p>::: {#stats-dump .section}
[]{#std-setting-STATS_DUMP}</p>
<h5 id="stats_dumpheaderlink"><a class="header" href="#stats_dumpheaderlink">STATS_DUMP<a href="#stats-dump" title="Permalink to this heading">¶</a>{.headerlink}</a></h5>
<p>Default: [<code>True</code>{.docutils .literal .notranslate}]{.pre}</p>
<p>Dump the <a href="index.html#topics-stats">[Scrapy stats]{.std
.std-ref}</a>{.hoverxref .tooltip .reference
.internal} (to the Scrapy log) once the spider finishes.</p>
<p>For more info see: <a href="index.html#topics-stats">[Stats Collection]{.std
.std-ref}</a>{.hoverxref .tooltip .reference
.internal}.
:::</p>
<p>::: {#statsmailer-rcpts .section}
[]{#std-setting-STATSMAILER_RCPTS}</p>
<h5 id="statsmailer_rcptsheaderlink"><a class="header" href="#statsmailer_rcptsheaderlink">STATSMAILER_RCPTS<a href="#statsmailer-rcpts" title="Permalink to this heading">¶</a>{.headerlink}</a></h5>
<p>Default: [<code>[]</code>{.docutils .literal .notranslate}]{.pre} (empty list)</p>
<p>Send Scrapy stats after spiders finish scraping. See
<a href="index.html#scrapy.extensions.statsmailer.StatsMailer" title="scrapy.extensions.statsmailer.StatsMailer">[<code>StatsMailer</code>{.xref .py .py-class .docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal} for more info.
:::</p>
<p>::: {#telnetconsole-enabled .section}
[]{#std-setting-TELNETCONSOLE_ENABLED}</p>
<h5 id="telnetconsole_enabledheaderlink"><a class="header" href="#telnetconsole_enabledheaderlink">TELNETCONSOLE_ENABLED<a href="#telnetconsole-enabled" title="Permalink to this heading">¶</a>{.headerlink}</a></h5>
<p>Default: [<code>True</code>{.docutils .literal .notranslate}]{.pre}</p>
<p>A boolean which specifies if the <a href="index.html#topics-telnetconsole">[telnet console]{.std
.std-ref}</a>{.hoverxref .tooltip
.reference .internal} will be enabled (provided its extension is also
enabled).
:::</p>
<p>::: {#templates-dir .section}
[]{#std-setting-TEMPLATES_DIR}</p>
<h5 id="templates_dirheaderlink"><a class="header" href="#templates_dirheaderlink">TEMPLATES_DIR<a href="#templates-dir" title="Permalink to this heading">¶</a>{.headerlink}</a></h5>
<p>Default: [<code>templates</code>{.docutils .literal .notranslate}]{.pre} dir inside
scrapy module</p>
<p>The directory where to look for templates when creating new projects
with <a href="index.html#std-command-startproject">[<code>startproject</code>{.xref .std .std-command .docutils .literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal} command and new spiders with
<a href="index.html#std-command-genspider">[<code>genspider</code>{.xref .std .std-command .docutils .literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal} command.</p>
<p>The project name must not conflict with the name of custom files or
directories in the [<code>project</code>{.docutils .literal .notranslate}]{.pre}
subdirectory.
:::</p>
<p>::: {#twisted-reactor .section}
[]{#std-setting-TWISTED_REACTOR}</p>
<h5 id="twisted_reactorheaderlink"><a class="header" href="#twisted_reactorheaderlink">TWISTED_REACTOR<a href="#twisted-reactor" title="Permalink to this heading">¶</a>{.headerlink}</a></h5>
<p>::: versionadded
[New in version 2.0.]{.versionmodified .added}
:::</p>
<p>Default: [<code>None</code>{.docutils .literal .notranslate}]{.pre}</p>
<p>Import path of a given <a href="https://docs.twisted.org/en/stable/api/twisted.internet.reactor.html" title="(in Twisted)">[<code>reactor</code>{.xref .py .py-mod .docutils .literal
.notranslate}]{.pre}</a>{.reference
.external}.</p>
<p>Scrapy will install this reactor if no other reactor is installed yet,
such as when the [<code>scrapy</code>{.docutils .literal .notranslate}]{.pre} CLI
program is invoked or when using the <a href="index.html#scrapy.crawler.CrawlerProcess" title="scrapy.crawler.CrawlerProcess">[<code>CrawlerProcess</code>{.xref .py
.py-class .docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal} class.</p>
<p>If you are using the <a href="index.html#scrapy.crawler.CrawlerRunner" title="scrapy.crawler.CrawlerRunner">[<code>CrawlerRunner</code>{.xref .py .py-class .docutils
.literal
.notranslate}]{.pre}</a>{.reference
.internal} class, you also need to install the correct reactor manually.
You can do that using <a href="#scrapy.utils.reactor.install_reactor" title="scrapy.utils.reactor.install_reactor">[<code>install_reactor()</code>{.xref .py .py-func .docutils
.literal
.notranslate}]{.pre}</a>{.reference
.internal}:</p>
<p>[[scrapy.utils.reactor.]{.pre}]{.sig-prename .descclassname}[[install_reactor]{.pre}]{.sig-name .descname}[(]{.sig-paren}<em>[[reactor_path]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[<a href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.12)">[str]{.pre}</a>{.reference .external}]{.n}</em>, <em>[[event_loop_path]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[<a href="https://docs.python.org/3/library/typing.html#typing.Optional" title="(in Python v3.12)">[Optional]{.pre}</a>{.reference .external}[[[]{.pre}]{.p}<a href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.12)">[str]{.pre}</a>{.reference .external}[[]]{.pre}]{.p}]{.n}[ ]{.w}[[=]{.pre}]{.o}[ ]{.w}[[None]{.pre}]{.default_value}</em>[)]{.sig-paren} [[→]{.sig-return-icon} [<a href="https://docs.python.org/3/library/constants.html#None" title="(in Python v3.12)">[None]{.pre}</a>{.reference .external}]{.sig-return-typehint}]{.sig-return}<a href="_modules/scrapy/utils/reactor.html#install_reactor">[[[source]]{.pre}]{.viewcode-link}</a>{.reference .internal}<a href="#scrapy.utils.reactor.install_reactor" title="Permalink to this definition">¶</a>{.headerlink}</p>
<p>:   Installs the <a href="https://docs.twisted.org/en/stable/api/twisted.internet.reactor.html" title="(in Twisted)">[<code>reactor</code>{.xref .py .py-mod .docutils .literal
.notranslate}]{.pre}</a>{.reference
.external} with the specified import path. Also installs the asyncio
event loop with the specified import path if the asyncio reactor is
enabled</p>
<p>If a reactor is already installed, <a href="#scrapy.utils.reactor.install_reactor" title="scrapy.utils.reactor.install_reactor">[<code>install_reactor()</code>{.xref .py
.py-func .docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal} has no effect.</p>
<p>[<code>CrawlerRunner.__init__</code>{.xref .py .py-meth .docutils .literal
.notranslate}]{.pre} raises <a href="https://docs.python.org/3/library/exceptions.html#Exception" title="(in Python v3.12)">[<code>Exception</code>{.xref .py .py-exc .docutils
.literal
.notranslate}]{.pre}</a>{.reference
.external} if the installed reactor does not match the
<a href="#std-setting-TWISTED_REACTOR">[<code>TWISTED_REACTOR</code>{.xref .std .std-setting .docutils .literal
.notranslate}]{.pre}</a>{.hoverxref .tooltip
.reference .internal} setting; therefore, having top-level
<a href="https://docs.twisted.org/en/stable/api/twisted.internet.reactor.html" title="(in Twisted)">[<code>reactor</code>{.xref .py .py-mod .docutils .literal
.notranslate}]{.pre}</a>{.reference
.external} imports in project files and imported third-party libraries
will make Scrapy raise <a href="https://docs.python.org/3/library/exceptions.html#Exception" title="(in Python v3.12)">[<code>Exception</code>{.xref .py .py-exc .docutils
.literal
.notranslate}]{.pre}</a>{.reference
.external} when it checks which reactor is installed.</p>
<p>In order to use the reactor installed by Scrapy:</p>
<p>::: {.highlight-python .notranslate}
::: highlight
import scrapy
from twisted.internet import reactor</p>
<pre><code>class QuotesSpider(scrapy.Spider):
    name = &quot;quotes&quot;

    def __init__(self, *args, **kwargs):
        self.timeout = int(kwargs.pop(&quot;timeout&quot;, &quot;60&quot;))
        super(QuotesSpider, self).__init__(*args, **kwargs)

    def start_requests(self):
        reactor.callLater(self.timeout, self.stop)

        urls = [&quot;https://quotes.toscrape.com/page/1&quot;]
        for url in urls:
            yield scrapy.Request(url=url, callback=self.parse)

    def parse(self, response):
        for quote in response.css(&quot;div.quote&quot;):
            yield {&quot;text&quot;: quote.css(&quot;span.text::text&quot;).get()}

    def stop(self):
        self.crawler.engine.close_spider(self, &quot;timeout&quot;)
</code></pre>
<p>:::
:::</p>
<p>which raises <a href="https://docs.python.org/3/library/exceptions.html#Exception" title="(in Python v3.12)">[<code>Exception</code>{.xref .py .py-exc .docutils .literal
.notranslate}]{.pre}</a>{.reference
.external}, becomes:</p>
<p>::: {.highlight-python .notranslate}
::: highlight
import scrapy</p>
<pre><code>class QuotesSpider(scrapy.Spider):
    name = &quot;quotes&quot;

    def __init__(self, *args, **kwargs):
        self.timeout = int(kwargs.pop(&quot;timeout&quot;, &quot;60&quot;))
        super(QuotesSpider, self).__init__(*args, **kwargs)

    def start_requests(self):
        from twisted.internet import reactor

        reactor.callLater(self.timeout, self.stop)

        urls = [&quot;https://quotes.toscrape.com/page/1&quot;]
        for url in urls:
            yield scrapy.Request(url=url, callback=self.parse)

    def parse(self, response):
        for quote in response.css(&quot;div.quote&quot;):
            yield {&quot;text&quot;: quote.css(&quot;span.text::text&quot;).get()}

    def stop(self):
        self.crawler.engine.close_spider(self, &quot;timeout&quot;)
</code></pre>
<p>:::
:::</p>
<p>The default value of the <a href="#std-setting-TWISTED_REACTOR">[<code>TWISTED_REACTOR</code>{.xref .std .std-setting
.docutils .literal
.notranslate}]{.pre}</a>{.hoverxref .tooltip
.reference .internal} setting is [<code>None</code>{.docutils .literal
.notranslate}]{.pre}, which means that Scrapy will use the existing
reactor if one is already installed, or install the default reactor
defined by Twisted for the current platform. This is to maintain
backward compatibility and avoid possible problems caused by using a
non-default reactor.</p>
<p>::: versionchanged
[Changed in version 2.7: ]{.versionmodified .changed}The
<a href="index.html#std-command-startproject">[<code>startproject</code>{.xref .std .std-command .docutils .literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal} command now sets this setting to
[<code>twisted.internet.asyncioreactor.AsyncioSelectorReactor</code>{.docutils
.literal .notranslate}]{.pre} in the generated [<code>settings.py</code>{.docutils
.literal .notranslate}]{.pre} file.
:::</p>
<p>For additional information, see <a href="https://docs.twisted.org/en/stable/core/howto/choosing-reactor.html" title="(in Twisted v23.10)">Choosing a Reactor and GUI Toolkit
Integration</a>{.reference
.external}.
:::</p>
<p>::: {#urllength-limit .section}
[]{#std-setting-URLLENGTH_LIMIT}</p>
<h5 id="urllength_limitheaderlink"><a class="header" href="#urllength_limitheaderlink">URLLENGTH_LIMIT<a href="#urllength-limit" title="Permalink to this heading">¶</a>{.headerlink}</a></h5>
<p>Default: [<code>2083</code>{.docutils .literal .notranslate}]{.pre}</p>
<p>Scope: [<code>spidermiddlewares.urllength</code>{.docutils .literal
.notranslate}]{.pre}</p>
<p>The maximum URL length to allow for crawled URLs.</p>
<p>This setting can act as a stopping condition in case of URLs of
ever-increasing length, which may be caused for example by a programming
error either in the target server or in your code. See also
<a href="index.html#std-setting-REDIRECT_MAX_TIMES">[<code>REDIRECT_MAX_TIMES</code>{.xref .std .std-setting .docutils .literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal} and <a href="#std-setting-DEPTH_LIMIT">[<code>DEPTH_LIMIT</code>{.xref .std
.std-setting .docutils .literal
.notranslate}]{.pre}</a>{.hoverxref .tooltip
.reference .internal}.</p>
<p>Use [<code>0</code>{.docutils .literal .notranslate}]{.pre} to allow URLs of any
length.</p>
<p>The default value is copied from the <a href="https://support.microsoft.com/en-us/topic/maximum-url-length-is-2-083-characters-in-internet-explorer-174e7c8a-6666-f4e0-6fd6-908b53c12246">Microsoft Internet Explorer
maximum URL
length</a>{.reference
.external}, even though this setting exists for different reasons.
:::</p>
<p>::: {#user-agent .section}
[]{#std-setting-USER_AGENT}</p>
<h5 id="user_agentheaderlink"><a class="header" href="#user_agentheaderlink">USER_AGENT<a href="#user-agent" title="Permalink to this heading">¶</a>{.headerlink}</a></h5>
<p>Default: [<code>&quot;Scrapy/VERSION</code>{.docutils .literal
.notranslate}]{.pre}<code> </code>{.docutils .literal
.notranslate}[<code>(+https://scrapy.org)&quot;</code>{.docutils .literal
.notranslate}]{.pre}</p>
<p>The default User-Agent to use when crawling, unless overridden. This
user agent is also used by <a href="index.html#scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware" title="scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware">[<code>RobotsTxtMiddleware</code>{.xref .py .py-class
.docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal} if <a href="#std-setting-ROBOTSTXT_USER_AGENT">[<code>ROBOTSTXT_USER_AGENT</code>{.xref .std .std-setting .docutils
.literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal} setting is [<code>None</code>{.docutils .literal
.notranslate}]{.pre} and there is no overriding User-Agent header
specified for the request.
:::</p>
<p>::: {#settings-documented-elsewhere .section}</p>
<h5 id="settings-documented-elsewhereheaderlink"><a class="header" href="#settings-documented-elsewhereheaderlink">Settings documented elsewhere:<a href="#settings-documented-elsewhere" title="Permalink to this heading">¶</a>{.headerlink}</a></h5>
<p>The following settings are documented elsewhere, please check each
specific case to see how to enable and use them.</p>
<ul>
<li>
<p><a href="index.html#std-setting-ADDONS">ADDONS</a>{.reference .internal}</p>
</li>
<li>
<p><a href="index.html#std-setting-AJAXCRAWL_ENABLED">AJAXCRAWL_ENABLED</a>{.reference
.internal}</p>
</li>
<li>
<p><a href="index.html#std-setting-ASYNCIO_EVENT_LOOP">ASYNCIO_EVENT_LOOP</a>{.reference
.internal}</p>
</li>
<li>
<p><a href="index.html#std-setting-AUTOTHROTTLE_DEBUG">AUTOTHROTTLE_DEBUG</a>{.reference
.internal}</p>
</li>
<li>
<p><a href="index.html#std-setting-AUTOTHROTTLE_ENABLED">AUTOTHROTTLE_ENABLED</a>{.reference
.internal}</p>
</li>
<li>
<p><a href="index.html#std-setting-AUTOTHROTTLE_MAX_DELAY">AUTOTHROTTLE_MAX_DELAY</a>{.reference
.internal}</p>
</li>
<li>
<p><a href="index.html#std-setting-AUTOTHROTTLE_START_DELAY">AUTOTHROTTLE_START_DELAY</a>{.reference
.internal}</p>
</li>
<li>
<p><a href="index.html#std-setting-AUTOTHROTTLE_TARGET_CONCURRENCY">AUTOTHROTTLE_TARGET_CONCURRENCY</a>{.reference
.internal}</p>
</li>
<li>
<p><a href="index.html#std-setting-AWS_ACCESS_KEY_ID">AWS_ACCESS_KEY_ID</a>{.reference
.internal}</p>
</li>
<li>
<p><a href="index.html#std-setting-AWS_ENDPOINT_URL">AWS_ENDPOINT_URL</a>{.reference
.internal}</p>
</li>
<li>
<p><a href="index.html#std-setting-AWS_REGION_NAME">AWS_REGION_NAME</a>{.reference
.internal}</p>
</li>
<li>
<p><a href="index.html#std-setting-AWS_SECRET_ACCESS_KEY">AWS_SECRET_ACCESS_KEY</a>{.reference
.internal}</p>
</li>
<li>
<p><a href="index.html#std-setting-AWS_SESSION_TOKEN">AWS_SESSION_TOKEN</a>{.reference
.internal}</p>
</li>
<li>
<p><a href="index.html#std-setting-AWS_USE_SSL">AWS_USE_SSL</a>{.reference
.internal}</p>
</li>
<li>
<p><a href="index.html#std-setting-AWS_VERIFY">AWS_VERIFY</a>{.reference
.internal}</p>
</li>
<li>
<p><a href="index.html#std-setting-BOT_NAME">BOT_NAME</a>{.reference .internal}</p>
</li>
<li>
<p><a href="index.html#std-setting-CLOSESPIDER_ERRORCOUNT">CLOSESPIDER_ERRORCOUNT</a>{.reference
.internal}</p>
</li>
<li>
<p><a href="index.html#std-setting-CLOSESPIDER_ITEMCOUNT">CLOSESPIDER_ITEMCOUNT</a>{.reference
.internal}</p>
</li>
<li>
<p><a href="index.html#std-setting-CLOSESPIDER_PAGECOUNT">CLOSESPIDER_PAGECOUNT</a>{.reference
.internal}</p>
</li>
<li>
<p><a href="index.html#std-setting-CLOSESPIDER_TIMEOUT">CLOSESPIDER_TIMEOUT</a>{.reference
.internal}</p>
</li>
<li>
<p><a href="index.html#std-setting-CLOSESPIDER_TIMEOUT_NO_ITEM">CLOSESPIDER_TIMEOUT_NO_ITEM</a>{.reference
.internal}</p>
</li>
<li>
<p><a href="index.html#std-setting-COMMANDS_MODULE">COMMANDS_MODULE</a>{.reference
.internal}</p>
</li>
<li>
<p><a href="index.html#std-setting-COMPRESSION_ENABLED">COMPRESSION_ENABLED</a>{.reference
.internal}</p>
</li>
<li>
<p><a href="index.html#std-setting-CONCURRENT_ITEMS">CONCURRENT_ITEMS</a>{.reference
.internal}</p>
</li>
<li>
<p><a href="index.html#std-setting-CONCURRENT_REQUESTS">CONCURRENT_REQUESTS</a>{.reference
.internal}</p>
</li>
<li>
<p><a href="index.html#std-setting-CONCURRENT_REQUESTS_PER_DOMAIN">CONCURRENT_REQUESTS_PER_DOMAIN</a>{.reference
.internal}</p>
</li>
<li>
<p><a href="index.html#std-setting-CONCURRENT_REQUESTS_PER_IP">CONCURRENT_REQUESTS_PER_IP</a>{.reference
.internal}</p>
</li>
<li>
<p><a href="index.html#std-setting-COOKIES_DEBUG">COOKIES_DEBUG</a>{.reference
.internal}</p>
</li>
<li>
<p><a href="index.html#std-setting-COOKIES_ENABLED">COOKIES_ENABLED</a>{.reference
.internal}</p>
</li>
<li>
<p><a href="index.html#std-setting-DEFAULT_ITEM_CLASS">DEFAULT_ITEM_CLASS</a>{.reference
.internal}</p>
</li>
<li>
<p><a href="index.html#std-setting-DEFAULT_REQUEST_HEADERS">DEFAULT_REQUEST_HEADERS</a>{.reference
.internal}</p>
</li>
<li>
<p><a href="index.html#std-setting-DEPTH_LIMIT">DEPTH_LIMIT</a>{.reference
.internal}</p>
</li>
<li>
<p><a href="index.html#std-setting-DEPTH_PRIORITY">DEPTH_PRIORITY</a>{.reference
.internal}</p>
</li>
<li>
<p><a href="index.html#std-setting-DEPTH_STATS_VERBOSE">DEPTH_STATS_VERBOSE</a>{.reference
.internal}</p>
</li>
<li>
<p><a href="index.html#std-setting-DNSCACHE_ENABLED">DNSCACHE_ENABLED</a>{.reference
.internal}</p>
</li>
<li>
<p><a href="index.html#std-setting-DNSCACHE_SIZE">DNSCACHE_SIZE</a>{.reference
.internal}</p>
</li>
<li>
<p><a href="index.html#std-setting-DNS_RESOLVER">DNS_RESOLVER</a>{.reference
.internal}</p>
</li>
<li>
<p><a href="index.html#std-setting-DNS_TIMEOUT">DNS_TIMEOUT</a>{.reference
.internal}</p>
</li>
<li>
<p><a href="index.html#std-setting-DOWNLOADER">DOWNLOADER</a>{.reference
.internal}</p>
</li>
<li>
<p><a href="index.html#std-setting-DOWNLOADER_CLIENTCONTEXTFACTORY">DOWNLOADER_CLIENTCONTEXTFACTORY</a>{.reference
.internal}</p>
</li>
<li>
<p><a href="index.html#std-setting-DOWNLOADER_CLIENT_TLS_CIPHERS">DOWNLOADER_CLIENT_TLS_CIPHERS</a>{.reference
.internal}</p>
</li>
<li>
<p><a href="index.html#std-setting-DOWNLOADER_CLIENT_TLS_METHOD">DOWNLOADER_CLIENT_TLS_METHOD</a>{.reference
.internal}</p>
</li>
<li>
<p><a href="index.html#std-setting-DOWNLOADER_CLIENT_TLS_VERBOSE_LOGGING">DOWNLOADER_CLIENT_TLS_VERBOSE_LOGGING</a>{.reference
.internal}</p>
</li>
<li>
<p><a href="index.html#std-setting-DOWNLOADER_HTTPCLIENTFACTORY">DOWNLOADER_HTTPCLIENTFACTORY</a>{.reference
.internal}</p>
</li>
<li>
<p><a href="index.html#std-setting-DOWNLOADER_MIDDLEWARES">DOWNLOADER_MIDDLEWARES</a>{.reference
.internal}</p>
</li>
<li>
<p><a href="index.html#std-setting-DOWNLOADER_MIDDLEWARES_BASE">DOWNLOADER_MIDDLEWARES_BASE</a>{.reference
.internal}</p>
</li>
<li>
<p><a href="index.html#std-setting-DOWNLOADER_STATS">DOWNLOADER_STATS</a>{.reference
.internal}</p>
</li>
<li>
<p><a href="index.html#std-setting-DOWNLOAD_DELAY">DOWNLOAD_DELAY</a>{.reference
.internal}</p>
</li>
<li>
<p><a href="index.html#std-setting-DOWNLOAD_FAIL_ON_DATALOSS">DOWNLOAD_FAIL_ON_DATALOSS</a>{.reference
.internal}</p>
</li>
<li>
<p><a href="index.html#std-setting-DOWNLOAD_HANDLERS">DOWNLOAD_HANDLERS</a>{.reference
.internal}</p>
</li>
<li>
<p><a href="index.html#std-setting-DOWNLOAD_HANDLERS_BASE">DOWNLOAD_HANDLERS_BASE</a>{.reference
.internal}</p>
</li>
<li>
<p><a href="index.html#std-setting-DOWNLOAD_MAXSIZE">DOWNLOAD_MAXSIZE</a>{.reference
.internal}</p>
</li>
<li>
<p><a href="index.html#std-setting-DOWNLOAD_SLOTS">DOWNLOAD_SLOTS</a>{.reference
.internal}</p>
</li>
<li>
<p><a href="index.html#std-setting-DOWNLOAD_TIMEOUT">DOWNLOAD_TIMEOUT</a>{.reference
.internal}</p>
</li>
<li>
<p><a href="index.html#std-setting-DOWNLOAD_WARNSIZE">DOWNLOAD_WARNSIZE</a>{.reference
.internal}</p>
</li>
<li>
<p><a href="index.html#std-setting-DUPEFILTER_CLASS">DUPEFILTER_CLASS</a>{.reference
.internal}</p>
</li>
<li>
<p><a href="index.html#std-setting-DUPEFILTER_DEBUG">DUPEFILTER_DEBUG</a>{.reference
.internal}</p>
</li>
<li>
<p><a href="index.html#std-setting-EDITOR">EDITOR</a>{.reference .internal}</p>
</li>
<li>
<p><a href="index.html#std-setting-EXTENSIONS">EXTENSIONS</a>{.reference
.internal}</p>
</li>
<li>
<p><a href="index.html#std-setting-EXTENSIONS_BASE">EXTENSIONS_BASE</a>{.reference
.internal}</p>
</li>
<li>
<p><a href="index.html#std-setting-FEEDS">FEEDS</a>{.reference .internal}</p>
</li>
<li>
<p><a href="index.html#std-setting-FEED_EXPORTERS">FEED_EXPORTERS</a>{.reference
.internal}</p>
</li>
<li>
<p><a href="index.html#std-setting-FEED_EXPORTERS_BASE">FEED_EXPORTERS_BASE</a>{.reference
.internal}</p>
</li>
<li>
<p><a href="index.html#std-setting-FEED_EXPORT_BATCH_ITEM_COUNT">FEED_EXPORT_BATCH_ITEM_COUNT</a>{.reference
.internal}</p>
</li>
<li>
<p><a href="index.html#std-setting-FEED_EXPORT_ENCODING">FEED_EXPORT_ENCODING</a>{.reference
.internal}</p>
</li>
<li>
<p><a href="index.html#std-setting-FEED_EXPORT_FIELDS">FEED_EXPORT_FIELDS</a>{.reference
.internal}</p>
</li>
<li>
<p><a href="index.html#std-setting-FEED_EXPORT_INDENT">FEED_EXPORT_INDENT</a>{.reference
.internal}</p>
</li>
<li>
<p><a href="index.html#std-setting-FEED_STORAGES">FEED_STORAGES</a>{.reference
.internal}</p>
</li>
<li>
<p><a href="index.html#std-setting-FEED_STORAGES_BASE">FEED_STORAGES_BASE</a>{.reference
.internal}</p>
</li>
<li>
<p><a href="index.html#std-setting-FEED_STORAGE_FTP_ACTIVE">FEED_STORAGE_FTP_ACTIVE</a>{.reference
.internal}</p>
</li>
<li>
<p><a href="index.html#std-setting-FEED_STORAGE_GCS_ACL">FEED_STORAGE_GCS_ACL</a>{.reference
.internal}</p>
</li>
<li>
<p><a href="index.html#std-setting-FEED_STORAGE_S3_ACL">FEED_STORAGE_S3_ACL</a>{.reference
.internal}</p>
</li>
<li>
<p><a href="index.html#std-setting-FEED_STORE_EMPTY">FEED_STORE_EMPTY</a>{.reference
.internal}</p>
</li>
<li>
<p><a href="index.html#std-setting-FEED_TEMPDIR">FEED_TEMPDIR</a>{.reference
.internal}</p>
</li>
<li>
<p><a href="index.html#std-setting-FEED_URI_PARAMS">FEED_URI_PARAMS</a>{.reference
.internal}</p>
</li>
<li>
<p><a href="index.html#std-setting-FILES_EXPIRES">FILES_EXPIRES</a>{.reference
.internal}</p>
</li>
<li>
<p><a href="index.html#std-setting-FILES_RESULT_FIELD">FILES_RESULT_FIELD</a>{.reference
.internal}</p>
</li>
<li>
<p><a href="index.html#std-setting-FILES_STORE">FILES_STORE</a>{.reference
.internal}</p>
</li>
<li>
<p><a href="index.html#std-setting-FILES_STORE_GCS_ACL">FILES_STORE_GCS_ACL</a>{.reference
.internal}</p>
</li>
<li>
<p><a href="index.html#std-setting-FILES_STORE_S3_ACL">FILES_STORE_S3_ACL</a>{.reference
.internal}</p>
</li>
<li>
<p><a href="index.html#std-setting-FILES_URLS_FIELD">FILES_URLS_FIELD</a>{.reference
.internal}</p>
</li>
<li>
<p><a href="index.html#std-setting-FTP_PASSIVE_MODE">FTP_PASSIVE_MODE</a>{.reference
.internal}</p>
</li>
<li>
<p><a href="index.html#std-setting-FTP_PASSWORD">FTP_PASSWORD</a>{.reference
.internal}</p>
</li>
<li>
<p><a href="index.html#std-setting-FTP_USER">FTP_USER</a>{.reference .internal}</p>
</li>
<li>
<p><a href="index.html#std-setting-GCS_PROJECT_ID">GCS_PROJECT_ID</a>{.reference
.internal}</p>
</li>
<li>
<p><a href="index.html#std-setting-HTTPCACHE_ALWAYS_STORE">HTTPCACHE_ALWAYS_STORE</a>{.reference
.internal}</p>
</li>
<li>
<p><a href="index.html#std-setting-HTTPCACHE_DBM_MODULE">HTTPCACHE_DBM_MODULE</a>{.reference
.internal}</p>
</li>
<li>
<p><a href="index.html#std-setting-HTTPCACHE_DIR">HTTPCACHE_DIR</a>{.reference
.internal}</p>
</li>
<li>
<p><a href="index.html#std-setting-HTTPCACHE_ENABLED">HTTPCACHE_ENABLED</a>{.reference
.internal}</p>
</li>
<li>
<p><a href="index.html#std-setting-HTTPCACHE_EXPIRATION_SECS">HTTPCACHE_EXPIRATION_SECS</a>{.reference
.internal}</p>
</li>
<li>
<p><a href="index.html#std-setting-HTTPCACHE_GZIP">HTTPCACHE_GZIP</a>{.reference
.internal}</p>
</li>
<li>
<p><a href="index.html#std-setting-HTTPCACHE_IGNORE_HTTP_CODES">HTTPCACHE_IGNORE_HTTP_CODES</a>{.reference
.internal}</p>
</li>
<li>
<p><a href="index.html#std-setting-HTTPCACHE_IGNORE_MISSING">HTTPCACHE_IGNORE_MISSING</a>{.reference
.internal}</p>
</li>
<li>
<p><a href="index.html#std-setting-HTTPCACHE_IGNORE_RESPONSE_CACHE_CONTROLS">HTTPCACHE_IGNORE_RESPONSE_CACHE_CONTROLS</a>{.reference
.internal}</p>
</li>
<li>
<p><a href="index.html#std-setting-HTTPCACHE_IGNORE_SCHEMES">HTTPCACHE_IGNORE_SCHEMES</a>{.reference
.internal}</p>
</li>
<li>
<p><a href="index.html#std-setting-HTTPCACHE_POLICY">HTTPCACHE_POLICY</a>{.reference
.internal}</p>
</li>
<li>
<p><a href="index.html#std-setting-HTTPCACHE_STORAGE">HTTPCACHE_STORAGE</a>{.reference
.internal}</p>
</li>
<li>
<p><a href="index.html#std-setting-HTTPERROR_ALLOWED_CODES">HTTPERROR_ALLOWED_CODES</a>{.reference
.internal}</p>
</li>
<li>
<p><a href="index.html#std-setting-HTTPERROR_ALLOW_ALL">HTTPERROR_ALLOW_ALL</a>{.reference
.internal}</p>
</li>
<li>
<p><a href="index.html#std-setting-HTTPPROXY_AUTH_ENCODING">HTTPPROXY_AUTH_ENCODING</a>{.reference
.internal}</p>
</li>
<li>
<p><a href="index.html#std-setting-HTTPPROXY_ENABLED">HTTPPROXY_ENABLED</a>{.reference
.internal}</p>
</li>
<li>
<p><a href="index.html#std-setting-IMAGES_EXPIRES">IMAGES_EXPIRES</a>{.reference
.internal}</p>
</li>
<li>
<p><a href="index.html#std-setting-IMAGES_MIN_HEIGHT">IMAGES_MIN_HEIGHT</a>{.reference
.internal}</p>
</li>
<li>
<p><a href="index.html#std-setting-IMAGES_MIN_WIDTH">IMAGES_MIN_WIDTH</a>{.reference
.internal}</p>
</li>
<li>
<p><a href="index.html#std-setting-IMAGES_RESULT_FIELD">IMAGES_RESULT_FIELD</a>{.reference
.internal}</p>
</li>
<li>
<p><a href="index.html#std-setting-IMAGES_STORE">IMAGES_STORE</a>{.reference
.internal}</p>
</li>
<li>
<p><a href="index.html#std-setting-IMAGES_STORE_GCS_ACL">IMAGES_STORE_GCS_ACL</a>{.reference
.internal}</p>
</li>
<li>
<p><a href="index.html#std-setting-IMAGES_STORE_S3_ACL">IMAGES_STORE_S3_ACL</a>{.reference
.internal}</p>
</li>
<li>
<p><a href="index.html#std-setting-IMAGES_THUMBS">IMAGES_THUMBS</a>{.reference
.internal}</p>
</li>
<li>
<p><a href="index.html#std-setting-IMAGES_URLS_FIELD">IMAGES_URLS_FIELD</a>{.reference
.internal}</p>
</li>
<li>
<p><a href="index.html#std-setting-ITEM_PIPELINES">ITEM_PIPELINES</a>{.reference
.internal}</p>
</li>
<li>
<p><a href="index.html#std-setting-ITEM_PIPELINES_BASE">ITEM_PIPELINES_BASE</a>{.reference
.internal}</p>
</li>
<li>
<p><a href="index.html#std-setting-JOBDIR">JOBDIR</a>{.reference .internal}</p>
</li>
<li>
<p><a href="index.html#std-setting-LOGSTATS_INTERVAL">LOGSTATS_INTERVAL</a>{.reference
.internal}</p>
</li>
<li>
<p><a href="index.html#std-setting-LOG_DATEFORMAT">LOG_DATEFORMAT</a>{.reference
.internal}</p>
</li>
<li>
<p><a href="index.html#std-setting-LOG_ENABLED">LOG_ENABLED</a>{.reference
.internal}</p>
</li>
<li>
<p><a href="index.html#std-setting-LOG_ENCODING">LOG_ENCODING</a>{.reference
.internal}</p>
</li>
<li>
<p><a href="index.html#std-setting-LOG_FILE">LOG_FILE</a>{.reference .internal}</p>
</li>
<li>
<p><a href="index.html#std-setting-LOG_FILE_APPEND">LOG_FILE_APPEND</a>{.reference
.internal}</p>
</li>
<li>
<p><a href="index.html#std-setting-LOG_FORMAT">LOG_FORMAT</a>{.reference
.internal}</p>
</li>
<li>
<p><a href="index.html#std-setting-LOG_FORMATTER">LOG_FORMATTER</a>{.reference
.internal}</p>
</li>
<li>
<p><a href="index.html#std-setting-LOG_LEVEL">LOG_LEVEL</a>{.reference .internal}</p>
</li>
<li>
<p><a href="index.html#std-setting-LOG_SHORT_NAMES">LOG_SHORT_NAMES</a>{.reference
.internal}</p>
</li>
<li>
<p><a href="index.html#std-setting-LOG_STDOUT">LOG_STDOUT</a>{.reference
.internal}</p>
</li>
<li>
<p><a href="index.html#std-setting-MAIL_FROM">MAIL_FROM</a>{.reference .internal}</p>
</li>
<li>
<p><a href="index.html#std-setting-MAIL_HOST">MAIL_HOST</a>{.reference .internal}</p>
</li>
<li>
<p><a href="index.html#std-setting-MAIL_PASS">MAIL_PASS</a>{.reference .internal}</p>
</li>
<li>
<p><a href="index.html#std-setting-MAIL_PORT">MAIL_PORT</a>{.reference .internal}</p>
</li>
<li>
<p><a href="index.html#std-setting-MAIL_SSL">MAIL_SSL</a>{.reference .internal}</p>
</li>
<li>
<p><a href="index.html#std-setting-MAIL_TLS">MAIL_TLS</a>{.reference .internal}</p>
</li>
<li>
<p><a href="index.html#std-setting-MAIL_USER">MAIL_USER</a>{.reference .internal}</p>
</li>
<li>
<p><a href="index.html#std-setting-MEDIA_ALLOW_REDIRECTS">MEDIA_ALLOW_REDIRECTS</a>{.reference
.internal}</p>
</li>
<li>
<p><a href="index.html#std-setting-MEMDEBUG_ENABLED">MEMDEBUG_ENABLED</a>{.reference
.internal}</p>
</li>
<li>
<p><a href="index.html#std-setting-MEMDEBUG_NOTIFY">MEMDEBUG_NOTIFY</a>{.reference
.internal}</p>
</li>
<li>
<p><a href="index.html#std-setting-MEMUSAGE_CHECK_INTERVAL_SECONDS">MEMUSAGE_CHECK_INTERVAL_SECONDS</a>{.reference
.internal}</p>
</li>
<li>
<p><a href="index.html#std-setting-MEMUSAGE_ENABLED">MEMUSAGE_ENABLED</a>{.reference
.internal}</p>
</li>
<li>
<p><a href="index.html#std-setting-MEMUSAGE_LIMIT_MB">MEMUSAGE_LIMIT_MB</a>{.reference
.internal}</p>
</li>
<li>
<p><a href="index.html#std-setting-MEMUSAGE_NOTIFY_MAIL">MEMUSAGE_NOTIFY_MAIL</a>{.reference
.internal}</p>
</li>
<li>
<p><a href="index.html#std-setting-MEMUSAGE_WARNING_MB">MEMUSAGE_WARNING_MB</a>{.reference
.internal}</p>
</li>
<li>
<p><a href="index.html#std-setting-METAREFRESH_ENABLED">METAREFRESH_ENABLED</a>{.reference
.internal}</p>
</li>
<li>
<p><a href="index.html#std-setting-METAREFRESH_IGNORE_TAGS">METAREFRESH_IGNORE_TAGS</a>{.reference
.internal}</p>
</li>
<li>
<p><a href="index.html#std-setting-METAREFRESH_MAXDELAY">METAREFRESH_MAXDELAY</a>{.reference
.internal}</p>
</li>
<li>
<p><a href="index.html#std-setting-NEWSPIDER_MODULE">NEWSPIDER_MODULE</a>{.reference
.internal}</p>
</li>
<li>
<p><a href="index.html#std-setting-PERIODIC_LOG_DELTA">PERIODIC_LOG_DELTA</a>{.reference
.internal}</p>
</li>
<li>
<p><a href="index.html#std-setting-PERIODIC_LOG_STATS">PERIODIC_LOG_STATS</a>{.reference
.internal}</p>
</li>
<li>
<p><a href="index.html#std-setting-PERIODIC_LOG_TIMING_ENABLED">PERIODIC_LOG_TIMING_ENABLED</a>{.reference
.internal}</p>
</li>
<li>
<p><a href="index.html#std-setting-RANDOMIZE_DOWNLOAD_DELAY">RANDOMIZE_DOWNLOAD_DELAY</a>{.reference
.internal}</p>
</li>
<li>
<p><a href="index.html#std-setting-REACTOR_THREADPOOL_MAXSIZE">REACTOR_THREADPOOL_MAXSIZE</a>{.reference
.internal}</p>
</li>
<li>
<p><a href="index.html#std-setting-REDIRECT_ENABLED">REDIRECT_ENABLED</a>{.reference
.internal}</p>
</li>
<li>
<p><a href="index.html#std-setting-REDIRECT_MAX_TIMES">REDIRECT_MAX_TIMES</a>{.reference
.internal}</p>
</li>
<li>
<p><a href="index.html#std-setting-REDIRECT_PRIORITY_ADJUST">REDIRECT_PRIORITY_ADJUST</a>{.reference
.internal}</p>
</li>
<li>
<p><a href="index.html#std-setting-REFERER_ENABLED">REFERER_ENABLED</a>{.reference
.internal}</p>
</li>
<li>
<p><a href="index.html#std-setting-REFERRER_POLICY">REFERRER_POLICY</a>{.reference
.internal}</p>
</li>
<li>
<p><a href="index.html#std-setting-REQUEST_FINGERPRINTER_CLASS">REQUEST_FINGERPRINTER_CLASS</a>{.reference
.internal}</p>
</li>
<li>
<p><a href="index.html#std-setting-REQUEST_FINGERPRINTER_IMPLEMENTATION">REQUEST_FINGERPRINTER_IMPLEMENTATION</a>{.reference
.internal}</p>
</li>
<li>
<p><a href="index.html#std-setting-RETRY_ENABLED">RETRY_ENABLED</a>{.reference
.internal}</p>
</li>
<li>
<p><a href="index.html#std-setting-RETRY_EXCEPTIONS">RETRY_EXCEPTIONS</a>{.reference
.internal}</p>
</li>
<li>
<p><a href="index.html#std-setting-RETRY_HTTP_CODES">RETRY_HTTP_CODES</a>{.reference
.internal}</p>
</li>
<li>
<p><a href="index.html#std-setting-RETRY_PRIORITY_ADJUST">RETRY_PRIORITY_ADJUST</a>{.reference
.internal}</p>
</li>
<li>
<p><a href="index.html#std-setting-RETRY_TIMES">RETRY_TIMES</a>{.reference
.internal}</p>
</li>
<li>
<p><a href="index.html#std-setting-ROBOTSTXT_OBEY">ROBOTSTXT_OBEY</a>{.reference
.internal}</p>
</li>
<li>
<p><a href="index.html#std-setting-ROBOTSTXT_PARSER">ROBOTSTXT_PARSER</a>{.reference
.internal}</p>
</li>
<li>
<p><a href="index.html#std-setting-ROBOTSTXT_USER_AGENT">ROBOTSTXT_USER_AGENT</a>{.reference
.internal}</p>
</li>
<li>
<p><a href="index.html#std-setting-SCHEDULER">SCHEDULER</a>{.reference .internal}</p>
</li>
<li>
<p><a href="index.html#std-setting-SCHEDULER_DEBUG">SCHEDULER_DEBUG</a>{.reference
.internal}</p>
</li>
<li>
<p><a href="index.html#std-setting-SCHEDULER_DISK_QUEUE">SCHEDULER_DISK_QUEUE</a>{.reference
.internal}</p>
</li>
<li>
<p><a href="index.html#std-setting-SCHEDULER_MEMORY_QUEUE">SCHEDULER_MEMORY_QUEUE</a>{.reference
.internal}</p>
</li>
<li>
<p><a href="index.html#std-setting-SCHEDULER_PRIORITY_QUEUE">SCHEDULER_PRIORITY_QUEUE</a>{.reference
.internal}</p>
</li>
<li>
<p><a href="index.html#std-setting-SCRAPER_SLOT_MAX_ACTIVE_SIZE">SCRAPER_SLOT_MAX_ACTIVE_SIZE</a>{.reference
.internal}</p>
</li>
<li>
<p><a href="index.html#std-setting-SPIDER_CONTRACTS">SPIDER_CONTRACTS</a>{.reference
.internal}</p>
</li>
<li>
<p><a href="index.html#std-setting-SPIDER_CONTRACTS_BASE">SPIDER_CONTRACTS_BASE</a>{.reference
.internal}</p>
</li>
<li>
<p><a href="index.html#std-setting-SPIDER_LOADER_CLASS">SPIDER_LOADER_CLASS</a>{.reference
.internal}</p>
</li>
<li>
<p><a href="index.html#std-setting-SPIDER_LOADER_WARN_ONLY">SPIDER_LOADER_WARN_ONLY</a>{.reference
.internal}</p>
</li>
<li>
<p><a href="index.html#std-setting-SPIDER_MIDDLEWARES">SPIDER_MIDDLEWARES</a>{.reference
.internal}</p>
</li>
<li>
<p><a href="index.html#std-setting-SPIDER_MIDDLEWARES_BASE">SPIDER_MIDDLEWARES_BASE</a>{.reference
.internal}</p>
</li>
<li>
<p><a href="index.html#std-setting-SPIDER_MODULES">SPIDER_MODULES</a>{.reference
.internal}</p>
</li>
<li>
<p><a href="index.html#std-setting-STATSMAILER_RCPTS">STATSMAILER_RCPTS</a>{.reference
.internal}</p>
</li>
<li>
<p><a href="index.html#std-setting-STATS_CLASS">STATS_CLASS</a>{.reference
.internal}</p>
</li>
<li>
<p><a href="index.html#std-setting-STATS_DUMP">STATS_DUMP</a>{.reference
.internal}</p>
</li>
<li>
<p><a href="index.html#std-setting-TELNETCONSOLE_ENABLED">TELNETCONSOLE_ENABLED</a>{.reference
.internal}</p>
</li>
<li>
<p><a href="index.html#std-setting-TELNETCONSOLE_HOST">TELNETCONSOLE_HOST</a>{.reference
.internal}</p>
</li>
<li>
<p><a href="index.html#std-setting-TELNETCONSOLE_PASSWORD">TELNETCONSOLE_PASSWORD</a>{.reference
.internal}</p>
</li>
<li>
<p><a href="index.html#std-setting-TELNETCONSOLE_PORT">TELNETCONSOLE_PORT</a>{.reference
.internal}</p>
</li>
<li>
<p><a href="index.html#std-setting-TELNETCONSOLE_USERNAME">TELNETCONSOLE_USERNAME</a>{.reference
.internal}</p>
</li>
<li>
<p><a href="index.html#std-setting-TEMPLATES_DIR">TEMPLATES_DIR</a>{.reference
.internal}</p>
</li>
<li>
<p><a href="index.html#std-setting-TWISTED_REACTOR">TWISTED_REACTOR</a>{.reference
.internal}</p>
</li>
<li>
<p><a href="index.html#std-setting-URLLENGTH_LIMIT">URLLENGTH_LIMIT</a>{.reference
.internal}</p>
</li>
<li>
<p><a href="index.html#std-setting-USER_AGENT">USER_AGENT</a>{.reference
.internal}
:::
:::
:::</p>
</li>
</ul>
<p>[]{#document-topics/exceptions}</p>
<p>::: {#module-scrapy.exceptions .section}
[]{#exceptions}[]{#topics-exceptions}</p>
<h3 id="exceptionsheaderlink"><a class="header" href="#exceptionsheaderlink">Exceptions<a href="#module-scrapy.exceptions" title="Permalink to this heading">¶</a>{.headerlink}</a></h3>
<p>::: {#built-in-exceptions-reference .section}
[]{#topics-exceptions-ref}</p>
<h4 id="built-in-exceptions-referenceheaderlink"><a class="header" href="#built-in-exceptions-referenceheaderlink">Built-in Exceptions reference<a href="#built-in-exceptions-reference" title="Permalink to this heading">¶</a>{.headerlink}</a></h4>
<p>Here's a list of all exceptions included in Scrapy and their usage.</p>
<p>::: {#closespider .section}</p>
<h5 id="closespiderheaderlink"><a class="header" href="#closespiderheaderlink">CloseSpider<a href="#closespider" title="Permalink to this heading">¶</a>{.headerlink}</a></h5>
<p><em>[exception]{.pre}[ ]{.w}</em>[[scrapy.exceptions.]{.pre}]{.sig-prename .descclassname}[[CloseSpider]{.pre}]{.sig-name .descname}[(]{.sig-paren}<em>[[reason]{.pre}]{.n}[[=]{.pre}]{.o}[['cancelled']{.pre}]{.default_value}</em>[)]{.sig-paren}<a href="_modules/scrapy/exceptions.html#CloseSpider">[[[source]]{.pre}]{.viewcode-link}</a>{.reference .internal}<a href="#scrapy.exceptions.CloseSpider" title="Permalink to this definition">¶</a>{.headerlink}</p>
<p>:   This exception can be raised from a spider callback to request the
spider to be closed/stopped. Supported arguments:</p>
<pre><code>Parameters

:   **reason**
    ([*str*](https://docs.python.org/3/library/stdtypes.html#str &quot;(in Python v3.12)&quot;){.reference
    .external}) -- the reason for closing
</code></pre>
<p>For example:</p>
<p>::: {.highlight-python .notranslate}
::: highlight
def parse_page(self, response):
if &quot;Bandwidth exceeded&quot; in response.body:
raise CloseSpider(&quot;bandwidth_exceeded&quot;)
:::
:::
:::</p>
<p>::: {#dontclosespider .section}</p>
<h5 id="dontclosespiderheaderlink"><a class="header" href="#dontclosespiderheaderlink">DontCloseSpider<a href="#dontclosespider" title="Permalink to this heading">¶</a>{.headerlink}</a></h5>
<p><em>[exception]{.pre}[ ]{.w}</em>[[scrapy.exceptions.]{.pre}]{.sig-prename .descclassname}[[DontCloseSpider]{.pre}]{.sig-name .descname}<a href="_modules/scrapy/exceptions.html#DontCloseSpider">[[[source]]{.pre}]{.viewcode-link}</a>{.reference .internal}<a href="#scrapy.exceptions.DontCloseSpider" title="Permalink to this definition">¶</a>{.headerlink}</p>
<p>:</p>
<p>This exception can be raised in a <a href="index.html#std-signal-spider_idle">[<code>spider_idle</code>{.xref .std .std-signal
.docutils .literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal} signal handler to prevent the spider from
being closed.
:::</p>
<p>::: {#dropitem .section}</p>
<h5 id="dropitemheaderlink"><a class="header" href="#dropitemheaderlink">DropItem<a href="#dropitem" title="Permalink to this heading">¶</a>{.headerlink}</a></h5>
<p><em>[exception]{.pre}[ ]{.w}</em>[[scrapy.exceptions.]{.pre}]{.sig-prename .descclassname}[[DropItem]{.pre}]{.sig-name .descname}<a href="_modules/scrapy/exceptions.html#DropItem">[[[source]]{.pre}]{.viewcode-link}</a>{.reference .internal}<a href="#scrapy.exceptions.DropItem" title="Permalink to this definition">¶</a>{.headerlink}</p>
<p>:</p>
<p>The exception that must be raised by item pipeline stages to stop
processing an Item. For more information see <a href="index.html#topics-item-pipeline">[Item Pipeline]{.std
.std-ref}</a>{.hoverxref .tooltip
.reference .internal}.
:::</p>
<p>::: {#ignorerequest .section}</p>
<h5 id="ignorerequestheaderlink"><a class="header" href="#ignorerequestheaderlink">IgnoreRequest<a href="#ignorerequest" title="Permalink to this heading">¶</a>{.headerlink}</a></h5>
<p><em>[exception]{.pre}[ ]{.w}</em>[[scrapy.exceptions.]{.pre}]{.sig-prename .descclassname}[[IgnoreRequest]{.pre}]{.sig-name .descname}<a href="_modules/scrapy/exceptions.html#IgnoreRequest">[[[source]]{.pre}]{.viewcode-link}</a>{.reference .internal}<a href="#scrapy.exceptions.IgnoreRequest" title="Permalink to this definition">¶</a>{.headerlink}</p>
<p>:</p>
<p>This exception can be raised by the Scheduler or any downloader
middleware to indicate that the request should be ignored.
:::</p>
<p>::: {#notconfigured .section}</p>
<h5 id="notconfiguredheaderlink"><a class="header" href="#notconfiguredheaderlink">NotConfigured<a href="#notconfigured" title="Permalink to this heading">¶</a>{.headerlink}</a></h5>
<p><em>[exception]{.pre}[ ]{.w}</em>[[scrapy.exceptions.]{.pre}]{.sig-prename .descclassname}[[NotConfigured]{.pre}]{.sig-name .descname}<a href="_modules/scrapy/exceptions.html#NotConfigured">[[[source]]{.pre}]{.viewcode-link}</a>{.reference .internal}<a href="#scrapy.exceptions.NotConfigured" title="Permalink to this definition">¶</a>{.headerlink}</p>
<p>:</p>
<p>This exception can be raised by some components to indicate that they
will remain disabled. Those components include:</p>
<ul>
<li>
<p>Extensions</p>
</li>
<li>
<p>Item pipelines</p>
</li>
<li>
<p>Downloader middlewares</p>
</li>
<li>
<p>Spider middlewares</p>
</li>
</ul>
<p>The exception must be raised in the component's [<code>__init__</code>{.docutils
.literal .notranslate}]{.pre} method.
:::</p>
<p>::: {#notsupported .section}</p>
<h5 id="notsupportedheaderlink"><a class="header" href="#notsupportedheaderlink">NotSupported<a href="#notsupported" title="Permalink to this heading">¶</a>{.headerlink}</a></h5>
<p><em>[exception]{.pre}[ ]{.w}</em>[[scrapy.exceptions.]{.pre}]{.sig-prename .descclassname}[[NotSupported]{.pre}]{.sig-name .descname}<a href="_modules/scrapy/exceptions.html#NotSupported">[[[source]]{.pre}]{.viewcode-link}</a>{.reference .internal}<a href="#scrapy.exceptions.NotSupported" title="Permalink to this definition">¶</a>{.headerlink}</p>
<p>:</p>
<p>This exception is raised to indicate an unsupported feature.
:::</p>
<p>::: {#stopdownload .section}</p>
<h5 id="stopdownloadheaderlink"><a class="header" href="#stopdownloadheaderlink">StopDownload<a href="#stopdownload" title="Permalink to this heading">¶</a>{.headerlink}</a></h5>
<p>::: versionadded
[New in version 2.2.]{.versionmodified .added}
:::</p>
<p><em>[exception]{.pre}[ ]{.w}</em>[[scrapy.exceptions.]{.pre}]{.sig-prename .descclassname}[[StopDownload]{.pre}]{.sig-name .descname}[(]{.sig-paren}<em>[[fail]{.pre}]{.n}[[=]{.pre}]{.o}[[True]{.pre}]{.default_value}</em>[)]{.sig-paren}<a href="_modules/scrapy/exceptions.html#StopDownload">[[[source]]{.pre}]{.viewcode-link}</a>{.reference .internal}<a href="#scrapy.exceptions.StopDownload" title="Permalink to this definition">¶</a>{.headerlink}</p>
<p>:</p>
<p>Raised from a <a href="index.html#scrapy.signals.bytes_received" title="scrapy.signals.bytes_received">[<code>bytes_received</code>{.xref .py .py-class .docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal} or <a href="index.html#scrapy.signals.headers_received" title="scrapy.signals.headers_received">[<code>headers_received</code>{.xref .py .py-class .docutils
.literal
.notranslate}]{.pre}</a>{.reference
.internal} signal handler to indicate that no further bytes should be
downloaded for a response.</p>
<p>The [<code>fail</code>{.docutils .literal .notranslate}]{.pre} boolean parameter
controls which method will handle the resulting response:</p>
<ul>
<li>
<p>If [<code>fail=True</code>{.docutils .literal .notranslate}]{.pre} (default),
the request errback is called. The response object is available as
the [<code>response</code>{.docutils .literal .notranslate}]{.pre} attribute of
the [<code>StopDownload</code>{.docutils .literal .notranslate}]{.pre}
exception, which is in turn stored as the [<code>value</code>{.docutils
.literal .notranslate}]{.pre} attribute of the received
<a href="https://docs.twisted.org/en/stable/api/twisted.python.failure.Failure.html" title="(in Twisted)">[<code>Failure</code>{.xref .py .py-class .docutils .literal
.notranslate}]{.pre}</a>{.reference
.external} object. This means that in an errback defined as
[<code>def</code>{.docutils .literal .notranslate}]{.pre}<code> </code>{.docutils .literal
.notranslate}[<code>errback(self,</code>{.docutils .literal
.notranslate}]{.pre}<code> </code>{.docutils .literal
.notranslate}[<code>failure)</code>{.docutils .literal .notranslate}]{.pre},
the response can be accessed though
[<code>failure.value.response</code>{.docutils .literal .notranslate}]{.pre}.</p>
</li>
<li>
<p>If [<code>fail=False</code>{.docutils .literal .notranslate}]{.pre}, the
request callback is called instead.</p>
</li>
</ul>
<p>In both cases, the response could have its body truncated: the body
contains all bytes received up until the exception is raised, including
the bytes received in the signal handler that raises the exception.
Also, the response object is marked with [<code>&quot;download_stopped&quot;</code>{.docutils
.literal .notranslate}]{.pre} in its [<code>Response.flags</code>{.xref .py
.py-attr .docutils .literal .notranslate}]{.pre} attribute.</p>
<p>::: {.admonition .note}
Note</p>
<p>[<code>fail</code>{.docutils .literal .notranslate}]{.pre} is a keyword-only
parameter, i.e. raising [<code>StopDownload(False)</code>{.docutils .literal
.notranslate}]{.pre} or [<code>StopDownload(True)</code>{.docutils .literal
.notranslate}]{.pre} will raise a <a href="https://docs.python.org/3/library/exceptions.html#TypeError" title="(in Python v3.12)">[<code>TypeError</code>{.xref .py .py-class
.docutils .literal
.notranslate}]{.pre}</a>{.reference
.external}.
:::</p>
<p>See the documentation for the <a href="index.html#scrapy.signals.bytes_received" title="scrapy.signals.bytes_received">[<code>bytes_received</code>{.xref .py .py-class
.docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal} and <a href="index.html#scrapy.signals.headers_received" title="scrapy.signals.headers_received">[<code>headers_received</code>{.xref .py .py-class .docutils
.literal
.notranslate}]{.pre}</a>{.reference
.internal} signals and the <a href="index.html#topics-stop-response-download">[Stopping the download of a Response]{.std
.std-ref}</a>{.hoverxref .tooltip
.reference .internal} topic for additional information and examples.
:::
:::
:::
:::</p>
<p><a href="index.html#document-topics/commands">[Command line tool]{.doc}</a>{.reference .internal}</p>
<p>:   Learn about the command-line tool used to manage your Scrapy
project.</p>
<p><a href="index.html#document-topics/spiders">[Spiders]{.doc}</a>{.reference .internal}</p>
<p>:   Write the rules to crawl your websites.</p>
<p><a href="index.html#document-topics/selectors">[Selectors]{.doc}</a>{.reference .internal}</p>
<p>:   Extract the data from web pages using XPath.</p>
<p><a href="index.html#document-topics/shell">[Scrapy shell]{.doc}</a>{.reference .internal}</p>
<p>:   Test your extraction code in an interactive environment.</p>
<p><a href="index.html#document-topics/items">[Items]{.doc}</a>{.reference .internal}</p>
<p>:   Define the data you want to scrape.</p>
<p><a href="index.html#document-topics/loaders">[Item Loaders]{.doc}</a>{.reference .internal}</p>
<p>:   Populate your items with the extracted data.</p>
<p><a href="index.html#document-topics/item-pipeline">[Item Pipeline]{.doc}</a>{.reference .internal}</p>
<p>:   Post-process and store your scraped data.</p>
<p><a href="index.html#document-topics/feed-exports">[Feed exports]{.doc}</a>{.reference .internal}</p>
<p>:   Output your scraped data using different formats and storages.</p>
<p><a href="index.html#document-topics/request-response">[Requests and Responses]{.doc}</a>{.reference .internal}</p>
<p>:   Understand the classes used to represent HTTP requests and
responses.</p>
<p><a href="index.html#document-topics/link-extractors">[Link Extractors]{.doc}</a>{.reference .internal}</p>
<p>:   Convenient classes to extract links to follow from pages.</p>
<p><a href="index.html#document-topics/settings">[Settings]{.doc}</a>{.reference .internal}</p>
<p>:   Learn how to configure Scrapy and see all <a href="index.html#topics-settings-ref">[available settings]{.std
.std-ref}</a>{.hoverxref .tooltip
.reference .internal}.</p>
<p><a href="index.html#document-topics/exceptions">[Exceptions]{.doc}</a>{.reference .internal}</p>
<p>:   See all available exceptions and their meaning.
:::</p>
<p>::: {#built-in-services .section}</p>
<h2 id="built-in-servicesheaderlink"><a class="header" href="#built-in-servicesheaderlink">Built-in services<a href="#built-in-services" title="Permalink to this heading">¶</a>{.headerlink}</a></h2>
<p>::: {.toctree-wrapper .compound}
[]{#document-topics/logging}</p>
<p>::: {#logging .section}
[]{#topics-logging}</p>
<h3 id="loggingheaderlink"><a class="header" href="#loggingheaderlink">Logging<a href="#logging" title="Permalink to this heading">¶</a>{.headerlink}</a></h3>
<p>::: {.admonition .note}
Note</p>
<p>[<code>scrapy.log</code>{.xref .py .py-mod .docutils .literal .notranslate}]{.pre}
has been deprecated alongside its functions in favor of explicit calls
to the Python standard logging. Keep reading to learn more about the new
logging system.
:::</p>
<p>Scrapy uses <a href="https://docs.python.org/3/library/logging.html#module-logging" title="(in Python v3.12)">[<code>logging</code>{.xref .py .py-mod .docutils .literal
.notranslate}]{.pre}</a>{.reference
.external} for event logging. We'll provide some simple examples to get
you started, but for more advanced use-cases it's strongly suggested to
read thoroughly its documentation.</p>
<p>Logging works out of the box, and can be configured to some extent with
the Scrapy settings listed in <a href="#topics-logging-settings">[Logging settings]{.std
.std-ref}</a>{.hoverxref .tooltip .reference
.internal}.</p>
<p>Scrapy calls <a href="#scrapy.utils.log.configure_logging" title="scrapy.utils.log.configure_logging">[<code>scrapy.utils.log.configure_logging()</code>{.xref .py .py-func
.docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal} to set some reasonable defaults and handle those settings in
<a href="#topics-logging-settings">[Logging settings]{.std .std-ref}</a>{.hoverxref
.tooltip .reference .internal} when running commands, so it's
recommended to manually call it if you're running Scrapy from scripts as
described in <a href="index.html#run-from-script">[Run Scrapy from a script]{.std
.std-ref}</a>{.hoverxref .tooltip .reference
.internal}.</p>
<p>::: {#log-levels .section}
[]{#topics-logging-levels}</p>
<h4 id="log-levelsheaderlink"><a class="header" href="#log-levelsheaderlink">Log levels<a href="#log-levels" title="Permalink to this heading">¶</a>{.headerlink}</a></h4>
<p>Python's builtin logging defines 5 different levels to indicate the
severity of a given log message. Here are the standard ones, listed in
decreasing order:</p>
<ol>
<li>
<p>[<code>logging.CRITICAL</code>{.docutils .literal .notranslate}]{.pre} - for
critical errors (highest severity)</p>
</li>
<li>
<p>[<code>logging.ERROR</code>{.docutils .literal .notranslate}]{.pre} - for
regular errors</p>
</li>
<li>
<p>[<code>logging.WARNING</code>{.docutils .literal .notranslate}]{.pre} - for
warning messages</p>
</li>
<li>
<p>[<code>logging.INFO</code>{.docutils .literal .notranslate}]{.pre} - for
informational messages</p>
</li>
<li>
<p>[<code>logging.DEBUG</code>{.docutils .literal .notranslate}]{.pre} - for
debugging messages (lowest severity)
:::</p>
</li>
</ol>
<p>::: {#how-to-log-messages .section}</p>
<h4 id="how-to-log-messagesheaderlink"><a class="header" href="#how-to-log-messagesheaderlink">How to log messages<a href="#how-to-log-messages" title="Permalink to this heading">¶</a>{.headerlink}</a></h4>
<p>Here's a quick example of how to log a message using the
[<code>logging.WARNING</code>{.docutils .literal .notranslate}]{.pre} level:</p>
<p>::: {.highlight-python .notranslate}
::: highlight
import logging</p>
<pre><code>logging.warning(&quot;This is a warning&quot;)
</code></pre>
<p>:::
:::</p>
<p>There are shortcuts for issuing log messages on any of the standard 5
levels, and there's also a general [<code>logging.log</code>{.docutils .literal
.notranslate}]{.pre} method which takes a given level as argument. If
needed, the last example could be rewritten as:</p>
<p>::: {.highlight-python .notranslate}
::: highlight
import logging</p>
<pre><code>logging.log(logging.WARNING, &quot;This is a warning&quot;)
</code></pre>
<p>:::
:::</p>
<p>On top of that, you can create different &quot;loggers&quot; to encapsulate
messages. (For example, a common practice is to create different loggers
for every module). These loggers can be configured independently, and
they allow hierarchical constructions.</p>
<p>The previous examples use the root logger behind the scenes, which is a
top level logger where all messages are propagated to (unless otherwise
specified). Using [<code>logging</code>{.docutils .literal .notranslate}]{.pre}
helpers is merely a shortcut for getting the root logger explicitly, so
this is also an equivalent of the last snippets:</p>
<p>::: {.highlight-python .notranslate}
::: highlight
import logging</p>
<pre><code>logger = logging.getLogger()
logger.warning(&quot;This is a warning&quot;)
</code></pre>
<p>:::
:::</p>
<p>You can use a different logger just by getting its name with the
[<code>logging.getLogger</code>{.docutils .literal .notranslate}]{.pre} function:</p>
<p>::: {.highlight-python .notranslate}
::: highlight
import logging</p>
<pre><code>logger = logging.getLogger(&quot;mycustomlogger&quot;)
logger.warning(&quot;This is a warning&quot;)
</code></pre>
<p>:::
:::</p>
<p>Finally, you can ensure having a custom logger for any module you're
working on by using the [<code>__name__</code>{.docutils .literal
.notranslate}]{.pre} variable, which is populated with current module's
path:</p>
<p>::: {.highlight-python .notranslate}
::: highlight
import logging</p>
<pre><code>logger = logging.getLogger(__name__)
logger.warning(&quot;This is a warning&quot;)
</code></pre>
<p>:::
:::</p>
<p>::: {.admonition .seealso}
See also</p>
<p>Module logging, <a href="https://docs.python.org/3/howto/logging.html" title="(in Python v3.12)">[HowTo]{.xref .std .std-doc}</a>{.reference .external}</p>
<p>:   Basic Logging Tutorial</p>
<p>Module logging, <a href="https://docs.python.org/3/library/logging.html#logger" title="(in Python v3.12)">[Loggers]{.xref .std .std-ref}</a>{.reference .external}</p>
<p>:   Further documentation on loggers
:::
:::</p>
<p>::: {#logging-from-spiders .section}
[]{#topics-logging-from-spiders}</p>
<h4 id="logging-from-spidersheaderlink"><a class="header" href="#logging-from-spidersheaderlink">Logging from Spiders<a href="#logging-from-spiders" title="Permalink to this heading">¶</a>{.headerlink}</a></h4>
<p>Scrapy provides a <a href="index.html#scrapy.Spider.logger" title="scrapy.Spider.logger">[<code>logger</code>{.xref .py .py-data .docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal} within each Spider instance, which can be accessed and used
like this:</p>
<p>::: {.highlight-python .notranslate}
::: highlight
import scrapy</p>
<pre><code>class MySpider(scrapy.Spider):
    name = &quot;myspider&quot;
    start_urls = [&quot;https://scrapy.org&quot;]

    def parse(self, response):
        self.logger.info(&quot;Parse function called on %s&quot;, response.url)
</code></pre>
<p>:::
:::</p>
<p>That logger is created using the Spider's name, but you can use any
custom Python logger you want. For example:</p>
<p>::: {.highlight-python .notranslate}
::: highlight
import logging
import scrapy</p>
<pre><code>logger = logging.getLogger(&quot;mycustomlogger&quot;)


class MySpider(scrapy.Spider):
    name = &quot;myspider&quot;
    start_urls = [&quot;https://scrapy.org&quot;]

    def parse(self, response):
        logger.info(&quot;Parse function called on %s&quot;, response.url)
</code></pre>
<p>:::
:::
:::</p>
<p>::: {#logging-configuration .section}
[]{#topics-logging-configuration}</p>
<h4 id="logging-configurationheaderlink"><a class="header" href="#logging-configurationheaderlink">Logging configuration<a href="#logging-configuration" title="Permalink to this heading">¶</a>{.headerlink}</a></h4>
<p>Loggers on their own don't manage how messages sent through them are
displayed. For this task, different &quot;handlers&quot; can be attached to any
logger instance and they will redirect those messages to appropriate
destinations, such as the standard output, files, emails, etc.</p>
<p>By default, Scrapy sets and configures a handler for the root logger,
based on the settings below.</p>
<p>::: {#logging-settings .section}
[]{#topics-logging-settings}</p>
<h5 id="logging-settingsheaderlink"><a class="header" href="#logging-settingsheaderlink">Logging settings<a href="#logging-settings" title="Permalink to this heading">¶</a>{.headerlink}</a></h5>
<p>These settings can be used to configure the logging:</p>
<ul>
<li>
<p><a href="index.html#std-setting-LOG_FILE">[<code>LOG_FILE</code>{.xref .std .std-setting .docutils .literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal}</p>
</li>
<li>
<p><a href="index.html#std-setting-LOG_FILE_APPEND">[<code>LOG_FILE_APPEND</code>{.xref .std .std-setting .docutils .literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal}</p>
</li>
<li>
<p><a href="index.html#std-setting-LOG_ENABLED">[<code>LOG_ENABLED</code>{.xref .std .std-setting .docutils .literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal}</p>
</li>
<li>
<p><a href="index.html#std-setting-LOG_ENCODING">[<code>LOG_ENCODING</code>{.xref .std .std-setting .docutils .literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal}</p>
</li>
<li>
<p><a href="index.html#std-setting-LOG_LEVEL">[<code>LOG_LEVEL</code>{.xref .std .std-setting .docutils .literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal}</p>
</li>
<li>
<p><a href="index.html#std-setting-LOG_FORMAT">[<code>LOG_FORMAT</code>{.xref .std .std-setting .docutils .literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal}</p>
</li>
<li>
<p><a href="index.html#std-setting-LOG_DATEFORMAT">[<code>LOG_DATEFORMAT</code>{.xref .std .std-setting .docutils .literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal}</p>
</li>
<li>
<p><a href="index.html#std-setting-LOG_STDOUT">[<code>LOG_STDOUT</code>{.xref .std .std-setting .docutils .literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal}</p>
</li>
<li>
<p><a href="index.html#std-setting-LOG_SHORT_NAMES">[<code>LOG_SHORT_NAMES</code>{.xref .std .std-setting .docutils .literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal}</p>
</li>
</ul>
<p>The first couple of settings define a destination for log messages. If
<a href="index.html#std-setting-LOG_FILE">[<code>LOG_FILE</code>{.xref .std .std-setting .docutils .literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal} is set, messages sent through the root
logger will be redirected to a file named <a href="index.html#std-setting-LOG_FILE">[<code>LOG_FILE</code>{.xref .std
.std-setting .docutils .literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal} with encoding <a href="index.html#std-setting-LOG_ENCODING">[<code>LOG_ENCODING</code>{.xref .std
.std-setting .docutils .literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal}. If unset and <a href="index.html#std-setting-LOG_ENABLED">[<code>LOG_ENABLED</code>{.xref .std
.std-setting .docutils .literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal} is [<code>True</code>{.docutils .literal
.notranslate}]{.pre}, log messages will be displayed on the standard
error. If <a href="index.html#std-setting-LOG_FILE">[<code>LOG_FILE</code>{.xref .std .std-setting .docutils .literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal} is set and <a href="index.html#std-setting-LOG_FILE_APPEND">[<code>LOG_FILE_APPEND</code>{.xref .std
.std-setting .docutils .literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal} is [<code>False</code>{.docutils .literal
.notranslate}]{.pre}, the file will be overwritten (discarding the
output from previous runs, if any). Lastly, if <a href="index.html#std-setting-LOG_ENABLED">[<code>LOG_ENABLED</code>{.xref
.std .std-setting .docutils .literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal} is [<code>False</code>{.docutils .literal
.notranslate}]{.pre}, there won't be any visible log output.</p>
<p><a href="index.html#std-setting-LOG_LEVEL">[<code>LOG_LEVEL</code>{.xref .std .std-setting .docutils .literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal} determines the minimum level of severity
to display, those messages with lower severity will be filtered out. It
ranges through the possible levels listed in <a href="#topics-logging-levels">[Log levels]{.std
.std-ref}</a>{.hoverxref .tooltip .reference
.internal}.</p>
<p><a href="index.html#std-setting-LOG_FORMAT">[<code>LOG_FORMAT</code>{.xref .std .std-setting .docutils .literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal} and <a href="index.html#std-setting-LOG_DATEFORMAT">[<code>LOG_DATEFORMAT</code>{.xref .std
.std-setting .docutils .literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal} specify formatting strings used as
layouts for all messages. Those strings can contain any placeholders
listed in <a href="https://docs.python.org/3/library/logging.html#logrecord-attributes" title="(in Python v3.12)">[logging's logrecord attributes docs]{.xref .std
.std-ref}</a>{.reference
.external} and <a href="https://docs.python.org/3/library/datetime.html#strftime-strptime-behavior" title="(in Python v3.12)">[datetime's strftime and strptime directives]{.xref .std
.std-ref}</a>{.reference
.external} respectively.</p>
<p>If <a href="index.html#std-setting-LOG_SHORT_NAMES">[<code>LOG_SHORT_NAMES</code>{.xref .std .std-setting .docutils .literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal} is set, then the logs will not display
the Scrapy component that prints the log. It is unset by default, hence
logs contain the Scrapy component responsible for that log output.
:::</p>
<p>::: {#command-line-options .section}</p>
<h5 id="command-line-optionsheaderlink"><a class="header" href="#command-line-optionsheaderlink">Command-line options<a href="#command-line-options" title="Permalink to this heading">¶</a>{.headerlink}</a></h5>
<p>There are command-line arguments, available for all commands, that you
can use to override some of the Scrapy settings regarding logging.</p>
<ul>
<li></li>
</ul>
<pre><code>[`--logfile`{.docutils .literal .notranslate}]{.pre}` `{.docutils .literal .notranslate}[`FILE`{.docutils .literal .notranslate}]{.pre}

:   Overrides [[`LOG_FILE`{.xref .std .std-setting .docutils
    .literal
    .notranslate}]{.pre}](index.html#std-setting-LOG_FILE){.hoverxref
    .tooltip .reference .internal}
</code></pre>
<ul>
<li></li>
</ul>
<pre><code>[`--loglevel/-L`{.docutils .literal .notranslate}]{.pre}` `{.docutils .literal .notranslate}[`LEVEL`{.docutils .literal .notranslate}]{.pre}

:   Overrides [[`LOG_LEVEL`{.xref .std .std-setting .docutils
    .literal
    .notranslate}]{.pre}](index.html#std-setting-LOG_LEVEL){.hoverxref
    .tooltip .reference .internal}
</code></pre>
<ul>
<li></li>
</ul>
<pre><code>[`--nolog`{.docutils .literal .notranslate}]{.pre}

:   Sets [[`LOG_ENABLED`{.xref .std .std-setting .docutils .literal
    .notranslate}]{.pre}](index.html#std-setting-LOG_ENABLED){.hoverxref
    .tooltip .reference .internal} to [`False`{.docutils .literal
    .notranslate}]{.pre}
</code></pre>
<p>::: {.admonition .seealso}
See also</p>
<p>Module <a href="https://docs.python.org/3/library/logging.handlers.html#module-logging.handlers" title="(in Python v3.12)">[<code>logging.handlers</code>{.xref .py .py-mod .docutils .literal .notranslate}]{.pre}</a>{.reference .external}</p>
<p>:   Further documentation on available handlers
:::
:::</p>
<p>::: {#custom-log-formats .section}
[]{#id1}</p>
<h5 id="custom-log-formatsheaderlink"><a class="header" href="#custom-log-formatsheaderlink">Custom Log Formats<a href="#custom-log-formats" title="Permalink to this heading">¶</a>{.headerlink}</a></h5>
<p>A custom log format can be set for different actions by extending
<a href="#scrapy.logformatter.LogFormatter" title="scrapy.logformatter.LogFormatter">[<code>LogFormatter</code>{.xref .py .py-class .docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal} class and making <a href="index.html#std-setting-LOG_FORMATTER">[<code>LOG_FORMATTER</code>{.xref .std .std-setting
.docutils .literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal} point to your new class.</p>
<p><em>[class]{.pre}[ ]{.w}</em>[[scrapy.logformatter.]{.pre}]{.sig-prename .descclassname}[[LogFormatter]{.pre}]{.sig-name .descname}<a href="_modules/scrapy/logformatter.html#LogFormatter">[[[source]]{.pre}]{.viewcode-link}</a>{.reference .internal}<a href="#scrapy.logformatter.LogFormatter" title="Permalink to this definition">¶</a>{.headerlink}</p>
<p>:   Class for generating log messages for different actions.</p>
<pre><code>All methods must return a dictionary listing the parameters
[`level`{.docutils .literal .notranslate}]{.pre}, [`msg`{.docutils
.literal .notranslate}]{.pre} and [`args`{.docutils .literal
.notranslate}]{.pre} which are going to be used for constructing the
log message when calling [`logging.log`{.docutils .literal
.notranslate}]{.pre}.

Dictionary keys for the method outputs:

-   [`level`{.docutils .literal .notranslate}]{.pre} is the log
    level for that action, you can use those from the [python
    logging
    library](https://docs.python.org/3/library/logging.html){.reference
    .external} : [`logging.DEBUG`{.docutils .literal
    .notranslate}]{.pre}, [`logging.INFO`{.docutils .literal
    .notranslate}]{.pre}, [`logging.WARNING`{.docutils .literal
    .notranslate}]{.pre}, [`logging.ERROR`{.docutils .literal
    .notranslate}]{.pre} and [`logging.CRITICAL`{.docutils .literal
    .notranslate}]{.pre}.

-   [`msg`{.docutils .literal .notranslate}]{.pre} should be a
    string that can contain different formatting placeholders. This
    string, formatted with the provided [`args`{.docutils .literal
    .notranslate}]{.pre}, is going to be the long message for that
    action.

-   [`args`{.docutils .literal .notranslate}]{.pre} should be a
    tuple or dict with the formatting placeholders for
    [`msg`{.docutils .literal .notranslate}]{.pre}. The final log
    message is computed as [`msg`{.docutils .literal
    .notranslate}]{.pre}` `{.docutils .literal
    .notranslate}[`%`{.docutils .literal
    .notranslate}]{.pre}` `{.docutils .literal
    .notranslate}[`args`{.docutils .literal .notranslate}]{.pre}.

Users can define their own [`LogFormatter`{.docutils .literal
.notranslate}]{.pre} class if they want to customize how each action
is logged or if they want to omit it entirely. In order to omit
logging an action the method must return [`None`{.docutils .literal
.notranslate}]{.pre}.

Here is an example on how to create a custom log formatter to lower
the severity level of the log message when an item is dropped from
the pipeline:

::: {.highlight-default .notranslate}
::: highlight
    class PoliteLogFormatter(logformatter.LogFormatter):
        def dropped(self, item, exception, response, spider):
            return {
                'level': logging.INFO, # lowering the level from logging.WARNING
                'msg': &quot;Dropped: %(exception)s&quot; + os.linesep + &quot;%(item)s&quot;,
                'args': {
                    'exception': exception,
                    'item': item,
                }
            }
:::
:::

[[crawled]{.pre}]{.sig-name .descname}[(]{.sig-paren}*[[request]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[[[Request]{.pre}](index.html#scrapy.http.Request &quot;scrapy.http.request.Request&quot;){.reference .internal}]{.n}*, *[[response]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[[[Response]{.pre}](index.html#scrapy.http.Response &quot;scrapy.http.response.Response&quot;){.reference .internal}]{.n}*, *[[spider]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[[[Spider]{.pre}](index.html#scrapy.spiders.Spider &quot;scrapy.spiders.Spider&quot;){.reference .internal}]{.n}*[)]{.sig-paren} [[→]{.sig-return-icon} [[[dict]{.pre}](https://docs.python.org/3/library/stdtypes.html#dict &quot;(in Python v3.12)&quot;){.reference .external}]{.sig-return-typehint}]{.sig-return}[[[\[source\]]{.pre}]{.viewcode-link}](_modules/scrapy/logformatter.html#LogFormatter.crawled){.reference .internal}[¶](#scrapy.logformatter.LogFormatter.crawled &quot;Permalink to this definition&quot;){.headerlink}

:   Logs a message when the crawler finds a webpage.

[[download_error]{.pre}]{.sig-name .descname}[(]{.sig-paren}*[[failure]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[[[Failure]{.pre}](https://docs.twisted.org/en/stable/api/twisted.python.failure.Failure.html &quot;(in Twisted)&quot;){.reference .external}]{.n}*, *[[request]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[[[Request]{.pre}](index.html#scrapy.http.Request &quot;scrapy.http.request.Request&quot;){.reference .internal}]{.n}*, *[[spider]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[[[Spider]{.pre}](index.html#scrapy.spiders.Spider &quot;scrapy.spiders.Spider&quot;){.reference .internal}]{.n}*, *[[errmsg]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[[[Optional]{.pre}](https://docs.python.org/3/library/typing.html#typing.Optional &quot;(in Python v3.12)&quot;){.reference .external}[[\[]{.pre}]{.p}[[str]{.pre}](https://docs.python.org/3/library/stdtypes.html#str &quot;(in Python v3.12)&quot;){.reference .external}[[\]]{.pre}]{.p}]{.n}[ ]{.w}[[=]{.pre}]{.o}[ ]{.w}[[None]{.pre}]{.default_value}*[)]{.sig-paren} [[→]{.sig-return-icon} [[[dict]{.pre}](https://docs.python.org/3/library/stdtypes.html#dict &quot;(in Python v3.12)&quot;){.reference .external}]{.sig-return-typehint}]{.sig-return}[[[\[source\]]{.pre}]{.viewcode-link}](_modules/scrapy/logformatter.html#LogFormatter.download_error){.reference .internal}[¶](#scrapy.logformatter.LogFormatter.download_error &quot;Permalink to this definition&quot;){.headerlink}

:   Logs a download error message from a spider (typically coming
    from the engine).

    ::: versionadded
    [New in version 2.0.]{.versionmodified .added}
    :::

[[dropped]{.pre}]{.sig-name .descname}[(]{.sig-paren}*[[item]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[[[Any]{.pre}](https://docs.python.org/3/library/typing.html#typing.Any &quot;(in Python v3.12)&quot;){.reference .external}]{.n}*, *[[exception]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[[[BaseException]{.pre}](https://docs.python.org/3/library/exceptions.html#BaseException &quot;(in Python v3.12)&quot;){.reference .external}]{.n}*, *[[response]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[[[Response]{.pre}](index.html#scrapy.http.Response &quot;scrapy.http.response.Response&quot;){.reference .internal}]{.n}*, *[[spider]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[[[Spider]{.pre}](index.html#scrapy.spiders.Spider &quot;scrapy.spiders.Spider&quot;){.reference .internal}]{.n}*[)]{.sig-paren} [[→]{.sig-return-icon} [[[dict]{.pre}](https://docs.python.org/3/library/stdtypes.html#dict &quot;(in Python v3.12)&quot;){.reference .external}]{.sig-return-typehint}]{.sig-return}[[[\[source\]]{.pre}]{.viewcode-link}](_modules/scrapy/logformatter.html#LogFormatter.dropped){.reference .internal}[¶](#scrapy.logformatter.LogFormatter.dropped &quot;Permalink to this definition&quot;){.headerlink}

:   Logs a message when an item is dropped while it is passing
    through the item pipeline.

[[item_error]{.pre}]{.sig-name .descname}[(]{.sig-paren}*[[item]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[[[Any]{.pre}](https://docs.python.org/3/library/typing.html#typing.Any &quot;(in Python v3.12)&quot;){.reference .external}]{.n}*, *[[exception]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[[[BaseException]{.pre}](https://docs.python.org/3/library/exceptions.html#BaseException &quot;(in Python v3.12)&quot;){.reference .external}]{.n}*, *[[response]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[[[Response]{.pre}](index.html#scrapy.http.Response &quot;scrapy.http.response.Response&quot;){.reference .internal}]{.n}*, *[[spider]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[[[Spider]{.pre}](index.html#scrapy.spiders.Spider &quot;scrapy.spiders.Spider&quot;){.reference .internal}]{.n}*[)]{.sig-paren} [[→]{.sig-return-icon} [[[dict]{.pre}](https://docs.python.org/3/library/stdtypes.html#dict &quot;(in Python v3.12)&quot;){.reference .external}]{.sig-return-typehint}]{.sig-return}[[[\[source\]]{.pre}]{.viewcode-link}](_modules/scrapy/logformatter.html#LogFormatter.item_error){.reference .internal}[¶](#scrapy.logformatter.LogFormatter.item_error &quot;Permalink to this definition&quot;){.headerlink}

:   Logs a message when an item causes an error while it is passing
    through the item pipeline.

    ::: versionadded
    [New in version 2.0.]{.versionmodified .added}
    :::

[[scraped]{.pre}]{.sig-name .descname}[(]{.sig-paren}*[[item]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[[[Any]{.pre}](https://docs.python.org/3/library/typing.html#typing.Any &quot;(in Python v3.12)&quot;){.reference .external}]{.n}*, *[[response]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[[[Union]{.pre}](https://docs.python.org/3/library/typing.html#typing.Union &quot;(in Python v3.12)&quot;){.reference .external}[[\[]{.pre}]{.p}[[Response]{.pre}](index.html#scrapy.http.Response &quot;scrapy.http.response.Response&quot;){.reference .internal}[[,]{.pre}]{.p}[ ]{.w}[[Failure]{.pre}](https://docs.twisted.org/en/stable/api/twisted.python.failure.Failure.html &quot;(in Twisted)&quot;){.reference .external}[[\]]{.pre}]{.p}]{.n}*, *[[spider]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[[[Spider]{.pre}](index.html#scrapy.spiders.Spider &quot;scrapy.spiders.Spider&quot;){.reference .internal}]{.n}*[)]{.sig-paren} [[→]{.sig-return-icon} [[[dict]{.pre}](https://docs.python.org/3/library/stdtypes.html#dict &quot;(in Python v3.12)&quot;){.reference .external}]{.sig-return-typehint}]{.sig-return}[[[\[source\]]{.pre}]{.viewcode-link}](_modules/scrapy/logformatter.html#LogFormatter.scraped){.reference .internal}[¶](#scrapy.logformatter.LogFormatter.scraped &quot;Permalink to this definition&quot;){.headerlink}

:   Logs a message when an item is scraped by a spider.

[[spider_error]{.pre}]{.sig-name .descname}[(]{.sig-paren}*[[failure]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[[[Failure]{.pre}](https://docs.twisted.org/en/stable/api/twisted.python.failure.Failure.html &quot;(in Twisted)&quot;){.reference .external}]{.n}*, *[[request]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[[[Request]{.pre}](index.html#scrapy.http.Request &quot;scrapy.http.request.Request&quot;){.reference .internal}]{.n}*, *[[response]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[[[Union]{.pre}](https://docs.python.org/3/library/typing.html#typing.Union &quot;(in Python v3.12)&quot;){.reference .external}[[\[]{.pre}]{.p}[[Response]{.pre}](index.html#scrapy.http.Response &quot;scrapy.http.response.Response&quot;){.reference .internal}[[,]{.pre}]{.p}[ ]{.w}[[Failure]{.pre}](https://docs.twisted.org/en/stable/api/twisted.python.failure.Failure.html &quot;(in Twisted)&quot;){.reference .external}[[\]]{.pre}]{.p}]{.n}*, *[[spider]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[[[Spider]{.pre}](index.html#scrapy.spiders.Spider &quot;scrapy.spiders.Spider&quot;){.reference .internal}]{.n}*[)]{.sig-paren} [[→]{.sig-return-icon} [[[dict]{.pre}](https://docs.python.org/3/library/stdtypes.html#dict &quot;(in Python v3.12)&quot;){.reference .external}]{.sig-return-typehint}]{.sig-return}[[[\[source\]]{.pre}]{.viewcode-link}](_modules/scrapy/logformatter.html#LogFormatter.spider_error){.reference .internal}[¶](#scrapy.logformatter.LogFormatter.spider_error &quot;Permalink to this definition&quot;){.headerlink}

:   Logs an error message from a spider.

    ::: versionadded
    [New in version 2.0.]{.versionmodified .added}
    :::
</code></pre>
<p>:::</p>
<p>::: {#advanced-customization .section}
[]{#topics-logging-advanced-customization}</p>
<h5 id="advanced-customizationheaderlink"><a class="header" href="#advanced-customizationheaderlink">Advanced customization<a href="#advanced-customization" title="Permalink to this heading">¶</a>{.headerlink}</a></h5>
<p>Because Scrapy uses stdlib logging module, you can customize logging
using all features of stdlib logging.</p>
<p>For example, let's say you're scraping a website which returns many HTTP
404 and 500 responses, and you want to hide all messages like this:</p>
<p>::: {.highlight-default .notranslate}
::: highlight
2016-12-16 22:00:06 [scrapy.spidermiddlewares.httperror] INFO: Ignoring
response &lt;500 https://quotes.toscrape.com/page/1-34/&gt;: HTTP status code
is not handled or not allowed
:::
:::</p>
<p>The first thing to note is a logger name - it is in brackets:
[<code>[scrapy.spidermiddlewares.httperror]</code>{.docutils .literal
.notranslate}]{.pre}. If you get just [<code>[scrapy]</code>{.docutils .literal
.notranslate}]{.pre} then <a href="index.html#std-setting-LOG_SHORT_NAMES">[<code>LOG_SHORT_NAMES</code>{.xref .std .std-setting
.docutils .literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal} is likely set to True; set it to False
and re-run the crawl.</p>
<p>Next, we can see that the message has INFO level. To hide it we should
set logging level for [<code>scrapy.spidermiddlewares.httperror</code>{.docutils
.literal .notranslate}]{.pre} higher than INFO; next level after INFO is
WARNING. It could be done e.g. in the spider's [<code>__init__</code>{.docutils
.literal .notranslate}]{.pre} method:</p>
<p>::: {.highlight-python .notranslate}
::: highlight
import logging
import scrapy</p>
<pre><code>class MySpider(scrapy.Spider):
    # ...
    def __init__(self, *args, **kwargs):
        logger = logging.getLogger(&quot;scrapy.spidermiddlewares.httperror&quot;)
        logger.setLevel(logging.WARNING)
        super().__init__(*args, **kwargs)
</code></pre>
<p>:::
:::</p>
<p>If you run this spider again then INFO messages from
[<code>scrapy.spidermiddlewares.httperror</code>{.docutils .literal
.notranslate}]{.pre} logger will be gone.</p>
<p>You can also filter log records by <a href="https://docs.python.org/3/library/logging.html#logging.LogRecord" title="(in Python v3.12)">[<code>LogRecord</code>{.xref .py .py-class
.docutils .literal
.notranslate}]{.pre}</a>{.reference
.external} data. For example, you can filter log records by message
content using a substring or a regular expression. Create a
<a href="https://docs.python.org/3/library/logging.html#logging.Filter" title="(in Python v3.12)">[<code>logging.Filter</code>{.xref .py .py-class .docutils .literal
.notranslate}]{.pre}</a>{.reference
.external} subclass and equip it with a regular expression pattern to
filter out unwanted messages:</p>
<p>::: {.highlight-python .notranslate}
::: highlight
import logging
import re</p>
<pre><code>class ContentFilter(logging.Filter):
    def filter(self, record):
        match = re.search(r&quot;\d{3} [Ee]rror, retrying&quot;, record.message)
        if match:
            return False
</code></pre>
<p>:::
:::</p>
<p>A project-level filter may be attached to the root handler created by
Scrapy, this is a wieldy way to filter all loggers in different parts of
the project (middlewares, spider, etc.):</p>
<p>::: {.highlight-python .notranslate}
::: highlight
import logging
import scrapy</p>
<pre><code>class MySpider(scrapy.Spider):
    # ...
    def __init__(self, *args, **kwargs):
        for handler in logging.root.handlers:
            handler.addFilter(ContentFilter())
</code></pre>
<p>:::
:::</p>
<p>Alternatively, you may choose a specific logger and hide it without
affecting other loggers:</p>
<p>::: {.highlight-python .notranslate}
::: highlight
import logging
import scrapy</p>
<pre><code>class MySpider(scrapy.Spider):
    # ...
    def __init__(self, *args, **kwargs):
        logger = logging.getLogger(&quot;my_logger&quot;)
        logger.addFilter(ContentFilter())
</code></pre>
<p>:::
:::
:::
:::</p>
<p>::: {#module-scrapy.utils.log .section}
[]{#scrapy-utils-log-module}</p>
<h4 id="scrapyutilslog-moduleheaderlink"><a class="header" href="#scrapyutilslog-moduleheaderlink">scrapy.utils.log module<a href="#module-scrapy.utils.log" title="Permalink to this heading">¶</a>{.headerlink}</a></h4>
<p>[[scrapy.utils.log.]{.pre}]{.sig-prename .descclassname}[[configure_logging]{.pre}]{.sig-name .descname}[(]{.sig-paren}<em>[[settings]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[<a href="https://docs.python.org/3/library/typing.html#typing.Optional" title="(in Python v3.12)">[Optional]{.pre}</a>{.reference .external}[[[]{.pre}]{.p}<a href="https://docs.python.org/3/library/typing.html#typing.Union" title="(in Python v3.12)">[Union]{.pre}</a>{.reference .external}[[[]{.pre}]{.p}<a href="index.html#scrapy.settings.Settings" title="scrapy.settings.Settings">[Settings]{.pre}</a>{.reference .internal}[[,]{.pre}]{.p}[ ]{.w}<a href="https://docs.python.org/3/library/stdtypes.html#dict" title="(in Python v3.12)">[dict]{.pre}</a>{.reference .external}[[]]{.pre}]{.p}[[]]{.pre}]{.p}]{.n}[ ]{.w}[[=]{.pre}]{.o}[ ]{.w}[[None]{.pre}]{.default_value}</em>, <em>[[install_root_handler]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[<a href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.12)">[bool]{.pre}</a>{.reference .external}]{.n}[ ]{.w}[[=]{.pre}]{.o}[ ]{.w}[[True]{.pre}]{.default_value}</em>[)]{.sig-paren} [[→]{.sig-return-icon} [<a href="https://docs.python.org/3/library/constants.html#None" title="(in Python v3.12)">[None]{.pre}</a>{.reference .external}]{.sig-return-typehint}]{.sig-return}<a href="_modules/scrapy/utils/log.html#configure_logging">[[[source]]{.pre}]{.viewcode-link}</a>{.reference .internal}<a href="#scrapy.utils.log.configure_logging" title="Permalink to this definition">¶</a>{.headerlink}</p>
<p>:   Initialize logging defaults for Scrapy.</p>
<pre><code>Parameters

:   -   **settings** (dict, [[`Settings`{.xref .py .py-class
        .docutils .literal
        .notranslate}]{.pre}](index.html#scrapy.settings.Settings &quot;scrapy.settings.Settings&quot;){.reference
        .internal} object or [`None`{.docutils .literal
        .notranslate}]{.pre}) -- settings used to create and
        configure a handler for the root logger (default: None).

    -   **install_root_handler**
        ([*bool*](https://docs.python.org/3/library/functions.html#bool &quot;(in Python v3.12)&quot;){.reference
        .external}) -- whether to install root logging handler
        (default: True)

This function does:

-   Route warnings and twisted logging through Python standard
    logging

-   Assign DEBUG and ERROR level to Scrapy and Twisted loggers
    respectively

-   Route stdout to log if LOG_STDOUT setting is True

When [`install_root_handler`{.docutils .literal .notranslate}]{.pre}
is True (default), this function also creates a handler for the root
logger according to given settings (see [[Logging settings]{.std
.std-ref}](#topics-logging-settings){.hoverxref .tooltip .reference
.internal}). You can override default options using
[`settings`{.docutils .literal .notranslate}]{.pre} argument. When
[`settings`{.docutils .literal .notranslate}]{.pre} is empty or
None, defaults are used.

[`configure_logging`{.docutils .literal .notranslate}]{.pre} is
automatically called when using Scrapy commands or
[[`CrawlerProcess`{.xref .py .py-class .docutils .literal
.notranslate}]{.pre}](index.html#scrapy.crawler.CrawlerProcess &quot;scrapy.crawler.CrawlerProcess&quot;){.reference
.internal}, but needs to be called explicitly when running custom
scripts using [[`CrawlerRunner`{.xref .py .py-class .docutils
.literal
.notranslate}]{.pre}](index.html#scrapy.crawler.CrawlerRunner &quot;scrapy.crawler.CrawlerRunner&quot;){.reference
.internal}. In that case, its usage is not required but it's
recommended.

Another option when running custom scripts is to manually configure
the logging. To do this you can use [[`logging.basicConfig()`{.xref
.py .py-func .docutils .literal
.notranslate}]{.pre}](https://docs.python.org/3/library/logging.html#logging.basicConfig &quot;(in Python v3.12)&quot;){.reference
.external} to set a basic root handler.

Note that [[`CrawlerProcess`{.xref .py .py-class .docutils .literal
.notranslate}]{.pre}](index.html#scrapy.crawler.CrawlerProcess &quot;scrapy.crawler.CrawlerProcess&quot;){.reference
.internal} automatically calls [`configure_logging`{.docutils
.literal .notranslate}]{.pre}, so it is recommended to only use
[[`logging.basicConfig()`{.xref .py .py-func .docutils .literal
.notranslate}]{.pre}](https://docs.python.org/3/library/logging.html#logging.basicConfig &quot;(in Python v3.12)&quot;){.reference
.external} together with [[`CrawlerRunner`{.xref .py .py-class
.docutils .literal
.notranslate}]{.pre}](index.html#scrapy.crawler.CrawlerRunner &quot;scrapy.crawler.CrawlerRunner&quot;){.reference
.internal}.

This is an example on how to redirect [`INFO`{.docutils .literal
.notranslate}]{.pre} or higher messages to a file:

::: {.highlight-python .notranslate}
::: highlight
    import logging

    logging.basicConfig(
        filename=&quot;log.txt&quot;, format=&quot;%(levelname)s: %(message)s&quot;, level=logging.INFO
    )
:::
:::

Refer to [[Run Scrapy from a script]{.std
.std-ref}](index.html#run-from-script){.hoverxref .tooltip
.reference .internal} for more details about using Scrapy this way.
</code></pre>
<p>:::
:::</p>
<p>[]{#document-topics/stats}</p>
<p>::: {#stats-collection .section}
[]{#topics-stats}</p>
<h3 id="stats-collectionheaderlink"><a class="header" href="#stats-collectionheaderlink">Stats Collection<a href="#stats-collection" title="Permalink to this heading">¶</a>{.headerlink}</a></h3>
<p>Scrapy provides a convenient facility for collecting stats in the form
of key/values, where values are often counters. The facility is called
the Stats Collector, and can be accessed through the <a href="index.html#scrapy.crawler.Crawler.stats" title="scrapy.crawler.Crawler.stats">[<code>stats</code>{.xref .py
.py-attr .docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal} attribute of the <a href="index.html#topics-api-crawler">[Crawler API]{.std
.std-ref}</a>{.hoverxref .tooltip .reference
.internal}, as illustrated by the examples in the <a href="#topics-stats-usecases">[Common Stats
Collector uses]{.std .std-ref}</a>{.hoverxref
.tooltip .reference .internal} section below.</p>
<p>However, the Stats Collector is always available, so you can always
import it in your module and use its API (to increment or set new stat
keys), regardless of whether the stats collection is enabled or not. If
it's disabled, the API will still work but it won't collect anything.
This is aimed at simplifying the stats collector usage: you should spend
no more than one line of code for collecting stats in your spider,
Scrapy extension, or whatever code you're using the Stats Collector
from.</p>
<p>Another feature of the Stats Collector is that it's very efficient (when
enabled) and extremely efficient (almost unnoticeable) when disabled.</p>
<p>The Stats Collector keeps a stats table per open spider which is
automatically opened when the spider is opened, and closed when the
spider is closed.</p>
<p>::: {#common-stats-collector-uses .section}
[]{#topics-stats-usecases}</p>
<h4 id="common-stats-collector-usesheaderlink"><a class="header" href="#common-stats-collector-usesheaderlink">Common Stats Collector uses<a href="#common-stats-collector-uses" title="Permalink to this heading">¶</a>{.headerlink}</a></h4>
<p>Access the stats collector through the <a href="index.html#scrapy.crawler.Crawler.stats" title="scrapy.crawler.Crawler.stats">[<code>stats</code>{.xref .py .py-attr
.docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal} attribute. Here is an example of an extension that access
stats:</p>
<p>::: {.highlight-python .notranslate}
::: highlight
class ExtensionThatAccessStats:
def <strong>init</strong>(self, stats):
self.stats = stats</p>
<pre><code>    @classmethod
    def from_crawler(cls, crawler):
        return cls(crawler.stats)
</code></pre>
<p>:::
:::</p>
<p>Set stat value:</p>
<p>::: {.highlight-python .notranslate}
::: highlight
stats.set_value(&quot;hostname&quot;, socket.gethostname())
:::
:::</p>
<p>Increment stat value:</p>
<p>::: {.highlight-python .notranslate}
::: highlight
stats.inc_value(&quot;custom_count&quot;)
:::
:::</p>
<p>Set stat value only if greater than previous:</p>
<p>::: {.highlight-python .notranslate}
::: highlight
stats.max_value(&quot;max_items_scraped&quot;, value)
:::
:::</p>
<p>Set stat value only if lower than previous:</p>
<p>::: {.highlight-python .notranslate}
::: highlight
stats.min_value(&quot;min_free_memory_percent&quot;, value)
:::
:::</p>
<p>Get stat value:</p>
<p>::: {.highlight-pycon .notranslate}
::: highlight
&gt;&gt;&gt; stats.get_value(&quot;custom_count&quot;)
1
:::
:::</p>
<p>Get all stats:</p>
<p>::: {.highlight-pycon .notranslate}
::: highlight
&gt;&gt;&gt; stats.get_stats()
{'custom_count': 1, 'start_time': datetime.datetime(2009, 7, 14, 21, 47, 28, 977139)}
:::
:::
:::</p>
<p>::: {#available-stats-collectors .section}</p>
<h4 id="available-stats-collectorsheaderlink"><a class="header" href="#available-stats-collectorsheaderlink">Available Stats Collectors<a href="#available-stats-collectors" title="Permalink to this heading">¶</a>{.headerlink}</a></h4>
<p>Besides the basic [<code>StatsCollector</code>{.xref .py .py-class .docutils
.literal .notranslate}]{.pre} there are other Stats Collectors available
in Scrapy which extend the basic Stats Collector. You can select which
Stats Collector to use through the <a href="index.html#std-setting-STATS_CLASS">[<code>STATS_CLASS</code>{.xref .std
.std-setting .docutils .literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal} setting. The default Stats Collector used
is the [<code>MemoryStatsCollector</code>{.xref .py .py-class .docutils .literal
.notranslate}]{.pre}.</p>
<p>::: {#memorystatscollector .section}</p>
<h5 id="memorystatscollectorheaderlink"><a class="header" href="#memorystatscollectorheaderlink">MemoryStatsCollector<a href="#memorystatscollector" title="Permalink to this heading">¶</a>{.headerlink}</a></h5>
<p><em>[class]{.pre}[ ]{.w}</em>[[scrapy.statscollectors.]{.pre}]{.sig-prename .descclassname}[[MemoryStatsCollector]{.pre}]{.sig-name .descname}<a href="_modules/scrapy/statscollectors.html#MemoryStatsCollector">[[[source]]{.pre}]{.viewcode-link}</a>{.reference .internal}<a href="#scrapy.statscollectors.MemoryStatsCollector" title="Permalink to this definition">¶</a>{.headerlink}</p>
<p>:   A simple stats collector that keeps the stats of the last scraping
run (for each spider) in memory, after they're closed. The stats can
be accessed through the <a href="#scrapy.statscollectors.MemoryStatsCollector.spider_stats" title="scrapy.statscollectors.MemoryStatsCollector.spider_stats">[<code>spider_stats</code>{.xref .py .py-attr
.docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal} attribute, which is a dict keyed by spider domain name.</p>
<pre><code>This is the default Stats Collector used in Scrapy.

[[spider_stats]{.pre}]{.sig-name .descname}[¶](#scrapy.statscollectors.MemoryStatsCollector.spider_stats &quot;Permalink to this definition&quot;){.headerlink}

:   A dict of dicts (keyed by spider name) containing the stats of
    the last scraping run for each spider.
</code></pre>
<p>:::</p>
<p>::: {#dummystatscollector .section}</p>
<h5 id="dummystatscollectorheaderlink"><a class="header" href="#dummystatscollectorheaderlink">DummyStatsCollector<a href="#dummystatscollector" title="Permalink to this heading">¶</a>{.headerlink}</a></h5>
<p><em>[class]{.pre}[ ]{.w}</em>[[scrapy.statscollectors.]{.pre}]{.sig-prename .descclassname}[[DummyStatsCollector]{.pre}]{.sig-name .descname}<a href="_modules/scrapy/statscollectors.html#DummyStatsCollector">[[[source]]{.pre}]{.viewcode-link}</a>{.reference .internal}<a href="#scrapy.statscollectors.DummyStatsCollector" title="Permalink to this definition">¶</a>{.headerlink}</p>
<p>:   A Stats collector which does nothing but is very efficient (because
it does nothing). This stats collector can be set via the
<a href="index.html#std-setting-STATS_CLASS">[<code>STATS_CLASS</code>{.xref .std .std-setting .docutils .literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal} setting, to disable stats collect in
order to improve performance. However, the performance penalty of
stats collection is usually marginal compared to other Scrapy
workload like parsing pages.
:::
:::
:::</p>
<p>[]{#document-topics/email}</p>
<p>::: {#module-scrapy.mail .section}
[]{#sending-e-mail}[]{#topics-email}</p>
<h3 id="sending-e-mailheaderlink"><a class="header" href="#sending-e-mailheaderlink">Sending e-mail<a href="#module-scrapy.mail" title="Permalink to this heading">¶</a>{.headerlink}</a></h3>
<p>Although Python makes sending e-mails relatively easy via the
<a href="https://docs.python.org/3/library/smtplib.html#module-smtplib" title="(in Python v3.12)">[<code>smtplib</code>{.xref .py .py-mod .docutils .literal
.notranslate}]{.pre}</a>{.reference
.external} library, Scrapy provides its own facility for sending e-mails
which is very easy to use and it's implemented using <a href="https://docs.twisted.org/en/stable/core/howto/defer-intro.html" title="(in Twisted v23.10)">[Twisted
non-blocking IO]{.xref .std
.std-doc}</a>{.reference
.external}, to avoid interfering with the non-blocking IO of the
crawler. It also provides a simple API for sending attachments and it's
very easy to configure, with a few <a href="#topics-email-settings">[settings]{.std
.std-ref}</a>{.hoverxref .tooltip .reference
.internal}.</p>
<p>::: {#quick-example .section}</p>
<h4 id="quick-exampleheaderlink"><a class="header" href="#quick-exampleheaderlink">Quick example<a href="#quick-example" title="Permalink to this heading">¶</a>{.headerlink}</a></h4>
<p>There are two ways to instantiate the mail sender. You can instantiate
it using the standard [<code>__init__</code>{.docutils .literal
.notranslate}]{.pre} method:</p>
<p>::: {.highlight-python .notranslate}
::: highlight
from scrapy.mail import MailSender</p>
<pre><code>mailer = MailSender()
</code></pre>
<p>:::
:::</p>
<p>Or you can instantiate it passing a Scrapy settings object, which will
respect the <a href="#topics-email-settings">[settings]{.std
.std-ref}</a>{.hoverxref .tooltip .reference
.internal}:</p>
<p>::: {.highlight-python .notranslate}
::: highlight
mailer = MailSender.from_settings(settings)
:::
:::</p>
<p>And here is how to use it to send an e-mail (without attachments):</p>
<p>::: {.highlight-python .notranslate}
::: highlight
mailer.send(
to=[&quot;someone@example.com&quot;],
subject=&quot;Some subject&quot;,
body=&quot;Some body&quot;,
cc=[&quot;another@example.com&quot;],
)
:::
:::
:::</p>
<p>::: {#mailsender-class-reference .section}</p>
<h4 id="mailsender-class-referenceheaderlink"><a class="header" href="#mailsender-class-referenceheaderlink">MailSender class reference<a href="#mailsender-class-reference" title="Permalink to this heading">¶</a>{.headerlink}</a></h4>
<p>MailSender is the preferred class to use for sending emails from Scrapy,
as it uses <a href="https://docs.twisted.org/en/stable/core/howto/defer-intro.html" title="(in Twisted v23.10)">[Twisted non-blocking IO]{.xref .std
.std-doc}</a>{.reference
.external}, like the rest of the framework.</p>
<p><em>[class]{.pre}[ ]{.w}</em>[[scrapy.mail.]{.pre}]{.sig-prename .descclassname}[[MailSender]{.pre}]{.sig-name .descname}[(]{.sig-paren}<em>[[smtphost]{.pre}]{.n}[[=]{.pre}]{.o}[[None]{.pre}]{.default_value}</em>, <em>[[mailfrom]{.pre}]{.n}[[=]{.pre}]{.o}[[None]{.pre}]{.default_value}</em>, <em>[[smtpuser]{.pre}]{.n}[[=]{.pre}]{.o}[[None]{.pre}]{.default_value}</em>, <em>[[smtppass]{.pre}]{.n}[[=]{.pre}]{.o}[[None]{.pre}]{.default_value}</em>, <em>[[smtpport]{.pre}]{.n}[[=]{.pre}]{.o}[[None]{.pre}]{.default_value}</em>[)]{.sig-paren}<a href="_modules/scrapy/mail.html#MailSender">[[[source]]{.pre}]{.viewcode-link}</a>{.reference .internal}<a href="#scrapy.mail.MailSender" title="Permalink to this definition">¶</a>{.headerlink}</p>
<p>:</p>
<pre><code>Parameters

:   -   **smtphost**
        ([*str*](https://docs.python.org/3/library/stdtypes.html#str &quot;(in Python v3.12)&quot;){.reference
        .external} *or*
        [*bytes*](https://docs.python.org/3/library/stdtypes.html#bytes &quot;(in Python v3.12)&quot;){.reference
        .external}) -- the SMTP host to use for sending the emails.
        If omitted, the [[`MAIL_HOST`{.xref .std .std-setting
        .docutils .literal
        .notranslate}]{.pre}](#std-setting-MAIL_HOST){.hoverxref
        .tooltip .reference .internal} setting will be used.

    -   **mailfrom**
        ([*str*](https://docs.python.org/3/library/stdtypes.html#str &quot;(in Python v3.12)&quot;){.reference
        .external}) -- the address used to send emails (in the
        [`From:`{.docutils .literal .notranslate}]{.pre} header). If
        omitted, the [[`MAIL_FROM`{.xref .std .std-setting .docutils
        .literal
        .notranslate}]{.pre}](#std-setting-MAIL_FROM){.hoverxref
        .tooltip .reference .internal} setting will be used.

    -   **smtpuser** -- the SMTP user. If omitted, the
        [[`MAIL_USER`{.xref .std .std-setting .docutils .literal
        .notranslate}]{.pre}](#std-setting-MAIL_USER){.hoverxref
        .tooltip .reference .internal} setting will be used. If not
        given, no SMTP authentication will be performed.

    -   **smtppass**
        ([*str*](https://docs.python.org/3/library/stdtypes.html#str &quot;(in Python v3.12)&quot;){.reference
        .external} *or*
        [*bytes*](https://docs.python.org/3/library/stdtypes.html#bytes &quot;(in Python v3.12)&quot;){.reference
        .external}) -- the SMTP pass for authentication.

    -   **smtpport**
        ([*int*](https://docs.python.org/3/library/functions.html#int &quot;(in Python v3.12)&quot;){.reference
        .external}) -- the SMTP port to connect to

    -   **smtptls**
        ([*bool*](https://docs.python.org/3/library/functions.html#bool &quot;(in Python v3.12)&quot;){.reference
        .external}) -- enforce using SMTP STARTTLS

    -   **smtpssl**
        ([*bool*](https://docs.python.org/3/library/functions.html#bool &quot;(in Python v3.12)&quot;){.reference
        .external}) -- enforce using a secure SSL connection

*[classmethod]{.pre}[ ]{.w}*[[from_settings]{.pre}]{.sig-name .descname}[(]{.sig-paren}*[[settings]{.pre}]{.n}*[)]{.sig-paren}[[[\[source\]]{.pre}]{.viewcode-link}](_modules/scrapy/mail.html#MailSender.from_settings){.reference .internal}[¶](#scrapy.mail.MailSender.from_settings &quot;Permalink to this definition&quot;){.headerlink}

:   Instantiate using a Scrapy settings object, which will respect
    [[these Scrapy settings]{.std
    .std-ref}](#topics-email-settings){.hoverxref .tooltip
    .reference .internal}.

    Parameters

    :   **settings** ([[`scrapy.settings.Settings`{.xref .py
        .py-class .docutils .literal
        .notranslate}]{.pre}](index.html#scrapy.settings.Settings &quot;scrapy.settings.Settings&quot;){.reference
        .internal} object) -- the e-mail recipients

[[send]{.pre}]{.sig-name .descname}[(]{.sig-paren}*[[to]{.pre}]{.n}*, *[[subject]{.pre}]{.n}*, *[[body]{.pre}]{.n}*, *[[cc]{.pre}]{.n}[[=]{.pre}]{.o}[[None]{.pre}]{.default_value}*, *[[attachs]{.pre}]{.n}[[=]{.pre}]{.o}[[()]{.pre}]{.default_value}*, *[[mimetype]{.pre}]{.n}[[=]{.pre}]{.o}[[\'text/plain\']{.pre}]{.default_value}*, *[[charset]{.pre}]{.n}[[=]{.pre}]{.o}[[None]{.pre}]{.default_value}*[)]{.sig-paren}[[[\[source\]]{.pre}]{.viewcode-link}](_modules/scrapy/mail.html#MailSender.send){.reference .internal}[¶](#scrapy.mail.MailSender.send &quot;Permalink to this definition&quot;){.headerlink}

:   Send email to the given recipients.

    Parameters

    :   -   **to**
            ([*str*](https://docs.python.org/3/library/stdtypes.html#str &quot;(in Python v3.12)&quot;){.reference
            .external} *or*
            [*list*](https://docs.python.org/3/library/stdtypes.html#list &quot;(in Python v3.12)&quot;){.reference
            .external}) -- the e-mail recipients as a string or as a
            list of strings

        -   **subject**
            ([*str*](https://docs.python.org/3/library/stdtypes.html#str &quot;(in Python v3.12)&quot;){.reference
            .external}) -- the subject of the e-mail

        -   **cc**
            ([*str*](https://docs.python.org/3/library/stdtypes.html#str &quot;(in Python v3.12)&quot;){.reference
            .external} *or*
            [*list*](https://docs.python.org/3/library/stdtypes.html#list &quot;(in Python v3.12)&quot;){.reference
            .external}) -- the e-mails to CC as a string or as a
            list of strings

        -   **body**
            ([*str*](https://docs.python.org/3/library/stdtypes.html#str &quot;(in Python v3.12)&quot;){.reference
            .external}) -- the e-mail body

        -   **attachs**
            ([*collections.abc.Iterable*](https://docs.python.org/3/library/collections.abc.html#collections.abc.Iterable &quot;(in Python v3.12)&quot;){.reference
            .external}) -- an iterable of tuples
            [`(attach_name,`{.docutils .literal
            .notranslate}]{.pre}` `{.docutils .literal
            .notranslate}[`mimetype,`{.docutils .literal
            .notranslate}]{.pre}` `{.docutils .literal
            .notranslate}[`file_object)`{.docutils .literal
            .notranslate}]{.pre} where [`attach_name`{.docutils
            .literal .notranslate}]{.pre} is a string with the name
            that will appear on the e-mail's attachment,
            [`mimetype`{.docutils .literal .notranslate}]{.pre} is
            the mimetype of the attachment and
            [`file_object`{.docutils .literal .notranslate}]{.pre}
            is a readable file object with the contents of the
            attachment

        -   **mimetype**
            ([*str*](https://docs.python.org/3/library/stdtypes.html#str &quot;(in Python v3.12)&quot;){.reference
            .external}) -- the MIME type of the e-mail

        -   **charset**
            ([*str*](https://docs.python.org/3/library/stdtypes.html#str &quot;(in Python v3.12)&quot;){.reference
            .external}) -- the character encoding to use for the
            e-mail contents
</code></pre>
<p>:::</p>
<p>::: {#mail-settings .section}
[]{#topics-email-settings}</p>
<h4 id="mail-settingsheaderlink"><a class="header" href="#mail-settingsheaderlink">Mail settings<a href="#mail-settings" title="Permalink to this heading">¶</a>{.headerlink}</a></h4>
<p>These settings define the default [<code>__init__</code>{.docutils .literal
.notranslate}]{.pre} method values of the <a href="#scrapy.mail.MailSender" title="scrapy.mail.MailSender">[<code>MailSender</code>{.xref .py
.py-class .docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal} class, and can be used to configure e-mail notifications in
your project without writing any code (for those extensions and code
that uses <a href="#scrapy.mail.MailSender" title="scrapy.mail.MailSender">[<code>MailSender</code>{.xref .py .py-class .docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal}).</p>
<p>::: {#mail-from .section}
[]{#std-setting-MAIL_FROM}</p>
<h5 id="mail_fromheaderlink"><a class="header" href="#mail_fromheaderlink">MAIL_FROM<a href="#mail-from" title="Permalink to this heading">¶</a>{.headerlink}</a></h5>
<p>Default: [<code>'scrapy@localhost'</code>{.docutils .literal .notranslate}]{.pre}</p>
<p>Sender email to use ([<code>From:</code>{.docutils .literal .notranslate}]{.pre}
header) for sending emails.
:::</p>
<p>::: {#mail-host .section}
[]{#std-setting-MAIL_HOST}</p>
<h5 id="mail_hostheaderlink"><a class="header" href="#mail_hostheaderlink">MAIL_HOST<a href="#mail-host" title="Permalink to this heading">¶</a>{.headerlink}</a></h5>
<p>Default: [<code>'localhost'</code>{.docutils .literal .notranslate}]{.pre}</p>
<p>SMTP host to use for sending emails.
:::</p>
<p>::: {#mail-port .section}
[]{#std-setting-MAIL_PORT}</p>
<h5 id="mail_portheaderlink"><a class="header" href="#mail_portheaderlink">MAIL_PORT<a href="#mail-port" title="Permalink to this heading">¶</a>{.headerlink}</a></h5>
<p>Default: [<code>25</code>{.docutils .literal .notranslate}]{.pre}</p>
<p>SMTP port to use for sending emails.
:::</p>
<p>::: {#mail-user .section}
[]{#std-setting-MAIL_USER}</p>
<h5 id="mail_userheaderlink"><a class="header" href="#mail_userheaderlink">MAIL_USER<a href="#mail-user" title="Permalink to this heading">¶</a>{.headerlink}</a></h5>
<p>Default: [<code>None</code>{.docutils .literal .notranslate}]{.pre}</p>
<p>User to use for SMTP authentication. If disabled no SMTP authentication
will be performed.
:::</p>
<p>::: {#mail-pass .section}
[]{#std-setting-MAIL_PASS}</p>
<h5 id="mail_passheaderlink"><a class="header" href="#mail_passheaderlink">MAIL_PASS<a href="#mail-pass" title="Permalink to this heading">¶</a>{.headerlink}</a></h5>
<p>Default: [<code>None</code>{.docutils .literal .notranslate}]{.pre}</p>
<p>Password to use for SMTP authentication, along with <a href="#std-setting-MAIL_USER">[<code>MAIL_USER</code>{.xref
.std .std-setting .docutils .literal
.notranslate}]{.pre}</a>{.hoverxref .tooltip
.reference .internal}.
:::</p>
<p>::: {#mail-tls .section}
[]{#std-setting-MAIL_TLS}</p>
<h5 id="mail_tlsheaderlink"><a class="header" href="#mail_tlsheaderlink">MAIL_TLS<a href="#mail-tls" title="Permalink to this heading">¶</a>{.headerlink}</a></h5>
<p>Default: [<code>False</code>{.docutils .literal .notranslate}]{.pre}</p>
<p>Enforce using STARTTLS. STARTTLS is a way to take an existing insecure
connection, and upgrade it to a secure connection using SSL/TLS.
:::</p>
<p>::: {#mail-ssl .section}
[]{#std-setting-MAIL_SSL}</p>
<h5 id="mail_sslheaderlink"><a class="header" href="#mail_sslheaderlink">MAIL_SSL<a href="#mail-ssl" title="Permalink to this heading">¶</a>{.headerlink}</a></h5>
<p>Default: [<code>False</code>{.docutils .literal .notranslate}]{.pre}</p>
<p>Enforce connecting using an SSL encrypted connection
:::
:::
:::</p>
<p>[]{#document-topics/telnetconsole}</p>
<p>::: {#telnet-console .section}
[]{#topics-telnetconsole}</p>
<h3 id="telnet-consoleheaderlink"><a class="header" href="#telnet-consoleheaderlink">Telnet Console<a href="#telnet-console" title="Permalink to this heading">¶</a>{.headerlink}</a></h3>
<p>Scrapy comes with a built-in telnet console for inspecting and
controlling a Scrapy running process. The telnet console is just a
regular python shell running inside the Scrapy process, so you can do
literally anything from it.</p>
<p>The telnet console is a <a href="index.html#topics-extensions-ref">[built-in Scrapy extension]{.std
.std-ref}</a>{.hoverxref .tooltip
.reference .internal} which comes enabled by default, but you can also
disable it if you want. For more information about the extension itself
see <a href="index.html#topics-extensions-ref-telnetconsole">[Telnet console extension]{.std
.std-ref}</a>{.hoverxref
.tooltip .reference .internal}.</p>
<p>::: {.admonition .warning}
Warning</p>
<p>It is not secure to use telnet console via public networks, as telnet
doesn't provide any transport-layer security. Having username/password
authentication doesn't change that.</p>
<p>Intended usage is connecting to a running Scrapy spider locally (spider
process and telnet client are on the same machine) or over a secure
connection (VPN, SSH tunnel). Please avoid using telnet console over
insecure connections, or disable it completely using
<a href="index.html#std-setting-TELNETCONSOLE_ENABLED">[<code>TELNETCONSOLE_ENABLED</code>{.xref .std .std-setting .docutils .literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal} option.
:::</p>
<p>::: {#how-to-access-the-telnet-console .section}</p>
<h4 id="how-to-access-the-telnet-consoleheaderlink"><a class="header" href="#how-to-access-the-telnet-consoleheaderlink">How to access the telnet console<a href="#how-to-access-the-telnet-console" title="Permalink to this heading">¶</a>{.headerlink}</a></h4>
<p>The telnet console listens in the TCP port defined in the
<a href="#std-setting-TELNETCONSOLE_PORT">[<code>TELNETCONSOLE_PORT</code>{.xref .std .std-setting .docutils .literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal} setting, which defaults to
[<code>6023</code>{.docutils .literal .notranslate}]{.pre}. To access the console
you need to type:</p>
<p>::: {.highlight-none .notranslate}
::: highlight
telnet localhost 6023
Trying localhost...
Connected to localhost.
Escape character is '^]'.
Username:
Password:
&gt;&gt;&gt;
:::
:::</p>
<p>By default Username is [<code>scrapy</code>{.docutils .literal .notranslate}]{.pre}
and Password is autogenerated. The autogenerated Password can be seen on
Scrapy logs like the example below:</p>
<p>::: {.highlight-none .notranslate}
::: highlight
2018-10-16 14:35:21 [scrapy.extensions.telnet] INFO: Telnet Password: 16f92501e8a59326
:::
:::</p>
<p>Default Username and Password can be overridden by the settings
<a href="#std-setting-TELNETCONSOLE_USERNAME">[<code>TELNETCONSOLE_USERNAME</code>{.xref .std .std-setting .docutils .literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal} and <a href="#std-setting-TELNETCONSOLE_PASSWORD">[<code>TELNETCONSOLE_PASSWORD</code>{.xref .std
.std-setting .docutils .literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal}.</p>
<p>::: {.admonition .warning}
Warning</p>
<p>Username and password provide only a limited protection, as telnet is
not using secure transport - by default traffic is not encrypted even if
username and password are set.
:::</p>
<p>You need the telnet program which comes installed by default in Windows,
and most Linux distros.
:::</p>
<p>::: {#available-variables-in-the-telnet-console .section}</p>
<h4 id="available-variables-in-the-telnet-consoleheaderlink"><a class="header" href="#available-variables-in-the-telnet-consoleheaderlink">Available variables in the telnet console<a href="#available-variables-in-the-telnet-console" title="Permalink to this heading">¶</a>{.headerlink}</a></h4>
<p>The telnet console is like a regular Python shell running inside the
Scrapy process, so you can do anything from it including importing new
modules, etc.</p>
<p>However, the telnet console comes with some default variables defined
for convenience:</p>
<p>+------------+---------------------------------------------------------+
| Shortcut   | Description                                             |
+============+=========================================================+
| [<code>crawler</code> | the Scrapy Crawler ([[<code>scrapy.crawler.Crawler</code>{.xref    |
| {.docutils | .py .py-class .docutils .literal                        |
| .literal   | .notranslate}]{.pre}](index.html#scra                   |
| .notransla | py.crawler.Crawler &quot;scrapy.crawler.Crawler&quot;){.reference |
| te}]{.pre} | .internal} object)                                      |
+------------+---------------------------------------------------------+
| [<code>engine</code>  | Crawler.engine attribute                                |
| {.docutils |                                                         |
| .literal   |                                                         |
| .notransla |                                                         |
| te}]{.pre} |                                                         |
+------------+---------------------------------------------------------+
| [<code>spider</code>  | the active spider                                       |
| {.docutils |                                                         |
| .literal   |                                                         |
| .notransla |                                                         |
| te}]{.pre} |                                                         |
+------------+---------------------------------------------------------+
| [<code>slot</code>    | the engine slot                                         |
| {.docutils |                                                         |
| .literal   |                                                         |
| .notransla |                                                         |
| te}]{.pre} |                                                         |
+------------+---------------------------------------------------------+
| [<code>e        | the Extension Manager (Crawler.extensions attribute)    | | xtensions</code> |                                                         |
| {.docutils |                                                         |
| .literal   |                                                         |
| .notransla |                                                         |
| te}]{.pre} |                                                         |
+------------+---------------------------------------------------------+
| [<code>stats</code>   | the Stats Collector (Crawler.stats attribute)           |
| {.docutils |                                                         |
| .literal   |                                                         |
| .notransla |                                                         |
| te}]{.pre} |                                                         |
+------------+---------------------------------------------------------+
| [          | the Scrapy settings object (Crawler.settings attribute) |
| <code>settings</code> |                                                         |
| {.docutils |                                                         |
| .literal   |                                                         |
| .notransla |                                                         |
| te}]{.pre} |                                                         |
+------------+---------------------------------------------------------+
| [<code>est</code>     | print a report of the engine status                     |
| {.docutils |                                                         |
| .literal   |                                                         |
| .notransla |                                                         |
| te}]{.pre} |                                                         |
+------------+---------------------------------------------------------+
| [<code>prefs</code>   | for memory debugging (see <a href="index.html#topics-leaks">[Debugging memory            |
| {.docutils | leaks]{.std                                             |
| .literal   | .std-ref}</a>{.hoverxref .tooltip |
| .notransla | .reference .internal})                                  |
| te}]{.pre} |                                                         |
+------------+---------------------------------------------------------+
| [<code>p</code>       | a shortcut to the [[<code>pprint.pprint()</code>{.xref .py         |
| {.docutils | .py-func .docutils .literal                             |
| .literal   | .no                                                     |
| .notransla | translate}]{.pre}](https://docs.python.org/3/library/pp |
| te}]{.pre} | rint.html#pprint.pprint &quot;(in Python v3.12)&quot;){.reference |
|            | .external} function                                     |
+------------+---------------------------------------------------------+
| [<code>hpy</code>     | for memory debugging (see <a href="index.html#topics-leaks">[Debugging memory            |
| {.docutils | leaks]{.std                                             |
| .literal   | .std-ref}</a>{.hoverxref .tooltip |
| .notransla | .reference .internal})                                  |
| te}]{.pre} |                                                         |
+------------+---------------------------------------------------------+
:::</p>
<p>::: {#telnet-console-usage-examples .section}</p>
<h4 id="telnet-console-usage-examplesheaderlink"><a class="header" href="#telnet-console-usage-examplesheaderlink">Telnet console usage examples<a href="#telnet-console-usage-examples" title="Permalink to this heading">¶</a>{.headerlink}</a></h4>
<p>Here are some example tasks you can do with the telnet console:</p>
<p>::: {#view-engine-status .section}</p>
<h5 id="view-engine-statusheaderlink"><a class="header" href="#view-engine-statusheaderlink">View engine status<a href="#view-engine-status" title="Permalink to this heading">¶</a>{.headerlink}</a></h5>
<p>You can use the [<code>est()</code>{.docutils .literal .notranslate}]{.pre} method
of the Scrapy engine to quickly show its state using the telnet console:</p>
<p>::: {.highlight-none .notranslate}
::: highlight
telnet localhost 6023
&gt;&gt;&gt; est()
Execution engine status</p>
<pre><code>time()-engine.start_time                        : 8.62972998619
len(engine.downloader.active)                   : 16
engine.scraper.is_idle()                        : False
engine.spider.name                              : followall
engine.spider_is_idle()                         : False
engine.slot.closing                             : False
len(engine.slot.inprogress)                     : 16
len(engine.slot.scheduler.dqs or [])            : 0
len(engine.slot.scheduler.mqs)                  : 92
len(engine.scraper.slot.queue)                  : 0
len(engine.scraper.slot.active)                 : 0
engine.scraper.slot.active_size                 : 0
engine.scraper.slot.itemproc_size               : 0
engine.scraper.slot.needs_backout()             : False
</code></pre>
<p>:::
:::
:::</p>
<p>::: {#pause-resume-and-stop-the-scrapy-engine .section}</p>
<h5 id="pause-resume-and-stop-the-scrapy-engineheaderlink"><a class="header" href="#pause-resume-and-stop-the-scrapy-engineheaderlink">Pause, resume and stop the Scrapy engine<a href="#pause-resume-and-stop-the-scrapy-engine" title="Permalink to this heading">¶</a>{.headerlink}</a></h5>
<p>To pause:</p>
<p>::: {.highlight-none .notranslate}
::: highlight
telnet localhost 6023
&gt;&gt;&gt; engine.pause()
&gt;&gt;&gt;
:::
:::</p>
<p>To resume:</p>
<p>::: {.highlight-none .notranslate}
::: highlight
telnet localhost 6023
&gt;&gt;&gt; engine.unpause()
&gt;&gt;&gt;
:::
:::</p>
<p>To stop:</p>
<p>::: {.highlight-none .notranslate}
::: highlight
telnet localhost 6023
&gt;&gt;&gt; engine.stop()
Connection closed by foreign host.
:::
:::
:::
:::</p>
<p>::: {#telnet-console-signals .section}</p>
<h4 id="telnet-console-signalsheaderlink"><a class="header" href="#telnet-console-signalsheaderlink">Telnet Console signals<a href="#telnet-console-signals" title="Permalink to this heading">¶</a>{.headerlink}</a></h4>
<p>[]{#std-signal-update_telnet_vars .target}</p>
<p>[[scrapy.extensions.telnet.]{.pre}]{.sig-prename .descclassname}[[update_telnet_vars]{.pre}]{.sig-name .descname}[(]{.sig-paren}<em>[[telnet_vars]{.pre}]{.n}</em>[)]{.sig-paren}<a href="#scrapy.extensions.telnet.update_telnet_vars" title="Permalink to this definition">¶</a>{.headerlink}</p>
<p>:   Sent just before the telnet console is opened. You can hook up to
this signal to add, remove or update the variables that will be
available in the telnet local namespace. In order to do that, you
need to update the [<code>telnet_vars</code>{.docutils .literal
.notranslate}]{.pre} dict in your handler.</p>
<pre><code>Parameters

:   **telnet_vars**
    ([*dict*](https://docs.python.org/3/library/stdtypes.html#dict &quot;(in Python v3.12)&quot;){.reference
    .external}) -- the dict of telnet variables
</code></pre>
<p>:::</p>
<p>::: {#telnet-settings .section}</p>
<h4 id="telnet-settingsheaderlink"><a class="header" href="#telnet-settingsheaderlink">Telnet settings<a href="#telnet-settings" title="Permalink to this heading">¶</a>{.headerlink}</a></h4>
<p>These are the settings that control the telnet console's behaviour:</p>
<p>::: {#telnetconsole-port .section}
[]{#std-setting-TELNETCONSOLE_PORT}</p>
<h5 id="telnetconsole_portheaderlink"><a class="header" href="#telnetconsole_portheaderlink">TELNETCONSOLE_PORT<a href="#telnetconsole-port" title="Permalink to this heading">¶</a>{.headerlink}</a></h5>
<p>Default: [<code>[6023,</code>{.docutils .literal .notranslate}]{.pre}<code> </code>{.docutils
.literal .notranslate}[<code>6073]</code>{.docutils .literal .notranslate}]{.pre}</p>
<p>The port range to use for the telnet console. If set to
[<code>None</code>{.docutils .literal .notranslate}]{.pre} or [<code>0</code>{.docutils
.literal .notranslate}]{.pre}, a dynamically assigned port is used.
:::</p>
<p>::: {#telnetconsole-host .section}
[]{#std-setting-TELNETCONSOLE_HOST}</p>
<h5 id="telnetconsole_hostheaderlink"><a class="header" href="#telnetconsole_hostheaderlink">TELNETCONSOLE_HOST<a href="#telnetconsole-host" title="Permalink to this heading">¶</a>{.headerlink}</a></h5>
<p>Default: [<code>'127.0.0.1'</code>{.docutils .literal .notranslate}]{.pre}</p>
<p>The interface the telnet console should listen on
:::</p>
<p>::: {#telnetconsole-username .section}
[]{#std-setting-TELNETCONSOLE_USERNAME}</p>
<h5 id="telnetconsole_usernameheaderlink"><a class="header" href="#telnetconsole_usernameheaderlink">TELNETCONSOLE_USERNAME<a href="#telnetconsole-username" title="Permalink to this heading">¶</a>{.headerlink}</a></h5>
<p>Default: [<code>'scrapy'</code>{.docutils .literal .notranslate}]{.pre}</p>
<p>The username used for the telnet console
:::</p>
<p>::: {#telnetconsole-password .section}
[]{#std-setting-TELNETCONSOLE_PASSWORD}</p>
<h5 id="telnetconsole_passwordheaderlink"><a class="header" href="#telnetconsole_passwordheaderlink">TELNETCONSOLE_PASSWORD<a href="#telnetconsole-password" title="Permalink to this heading">¶</a>{.headerlink}</a></h5>
<p>Default: [<code>None</code>{.docutils .literal .notranslate}]{.pre}</p>
<p>The password used for the telnet console, default behaviour is to have
it autogenerated
:::
:::
:::
:::</p>
<p><a href="index.html#document-topics/logging">[Logging]{.doc}</a>{.reference .internal}</p>
<p>:   Learn how to use Python's builtin logging on Scrapy.</p>
<p><a href="index.html#document-topics/stats">[Stats Collection]{.doc}</a>{.reference .internal}</p>
<p>:   Collect statistics about your scraping crawler.</p>
<p><a href="index.html#document-topics/email">[Sending e-mail]{.doc}</a>{.reference .internal}</p>
<p>:   Send email notifications when certain events occur.</p>
<p><a href="index.html#document-topics/telnetconsole">[Telnet Console]{.doc}</a>{.reference .internal}</p>
<p>:   Inspect a running crawler using a built-in Python console.
:::</p>
<p>::: {#solving-specific-problems .section}</p>
<h2 id="solving-specific-problemsheaderlink"><a class="header" href="#solving-specific-problemsheaderlink">Solving specific problems<a href="#solving-specific-problems" title="Permalink to this heading">¶</a>{.headerlink}</a></h2>
<p>::: {.toctree-wrapper .compound}
[]{#document-faq}</p>
<p>::: {#frequently-asked-questions .section}
[]{#faq}</p>
<h3 id="frequently-asked-questionsheaderlink"><a class="header" href="#frequently-asked-questionsheaderlink">Frequently Asked Questions<a href="#frequently-asked-questions" title="Permalink to this heading">¶</a>{.headerlink}</a></h3>
<p>::: {#how-does-scrapy-compare-to-beautifulsoup-or-lxml .section}
[]{#faq-scrapy-bs-cmp}</p>
<h4 id="how-does-scrapy-compare-to-beautifulsoup-or-lxmlheaderlink"><a class="header" href="#how-does-scrapy-compare-to-beautifulsoup-or-lxmlheaderlink">How does Scrapy compare to BeautifulSoup or lxml?<a href="#how-does-scrapy-compare-to-beautifulsoup-or-lxml" title="Permalink to this heading">¶</a>{.headerlink}</a></h4>
<p><a href="https://www.crummy.com/software/BeautifulSoup/">BeautifulSoup</a>{.reference
.external} and <a href="https://lxml.de/">lxml</a>{.reference .external} are
libraries for parsing HTML and XML. Scrapy is an application framework
for writing web spiders that crawl web sites and extract data from them.</p>
<p>Scrapy provides a built-in mechanism for extracting data (called
<a href="index.html#topics-selectors">[selectors]{.std .std-ref}</a>{.hoverxref
.tooltip .reference .internal}) but you can easily use
<a href="https://www.crummy.com/software/BeautifulSoup/">BeautifulSoup</a>{.reference
.external} (or <a href="https://lxml.de/">lxml</a>{.reference .external}) instead,
if you feel more comfortable working with them. After all, they're just
parsing libraries which can be imported and used from any Python code.</p>
<p>In other words, comparing
<a href="https://www.crummy.com/software/BeautifulSoup/">BeautifulSoup</a>{.reference
.external} (or <a href="https://lxml.de/">lxml</a>{.reference .external}) to Scrapy
is like comparing
<a href="https://palletsprojects.com/p/jinja/">jinja2</a>{.reference .external} to
<a href="https://www.djangoproject.com/">Django</a>{.reference .external}.
:::</p>
<p>::: {#can-i-use-scrapy-with-beautifulsoup .section}</p>
<h4 id="can-i-use-scrapy-with-beautifulsoupheaderlink"><a class="header" href="#can-i-use-scrapy-with-beautifulsoupheaderlink">Can I use Scrapy with BeautifulSoup?<a href="#can-i-use-scrapy-with-beautifulsoup" title="Permalink to this heading">¶</a>{.headerlink}</a></h4>
<p>Yes, you can. As mentioned <a href="#faq-scrapy-bs-cmp">[above]{.std
.std-ref}</a>{.hoverxref .tooltip .reference
.internal},
<a href="https://www.crummy.com/software/BeautifulSoup/">BeautifulSoup</a>{.reference
.external} can be used for parsing HTML responses in Scrapy callbacks.
You just have to feed the response's body into a
[<code>BeautifulSoup</code>{.docutils .literal .notranslate}]{.pre} object and
extract whatever data you need from it.</p>
<p>Here's an example spider using BeautifulSoup API, with [<code>lxml</code>{.docutils
.literal .notranslate}]{.pre} as the HTML parser:</p>
<p>::: {.highlight-python .notranslate}
::: highlight
from bs4 import BeautifulSoup
import scrapy</p>
<pre><code>class ExampleSpider(scrapy.Spider):
    name = &quot;example&quot;
    allowed_domains = [&quot;example.com&quot;]
    start_urls = (&quot;http://www.example.com/&quot;,)

    def parse(self, response):
        # use lxml to get decent HTML parsing speed
        soup = BeautifulSoup(response.text, &quot;lxml&quot;)
        yield {&quot;url&quot;: response.url, &quot;title&quot;: soup.h1.string}
</code></pre>
<p>:::
:::</p>
<p>::: {.admonition .note}
Note</p>
<p>[<code>BeautifulSoup</code>{.docutils .literal .notranslate}]{.pre} supports
several HTML/XML parsers. See <a href="https://www.crummy.com/software/BeautifulSoup/bs4/doc/#specifying-the-parser-to-use">BeautifulSoup's official
documentation</a>{.reference
.external} on which ones are available.
:::
:::</p>
<p>::: {#did-scrapy-steal-x-from-django .section}</p>
<h4 id="did-scrapy-steal-x-from-djangoheaderlink"><a class="header" href="#did-scrapy-steal-x-from-djangoheaderlink">Did Scrapy &quot;steal&quot; X from Django?<a href="#did-scrapy-steal-x-from-django" title="Permalink to this heading">¶</a>{.headerlink}</a></h4>
<p>Probably, but we don't like that word. We think
<a href="https://www.djangoproject.com/">Django</a>{.reference .external} is a
great open source project and an example to follow, so we've used it as
an inspiration for Scrapy.</p>
<p>We believe that, if something is already done well, there's no need to
reinvent it. This concept, besides being one of the foundations for open
source and free software, not only applies to software but also to
documentation, procedures, policies, etc. So, instead of going through
each problem ourselves, we choose to copy ideas from those projects that
have already solved them properly, and focus on the real problems we
need to solve.</p>
<p>We'd be proud if Scrapy serves as an inspiration for other projects.
Feel free to steal from us!
:::</p>
<p>::: {#does-scrapy-work-with-http-proxies .section}</p>
<h4 id="does-scrapy-work-with-http-proxiesheaderlink"><a class="header" href="#does-scrapy-work-with-http-proxiesheaderlink">Does Scrapy work with HTTP proxies?<a href="#does-scrapy-work-with-http-proxies" title="Permalink to this heading">¶</a>{.headerlink}</a></h4>
<p>Yes. Support for HTTP proxies is provided (since Scrapy 0.8) through the
HTTP Proxy downloader middleware. See <a href="index.html#scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware" title="scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware">[<code>HttpProxyMiddleware</code>{.xref .py
.py-class .docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal}.
:::</p>
<p>::: {#how-can-i-scrape-an-item-with-attributes-in-different-pages .section}</p>
<h4 id="how-can-i-scrape-an-item-with-attributes-in-different-pagesheaderlink"><a class="header" href="#how-can-i-scrape-an-item-with-attributes-in-different-pagesheaderlink">How can I scrape an item with attributes in different pages?<a href="#how-can-i-scrape-an-item-with-attributes-in-different-pages" title="Permalink to this heading">¶</a>{.headerlink}</a></h4>
<p>See <a href="index.html#topics-request-response-ref-request-callback-arguments">[Passing additional data to callback functions]{.std
.std-ref}</a>{.hoverxref
.tooltip .reference .internal}.
:::</p>
<p>::: {#how-can-i-simulate-a-user-login-in-my-spider .section}</p>
<h4 id="how-can-i-simulate-a-user-login-in-my-spiderheaderlink"><a class="header" href="#how-can-i-simulate-a-user-login-in-my-spiderheaderlink">How can I simulate a user login in my spider?<a href="#how-can-i-simulate-a-user-login-in-my-spider" title="Permalink to this heading">¶</a>{.headerlink}</a></h4>
<p>See <a href="index.html#topics-request-response-ref-request-userlogin">[Using FormRequest.from_response() to simulate a user login]{.std
.std-ref}</a>{.hoverxref
.tooltip .reference .internal}.
:::</p>
<p>::: {#does-scrapy-crawl-in-breadth-first-or-depth-first-order .section}
[]{#faq-bfo-dfo}</p>
<h4 id="does-scrapy-crawl-in-breadth-first-or-depth-first-orderheaderlink"><a class="header" href="#does-scrapy-crawl-in-breadth-first-or-depth-first-orderheaderlink">Does Scrapy crawl in breadth-first or depth-first order?<a href="#does-scrapy-crawl-in-breadth-first-or-depth-first-order" title="Permalink to this heading">¶</a>{.headerlink}</a></h4>
<p>By default, Scrapy uses a
<a href="https://en.wikipedia.org/wiki/Stack_(abstract_data_type)">LIFO</a>{.reference
.external} queue for storing pending requests, which basically means
that it crawls in <a href="https://en.wikipedia.org/wiki/Depth-first_search">DFO
order</a>{.reference
.external}. This order is more convenient in most cases.</p>
<p>If you do want to crawl in true <a href="https://en.wikipedia.org/wiki/Breadth-first_search">BFO
order</a>{.reference
.external}, you can do it by setting the following settings:</p>
<p>::: {.highlight-python .notranslate}
::: highlight
DEPTH_PRIORITY = 1
SCHEDULER_DISK_QUEUE = &quot;scrapy.squeues.PickleFifoDiskQueue&quot;
SCHEDULER_MEMORY_QUEUE = &quot;scrapy.squeues.FifoMemoryQueue&quot;
:::
:::</p>
<p>While pending requests are below the configured values of
<a href="index.html#std-setting-CONCURRENT_REQUESTS">[<code>CONCURRENT_REQUESTS</code>{.xref .std .std-setting .docutils .literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal}, <a href="index.html#std-setting-CONCURRENT_REQUESTS_PER_DOMAIN">[<code>CONCURRENT_REQUESTS_PER_DOMAIN</code>{.xref
.std .std-setting .docutils .literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal} or <a href="index.html#std-setting-CONCURRENT_REQUESTS_PER_IP">[<code>CONCURRENT_REQUESTS_PER_IP</code>{.xref
.std .std-setting .docutils .literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal}, those requests are sent concurrently. As
a result, the first few requests of a crawl rarely follow the desired
order. Lowering those settings to [<code>1</code>{.docutils .literal
.notranslate}]{.pre} enforces the desired order, but it significantly
slows down the crawl as a whole.
:::</p>
<p>::: {#my-scrapy-crawler-has-memory-leaks-what-can-i-do .section}</p>
<h4 id="my-scrapy-crawler-has-memory-leaks-what-can-i-doheaderlink"><a class="header" href="#my-scrapy-crawler-has-memory-leaks-what-can-i-doheaderlink">My Scrapy crawler has memory leaks. What can I do?<a href="#my-scrapy-crawler-has-memory-leaks-what-can-i-do" title="Permalink to this heading">¶</a>{.headerlink}</a></h4>
<p>See <a href="index.html#topics-leaks">[Debugging memory leaks]{.std
.std-ref}</a>{.hoverxref .tooltip .reference
.internal}.</p>
<p>Also, Python has a builtin memory leak issue which is described in
<a href="index.html#topics-leaks-without-leaks">[Leaks without leaks]{.std
.std-ref}</a>{.hoverxref .tooltip
.reference .internal}.
:::</p>
<p>::: {#how-can-i-make-scrapy-consume-less-memory .section}</p>
<h4 id="how-can-i-make-scrapy-consume-less-memoryheaderlink"><a class="header" href="#how-can-i-make-scrapy-consume-less-memoryheaderlink">How can I make Scrapy consume less memory?<a href="#how-can-i-make-scrapy-consume-less-memory" title="Permalink to this heading">¶</a>{.headerlink}</a></h4>
<p>See previous question.
:::</p>
<p>::: {#how-can-i-prevent-memory-errors-due-to-many-allowed-domains .section}</p>
<h4 id="how-can-i-prevent-memory-errors-due-to-many-allowed-domainsheaderlink"><a class="header" href="#how-can-i-prevent-memory-errors-due-to-many-allowed-domainsheaderlink">How can I prevent memory errors due to many allowed domains?<a href="#how-can-i-prevent-memory-errors-due-to-many-allowed-domains" title="Permalink to this heading">¶</a>{.headerlink}</a></h4>
<p>If you have a spider with a long list of <a href="index.html#scrapy.Spider.allowed_domains" title="scrapy.Spider.allowed_domains">[<code>allowed_domains</code>{.xref .py
.py-attr .docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal} (e.g. 50,000+), consider replacing the default
<a href="index.html#scrapy.spidermiddlewares.offsite.OffsiteMiddleware" title="scrapy.spidermiddlewares.offsite.OffsiteMiddleware">[<code>OffsiteMiddleware</code>{.xref .py .py-class .docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal} spider middleware with a <a href="index.html#custom-spider-middleware">[custom spider middleware]{.std
.std-ref}</a>{.hoverxref .tooltip
.reference .internal} that requires less memory. For example:</p>
<ul>
<li>
<p>If your domain names are similar enough, use your own regular
expression instead joining the strings in <a href="index.html#scrapy.Spider.allowed_domains" title="scrapy.Spider.allowed_domains">[<code>allowed_domains</code>{.xref
.py .py-attr .docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal} into a complex regular expression.</p>
</li>
<li>
<p>If you can <a href="https://github.com/andreasvc/pyre2#installation">meet the installation
requirements</a>{.reference
.external}, use
<a href="https://github.com/andreasvc/pyre2">pyre2</a>{.reference .external}
instead of Python's
<a href="https://docs.python.org/library/re.html">re</a>{.reference .external}
to compile your URL-filtering regular expression. See <a href="https://github.com/scrapy/scrapy/issues/1908">issue
1908</a>{.reference
.external}.</p>
</li>
</ul>
<p>See also other suggestions at
<a href="https://stackoverflow.com/q/36440681/939364">StackOverflow</a>{.reference
.external}.</p>
<p>::: {.admonition .note}
Note</p>
<p>Remember to disable
<a href="index.html#scrapy.spidermiddlewares.offsite.OffsiteMiddleware" title="scrapy.spidermiddlewares.offsite.OffsiteMiddleware">[<code>scrapy.spidermiddlewares.offsite.OffsiteMiddleware</code>{.xref .py
.py-class .docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal} when you enable your custom implementation:</p>
<p>::: {.highlight-python .notranslate}
::: highlight
SPIDER_MIDDLEWARES = {
&quot;scrapy.spidermiddlewares.offsite.OffsiteMiddleware&quot;: None,
&quot;myproject.middlewares.CustomOffsiteMiddleware&quot;: 500,
}
:::
:::
:::
:::</p>
<p>::: {#can-i-use-basic-http-authentication-in-my-spiders .section}</p>
<h4 id="can-i-use-basic-http-authentication-in-my-spidersheaderlink"><a class="header" href="#can-i-use-basic-http-authentication-in-my-spidersheaderlink">Can I use Basic HTTP Authentication in my spiders?<a href="#can-i-use-basic-http-authentication-in-my-spiders" title="Permalink to this heading">¶</a>{.headerlink}</a></h4>
<p>Yes, see <a href="index.html#scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware" title="scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware">[<code>HttpAuthMiddleware</code>{.xref .py .py-class .docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal}.
:::</p>
<p>::: {#why-does-scrapy-download-pages-in-english-instead-of-my-native-language .section}</p>
<h4 id="why-does-scrapy-download-pages-in-english-instead-of-my-native-languageheaderlink"><a class="header" href="#why-does-scrapy-download-pages-in-english-instead-of-my-native-languageheaderlink">Why does Scrapy download pages in English instead of my native language?<a href="#why-does-scrapy-download-pages-in-english-instead-of-my-native-language" title="Permalink to this heading">¶</a>{.headerlink}</a></h4>
<p>Try changing the default
<a href="https://www.w3.org/Protocols/rfc2616/rfc2616-sec14.html#sec14.4">Accept-Language</a>{.reference
.external} request header by overriding the
<a href="index.html#std-setting-DEFAULT_REQUEST_HEADERS">[<code>DEFAULT_REQUEST_HEADERS</code>{.xref .std .std-setting .docutils .literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal} setting.
:::</p>
<p>::: {#where-can-i-find-some-example-scrapy-projects .section}</p>
<h4 id="where-can-i-find-some-example-scrapy-projectsheaderlink"><a class="header" href="#where-can-i-find-some-example-scrapy-projectsheaderlink">Where can I find some example Scrapy projects?<a href="#where-can-i-find-some-example-scrapy-projects" title="Permalink to this heading">¶</a>{.headerlink}</a></h4>
<p>See <a href="index.html#intro-examples">[Examples]{.std .std-ref}</a>{.hoverxref
.tooltip .reference .internal}.
:::</p>
<p>::: {#can-i-run-a-spider-without-creating-a-project .section}</p>
<h4 id="can-i-run-a-spider-without-creating-a-projectheaderlink"><a class="header" href="#can-i-run-a-spider-without-creating-a-projectheaderlink">Can I run a spider without creating a project?<a href="#can-i-run-a-spider-without-creating-a-project" title="Permalink to this heading">¶</a>{.headerlink}</a></h4>
<p>Yes. You can use the <a href="index.html#std-command-runspider">[<code>runspider</code>{.xref .std .std-command .docutils
.literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal} command. For example, if you have a
spider written in a [<code>my_spider.py</code>{.docutils .literal
.notranslate}]{.pre} file you can run it with:</p>
<p>::: {.highlight-default .notranslate}
::: highlight
scrapy runspider my_spider.py
:::
:::</p>
<p>See <a href="index.html#std-command-runspider">[<code>runspider</code>{.xref .std .std-command .docutils .literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal} command for more info.
:::</p>
<p>::: {#i-get-filtered-offsite-request-messages-how-can-i-fix-them .section}</p>
<h4 id="i-get-filtered-offsite-request-messages-how-can-i-fix-themheaderlink"><a class="header" href="#i-get-filtered-offsite-request-messages-how-can-i-fix-themheaderlink">I get &quot;Filtered offsite request&quot; messages. How can I fix them?<a href="#i-get-filtered-offsite-request-messages-how-can-i-fix-them" title="Permalink to this heading">¶</a>{.headerlink}</a></h4>
<p>Those messages (logged with [<code>DEBUG</code>{.docutils .literal
.notranslate}]{.pre} level) don't necessarily mean there is a problem,
so you may not need to fix them.</p>
<p>Those messages are thrown by the Offsite Spider Middleware, which is a
spider middleware (enabled by default) whose purpose is to filter out
requests to domains outside the ones covered by the spider.</p>
<p>For more info see: <a href="index.html#scrapy.spidermiddlewares.offsite.OffsiteMiddleware" title="scrapy.spidermiddlewares.offsite.OffsiteMiddleware">[<code>OffsiteMiddleware</code>{.xref .py .py-class .docutils
.literal
.notranslate}]{.pre}</a>{.reference
.internal}.
:::</p>
<p>::: {#what-is-the-recommended-way-to-deploy-a-scrapy-crawler-in-production .section}</p>
<h4 id="what-is-the-recommended-way-to-deploy-a-scrapy-crawler-in-productionheaderlink"><a class="header" href="#what-is-the-recommended-way-to-deploy-a-scrapy-crawler-in-productionheaderlink">What is the recommended way to deploy a Scrapy crawler in production?<a href="#what-is-the-recommended-way-to-deploy-a-scrapy-crawler-in-production" title="Permalink to this heading">¶</a>{.headerlink}</a></h4>
<p>See <a href="index.html#topics-deploy">[Deploying Spiders]{.std
.std-ref}</a>{.hoverxref .tooltip .reference
.internal}.
:::</p>
<p>::: {#can-i-use-json-for-large-exports .section}</p>
<h4 id="can-i-use-json-for-large-exportsheaderlink"><a class="header" href="#can-i-use-json-for-large-exportsheaderlink">Can I use JSON for large exports?<a href="#can-i-use-json-for-large-exports" title="Permalink to this heading">¶</a>{.headerlink}</a></h4>
<p>It'll depend on how large your output is. See <a href="index.html#json-with-large-data">[this warning]{.std
.std-ref}</a>{.hoverxref .tooltip
.reference .internal} in <a href="index.html#scrapy.exporters.JsonItemExporter" title="scrapy.exporters.JsonItemExporter">[<code>JsonItemExporter</code>{.xref .py .py-class
.docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal} documentation.
:::</p>
<p>::: {#can-i-return-twisted-deferreds-from-signal-handlers .section}</p>
<h4 id="can-i-return-twisted-deferreds-from-signal-handlersheaderlink"><a class="header" href="#can-i-return-twisted-deferreds-from-signal-handlersheaderlink">Can I return (Twisted) deferreds from signal handlers?<a href="#can-i-return-twisted-deferreds-from-signal-handlers" title="Permalink to this heading">¶</a>{.headerlink}</a></h4>
<p>Some signals support returning deferreds from their handlers, others
don't. See the <a href="index.html#topics-signals-ref">[Built-in signals reference]{.std
.std-ref}</a>{.hoverxref .tooltip .reference
.internal} to know which ones.
:::</p>
<p>::: {#what-does-the-response-status-code-999-mean .section}</p>
<h4 id="what-does-the-response-status-code-999-meanheaderlink"><a class="header" href="#what-does-the-response-status-code-999-meanheaderlink">What does the response status code 999 mean?<a href="#what-does-the-response-status-code-999-mean" title="Permalink to this heading">¶</a>{.headerlink}</a></h4>
<p>999 is a custom response status code used by Yahoo sites to throttle
requests. Try slowing down the crawling speed by using a download delay
of [<code>2</code>{.docutils .literal .notranslate}]{.pre} (or higher) in your
spider:</p>
<p>::: {.highlight-python .notranslate}
::: highlight
from scrapy.spiders import CrawlSpider</p>
<pre><code>class MySpider(CrawlSpider):
    name = &quot;myspider&quot;

    download_delay = 2

    # [ ... rest of the spider code ... ]
</code></pre>
<p>:::
:::</p>
<p>Or by setting a global download delay in your project with the
<a href="index.html#std-setting-DOWNLOAD_DELAY">[<code>DOWNLOAD_DELAY</code>{.xref .std .std-setting .docutils .literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal} setting.
:::</p>
<p>::: {#can-i-call-pdb-set-trace-from-my-spiders-to-debug-them .section}</p>
<h4 id="can-i-call-pdbset_tracedocutils-literal-notranslatepre-from-my-spiders-to-debug-themheaderlink"><a class="header" href="#can-i-call-pdbset_tracedocutils-literal-notranslatepre-from-my-spiders-to-debug-themheaderlink">Can I call [<code>pdb.set_trace()</code>{.docutils .literal .notranslate}]{.pre} from my spiders to debug them?<a href="#can-i-call-pdb-set-trace-from-my-spiders-to-debug-them" title="Permalink to this heading">¶</a>{.headerlink}</a></h4>
<p>Yes, but you can also use the Scrapy shell which allows you to quickly
analyze (and even modify) the response being processed by your spider,
which is, quite often, more useful than plain old
[<code>pdb.set_trace()</code>{.docutils .literal .notranslate}]{.pre}.</p>
<p>For more info see <a href="index.html#topics-shell-inspect-response">[Invoking the shell from spiders to inspect
responses]{.std
.std-ref}</a>{.hoverxref .tooltip
.reference .internal}.
:::</p>
<p>::: {#simplest-way-to-dump-all-my-scraped-items-into-a-json-csv-xml-file .section}</p>
<h4 id="simplest-way-to-dump-all-my-scraped-items-into-a-jsoncsvxml-fileheaderlink"><a class="header" href="#simplest-way-to-dump-all-my-scraped-items-into-a-jsoncsvxml-fileheaderlink">Simplest way to dump all my scraped items into a JSON/CSV/XML file?<a href="#simplest-way-to-dump-all-my-scraped-items-into-a-json-csv-xml-file" title="Permalink to this heading">¶</a>{.headerlink}</a></h4>
<p>To dump into a JSON file:</p>
<p>::: {.highlight-default .notranslate}
::: highlight
scrapy crawl myspider -O items.json
:::
:::</p>
<p>To dump into a CSV file:</p>
<p>::: {.highlight-default .notranslate}
::: highlight
scrapy crawl myspider -O items.csv
:::
:::</p>
<p>To dump into a XML file:</p>
<p>::: {.highlight-default .notranslate}
::: highlight
scrapy crawl myspider -O items.xml
:::
:::</p>
<p>For more information see <a href="index.html#topics-feed-exports">[Feed exports]{.std
.std-ref}</a>{.hoverxref .tooltip
.reference .internal}
:::</p>
<p>::: {#what-s-this-huge-cryptic-viewstate-parameter-used-in-some-forms .section}</p>
<h4 id="whats-this-huge-cryptic-__viewstatedocutils-literal-notranslatepre-parameter-used-in-some-formsheaderlink"><a class="header" href="#whats-this-huge-cryptic-__viewstatedocutils-literal-notranslatepre-parameter-used-in-some-formsheaderlink">What's this huge cryptic [<code>__VIEWSTATE</code>{.docutils .literal .notranslate}]{.pre} parameter used in some forms?<a href="#what-s-this-huge-cryptic-viewstate-parameter-used-in-some-forms" title="Permalink to this heading">¶</a>{.headerlink}</a></h4>
<p>The [<code>__VIEWSTATE</code>{.docutils .literal .notranslate}]{.pre} parameter is
used in sites built with ASP.NET/VB.NET. For more info on how it works
see <a href="https://metacpan.org/pod/release/ECARROLL/HTML-TreeBuilderX-ASP_NET-0.09/lib/HTML/TreeBuilderX/ASP_NET.pm">this
page</a>{.reference
.external}. Also, here's an <a href="https://github.com/AmbientLighter/rpn-fas/blob/master/fas/spiders/rnp.py">example
spider</a>{.reference
.external} which scrapes one of these sites.
:::</p>
<p>::: {#what-s-the-best-way-to-parse-big-xml-csv-data-feeds .section}</p>
<h4 id="whats-the-best-way-to-parse-big-xmlcsv-data-feedsheaderlink"><a class="header" href="#whats-the-best-way-to-parse-big-xmlcsv-data-feedsheaderlink">What's the best way to parse big XML/CSV data feeds?<a href="#what-s-the-best-way-to-parse-big-xml-csv-data-feeds" title="Permalink to this heading">¶</a>{.headerlink}</a></h4>
<p>Parsing big feeds with XPath selectors can be problematic since they
need to build the DOM of the entire feed in memory, and this can be
quite slow and consume a lot of memory.</p>
<p>In order to avoid parsing all the entire feed at once in memory, you can
use the functions [<code>xmliter</code>{.docutils .literal .notranslate}]{.pre} and
[<code>csviter</code>{.docutils .literal .notranslate}]{.pre} from
[<code>scrapy.utils.iterators</code>{.docutils .literal .notranslate}]{.pre}
module. In fact, this is what the feed spiders (see <a href="index.html#topics-spiders">[Spiders]{.std
.std-ref}</a>{.hoverxref .tooltip .reference
.internal}) use under the cover.
:::</p>
<p>::: {#does-scrapy-manage-cookies-automatically .section}</p>
<h4 id="does-scrapy-manage-cookies-automaticallyheaderlink"><a class="header" href="#does-scrapy-manage-cookies-automaticallyheaderlink">Does Scrapy manage cookies automatically?<a href="#does-scrapy-manage-cookies-automatically" title="Permalink to this heading">¶</a>{.headerlink}</a></h4>
<p>Yes, Scrapy receives and keeps track of cookies sent by servers, and
sends them back on subsequent requests, like any regular web browser
does.</p>
<p>For more info see <a href="index.html#topics-request-response">[Requests and Responses]{.std
.std-ref}</a>{.hoverxref .tooltip
.reference .internal} and <a href="index.html#cookies-mw">[CookiesMiddleware]{.std
.std-ref}</a>{.hoverxref .tooltip .reference
.internal}.
:::</p>
<p>::: {#how-can-i-see-the-cookies-being-sent-and-received-from-scrapy .section}</p>
<h4 id="how-can-i-see-the-cookies-being-sent-and-received-from-scrapyheaderlink"><a class="header" href="#how-can-i-see-the-cookies-being-sent-and-received-from-scrapyheaderlink">How can I see the cookies being sent and received from Scrapy?<a href="#how-can-i-see-the-cookies-being-sent-and-received-from-scrapy" title="Permalink to this heading">¶</a>{.headerlink}</a></h4>
<p>Enable the <a href="index.html#std-setting-COOKIES_DEBUG">[<code>COOKIES_DEBUG</code>{.xref .std .std-setting .docutils .literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal} setting.
:::</p>
<p>::: {#how-can-i-instruct-a-spider-to-stop-itself .section}</p>
<h4 id="how-can-i-instruct-a-spider-to-stop-itselfheaderlink"><a class="header" href="#how-can-i-instruct-a-spider-to-stop-itselfheaderlink">How can I instruct a spider to stop itself?<a href="#how-can-i-instruct-a-spider-to-stop-itself" title="Permalink to this heading">¶</a>{.headerlink}</a></h4>
<p>Raise the <a href="index.html#scrapy.exceptions.CloseSpider" title="scrapy.exceptions.CloseSpider">[<code>CloseSpider</code>{.xref .py .py-exc .docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal} exception from a callback. For more info see:
<a href="index.html#scrapy.exceptions.CloseSpider" title="scrapy.exceptions.CloseSpider">[<code>CloseSpider</code>{.xref .py .py-exc .docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal}.
:::</p>
<p>::: {#how-can-i-prevent-my-scrapy-bot-from-getting-banned .section}</p>
<h4 id="how-can-i-prevent-my-scrapy-bot-from-getting-bannedheaderlink"><a class="header" href="#how-can-i-prevent-my-scrapy-bot-from-getting-bannedheaderlink">How can I prevent my Scrapy bot from getting banned?<a href="#how-can-i-prevent-my-scrapy-bot-from-getting-banned" title="Permalink to this heading">¶</a>{.headerlink}</a></h4>
<p>See <a href="index.html#bans">[Avoiding getting banned]{.std
.std-ref}</a>{.hoverxref .tooltip .reference .internal}.
:::</p>
<p>::: {#should-i-use-spider-arguments-or-settings-to-configure-my-spider .section}</p>
<h4 id="should-i-use-spider-arguments-or-settings-to-configure-my-spiderheaderlink"><a class="header" href="#should-i-use-spider-arguments-or-settings-to-configure-my-spiderheaderlink">Should I use spider arguments or settings to configure my spider?<a href="#should-i-use-spider-arguments-or-settings-to-configure-my-spider" title="Permalink to this heading">¶</a>{.headerlink}</a></h4>
<p>Both <a href="index.html#spiderargs">[spider arguments]{.std
.std-ref}</a>{.hoverxref .tooltip .reference
.internal} and <a href="index.html#topics-settings">[settings]{.std
.std-ref}</a>{.hoverxref .tooltip .reference
.internal} can be used to configure your spider. There is no strict rule
that mandates to use one or the other, but settings are more suited for
parameters that, once set, don't change much, while spider arguments are
meant to change more often, even on each spider run and sometimes are
required for the spider to run at all (for example, to set the start url
of a spider).</p>
<p>To illustrate with an example, assuming you have a spider that needs to
log into a site to scrape data, and you only want to scrape data from a
certain section of the site (which varies each time). In that case, the
credentials to log in would be settings, while the url of the section to
scrape would be a spider argument.
:::</p>
<p>::: {#i-m-scraping-a-xml-document-and-my-xpath-selector-doesn-t-return-any-items .section}</p>
<h4 id="im-scraping-a-xml-document-and-my-xpath-selector-doesnt-return-any-itemsheaderlink"><a class="header" href="#im-scraping-a-xml-document-and-my-xpath-selector-doesnt-return-any-itemsheaderlink">I'm scraping a XML document and my XPath selector doesn't return any items<a href="#i-m-scraping-a-xml-document-and-my-xpath-selector-doesn-t-return-any-items" title="Permalink to this heading">¶</a>{.headerlink}</a></h4>
<p>You may need to remove namespaces. See <a href="index.html#removing-namespaces">[Removing namespaces]{.std
.std-ref}</a>{.hoverxref .tooltip
.reference .internal}.
:::</p>
<p>::: {#how-to-split-an-item-into-multiple-items-in-an-item-pipeline .section}
[]{#faq-split-item}</p>
<h4 id="how-to-split-an-item-into-multiple-items-in-an-item-pipelineheaderlink"><a class="header" href="#how-to-split-an-item-into-multiple-items-in-an-item-pipelineheaderlink">How to split an item into multiple items in an item pipeline?<a href="#how-to-split-an-item-into-multiple-items-in-an-item-pipeline" title="Permalink to this heading">¶</a>{.headerlink}</a></h4>
<p><a href="index.html#topics-item-pipeline">[Item pipelines]{.std
.std-ref}</a>{.hoverxref .tooltip
.reference .internal} cannot yield multiple items per input item.
<a href="index.html#custom-spider-middleware">[Create a spider middleware]{.std
.std-ref}</a>{.hoverxref .tooltip
.reference .internal} instead, and use its
<a href="index.html#scrapy.spidermiddlewares.SpiderMiddleware.process_spider_output" title="scrapy.spidermiddlewares.SpiderMiddleware.process_spider_output">[<code>process_spider_output()</code>{.xref .py .py-meth .docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal} method for this purpose. For example:</p>
<p>::: {.highlight-python .notranslate}
::: highlight
from copy import deepcopy</p>
<pre><code>from itemadapter import is_item, ItemAdapter


class MultiplyItemsMiddleware:
    def process_spider_output(self, response, result, spider):
        for item in result:
            if is_item(item):
                adapter = ItemAdapter(item)
                for _ in range(adapter[&quot;multiply_by&quot;]):
                    yield deepcopy(item)
</code></pre>
<p>:::
:::
:::</p>
<p>::: {#does-scrapy-support-ipv6-addresses .section}</p>
<h4 id="does-scrapy-support-ipv6-addressesheaderlink"><a class="header" href="#does-scrapy-support-ipv6-addressesheaderlink">Does Scrapy support IPv6 addresses?<a href="#does-scrapy-support-ipv6-addresses" title="Permalink to this heading">¶</a>{.headerlink}</a></h4>
<p>Yes, by setting <a href="index.html#std-setting-DNS_RESOLVER">[<code>DNS_RESOLVER</code>{.xref .std .std-setting .docutils
.literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal} to
[<code>scrapy.resolver.CachingHostnameResolver</code>{.docutils .literal
.notranslate}]{.pre}. Note that by doing so, you lose the ability to set
a specific timeout for DNS requests (the value of the
<a href="index.html#std-setting-DNS_TIMEOUT">[<code>DNS_TIMEOUT</code>{.xref .std .std-setting .docutils .literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal} setting is ignored).
:::</p>
<p>::: {#how-to-deal-with-class-valueerror-filedescriptor-out-of-range-in-select-exceptions .section}
[]{#faq-specific-reactor}</p>
<h4 id="how-to-deal-with-classdocutils-literal-notranslatepre-docutils-literal-notranslatevalueerrordocutils-literal-notranslatepre-docutils-literal-notranslatefiledescriptordocutils-literal-notranslatepre-docutils-literal-notranslateoutdocutils-literal-notranslatepre-docutils-literal-notranslateofdocutils-literal-notranslatepre-docutils-literal-notranslaterangedocutils-literal-notranslatepre-docutils-literal-notranslateindocutils-literal-notranslatepre-docutils-literal-notranslateselectdocutils-literal-notranslatepre-exceptionsheaderlink"><a class="header" href="#how-to-deal-with-classdocutils-literal-notranslatepre-docutils-literal-notranslatevalueerrordocutils-literal-notranslatepre-docutils-literal-notranslatefiledescriptordocutils-literal-notranslatepre-docutils-literal-notranslateoutdocutils-literal-notranslatepre-docutils-literal-notranslateofdocutils-literal-notranslatepre-docutils-literal-notranslaterangedocutils-literal-notranslatepre-docutils-literal-notranslateindocutils-literal-notranslatepre-docutils-literal-notranslateselectdocutils-literal-notranslatepre-exceptionsheaderlink">How to deal with [<code>&lt;class</code>{.docutils .literal .notranslate}]{.pre}<code> </code>{.docutils .literal .notranslate}[<code>'ValueError'&gt;:</code>{.docutils .literal .notranslate}]{.pre}<code> </code>{.docutils .literal .notranslate}[<code>filedescriptor</code>{.docutils .literal .notranslate}]{.pre}<code> </code>{.docutils .literal .notranslate}[<code>out</code>{.docutils .literal .notranslate}]{.pre}<code> </code>{.docutils .literal .notranslate}[<code>of</code>{.docutils .literal .notranslate}]{.pre}<code> </code>{.docutils .literal .notranslate}[<code>range</code>{.docutils .literal .notranslate}]{.pre}<code> </code>{.docutils .literal .notranslate}[<code>in</code>{.docutils .literal .notranslate}]{.pre}<code> </code>{.docutils .literal .notranslate}[<code>select()</code>{.docutils .literal .notranslate}]{.pre} exceptions?<a href="#how-to-deal-with-class-valueerror-filedescriptor-out-of-range-in-select-exceptions" title="Permalink to this heading">¶</a>{.headerlink}</a></h4>
<p>This issue <a href="https://github.com/scrapy/scrapy/issues/2905">has been
reported</a>{.reference
.external} to appear when running broad crawls in macOS, where the
default Twisted reactor is
<a href="https://docs.twisted.org/en/stable/api/twisted.internet.selectreactor.SelectReactor.html" title="(in Twisted)">[<code>twisted.internet.selectreactor.SelectReactor</code>{.xref .py .py-class
.docutils .literal
.notranslate}]{.pre}</a>{.reference
.external}. Switching to a different reactor is possible by using the
<a href="index.html#std-setting-TWISTED_REACTOR">[<code>TWISTED_REACTOR</code>{.xref .std .std-setting .docutils .literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal} setting.
:::</p>
<p>::: {#how-can-i-cancel-the-download-of-a-given-response .section}
[]{#faq-stop-response-download}</p>
<h4 id="how-can-i-cancel-the-download-of-a-given-responseheaderlink"><a class="header" href="#how-can-i-cancel-the-download-of-a-given-responseheaderlink">How can I cancel the download of a given response?<a href="#how-can-i-cancel-the-download-of-a-given-response" title="Permalink to this heading">¶</a>{.headerlink}</a></h4>
<p>In some situations, it might be useful to stop the download of a certain
response. For instance, sometimes you can determine whether or not you
need the full contents of a response by inspecting its headers or the
first bytes of its body. In that case, you could save resources by
attaching a handler to the <a href="index.html#scrapy.signals.bytes_received" title="scrapy.signals.bytes_received">[<code>bytes_received</code>{.xref .py .py-class
.docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal} or <a href="index.html#scrapy.signals.headers_received" title="scrapy.signals.headers_received">[<code>headers_received</code>{.xref .py .py-class .docutils
.literal
.notranslate}]{.pre}</a>{.reference
.internal} signals and raising a <a href="index.html#scrapy.exceptions.StopDownload" title="scrapy.exceptions.StopDownload">[<code>StopDownload</code>{.xref .py .py-exc
.docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal} exception. Please refer to the <a href="index.html#topics-stop-response-download">[Stopping the download of a
Response]{.std
.std-ref}</a>{.hoverxref .tooltip
.reference .internal} topic for additional information and examples.
:::</p>
<p>::: {#running-runspider-i-get-error-no-spider-found-in-file-filename .section}</p>
<h4 id="running-runspiderdocutils-literal-notranslatepre-i-get-errordocutils-literal-notranslatepre-docutils-literal-notranslatenodocutils-literal-notranslatepre-docutils-literal-notranslatespiderdocutils-literal-notranslatepre-docutils-literal-notranslatefounddocutils-literal-notranslatepre-docutils-literal-notranslateindocutils-literal-notranslatepre-docutils-literal-notranslatefiledocutils-literal-notranslatepre-docutils-literal-notranslatefilenamedocutils-literal-notranslatepreheaderlink"><a class="header" href="#running-runspiderdocutils-literal-notranslatepre-i-get-errordocutils-literal-notranslatepre-docutils-literal-notranslatenodocutils-literal-notranslatepre-docutils-literal-notranslatespiderdocutils-literal-notranslatepre-docutils-literal-notranslatefounddocutils-literal-notranslatepre-docutils-literal-notranslateindocutils-literal-notranslatepre-docutils-literal-notranslatefiledocutils-literal-notranslatepre-docutils-literal-notranslatefilenamedocutils-literal-notranslatepreheaderlink">Running [<code>runspider</code>{.docutils .literal .notranslate}]{.pre} I get [<code>error:</code>{.docutils .literal .notranslate}]{.pre}<code> </code>{.docutils .literal .notranslate}[<code>No</code>{.docutils .literal .notranslate}]{.pre}<code> </code>{.docutils .literal .notranslate}[<code>spider</code>{.docutils .literal .notranslate}]{.pre}<code> </code>{.docutils .literal .notranslate}[<code>found</code>{.docutils .literal .notranslate}]{.pre}<code> </code>{.docutils .literal .notranslate}[<code>in</code>{.docutils .literal .notranslate}]{.pre}<code> </code>{.docutils .literal .notranslate}[<code>file:</code>{.docutils .literal .notranslate}]{.pre}<code> </code>{.docutils .literal .notranslate}[<code>&lt;filename&gt;</code>{.docutils .literal .notranslate}]{.pre}<a href="#running-runspider-i-get-error-no-spider-found-in-file-filename" title="Permalink to this heading">¶</a>{.headerlink}</a></h4>
<p>This may happen if your Scrapy project has a spider module with a name
that conflicts with the name of one of the <a href="https://docs.python.org/py-modindex.html">Python standard library
modules</a>{.reference
.external}, such as [<code>csv.py</code>{.docutils .literal .notranslate}]{.pre} or
[<code>os.py</code>{.docutils .literal .notranslate}]{.pre}, or any <a href="https://pypi.org/">Python
package</a>{.reference .external} that you have
installed. See <a href="https://github.com/scrapy/scrapy/issues/2680">issue
2680</a>{.reference
.external}.
:::
:::</p>
<p>[]{#document-topics/debug}</p>
<p>::: {#debugging-spiders .section}
[]{#topics-debug}</p>
<h3 id="debugging-spidersheaderlink"><a class="header" href="#debugging-spidersheaderlink">Debugging Spiders<a href="#debugging-spiders" title="Permalink to this heading">¶</a>{.headerlink}</a></h3>
<p>This document explains the most common techniques for debugging spiders.
Consider the following Scrapy spider below:</p>
<p>::: {.highlight-python .notranslate}
::: highlight
import scrapy
from myproject.items import MyItem</p>
<pre><code>class MySpider(scrapy.Spider):
    name = &quot;myspider&quot;
    start_urls = (
        &quot;http://example.com/page1&quot;,
        &quot;http://example.com/page2&quot;,
    )

    def parse(self, response):
        # &lt;processing code not shown&gt;
        # collect `item_urls`
        for item_url in item_urls:
            yield scrapy.Request(item_url, self.parse_item)

    def parse_item(self, response):
        # &lt;processing code not shown&gt;
        item = MyItem()
        # populate `item` fields
        # and extract item_details_url
        yield scrapy.Request(
            item_details_url, self.parse_details, cb_kwargs={&quot;item&quot;: item}
        )

    def parse_details(self, response, item):
        # populate more `item` fields
        return item
</code></pre>
<p>:::
:::</p>
<p>Basically this is a simple spider which parses two pages of items (the
start_urls). Items also have a details page with additional information,
so we use the [<code>cb_kwargs</code>{.docutils .literal .notranslate}]{.pre}
functionality of [<code>Request</code>{.xref .py .py-class .docutils .literal
.notranslate}]{.pre} to pass a partially populated item.</p>
<p>::: {#parse-command .section}</p>
<h4 id="parse-commandheaderlink"><a class="header" href="#parse-commandheaderlink">Parse Command<a href="#parse-command" title="Permalink to this heading">¶</a>{.headerlink}</a></h4>
<p>The most basic way of checking the output of your spider is to use the
<a href="index.html#std-command-parse">[<code>parse</code>{.xref .std .std-command .docutils .literal
.notranslate}]{.pre}</a>{.hoverxref .tooltip
.reference .internal} command. It allows to check the behaviour of
different parts of the spider at the method level. It has the advantage
of being flexible and simple to use, but does not allow debugging code
inside a method.</p>
<p>In order to see the item scraped from a specific url:</p>
<p>::: {.highlight-none .notranslate}
::: highlight
$ scrapy parse --spider=myspider -c parse_item -d 2 &lt;item_url&gt;
[ ... scrapy log lines crawling example.com spider ... ]</p>
<pre><code>&gt;&gt;&gt; STATUS DEPTH LEVEL 2 &lt;&lt;&lt;
# Scraped Items  ------------------------------------------------------------
[{'url': &lt;item_url&gt;}]

# Requests  -----------------------------------------------------------------
[]
</code></pre>
<p>:::
:::</p>
<p>Using the [<code>--verbose</code>{.docutils .literal .notranslate}]{.pre} or
[<code>-v</code>{.docutils .literal .notranslate}]{.pre} option we can see the
status at each depth level:</p>
<p>::: {.highlight-none .notranslate}
::: highlight
$ scrapy parse --spider=myspider -c parse_item -d 2 -v &lt;item_url&gt;
[ ... scrapy log lines crawling example.com spider ... ]</p>
<pre><code>&gt;&gt;&gt; DEPTH LEVEL: 1 &lt;&lt;&lt;
# Scraped Items  ------------------------------------------------------------
[]

# Requests  -----------------------------------------------------------------
[&lt;GET item_details_url&gt;]


&gt;&gt;&gt; DEPTH LEVEL: 2 &lt;&lt;&lt;
# Scraped Items  ------------------------------------------------------------
[{'url': &lt;item_url&gt;}]

# Requests  -----------------------------------------------------------------
[]
</code></pre>
<p>:::
:::</p>
<p>Checking items scraped from a single start_url, can also be easily
achieved using:</p>
<p>::: {.highlight-none .notranslate}
::: highlight
$ scrapy parse --spider=myspider -d 3 'http://example.com/page1'
:::
:::
:::</p>
<p>::: {#scrapy-shell .section}</p>
<h4 id="scrapy-shellheaderlink-1"><a class="header" href="#scrapy-shellheaderlink-1">Scrapy Shell<a href="#scrapy-shell" title="Permalink to this heading">¶</a>{.headerlink}</a></h4>
<p>While the <a href="index.html#std-command-parse">[<code>parse</code>{.xref .std .std-command .docutils .literal
.notranslate}]{.pre}</a>{.hoverxref .tooltip
.reference .internal} command is very useful for checking behaviour of a
spider, it is of little help to check what happens inside a callback,
besides showing the response received and the output. How to debug the
situation when [<code>parse_details</code>{.docutils .literal .notranslate}]{.pre}
sometimes receives no item?</p>
<p>Fortunately, the <a href="index.html#std-command-shell">[<code>shell</code>{.xref .std .std-command .docutils .literal
.notranslate}]{.pre}</a>{.hoverxref .tooltip
.reference .internal} is your bread and butter in this case (see
<a href="index.html#topics-shell-inspect-response">[Invoking the shell from spiders to inspect responses]{.std
.std-ref}</a>{.hoverxref .tooltip
.reference .internal}):</p>
<p>::: {.highlight-python .notranslate}
::: highlight
from scrapy.shell import inspect_response</p>
<pre><code>def parse_details(self, response, item=None):
    if item:
        # populate more `item` fields
        return item
    else:
        inspect_response(response, self)
</code></pre>
<p>:::
:::</p>
<p>See also: <a href="index.html#topics-shell-inspect-response">[Invoking the shell from spiders to inspect responses]{.std
.std-ref}</a>{.hoverxref .tooltip
.reference .internal}.
:::</p>
<p>::: {#open-in-browser .section}</p>
<h4 id="open-in-browserheaderlink"><a class="header" href="#open-in-browserheaderlink">Open in browser<a href="#open-in-browser" title="Permalink to this heading">¶</a>{.headerlink}</a></h4>
<p>Sometimes you just want to see how a certain response looks in a
browser, you can use the [<code>open_in_browser</code>{.docutils .literal
.notranslate}]{.pre} function for that. Here is an example of how you
would use it:</p>
<p>::: {.highlight-python .notranslate}
::: highlight
from scrapy.utils.response import open_in_browser</p>
<pre><code>def parse_details(self, response):
    if &quot;item name&quot; not in response.body:
        open_in_browser(response)
</code></pre>
<p>:::
:::</p>
<p>[<code>open_in_browser</code>{.docutils .literal .notranslate}]{.pre} will open a
browser with the response received by Scrapy at that point, adjusting
the <a href="https://www.w3schools.com/tags/tag_base.asp">base tag</a>{.reference
.external} so that images and styles are displayed properly.
:::</p>
<p>::: {#logging .section}</p>
<h4 id="loggingheaderlink-1"><a class="header" href="#loggingheaderlink-1">Logging<a href="#logging" title="Permalink to this heading">¶</a>{.headerlink}</a></h4>
<p>Logging is another useful option for getting information about your
spider run. Although not as convenient, it comes with the advantage that
the logs will be available in all future runs should they be necessary
again:</p>
<p>::: {.highlight-python .notranslate}
::: highlight
def parse_details(self, response, item=None):
if item:
# populate more <code>item</code> fields
return item
else:
self.logger.warning(&quot;No item received for %s&quot;, response.url)
:::
:::</p>
<p>For more information, check the <a href="index.html#topics-logging">[Logging]{.std
.std-ref}</a>{.hoverxref .tooltip .reference
.internal} section.
:::</p>
<p>::: {#visual-studio-code .section}
[]{#debug-vscode}</p>
<h4 id="visual-studio-codeheaderlink"><a class="header" href="#visual-studio-codeheaderlink">Visual Studio Code<a href="#visual-studio-code" title="Permalink to this heading">¶</a>{.headerlink}</a></h4>
<p>To debug spiders with Visual Studio Code you can use the following
[<code>launch.json</code>{.docutils .literal .notranslate}]{.pre}:</p>
<p>::: {.highlight-json .notranslate}
::: highlight
{
&quot;version&quot;: &quot;0.1.0&quot;,
&quot;configurations&quot;: [
{
&quot;name&quot;: &quot;Python: Launch Scrapy Spider&quot;,
&quot;type&quot;: &quot;python&quot;,
&quot;request&quot;: &quot;launch&quot;,
&quot;module&quot;: &quot;scrapy&quot;,
&quot;args&quot;: [
&quot;runspider&quot;,
&quot;${file}&quot;
],
&quot;console&quot;: &quot;integratedTerminal&quot;
}
]
}
:::
:::</p>
<p>Also, make sure you enable &quot;User Uncaught Exceptions&quot;, to catch
exceptions in your Scrapy spider.
:::
:::</p>
<p>[]{#document-topics/contracts}</p>
<p>::: {#spiders-contracts .section}
[]{#topics-contracts}</p>
<h3 id="spiders-contractsheaderlink"><a class="header" href="#spiders-contractsheaderlink">Spiders Contracts<a href="#spiders-contracts" title="Permalink to this heading">¶</a>{.headerlink}</a></h3>
<p>Testing spiders can get particularly annoying and while nothing prevents
you from writing unit tests the task gets cumbersome quickly. Scrapy
offers an integrated way of testing your spiders by the means of
contracts.</p>
<p>This allows you to test each callback of your spider by hardcoding a
sample url and check various constraints for how the callback processes
the response. Each contract is prefixed with an [<code>@</code>{.docutils .literal
.notranslate}]{.pre} and included in the docstring. See the following
example:</p>
<p>::: {.highlight-python .notranslate}
::: highlight
def parse(self, response):
&quot;&quot;&quot;
This function parses a sample response. Some contracts are mingled
with this docstring.</p>
<pre><code>    @url http://www.amazon.com/s?field-keywords=selfish+gene
    @returns items 1 16
    @returns requests 0 0
    @scrapes Title Author Year Price
    &quot;&quot;&quot;
</code></pre>
<p>:::
:::</p>
<p>This callback is tested using three built-in contracts:</p>
<p>[]{#module-scrapy.contracts.default .target}</p>
<p><em>[class]{.pre}[ ]{.w}</em>[[scrapy.contracts.default.]{.pre}]{.sig-prename .descclassname}[[UrlContract]{.pre}]{.sig-name .descname}<a href="_modules/scrapy/contracts/default.html#UrlContract">[[[source]]{.pre}]{.viewcode-link}</a>{.reference .internal}<a href="#scrapy.contracts.default.UrlContract" title="Permalink to this definition">¶</a>{.headerlink}</p>
<p>:   This contract ([<code>@url</code>{.docutils .literal .notranslate}]{.pre}) sets
the sample URL used when checking other contract conditions for this
spider. This contract is mandatory. All callbacks lacking this
contract are ignored when running the checks:</p>
<pre><code>::: {.highlight-default .notranslate}
::: highlight
    @url url
:::
:::
</code></pre>
<pre><code class="language-{=html}">&lt;!-- --&gt;
</code></pre>
<p><em>[class]{.pre}[ ]{.w}</em>[[scrapy.contracts.default.]{.pre}]{.sig-prename .descclassname}[[CallbackKeywordArgumentsContract]{.pre}]{.sig-name .descname}<a href="_modules/scrapy/contracts/default.html#CallbackKeywordArgumentsContract">[[[source]]{.pre}]{.viewcode-link}</a>{.reference .internal}<a href="#scrapy.contracts.default.CallbackKeywordArgumentsContract" title="Permalink to this definition">¶</a>{.headerlink}</p>
<p>:   This contract ([<code>@cb_kwargs</code>{.docutils .literal
.notranslate}]{.pre}) sets the [<code>cb_kwargs</code>{.xref .py .py-attr
.docutils .literal .notranslate}]{.pre} attribute for the sample
request. It must be a valid JSON dictionary.</p>
<pre><code>::: {.highlight-default .notranslate}
::: highlight
    @cb_kwargs {&quot;arg1&quot;: &quot;value1&quot;, &quot;arg2&quot;: &quot;value2&quot;, ...}
:::
:::
</code></pre>
<pre><code class="language-{=html}">&lt;!-- --&gt;
</code></pre>
<p><em>[class]{.pre}[ ]{.w}</em>[[scrapy.contracts.default.]{.pre}]{.sig-prename .descclassname}[[ReturnsContract]{.pre}]{.sig-name .descname}<a href="_modules/scrapy/contracts/default.html#ReturnsContract">[[[source]]{.pre}]{.viewcode-link}</a>{.reference .internal}<a href="#scrapy.contracts.default.ReturnsContract" title="Permalink to this definition">¶</a>{.headerlink}</p>
<p>:   This contract ([<code>@returns</code>{.docutils .literal .notranslate}]{.pre})
sets lower and upper bounds for the items and requests returned by
the spider. The upper bound is optional:</p>
<pre><code>::: {.highlight-default .notranslate}
::: highlight
    @returns item(s)|request(s) [min [max]]
:::
:::
</code></pre>
<pre><code class="language-{=html}">&lt;!-- --&gt;
</code></pre>
<p><em>[class]{.pre}[ ]{.w}</em>[[scrapy.contracts.default.]{.pre}]{.sig-prename .descclassname}[[ScrapesContract]{.pre}]{.sig-name .descname}<a href="_modules/scrapy/contracts/default.html#ScrapesContract">[[[source]]{.pre}]{.viewcode-link}</a>{.reference .internal}<a href="#scrapy.contracts.default.ScrapesContract" title="Permalink to this definition">¶</a>{.headerlink}</p>
<p>:   This contract ([<code>@scrapes</code>{.docutils .literal .notranslate}]{.pre})
checks that all the items returned by the callback have the
specified fields:</p>
<pre><code>::: {.highlight-default .notranslate}
::: highlight
    @scrapes field_1 field_2 ...
:::
:::
</code></pre>
<p>Use the <a href="index.html#std-command-check">[<code>check</code>{.xref .std .std-command .docutils .literal
.notranslate}]{.pre}</a>{.hoverxref .tooltip
.reference .internal} command to run the contract checks.</p>
<p>::: {#custom-contracts .section}</p>
<h4 id="custom-contractsheaderlink"><a class="header" href="#custom-contractsheaderlink">Custom Contracts<a href="#custom-contracts" title="Permalink to this heading">¶</a>{.headerlink}</a></h4>
<p>If you find you need more power than the built-in Scrapy contracts you
can create and load your own contracts in the project by using the
<a href="index.html#std-setting-SPIDER_CONTRACTS">[<code>SPIDER_CONTRACTS</code>{.xref .std .std-setting .docutils .literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal} setting:</p>
<p>::: {.highlight-python .notranslate}
::: highlight
SPIDER_CONTRACTS = {
&quot;myproject.contracts.ResponseCheck&quot;: 10,
&quot;myproject.contracts.ItemValidate&quot;: 10,
}
:::
:::</p>
<p>Each contract must inherit from <a href="#scrapy.contracts.Contract" title="scrapy.contracts.Contract">[<code>Contract</code>{.xref .py .py-class
.docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal} and can override three methods:</p>
<p>[]{#module-scrapy.contracts .target}</p>
<p><em>[class]{.pre}[ ]{.w}</em>[[scrapy.contracts.]{.pre}]{.sig-prename .descclassname}[[Contract]{.pre}]{.sig-name .descname}[(]{.sig-paren}<em>[[method]{.pre}]{.n}</em>, <em>[[*]{.pre}]{.o}[[args]{.pre}]{.n}</em>[)]{.sig-paren}<a href="_modules/scrapy/contracts.html#Contract">[[[source]]{.pre}]{.viewcode-link}</a>{.reference .internal}<a href="#scrapy.contracts.Contract" title="Permalink to this definition">¶</a>{.headerlink}</p>
<p>:</p>
<pre><code>Parameters

:   -   **method**
        ([*collections.abc.Callable*](https://docs.python.org/3/library/collections.abc.html#collections.abc.Callable &quot;(in Python v3.12)&quot;){.reference
        .external}) -- callback function to which the contract is
        associated

    -   **args**
        ([*list*](https://docs.python.org/3/library/stdtypes.html#list &quot;(in Python v3.12)&quot;){.reference
        .external}) -- list of arguments passed into the docstring
        (whitespace separated)

[[adjust_request_args]{.pre}]{.sig-name .descname}[(]{.sig-paren}*[[args]{.pre}]{.n}*[)]{.sig-paren}[[[\[source\]]{.pre}]{.viewcode-link}](_modules/scrapy/contracts.html#Contract.adjust_request_args){.reference .internal}[¶](#scrapy.contracts.Contract.adjust_request_args &quot;Permalink to this definition&quot;){.headerlink}

:   This receives a [`dict`{.docutils .literal .notranslate}]{.pre}
    as an argument containing default arguments for request object.
    [`Request`{.xref .py .py-class .docutils .literal
    .notranslate}]{.pre} is used by default, but this can be changed
    with the [`request_cls`{.docutils .literal .notranslate}]{.pre}
    attribute. If multiple contracts in chain have this attribute
    defined, the last one is used.

    Must return the same or a modified version of it.

[[pre_process]{.pre}]{.sig-name .descname}[(]{.sig-paren}*[[response]{.pre}]{.n}*[)]{.sig-paren}[¶](#scrapy.contracts.Contract.pre_process &quot;Permalink to this definition&quot;){.headerlink}

:   This allows hooking in various checks on the response received
    from the sample request, before it's being passed to the
    callback.

[[post_process]{.pre}]{.sig-name .descname}[(]{.sig-paren}*[[output]{.pre}]{.n}*[)]{.sig-paren}[¶](#scrapy.contracts.Contract.post_process &quot;Permalink to this definition&quot;){.headerlink}

:   This allows processing the output of the callback. Iterators are
    converted to lists before being passed to this hook.
</code></pre>
<p>Raise <a href="#scrapy.exceptions.ContractFail" title="scrapy.exceptions.ContractFail">[<code>ContractFail</code>{.xref .py .py-class .docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal} from <a href="#scrapy.contracts.Contract.pre_process" title="scrapy.contracts.Contract.pre_process">[<code>pre_process</code>{.xref .py .py-class .docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal} or <a href="#scrapy.contracts.Contract.post_process" title="scrapy.contracts.Contract.post_process">[<code>post_process</code>{.xref .py .py-class .docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal} if expectations are not met:</p>
<p><em>[class]{.pre}[ ]{.w}</em>[[scrapy.exceptions.]{.pre}]{.sig-prename .descclassname}[[ContractFail]{.pre}]{.sig-name .descname}<a href="_modules/scrapy/exceptions.html#ContractFail">[[[source]]{.pre}]{.viewcode-link}</a>{.reference .internal}<a href="#scrapy.exceptions.ContractFail" title="Permalink to this definition">¶</a>{.headerlink}</p>
<p>:   Error raised in case of a failing contract</p>
<p>Here is a demo contract which checks the presence of a custom header in
the response received:</p>
<p>::: {.highlight-python .notranslate}
::: highlight
from scrapy.contracts import Contract
from scrapy.exceptions import ContractFail</p>
<pre><code>class HasHeaderContract(Contract):
    &quot;&quot;&quot;
    Demo contract which checks the presence of a custom header
    @has_header X-CustomHeader
    &quot;&quot;&quot;

    name = &quot;has_header&quot;

    def pre_process(self, response):
        for header in self.args:
            if header not in response.headers:
                raise ContractFail(&quot;X-CustomHeader not present&quot;)
</code></pre>
<p>:::
:::
:::</p>
<p>::: {#detecting-check-runs .section}
[]{#detecting-contract-check-runs}</p>
<h4 id="detecting-check-runsheaderlink"><a class="header" href="#detecting-check-runsheaderlink">Detecting check runs<a href="#detecting-check-runs" title="Permalink to this heading">¶</a>{.headerlink}</a></h4>
<p>When [<code>scrapy</code>{.docutils .literal .notranslate}]{.pre}<code> </code>{.docutils
.literal .notranslate}[<code>check</code>{.docutils .literal .notranslate}]{.pre}
is running, the [<code>SCRAPY_CHECK</code>{.docutils .literal .notranslate}]{.pre}
environment variable is set to the [<code>true</code>{.docutils .literal
.notranslate}]{.pre} string. You can use <a href="https://docs.python.org/3/library/os.html#os.environ" title="(in Python v3.12)">[<code>os.environ</code>{.xref .py
.py-data .docutils .literal
.notranslate}]{.pre}</a>{.reference
.external} to perform any change to your spiders or your settings when
[<code>scrapy</code>{.docutils .literal .notranslate}]{.pre}<code> </code>{.docutils .literal
.notranslate}[<code>check</code>{.docutils .literal .notranslate}]{.pre} is used:</p>
<p>::: {.highlight-python .notranslate}
::: highlight
import os
import scrapy</p>
<pre><code>class ExampleSpider(scrapy.Spider):
    name = &quot;example&quot;

    def __init__(self):
        if os.environ.get(&quot;SCRAPY_CHECK&quot;):
            pass  # Do some scraper adjustments when a check is running
</code></pre>
<p>:::
:::
:::
:::</p>
<p>[]{#document-topics/practices}</p>
<p>::: {#common-practices .section}
[]{#topics-practices}</p>
<h3 id="common-practicesheaderlink"><a class="header" href="#common-practicesheaderlink">Common Practices<a href="#common-practices" title="Permalink to this heading">¶</a>{.headerlink}</a></h3>
<p>This section documents common practices when using Scrapy. These are
things that cover many topics and don't often fall into any other
specific section.</p>
<p>::: {#run-scrapy-from-a-script .section}
[]{#run-from-script}</p>
<h4 id="run-scrapy-from-a-scriptheaderlink"><a class="header" href="#run-scrapy-from-a-scriptheaderlink">Run Scrapy from a script<a href="#run-scrapy-from-a-script" title="Permalink to this heading">¶</a>{.headerlink}</a></h4>
<p>You can use the <a href="index.html#topics-api">[API]{.std .std-ref}</a>{.hoverxref
.tooltip .reference .internal} to run Scrapy from a script, instead of
the typical way of running Scrapy via [<code>scrapy</code>{.docutils .literal
.notranslate}]{.pre}<code> </code>{.docutils .literal
.notranslate}[<code>crawl</code>{.docutils .literal .notranslate}]{.pre}.</p>
<p>Remember that Scrapy is built on top of the Twisted asynchronous
networking library, so you need to run it inside the Twisted reactor.</p>
<p>The first utility you can use to run your spiders is
<a href="index.html#scrapy.crawler.CrawlerProcess" title="scrapy.crawler.CrawlerProcess">[<code>scrapy.crawler.CrawlerProcess</code>{.xref .py .py-class .docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal}. This class will start a Twisted reactor for you, configuring
the logging and setting shutdown handlers. This class is the one used by
all Scrapy commands.</p>
<p>Here's an example showing how to run a single spider with it.</p>
<p>::: {.highlight-python .notranslate}
::: highlight
import scrapy
from scrapy.crawler import CrawlerProcess</p>
<pre><code>class MySpider(scrapy.Spider):
    # Your spider definition
    ...


process = CrawlerProcess(
    settings={
        &quot;FEEDS&quot;: {
            &quot;items.json&quot;: {&quot;format&quot;: &quot;json&quot;},
        },
    }
)

process.crawl(MySpider)
process.start()  # the script will block here until the crawling is finished
</code></pre>
<p>:::
:::</p>
<p>Define settings within dictionary in CrawlerProcess. Make sure to check
<a href="index.html#scrapy.crawler.CrawlerProcess" title="scrapy.crawler.CrawlerProcess">[<code>CrawlerProcess</code>{.xref .py .py-class .docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal} documentation to get acquainted with its usage details.</p>
<p>If you are inside a Scrapy project there are some additional helpers you
can use to import those components within the project. You can
automatically import your spiders passing their name to
<a href="index.html#scrapy.crawler.CrawlerProcess" title="scrapy.crawler.CrawlerProcess">[<code>CrawlerProcess</code>{.xref .py .py-class .docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal}, and use [<code>get_project_settings</code>{.docutils .literal
.notranslate}]{.pre} to get a <a href="index.html#scrapy.settings.Settings" title="scrapy.settings.Settings">[<code>Settings</code>{.xref .py .py-class .docutils
.literal
.notranslate}]{.pre}</a>{.reference
.internal} instance with your project settings.</p>
<p>What follows is a working example of how to do that, using the
<a href="https://github.com/scrapinghub/testspiders">testspiders</a>{.reference
.external} project as example.</p>
<p>::: {.highlight-python .notranslate}
::: highlight
from scrapy.crawler import CrawlerProcess
from scrapy.utils.project import get_project_settings</p>
<pre><code>process = CrawlerProcess(get_project_settings())

# 'followall' is the name of one of the spiders of the project.
process.crawl(&quot;followall&quot;, domain=&quot;scrapy.org&quot;)
process.start()  # the script will block here until the crawling is finished
</code></pre>
<p>:::
:::</p>
<p>There's another Scrapy utility that provides more control over the
crawling process: <a href="index.html#scrapy.crawler.CrawlerRunner" title="scrapy.crawler.CrawlerRunner">[<code>scrapy.crawler.CrawlerRunner</code>{.xref .py .py-class
.docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal}. This class is a thin wrapper that encapsulates some simple
helpers to run multiple crawlers, but it won't start or interfere with
existing reactors in any way.</p>
<p>Using this class the reactor should be explicitly run after scheduling
your spiders. It's recommended you use <a href="index.html#scrapy.crawler.CrawlerRunner" title="scrapy.crawler.CrawlerRunner">[<code>CrawlerRunner</code>{.xref .py
.py-class .docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal} instead of <a href="index.html#scrapy.crawler.CrawlerProcess" title="scrapy.crawler.CrawlerProcess">[<code>CrawlerProcess</code>{.xref .py .py-class .docutils
.literal
.notranslate}]{.pre}</a>{.reference
.internal} if your application is already using Twisted and you want to
run Scrapy in the same reactor.</p>
<p>Note that you will also have to shutdown the Twisted reactor yourself
after the spider is finished. This can be achieved by adding callbacks
to the deferred returned by the <a href="index.html#scrapy.crawler.CrawlerRunner.crawl" title="scrapy.crawler.CrawlerRunner.crawl">[<code>CrawlerRunner.crawl</code>{.xref .py
.py-meth .docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal} method.</p>
<p>Here's an example of its usage, along with a callback to manually stop
the reactor after [<code>MySpider</code>{.docutils .literal .notranslate}]{.pre}
has finished running.</p>
<p>::: {.highlight-python .notranslate}
::: highlight
from twisted.internet import reactor
import scrapy
from scrapy.crawler import CrawlerRunner
from scrapy.utils.log import configure_logging</p>
<pre><code>class MySpider(scrapy.Spider):
    # Your spider definition
    ...


configure_logging({&quot;LOG_FORMAT&quot;: &quot;%(levelname)s: %(message)s&quot;})
runner = CrawlerRunner()

d = runner.crawl(MySpider)
d.addBoth(lambda _: reactor.stop())
reactor.run()  # the script will block here until the crawling is finished
</code></pre>
<p>:::
:::</p>
<p>::: {.admonition .seealso}
See also</p>
<p><a href="https://docs.twisted.org/en/stable/core/howto/reactor-basics.html" title="(in Twisted v23.10)">Reactor
Overview</a>{.reference
.external}
:::
:::</p>
<p>::: {#running-multiple-spiders-in-the-same-process .section}
[]{#run-multiple-spiders}</p>
<h4 id="running-multiple-spiders-in-the-same-processheaderlink"><a class="header" href="#running-multiple-spiders-in-the-same-processheaderlink">Running multiple spiders in the same process<a href="#running-multiple-spiders-in-the-same-process" title="Permalink to this heading">¶</a>{.headerlink}</a></h4>
<p>By default, Scrapy runs a single spider per process when you run
[<code>scrapy</code>{.docutils .literal .notranslate}]{.pre}<code> </code>{.docutils .literal
.notranslate}[<code>crawl</code>{.docutils .literal .notranslate}]{.pre}. However,
Scrapy supports running multiple spiders per process using the
<a href="index.html#topics-api">[internal API]{.std .std-ref}</a>{.hoverxref
.tooltip .reference .internal}.</p>
<p>Here is an example that runs multiple spiders simultaneously:</p>
<p>::: {.highlight-python .notranslate}
::: highlight
import scrapy
from scrapy.crawler import CrawlerProcess
from scrapy.utils.project import get_project_settings</p>
<pre><code>class MySpider1(scrapy.Spider):
    # Your first spider definition
    ...


class MySpider2(scrapy.Spider):
    # Your second spider definition
    ...


settings = get_project_settings()
process = CrawlerProcess(settings)
process.crawl(MySpider1)
process.crawl(MySpider2)
process.start()  # the script will block here until all crawling jobs are finished
</code></pre>
<p>:::
:::</p>
<p>Same example using <a href="index.html#scrapy.crawler.CrawlerRunner" title="scrapy.crawler.CrawlerRunner">[<code>CrawlerRunner</code>{.xref .py .py-class .docutils
.literal
.notranslate}]{.pre}</a>{.reference
.internal}:</p>
<p>::: {.highlight-python .notranslate}
::: highlight
import scrapy
from twisted.internet import reactor
from scrapy.crawler import CrawlerRunner
from scrapy.utils.log import configure_logging
from scrapy.utils.project import get_project_settings</p>
<pre><code>class MySpider1(scrapy.Spider):
    # Your first spider definition
    ...


class MySpider2(scrapy.Spider):
    # Your second spider definition
    ...


configure_logging()
settings = get_project_settings()
runner = CrawlerRunner(settings)
runner.crawl(MySpider1)
runner.crawl(MySpider2)
d = runner.join()
d.addBoth(lambda _: reactor.stop())

reactor.run()  # the script will block here until all crawling jobs are finished
</code></pre>
<p>:::
:::</p>
<p>Same example but running the spiders sequentially by chaining the
deferreds:</p>
<p>::: {.highlight-python .notranslate}
::: highlight
from twisted.internet import reactor, defer
from scrapy.crawler import CrawlerRunner
from scrapy.utils.log import configure_logging
from scrapy.utils.project import get_project_settings</p>
<pre><code>class MySpider1(scrapy.Spider):
    # Your first spider definition
    ...


class MySpider2(scrapy.Spider):
    # Your second spider definition
    ...


settings = get_project_settings()
configure_logging(settings)
runner = CrawlerRunner(settings)


@defer.inlineCallbacks
def crawl():
    yield runner.crawl(MySpider1)
    yield runner.crawl(MySpider2)
    reactor.stop()


crawl()
reactor.run()  # the script will block here until the last crawl call is finished
</code></pre>
<p>:::
:::</p>
<p>Different spiders can set different values for the same setting, but
when they run in the same process it may be impossible, by design or
because of some limitations, to use these different values. What happens
in practice is different for different settings:</p>
<ul>
<li>
<p><a href="index.html#std-setting-SPIDER_LOADER_CLASS">[<code>SPIDER_LOADER_CLASS</code>{.xref .std .std-setting .docutils .literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal} and the ones used by its value
(<a href="index.html#std-setting-SPIDER_MODULES">[<code>SPIDER_MODULES</code>{.xref .std .std-setting .docutils .literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal}, <a href="index.html#std-setting-SPIDER_LOADER_WARN_ONLY">[<code>SPIDER_LOADER_WARN_ONLY</code>{.xref
.std .std-setting .docutils .literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal} for the default one) cannot be read
from the per-spider settings. These are applied when the
<a href="index.html#scrapy.crawler.CrawlerRunner" title="scrapy.crawler.CrawlerRunner">[<code>CrawlerRunner</code>{.xref .py .py-class .docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal} or <a href="index.html#scrapy.crawler.CrawlerProcess" title="scrapy.crawler.CrawlerProcess">[<code>CrawlerProcess</code>{.xref .py .py-class .docutils
.literal
.notranslate}]{.pre}</a>{.reference
.internal} object is created.</p>
</li>
<li>
<p>For <a href="index.html#std-setting-TWISTED_REACTOR">[<code>TWISTED_REACTOR</code>{.xref .std .std-setting .docutils .literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal} and <a href="index.html#std-setting-ASYNCIO_EVENT_LOOP">[<code>ASYNCIO_EVENT_LOOP</code>{.xref .std
.std-setting .docutils .literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal} the first available value is used,
and if a spider requests a different reactor an exception will be
raised. These are applied when the reactor is installed.</p>
</li>
<li>
<p>For <a href="index.html#std-setting-REACTOR_THREADPOOL_MAXSIZE">[<code>REACTOR_THREADPOOL_MAXSIZE</code>{.xref .std .std-setting .docutils
.literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal}, <a href="index.html#std-setting-DNS_RESOLVER">[<code>DNS_RESOLVER</code>{.xref .std
.std-setting .docutils .literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal} and the ones used by the resolver
(<a href="index.html#std-setting-DNSCACHE_ENABLED">[<code>DNSCACHE_ENABLED</code>{.xref .std .std-setting .docutils .literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal}, <a href="index.html#std-setting-DNSCACHE_SIZE">[<code>DNSCACHE_SIZE</code>{.xref .std
.std-setting .docutils .literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal}, <a href="index.html#std-setting-DNS_TIMEOUT">[<code>DNS_TIMEOUT</code>{.xref .std
.std-setting .docutils .literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal} for ones included in Scrapy) the
first available value is used. These are applied when the reactor is
started.</p>
</li>
</ul>
<p>::: {.admonition .seealso}
See also</p>
<p><a href="#run-from-script">[Run Scrapy from a script]{.std .std-ref}</a>{.hoverxref
.tooltip .reference .internal}.
:::
:::</p>
<p>::: {#distributed-crawls .section}
[]{#id1}</p>
<h4 id="distributed-crawlsheaderlink"><a class="header" href="#distributed-crawlsheaderlink">Distributed crawls<a href="#distributed-crawls" title="Permalink to this heading">¶</a>{.headerlink}</a></h4>
<p>Scrapy doesn't provide any built-in facility for running crawls in a
distribute (multi-server) manner. However, there are some ways to
distribute crawls, which vary depending on how you plan to distribute
them.</p>
<p>If you have many spiders, the obvious way to distribute the load is to
setup many Scrapyd instances and distribute spider runs among those.</p>
<p>If you instead want to run a single (big) spider through many machines,
what you usually do is partition the urls to crawl and send them to each
separate spider. Here is a concrete example:</p>
<p>First, you prepare the list of urls to crawl and put them into separate
files/urls:</p>
<p>::: {.highlight-default .notranslate}
::: highlight
http://somedomain.com/urls-to-crawl/spider1/part1.list
http://somedomain.com/urls-to-crawl/spider1/part2.list
http://somedomain.com/urls-to-crawl/spider1/part3.list
:::
:::</p>
<p>Then you fire a spider run on 3 different Scrapyd servers. The spider
would receive a (spider) argument [<code>part</code>{.docutils .literal
.notranslate}]{.pre} with the number of the partition to crawl:</p>
<p>::: {.highlight-default .notranslate}
::: highlight
curl http://scrapy1.mycompany.com:6800/schedule.json -d project=myproject -d spider=spider1 -d part=1
curl http://scrapy2.mycompany.com:6800/schedule.json -d project=myproject -d spider=spider1 -d part=2
curl http://scrapy3.mycompany.com:6800/schedule.json -d project=myproject -d spider=spider1 -d part=3
:::
:::
:::</p>
<p>::: {#avoiding-getting-banned .section}
[]{#bans}</p>
<h4 id="avoiding-getting-bannedheaderlink"><a class="header" href="#avoiding-getting-bannedheaderlink">Avoiding getting banned<a href="#avoiding-getting-banned" title="Permalink to this heading">¶</a>{.headerlink}</a></h4>
<p>Some websites implement certain measures to prevent bots from crawling
them, with varying degrees of sophistication. Getting around those
measures can be difficult and tricky, and may sometimes require special
infrastructure. Please consider contacting <a href="https://scrapy.org/support/">commercial
support</a>{.reference .external} if in doubt.</p>
<p>Here are some tips to keep in mind when dealing with these kinds of
sites:</p>
<ul>
<li>
<p>rotate your user agent from a pool of well-known ones from browsers
(google around to get a list of them)</p>
</li>
<li>
<p>disable cookies (see <a href="index.html#std-setting-COOKIES_ENABLED">[<code>COOKIES_ENABLED</code>{.xref .std .std-setting
.docutils .literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal}) as some sites may use cookies to
spot bot behaviour</p>
</li>
<li>
<p>use download delays (2 or higher). See <a href="index.html#std-setting-DOWNLOAD_DELAY">[<code>DOWNLOAD_DELAY</code>{.xref .std
.std-setting .docutils .literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal} setting.</p>
</li>
<li>
<p>if possible, use <a href="https://commoncrawl.org/">Common Crawl</a>{.reference
.external} to fetch pages, instead of hitting the sites directly</p>
</li>
<li>
<p>use a pool of rotating IPs. For example, the free <a href="https://www.torproject.org/">Tor
project</a>{.reference .external} or paid
services like <a href="https://proxymesh.com/">ProxyMesh</a>{.reference
.external}. An open source alternative is
<a href="https://scrapoxy.io/">scrapoxy</a>{.reference .external}, a super
proxy that you can attach your own proxies to.</p>
</li>
<li>
<p>use a ban avoidance service, such as <a href="https://docs.zyte.com/zyte-api/get-started.html">Zyte
API</a>{.reference
.external}, which provides a <a href="https://github.com/scrapy-plugins/scrapy-zyte-api">Scrapy
plugin</a>{.reference
.external}</p>
</li>
</ul>
<p>If you are still unable to prevent your bot getting banned, consider
contacting <a href="https://scrapy.org/support/">commercial support</a>{.reference
.external}.
:::
:::</p>
<p>[]{#document-topics/broad-crawls}</p>
<p>::: {#broad-crawls .section}
[]{#topics-broad-crawls}</p>
<h3 id="broad-crawlsheaderlink"><a class="header" href="#broad-crawlsheaderlink">Broad Crawls<a href="#broad-crawls" title="Permalink to this heading">¶</a>{.headerlink}</a></h3>
<p>Scrapy defaults are optimized for crawling specific sites. These sites
are often handled by a single Scrapy spider, although this is not
necessary or required (for example, there are generic spiders that
handle any given site thrown at them).</p>
<p>In addition to this &quot;focused crawl&quot;, there is another common type of
crawling which covers a large (potentially unlimited) number of domains,
and is only limited by time or other arbitrary constraint, rather than
stopping when the domain was crawled to completion or when there are no
more requests to perform. These are called &quot;broad crawls&quot; and is the
typical crawlers employed by search engines.</p>
<p>These are some common properties often found in broad crawls:</p>
<ul>
<li>
<p>they crawl many domains (often, unbounded) instead of a specific set
of sites</p>
</li>
<li>
<p>they don't necessarily crawl domains to completion, because it would
be impractical (or impossible) to do so, and instead limit the crawl
by time or number of pages crawled</p>
</li>
<li>
<p>they are simpler in logic (as opposed to very complex spiders with
many extraction rules) because data is often post-processed in a
separate stage</p>
</li>
<li>
<p>they crawl many domains concurrently, which allows them to achieve
faster crawl speeds by not being limited by any particular site
constraint (each site is crawled slowly to respect politeness, but
many sites are crawled in parallel)</p>
</li>
</ul>
<p>As said above, Scrapy default settings are optimized for focused crawls,
not broad crawls. However, due to its asynchronous architecture, Scrapy
is very well suited for performing fast broad crawls. This page
summarizes some things you need to keep in mind when using Scrapy for
doing broad crawls, along with concrete suggestions of Scrapy settings
to tune in order to achieve an efficient broad crawl.</p>
<p>::: {#use-the-right-scheduler-priority-queue .section}
[]{#broad-crawls-scheduler-priority-queue}</p>
<h4 id="use-the-right-scheduler_priority_queuexref-std-std-setting-docutils-literal-notranslateprehoverxref-tooltip-reference-internalheaderlink"><a class="header" href="#use-the-right-scheduler_priority_queuexref-std-std-setting-docutils-literal-notranslateprehoverxref-tooltip-reference-internalheaderlink">Use the right <a href="index.html#std-setting-SCHEDULER_PRIORITY_QUEUE">[<code>SCHEDULER_PRIORITY_QUEUE</code>{.xref .std .std-setting .docutils .literal .notranslate}]{.pre}</a>{.hoverxref .tooltip .reference .internal}<a href="#use-the-right-scheduler-priority-queue" title="Permalink to this heading">¶</a>{.headerlink}</a></h4>
<p>Scrapy's default scheduler priority queue is
[<code>'scrapy.pqueues.ScrapyPriorityQueue'</code>{.docutils .literal
.notranslate}]{.pre}. It works best during single-domain crawl. It does
not work well with crawling many different domains in parallel</p>
<p>To apply the recommended priority queue use:</p>
<p>::: {.highlight-python .notranslate}
::: highlight
SCHEDULER_PRIORITY_QUEUE = &quot;scrapy.pqueues.DownloaderAwarePriorityQueue&quot;
:::
:::
:::</p>
<p>::: {#increase-concurrency .section}
[]{#broad-crawls-concurrency}</p>
<h4 id="increase-concurrencyheaderlink"><a class="header" href="#increase-concurrencyheaderlink">Increase concurrency<a href="#increase-concurrency" title="Permalink to this heading">¶</a>{.headerlink}</a></h4>
<p>Concurrency is the number of requests that are processed in parallel.
There is a global limit (<a href="index.html#std-setting-CONCURRENT_REQUESTS">[<code>CONCURRENT_REQUESTS</code>{.xref .std .std-setting
.docutils .literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal}) and an additional limit that can be set
either per domain (<a href="index.html#std-setting-CONCURRENT_REQUESTS_PER_DOMAIN">[<code>CONCURRENT_REQUESTS_PER_DOMAIN</code>{.xref .std
.std-setting .docutils .literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal}) or per IP
(<a href="index.html#std-setting-CONCURRENT_REQUESTS_PER_IP">[<code>CONCURRENT_REQUESTS_PER_IP</code>{.xref .std .std-setting .docutils
.literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal}).</p>
<p>::: {.admonition .note}
Note</p>
<p>The scheduler priority queue <a href="#broad-crawls-scheduler-priority-queue">[recommended for broad crawls]{.std
.std-ref}</a>{.hoverxref .tooltip
.reference .internal} does not support
<a href="index.html#std-setting-CONCURRENT_REQUESTS_PER_IP">[<code>CONCURRENT_REQUESTS_PER_IP</code>{.xref .std .std-setting .docutils
.literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal}.
:::</p>
<p>The default global concurrency limit in Scrapy is not suitable for
crawling many different domains in parallel, so you will want to
increase it. How much to increase it will depend on how much CPU and
memory your crawler will have available.</p>
<p>A good starting point is [<code>100</code>{.docutils .literal .notranslate}]{.pre}:</p>
<p>::: {.highlight-python .notranslate}
::: highlight
CONCURRENT_REQUESTS = 100
:::
:::</p>
<p>But the best way to find out is by doing some trials and identifying at
what concurrency your Scrapy process gets CPU bounded. For optimum
performance, you should pick a concurrency where CPU usage is at 80-90%.</p>
<p>Increasing concurrency also increases memory usage. If memory usage is a
concern, you might need to lower your global concurrency limit
accordingly.
:::</p>
<p>::: {#increase-twisted-io-thread-pool-maximum-size .section}</p>
<h4 id="increase-twisted-io-thread-pool-maximum-sizeheaderlink"><a class="header" href="#increase-twisted-io-thread-pool-maximum-sizeheaderlink">Increase Twisted IO thread pool maximum size<a href="#increase-twisted-io-thread-pool-maximum-size" title="Permalink to this heading">¶</a>{.headerlink}</a></h4>
<p>Currently Scrapy does DNS resolution in a blocking way with usage of
thread pool. With higher concurrency levels the crawling could be slow
or even fail hitting DNS resolver timeouts. Possible solution to
increase the number of threads handling DNS queries. The DNS queue will
be processed faster speeding up establishing of connection and crawling
overall.</p>
<p>To increase maximum thread pool size use:</p>
<p>::: {.highlight-python .notranslate}
::: highlight
REACTOR_THREADPOOL_MAXSIZE = 20
:::
:::
:::</p>
<p>::: {#setup-your-own-dns .section}</p>
<h4 id="setup-your-own-dnsheaderlink"><a class="header" href="#setup-your-own-dnsheaderlink">Setup your own DNS<a href="#setup-your-own-dns" title="Permalink to this heading">¶</a>{.headerlink}</a></h4>
<p>If you have multiple crawling processes and single central DNS, it can
act like DoS attack on the DNS server resulting to slow down of entire
network or even blocking your machines. To avoid this setup your own DNS
server with local cache and upstream to some large DNS like OpenDNS or
Verizon.
:::</p>
<p>::: {#reduce-log-level .section}</p>
<h4 id="reduce-log-levelheaderlink"><a class="header" href="#reduce-log-levelheaderlink">Reduce log level<a href="#reduce-log-level" title="Permalink to this heading">¶</a>{.headerlink}</a></h4>
<p>When doing broad crawls you are often only interested in the crawl rates
you get and any errors found. These stats are reported by Scrapy when
using the [<code>INFO</code>{.docutils .literal .notranslate}]{.pre} log level. In
order to save CPU (and log storage requirements) you should not use
[<code>DEBUG</code>{.docutils .literal .notranslate}]{.pre} log level when
preforming large broad crawls in production. Using [<code>DEBUG</code>{.docutils
.literal .notranslate}]{.pre} level when developing your (broad) crawler
may be fine though.</p>
<p>To set the log level use:</p>
<p>::: {.highlight-python .notranslate}
::: highlight
LOG_LEVEL = &quot;INFO&quot;
:::
:::
:::</p>
<p>::: {#disable-cookies .section}</p>
<h4 id="disable-cookiesheaderlink"><a class="header" href="#disable-cookiesheaderlink">Disable cookies<a href="#disable-cookies" title="Permalink to this heading">¶</a>{.headerlink}</a></h4>
<p>Disable cookies unless you <em>really</em> need. Cookies are often not needed
when doing broad crawls (search engine crawlers ignore them), and they
improve performance by saving some CPU cycles and reducing the memory
footprint of your Scrapy crawler.</p>
<p>To disable cookies use:</p>
<p>::: {.highlight-python .notranslate}
::: highlight
COOKIES_ENABLED = False
:::
:::
:::</p>
<p>::: {#disable-retries .section}</p>
<h4 id="disable-retriesheaderlink"><a class="header" href="#disable-retriesheaderlink">Disable retries<a href="#disable-retries" title="Permalink to this heading">¶</a>{.headerlink}</a></h4>
<p>Retrying failed HTTP requests can slow down the crawls substantially,
specially when sites causes are very slow (or fail) to respond, thus
causing a timeout error which gets retried many times, unnecessarily,
preventing crawler capacity to be reused for other domains.</p>
<p>To disable retries use:</p>
<p>::: {.highlight-python .notranslate}
::: highlight
RETRY_ENABLED = False
:::
:::
:::</p>
<p>::: {#reduce-download-timeout .section}</p>
<h4 id="reduce-download-timeoutheaderlink"><a class="header" href="#reduce-download-timeoutheaderlink">Reduce download timeout<a href="#reduce-download-timeout" title="Permalink to this heading">¶</a>{.headerlink}</a></h4>
<p>Unless you are crawling from a very slow connection (which shouldn't be
the case for broad crawls) reduce the download timeout so that stuck
requests are discarded quickly and free up capacity to process the next
ones.</p>
<p>To reduce the download timeout use:</p>
<p>::: {.highlight-python .notranslate}
::: highlight
DOWNLOAD_TIMEOUT = 15
:::
:::
:::</p>
<p>::: {#disable-redirects .section}</p>
<h4 id="disable-redirectsheaderlink"><a class="header" href="#disable-redirectsheaderlink">Disable redirects<a href="#disable-redirects" title="Permalink to this heading">¶</a>{.headerlink}</a></h4>
<p>Consider disabling redirects, unless you are interested in following
them. When doing broad crawls it's common to save redirects and resolve
them when revisiting the site at a later crawl. This also help to keep
the number of request constant per crawl batch, otherwise redirect loops
may cause the crawler to dedicate too many resources on any specific
domain.</p>
<p>To disable redirects use:</p>
<p>::: {.highlight-python .notranslate}
::: highlight
REDIRECT_ENABLED = False
:::
:::
:::</p>
<p>::: {#enable-crawling-of-ajax-crawlable-pages .section}</p>
<h4 id="enable-crawling-of-ajax-crawlable-pagesheaderlink"><a class="header" href="#enable-crawling-of-ajax-crawlable-pagesheaderlink">Enable crawling of &quot;Ajax Crawlable Pages&quot;<a href="#enable-crawling-of-ajax-crawlable-pages" title="Permalink to this heading">¶</a>{.headerlink}</a></h4>
<p>Some pages (up to 1%, based on empirical data from year 2013) declare
themselves as <a href="https://developers.google.com/search/docs/ajax-crawling/docs/getting-started">ajax
crawlable</a>{.reference
.external}. This means they provide plain HTML version of content that
is usually available only via AJAX. Pages can indicate it in two ways:</p>
<ol>
<li>
<p>by using [<code>#!</code>{.docutils .literal .notranslate}]{.pre} in URL - this
is the default way;</p>
</li>
<li>
<p>by using a special meta tag - this way is used on &quot;main&quot;, &quot;index&quot;
website pages.</p>
</li>
</ol>
<p>Scrapy handles (1) automatically; to handle (2) enable
<a href="index.html#ajaxcrawl-middleware">[AjaxCrawlMiddleware]{.std
.std-ref}</a>{.hoverxref .tooltip
.reference .internal}:</p>
<p>::: {.highlight-python .notranslate}
::: highlight
AJAXCRAWL_ENABLED = True
:::
:::</p>
<p>When doing broad crawls it's common to crawl a lot of &quot;index&quot; web pages;
AjaxCrawlMiddleware helps to crawl them correctly. It is turned OFF by
default because it has some performance overhead, and enabling it for
focused crawls doesn't make much sense.
:::</p>
<p>::: {#crawl-in-bfo-order .section}
[]{#broad-crawls-bfo}</p>
<h4 id="crawl-in-bfo-orderheaderlink"><a class="header" href="#crawl-in-bfo-orderheaderlink">Crawl in BFO order<a href="#crawl-in-bfo-order" title="Permalink to this heading">¶</a>{.headerlink}</a></h4>
<p><a href="index.html#faq-bfo-dfo">[Scrapy crawls in DFO order by default]{.std
.std-ref}</a>{.hoverxref .tooltip .reference
.internal}.</p>
<p>In broad crawls, however, page crawling tends to be faster than page
processing. As a result, unprocessed early requests stay in memory until
the final depth is reached, which can significantly increase memory
usage.</p>
<p><a href="index.html#faq-bfo-dfo">[Crawl in BFO order]{.std .std-ref}</a>{.hoverxref
.tooltip .reference .internal} instead to save memory.
:::</p>
<p>::: {#be-mindful-of-memory-leaks .section}</p>
<h4 id="be-mindful-of-memory-leaksheaderlink"><a class="header" href="#be-mindful-of-memory-leaksheaderlink">Be mindful of memory leaks<a href="#be-mindful-of-memory-leaks" title="Permalink to this heading">¶</a>{.headerlink}</a></h4>
<p>If your broad crawl shows a high memory usage, in addition to <a href="#broad-crawls-bfo">[crawling
in BFO order]{.std .std-ref}</a>{.hoverxref .tooltip
.reference .internal} and <a href="#broad-crawls-concurrency">[lowering concurrency]{.std
.std-ref}</a>{.hoverxref .tooltip .reference
.internal} you should <a href="index.html#topics-leaks">[debug your memory leaks]{.std
.std-ref}</a>{.hoverxref .tooltip .reference
.internal}.
:::</p>
<p>::: {#install-a-specific-twisted-reactor .section}</p>
<h4 id="install-a-specific-twisted-reactorheaderlink"><a class="header" href="#install-a-specific-twisted-reactorheaderlink">Install a specific Twisted reactor<a href="#install-a-specific-twisted-reactor" title="Permalink to this heading">¶</a>{.headerlink}</a></h4>
<p>If the crawl is exceeding the system's capabilities, you might want to
try installing a specific Twisted reactor, via the
<a href="index.html#std-setting-TWISTED_REACTOR">[<code>TWISTED_REACTOR</code>{.xref .std .std-setting .docutils .literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal} setting.
:::
:::</p>
<p>[]{#document-topics/developer-tools}</p>
<p>::: {#using-your-browser-s-developer-tools-for-scraping .section}
[]{#topics-developer-tools}</p>
<h3 id="using-your-browsers-developer-tools-for-scrapingheaderlink"><a class="header" href="#using-your-browsers-developer-tools-for-scrapingheaderlink">Using your browser's Developer Tools for scraping<a href="#using-your-browser-s-developer-tools-for-scraping" title="Permalink to this heading">¶</a>{.headerlink}</a></h3>
<p>Here is a general guide on how to use your browser's Developer Tools to
ease the scraping process. Today almost all browsers come with built in
<a href="https://en.wikipedia.org/wiki/Web_development_tools">Developer
Tools</a>{.reference
.external} and although we will use Firefox in this guide, the concepts
are applicable to any other browser.</p>
<p>In this guide we'll introduce the basic tools to use from a browser's
Developer Tools by scraping
<a href="https://quotes.toscrape.com">quotes.toscrape.com</a>{.reference
.external}.</p>
<p>::: {#caveats-with-inspecting-the-live-browser-dom .section}
[]{#topics-livedom}</p>
<h4 id="caveats-with-inspecting-the-live-browser-domheaderlink"><a class="header" href="#caveats-with-inspecting-the-live-browser-domheaderlink">Caveats with inspecting the live browser DOM<a href="#caveats-with-inspecting-the-live-browser-dom" title="Permalink to this heading">¶</a>{.headerlink}</a></h4>
<p>Since Developer Tools operate on a live browser DOM, what you'll
actually see when inspecting the page source is not the original HTML,
but a modified one after applying some browser clean up and executing
JavaScript code. Firefox, in particular, is known for adding
[<code>&lt;tbody&gt;</code>{.docutils .literal .notranslate}]{.pre} elements to tables.
Scrapy, on the other hand, does not modify the original page HTML, so
you won't be able to extract any data if you use [<code>&lt;tbody&gt;</code>{.docutils
.literal .notranslate}]{.pre} in your XPath expressions.</p>
<p>Therefore, you should keep in mind the following things:</p>
<ul>
<li>
<p>Disable JavaScript while inspecting the DOM looking for XPaths to be
used in Scrapy (in the Developer Tools settings click Disable
JavaScript)</p>
</li>
<li>
<p>Never use full XPath paths, use relative and clever ones based on
attributes (such as [<code>id</code>{.docutils .literal .notranslate}]{.pre},
[<code>class</code>{.docutils .literal .notranslate}]{.pre}, [<code>width</code>{.docutils
.literal .notranslate}]{.pre}, etc) or any identifying features like
[<code>contains(@href,</code>{.docutils .literal
.notranslate}]{.pre}<code> </code>{.docutils .literal
.notranslate}[<code>'image')</code>{.docutils .literal .notranslate}]{.pre}.</p>
</li>
<li>
<p>Never include [<code>&lt;tbody&gt;</code>{.docutils .literal .notranslate}]{.pre}
elements in your XPath expressions unless you really know what
you're doing
:::</p>
</li>
</ul>
<p>::: {#inspecting-a-website .section}
[]{#topics-inspector}</p>
<h4 id="inspecting-a-websiteheaderlink"><a class="header" href="#inspecting-a-websiteheaderlink">Inspecting a website<a href="#inspecting-a-website" title="Permalink to this heading">¶</a>{.headerlink}</a></h4>
<p>By far the most handy feature of the Developer Tools is the Inspector
feature, which allows you to inspect the underlying HTML code of any
webpage. To demonstrate the Inspector, let's look at the
<a href="https://quotes.toscrape.com">quotes.toscrape.com</a>{.reference
.external}-site.</p>
<p>On the site we have a total of ten quotes from various authors with
specific tags, as well as the Top Ten Tags. Let's say we want to extract
all the quotes on this page, without any meta-information about authors,
tags, etc.</p>
<p>Instead of viewing the whole source code for the page, we can simply
right click on a quote and select [<code>Inspect</code>{.docutils .literal
.notranslate}]{.pre}<code> </code>{.docutils .literal
.notranslate}[<code>Element</code>{.docutils .literal
.notranslate}]{.pre}<code> </code>{.docutils .literal .notranslate}[<code>(Q)</code>{.docutils
.literal .notranslate}]{.pre}, which opens up the Inspector. In it you
should see something like this:</p>
<p><a href="_images/inspector_01.png"><img src="_images/inspector_01.png" alt="Firefox's Inspector-tool" />{style=&quot;width: 777px; height: 469px;&quot;}</a>{.reference
.internal .image-reference}</p>
<p>The interesting part for us is this:</p>
<p>::: {.highlight-html .notranslate}
::: highlight
<div class="quote" itemscope="" itemtype="http://schema.org/CreativeWork">
<span class="text" itemprop="text">(...)</span>
<span>(...)</span>
<div class="tags">(...)</div>
</div>
:::
:::</p>
<p>If you hover over the first [<code>div</code>{.docutils .literal
.notranslate}]{.pre} directly above the [<code>span</code>{.docutils .literal
.notranslate}]{.pre} tag highlighted in the screenshot, you'll see that
the corresponding section of the webpage gets highlighted as well. So
now we have a section, but we can't find our quote text anywhere.</p>
<p>The advantage of the Inspector is that it automatically expands and
collapses sections and tags of a webpage, which greatly improves
readability. You can expand and collapse a tag by clicking on the arrow
in front of it or by double clicking directly on the tag. If we expand
the [<code>span</code>{.docutils .literal .notranslate}]{.pre} tag with the
[<code>class=</code>{.docutils .literal .notranslate}]{.pre}<code> </code>{.docutils .literal
.notranslate}[<code>&quot;text&quot;</code>{.docutils .literal .notranslate}]{.pre} we will
see the quote-text we clicked on. The Inspector lets you copy XPaths to
selected elements. Let's try it out.</p>
<p>First open the Scrapy shell at
<a href="https://quotes.toscrape.com/">https://quotes.toscrape.com/</a>{.reference
.external} in a terminal:</p>
<p>::: {.highlight-none .notranslate}
::: highlight
$ scrapy shell &quot;https://quotes.toscrape.com/&quot;
:::
:::</p>
<p>Then, back to your web browser, right-click on the [<code>span</code>{.docutils
.literal .notranslate}]{.pre} tag, select [<code>Copy</code>{.docutils .literal
.notranslate}]{.pre}<code> </code>{.docutils .literal .notranslate}[<code>&gt;</code>{.docutils
.literal .notranslate}]{.pre}<code> </code>{.docutils .literal
.notranslate}[<code>XPath</code>{.docutils .literal .notranslate}]{.pre} and paste
it in the Scrapy shell like so:</p>
<p>::: {.highlight-pycon .notranslate}
::: highlight
&gt;&gt;&gt; response.xpath(&quot;/html/body/div/div[2]/div[1]/div[1]/span[1]/text()&quot;).getall()
['“The world as we have created it is a process of our thinking. It cannot be changed without changing our thinking.”']
:::
:::</p>
<p>Adding [<code>text()</code>{.docutils .literal .notranslate}]{.pre} at the end we
are able to extract the first quote with this basic selector. But this
XPath is not really that clever. All it does is go down a desired path
in the source code starting from [<code>html</code>{.docutils .literal
.notranslate}]{.pre}. So let's see if we can refine our XPath a bit:</p>
<p>If we check the Inspector again we'll see that directly beneath our
expanded [<code>div</code>{.docutils .literal .notranslate}]{.pre} tag we have nine
identical [<code>div</code>{.docutils .literal .notranslate}]{.pre} tags, each with
the same attributes as our first. If we expand any of them, we'll see
the same structure as with our first quote: Two [<code>span</code>{.docutils
.literal .notranslate}]{.pre} tags and one [<code>div</code>{.docutils .literal
.notranslate}]{.pre} tag. We can expand each [<code>span</code>{.docutils .literal
.notranslate}]{.pre} tag with the [<code>class=&quot;text&quot;</code>{.docutils .literal
.notranslate}]{.pre} inside our [<code>div</code>{.docutils .literal
.notranslate}]{.pre} tags and see each quote:</p>
<p>::: {.highlight-html .notranslate}
::: highlight
<div class="quote" itemscope="" itemtype="http://schema.org/CreativeWork">
<span class="text" itemprop="text">
“The world as we have created it is a process of our thinking. It cannot be changed without changing our thinking.”
</span>
<span>(...)</span>
<div class="tags">(...)</div>
</div>
:::
:::</p>
<p>With this knowledge we can refine our XPath: Instead of a path to
follow, we'll simply select all [<code>span</code>{.docutils .literal
.notranslate}]{.pre} tags with the [<code>class=&quot;text&quot;</code>{.docutils .literal
.notranslate}]{.pre} by using the
<a href="https://parsel.readthedocs.io/en/latest/usage.html#other-xpath-extensions">has-class-extension</a>{.reference
.external}:</p>
<p>::: {.highlight-pycon .notranslate}
::: highlight
&gt;&gt;&gt; response.xpath('//span[has-class(&quot;text&quot;)]/text()').getall()
['“The world as we have created it is a process of our thinking. It cannot be changed without changing our thinking.”',
'“It is our choices, Harry, that show what we truly are, far more than our abilities.”',
'“There are only two ways to live your life. One is as though nothing is a miracle. The other is as though everything is a miracle.”',
...]
:::
:::</p>
<p>And with one simple, cleverer XPath we are able to extract all quotes
from the page. We could have constructed a loop over our first XPath to
increase the number of the last [<code>div</code>{.docutils .literal
.notranslate}]{.pre}, but this would have been unnecessarily complex and
by simply constructing an XPath with [<code>has-class(&quot;text&quot;)</code>{.docutils
.literal .notranslate}]{.pre} we were able to extract all quotes in one
line.</p>
<p>The Inspector has a lot of other helpful features, such as searching in
the source code or directly scrolling to an element you selected. Let's
demonstrate a use case:</p>
<p>Say you want to find the [<code>Next</code>{.docutils .literal .notranslate}]{.pre}
button on the page. Type [<code>Next</code>{.docutils .literal .notranslate}]{.pre}
into the search bar on the top right of the Inspector. You should get
two results. The first is a [<code>li</code>{.docutils .literal
.notranslate}]{.pre} tag with the [<code>class=&quot;next&quot;</code>{.docutils .literal
.notranslate}]{.pre}, the second the text of an [<code>a</code>{.docutils .literal
.notranslate}]{.pre} tag. Right click on the [<code>a</code>{.docutils .literal
.notranslate}]{.pre} tag and select [<code>Scroll</code>{.docutils .literal
.notranslate}]{.pre}<code> </code>{.docutils .literal
.notranslate}[<code>into</code>{.docutils .literal
.notranslate}]{.pre}<code> </code>{.docutils .literal
.notranslate}[<code>View</code>{.docutils .literal .notranslate}]{.pre}. If you
hover over the tag, you'll see the button highlighted. From here we
could easily create a <a href="index.html#topics-link-extractors">[Link Extractor]{.std
.std-ref}</a>{.hoverxref .tooltip
.reference .internal} to follow the pagination. On a simple site such as
this, there may not be the need to find an element visually but the
[<code>Scroll</code>{.docutils .literal .notranslate}]{.pre}<code> </code>{.docutils .literal
.notranslate}[<code>into</code>{.docutils .literal
.notranslate}]{.pre}<code> </code>{.docutils .literal
.notranslate}[<code>View</code>{.docutils .literal .notranslate}]{.pre} function
can be quite useful on complex sites.</p>
<p>Note that the search bar can also be used to search for and test CSS
selectors. For example, you could search for [<code>span.text</code>{.docutils
.literal .notranslate}]{.pre} to find all quote texts. Instead of a full
text search, this searches for exactly the [<code>span</code>{.docutils .literal
.notranslate}]{.pre} tag with the [<code>class=&quot;text&quot;</code>{.docutils .literal
.notranslate}]{.pre} in the page.
:::</p>
<p>::: {#the-network-tool .section}
[]{#topics-network-tool}</p>
<h4 id="the-network-toolheaderlink"><a class="header" href="#the-network-toolheaderlink">The Network-tool<a href="#the-network-tool" title="Permalink to this heading">¶</a>{.headerlink}</a></h4>
<p>While scraping you may come across dynamic webpages where some parts of
the page are loaded dynamically through multiple requests. While this
can be quite tricky, the Network-tool in the Developer Tools greatly
facilitates this task. To demonstrate the Network-tool, let's take a
look at the page
<a href="https://quotes.toscrape.com/scroll">quotes.toscrape.com/scroll</a>{.reference
.external}.</p>
<p>The page is quite similar to the basic
<a href="https://quotes.toscrape.com">quotes.toscrape.com</a>{.reference
.external}-page, but instead of the above-mentioned [<code>Next</code>{.docutils
.literal .notranslate}]{.pre} button, the page automatically loads new
quotes when you scroll to the bottom. We could go ahead and try out
different XPaths directly, but instead we'll check another quite useful
command from the Scrapy shell:</p>
<p>::: {.highlight-none .notranslate}
::: highlight
$ scrapy shell &quot;quotes.toscrape.com/scroll&quot;
(...)
&gt;&gt;&gt; view(response)
:::
:::</p>
<p>A browser window should open with the webpage but with one crucial
difference: Instead of the quotes we just see a greenish bar with the
word [<code>Loading...</code>{.docutils .literal .notranslate}]{.pre}.</p>
<p><a href="_images/network_01.png"><img src="_images/network_01.png" alt="Response from quotes.toscrape.com/scroll" />{style=&quot;width: 777px; height: 296px;&quot;}</a>{.reference
.internal .image-reference}</p>
<p>The [<code>view(response)</code>{.docutils .literal .notranslate}]{.pre} command
let's us view the response our shell or later our spider receives from
the server. Here we see that some basic template is loaded which
includes the title, the login-button and the footer, but the quotes are
missing. This tells us that the quotes are being loaded from a different
request than [<code>quotes.toscrape/scroll</code>{.docutils .literal
.notranslate}]{.pre}.</p>
<p>If you click on the [<code>Network</code>{.docutils .literal .notranslate}]{.pre}
tab, you will probably only see two entries. The first thing we do is
enable persistent logs by clicking on [<code>Persist</code>{.docutils .literal
.notranslate}]{.pre}<code> </code>{.docutils .literal
.notranslate}[<code>Logs</code>{.docutils .literal .notranslate}]{.pre}. If this
option is disabled, the log is automatically cleared each time you
navigate to a different page. Enabling this option is a good default,
since it gives us control on when to clear the logs.</p>
<p>If we reload the page now, you'll see the log get populated with six new
requests.</p>
<p><a href="_images/network_02.png"><img src="_images/network_02.png" alt="Network tab with persistent logs and requests" />{style=&quot;width: 777px; height: 241px;&quot;}</a>{.reference
.internal .image-reference}</p>
<p>Here we see every request that has been made when reloading the page and
can inspect each request and its response. So let's find out where our
quotes are coming from:</p>
<p>First click on the request with the name [<code>scroll</code>{.docutils .literal
.notranslate}]{.pre}. On the right you can now inspect the request. In
[<code>Headers</code>{.docutils .literal .notranslate}]{.pre} you'll find details
about the request headers, such as the URL, the method, the IP-address,
and so on. We'll ignore the other tabs and click directly on
[<code>Response</code>{.docutils .literal .notranslate}]{.pre}.</p>
<p>What you should see in the [<code>Preview</code>{.docutils .literal
.notranslate}]{.pre} pane is the rendered HTML-code, that is exactly
what we saw when we called [<code>view(response)</code>{.docutils .literal
.notranslate}]{.pre} in the shell. Accordingly the [<code>type</code>{.docutils
.literal .notranslate}]{.pre} of the request in the log is
[<code>html</code>{.docutils .literal .notranslate}]{.pre}. The other requests have
types like [<code>css</code>{.docutils .literal .notranslate}]{.pre} or
[<code>js</code>{.docutils .literal .notranslate}]{.pre}, but what interests us is
the one request called [<code>quotes?page=1</code>{.docutils .literal
.notranslate}]{.pre} with the type [<code>json</code>{.docutils .literal
.notranslate}]{.pre}.</p>
<p>If we click on this request, we see that the request URL is
[<code>https://quotes.toscrape.com/api/quotes?page=1</code>{.docutils .literal
.notranslate}]{.pre} and the response is a JSON-object that contains our
quotes. We can also right-click on the request and open
[<code>Open</code>{.docutils .literal .notranslate}]{.pre}<code> </code>{.docutils .literal
.notranslate}[<code>in</code>{.docutils .literal .notranslate}]{.pre}<code> </code>{.docutils
.literal .notranslate}[<code>new</code>{.docutils .literal
.notranslate}]{.pre}<code> </code>{.docutils .literal .notranslate}[<code>tab</code>{.docutils
.literal .notranslate}]{.pre} to get a better overview.</p>
<p><a href="_images/network_03.png"><img src="_images/network_03.png" alt="JSON-object returned from the quotes.toscrape API" />{style=&quot;width: 777px; height: 375px;&quot;}</a>{.reference
.internal .image-reference}</p>
<p>With this response we can now easily parse the JSON-object and also
request each page to get every quote on the site:</p>
<p>::: {.highlight-python .notranslate}
::: highlight
import scrapy
import json</p>
<pre><code>class QuoteSpider(scrapy.Spider):
    name = &quot;quote&quot;
    allowed_domains = [&quot;quotes.toscrape.com&quot;]
    page = 1
    start_urls = [&quot;https://quotes.toscrape.com/api/quotes?page=1&quot;]

    def parse(self, response):
        data = json.loads(response.text)
        for quote in data[&quot;quotes&quot;]:
            yield {&quot;quote&quot;: quote[&quot;text&quot;]}
        if data[&quot;has_next&quot;]:
            self.page += 1
            url = f&quot;https://quotes.toscrape.com/api/quotes?page={self.page}&quot;
            yield scrapy.Request(url=url, callback=self.parse)
</code></pre>
<p>:::
:::</p>
<p>This spider starts at the first page of the quotes-API. With each
response, we parse the [<code>response.text</code>{.docutils .literal
.notranslate}]{.pre} and assign it to [<code>data</code>{.docutils .literal
.notranslate}]{.pre}. This lets us operate on the JSON-object like on a
Python dictionary. We iterate through the [<code>quotes</code>{.docutils .literal
.notranslate}]{.pre} and print out the [<code>quote[&quot;text&quot;]</code>{.docutils
.literal .notranslate}]{.pre}. If the handy [<code>has_next</code>{.docutils
.literal .notranslate}]{.pre} element is [<code>true</code>{.docutils .literal
.notranslate}]{.pre} (try loading
<a href="https://quotes.toscrape.com/api/quotes?page=10">quotes.toscrape.com/api/quotes?page=10</a>{.reference
.external} in your browser or a page-number greater than 10), we
increment the [<code>page</code>{.docutils .literal .notranslate}]{.pre} attribute
and [<code>yield</code>{.docutils .literal .notranslate}]{.pre} a new request,
inserting the incremented page-number into our [<code>url</code>{.docutils .literal
.notranslate}]{.pre}.</p>
<p>In more complex websites, it could be difficult to easily reproduce the
requests, as we could need to add [<code>headers</code>{.docutils .literal
.notranslate}]{.pre} or [<code>cookies</code>{.docutils .literal
.notranslate}]{.pre} to make it work. In those cases you can export the
requests in <a href="https://curl.haxx.se/">cURL</a>{.reference .external} format,
by right-clicking on each of them in the network tool and using the
[<code>from_curl()</code>{.xref .py .py-meth .docutils .literal
.notranslate}]{.pre} method to generate an equivalent request:</p>
<p>::: {.highlight-python .notranslate}
::: highlight
from scrapy import Request</p>
<pre><code>request = Request.from_curl(
    &quot;curl 'https://quotes.toscrape.com/api/quotes?page=1' -H 'User-Agent: Mozil&quot;
    &quot;la/5.0 (X11; Linux x86_64; rv:67.0) Gecko/20100101 Firefox/67.0' -H 'Acce&quot;
    &quot;pt: */*' -H 'Accept-Language: ca,en-US;q=0.7,en;q=0.3' --compressed -H 'X&quot;
    &quot;-Requested-With: XMLHttpRequest' -H 'Proxy-Authorization: Basic QFRLLTAzM&quot;
    &quot;zEwZTAxLTk5MWUtNDFiNC1iZWRmLTJjNGI4M2ZiNDBmNDpAVEstMDMzMTBlMDEtOTkxZS00MW&quot;
    &quot;I0LWJlZGYtMmM0YjgzZmI0MGY0' -H 'Connection: keep-alive' -H 'Referer: http&quot;
    &quot;://quotes.toscrape.com/scroll' -H 'Cache-Control: max-age=0'&quot;
)
</code></pre>
<p>:::
:::</p>
<p>Alternatively, if you want to know the arguments needed to recreate that
request you can use the <a href="#scrapy.utils.curl.curl_to_request_kwargs" title="scrapy.utils.curl.curl_to_request_kwargs">[<code>curl_to_request_kwargs()</code>{.xref .py .py-func
.docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal} function to get a dictionary with the equivalent arguments:</p>
<p>[[scrapy.utils.curl.]{.pre}]{.sig-prename .descclassname}[[curl_to_request_kwargs]{.pre}]{.sig-name .descname}[(]{.sig-paren}<em>[[curl_command]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[<a href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.12)">[str]{.pre}</a>{.reference .external}]{.n}</em>, <em>[[ignore_unknown_options]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[<a href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.12)">[bool]{.pre}</a>{.reference .external}]{.n}[ ]{.w}[[=]{.pre}]{.o}[ ]{.w}[[True]{.pre}]{.default_value}</em>[)]{.sig-paren} [[→]{.sig-return-icon} [<a href="https://docs.python.org/3/library/stdtypes.html#dict" title="(in Python v3.12)">[dict]{.pre}</a>{.reference .external}]{.sig-return-typehint}]{.sig-return}<a href="_modules/scrapy/utils/curl.html#curl_to_request_kwargs">[[[source]]{.pre}]{.viewcode-link}</a>{.reference .internal}<a href="#scrapy.utils.curl.curl_to_request_kwargs" title="Permalink to this definition">¶</a>{.headerlink}</p>
<p>:   Convert a cURL command syntax to Request kwargs.</p>
<pre><code>Parameters

:   -   **curl_command**
        ([*str*](https://docs.python.org/3/library/stdtypes.html#str &quot;(in Python v3.12)&quot;){.reference
        .external}) -- string containing the curl command

    -   **ignore_unknown_options**
        ([*bool*](https://docs.python.org/3/library/functions.html#bool &quot;(in Python v3.12)&quot;){.reference
        .external}) -- If true, only a warning is emitted when cURL
        options are unknown. Otherwise raises an error. (default:
        True)

Returns

:   dictionary of Request kwargs
</code></pre>
<p>Note that to translate a cURL command into a Scrapy request, you may use
<a href="https://michael-shub.github.io/curl2scrapy/">curl2scrapy</a>{.reference
.external}.</p>
<p>As you can see, with a few inspections in the Network-tool we were able
to easily replicate the dynamic requests of the scrolling functionality
of the page. Crawling dynamic pages can be quite daunting and pages can
be very complex, but it (mostly) boils down to identifying the correct
request and replicating it in your spider.
:::
:::</p>
<p>[]{#document-topics/dynamic-content}</p>
<p>::: {#selecting-dynamically-loaded-content .section}
[]{#topics-dynamic-content}</p>
<h3 id="selecting-dynamically-loaded-contentheaderlink"><a class="header" href="#selecting-dynamically-loaded-contentheaderlink">Selecting dynamically-loaded content<a href="#selecting-dynamically-loaded-content" title="Permalink to this heading">¶</a>{.headerlink}</a></h3>
<p>Some webpages show the desired data when you load them in a web browser.
However, when you download them using Scrapy, you cannot reach the
desired data using <a href="index.html#topics-selectors">[selectors]{.std
.std-ref}</a>{.hoverxref .tooltip .reference
.internal}.</p>
<p>When this happens, the recommended approach is to <a href="#topics-finding-data-source">[find the data
source]{.std .std-ref}</a>{.hoverxref .tooltip
.reference .internal} and extract the data from it.</p>
<p>If you fail to do that, and you can nonetheless access the desired data
through the <a href="index.html#topics-livedom">[DOM]{.std .std-ref}</a>{.hoverxref
.tooltip .reference .internal} from your web browser, see
<a href="#topics-javascript-rendering">[Pre-rendering JavaScript]{.std
.std-ref}</a>{.hoverxref .tooltip .reference
.internal}.</p>
<p>::: {#finding-the-data-source .section}
[]{#topics-finding-data-source}</p>
<h4 id="finding-the-data-sourceheaderlink"><a class="header" href="#finding-the-data-sourceheaderlink">Finding the data source<a href="#finding-the-data-source" title="Permalink to this heading">¶</a>{.headerlink}</a></h4>
<p>To extract the desired data, you must first find its source location.</p>
<p>If the data is in a non-text-based format, such as an image or a PDF
document, use the <a href="index.html#topics-network-tool">[network tool]{.std
.std-ref}</a>{.hoverxref .tooltip
.reference .internal} of your web browser to find the corresponding
request, and <a href="#topics-reproducing-requests">[reproduce it]{.std
.std-ref}</a>{.hoverxref .tooltip .reference
.internal}.</p>
<p>If your web browser lets you select the desired data as text, the data
may be defined in embedded JavaScript code, or loaded from an external
resource in a text-based format.</p>
<p>In that case, you can use a tool like
<a href="https://github.com/stav/wgrep">wgrep</a>{.reference .external} to find the
URL of that resource.</p>
<p>If the data turns out to come from the original URL itself, you must
<a href="#topics-inspecting-source">[inspect the source code of the webpage]{.std
.std-ref}</a>{.hoverxref .tooltip .reference
.internal} to determine where the data is located.</p>
<p>If the data comes from a different URL, you will need to <a href="#topics-reproducing-requests">[reproduce the
corresponding request]{.std
.std-ref}</a>{.hoverxref .tooltip .reference
.internal}.
:::</p>
<p>::: {#inspecting-the-source-code-of-a-webpage .section}
[]{#topics-inspecting-source}</p>
<h4 id="inspecting-the-source-code-of-a-webpageheaderlink"><a class="header" href="#inspecting-the-source-code-of-a-webpageheaderlink">Inspecting the source code of a webpage<a href="#inspecting-the-source-code-of-a-webpage" title="Permalink to this heading">¶</a>{.headerlink}</a></h4>
<p>Sometimes you need to inspect the source code of a webpage (not the
<a href="index.html#topics-livedom">[DOM]{.std .std-ref}</a>{.hoverxref .tooltip
.reference .internal}) to determine where some desired data is located.</p>
<p>Use Scrapy's <a href="index.html#std-command-fetch">[<code>fetch</code>{.xref .std .std-command .docutils .literal
.notranslate}]{.pre}</a>{.hoverxref .tooltip
.reference .internal} command to download the webpage contents as seen
by Scrapy:</p>
<p>::: {.highlight-default .notranslate}
::: highlight
scrapy fetch --nolog https://example.com &gt; response.html
:::
:::</p>
<p>If the desired data is in embedded JavaScript code within a
[<code>&lt;script/&gt;</code>{.docutils .literal .notranslate}]{.pre} element, see
<a href="#topics-parsing-javascript">[Parsing JavaScript code]{.std
.std-ref}</a>{.hoverxref .tooltip .reference
.internal}.</p>
<p>If you cannot find the desired data, first make sure it's not just
Scrapy: download the webpage with an HTTP client like
<a href="https://curl.haxx.se/">curl</a>{.reference .external} or
<a href="https://www.gnu.org/software/wget/">wget</a>{.reference .external} and see
if the information can be found in the response they get.</p>
<p>If they get a response with the desired data, modify your Scrapy
[<code>Request</code>{.xref .py .py-class .docutils .literal .notranslate}]{.pre}
to match that of the other HTTP client. For example, try using the same
user-agent string (<a href="index.html#std-setting-USER_AGENT">[<code>USER_AGENT</code>{.xref .std .std-setting .docutils
.literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal}) or the same [<code>headers</code>{.xref .py
.py-attr .docutils .literal .notranslate}]{.pre}.</p>
<p>If they also get a response without the desired data, you'll need to
take steps to make your request more similar to that of the web browser.
See <a href="#topics-reproducing-requests">[Reproducing requests]{.std
.std-ref}</a>{.hoverxref .tooltip .reference
.internal}.
:::</p>
<p>::: {#reproducing-requests .section}
[]{#topics-reproducing-requests}</p>
<h4 id="reproducing-requestsheaderlink"><a class="header" href="#reproducing-requestsheaderlink">Reproducing requests<a href="#reproducing-requests" title="Permalink to this heading">¶</a>{.headerlink}</a></h4>
<p>Sometimes we need to reproduce a request the way our web browser
performs it.</p>
<p>Use the <a href="index.html#topics-network-tool">[network tool]{.std
.std-ref}</a>{.hoverxref .tooltip
.reference .internal} of your web browser to see how your web browser
performs the desired request, and try to reproduce that request with
Scrapy.</p>
<p>It might be enough to yield a [<code>Request</code>{.xref .py .py-class .docutils
.literal .notranslate}]{.pre} with the same HTTP method and URL.
However, you may also need to reproduce the body, headers and form
parameters (see [<code>FormRequest</code>{.xref .py .py-class .docutils .literal
.notranslate}]{.pre}) of that request.</p>
<p>As all major browsers allow to export the requests in
<a href="https://curl.haxx.se/">cURL</a>{.reference .external} format, Scrapy
incorporates the method [<code>from_curl()</code>{.xref .py .py-meth .docutils
.literal .notranslate}]{.pre} to generate an equivalent [<code>Request</code>{.xref
.py .py-class .docutils .literal .notranslate}]{.pre} from a cURL
command. To get more information visit <a href="index.html#requests-from-curl">[request from curl]{.std
.std-ref}</a>{.hoverxref .tooltip .reference
.internal} inside the network tool section.</p>
<p>Once you get the expected response, you can <a href="#topics-handling-response-formats">[extract the desired data
from it]{.std .std-ref}</a>{.hoverxref
.tooltip .reference .internal}.</p>
<p>You can reproduce any request with Scrapy. However, some times
reproducing all necessary requests may not seem efficient in developer
time. If that is your case, and crawling speed is not a major concern
for you, you can alternatively consider <a href="#topics-javascript-rendering">[JavaScript pre-rendering]{.std
.std-ref}</a>{.hoverxref .tooltip .reference
.internal}.</p>
<p>If you get the expected response sometimes, but not always, the issue is
probably not your request, but the target server. The target server
might be buggy, overloaded, or <a href="index.html#bans">[banning]{.std
.std-ref}</a>{.hoverxref .tooltip .reference .internal}
some of your requests.</p>
<p>Note that to translate a cURL command into a Scrapy request, you may use
<a href="https://michael-shub.github.io/curl2scrapy/">curl2scrapy</a>{.reference
.external}.
:::</p>
<p>::: {#handling-different-response-formats .section}
[]{#topics-handling-response-formats}</p>
<h4 id="handling-different-response-formatsheaderlink"><a class="header" href="#handling-different-response-formatsheaderlink">Handling different response formats<a href="#handling-different-response-formats" title="Permalink to this heading">¶</a>{.headerlink}</a></h4>
<p>Once you have a response with the desired data, how you extract the
desired data from it depends on the type of response:</p>
<ul>
<li>
<p>If the response is HTML or XML, use <a href="index.html#topics-selectors">[selectors]{.std
.std-ref}</a>{.hoverxref .tooltip
.reference .internal} as usual.</p>
</li>
<li>
<p>If the response is JSON, use <a href="https://docs.python.org/3/library/json.html#json.loads" title="(in Python v3.12)">[<code>json.loads()</code>{.xref .py .py-func
.docutils .literal
.notranslate}]{.pre}</a>{.reference
.external} to load the desired data from <a href="index.html#scrapy.http.TextResponse.text" title="scrapy.http.TextResponse.text">[<code>response.text</code>{.xref .py
.py-attr .docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal}:</p>
<p>::: {.highlight-python .notranslate}
::: highlight
data = json.loads(response.text)
:::
:::</p>
<p>If the desired data is inside HTML or XML code embedded within JSON
data, you can load that HTML or XML code into a [<code>Selector</code>{.xref
.py .py-class .docutils .literal .notranslate}]{.pre} and then <a href="index.html#topics-selectors">[use
it]{.std .std-ref}</a>{.hoverxref .tooltip
.reference .internal} as usual:</p>
<p>::: {.highlight-python .notranslate}
::: highlight
selector = Selector(data[&quot;html&quot;])
:::
:::</p>
</li>
<li>
<p>If the response is JavaScript, or HTML with a [<code>&lt;script/&gt;</code>{.docutils
.literal .notranslate}]{.pre} element containing the desired data,
see <a href="#topics-parsing-javascript">[Parsing JavaScript code]{.std
.std-ref}</a>{.hoverxref .tooltip
.reference .internal}.</p>
</li>
<li>
<p>If the response is CSS, use a <a href="https://docs.python.org/3/library/re.html" title="(in Python v3.12)">[regular expression]{.xref .std
.std-doc}</a>{.reference
.external} to extract the desired data from <a href="index.html#scrapy.http.TextResponse.text" title="scrapy.http.TextResponse.text">[<code>response.text</code>{.xref
.py .py-attr .docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal}.</p>
</li>
</ul>
<pre><code class="language-{=html}">&lt;!-- --&gt;
</code></pre>
<ul>
<li>
<p>If the response is an image or another format based on images (e.g.
PDF), read the response as bytes from [<code>response.body</code>{.xref .py
.py-attr .docutils .literal .notranslate}]{.pre} and use an OCR
solution to extract the desired data as text.</p>
<p>For example, you can use
<a href="https://github.com/madmaze/pytesseract">pytesseract</a>{.reference
.external}. To read a table from a PDF,
<a href="https://github.com/chezou/tabula-py">tabula-py</a>{.reference
.external} may be a better choice.</p>
</li>
<li>
<p>If the response is SVG, or HTML with embedded SVG containing the
desired data, you may be able to extract the desired data using
<a href="index.html#topics-selectors">[selectors]{.std .std-ref}</a>{.hoverxref
.tooltip .reference .internal}, since SVG is based on XML.</p>
<p>Otherwise, you might need to convert the SVG code into a raster
image, and <a href="#topics-parsing-images">[handle that raster image]{.std
.std-ref}</a>{.hoverxref .tooltip .reference
.internal}.
:::</p>
</li>
</ul>
<p>::: {#parsing-javascript-code .section}
[]{#topics-parsing-javascript}</p>
<h4 id="parsing-javascript-codeheaderlink"><a class="header" href="#parsing-javascript-codeheaderlink">Parsing JavaScript code<a href="#parsing-javascript-code" title="Permalink to this heading">¶</a>{.headerlink}</a></h4>
<p>If the desired data is hardcoded in JavaScript, you first need to get
the JavaScript code:</p>
<ul>
<li>
<p>If the JavaScript code is in a JavaScript file, simply read
<a href="index.html#scrapy.http.TextResponse.text" title="scrapy.http.TextResponse.text">[<code>response.text</code>{.xref .py .py-attr .docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal}.</p>
</li>
<li>
<p>If the JavaScript code is within a [<code>&lt;script/&gt;</code>{.docutils .literal
.notranslate}]{.pre} element of an HTML page, use <a href="index.html#topics-selectors">[selectors]{.std
.std-ref}</a>{.hoverxref .tooltip
.reference .internal} to extract the text within that
[<code>&lt;script/&gt;</code>{.docutils .literal .notranslate}]{.pre} element.</p>
</li>
</ul>
<p>Once you have a string with the JavaScript code, you can extract the
desired data from it:</p>
<ul>
<li>
<p>You might be able to use a <a href="https://docs.python.org/3/library/re.html" title="(in Python v3.12)">[regular expression]{.xref .std
.std-doc}</a>{.reference
.external} to extract the desired data in JSON format, which you can
then parse with <a href="https://docs.python.org/3/library/json.html#json.loads" title="(in Python v3.12)">[<code>json.loads()</code>{.xref .py .py-func .docutils
.literal
.notranslate}]{.pre}</a>{.reference
.external}.</p>
<p>For example, if the JavaScript code contains a separate line like
[<code>var</code>{.docutils .literal .notranslate}]{.pre}<code> </code>{.docutils .literal
.notranslate}[<code>data</code>{.docutils .literal
.notranslate}]{.pre}<code> </code>{.docutils .literal
.notranslate}[<code>=</code>{.docutils .literal
.notranslate}]{.pre}<code> </code>{.docutils .literal
.notranslate}[<code>{&quot;field&quot;:</code>{.docutils .literal
.notranslate}]{.pre}<code> </code>{.docutils .literal
.notranslate}[<code>&quot;value&quot;};</code>{.docutils .literal .notranslate}]{.pre}
you can extract that data as follows:</p>
<p>::: {.highlight-pycon .notranslate}
::: highlight
&gt;&gt;&gt; pattern = r&quot;\bvar\s+data\s*=\s*({.<em>?})\s</em>;\s*\n&quot;
&gt;&gt;&gt; json_data = response.css(&quot;script::text&quot;).re_first(pattern)
&gt;&gt;&gt; json.loads(json_data)
{'field': 'value'}
:::
:::</p>
</li>
<li>
<p><a href="https://github.com/Nykakin/chompjs">chompjs</a>{.reference .external}
provides an API to parse JavaScript objects into a <a href="https://docs.python.org/3/library/stdtypes.html#dict" title="(in Python v3.12)">[<code>dict</code>{.xref
.py .py-class .docutils .literal
.notranslate}]{.pre}</a>{.reference
.external}.</p>
<p>For example, if the JavaScript code contains [<code>var</code>{.docutils
.literal .notranslate}]{.pre}<code> </code>{.docutils .literal
.notranslate}[<code>data</code>{.docutils .literal
.notranslate}]{.pre}<code> </code>{.docutils .literal
.notranslate}[<code>=</code>{.docutils .literal
.notranslate}]{.pre}<code> </code>{.docutils .literal
.notranslate}[<code>{field:</code>{.docutils .literal
.notranslate}]{.pre}<code> </code>{.docutils .literal
.notranslate}[<code>&quot;value&quot;,</code>{.docutils .literal
.notranslate}]{.pre}<code> </code>{.docutils .literal
.notranslate}[<code>secondField:</code>{.docutils .literal
.notranslate}]{.pre}<code> </code>{.docutils .literal
.notranslate}[<code>&quot;second</code>{.docutils .literal
.notranslate}]{.pre}<code> </code>{.docutils .literal
.notranslate}[<code>value&quot;};</code>{.docutils .literal .notranslate}]{.pre} you
can extract that data as follows:</p>
<p>::: {.highlight-pycon .notranslate}
::: highlight
&gt;&gt;&gt; import chompjs
&gt;&gt;&gt; javascript = response.css(&quot;script::text&quot;).get()
&gt;&gt;&gt; data = chompjs.parse_js_object(javascript)
&gt;&gt;&gt; data
{'field': 'value', 'secondField': 'second value'}
:::
:::</p>
</li>
<li>
<p>Otherwise, use
<a href="https://github.com/scrapinghub/js2xml">js2xml</a>{.reference
.external} to convert the JavaScript code into an XML document that
you can parse using <a href="index.html#topics-selectors">[selectors]{.std
.std-ref}</a>{.hoverxref .tooltip
.reference .internal}.</p>
<p>For example, if the JavaScript code contains [<code>var</code>{.docutils
.literal .notranslate}]{.pre}<code> </code>{.docutils .literal
.notranslate}[<code>data</code>{.docutils .literal
.notranslate}]{.pre}<code> </code>{.docutils .literal
.notranslate}[<code>=</code>{.docutils .literal
.notranslate}]{.pre}<code> </code>{.docutils .literal
.notranslate}[<code>{field:</code>{.docutils .literal
.notranslate}]{.pre}<code> </code>{.docutils .literal
.notranslate}[<code>&quot;value&quot;};</code>{.docutils .literal .notranslate}]{.pre}
you can extract that data as follows:</p>
<p>::: {.highlight-pycon .notranslate}
::: highlight
&gt;&gt;&gt; import js2xml
&gt;&gt;&gt; import lxml.etree
&gt;&gt;&gt; from parsel import Selector
&gt;&gt;&gt; javascript = response.css(&quot;script::text&quot;).get()
&gt;&gt;&gt; xml = lxml.etree.tostring(js2xml.parse(javascript), encoding=&quot;unicode&quot;)
&gt;&gt;&gt; selector = Selector(text=xml)
&gt;&gt;&gt; selector.css('var[name=&quot;data&quot;]').get()
'<var name="data"><object><property name="field"><string>value</string></property></object></var>'
:::
:::
:::</p>
</li>
</ul>
<p>::: {#pre-rendering-javascript .section}
[]{#topics-javascript-rendering}</p>
<h4 id="pre-rendering-javascriptheaderlink"><a class="header" href="#pre-rendering-javascriptheaderlink">Pre-rendering JavaScript<a href="#pre-rendering-javascript" title="Permalink to this heading">¶</a>{.headerlink}</a></h4>
<p>On webpages that fetch data from additional requests, reproducing those
requests that contain the desired data is the preferred approach. The
effort is often worth the result: structured, complete data with minimum
parsing time and network transfer.</p>
<p>However, sometimes it can be really hard to reproduce certain requests.
Or you may need something that no request can give you, such as a
screenshot of a webpage as seen in a web browser.</p>
<p>In these cases use the
<a href="https://github.com/scrapinghub/splash">Splash</a>{.reference .external}
JavaScript-rendering service, along with
<a href="https://github.com/scrapy-plugins/scrapy-splash">scrapy-splash</a>{.reference
.external} for seamless integration.</p>
<p>Splash returns as HTML the <a href="index.html#topics-livedom">[DOM]{.std
.std-ref}</a>{.hoverxref .tooltip .reference
.internal} of a webpage, so that you can parse it with <a href="index.html#topics-selectors">[selectors]{.std
.std-ref}</a>{.hoverxref .tooltip .reference
.internal}. It provides great flexibility through
<a href="https://splash.readthedocs.io/en/stable/api.html">configuration</a>{.reference
.external} or
<a href="https://splash.readthedocs.io/en/stable/scripting-tutorial.html">scripting</a>{.reference
.external}.</p>
<p>If you need something beyond what Splash offers, such as interacting
with the DOM on-the-fly from Python code instead of using a
previously-written script, or handling multiple web browser windows, you
might need to <a href="#topics-headless-browsing">[use a headless browser]{.std
.std-ref}</a>{.hoverxref .tooltip .reference
.internal} instead.
:::</p>
<p>::: {#using-a-headless-browser .section}
[]{#topics-headless-browsing}</p>
<h4 id="using-a-headless-browserheaderlink"><a class="header" href="#using-a-headless-browserheaderlink">Using a headless browser<a href="#using-a-headless-browser" title="Permalink to this heading">¶</a>{.headerlink}</a></h4>
<p>A <a href="https://en.wikipedia.org/wiki/Headless_browser">headless
browser</a>{.reference
.external} is a special web browser that provides an API for automation.
By installing the <a href="index.html#install-asyncio">[asyncio reactor]{.std
.std-ref}</a>{.hoverxref .tooltip .reference
.internal}, it is possible to integrate [<code>asyncio</code>{.docutils .literal
.notranslate}]{.pre}-based libraries which handle headless browsers.</p>
<p>One such library is
<a href="https://github.com/microsoft/playwright-python">playwright-python</a>{.reference
.external} (an official Python port of
<a href="https://github.com/microsoft/playwright">playwright</a>{.reference
.external}). The following is a simple snippet to illustrate its usage
within a Scrapy spider:</p>
<p>::: {.highlight-python .notranslate}
::: highlight
import scrapy
from playwright.async_api import async_playwright</p>
<pre><code>class PlaywrightSpider(scrapy.Spider):
    name = &quot;playwright&quot;
    start_urls = [&quot;data:,&quot;]  # avoid using the default Scrapy downloader

    async def parse(self, response):
        async with async_playwright() as pw:
            browser = await pw.chromium.launch()
            page = await browser.new_page()
            await page.goto(&quot;https://example.org&quot;)
            title = await page.title()
            return {&quot;title&quot;: title}
</code></pre>
<p>:::
:::</p>
<p>However, using
<a href="https://github.com/microsoft/playwright-python">playwright-python</a>{.reference
.external} directly as in the above example circumvents most of the
Scrapy components (middlewares, dupefilter, etc). We recommend using
<a href="https://github.com/scrapy-plugins/scrapy-playwright">scrapy-playwright</a>{.reference
.external} for a better integration.
:::
:::</p>
<p>[]{#document-topics/leaks}</p>
<p>::: {#debugging-memory-leaks .section}
[]{#topics-leaks}</p>
<h3 id="debugging-memory-leaksheaderlink"><a class="header" href="#debugging-memory-leaksheaderlink">Debugging memory leaks<a href="#debugging-memory-leaks" title="Permalink to this heading">¶</a>{.headerlink}</a></h3>
<p>In Scrapy, objects such as requests, responses and items have a finite
lifetime: they are created, used for a while, and finally destroyed.</p>
<p>From all those objects, the Request is probably the one with the longest
lifetime, as it stays waiting in the Scheduler queue until it's time to
process it. For more info see <a href="index.html#topics-architecture">[Architecture overview]{.std
.std-ref}</a>{.hoverxref .tooltip
.reference .internal}.</p>
<p>As these Scrapy objects have a (rather long) lifetime, there is always
the risk of accumulating them in memory without releasing them properly
and thus causing what is known as a &quot;memory leak&quot;.</p>
<p>To help debugging memory leaks, Scrapy provides a built-in mechanism for
tracking objects references called <a href="#topics-leaks-trackrefs">[trackref]{.std
.std-ref}</a>{.hoverxref .tooltip .reference
.internal}, and you can also use a third-party library called
<a href="#topics-leaks-muppy">[muppy]{.std .std-ref}</a>{.hoverxref .tooltip
.reference .internal} for more advanced memory debugging (see below for
more info). Both mechanisms must be used from the <a href="index.html#topics-telnetconsole">[Telnet Console]{.std
.std-ref}</a>{.hoverxref .tooltip
.reference .internal}.</p>
<p>::: {#common-causes-of-memory-leaks .section}</p>
<h4 id="common-causes-of-memory-leaksheaderlink"><a class="header" href="#common-causes-of-memory-leaksheaderlink">Common causes of memory leaks<a href="#common-causes-of-memory-leaks" title="Permalink to this heading">¶</a>{.headerlink}</a></h4>
<p>It happens quite often (sometimes by accident, sometimes on purpose)
that the Scrapy developer passes objects referenced in Requests (for
example, using the [<code>cb_kwargs</code>{.xref .py .py-attr .docutils .literal
.notranslate}]{.pre} or [<code>meta</code>{.xref .py .py-attr .docutils .literal
.notranslate}]{.pre} attributes or the request callback function) and
that effectively bounds the lifetime of those referenced objects to the
lifetime of the Request. This is, by far, the most common cause of
memory leaks in Scrapy projects, and a quite difficult one to debug for
newcomers.</p>
<p>In big projects, the spiders are typically written by different people
and some of those spiders could be &quot;leaking&quot; and thus affecting the rest
of the other (well-written) spiders when they get to run concurrently,
which, in turn, affects the whole crawling process.</p>
<p>The leak could also come from a custom middleware, pipeline or extension
that you have written, if you are not releasing the (previously
allocated) resources properly. For example, allocating resources on
<a href="index.html#std-signal-spider_opened">[<code>spider_opened</code>{.xref .std .std-signal .docutils .literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal} but not releasing them on
<a href="index.html#std-signal-spider_closed">[<code>spider_closed</code>{.xref .std .std-signal .docutils .literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal} may cause problems if you're running
<a href="index.html#run-multiple-spiders">[multiple spiders per process]{.std
.std-ref}</a>{.hoverxref .tooltip
.reference .internal}.</p>
<p>::: {#too-many-requests .section}</p>
<h5 id="too-many-requestsheaderlink"><a class="header" href="#too-many-requestsheaderlink">Too Many Requests?<a href="#too-many-requests" title="Permalink to this heading">¶</a>{.headerlink}</a></h5>
<p>By default Scrapy keeps the request queue in memory; it includes
[<code>Request</code>{.xref .py .py-class .docutils .literal .notranslate}]{.pre}
objects and all objects referenced in Request attributes (e.g. in
[<code>cb_kwargs</code>{.xref .py .py-attr .docutils .literal .notranslate}]{.pre}
and [<code>meta</code>{.xref .py .py-attr .docutils .literal .notranslate}]{.pre}).
While not necessarily a leak, this can take a lot of memory. Enabling
<a href="index.html#topics-jobs">[persistent job queue]{.std
.std-ref}</a>{.hoverxref .tooltip .reference
.internal} could help keeping memory usage in control.
:::
:::</p>
<p>::: {#debugging-memory-leaks-with-trackref .section}
[]{#topics-leaks-trackrefs}</p>
<h4 id="debugging-memory-leaks-with-trackrefdocutils-literal-notranslatepreheaderlink"><a class="header" href="#debugging-memory-leaks-with-trackrefdocutils-literal-notranslatepreheaderlink">Debugging memory leaks with [<code>trackref</code>{.docutils .literal .notranslate}]{.pre}<a href="#debugging-memory-leaks-with-trackref" title="Permalink to this heading">¶</a>{.headerlink}</a></h4>
<p>[<code>trackref</code>{.xref .py .py-mod .docutils .literal .notranslate}]{.pre} is
a module provided by Scrapy to debug the most common cases of memory
leaks. It basically tracks the references to all live Request, Response,
Item, Spider and Selector objects.</p>
<p>You can enter the telnet console and inspect how many objects (of the
classes mentioned above) are currently alive using the
[<code>prefs()</code>{.docutils .literal .notranslate}]{.pre} function which is an
alias to the <a href="#scrapy.utils.trackref.print_live_refs" title="scrapy.utils.trackref.print_live_refs">[<code>print_live_refs()</code>{.xref .py .py-func .docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal} function:</p>
<p>::: {.highlight-default .notranslate}
::: highlight
telnet localhost 6023</p>
<pre><code>.. code-block:: pycon

    &gt;&gt;&gt; prefs()
    Live References

    ExampleSpider                       1   oldest: 15s ago
    HtmlResponse                       10   oldest: 1s ago
    Selector                            2   oldest: 0s ago
    FormRequest                       878   oldest: 7s ago
</code></pre>
<p>:::
:::</p>
<p>As you can see, that report also shows the &quot;age&quot; of the oldest object in
each class. If you're running multiple spiders per process chances are
you can figure out which spider is leaking by looking at the oldest
request or response. You can get the oldest object of each class using
the <a href="#scrapy.utils.trackref.get_oldest" title="scrapy.utils.trackref.get_oldest">[<code>get_oldest()</code>{.xref .py .py-func .docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal} function (from the telnet console).</p>
<p>::: {#which-objects-are-tracked .section}</p>
<h5 id="which-objects-are-trackedheaderlink"><a class="header" href="#which-objects-are-trackedheaderlink">Which objects are tracked?<a href="#which-objects-are-tracked" title="Permalink to this heading">¶</a>{.headerlink}</a></h5>
<p>The objects tracked by [<code>trackrefs</code>{.docutils .literal
.notranslate}]{.pre} are all from these classes (and all its
subclasses):</p>
<ul>
<li>
<p>[<code>scrapy.Request</code>{.xref .py .py-class .docutils .literal
.notranslate}]{.pre}</p>
</li>
<li>
<p><a href="index.html#scrapy.http.Response" title="scrapy.http.Response">[<code>scrapy.http.Response</code>{.xref .py .py-class .docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal}</p>
</li>
<li>
<p>[<code>scrapy.Item</code>{.xref .py .py-class .docutils .literal
.notranslate}]{.pre}</p>
</li>
<li>
<p>[<code>scrapy.Selector</code>{.xref .py .py-class .docutils .literal
.notranslate}]{.pre}</p>
</li>
<li>
<p><a href="index.html#scrapy.Spider" title="scrapy.Spider">[<code>scrapy.Spider</code>{.xref .py .py-class .docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal}
:::</p>
</li>
</ul>
<p>::: {#a-real-example .section}</p>
<h5 id="a-real-exampleheaderlink"><a class="header" href="#a-real-exampleheaderlink">A real example<a href="#a-real-example" title="Permalink to this heading">¶</a>{.headerlink}</a></h5>
<p>Let's see a concrete example of a hypothetical case of memory leaks.
Suppose we have some spider with a line similar to this one:</p>
<p>::: {.highlight-default .notranslate}
::: highlight
return Request(f&quot;http://www.somenastyspider.com/product.php?pid={product_id}&quot;,
callback=self.parse, cb_kwargs={'referer': response})
:::
:::</p>
<p>That line is passing a response reference inside a request which
effectively ties the response lifetime to the requests' one, and that
would definitely cause memory leaks.</p>
<p>Let's see how we can discover the cause (without knowing it a priori, of
course) by using the [<code>trackref</code>{.docutils .literal .notranslate}]{.pre}
tool.</p>
<p>After the crawler is running for a few minutes and we notice its memory
usage has grown a lot, we can enter its telnet console and check the
live references:</p>
<p>::: {.highlight-pycon .notranslate}
::: highlight
&gt;&gt;&gt; prefs()
Live References</p>
<pre><code>SomenastySpider                     1   oldest: 15s ago
HtmlResponse                     3890   oldest: 265s ago
Selector                            2   oldest: 0s ago
Request                          3878   oldest: 250s ago
</code></pre>
<p>:::
:::</p>
<p>The fact that there are so many live responses (and that they're so old)
is definitely suspicious, as responses should have a relatively short
lifetime compared to Requests. The number of responses is similar to the
number of requests, so it looks like they are tied in a some way. We can
now go and check the code of the spider to discover the nasty line that
is generating the leaks (passing response references inside requests).</p>
<p>Sometimes extra information about live objects can be helpful. Let's
check the oldest response:</p>
<p>::: {.highlight-pycon .notranslate}
::: highlight
&gt;&gt;&gt; from scrapy.utils.trackref import get_oldest
&gt;&gt;&gt; r = get_oldest(&quot;HtmlResponse&quot;)
&gt;&gt;&gt; r.url
'http://www.somenastyspider.com/product.php?pid=123'
:::
:::</p>
<p>If you want to iterate over all objects, instead of getting the oldest
one, you can use the <a href="#scrapy.utils.trackref.iter_all" title="scrapy.utils.trackref.iter_all">[<code>scrapy.utils.trackref.iter_all()</code>{.xref .py
.py-func .docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal} function:</p>
<p>::: {.highlight-pycon .notranslate}
::: highlight
&gt;&gt;&gt; from scrapy.utils.trackref import iter_all
&gt;&gt;&gt; [r.url for r in iter_all(&quot;HtmlResponse&quot;)]
['http://www.somenastyspider.com/product.php?pid=123',
'http://www.somenastyspider.com/product.php?pid=584',
...]
:::
:::
:::</p>
<p>::: {#too-many-spiders .section}</p>
<h5 id="too-many-spidersheaderlink"><a class="header" href="#too-many-spidersheaderlink">Too many spiders?<a href="#too-many-spiders" title="Permalink to this heading">¶</a>{.headerlink}</a></h5>
<p>If your project has too many spiders executed in parallel, the output of
[<code>prefs()</code>{.xref .py .py-func .docutils .literal .notranslate}]{.pre}
can be difficult to read. For this reason, that function has a
[<code>ignore</code>{.docutils .literal .notranslate}]{.pre} argument which can be
used to ignore a particular class (and all its subclasses). For example,
this won't show any live references to spiders:</p>
<p>::: {.highlight-pycon .notranslate}
::: highlight
&gt;&gt;&gt; from scrapy.spiders import Spider
&gt;&gt;&gt; prefs(ignore=Spider)
:::
:::</p>
<p>[]{#module-scrapy.utils.trackref .target}
:::</p>
<p>::: {#scrapy-utils-trackref-module .section}</p>
<h5 id="scrapyutilstrackref-moduleheaderlink"><a class="header" href="#scrapyutilstrackref-moduleheaderlink">scrapy.utils.trackref module<a href="#scrapy-utils-trackref-module" title="Permalink to this heading">¶</a>{.headerlink}</a></h5>
<p>Here are the functions available in the <a href="#module-scrapy.utils.trackref" title="scrapy.utils.trackref: Track references of live objects">[<code>trackref</code>{.xref .py .py-mod
.docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal} module.</p>
<p><em>[class]{.pre}[ ]{.w}</em>[[scrapy.utils.trackref.]{.pre}]{.sig-prename .descclassname}[[object_ref]{.pre}]{.sig-name .descname}<a href="_modules/scrapy/utils/trackref.html#object_ref">[[[source]]{.pre}]{.viewcode-link}</a>{.reference .internal}<a href="#scrapy.utils.trackref.object_ref" title="Permalink to this definition">¶</a>{.headerlink}</p>
<p>:   Inherit from this class if you want to track live instances with the
[<code>trackref</code>{.docutils .literal .notranslate}]{.pre} module.</p>
<pre><code class="language-{=html}">&lt;!-- --&gt;
</code></pre>
<p>[[scrapy.utils.trackref.]{.pre}]{.sig-prename .descclassname}[[print_live_refs]{.pre}]{.sig-name .descname}[(]{.sig-paren}<em>[[class_name]{.pre}]{.n}</em>, <em>[[ignore]{.pre}]{.n}[[=]{.pre}]{.o}[[NoneType]{.pre}]{.default_value}</em>[)]{.sig-paren}<a href="_modules/scrapy/utils/trackref.html#print_live_refs">[[[source]]{.pre}]{.viewcode-link}</a>{.reference .internal}<a href="#scrapy.utils.trackref.print_live_refs" title="Permalink to this definition">¶</a>{.headerlink}</p>
<p>:   Print a report of live references, grouped by class name.</p>
<pre><code>Parameters

:   **ignore**
    ([*type*](https://docs.python.org/3/library/functions.html#type &quot;(in Python v3.12)&quot;){.reference
    .external} *or*
    [*tuple*](https://docs.python.org/3/library/stdtypes.html#tuple &quot;(in Python v3.12)&quot;){.reference
    .external}) -- if given, all objects from the specified class
    (or tuple of classes) will be ignored.
</code></pre>
<pre><code class="language-{=html}">&lt;!-- --&gt;
</code></pre>
<p>[[scrapy.utils.trackref.]{.pre}]{.sig-prename .descclassname}[[get_oldest]{.pre}]{.sig-name .descname}[(]{.sig-paren}<em>[[class_name]{.pre}]{.n}</em>[)]{.sig-paren}<a href="_modules/scrapy/utils/trackref.html#get_oldest">[[[source]]{.pre}]{.viewcode-link}</a>{.reference .internal}<a href="#scrapy.utils.trackref.get_oldest" title="Permalink to this definition">¶</a>{.headerlink}</p>
<p>:   Return the oldest object alive with the given class name, or
[<code>None</code>{.docutils .literal .notranslate}]{.pre} if none is found.
Use <a href="#scrapy.utils.trackref.print_live_refs" title="scrapy.utils.trackref.print_live_refs">[<code>print_live_refs()</code>{.xref .py .py-func .docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal} first to get a list of all tracked live objects per class
name.</p>
<pre><code class="language-{=html}">&lt;!-- --&gt;
</code></pre>
<p>[[scrapy.utils.trackref.]{.pre}]{.sig-prename .descclassname}[[iter_all]{.pre}]{.sig-name .descname}[(]{.sig-paren}<em>[[class_name]{.pre}]{.n}</em>[)]{.sig-paren}<a href="_modules/scrapy/utils/trackref.html#iter_all">[[[source]]{.pre}]{.viewcode-link}</a>{.reference .internal}<a href="#scrapy.utils.trackref.iter_all" title="Permalink to this definition">¶</a>{.headerlink}</p>
<p>:   Return an iterator over all objects alive with the given class name,
or [<code>None</code>{.docutils .literal .notranslate}]{.pre} if none is found.
Use <a href="#scrapy.utils.trackref.print_live_refs" title="scrapy.utils.trackref.print_live_refs">[<code>print_live_refs()</code>{.xref .py .py-func .docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal} first to get a list of all tracked live objects per class
name.
:::
:::</p>
<p>::: {#debugging-memory-leaks-with-muppy .section}
[]{#topics-leaks-muppy}</p>
<h4 id="debugging-memory-leaks-with-muppyheaderlink"><a class="header" href="#debugging-memory-leaks-with-muppyheaderlink">Debugging memory leaks with muppy<a href="#debugging-memory-leaks-with-muppy" title="Permalink to this heading">¶</a>{.headerlink}</a></h4>
<p>[<code>trackref</code>{.docutils .literal .notranslate}]{.pre} provides a very
convenient mechanism for tracking down memory leaks, but it only keeps
track of the objects that are more likely to cause memory leaks.
However, there are other cases where the memory leaks could come from
other (more or less obscure) objects. If this is your case, and you
can't find your leaks using [<code>trackref</code>{.docutils .literal
.notranslate}]{.pre}, you still have another resource: the muppy
library.</p>
<p>You can use muppy from
<a href="https://pypi.org/project/Pympler/">Pympler</a>{.reference .external}.</p>
<p>If you use [<code>pip</code>{.docutils .literal .notranslate}]{.pre}, you can
install muppy with the following command:</p>
<p>::: {.highlight-default .notranslate}
::: highlight
pip install Pympler
:::
:::</p>
<p>Here's an example to view all Python objects available in the heap using
muppy:</p>
<p>::: {.highlight-pycon .notranslate}
::: highlight
&gt;&gt;&gt; from pympler import muppy
&gt;&gt;&gt; all_objects = muppy.get_objects()
&gt;&gt;&gt; len(all_objects)
28667
&gt;&gt;&gt; from pympler import summary
&gt;&gt;&gt; suml = summary.summarize(all_objects)
&gt;&gt;&gt; summary.print_(suml)
types |   # objects |   total size
==================================== | =========== | ============
&lt;class 'str |        9822 |      1.10 MB
&lt;class 'dict |        1658 |    856.62 KB
&lt;class 'type |         436 |    443.60 KB
&lt;class 'code |        2974 |    419.56 KB
&lt;class '_io.BufferedWriter |           2 |    256.34 KB
&lt;class 'set |         420 |    159.88 KB
&lt;class '_io.BufferedReader |           1 |    128.17 KB
&lt;class 'wrapper_descriptor |        1130 |     88.28 KB
&lt;class 'tuple |        1304 |     86.57 KB
&lt;class 'weakref |        1013 |     79.14 KB
&lt;class 'builtin_function_or_method |         958 |     67.36 KB
&lt;class 'method_descriptor |         865 |     60.82 KB
&lt;class 'abc.ABCMeta |          62 |     59.96 KB
&lt;class 'list |         446 |     58.52 KB
&lt;class 'int |        1425 |     43.20 KB
:::
:::</p>
<p>For more info about muppy, refer to the <a href="https://pythonhosted.org/Pympler/muppy.html">muppy
documentation</a>{.reference
.external}.
:::</p>
<p>::: {#leaks-without-leaks .section}
[]{#topics-leaks-without-leaks}</p>
<h4 id="leaks-without-leaksheaderlink"><a class="header" href="#leaks-without-leaksheaderlink">Leaks without leaks<a href="#leaks-without-leaks" title="Permalink to this heading">¶</a>{.headerlink}</a></h4>
<p>Sometimes, you may notice that the memory usage of your Scrapy process
will only increase, but never decrease. Unfortunately, this could happen
even though neither Scrapy nor your project are leaking memory. This is
due to a (not so well) known problem of Python, which may not return
released memory to the operating system in some cases. For more
information on this issue see:</p>
<ul>
<li>
<p><a href="https://www.evanjones.ca/python-memory.html">Python Memory
Management</a>{.reference
.external}</p>
</li>
<li>
<p><a href="https://www.evanjones.ca/python-memory-part2.html">Python Memory Management Part
2</a>{.reference
.external}</p>
</li>
<li>
<p><a href="https://www.evanjones.ca/python-memory-part3.html">Python Memory Management Part
3</a>{.reference
.external}</p>
</li>
</ul>
<p>The improvements proposed by Evan Jones, which are detailed in <a href="https://www.evanjones.ca/memoryallocator/">this
paper</a>{.reference .external},
got merged in Python 2.5, but this only reduces the problem, it doesn't
fix it completely. To quote the paper:</p>
<blockquote>
<div>
<p><em>Unfortunately, this patch can only free an arena if there are no more
objects allocated in it anymore. This means that fragmentation is a
large issue. An application could have many megabytes of free memory,
scattered throughout all the arenas, but it will be unable to free any
of it. This is a problem experienced by all memory allocators. The
only way to solve it is to move to a compacting garbage collector,
which is able to move objects in memory. This would require
significant changes to the Python interpreter.</em></p>
</div>
</blockquote>
<p>To keep memory consumption reasonable you can split the job into several
smaller jobs or enable <a href="index.html#topics-jobs">[persistent job queue]{.std
.std-ref}</a>{.hoverxref .tooltip .reference
.internal} and stop/start spider from time to time.
:::
:::</p>
<p>[]{#document-topics/media-pipeline}</p>
<p>::: {#downloading-and-processing-files-and-images .section}
[]{#topics-media-pipeline}</p>
<h3 id="downloading-and-processing-files-and-imagesheaderlink"><a class="header" href="#downloading-and-processing-files-and-imagesheaderlink">Downloading and processing files and images<a href="#downloading-and-processing-files-and-images" title="Permalink to this heading">¶</a>{.headerlink}</a></h3>
<p>Scrapy provides reusable <a href="index.html#document-topics/item-pipeline">[item
pipelines]{.doc}</a>{.reference
.internal} for downloading files attached to a particular item (for
example, when you scrape products and also want to download their images
locally). These pipelines share a bit of functionality and structure (we
refer to them as media pipelines), but typically you'll either use the
Files Pipeline or the Images Pipeline.</p>
<p>Both pipelines implement these features:</p>
<ul>
<li>
<p>Avoid re-downloading media that was downloaded recently</p>
</li>
<li>
<p>Specifying where to store the media (filesystem directory, FTP
server, Amazon S3 bucket, Google Cloud Storage bucket)</p>
</li>
</ul>
<p>The Images Pipeline has a few extra functions for processing images:</p>
<ul>
<li>
<p>Convert all downloaded images to a common format (JPG) and mode
(RGB)</p>
</li>
<li>
<p>Thumbnail generation</p>
</li>
<li>
<p>Check images width/height to make sure they meet a minimum
constraint</p>
</li>
</ul>
<p>The pipelines also keep an internal queue of those media URLs which are
currently being scheduled for download, and connect those responses that
arrive containing the same media to that queue. This avoids downloading
the same media more than once when it's shared by several items.</p>
<p>::: {#using-the-files-pipeline .section}</p>
<h4 id="using-the-files-pipelineheaderlink"><a class="header" href="#using-the-files-pipelineheaderlink">Using the Files Pipeline<a href="#using-the-files-pipeline" title="Permalink to this heading">¶</a>{.headerlink}</a></h4>
<p>The typical workflow, when using the [<code>FilesPipeline</code>{.xref .py
.py-class .docutils .literal .notranslate}]{.pre} goes like this:</p>
<ol>
<li>
<p>In a Spider, you scrape an item and put the URLs of the desired into
a [<code>file_urls</code>{.docutils .literal .notranslate}]{.pre} field.</p>
</li>
<li>
<p>The item is returned from the spider and goes to the item pipeline.</p>
</li>
<li>
<p>When the item reaches the [<code>FilesPipeline</code>{.xref .py .py-class
.docutils .literal .notranslate}]{.pre}, the URLs in the
[<code>file_urls</code>{.docutils .literal .notranslate}]{.pre} field are
scheduled for download using the standard Scrapy scheduler and
downloader (which means the scheduler and downloader middlewares are
reused), but with a higher priority, processing them before other
pages are scraped. The item remains &quot;locked&quot; at that particular
pipeline stage until the files have finish downloading (or fail for
some reason).</p>
</li>
<li>
<p>When the files are downloaded, another field ([<code>files</code>{.docutils
.literal .notranslate}]{.pre}) will be populated with the results.
This field will contain a list of dicts with information about the
downloaded files, such as the downloaded path, the original scraped
url (taken from the [<code>file_urls</code>{.docutils .literal
.notranslate}]{.pre} field), the file checksum and the file status.
The files in the list of the [<code>files</code>{.docutils .literal
.notranslate}]{.pre} field will retain the same order of the
original [<code>file_urls</code>{.docutils .literal .notranslate}]{.pre} field.
If some file failed downloading, an error will be logged and the
file won't be present in the [<code>files</code>{.docutils .literal
.notranslate}]{.pre} field.
:::</p>
</li>
</ol>
<p>::: {#using-the-images-pipeline .section}
[]{#images-pipeline}</p>
<h4 id="using-the-images-pipelineheaderlink"><a class="header" href="#using-the-images-pipelineheaderlink">Using the Images Pipeline<a href="#using-the-images-pipeline" title="Permalink to this heading">¶</a>{.headerlink}</a></h4>
<p>Using the <a href="#scrapy.pipelines.images.ImagesPipeline" title="scrapy.pipelines.images.ImagesPipeline">[<code>ImagesPipeline</code>{.xref .py .py-class .docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal} is a lot like using the [<code>FilesPipeline</code>{.xref .py .py-class
.docutils .literal .notranslate}]{.pre}, except the default field names
used are different: you use [<code>image_urls</code>{.docutils .literal
.notranslate}]{.pre} for the image URLs of an item and it will populate
an [<code>images</code>{.docutils .literal .notranslate}]{.pre} field for the
information about the downloaded images.</p>
<p>The advantage of using the <a href="#scrapy.pipelines.images.ImagesPipeline" title="scrapy.pipelines.images.ImagesPipeline">[<code>ImagesPipeline</code>{.xref .py .py-class
.docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal} for image files is that you can configure some extra
functions like generating thumbnails and filtering the images based on
their size.</p>
<p>The Images Pipeline requires
<a href="https://github.com/python-pillow/Pillow">Pillow</a>{.reference .external}
7.1.0 or greater. It is used for thumbnailing and normalizing images to
JPEG/RGB format.
:::</p>
<p>::: {#enabling-your-media-pipeline .section}
[]{#topics-media-pipeline-enabling}</p>
<h4 id="enabling-your-media-pipelineheaderlink"><a class="header" href="#enabling-your-media-pipelineheaderlink">Enabling your Media Pipeline<a href="#enabling-your-media-pipeline" title="Permalink to this heading">¶</a>{.headerlink}</a></h4>
<p>[]{#std-setting-IMAGES_STORE .target}</p>
<p>To enable your media pipeline you must first add it to your project
<a href="index.html#std-setting-ITEM_PIPELINES">[<code>ITEM_PIPELINES</code>{.xref .std .std-setting .docutils .literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal} setting.</p>
<p>For Images Pipeline, use:</p>
<p>::: {.highlight-python .notranslate}
::: highlight
ITEM_PIPELINES = {&quot;scrapy.pipelines.images.ImagesPipeline&quot;: 1}
:::
:::</p>
<p>For Files Pipeline, use:</p>
<p>::: {.highlight-python .notranslate}
::: highlight
ITEM_PIPELINES = {&quot;scrapy.pipelines.files.FilesPipeline&quot;: 1}
:::
:::</p>
<p>::: {.admonition .note}
Note</p>
<p>You can also use both the Files and Images Pipeline at the same time.
:::</p>
<p>Then, configure the target storage setting to a valid value that will be
used for storing the downloaded images. Otherwise the pipeline will
remain disabled, even if you include it in the <a href="index.html#std-setting-ITEM_PIPELINES">[<code>ITEM_PIPELINES</code>{.xref
.std .std-setting .docutils .literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal} setting.</p>
<p>For the Files Pipeline, set the <a href="#std-setting-FILES_STORE">[<code>FILES_STORE</code>{.xref .std .std-setting
.docutils .literal
.notranslate}]{.pre}</a>{.hoverxref .tooltip
.reference .internal} setting:</p>
<p>::: {.highlight-python .notranslate}
::: highlight
FILES_STORE = &quot;/path/to/valid/dir&quot;
:::
:::</p>
<p>For the Images Pipeline, set the <a href="#std-setting-IMAGES_STORE">[<code>IMAGES_STORE</code>{.xref .std
.std-setting .docutils .literal
.notranslate}]{.pre}</a>{.hoverxref .tooltip
.reference .internal} setting:</p>
<p>::: {.highlight-python .notranslate}
::: highlight
IMAGES_STORE = &quot;/path/to/valid/dir&quot;
:::
:::
:::</p>
<p>::: {#file-naming .section}
[]{#topics-file-naming}</p>
<h4 id="file-namingheaderlink"><a class="header" href="#file-namingheaderlink">File Naming<a href="#file-naming" title="Permalink to this heading">¶</a>{.headerlink}</a></h4>
<p>::: {#default-file-naming .section}</p>
<h5 id="default-file-namingheaderlink"><a class="header" href="#default-file-namingheaderlink">Default File Naming<a href="#default-file-naming" title="Permalink to this heading">¶</a>{.headerlink}</a></h5>
<p>By default, files are stored using an <a href="https://en.wikipedia.org/wiki/SHA_hash_functions">SHA-1
hash</a>{.reference
.external} of their URLs for the file names.</p>
<p>For example, the following image URL:</p>
<p>::: {.highlight-default .notranslate}
::: highlight
http://www.example.com/image.jpg
:::
:::</p>
<p>Whose [<code>SHA-1</code>{.docutils .literal .notranslate}]{.pre}<code> </code>{.docutils
.literal .notranslate}[<code>hash</code>{.docutils .literal .notranslate}]{.pre}
is:</p>
<p>::: {.highlight-default .notranslate}
::: highlight
3afec3b4765f8f0a07b78f98c07b83f013567a0a
:::
:::</p>
<p>Will be downloaded and stored using your chosen <a href="#topics-supported-storage">[storage method]{.std
.std-ref}</a>{.hoverxref .tooltip .reference
.internal} and the following file name:</p>
<p>::: {.highlight-default .notranslate}
::: highlight
3afec3b4765f8f0a07b78f98c07b83f013567a0a.jpg
:::
:::
:::</p>
<p>::: {#custom-file-naming .section}</p>
<h5 id="custom-file-namingheaderlink"><a class="header" href="#custom-file-namingheaderlink">Custom File Naming<a href="#custom-file-naming" title="Permalink to this heading">¶</a>{.headerlink}</a></h5>
<p>You may wish to use a different calculated file name for saved files.
For example, classifying an image by including meta in the file name.</p>
<p>Customize file names by overriding the [<code>file_path</code>{.docutils .literal
.notranslate}]{.pre} method of your media pipeline.</p>
<p>For example, an image pipeline with image URL:</p>
<p>::: {.highlight-default .notranslate}
::: highlight
http://www.example.com/product/images/large/front/0000000004166
:::
:::</p>
<p>Can be processed into a file name with a condensed hash and the
perspective [<code>front</code>{.docutils .literal .notranslate}]{.pre}:</p>
<p>::: {.highlight-default .notranslate}
::: highlight
00b08510e4_front.jpg
:::
:::</p>
<p>By overriding [<code>file_path</code>{.docutils .literal .notranslate}]{.pre} like
this:</p>
<p>::: {.highlight-python .notranslate}
::: highlight
import hashlib</p>
<pre><code>def file_path(self, request, response=None, info=None, *, item=None):
    image_url_hash = hashlib.shake_256(request.url.encode()).hexdigest(5)
    image_perspective = request.url.split(&quot;/&quot;)[-2]
    image_filename = f&quot;{image_url_hash}_{image_perspective}.jpg&quot;

    return image_filename
</code></pre>
<p>:::
:::</p>
<p>::: {.admonition .warning}
Warning</p>
<p>If your custom file name scheme relies on meta data that can vary
between scrapes it may lead to unexpected re-downloading of existing
media using new file names.</p>
<p>For example, if your custom file name scheme uses a product title and
the site changes an item's product title between scrapes, Scrapy will
re-download the same media using updated file names.
:::</p>
<p>For more information about the [<code>file_path</code>{.docutils .literal
.notranslate}]{.pre} method, see <a href="#topics-media-pipeline-override">[Extending the Media Pipelines]{.std
.std-ref}</a>{.hoverxref .tooltip
.reference .internal}.
:::
:::</p>
<p>::: {#supported-storage .section}
[]{#topics-supported-storage}</p>
<h4 id="supported-storageheaderlink"><a class="header" href="#supported-storageheaderlink">Supported Storage<a href="#supported-storage" title="Permalink to this heading">¶</a>{.headerlink}</a></h4>
<p>::: {#file-system-storage .section}</p>
<h5 id="file-system-storageheaderlink"><a class="header" href="#file-system-storageheaderlink">File system storage<a href="#file-system-storage" title="Permalink to this heading">¶</a>{.headerlink}</a></h5>
<p>File system storage will save files to the following path:</p>
<p>::: {.highlight-default .notranslate}
::: highlight
&lt;IMAGES_STORE&gt;/full/&lt;FILE_NAME&gt;
:::
:::</p>
<p>Where:</p>
<ul>
<li>
<p>[<code>&lt;IMAGES_STORE&gt;</code>{.docutils .literal .notranslate}]{.pre} is the
directory defined in <a href="#std-setting-IMAGES_STORE">[<code>IMAGES_STORE</code>{.xref .std .std-setting
.docutils .literal
.notranslate}]{.pre}</a>{.hoverxref .tooltip
.reference .internal} setting for the Images Pipeline.</p>
</li>
<li>
<p>[<code>full</code>{.docutils .literal .notranslate}]{.pre} is a sub-directory
to separate full images from thumbnails (if used). For more info see
<a href="#topics-images-thumbnails">[Thumbnail generation for images]{.std
.std-ref}</a>{.hoverxref .tooltip .reference
.internal}.</p>
</li>
<li>
<p>[<code>&lt;FILE_NAME&gt;</code>{.docutils .literal .notranslate}]{.pre} is the file
name assigned to the file. For more info see <a href="#topics-file-naming">[File Naming]{.std
.std-ref}</a>{.hoverxref .tooltip .reference
.internal}.
:::</p>
</li>
</ul>
<p>::: {#ftp-server-storage .section}
[]{#media-pipeline-ftp}</p>
<h5 id="ftp-server-storageheaderlink"><a class="header" href="#ftp-server-storageheaderlink">FTP server storage<a href="#ftp-server-storage" title="Permalink to this heading">¶</a>{.headerlink}</a></h5>
<p>::: versionadded
[New in version 2.0.]{.versionmodified .added}
:::</p>
<p><a href="#std-setting-FILES_STORE">[<code>FILES_STORE</code>{.xref .std .std-setting .docutils .literal
.notranslate}]{.pre}</a>{.hoverxref .tooltip
.reference .internal} and <a href="#std-setting-IMAGES_STORE">[<code>IMAGES_STORE</code>{.xref .std .std-setting
.docutils .literal
.notranslate}]{.pre}</a>{.hoverxref .tooltip
.reference .internal} can point to an FTP server. Scrapy will
automatically upload the files to the server.</p>
<p><a href="#std-setting-FILES_STORE">[<code>FILES_STORE</code>{.xref .std .std-setting .docutils .literal
.notranslate}]{.pre}</a>{.hoverxref .tooltip
.reference .internal} and <a href="#std-setting-IMAGES_STORE">[<code>IMAGES_STORE</code>{.xref .std .std-setting
.docutils .literal
.notranslate}]{.pre}</a>{.hoverxref .tooltip
.reference .internal} should be written in one of the following forms:</p>
<p>::: {.highlight-default .notranslate}
::: highlight
ftp://username:password@address:port/path
ftp://address:port/path
:::
:::</p>
<p>If [<code>username</code>{.docutils .literal .notranslate}]{.pre} and
[<code>password</code>{.docutils .literal .notranslate}]{.pre} are not provided,
they are taken from the <a href="index.html#std-setting-FTP_USER">[<code>FTP_USER</code>{.xref .std .std-setting .docutils
.literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal} and <a href="index.html#std-setting-FTP_PASSWORD">[<code>FTP_PASSWORD</code>{.xref .std
.std-setting .docutils .literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal} settings respectively.</p>
<p>FTP supports two different connection modes: active or passive. Scrapy
uses the passive connection mode by default. To use the active
connection mode instead, set the <a href="index.html#std-setting-FEED_STORAGE_FTP_ACTIVE">[<code>FEED_STORAGE_FTP_ACTIVE</code>{.xref .std
.std-setting .docutils .literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal} setting to [<code>True</code>{.docutils .literal
.notranslate}]{.pre}.
:::</p>
<p>::: {#amazon-s3-storage .section}
[]{#media-pipelines-s3}</p>
<h5 id="amazon-s3-storageheaderlink"><a class="header" href="#amazon-s3-storageheaderlink">Amazon S3 storage<a href="#amazon-s3-storage" title="Permalink to this heading">¶</a>{.headerlink}</a></h5>
<p>[]{#std-setting-FILES_STORE_S3_ACL .target}</p>
<p>If <a href="https://github.com/boto/botocore">botocore</a>{.reference .external}
&gt;= 1.4.87 is installed, <a href="#std-setting-FILES_STORE">[<code>FILES_STORE</code>{.xref .std .std-setting
.docutils .literal
.notranslate}]{.pre}</a>{.hoverxref .tooltip
.reference .internal} and <a href="#std-setting-IMAGES_STORE">[<code>IMAGES_STORE</code>{.xref .std .std-setting
.docutils .literal
.notranslate}]{.pre}</a>{.hoverxref .tooltip
.reference .internal} can represent an Amazon S3 bucket. Scrapy will
automatically upload the files to the bucket.</p>
<p>For example, this is a valid <a href="#std-setting-IMAGES_STORE">[<code>IMAGES_STORE</code>{.xref .std .std-setting
.docutils .literal
.notranslate}]{.pre}</a>{.hoverxref .tooltip
.reference .internal} value:</p>
<p>::: {.highlight-python .notranslate}
::: highlight
IMAGES_STORE = &quot;s3://bucket/images&quot;
:::
:::</p>
<p>You can modify the Access Control List (ACL) policy used for the stored
files, which is defined by the <a href="#std-setting-FILES_STORE_S3_ACL">[<code>FILES_STORE_S3_ACL</code>{.xref .std
.std-setting .docutils .literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal} and <a href="#std-setting-IMAGES_STORE_S3_ACL">[<code>IMAGES_STORE_S3_ACL</code>{.xref .std
.std-setting .docutils .literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal} settings. By default, the ACL is set to
[<code>private</code>{.docutils .literal .notranslate}]{.pre}. To make the files
publicly available use the [<code>public-read</code>{.docutils .literal
.notranslate}]{.pre} policy:</p>
<p>::: {.highlight-python .notranslate}
::: highlight
IMAGES_STORE_S3_ACL = &quot;public-read&quot;
:::
:::</p>
<p>For more information, see <a href="https://docs.aws.amazon.com/AmazonS3/latest/dev/acl-overview.html#canned-acl">canned
ACLs</a>{.reference
.external} in the Amazon S3 Developer Guide.</p>
<p>You can also use other S3-like storages. Storages like self-hosted
<a href="https://github.com/minio/minio">Minio</a>{.reference .external} or
<a href="https://s3.scality.com/">s3.scality</a>{.reference .external}. All you
need to do is set endpoint option in you Scrapy settings:</p>
<p>::: {.highlight-python .notranslate}
::: highlight
AWS_ENDPOINT_URL = &quot;http://minio.example.com:9000&quot;
:::
:::</p>
<p>For self-hosting you also might feel the need not to use SSL and not to
verify SSL connection:</p>
<p>::: {.highlight-python .notranslate}
::: highlight
AWS_USE_SSL = False  # or True (None by default)
AWS_VERIFY = False  # or True (None by default)
:::
:::
:::</p>
<p>::: {#google-cloud-storage .section}
[]{#media-pipeline-gcs}</p>
<h5 id="google-cloud-storageheaderlink"><a class="header" href="#google-cloud-storageheaderlink">Google Cloud Storage<a href="#google-cloud-storage" title="Permalink to this heading">¶</a>{.headerlink}</a></h5>
<p>[]{#std-setting-FILES_STORE_GCS_ACL .target}</p>
<p><a href="#std-setting-FILES_STORE">[<code>FILES_STORE</code>{.xref .std .std-setting .docutils .literal
.notranslate}]{.pre}</a>{.hoverxref .tooltip
.reference .internal} and <a href="#std-setting-IMAGES_STORE">[<code>IMAGES_STORE</code>{.xref .std .std-setting
.docutils .literal
.notranslate}]{.pre}</a>{.hoverxref .tooltip
.reference .internal} can represent a Google Cloud Storage bucket.
Scrapy will automatically upload the files to the bucket. (requires
<a href="https://cloud.google.com/storage/docs/reference/libraries#client-libraries-install-python">google-cloud-storage</a>{.reference
.external} )</p>
<p>For example, these are valid <a href="#std-setting-IMAGES_STORE">[<code>IMAGES_STORE</code>{.xref .std .std-setting
.docutils .literal
.notranslate}]{.pre}</a>{.hoverxref .tooltip
.reference .internal} and <a href="index.html#std-setting-GCS_PROJECT_ID">[<code>GCS_PROJECT_ID</code>{.xref .std .std-setting
.docutils .literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal} settings:</p>
<p>::: {.highlight-python .notranslate}
::: highlight
IMAGES_STORE = &quot;gs://bucket/images/&quot;
GCS_PROJECT_ID = &quot;project_id&quot;
:::
:::</p>
<p>For information about authentication, see this
<a href="https://cloud.google.com/docs/authentication/production">documentation</a>{.reference
.external}.</p>
<p>You can modify the Access Control List (ACL) policy used for the stored
files, which is defined by the <a href="#std-setting-FILES_STORE_GCS_ACL">[<code>FILES_STORE_GCS_ACL</code>{.xref .std
.std-setting .docutils .literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal} and <a href="#std-setting-IMAGES_STORE_GCS_ACL">[<code>IMAGES_STORE_GCS_ACL</code>{.xref .std
.std-setting .docutils .literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal} settings. By default, the ACL is set to
[<code>''</code>{.docutils .literal .notranslate}]{.pre} (empty string) which means
that Cloud Storage applies the bucket's default object ACL to the
object. To make the files publicly available use the
[<code>publicRead</code>{.docutils .literal .notranslate}]{.pre} policy:</p>
<p>::: {.highlight-python .notranslate}
::: highlight
IMAGES_STORE_GCS_ACL = &quot;publicRead&quot;
:::
:::</p>
<p>For more information, see <a href="https://cloud.google.com/storage/docs/access-control/lists#predefined-acl">Predefined
ACLs</a>{.reference
.external} in the Google Cloud Platform Developer Guide.
:::
:::</p>
<p>::: {#usage-example .section}</p>
<h4 id="usage-exampleheaderlink"><a class="header" href="#usage-exampleheaderlink">Usage example<a href="#usage-example" title="Permalink to this heading">¶</a>{.headerlink}</a></h4>
<p>[]{#std-setting-FILES_URLS_FIELD
.target}[]{#std-setting-FILES_RESULT_FIELD
.target}[]{#std-setting-IMAGES_URLS_FIELD .target}</p>
<p>In order to use a media pipeline, first <a href="#topics-media-pipeline-enabling">[enable it]{.std
.std-ref}</a>{.hoverxref .tooltip
.reference .internal}.</p>
<p>Then, if a spider returns an <a href="index.html#topics-items">[item object]{.std
.std-ref}</a>{.hoverxref .tooltip .reference
.internal} with the URLs field ([<code>file_urls</code>{.docutils .literal
.notranslate}]{.pre} or [<code>image_urls</code>{.docutils .literal
.notranslate}]{.pre}, for the Files or Images Pipeline respectively),
the pipeline will put the results under the respective field
([<code>files</code>{.docutils .literal .notranslate}]{.pre} or [<code>images</code>{.docutils
.literal .notranslate}]{.pre}).</p>
<p>When using <a href="index.html#item-types">[item types]{.std
.std-ref}</a>{.hoverxref .tooltip .reference
.internal} for which fields are defined beforehand, you must define both
the URLs field and the results field. For example, when using the images
pipeline, items must define both the [<code>image_urls</code>{.docutils .literal
.notranslate}]{.pre} and the [<code>images</code>{.docutils .literal
.notranslate}]{.pre} field. For instance, using the [<code>Item</code>{.xref .py
.py-class .docutils .literal .notranslate}]{.pre} class:</p>
<p>::: {.highlight-python .notranslate}
::: highlight
import scrapy</p>
<pre><code>class MyItem(scrapy.Item):
    # ... other item fields ...
    image_urls = scrapy.Field()
    images = scrapy.Field()
</code></pre>
<p>:::
:::</p>
<p>If you want to use another field name for the URLs key or for the
results key, it is also possible to override it.</p>
<p>For the Files Pipeline, set <a href="#std-setting-FILES_URLS_FIELD">[<code>FILES_URLS_FIELD</code>{.xref .std .std-setting
.docutils .literal
.notranslate}]{.pre}</a>{.hoverxref .tooltip
.reference .internal} and/or <a href="#std-setting-FILES_RESULT_FIELD">[<code>FILES_RESULT_FIELD</code>{.xref .std
.std-setting .docutils .literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal} settings:</p>
<p>::: {.highlight-python .notranslate}
::: highlight
FILES_URLS_FIELD = &quot;field_name_for_your_files_urls&quot;
FILES_RESULT_FIELD = &quot;field_name_for_your_processed_files&quot;
:::
:::</p>
<p>For the Images Pipeline, set <a href="#std-setting-IMAGES_URLS_FIELD">[<code>IMAGES_URLS_FIELD</code>{.xref .std
.std-setting .docutils .literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal} and/or <a href="#std-setting-IMAGES_RESULT_FIELD">[<code>IMAGES_RESULT_FIELD</code>{.xref .std
.std-setting .docutils .literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal} settings:</p>
<p>::: {.highlight-python .notranslate}
::: highlight
IMAGES_URLS_FIELD = &quot;field_name_for_your_images_urls&quot;
IMAGES_RESULT_FIELD = &quot;field_name_for_your_processed_images&quot;
:::
:::</p>
<p>If you need something more complex and want to override the custom
pipeline behaviour, see <a href="#topics-media-pipeline-override">[Extending the Media Pipelines]{.std
.std-ref}</a>{.hoverxref .tooltip
.reference .internal}.</p>
<p>If you have multiple image pipelines inheriting from ImagePipeline and
you want to have different settings in different pipelines you can set
setting keys preceded with uppercase name of your pipeline class. E.g.
if your pipeline is called MyPipeline and you want to have custom
IMAGES_URLS_FIELD you define setting MYPIPELINE_IMAGES_URLS_FIELD and
your custom settings will be used.
:::</p>
<p>::: {#additional-features .section}</p>
<h4 id="additional-featuresheaderlink"><a class="header" href="#additional-featuresheaderlink">Additional features<a href="#additional-features" title="Permalink to this heading">¶</a>{.headerlink}</a></h4>
<p>::: {#file-expiration .section}
[]{#id2}</p>
<h5 id="file-expirationheaderlink"><a class="header" href="#file-expirationheaderlink">File expiration<a href="#file-expiration" title="Permalink to this heading">¶</a>{.headerlink}</a></h5>
<p>[]{#std-setting-IMAGES_EXPIRES .target}</p>
<p>The Image Pipeline avoids downloading files that were downloaded
recently. To adjust this retention delay use the <a href="#std-setting-FILES_EXPIRES">[<code>FILES_EXPIRES</code>{.xref
.std .std-setting .docutils .literal
.notranslate}]{.pre}</a>{.hoverxref .tooltip
.reference .internal} setting (or <a href="#std-setting-IMAGES_EXPIRES">[<code>IMAGES_EXPIRES</code>{.xref .std
.std-setting .docutils .literal
.notranslate}]{.pre}</a>{.hoverxref .tooltip
.reference .internal}, in case of Images Pipeline), which specifies the
delay in number of days:</p>
<p>::: {.highlight-python .notranslate}
::: highlight
# 120 days of delay for files expiration
FILES_EXPIRES = 120</p>
<pre><code># 30 days of delay for images expiration
IMAGES_EXPIRES = 30
</code></pre>
<p>:::
:::</p>
<p>The default value for both settings is 90 days.</p>
<p>If you have pipeline that subclasses FilesPipeline and you'd like to
have different setting for it you can set setting keys preceded by
uppercase class name. E.g. given pipeline class called MyPipeline you
can set setting key:</p>
<blockquote>
<div>
<p>MYPIPELINE_FILES_EXPIRES = 180</p>
</div>
</blockquote>
<p>and pipeline class MyPipeline will have expiration time set to 180.</p>
<p>The last modified time from the file is used to determine the age of the
file in days, which is then compared to the set expiration time to
determine if the file is expired.
:::</p>
<p>::: {#thumbnail-generation-for-images .section}
[]{#topics-images-thumbnails}</p>
<h5 id="thumbnail-generation-for-imagesheaderlink"><a class="header" href="#thumbnail-generation-for-imagesheaderlink">Thumbnail generation for images<a href="#thumbnail-generation-for-images" title="Permalink to this heading">¶</a>{.headerlink}</a></h5>
<p>The Images Pipeline can automatically create thumbnails of the
downloaded images.</p>
<p>In order to use this feature, you must set <a href="#std-setting-IMAGES_THUMBS">[<code>IMAGES_THUMBS</code>{.xref .std
.std-setting .docutils .literal
.notranslate}]{.pre}</a>{.hoverxref .tooltip
.reference .internal} to a dictionary where the keys are the thumbnail
names and the values are their dimensions.</p>
<p>For example:</p>
<p>::: {.highlight-python .notranslate}
::: highlight
IMAGES_THUMBS = {
&quot;small&quot;: (50, 50),
&quot;big&quot;: (270, 270),
}
:::
:::</p>
<p>When you use this feature, the Images Pipeline will create thumbnails of
the each specified size with this format:</p>
<p>::: {.highlight-default .notranslate}
::: highlight
&lt;IMAGES_STORE&gt;/thumbs/&lt;size_name&gt;/&lt;image_id&gt;.jpg
:::
:::</p>
<p>Where:</p>
<ul>
<li>
<p>[<code>&lt;size_name&gt;</code>{.docutils .literal .notranslate}]{.pre} is the one
specified in the <a href="#std-setting-IMAGES_THUMBS">[<code>IMAGES_THUMBS</code>{.xref .std .std-setting .docutils
.literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal} dictionary keys ([<code>small</code>{.docutils
.literal .notranslate}]{.pre}, [<code>big</code>{.docutils .literal
.notranslate}]{.pre}, etc)</p>
</li>
<li>
<p>[<code>&lt;image_id&gt;</code>{.docutils .literal .notranslate}]{.pre} is the <a href="https://en.wikipedia.org/wiki/SHA_hash_functions">SHA-1
hash</a>{.reference
.external} of the image url</p>
</li>
</ul>
<p>Example of image files stored using [<code>small</code>{.docutils .literal
.notranslate}]{.pre} and [<code>big</code>{.docutils .literal .notranslate}]{.pre}
thumbnail names:</p>
<p>::: {.highlight-default .notranslate}
::: highlight
&lt;IMAGES_STORE&gt;/full/63bbfea82b8880ed33cdb762aa11fab722a90a24.jpg
&lt;IMAGES_STORE&gt;/thumbs/small/63bbfea82b8880ed33cdb762aa11fab722a90a24.jpg
&lt;IMAGES_STORE&gt;/thumbs/big/63bbfea82b8880ed33cdb762aa11fab722a90a24.jpg
:::
:::</p>
<p>The first one is the full image, as downloaded from the site.
:::</p>
<p>::: {#filtering-out-small-images .section}</p>
<h5 id="filtering-out-small-imagesheaderlink"><a class="header" href="#filtering-out-small-imagesheaderlink">Filtering out small images<a href="#filtering-out-small-images" title="Permalink to this heading">¶</a>{.headerlink}</a></h5>
<p>[]{#std-setting-IMAGES_MIN_HEIGHT .target}</p>
<p>When using the Images Pipeline, you can drop images which are too small,
by specifying the minimum allowed size in the
<a href="#std-setting-IMAGES_MIN_HEIGHT">[<code>IMAGES_MIN_HEIGHT</code>{.xref .std .std-setting .docutils .literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal} and <a href="#std-setting-IMAGES_MIN_WIDTH">[<code>IMAGES_MIN_WIDTH</code>{.xref .std
.std-setting .docutils .literal
.notranslate}]{.pre}</a>{.hoverxref .tooltip
.reference .internal} settings.</p>
<p>For example:</p>
<p>::: {.highlight-default .notranslate}
::: highlight
IMAGES_MIN_HEIGHT = 110
IMAGES_MIN_WIDTH = 110
:::
:::</p>
<p>::: {.admonition .note}
Note</p>
<p>The size constraints don't affect thumbnail generation at all.
:::</p>
<p>It is possible to set just one size constraint or both. When setting
both of them, only images that satisfy both minimum sizes will be saved.
For the above example, images of sizes (105 x 105) or (105 x 200) or
(200 x 105) will all be dropped because at least one dimension is
shorter than the constraint.</p>
<p>By default, there are no size constraints, so all images are processed.
:::</p>
<p>::: {#allowing-redirections .section}</p>
<h5 id="allowing-redirectionsheaderlink"><a class="header" href="#allowing-redirectionsheaderlink">Allowing redirections<a href="#allowing-redirections" title="Permalink to this heading">¶</a>{.headerlink}</a></h5>
<p>By default media pipelines ignore redirects, i.e. an HTTP redirection to
a media file URL request will mean the media download is considered
failed.</p>
<p>To handle media redirections, set this setting to [<code>True</code>{.docutils
.literal .notranslate}]{.pre}:</p>
<p>::: {.highlight-default .notranslate}
::: highlight
MEDIA_ALLOW_REDIRECTS = True
:::
:::
:::
:::</p>
<p>::: {#module-scrapy.pipelines.files .section}
[]{#extending-the-media-pipelines}[]{#topics-media-pipeline-override}</p>
<h4 id="extending-the-media-pipelinesheaderlink"><a class="header" href="#extending-the-media-pipelinesheaderlink">Extending the Media Pipelines<a href="#module-scrapy.pipelines.files" title="Permalink to this heading">¶</a>{.headerlink}</a></h4>
<p>See here the methods that you can override in your custom Files
Pipeline:</p>
<p><em>[class]{.pre}[ ]{.w}</em>[[scrapy.pipelines.files.]{.pre}]{.sig-prename .descclassname}[[FilesPipeline]{.pre}]{.sig-name .descname}<a href="_modules/scrapy/pipelines/files.html#FilesPipeline">[[[source]]{.pre}]{.viewcode-link}</a>{.reference .internal}<a href="#scrapy.pipelines.files.FilesPipeline" title="Permalink to this definition">¶</a>{.headerlink}</p>
<p>:</p>
<pre><code>[[file_path]{.pre}]{.sig-name .descname}[(]{.sig-paren}*[[self]{.pre}]{.n}*, *[[request]{.pre}]{.n}*, *[[response]{.pre}]{.n}[[=]{.pre}]{.o}[[None]{.pre}]{.default_value}*, *[[info]{.pre}]{.n}[[=]{.pre}]{.o}[[None]{.pre}]{.default_value}*, *[[\*]{.pre}]{.o}*, *[[item]{.pre}]{.n}[[=]{.pre}]{.o}[[None]{.pre}]{.default_value}*[)]{.sig-paren}[[[\[source\]]{.pre}]{.viewcode-link}](_modules/scrapy/pipelines/files.html#FilesPipeline.file_path){.reference .internal}[¶](#scrapy.pipelines.files.FilesPipeline.file_path &quot;Permalink to this definition&quot;){.headerlink}

:   This method is called once per downloaded item. It returns the
    download path of the file originating from the specified
    [[`response`{.xref .py .py-class .docutils .literal
    .notranslate}]{.pre}](index.html#scrapy.http.Response &quot;scrapy.http.Response&quot;){.reference
    .internal}.

    In addition to [`response`{.docutils .literal
    .notranslate}]{.pre}, this method receives the original
    [`request`{.xref .py .py-class .docutils .literal
    .notranslate}]{.pre}, [`info`{.xref .py .py-class .docutils
    .literal .notranslate}]{.pre} and [`item`{.xref .py .py-class
    .docutils .literal .notranslate}]{.pre}

    You can override this method to customize the download path of
    each file.

    For example, if file URLs end like regular paths (e.g.
    [`https://example.com/a/b/c/foo.png`{.docutils .literal
    .notranslate}]{.pre}), you can use the following approach to
    download all files into the [`files`{.docutils .literal
    .notranslate}]{.pre} folder with their original filenames (e.g.
    [`files/foo.png`{.docutils .literal .notranslate}]{.pre}):

    ::: {.highlight-python .notranslate}
    ::: highlight
        from pathlib import PurePosixPath
        from urllib.parse import urlparse

        from scrapy.pipelines.files import FilesPipeline


        class MyFilesPipeline(FilesPipeline):
            def file_path(self, request, response=None, info=None, *, item=None):
                return &quot;files/&quot; + PurePosixPath(urlparse(request.url).path).name
    :::
    :::

    Similarly, you can use the [`item`{.docutils .literal
    .notranslate}]{.pre} to determine the file path based on some
    item property.

    By default the [[`file_path()`{.xref .py .py-meth .docutils
    .literal
    .notranslate}]{.pre}](#scrapy.pipelines.files.FilesPipeline.file_path &quot;scrapy.pipelines.files.FilesPipeline.file_path&quot;){.reference
    .internal} method returns [`full/&lt;request`{.docutils .literal
    .notranslate}]{.pre}` `{.docutils .literal
    .notranslate}[`URL`{.docutils .literal
    .notranslate}]{.pre}` `{.docutils .literal
    .notranslate}[`hash&gt;.&lt;extension&gt;`{.docutils .literal
    .notranslate}]{.pre}.

    ::: versionadded
    [New in version 2.4: ]{.versionmodified .added}The *item*
    parameter.
    :::

[[get_media_requests]{.pre}]{.sig-name .descname}[(]{.sig-paren}*[[item]{.pre}]{.n}*, *[[info]{.pre}]{.n}*[)]{.sig-paren}[[[\[source\]]{.pre}]{.viewcode-link}](_modules/scrapy/pipelines/files.html#FilesPipeline.get_media_requests){.reference .internal}[¶](#scrapy.pipelines.files.FilesPipeline.get_media_requests &quot;Permalink to this definition&quot;){.headerlink}

:   As seen on the workflow, the pipeline will get the URLs of the
    images to download from the item. In order to do this, you can
    override the [[`get_media_requests()`{.xref .py .py-meth
    .docutils .literal
    .notranslate}]{.pre}](#scrapy.pipelines.files.FilesPipeline.get_media_requests &quot;scrapy.pipelines.files.FilesPipeline.get_media_requests&quot;){.reference
    .internal} method and return a Request for each file URL:

    ::: {.highlight-python .notranslate}
    ::: highlight
        from itemadapter import ItemAdapter


        def get_media_requests(self, item, info):
            adapter = ItemAdapter(item)
            for file_url in adapter[&quot;file_urls&quot;]:
                yield scrapy.Request(file_url)
    :::
    :::

    Those requests will be processed by the pipeline and, when they
    have finished downloading, the results will be sent to the
    [[`item_completed()`{.xref .py .py-meth .docutils .literal
    .notranslate}]{.pre}](#scrapy.pipelines.files.FilesPipeline.item_completed &quot;scrapy.pipelines.files.FilesPipeline.item_completed&quot;){.reference
    .internal} method, as a list of 2-element tuples. Each tuple
    will contain [`(success,`{.docutils .literal
    .notranslate}]{.pre}` `{.docutils .literal
    .notranslate}[`file_info_or_error)`{.docutils .literal
    .notranslate}]{.pre} where:

    -   [`success`{.docutils .literal .notranslate}]{.pre} is a
        boolean which is [`True`{.docutils .literal
        .notranslate}]{.pre} if the image was downloaded
        successfully or [`False`{.docutils .literal
        .notranslate}]{.pre} if it failed for some reason

    -   [`file_info_or_error`{.docutils .literal
        .notranslate}]{.pre} is a dict containing the following keys
        (if success is [`True`{.docutils .literal
        .notranslate}]{.pre}) or a [[`Failure`{.xref .py .py-exc
        .docutils .literal
        .notranslate}]{.pre}](https://docs.twisted.org/en/stable/api/twisted.python.failure.Failure.html &quot;(in Twisted)&quot;){.reference
        .external} if there was a problem.

        -   [`url`{.docutils .literal .notranslate}]{.pre} - the url
            where the file was downloaded from. This is the url of
            the request returned from the
            [[`get_media_requests()`{.xref .py .py-meth .docutils
            .literal
            .notranslate}]{.pre}](#scrapy.pipelines.files.FilesPipeline.get_media_requests &quot;scrapy.pipelines.files.FilesPipeline.get_media_requests&quot;){.reference
            .internal} method.

        -   [`path`{.docutils .literal .notranslate}]{.pre} - the
            path (relative to [[`FILES_STORE`{.xref .std
            .std-setting .docutils .literal
            .notranslate}]{.pre}](#std-setting-FILES_STORE){.hoverxref
            .tooltip .reference .internal}) where the file was
            stored

        -   [`checksum`{.docutils .literal .notranslate}]{.pre} - a
            [MD5 hash](https://en.wikipedia.org/wiki/MD5){.reference
            .external} of the image contents

        -   [`status`{.docutils .literal .notranslate}]{.pre} - the
            file status indication.

            ::: versionadded
            [New in version 2.2.]{.versionmodified .added}
            :::

            It can be one of the following:

            -   [`downloaded`{.docutils .literal
                .notranslate}]{.pre} - file was downloaded.

            -   [`uptodate`{.docutils .literal
                .notranslate}]{.pre} - file was not downloaded, as
                it was downloaded recently, according to the file
                expiration policy.

            -   [`cached`{.docutils .literal .notranslate}]{.pre} -
                file was already scheduled for download, by another
                item sharing the same file.

    The list of tuples received by [[`item_completed()`{.xref .py
    .py-meth .docutils .literal
    .notranslate}]{.pre}](#scrapy.pipelines.files.FilesPipeline.item_completed &quot;scrapy.pipelines.files.FilesPipeline.item_completed&quot;){.reference
    .internal} is guaranteed to retain the same order of the
    requests returned from the [[`get_media_requests()`{.xref .py
    .py-meth .docutils .literal
    .notranslate}]{.pre}](#scrapy.pipelines.files.FilesPipeline.get_media_requests &quot;scrapy.pipelines.files.FilesPipeline.get_media_requests&quot;){.reference
    .internal} method.

    Here's a typical value of the [`results`{.docutils .literal
    .notranslate}]{.pre} argument:

    ::: {.highlight-python .notranslate}
    ::: highlight
        [
            (
                True,
                {
                    &quot;checksum&quot;: &quot;2b00042f7481c7b056c4b410d28f33cf&quot;,
                    &quot;path&quot;: &quot;full/0a79c461a4062ac383dc4fade7bc09f1384a3910.jpg&quot;,
                    &quot;url&quot;: &quot;http://www.example.com/files/product1.pdf&quot;,
                    &quot;status&quot;: &quot;downloaded&quot;,
                },
            ),
            (False, Failure(...)),
        ]
    :::
    :::

    By default the [[`get_media_requests()`{.xref .py .py-meth
    .docutils .literal
    .notranslate}]{.pre}](#scrapy.pipelines.files.FilesPipeline.get_media_requests &quot;scrapy.pipelines.files.FilesPipeline.get_media_requests&quot;){.reference
    .internal} method returns [`None`{.docutils .literal
    .notranslate}]{.pre} which means there are no files to download
    for the item.

[[item_completed]{.pre}]{.sig-name .descname}[(]{.sig-paren}*[[results]{.pre}]{.n}*, *[[item]{.pre}]{.n}*, *[[info]{.pre}]{.n}*[)]{.sig-paren}[[[\[source\]]{.pre}]{.viewcode-link}](_modules/scrapy/pipelines/files.html#FilesPipeline.item_completed){.reference .internal}[¶](#scrapy.pipelines.files.FilesPipeline.item_completed &quot;Permalink to this definition&quot;){.headerlink}

:   The [[`FilesPipeline.item_completed()`{.xref .py .py-meth
    .docutils .literal
    .notranslate}]{.pre}](#scrapy.pipelines.files.FilesPipeline.item_completed &quot;scrapy.pipelines.files.FilesPipeline.item_completed&quot;){.reference
    .internal} method called when all file requests for a single
    item have completed (either finished downloading, or failed for
    some reason).

    The [[`item_completed()`{.xref .py .py-meth .docutils .literal
    .notranslate}]{.pre}](#scrapy.pipelines.files.FilesPipeline.item_completed &quot;scrapy.pipelines.files.FilesPipeline.item_completed&quot;){.reference
    .internal} method must return the output that will be sent to
    subsequent item pipeline stages, so you must return (or drop)
    the item, as you would in any pipeline.

    Here is an example of the [[`item_completed()`{.xref .py
    .py-meth .docutils .literal
    .notranslate}]{.pre}](#scrapy.pipelines.files.FilesPipeline.item_completed &quot;scrapy.pipelines.files.FilesPipeline.item_completed&quot;){.reference
    .internal} method where we store the downloaded file paths
    (passed in results) in the [`file_paths`{.docutils .literal
    .notranslate}]{.pre} item field, and we drop the item if it
    doesn't contain any files:

    ::: {.highlight-python .notranslate}
    ::: highlight
        from itemadapter import ItemAdapter
        from scrapy.exceptions import DropItem


        def item_completed(self, results, item, info):
            file_paths = [x[&quot;path&quot;] for ok, x in results if ok]
            if not file_paths:
                raise DropItem(&quot;Item contains no files&quot;)
            adapter = ItemAdapter(item)
            adapter[&quot;file_paths&quot;] = file_paths
            return item
    :::
    :::

    By default, the [[`item_completed()`{.xref .py .py-meth
    .docutils .literal
    .notranslate}]{.pre}](#scrapy.pipelines.files.FilesPipeline.item_completed &quot;scrapy.pipelines.files.FilesPipeline.item_completed&quot;){.reference
    .internal} method returns the item.
</code></pre>
<p>[]{#module-scrapy.pipelines.images .target}</p>
<p>See here the methods that you can override in your custom Images
Pipeline:</p>
<p><em>[class]{.pre}[ ]{.w}</em>[[scrapy.pipelines.images.]{.pre}]{.sig-prename .descclassname}[[ImagesPipeline]{.pre}]{.sig-name .descname}<a href="_modules/scrapy/pipelines/images.html#ImagesPipeline">[[[source]]{.pre}]{.viewcode-link}</a>{.reference .internal}<a href="#scrapy.pipelines.images.ImagesPipeline" title="Permalink to this definition">¶</a>{.headerlink}</p>
<p>:   <div>
&gt;
&gt; The <a href="#scrapy.pipelines.images.ImagesPipeline" title="scrapy.pipelines.images.ImagesPipeline">[<code>ImagesPipeline</code>{.xref .py .py-class .docutils .literal
&gt; .notranslate}]{.pre}</a>{.reference
&gt; .internal} is an extension of the [<code>FilesPipeline</code>{.xref .py
&gt; .py-class .docutils .literal .notranslate}]{.pre}, customizing the
&gt; field names and adding custom behavior for images.
&gt;
&gt; </div></p>
<pre><code>[[file_path]{.pre}]{.sig-name .descname}[(]{.sig-paren}*[[self]{.pre}]{.n}*, *[[request]{.pre}]{.n}*, *[[response]{.pre}]{.n}[[=]{.pre}]{.o}[[None]{.pre}]{.default_value}*, *[[info]{.pre}]{.n}[[=]{.pre}]{.o}[[None]{.pre}]{.default_value}*, *[[\*]{.pre}]{.o}*, *[[item]{.pre}]{.n}[[=]{.pre}]{.o}[[None]{.pre}]{.default_value}*[)]{.sig-paren}[[[\[source\]]{.pre}]{.viewcode-link}](_modules/scrapy/pipelines/images.html#ImagesPipeline.file_path){.reference .internal}[¶](#scrapy.pipelines.images.ImagesPipeline.file_path &quot;Permalink to this definition&quot;){.headerlink}

:   This method is called once per downloaded item. It returns the
    download path of the file originating from the specified
    [[`response`{.xref .py .py-class .docutils .literal
    .notranslate}]{.pre}](index.html#scrapy.http.Response &quot;scrapy.http.Response&quot;){.reference
    .internal}.

    In addition to [`response`{.docutils .literal
    .notranslate}]{.pre}, this method receives the original
    [`request`{.xref .py .py-class .docutils .literal
    .notranslate}]{.pre}, [`info`{.xref .py .py-class .docutils
    .literal .notranslate}]{.pre} and [`item`{.xref .py .py-class
    .docutils .literal .notranslate}]{.pre}

    You can override this method to customize the download path of
    each file.

    For example, if file URLs end like regular paths (e.g.
    [`https://example.com/a/b/c/foo.png`{.docutils .literal
    .notranslate}]{.pre}), you can use the following approach to
    download all files into the [`files`{.docutils .literal
    .notranslate}]{.pre} folder with their original filenames (e.g.
    [`files/foo.png`{.docutils .literal .notranslate}]{.pre}):

    ::: {.highlight-python .notranslate}
    ::: highlight
        from pathlib import PurePosixPath
        from urllib.parse import urlparse

        from scrapy.pipelines.images import ImagesPipeline


        class MyImagesPipeline(ImagesPipeline):
            def file_path(self, request, response=None, info=None, *, item=None):
                return &quot;files/&quot; + PurePosixPath(urlparse(request.url).path).name
    :::
    :::

    Similarly, you can use the [`item`{.docutils .literal
    .notranslate}]{.pre} to determine the file path based on some
    item property.

    By default the [[`file_path()`{.xref .py .py-meth .docutils
    .literal
    .notranslate}]{.pre}](#scrapy.pipelines.images.ImagesPipeline.file_path &quot;scrapy.pipelines.images.ImagesPipeline.file_path&quot;){.reference
    .internal} method returns [`full/&lt;request`{.docutils .literal
    .notranslate}]{.pre}` `{.docutils .literal
    .notranslate}[`URL`{.docutils .literal
    .notranslate}]{.pre}` `{.docutils .literal
    .notranslate}[`hash&gt;.&lt;extension&gt;`{.docutils .literal
    .notranslate}]{.pre}.

    ::: versionadded
    [New in version 2.4: ]{.versionmodified .added}The *item*
    parameter.
    :::

[[thumb_path]{.pre}]{.sig-name .descname}[(]{.sig-paren}*[[self]{.pre}]{.n}*, *[[request]{.pre}]{.n}*, *[[thumb_id]{.pre}]{.n}*, *[[response]{.pre}]{.n}[[=]{.pre}]{.o}[[None]{.pre}]{.default_value}*, *[[info]{.pre}]{.n}[[=]{.pre}]{.o}[[None]{.pre}]{.default_value}*, *[[\*]{.pre}]{.o}*, *[[item]{.pre}]{.n}[[=]{.pre}]{.o}[[None]{.pre}]{.default_value}*[)]{.sig-paren}[[[\[source\]]{.pre}]{.viewcode-link}](_modules/scrapy/pipelines/images.html#ImagesPipeline.thumb_path){.reference .internal}[¶](#scrapy.pipelines.images.ImagesPipeline.thumb_path &quot;Permalink to this definition&quot;){.headerlink}

:   This method is called for every item of [[`IMAGES_THUMBS`{.xref
    .std .std-setting .docutils .literal
    .notranslate}]{.pre}](#std-setting-IMAGES_THUMBS){.hoverxref
    .tooltip .reference .internal} per downloaded item. It returns
    the thumbnail download path of the image originating from the
    specified [[`response`{.xref .py .py-class .docutils .literal
    .notranslate}]{.pre}](index.html#scrapy.http.Response &quot;scrapy.http.Response&quot;){.reference
    .internal}.

    In addition to [`response`{.docutils .literal
    .notranslate}]{.pre}, this method receives the original
    [`request`{.xref .py .py-class .docutils .literal
    .notranslate}]{.pre}, [`thumb_id`{.docutils .literal
    .notranslate}]{.pre}, [`info`{.xref .py .py-class .docutils
    .literal .notranslate}]{.pre} and [`item`{.xref .py .py-class
    .docutils .literal .notranslate}]{.pre}.

    You can override this method to customize the thumbnail download
    path of each image. You can use the [`item`{.docutils .literal
    .notranslate}]{.pre} to determine the file path based on some
    item property.

    By default the [[`thumb_path()`{.xref .py .py-meth .docutils
    .literal
    .notranslate}]{.pre}](#scrapy.pipelines.images.ImagesPipeline.thumb_path &quot;scrapy.pipelines.images.ImagesPipeline.thumb_path&quot;){.reference
    .internal} method returns [`thumbs/&lt;size`{.docutils .literal
    .notranslate}]{.pre}` `{.docutils .literal
    .notranslate}[`name&gt;/&lt;request`{.docutils .literal
    .notranslate}]{.pre}` `{.docutils .literal
    .notranslate}[`URL`{.docutils .literal
    .notranslate}]{.pre}` `{.docutils .literal
    .notranslate}[`hash&gt;.&lt;extension&gt;`{.docutils .literal
    .notranslate}]{.pre}.

[[get_media_requests]{.pre}]{.sig-name .descname}[(]{.sig-paren}*[[item]{.pre}]{.n}*, *[[info]{.pre}]{.n}*[)]{.sig-paren}[[[\[source\]]{.pre}]{.viewcode-link}](_modules/scrapy/pipelines/images.html#ImagesPipeline.get_media_requests){.reference .internal}[¶](#scrapy.pipelines.images.ImagesPipeline.get_media_requests &quot;Permalink to this definition&quot;){.headerlink}

:   Works the same way as
    [`FilesPipeline.get_media_requests()`{.xref .py .py-meth
    .docutils .literal .notranslate}]{.pre} method, but using a
    different field name for image urls.

    Must return a Request for each image URL.

[[item_completed]{.pre}]{.sig-name .descname}[(]{.sig-paren}*[[results]{.pre}]{.n}*, *[[item]{.pre}]{.n}*, *[[info]{.pre}]{.n}*[)]{.sig-paren}[[[\[source\]]{.pre}]{.viewcode-link}](_modules/scrapy/pipelines/images.html#ImagesPipeline.item_completed){.reference .internal}[¶](#scrapy.pipelines.images.ImagesPipeline.item_completed &quot;Permalink to this definition&quot;){.headerlink}

:   The [[`ImagesPipeline.item_completed()`{.xref .py .py-meth
    .docutils .literal
    .notranslate}]{.pre}](#scrapy.pipelines.images.ImagesPipeline.item_completed &quot;scrapy.pipelines.images.ImagesPipeline.item_completed&quot;){.reference
    .internal} method is called when all image requests for a single
    item have completed (either finished downloading, or failed for
    some reason).

    Works the same way as [`FilesPipeline.item_completed()`{.xref
    .py .py-meth .docutils .literal .notranslate}]{.pre} method, but
    using a different field names for storing image downloading
    results.

    By default, the [[`item_completed()`{.xref .py .py-meth
    .docutils .literal
    .notranslate}]{.pre}](#scrapy.pipelines.images.ImagesPipeline.item_completed &quot;scrapy.pipelines.images.ImagesPipeline.item_completed&quot;){.reference
    .internal} method returns the item.
</code></pre>
<p>:::</p>
<p>::: {#custom-images-pipeline-example .section}
[]{#media-pipeline-example}</p>
<h4 id="custom-images-pipeline-exampleheaderlink"><a class="header" href="#custom-images-pipeline-exampleheaderlink">Custom Images pipeline example<a href="#custom-images-pipeline-example" title="Permalink to this heading">¶</a>{.headerlink}</a></h4>
<p>Here is a full example of the Images Pipeline whose methods are
exemplified above:</p>
<p>::: {.highlight-python .notranslate}
::: highlight
import scrapy
from itemadapter import ItemAdapter
from scrapy.exceptions import DropItem
from scrapy.pipelines.images import ImagesPipeline</p>
<pre><code>class MyImagesPipeline(ImagesPipeline):
    def get_media_requests(self, item, info):
        for image_url in item[&quot;image_urls&quot;]:
            yield scrapy.Request(image_url)

    def item_completed(self, results, item, info):
        image_paths = [x[&quot;path&quot;] for ok, x in results if ok]
        if not image_paths:
            raise DropItem(&quot;Item contains no images&quot;)
        adapter = ItemAdapter(item)
        adapter[&quot;image_paths&quot;] = image_paths
        return item
</code></pre>
<p>:::
:::</p>
<p>To enable your custom media pipeline component you must add its class
import path to the <a href="index.html#std-setting-ITEM_PIPELINES">[<code>ITEM_PIPELINES</code>{.xref .std .std-setting .docutils
.literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal} setting, like in the following example:</p>
<p>::: {.highlight-python .notranslate}
::: highlight
ITEM_PIPELINES = {&quot;myproject.pipelines.MyImagesPipeline&quot;: 300}
:::
:::
:::
:::</p>
<p>[]{#document-topics/deploy}</p>
<p>::: {#deploying-spiders .section}
[]{#topics-deploy}</p>
<h3 id="deploying-spidersheaderlink"><a class="header" href="#deploying-spidersheaderlink">Deploying Spiders<a href="#deploying-spiders" title="Permalink to this heading">¶</a>{.headerlink}</a></h3>
<p>This section describes the different options you have for deploying your
Scrapy spiders to run them on a regular basis. Running Scrapy spiders in
your local machine is very convenient for the (early) development stage,
but not so much when you need to execute long-running spiders or move
spiders to run in production continuously. This is where the solutions
for deploying Scrapy spiders come in.</p>
<p>Popular choices for deploying Scrapy spiders are:</p>
<ul>
<li>
<p><a href="#deploy-scrapyd">[Scrapyd]{.std .std-ref}</a>{.hoverxref .tooltip
.reference .internal} (open source)</p>
</li>
<li>
<p><a href="#deploy-scrapy-cloud">[Zyte Scrapy Cloud]{.std
.std-ref}</a>{.hoverxref .tooltip .reference
.internal} (cloud-based)</p>
</li>
</ul>
<p>::: {#deploying-to-a-scrapyd-server .section}
[]{#deploy-scrapyd}</p>
<h4 id="deploying-to-a-scrapyd-serverheaderlink"><a class="header" href="#deploying-to-a-scrapyd-serverheaderlink">Deploying to a Scrapyd Server<a href="#deploying-to-a-scrapyd-server" title="Permalink to this heading">¶</a>{.headerlink}</a></h4>
<p><a href="https://github.com/scrapy/scrapyd">Scrapyd</a>{.reference .external} is an
open source application to run Scrapy spiders. It provides a server with
HTTP API, capable of running and monitoring Scrapy spiders.</p>
<p>To deploy spiders to Scrapyd, you can use the scrapyd-deploy tool
provided by the
<a href="https://github.com/scrapy/scrapyd-client">scrapyd-client</a>{.reference
.external} package. Please refer to the <a href="https://scrapyd.readthedocs.io/en/latest/deploy.html">scrapyd-deploy
documentation</a>{.reference
.external} for more information.</p>
<p>Scrapyd is maintained by some of the Scrapy developers.
:::</p>
<p>::: {#deploying-to-zyte-scrapy-cloud .section}
[]{#deploy-scrapy-cloud}</p>
<h4 id="deploying-to-zyte-scrapy-cloudheaderlink"><a class="header" href="#deploying-to-zyte-scrapy-cloudheaderlink">Deploying to Zyte Scrapy Cloud<a href="#deploying-to-zyte-scrapy-cloud" title="Permalink to this heading">¶</a>{.headerlink}</a></h4>
<p><a href="https://www.zyte.com/scrapy-cloud/">Zyte Scrapy Cloud</a>{.reference
.external} is a hosted, cloud-based service by
<a href="https://zyte.com/">Zyte</a>{.reference .external}, the company behind
Scrapy.</p>
<p>Zyte Scrapy Cloud removes the need to setup and monitor servers and
provides a nice UI to manage spiders and review scraped items, logs and
stats.</p>
<p>To deploy spiders to Zyte Scrapy Cloud you can use the
<a href="https://shub.readthedocs.io/en/latest/">shub</a>{.reference .external}
command line tool. Please refer to the <a href="https://docs.zyte.com/scrapy-cloud.html">Zyte Scrapy Cloud
documentation</a>{.reference
.external} for more information.</p>
<p>Zyte Scrapy Cloud is compatible with Scrapyd and one can switch between
them as needed - the configuration is read from the
[<code>scrapy.cfg</code>{.docutils .literal .notranslate}]{.pre} file just like
[<code>scrapyd-deploy</code>{.docutils .literal .notranslate}]{.pre}.
:::
:::</p>
<p>[]{#document-topics/autothrottle}</p>
<p>::: {#autothrottle-extension .section}
[]{#topics-autothrottle}</p>
<h3 id="autothrottle-extensionheaderlink"><a class="header" href="#autothrottle-extensionheaderlink">AutoThrottle extension<a href="#autothrottle-extension" title="Permalink to this heading">¶</a>{.headerlink}</a></h3>
<p>This is an extension for automatically throttling crawling speed based
on load of both the Scrapy server and the website you are crawling.</p>
<p>::: {#design-goals .section}</p>
<h4 id="design-goalsheaderlink"><a class="header" href="#design-goalsheaderlink">Design goals<a href="#design-goals" title="Permalink to this heading">¶</a>{.headerlink}</a></h4>
<ol>
<li>
<p>be nicer to sites instead of using default download delay of zero</p>
</li>
<li>
<p>automatically adjust Scrapy to the optimum crawling speed, so the
user doesn't have to tune the download delays to find the optimum
one. The user only needs to specify the maximum concurrent requests
it allows, and the extension does the rest.
:::</p>
</li>
</ol>
<p>::: {#how-it-works .section}
[]{#autothrottle-algorithm}</p>
<h4 id="how-it-worksheaderlink"><a class="header" href="#how-it-worksheaderlink">How it works<a href="#how-it-works" title="Permalink to this heading">¶</a>{.headerlink}</a></h4>
<p>AutoThrottle extension adjusts download delays dynamically to make
spider send <a href="#std-setting-AUTOTHROTTLE_TARGET_CONCURRENCY">[<code>AUTOTHROTTLE_TARGET_CONCURRENCY</code>{.xref .std .std-setting
.docutils .literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal} concurrent requests on average to each
remote website.</p>
<p>It uses download latency to compute the delays. The main idea is the
following: if a server needs [<code>latency</code>{.docutils .literal
.notranslate}]{.pre} seconds to respond, a client should send a request
each [<code>latency/N</code>{.docutils .literal .notranslate}]{.pre} seconds to
have [<code>N</code>{.docutils .literal .notranslate}]{.pre} requests processed in
parallel.</p>
<p>Instead of adjusting the delays one can just set a small fixed download
delay and impose hard limits on concurrency using
<a href="index.html#std-setting-CONCURRENT_REQUESTS_PER_DOMAIN">[<code>CONCURRENT_REQUESTS_PER_DOMAIN</code>{.xref .std .std-setting .docutils
.literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal} or <a href="index.html#std-setting-CONCURRENT_REQUESTS_PER_IP">[<code>CONCURRENT_REQUESTS_PER_IP</code>{.xref
.std .std-setting .docutils .literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal} options. It will provide a similar
effect, but there are some important differences:</p>
<ul>
<li>
<p>because the download delay is small there will be occasional bursts
of requests;</p>
</li>
<li>
<p>often non-200 (error) responses can be returned faster than regular
responses, so with a small download delay and a hard concurrency
limit crawler will be sending requests to server faster when server
starts to return errors. But this is an opposite of what crawler
should do - in case of errors it makes more sense to slow down:
these errors may be caused by the high request rate.</p>
</li>
</ul>
<p>AutoThrottle doesn't have these issues.
:::</p>
<p>::: {#throttling-algorithm .section}</p>
<h4 id="throttling-algorithmheaderlink"><a class="header" href="#throttling-algorithmheaderlink">Throttling algorithm<a href="#throttling-algorithm" title="Permalink to this heading">¶</a>{.headerlink}</a></h4>
<p>AutoThrottle algorithm adjusts download delays based on the following
rules:</p>
<ol>
<li>
<p>spiders always start with a download delay of
<a href="#std-setting-AUTOTHROTTLE_START_DELAY">[<code>AUTOTHROTTLE_START_DELAY</code>{.xref .std .std-setting .docutils
.literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal};</p>
</li>
<li>
<p>when a response is received, the target download delay is calculated
as [<code>latency</code>{.docutils .literal .notranslate}]{.pre}<code> </code>{.docutils
.literal .notranslate}[<code>/</code>{.docutils .literal
.notranslate}]{.pre}<code> </code>{.docutils .literal
.notranslate}[<code>N</code>{.docutils .literal .notranslate}]{.pre} where
[<code>latency</code>{.docutils .literal .notranslate}]{.pre} is a latency of
the response, and [<code>N</code>{.docutils .literal .notranslate}]{.pre} is
<a href="#std-setting-AUTOTHROTTLE_TARGET_CONCURRENCY">[<code>AUTOTHROTTLE_TARGET_CONCURRENCY</code>{.xref .std .std-setting
.docutils .literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal}.</p>
</li>
<li>
<p>download delay for next requests is set to the average of previous
download delay and the target download delay;</p>
</li>
<li>
<p>latencies of non-200 responses are not allowed to decrease the
delay;</p>
</li>
<li>
<p>download delay can't become less than <a href="index.html#std-setting-DOWNLOAD_DELAY">[<code>DOWNLOAD_DELAY</code>{.xref .std
.std-setting .docutils .literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal} or greater than
<a href="#std-setting-AUTOTHROTTLE_MAX_DELAY">[<code>AUTOTHROTTLE_MAX_DELAY</code>{.xref .std .std-setting .docutils
.literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal}</p>
</li>
</ol>
<p>::: {.admonition .note}
Note</p>
<p>The AutoThrottle extension honours the standard Scrapy settings for
concurrency and delay. This means that it will respect
<a href="index.html#std-setting-CONCURRENT_REQUESTS_PER_DOMAIN">[<code>CONCURRENT_REQUESTS_PER_DOMAIN</code>{.xref .std .std-setting .docutils
.literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal} and <a href="index.html#std-setting-CONCURRENT_REQUESTS_PER_IP">[<code>CONCURRENT_REQUESTS_PER_IP</code>{.xref
.std .std-setting .docutils .literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal} options and never set a download delay
lower than <a href="index.html#std-setting-DOWNLOAD_DELAY">[<code>DOWNLOAD_DELAY</code>{.xref .std .std-setting .docutils .literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal}.
:::</p>
<p>In Scrapy, the download latency is measured as the time elapsed between
establishing the TCP connection and receiving the HTTP headers.</p>
<p>Note that these latencies are very hard to measure accurately in a
cooperative multitasking environment because Scrapy may be busy
processing a spider callback, for example, and unable to attend
downloads. However, these latencies should still give a reasonable
estimate of how busy Scrapy (and ultimately, the server) is, and this
extension builds on that premise.
:::</p>
<p>::: {#settings .section}</p>
<h4 id="settingsheaderlink-3"><a class="header" href="#settingsheaderlink-3">Settings<a href="#settings" title="Permalink to this heading">¶</a>{.headerlink}</a></h4>
<p>The settings used to control the AutoThrottle extension are:</p>
<ul>
<li>
<p><a href="#std-setting-AUTOTHROTTLE_ENABLED">[<code>AUTOTHROTTLE_ENABLED</code>{.xref .std .std-setting .docutils .literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal}</p>
</li>
<li>
<p><a href="#std-setting-AUTOTHROTTLE_START_DELAY">[<code>AUTOTHROTTLE_START_DELAY</code>{.xref .std .std-setting .docutils
.literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal}</p>
</li>
<li>
<p><a href="#std-setting-AUTOTHROTTLE_MAX_DELAY">[<code>AUTOTHROTTLE_MAX_DELAY</code>{.xref .std .std-setting .docutils
.literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal}</p>
</li>
<li>
<p><a href="#std-setting-AUTOTHROTTLE_TARGET_CONCURRENCY">[<code>AUTOTHROTTLE_TARGET_CONCURRENCY</code>{.xref .std .std-setting
.docutils .literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal}</p>
</li>
<li>
<p><a href="#std-setting-AUTOTHROTTLE_DEBUG">[<code>AUTOTHROTTLE_DEBUG</code>{.xref .std .std-setting .docutils .literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal}</p>
</li>
<li>
<p><a href="index.html#std-setting-CONCURRENT_REQUESTS_PER_DOMAIN">[<code>CONCURRENT_REQUESTS_PER_DOMAIN</code>{.xref .std .std-setting .docutils
.literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal}</p>
</li>
<li>
<p><a href="index.html#std-setting-CONCURRENT_REQUESTS_PER_IP">[<code>CONCURRENT_REQUESTS_PER_IP</code>{.xref .std .std-setting .docutils
.literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal}</p>
</li>
<li>
<p><a href="index.html#std-setting-DOWNLOAD_DELAY">[<code>DOWNLOAD_DELAY</code>{.xref .std .std-setting .docutils .literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal}</p>
</li>
</ul>
<p>For more information see <a href="#autothrottle-algorithm">[How it works]{.std
.std-ref}</a>{.hoverxref .tooltip .reference
.internal}.</p>
<p>::: {#autothrottle-enabled .section}
[]{#std-setting-AUTOTHROTTLE_ENABLED}</p>
<h5 id="autothrottle_enabledheaderlink"><a class="header" href="#autothrottle_enabledheaderlink">AUTOTHROTTLE_ENABLED<a href="#autothrottle-enabled" title="Permalink to this heading">¶</a>{.headerlink}</a></h5>
<p>Default: [<code>False</code>{.docutils .literal .notranslate}]{.pre}</p>
<p>Enables the AutoThrottle extension.
:::</p>
<p>::: {#autothrottle-start-delay .section}
[]{#std-setting-AUTOTHROTTLE_START_DELAY}</p>
<h5 id="autothrottle_start_delayheaderlink"><a class="header" href="#autothrottle_start_delayheaderlink">AUTOTHROTTLE_START_DELAY<a href="#autothrottle-start-delay" title="Permalink to this heading">¶</a>{.headerlink}</a></h5>
<p>Default: [<code>5.0</code>{.docutils .literal .notranslate}]{.pre}</p>
<p>The initial download delay (in seconds).
:::</p>
<p>::: {#autothrottle-max-delay .section}
[]{#std-setting-AUTOTHROTTLE_MAX_DELAY}</p>
<h5 id="autothrottle_max_delayheaderlink"><a class="header" href="#autothrottle_max_delayheaderlink">AUTOTHROTTLE_MAX_DELAY<a href="#autothrottle-max-delay" title="Permalink to this heading">¶</a>{.headerlink}</a></h5>
<p>Default: [<code>60.0</code>{.docutils .literal .notranslate}]{.pre}</p>
<p>The maximum download delay (in seconds) to be set in case of high
latencies.
:::</p>
<p>::: {#autothrottle-target-concurrency .section}
[]{#std-setting-AUTOTHROTTLE_TARGET_CONCURRENCY}</p>
<h5 id="autothrottle_target_concurrencyheaderlink"><a class="header" href="#autothrottle_target_concurrencyheaderlink">AUTOTHROTTLE_TARGET_CONCURRENCY<a href="#autothrottle-target-concurrency" title="Permalink to this heading">¶</a>{.headerlink}</a></h5>
<p>Default: [<code>1.0</code>{.docutils .literal .notranslate}]{.pre}</p>
<p>Average number of requests Scrapy should be sending in parallel to
remote websites.</p>
<p>By default, AutoThrottle adjusts the delay to send a single concurrent
request to each of the remote websites. Set this option to a higher
value (e.g. [<code>2.0</code>{.docutils .literal .notranslate}]{.pre}) to increase
the throughput and the load on remote servers. A lower
[<code>AUTOTHROTTLE_TARGET_CONCURRENCY</code>{.docutils .literal
.notranslate}]{.pre} value (e.g. [<code>0.5</code>{.docutils .literal
.notranslate}]{.pre}) makes the crawler more conservative and polite.</p>
<p>Note that <a href="index.html#std-setting-CONCURRENT_REQUESTS_PER_DOMAIN">[<code>CONCURRENT_REQUESTS_PER_DOMAIN</code>{.xref .std .std-setting
.docutils .literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal} and <a href="index.html#std-setting-CONCURRENT_REQUESTS_PER_IP">[<code>CONCURRENT_REQUESTS_PER_IP</code>{.xref
.std .std-setting .docutils .literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal} options are still respected when
AutoThrottle extension is enabled. This means that if
[<code>AUTOTHROTTLE_TARGET_CONCURRENCY</code>{.docutils .literal
.notranslate}]{.pre} is set to a value higher than
<a href="index.html#std-setting-CONCURRENT_REQUESTS_PER_DOMAIN">[<code>CONCURRENT_REQUESTS_PER_DOMAIN</code>{.xref .std .std-setting .docutils
.literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal} or <a href="index.html#std-setting-CONCURRENT_REQUESTS_PER_IP">[<code>CONCURRENT_REQUESTS_PER_IP</code>{.xref
.std .std-setting .docutils .literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal}, the crawler won't reach this number of
concurrent requests.</p>
<p>At every given time point Scrapy can be sending more or less concurrent
requests than [<code>AUTOTHROTTLE_TARGET_CONCURRENCY</code>{.docutils .literal
.notranslate}]{.pre}; it is a suggested value the crawler tries to
approach, not a hard limit.
:::</p>
<p>::: {#autothrottle-debug .section}
[]{#std-setting-AUTOTHROTTLE_DEBUG}</p>
<h5 id="autothrottle_debugheaderlink"><a class="header" href="#autothrottle_debugheaderlink">AUTOTHROTTLE_DEBUG<a href="#autothrottle-debug" title="Permalink to this heading">¶</a>{.headerlink}</a></h5>
<p>Default: [<code>False</code>{.docutils .literal .notranslate}]{.pre}</p>
<p>Enable AutoThrottle debug mode which will display stats on every
response received, so you can see how the throttling parameters are
being adjusted in real time.
:::
:::
:::</p>
<p>[]{#document-topics/benchmarking}</p>
<p>::: {#benchmarking .section}
[]{#id1}</p>
<h3 id="benchmarkingheaderlink"><a class="header" href="#benchmarkingheaderlink">Benchmarking<a href="#benchmarking" title="Permalink to this heading">¶</a>{.headerlink}</a></h3>
<p>Scrapy comes with a simple benchmarking suite that spawns a local HTTP
server and crawls it at the maximum possible speed. The goal of this
benchmarking is to get an idea of how Scrapy performs in your hardware,
in order to have a common baseline for comparisons. It uses a simple
spider that does nothing and just follows links.</p>
<p>To run it use:</p>
<p>::: {.highlight-default .notranslate}
::: highlight
scrapy bench
:::
:::</p>
<p>You should see an output like this:</p>
<p>::: {.highlight-default .notranslate}
::: highlight
2016-12-16 21:18:48 [scrapy.utils.log] INFO: Scrapy 1.2.2 started (bot: quotesbot)
2016-12-16 21:18:48 [scrapy.utils.log] INFO: Overridden settings: {'CLOSESPIDER_TIMEOUT': 10, 'ROBOTSTXT_OBEY': True, 'SPIDER_MODULES': ['quotesbot.spiders'], 'LOGSTATS_INTERVAL': 1, 'BOT_NAME': 'quotesbot', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'quotesbot.spiders'}
2016-12-16 21:18:49 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.closespider.CloseSpider',
'scrapy.extensions.logstats.LogStats',
'scrapy.extensions.telnet.TelnetConsole',
'scrapy.extensions.corestats.CoreStats']
2016-12-16 21:18:49 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
'scrapy.downloadermiddlewares.retry.RetryMiddleware',
'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
'scrapy.downloadermiddlewares.stats.DownloaderStats']
2016-12-16 21:18:49 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
'scrapy.spidermiddlewares.referer.RefererMiddleware',
'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
'scrapy.spidermiddlewares.depth.DepthMiddleware']
2016-12-16 21:18:49 [scrapy.middleware] INFO: Enabled item pipelines:
[]
2016-12-16 21:18:49 [scrapy.core.engine] INFO: Spider opened
2016-12-16 21:18:49 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2016-12-16 21:18:50 [scrapy.extensions.logstats] INFO: Crawled 70 pages (at 4200 pages/min), scraped 0 items (at 0 items/min)
2016-12-16 21:18:51 [scrapy.extensions.logstats] INFO: Crawled 134 pages (at 3840 pages/min), scraped 0 items (at 0 items/min)
2016-12-16 21:18:52 [scrapy.extensions.logstats] INFO: Crawled 198 pages (at 3840 pages/min), scraped 0 items (at 0 items/min)
2016-12-16 21:18:53 [scrapy.extensions.logstats] INFO: Crawled 254 pages (at 3360 pages/min), scraped 0 items (at 0 items/min)
2016-12-16 21:18:54 [scrapy.extensions.logstats] INFO: Crawled 302 pages (at 2880 pages/min), scraped 0 items (at 0 items/min)
2016-12-16 21:18:55 [scrapy.extensions.logstats] INFO: Crawled 358 pages (at 3360 pages/min), scraped 0 items (at 0 items/min)
2016-12-16 21:18:56 [scrapy.extensions.logstats] INFO: Crawled 406 pages (at 2880 pages/min), scraped 0 items (at 0 items/min)
2016-12-16 21:18:57 [scrapy.extensions.logstats] INFO: Crawled 438 pages (at 1920 pages/min), scraped 0 items (at 0 items/min)
2016-12-16 21:18:58 [scrapy.extensions.logstats] INFO: Crawled 470 pages (at 1920 pages/min), scraped 0 items (at 0 items/min)
2016-12-16 21:18:59 [scrapy.core.engine] INFO: Closing spider (closespider_timeout)
2016-12-16 21:18:59 [scrapy.extensions.logstats] INFO: Crawled 518 pages (at 2880 pages/min), scraped 0 items (at 0 items/min)
2016-12-16 21:19:00 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/request_bytes': 229995,
'downloader/request_count': 534,
'downloader/request_method_count/GET': 534,
'downloader/response_bytes': 1565504,
'downloader/response_count': 534,
'downloader/response_status_count/200': 534,
'finish_reason': 'closespider_timeout',
'finish_time': datetime.datetime(2016, 12, 16, 16, 19, 0, 647725),
'log_count/INFO': 17,
'request_depth_max': 19,
'response_received_count': 534,
'scheduler/dequeued': 533,
'scheduler/dequeued/memory': 533,
'scheduler/enqueued': 10661,
'scheduler/enqueued/memory': 10661,
'start_time': datetime.datetime(2016, 12, 16, 16, 18, 49, 799869)}
2016-12-16 21:19:00 [scrapy.core.engine] INFO: Spider closed (closespider_timeout)
:::
:::</p>
<p>That tells you that Scrapy is able to crawl about 3000 pages per minute
in the hardware where you run it. Note that this is a very simple spider
intended to follow links, any custom spider you write will probably do
more stuff which results in slower crawl rates. How slower depends on
how much your spider does and how well it's written.</p>
<p>Use <a href="https://github.com/scrapy/scrapy-bench">scrapy-bench</a>{.reference
.external} for more complex benchmarking.
:::</p>
<p>[]{#document-topics/jobs}</p>
<p>::: {#jobs-pausing-and-resuming-crawls .section}
[]{#topics-jobs}</p>
<h3 id="jobs-pausing-and-resuming-crawlsheaderlink"><a class="header" href="#jobs-pausing-and-resuming-crawlsheaderlink">Jobs: pausing and resuming crawls<a href="#jobs-pausing-and-resuming-crawls" title="Permalink to this heading">¶</a>{.headerlink}</a></h3>
<p>Sometimes, for big sites, it's desirable to pause crawls and be able to
resume them later.</p>
<p>Scrapy supports this functionality out of the box by providing the
following facilities:</p>
<ul>
<li>
<p>a scheduler that persists scheduled requests on disk</p>
</li>
<li>
<p>a duplicates filter that persists visited requests on disk</p>
</li>
<li>
<p>an extension that keeps some spider state (key/value pairs)
persistent between batches</p>
</li>
</ul>
<p>::: {#job-directory .section}</p>
<h4 id="job-directoryheaderlink"><a class="header" href="#job-directoryheaderlink">Job directory<a href="#job-directory" title="Permalink to this heading">¶</a>{.headerlink}</a></h4>
<p>To enable persistence support you just need to define a <em>job directory</em>
through the [<code>JOBDIR</code>{.docutils .literal .notranslate}]{.pre} setting.
This directory will be for storing all required data to keep the state
of a single job (i.e. a spider run). It's important to note that this
directory must not be shared by different spiders, or even different
jobs/runs of the same spider, as it's meant to be used for storing the
state of a <em>single</em> job.
:::</p>
<p>::: {#how-to-use-it .section}</p>
<h4 id="how-to-use-itheaderlink"><a class="header" href="#how-to-use-itheaderlink">How to use it<a href="#how-to-use-it" title="Permalink to this heading">¶</a>{.headerlink}</a></h4>
<p>To start a spider with persistence support enabled, run it like this:</p>
<p>::: {.highlight-default .notranslate}
::: highlight
scrapy crawl somespider -s JOBDIR=crawls/somespider-1
:::
:::</p>
<p>Then, you can stop the spider safely at any time (by pressing Ctrl-C or
sending a signal), and resume it later by issuing the same command:</p>
<p>::: {.highlight-default .notranslate}
::: highlight
scrapy crawl somespider -s JOBDIR=crawls/somespider-1
:::
:::
:::</p>
<p>::: {#keeping-persistent-state-between-batches .section}
[]{#topics-keeping-persistent-state-between-batches}</p>
<h4 id="keeping-persistent-state-between-batchesheaderlink"><a class="header" href="#keeping-persistent-state-between-batchesheaderlink">Keeping persistent state between batches<a href="#keeping-persistent-state-between-batches" title="Permalink to this heading">¶</a>{.headerlink}</a></h4>
<p>Sometimes you'll want to keep some persistent spider state between
pause/resume batches. You can use the [<code>spider.state</code>{.docutils .literal
.notranslate}]{.pre} attribute for that, which should be a dict. There's
a built-in extension that takes care of serializing, storing and loading
that attribute from the job directory, when the spider starts and stops.</p>
<p>Here's an example of a callback that uses the spider state (other spider
code is omitted for brevity):</p>
<p>::: {.highlight-python .notranslate}
::: highlight
def parse_item(self, response):
# parse item here
self.state[&quot;items_count&quot;] = self.state.get(&quot;items_count&quot;, 0) + 1
:::
:::
:::</p>
<p>::: {#persistence-gotchas .section}</p>
<h4 id="persistence-gotchasheaderlink"><a class="header" href="#persistence-gotchasheaderlink">Persistence gotchas<a href="#persistence-gotchas" title="Permalink to this heading">¶</a>{.headerlink}</a></h4>
<p>There are a few things to keep in mind if you want to be able to use the
Scrapy persistence support:</p>
<p>::: {#cookies-expiration .section}</p>
<h5 id="cookies-expirationheaderlink"><a class="header" href="#cookies-expirationheaderlink">Cookies expiration<a href="#cookies-expiration" title="Permalink to this heading">¶</a>{.headerlink}</a></h5>
<p>Cookies may expire. So, if you don't resume your spider quickly the
requests scheduled may no longer work. This won't be an issue if your
spider doesn't rely on cookies.
:::</p>
<p>::: {#request-serialization .section}
[]{#id1}</p>
<h5 id="request-serializationheaderlink"><a class="header" href="#request-serializationheaderlink">Request serialization<a href="#request-serialization" title="Permalink to this heading">¶</a>{.headerlink}</a></h5>
<p>For persistence to work, [<code>Request</code>{.xref .py .py-class .docutils
.literal .notranslate}]{.pre} objects must be serializable with
<a href="https://docs.python.org/3/library/pickle.html#module-pickle" title="(in Python v3.12)">[<code>pickle</code>{.xref .py .py-mod .docutils .literal
.notranslate}]{.pre}</a>{.reference
.external}, except for the [<code>callback</code>{.docutils .literal
.notranslate}]{.pre} and [<code>errback</code>{.docutils .literal
.notranslate}]{.pre} values passed to their [<code>__init__</code>{.docutils
.literal .notranslate}]{.pre} method, which must be methods of the
running <a href="index.html#scrapy.Spider" title="scrapy.Spider">[<code>Spider</code>{.xref .py .py-class .docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal} class.</p>
<p>If you wish to log the requests that couldn't be serialized, you can set
the <a href="index.html#std-setting-SCHEDULER_DEBUG">[<code>SCHEDULER_DEBUG</code>{.xref .std .std-setting .docutils .literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal} setting to [<code>True</code>{.docutils .literal
.notranslate}]{.pre} in the project's settings page. It is
[<code>False</code>{.docutils .literal .notranslate}]{.pre} by default.
:::
:::
:::</p>
<p>[]{#document-topics/coroutines}</p>
<p>::: {#coroutines .section}
[]{#topics-coroutines}</p>
<h3 id="coroutinesheaderlink"><a class="header" href="#coroutinesheaderlink">Coroutines<a href="#coroutines" title="Permalink to this heading">¶</a>{.headerlink}</a></h3>
<p>::: versionadded
[New in version 2.0.]{.versionmodified .added}
:::</p>
<p>Scrapy has <a href="#coroutine-support">[partial support]{.std
.std-ref}</a>{.hoverxref .tooltip .reference .internal}
for the <a href="https://docs.python.org/3/reference/compound_stmts.html#async" title="(in Python v3.12)">[coroutine syntax]{.xref .std
.std-ref}</a>{.reference
.external}.</p>
<p>::: {#supported-callables .section}
[]{#coroutine-support}</p>
<h4 id="supported-callablesheaderlink"><a class="header" href="#supported-callablesheaderlink">Supported callables<a href="#supported-callables" title="Permalink to this heading">¶</a>{.headerlink}</a></h4>
<p>The following callables may be defined as coroutines using
[<code>async</code>{.docutils .literal .notranslate}]{.pre}<code> </code>{.docutils .literal
.notranslate}[<code>def</code>{.docutils .literal .notranslate}]{.pre}, and hence
use coroutine syntax (e.g. [<code>await</code>{.docutils .literal
.notranslate}]{.pre}, [<code>async</code>{.docutils .literal
.notranslate}]{.pre}<code> </code>{.docutils .literal .notranslate}[<code>for</code>{.docutils
.literal .notranslate}]{.pre}, [<code>async</code>{.docutils .literal
.notranslate}]{.pre}<code> </code>{.docutils .literal
.notranslate}[<code>with</code>{.docutils .literal .notranslate}]{.pre}):</p>
<ul>
<li>
<p>[<code>Request</code>{.xref .py .py-class .docutils .literal
.notranslate}]{.pre} callbacks.</p>
<p>If you are using any custom or third-party <a href="index.html#topics-spider-middleware">[spider middleware]{.std
.std-ref}</a>{.hoverxref .tooltip
.reference .internal}, see <a href="#sync-async-spider-middleware">[Mixing synchronous and asynchronous
spider middlewares]{.std
.std-ref}</a>{.hoverxref .tooltip
.reference .internal}.</p>
<p>::: versionchanged
[Changed in version 2.7: ]{.versionmodified .changed}Output of async
callbacks is now processed asynchronously instead of collecting all
of it first.
:::</p>
</li>
<li>
<p>The <a href="index.html#process_item" title="process_item">[<code>process_item()</code>{.xref .py .py-meth .docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal} method of <a href="index.html#topics-item-pipeline">[item pipelines]{.std
.std-ref}</a>{.hoverxref .tooltip
.reference .internal}.</p>
</li>
<li>
<p>The <a href="index.html#scrapy.downloadermiddlewares.DownloaderMiddleware.process_request" title="scrapy.downloadermiddlewares.DownloaderMiddleware.process_request">[<code>process_request()</code>{.xref .py .py-meth .docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal}, <a href="index.html#scrapy.downloadermiddlewares.DownloaderMiddleware.process_response" title="scrapy.downloadermiddlewares.DownloaderMiddleware.process_response">[<code>process_response()</code>{.xref .py .py-meth .docutils
.literal
.notranslate}]{.pre}</a>{.reference
.internal}, and <a href="index.html#scrapy.downloadermiddlewares.DownloaderMiddleware.process_exception" title="scrapy.downloadermiddlewares.DownloaderMiddleware.process_exception">[<code>process_exception()</code>{.xref .py .py-meth .docutils
.literal
.notranslate}]{.pre}</a>{.reference
.internal} methods of <a href="index.html#topics-downloader-middleware-custom">[downloader middlewares]{.std
.std-ref}</a>{.hoverxref
.tooltip .reference .internal}.</p>
</li>
<li>
<p><a href="index.html#signal-deferred">[Signal handlers that support deferreds]{.std
.std-ref}</a>{.hoverxref .tooltip
.reference .internal}.</p>
</li>
<li>
<p>The <a href="index.html#scrapy.spidermiddlewares.SpiderMiddleware.process_spider_output" title="scrapy.spidermiddlewares.SpiderMiddleware.process_spider_output">[<code>process_spider_output()</code>{.xref .py .py-meth .docutils
.literal
.notranslate}]{.pre}</a>{.reference
.internal} method of <a href="index.html#topics-spider-middleware">[spider middlewares]{.std
.std-ref}</a>{.hoverxref .tooltip
.reference .internal}.</p>
<p>It must be defined as an <a href="https://docs.python.org/3/glossary.html#term-asynchronous-generator" title="(in Python v3.12)">[asynchronous generator]{.xref .std
.std-term}</a>{.reference
.external}. The input [<code>result</code>{.docutils .literal
.notranslate}]{.pre} parameter is an <a href="https://docs.python.org/3/glossary.html#term-asynchronous-iterable" title="(in Python v3.12)">[asynchronous iterable]{.xref
.std
.std-term}</a>{.reference
.external}.</p>
<p>See also <a href="#sync-async-spider-middleware">[Mixing synchronous and asynchronous spider
middlewares]{.std
.std-ref}</a>{.hoverxref .tooltip
.reference .internal} and <a href="#universal-spider-middleware">[Universal spider middlewares]{.std
.std-ref}</a>{.hoverxref .tooltip
.reference .internal}.</p>
<p>::: versionadded
[New in version 2.7.]{.versionmodified .added}
:::
:::</p>
</li>
</ul>
<p>::: {#general-usage .section}</p>
<h4 id="general-usageheaderlink"><a class="header" href="#general-usageheaderlink">General usage<a href="#general-usage" title="Permalink to this heading">¶</a>{.headerlink}</a></h4>
<p>There are several use cases for coroutines in Scrapy.</p>
<p>Code that would return Deferreds when written for previous Scrapy
versions, such as downloader middlewares and signal handlers, can be
rewritten to be shorter and cleaner:</p>
<p>::: {.highlight-python .notranslate}
::: highlight
from itemadapter import ItemAdapter</p>
<pre><code>class DbPipeline:
    def _update_item(self, data, item):
        adapter = ItemAdapter(item)
        adapter[&quot;field&quot;] = data
        return item

    def process_item(self, item, spider):
        adapter = ItemAdapter(item)
        dfd = db.get_some_data(adapter[&quot;id&quot;])
        dfd.addCallback(self._update_item, item)
        return dfd
</code></pre>
<p>:::
:::</p>
<p>becomes:</p>
<p>::: {.highlight-python .notranslate}
::: highlight
from itemadapter import ItemAdapter</p>
<pre><code>class DbPipeline:
    async def process_item(self, item, spider):
        adapter = ItemAdapter(item)
        adapter[&quot;field&quot;] = await db.get_some_data(adapter[&quot;id&quot;])
        return item
</code></pre>
<p>:::
:::</p>
<p>Coroutines may be used to call asynchronous code. This includes other
coroutines, functions that return Deferreds and functions that return
<a href="https://docs.python.org/3/glossary.html#term-awaitable" title="(in Python v3.12)">[awaitable objects]{.xref .std
.std-term}</a>{.reference
.external} such as <a href="https://docs.python.org/3/library/asyncio-future.html#asyncio.Future" title="(in Python v3.12)">[<code>Future</code>{.xref .py .py-class .docutils .literal
.notranslate}]{.pre}</a>{.reference
.external}. This means you can use many useful Python libraries
providing such code:</p>
<p>::: {.highlight-python .notranslate}
::: highlight
class MySpiderDeferred(Spider):
# ...
async def parse(self, response):
additional_response = await treq.get(&quot;https://additional.url&quot;)
additional_data = await treq.content(additional_response)
# ... use response and additional_data to yield items and requests</p>
<pre><code>class MySpiderAsyncio(Spider):
    # ...
    async def parse(self, response):
        async with aiohttp.ClientSession() as session:
            async with session.get(&quot;https://additional.url&quot;) as additional_response:
                additional_data = await additional_response.text()
        # ... use response and additional_data to yield items and requests
</code></pre>
<p>:::
:::</p>
<p>::: {.admonition .note}
Note</p>
<p>Many libraries that use coroutines, such as
<a href="https://github.com/aio-libs">aio-libs</a>{.reference .external}, require
the <a href="https://docs.python.org/3/library/asyncio.html#module-asyncio" title="(in Python v3.12)">[<code>asyncio</code>{.xref .py .py-mod .docutils .literal
.notranslate}]{.pre}</a>{.reference
.external} loop and to use them you need to <a href="index.html#document-topics/asyncio">[enable asyncio support in
Scrapy]{.doc}</a>{.reference
.internal}.
:::</p>
<p>::: {.admonition .note}
Note</p>
<p>If you want to [<code>await</code>{.docutils .literal .notranslate}]{.pre} on
Deferreds while using the asyncio reactor, you need to <a href="index.html#asyncio-await-dfd">[wrap them]{.std
.std-ref}</a>{.hoverxref .tooltip .reference
.internal}.
:::</p>
<p>Common use cases for asynchronous code include:</p>
<ul>
<li>
<p>requesting data from websites, databases and other services (in
callbacks, pipelines and middlewares);</p>
</li>
<li>
<p>storing data in databases (in pipelines and middlewares);</p>
</li>
<li>
<p>delaying the spider initialization until some external event (in the
<a href="index.html#std-signal-spider_opened">[<code>spider_opened</code>{.xref .std .std-signal .docutils .literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal} handler);</p>
</li>
<li>
<p>calling asynchronous Scrapy methods like
[<code>ExecutionEngine.download()</code>{.xref .py .py-meth .docutils .literal
.notranslate}]{.pre} (see <a href="index.html#screenshotpipeline">[the screenshot pipeline example]{.std
.std-ref}</a>{.hoverxref .tooltip
.reference .internal}).
:::</p>
</li>
</ul>
<p>::: {#inline-requests .section}
[]{#id1}</p>
<h4 id="inline-requestsheaderlink"><a class="header" href="#inline-requestsheaderlink">Inline requests<a href="#inline-requests" title="Permalink to this heading">¶</a>{.headerlink}</a></h4>
<p>The spider below shows how to send a request and await its response all
from within a spider callback:</p>
<p>::: {.highlight-python .notranslate}
::: highlight
from scrapy import Spider, Request
from scrapy.utils.defer import maybe_deferred_to_future</p>
<pre><code>class SingleRequestSpider(Spider):
    name = &quot;single&quot;
    start_urls = [&quot;https://example.org/product&quot;]

    async def parse(self, response, **kwargs):
        additional_request = Request(&quot;https://example.org/price&quot;)
        deferred = self.crawler.engine.download(additional_request)
        additional_response = await maybe_deferred_to_future(deferred)
        yield {
            &quot;h1&quot;: response.css(&quot;h1&quot;).get(),
            &quot;price&quot;: additional_response.css(&quot;#price&quot;).get(),
        }
</code></pre>
<p>:::
:::</p>
<p>You can also send multiple requests in parallel:</p>
<p>::: {.highlight-python .notranslate}
::: highlight
from scrapy import Spider, Request
from scrapy.utils.defer import maybe_deferred_to_future
from twisted.internet.defer import DeferredList</p>
<pre><code>class MultipleRequestsSpider(Spider):
    name = &quot;multiple&quot;
    start_urls = [&quot;https://example.com/product&quot;]

    async def parse(self, response, **kwargs):
        additional_requests = [
            Request(&quot;https://example.com/price&quot;),
            Request(&quot;https://example.com/color&quot;),
        ]
        deferreds = []
        for r in additional_requests:
            deferred = self.crawler.engine.download(r)
            deferreds.append(deferred)
        responses = await maybe_deferred_to_future(DeferredList(deferreds))
        yield {
            &quot;h1&quot;: response.css(&quot;h1::text&quot;).get(),
            &quot;price&quot;: responses[0][1].css(&quot;.price::text&quot;).get(),
            &quot;price2&quot;: responses[1][1].css(&quot;.color::text&quot;).get(),
        }
</code></pre>
<p>:::
:::
:::</p>
<p>::: {#mixing-synchronous-and-asynchronous-spider-middlewares .section}
[]{#sync-async-spider-middleware}</p>
<h4 id="mixing-synchronous-and-asynchronous-spider-middlewaresheaderlink"><a class="header" href="#mixing-synchronous-and-asynchronous-spider-middlewaresheaderlink">Mixing synchronous and asynchronous spider middlewares<a href="#mixing-synchronous-and-asynchronous-spider-middlewares" title="Permalink to this heading">¶</a>{.headerlink}</a></h4>
<p>::: versionadded
[New in version 2.7.]{.versionmodified .added}
:::</p>
<p>The output of a [<code>Request</code>{.xref .py .py-class .docutils .literal
.notranslate}]{.pre} callback is passed as the [<code>result</code>{.docutils
.literal .notranslate}]{.pre} parameter to the
<a href="index.html#scrapy.spidermiddlewares.SpiderMiddleware.process_spider_output" title="scrapy.spidermiddlewares.SpiderMiddleware.process_spider_output">[<code>process_spider_output()</code>{.xref .py .py-meth .docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal} method of the first <a href="index.html#topics-spider-middleware">[spider middleware]{.std
.std-ref}</a>{.hoverxref .tooltip
.reference .internal} from the <a href="index.html#topics-spider-middleware-setting">[list of active spider middlewares]{.std
.std-ref}</a>{.hoverxref
.tooltip .reference .internal}. Then the output of that
[<code>process_spider_output</code>{.docutils .literal .notranslate}]{.pre} method
is passed to the [<code>process_spider_output</code>{.docutils .literal
.notranslate}]{.pre} method of the next spider middleware, and so on for
every active spider middleware.</p>
<p>Scrapy supports mixing <a href="https://docs.python.org/3/reference/compound_stmts.html#async" title="(in Python v3.12)">[coroutine methods]{.xref .std
.std-ref}</a>{.reference
.external} and synchronous methods in this chain of calls.</p>
<p>However, if any of the [<code>process_spider_output</code>{.docutils .literal
.notranslate}]{.pre} methods is defined as a synchronous method, and the
previous [<code>Request</code>{.docutils .literal .notranslate}]{.pre} callback or
[<code>process_spider_output</code>{.docutils .literal .notranslate}]{.pre} method
is a coroutine, there are some drawbacks to the
asynchronous-to-synchronous conversion that Scrapy does so that the
synchronous [<code>process_spider_output</code>{.docutils .literal
.notranslate}]{.pre} method gets a synchronous iterable as its
[<code>result</code>{.docutils .literal .notranslate}]{.pre} parameter:</p>
<ul>
<li>
<p>The whole output of the previous [<code>Request</code>{.docutils .literal
.notranslate}]{.pre} callback or [<code>process_spider_output</code>{.docutils
.literal .notranslate}]{.pre} method is awaited at this point.</p>
</li>
<li>
<p>If an exception raises while awaiting the output of the previous
[<code>Request</code>{.docutils .literal .notranslate}]{.pre} callback or
[<code>process_spider_output</code>{.docutils .literal .notranslate}]{.pre}
method, none of that output will be processed.</p>
<p>This contrasts with the regular behavior, where all items yielded
before an exception raises are processed.</p>
</li>
</ul>
<p>Asynchronous-to-synchronous conversions are supported for backward
compatibility, but they are deprecated and will stop working in a future
version of Scrapy.</p>
<p>To avoid asynchronous-to-synchronous conversions, when defining
[<code>Request</code>{.docutils .literal .notranslate}]{.pre} callbacks as
coroutine methods or when using spider middlewares whose
[<code>process_spider_output</code>{.docutils .literal .notranslate}]{.pre} method
is an <a href="https://docs.python.org/3/glossary.html#term-asynchronous-generator" title="(in Python v3.12)">[asynchronous generator]{.xref .std
.std-term}</a>{.reference
.external}, all active spider middlewares must either have their
[<code>process_spider_output</code>{.docutils .literal .notranslate}]{.pre} method
defined as an asynchronous generator or <a href="#universal-spider-middleware">[define a
process_spider_output_async method]{.std
.std-ref}</a>{.hoverxref .tooltip .reference
.internal}.</p>
<p>::: {.admonition .note}
Note</p>
<p>When using third-party spider middlewares that only define a synchronous
[<code>process_spider_output</code>{.docutils .literal .notranslate}]{.pre} method,
consider <a href="#universal-spider-middleware">[making them universal]{.std
.std-ref}</a>{.hoverxref .tooltip .reference
.internal} through <a href="https://docs.python.org/3/tutorial/classes.html#tut-inheritance" title="(in Python v3.12)">[subclassing]{.xref .std
.std-ref}</a>{.reference
.external}.
:::
:::</p>
<p>::: {#universal-spider-middlewares .section}
[]{#universal-spider-middleware}</p>
<h4 id="universal-spider-middlewaresheaderlink"><a class="header" href="#universal-spider-middlewaresheaderlink">Universal spider middlewares<a href="#universal-spider-middlewares" title="Permalink to this heading">¶</a>{.headerlink}</a></h4>
<p>::: versionadded
[New in version 2.7.]{.versionmodified .added}
:::</p>
<p>To allow writing a spider middleware that supports asynchronous
execution of its [<code>process_spider_output</code>{.docutils .literal
.notranslate}]{.pre} method in Scrapy 2.7 and later (avoiding
<a href="#sync-async-spider-middleware">[asynchronous-to-synchronous conversions]{.std
.std-ref}</a>{.hoverxref .tooltip .reference
.internal}) while maintaining support for older Scrapy versions, you may
define [<code>process_spider_output</code>{.docutils .literal .notranslate}]{.pre}
as a synchronous method and define an <a href="https://docs.python.org/3/glossary.html#term-asynchronous-generator" title="(in Python v3.12)">[asynchronous generator]{.xref
.std
.std-term}</a>{.reference
.external} version of that method with an alternative name:
[<code>process_spider_output_async</code>{.docutils .literal .notranslate}]{.pre}.</p>
<p>For example:</p>
<p>::: {.highlight-python .notranslate}
::: highlight
class UniversalSpiderMiddleware:
def process_spider_output(self, response, result, spider):
for r in result:
# ... do something with r
yield r</p>
<pre><code>    async def process_spider_output_async(self, response, result, spider):
        async for r in result:
            # ... do something with r
            yield r
</code></pre>
<p>:::
:::</p>
<p>::: {.admonition .note}
Note</p>
<p>This is an interim measure to allow, for a time, to write code that
works in Scrapy 2.7 and later without requiring
asynchronous-to-synchronous conversions, and works in earlier Scrapy
versions as well.</p>
<p>In some future version of Scrapy, however, this feature will be
deprecated and, eventually, in a later version of Scrapy, this feature
will be removed, and all spider middlewares will be expected to define
their [<code>process_spider_output</code>{.docutils .literal .notranslate}]{.pre}
method as an asynchronous generator.
:::
:::
:::</p>
<p>[]{#document-topics/asyncio}</p>
<p>::: {#asyncio .section}
[]{#using-asyncio}</p>
<h3 id="asyncioheaderlink"><a class="header" href="#asyncioheaderlink">asyncio<a href="#asyncio" title="Permalink to this heading">¶</a>{.headerlink}</a></h3>
<p>::: versionadded
[New in version 2.0.]{.versionmodified .added}
:::</p>
<p>Scrapy has partial support for <a href="https://docs.python.org/3/library/asyncio.html#module-asyncio" title="(in Python v3.12)">[<code>asyncio</code>{.xref .py .py-mod .docutils
.literal
.notranslate}]{.pre}</a>{.reference
.external}. After you <a href="#install-asyncio">[install the asyncio reactor]{.std
.std-ref}</a>{.hoverxref .tooltip .reference .internal},
you may use <a href="https://docs.python.org/3/library/asyncio.html#module-asyncio" title="(in Python v3.12)">[<code>asyncio</code>{.xref .py .py-mod .docutils .literal
.notranslate}]{.pre}</a>{.reference
.external} and <a href="https://docs.python.org/3/library/asyncio.html#module-asyncio" title="(in Python v3.12)">[<code>asyncio</code>{.xref .py .py-mod .docutils .literal
.notranslate}]{.pre}</a>{.reference
.external}-powered libraries in any
<a href="index.html#document-topics/coroutines">[coroutine]{.doc}</a>{.reference
.internal}.</p>
<p>::: {#installing-the-asyncio-reactor .section}
[]{#install-asyncio}</p>
<h4 id="installing-the-asyncio-reactorheaderlink"><a class="header" href="#installing-the-asyncio-reactorheaderlink">Installing the asyncio reactor<a href="#installing-the-asyncio-reactor" title="Permalink to this heading">¶</a>{.headerlink}</a></h4>
<p>To enable <a href="https://docs.python.org/3/library/asyncio.html#module-asyncio" title="(in Python v3.12)">[<code>asyncio</code>{.xref .py .py-mod .docutils .literal
.notranslate}]{.pre}</a>{.reference
.external} support, set the <a href="index.html#std-setting-TWISTED_REACTOR">[<code>TWISTED_REACTOR</code>{.xref .std .std-setting
.docutils .literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal} setting to
[<code>'twisted.internet.asyncioreactor.AsyncioSelectorReactor'</code>{.docutils
.literal .notranslate}]{.pre}.</p>
<p>If you are using <a href="index.html#scrapy.crawler.CrawlerRunner" title="scrapy.crawler.CrawlerRunner">[<code>CrawlerRunner</code>{.xref .py .py-class .docutils
.literal
.notranslate}]{.pre}</a>{.reference
.internal}, you also need to install the
<a href="https://docs.twisted.org/en/stable/api/twisted.internet.asyncioreactor.AsyncioSelectorReactor.html" title="(in Twisted)">[<code>AsyncioSelectorReactor</code>{.xref .py .py-class .docutils .literal
.notranslate}]{.pre}</a>{.reference
.external} reactor manually. You can do that using
<a href="index.html#scrapy.utils.reactor.install_reactor" title="scrapy.utils.reactor.install_reactor">[<code>install_reactor()</code>{.xref .py .py-func .docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal}:</p>
<p>::: {.highlight-default .notranslate}
::: highlight
install_reactor('twisted.internet.asyncioreactor.AsyncioSelectorReactor')
:::
:::
:::</p>
<p>::: {#handling-a-pre-installed-reactor .section}
[]{#asyncio-preinstalled-reactor}</p>
<h4 id="handling-a-pre-installed-reactorheaderlink"><a class="header" href="#handling-a-pre-installed-reactorheaderlink">Handling a pre-installed reactor<a href="#handling-a-pre-installed-reactor" title="Permalink to this heading">¶</a>{.headerlink}</a></h4>
<p>[<code>twisted.internet.reactor</code>{.docutils .literal .notranslate}]{.pre} and
some other Twisted imports install the default Twisted reactor as a side
effect. Once a Twisted reactor is installed, it is not possible to
switch to a different reactor at run time.</p>
<p>If you <a href="#install-asyncio">[configure the asyncio Twisted reactor]{.std
.std-ref}</a>{.hoverxref .tooltip .reference .internal}
and, at run time, Scrapy complains that a different reactor is already
installed, chances are you have some such imports in your code.</p>
<p>You can usually fix the issue by moving those offending module-level
Twisted imports to the method or function definitions where they are
used. For example, if you have something like:</p>
<p>::: {.highlight-python .notranslate}
::: highlight
from twisted.internet import reactor</p>
<pre><code>def my_function():
    reactor.callLater(...)
</code></pre>
<p>:::
:::</p>
<p>Switch to something like:</p>
<p>::: {.highlight-python .notranslate}
::: highlight
def my_function():
from twisted.internet import reactor</p>
<pre><code>    reactor.callLater(...)
</code></pre>
<p>:::
:::</p>
<p>Alternatively, you can try to <a href="#install-asyncio">[manually install the asyncio
reactor]{.std .std-ref}</a>{.hoverxref .tooltip
.reference .internal}, with <a href="index.html#scrapy.utils.reactor.install_reactor" title="scrapy.utils.reactor.install_reactor">[<code>install_reactor()</code>{.xref .py .py-func
.docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal}, before those imports happen.
:::</p>
<p>::: {#awaiting-on-deferreds .section}
[]{#asyncio-await-dfd}</p>
<h4 id="awaiting-on-deferredsheaderlink"><a class="header" href="#awaiting-on-deferredsheaderlink">Awaiting on Deferreds<a href="#awaiting-on-deferreds" title="Permalink to this heading">¶</a>{.headerlink}</a></h4>
<p>When the asyncio reactor isn't installed, you can await on Deferreds in
the coroutines directly. When it is installed, this is not possible
anymore, due to specifics of the Scrapy coroutine integration (the
coroutines are wrapped into <a href="https://docs.python.org/3/library/asyncio-future.html#asyncio.Future" title="(in Python v3.12)">[<code>asyncio.Future</code>{.xref .py .py-class
.docutils .literal
.notranslate}]{.pre}</a>{.reference
.external} objects, not into <a href="https://docs.twisted.org/en/stable/api/twisted.internet.defer.Deferred.html" title="(in Twisted)">[<code>Deferred</code>{.xref .py .py-class .docutils
.literal
.notranslate}]{.pre}</a>{.reference
.external} directly), and you need to wrap them into Futures. Scrapy
provides two helpers for this:</p>
<p>[[scrapy.utils.defer.]{.pre}]{.sig-prename .descclassname}[[deferred_to_future]{.pre}]{.sig-name .descname}[(]{.sig-paren}<em>[[d]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[<a href="https://docs.twisted.org/en/stable/api/twisted.internet.defer.Deferred.html" title="(in Twisted)">[Deferred]{.pre}</a>{.reference .external}]{.n}</em>[)]{.sig-paren} [[→]{.sig-return-icon} [[Future]{.pre}]{.sig-return-typehint}]{.sig-return}<a href="_modules/scrapy/utils/defer.html#deferred_to_future">[[[source]]{.pre}]{.viewcode-link}</a>{.reference .internal}<a href="#scrapy.utils.defer.deferred_to_future" title="Permalink to this definition">¶</a>{.headerlink}</p>
<p>:   ::: versionadded
[New in version 2.6.0.]{.versionmodified .added}
:::</p>
<pre><code>Return an [[`asyncio.Future`{.xref .py .py-class .docutils .literal
.notranslate}]{.pre}](https://docs.python.org/3/library/asyncio-future.html#asyncio.Future &quot;(in Python v3.12)&quot;){.reference
.external} object that wraps *d*.

When [[using the asyncio reactor]{.std
.std-ref}](#install-asyncio){.hoverxref .tooltip .reference
.internal}, you cannot await on [[`Deferred`{.xref .py .py-class
.docutils .literal
.notranslate}]{.pre}](https://docs.twisted.org/en/stable/api/twisted.internet.defer.Deferred.html &quot;(in Twisted)&quot;){.reference
.external} objects from [[Scrapy callables defined as
coroutines]{.std .std-ref}](index.html#coroutine-support){.hoverxref
.tooltip .reference .internal}, you can only await on
[`Future`{.docutils .literal .notranslate}]{.pre} objects. Wrapping
[`Deferred`{.docutils .literal .notranslate}]{.pre} objects into
[`Future`{.docutils .literal .notranslate}]{.pre} objects allows you
to wait on them:

::: {.highlight-default .notranslate}
::: highlight
    class MySpider(Spider):
        ...
        async def parse(self, response):
            additional_request = scrapy.Request('https://example.org/price')
            deferred = self.crawler.engine.download(additional_request)
            additional_response = await deferred_to_future(deferred)
:::
:::
</code></pre>
<pre><code class="language-{=html}">&lt;!-- --&gt;
</code></pre>
<p>[[scrapy.utils.defer.]{.pre}]{.sig-prename .descclassname}[[maybe_deferred_to_future]{.pre}]{.sig-name .descname}[(]{.sig-paren}<em>[[d]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[<a href="https://docs.twisted.org/en/stable/api/twisted.internet.defer.Deferred.html" title="(in Twisted)">[Deferred]{.pre}</a>{.reference .external}]{.n}</em>[)]{.sig-paren} [[→]{.sig-return-icon} [<a href="https://docs.python.org/3/library/typing.html#typing.Union" title="(in Python v3.12)">[Union]{.pre}</a>{.reference .external}[[[]{.pre}]{.p}<a href="https://docs.twisted.org/en/stable/api/twisted.internet.defer.Deferred.html" title="(in Twisted)">[Deferred]{.pre}</a>{.reference .external}[[,]{.pre}]{.p}[ ]{.w}[Future]{.pre}[[]]{.pre}]{.p}]{.sig-return-typehint}]{.sig-return}<a href="_modules/scrapy/utils/defer.html#maybe_deferred_to_future">[[[source]]{.pre}]{.viewcode-link}</a>{.reference .internal}<a href="#scrapy.utils.defer.maybe_deferred_to_future" title="Permalink to this definition">¶</a>{.headerlink}</p>
<p>:   ::: versionadded
[New in version 2.6.0.]{.versionmodified .added}
:::</p>
<pre><code>Return *d* as an object that can be awaited from a [[Scrapy callable
defined as a coroutine]{.std
.std-ref}](index.html#coroutine-support){.hoverxref .tooltip
.reference .internal}.

What you can await in Scrapy callables defined as coroutines depends
on the value of [[`TWISTED_REACTOR`{.xref .std .std-setting
.docutils .literal
.notranslate}]{.pre}](index.html#std-setting-TWISTED_REACTOR){.hoverxref
.tooltip .reference .internal}:

-   When not using the asyncio reactor, you can only await on
    [[`Deferred`{.xref .py .py-class .docutils .literal
    .notranslate}]{.pre}](https://docs.twisted.org/en/stable/api/twisted.internet.defer.Deferred.html &quot;(in Twisted)&quot;){.reference
    .external} objects.

-   When [[using the asyncio reactor]{.std
    .std-ref}](#install-asyncio){.hoverxref .tooltip .reference
    .internal}, you can only await on [[`asyncio.Future`{.xref .py
    .py-class .docutils .literal
    .notranslate}]{.pre}](https://docs.python.org/3/library/asyncio-future.html#asyncio.Future &quot;(in Python v3.12)&quot;){.reference
    .external} objects.

If you want to write code that uses [`Deferred`{.docutils .literal
.notranslate}]{.pre} objects but works with any reactor, use this
function on all [`Deferred`{.docutils .literal .notranslate}]{.pre}
objects:

::: {.highlight-default .notranslate}
::: highlight
    class MySpider(Spider):
        ...
        async def parse(self, response):
            additional_request = scrapy.Request('https://example.org/price')
            deferred = self.crawler.engine.download(additional_request)
            additional_response = await maybe_deferred_to_future(deferred)
:::
:::
</code></pre>
<p>::: {.admonition .tip}
Tip</p>
<p>If you need to use these functions in code that aims to be compatible
with lower versions of Scrapy that do not provide these functions, down
to Scrapy 2.0 (earlier versions do not support <a href="https://docs.python.org/3/library/asyncio.html#module-asyncio" title="(in Python v3.12)">[<code>asyncio</code>{.xref .py
.py-mod .docutils .literal
.notranslate}]{.pre}</a>{.reference
.external}), you can copy the implementation of these functions into
your own code.
:::
:::</p>
<p>::: {#enforcing-asyncio-as-a-requirement .section}
[]{#enforce-asyncio-requirement}</p>
<h4 id="enforcing-asyncio-as-a-requirementheaderlink"><a class="header" href="#enforcing-asyncio-as-a-requirementheaderlink">Enforcing asyncio as a requirement<a href="#enforcing-asyncio-as-a-requirement" title="Permalink to this heading">¶</a>{.headerlink}</a></h4>
<p>If you are writing a <a href="index.html#topics-components">[component]{.std
.std-ref}</a>{.hoverxref .tooltip .reference
.internal} that requires asyncio to work, use
[<code>scrapy.utils.reactor.is_asyncio_reactor_installed()</code>{.xref .py
.py-func .docutils .literal .notranslate}]{.pre} to <a href="index.html#enforce-component-requirements">[enforce it as a
requirement]{.std
.std-ref}</a>{.hoverxref
.tooltip .reference .internal}. For example:</p>
<p>::: {.highlight-python .notranslate}
::: highlight
from scrapy.utils.reactor import is_asyncio_reactor_installed</p>
<pre><code>class MyComponent:
    def __init__(self):
        if not is_asyncio_reactor_installed():
            raise ValueError(
                f&quot;{MyComponent.__qualname__} requires the asyncio Twisted &quot;
                f&quot;reactor. Make sure you have it configured in the &quot;
                f&quot;TWISTED_REACTOR setting. See the asyncio documentation &quot;
                f&quot;of Scrapy for more information.&quot;
            )
</code></pre>
<p>:::
:::
:::</p>
<p>::: {#windows-specific-notes .section}
[]{#asyncio-windows}</p>
<h4 id="windows-specific-notesheaderlink"><a class="header" href="#windows-specific-notesheaderlink">Windows-specific notes<a href="#windows-specific-notes" title="Permalink to this heading">¶</a>{.headerlink}</a></h4>
<p>The Windows implementation of <a href="https://docs.python.org/3/library/asyncio.html#module-asyncio" title="(in Python v3.12)">[<code>asyncio</code>{.xref .py .py-mod .docutils
.literal
.notranslate}]{.pre}</a>{.reference
.external} can use two event loop implementations,
<a href="https://docs.python.org/3/library/asyncio-eventloop.html#asyncio.ProactorEventLoop" title="(in Python v3.12)">[<code>ProactorEventLoop</code>{.xref .py .py-class .docutils .literal
.notranslate}]{.pre}</a>{.reference
.external} (default) and <a href="https://docs.python.org/3/library/asyncio-eventloop.html#asyncio.SelectorEventLoop" title="(in Python v3.12)">[<code>SelectorEventLoop</code>{.xref .py .py-class
.docutils .literal
.notranslate}]{.pre}</a>{.reference
.external}. However, only <a href="https://docs.python.org/3/library/asyncio-eventloop.html#asyncio.SelectorEventLoop" title="(in Python v3.12)">[<code>SelectorEventLoop</code>{.xref .py .py-class
.docutils .literal
.notranslate}]{.pre}</a>{.reference
.external} works with Twisted.</p>
<p>Scrapy changes the event loop class to <a href="https://docs.python.org/3/library/asyncio-eventloop.html#asyncio.SelectorEventLoop" title="(in Python v3.12)">[<code>SelectorEventLoop</code>{.xref .py
.py-class .docutils .literal
.notranslate}]{.pre}</a>{.reference
.external} automatically when you change the <a href="index.html#std-setting-TWISTED_REACTOR">[<code>TWISTED_REACTOR</code>{.xref
.std .std-setting .docutils .literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal} setting or call
<a href="index.html#scrapy.utils.reactor.install_reactor" title="scrapy.utils.reactor.install_reactor">[<code>install_reactor()</code>{.xref .py .py-func .docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal}.</p>
<p>::: {.admonition .note}
Note</p>
<p>Other libraries you use may require <a href="https://docs.python.org/3/library/asyncio-eventloop.html#asyncio.ProactorEventLoop" title="(in Python v3.12)">[<code>ProactorEventLoop</code>{.xref .py
.py-class .docutils .literal
.notranslate}]{.pre}</a>{.reference
.external}, e.g. because it supports subprocesses (this is the case with
<a href="https://github.com/microsoft/playwright-python">playwright</a>{.reference
.external}), so you cannot use them together with Scrapy on Windows (but
you should be able to use them on WSL or native Linux).
:::
:::</p>
<p>::: {#using-custom-asyncio-loops .section}
[]{#using-custom-loops}</p>
<h4 id="using-custom-asyncio-loopsheaderlink"><a class="header" href="#using-custom-asyncio-loopsheaderlink">Using custom asyncio loops<a href="#using-custom-asyncio-loops" title="Permalink to this heading">¶</a>{.headerlink}</a></h4>
<p>You can also use custom asyncio event loops with the asyncio reactor.
Set the <a href="index.html#std-setting-ASYNCIO_EVENT_LOOP">[<code>ASYNCIO_EVENT_LOOP</code>{.xref .std .std-setting .docutils
.literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal} setting to the import path of the desired
event loop class to use it instead of the default asyncio event loop.
:::
:::
:::</p>
<p><a href="index.html#document-faq">[Frequently Asked Questions]{.doc}</a>{.reference .internal}</p>
<p>:   Get answers to most frequently asked questions.</p>
<p><a href="index.html#document-topics/debug">[Debugging Spiders]{.doc}</a>{.reference .internal}</p>
<p>:   Learn how to debug common problems of your Scrapy spider.</p>
<p><a href="index.html#document-topics/contracts">[Spiders Contracts]{.doc}</a>{.reference .internal}</p>
<p>:   Learn how to use contracts for testing your spiders.</p>
<p><a href="index.html#document-topics/practices">[Common Practices]{.doc}</a>{.reference .internal}</p>
<p>:   Get familiar with some Scrapy common practices.</p>
<p><a href="index.html#document-topics/broad-crawls">[Broad Crawls]{.doc}</a>{.reference .internal}</p>
<p>:   Tune Scrapy for crawling a lot domains in parallel.</p>
<p><a href="index.html#document-topics/developer-tools">[Using your browser's Developer Tools for scraping]{.doc}</a>{.reference .internal}</p>
<p>:   Learn how to scrape with your browser's developer tools.</p>
<p><a href="index.html#document-topics/dynamic-content">[Selecting dynamically-loaded content]{.doc}</a>{.reference .internal}</p>
<p>:   Read webpage data that is loaded dynamically.</p>
<p><a href="index.html#document-topics/leaks">[Debugging memory leaks]{.doc}</a>{.reference .internal}</p>
<p>:   Learn how to find and get rid of memory leaks in your crawler.</p>
<p><a href="index.html#document-topics/media-pipeline">[Downloading and processing files and images]{.doc}</a>{.reference .internal}</p>
<p>:   Download files and/or images associated with your scraped items.</p>
<p><a href="index.html#document-topics/deploy">[Deploying Spiders]{.doc}</a>{.reference .internal}</p>
<p>:   Deploying your Scrapy spiders and run them in a remote server.</p>
<p><a href="index.html#document-topics/autothrottle">[AutoThrottle extension]{.doc}</a>{.reference .internal}</p>
<p>:   Adjust crawl rate dynamically based on load.</p>
<p><a href="index.html#document-topics/benchmarking">[Benchmarking]{.doc}</a>{.reference .internal}</p>
<p>:   Check how Scrapy performs on your hardware.</p>
<p><a href="index.html#document-topics/jobs">[Jobs: pausing and resuming crawls]{.doc}</a>{.reference .internal}</p>
<p>:   Learn how to pause and resume crawls for large spiders.</p>
<p><a href="index.html#document-topics/coroutines">[Coroutines]{.doc}</a>{.reference .internal}</p>
<p>:   Use the <a href="https://docs.python.org/3/reference/compound_stmts.html#async" title="(in Python v3.12)">[coroutine syntax]{.xref .std
.std-ref}</a>{.reference
.external}.</p>
<p><a href="index.html#document-topics/asyncio">[asyncio]{.doc}</a>{.reference .internal}</p>
<p>:   Use <a href="https://docs.python.org/3/library/asyncio.html#module-asyncio" title="(in Python v3.12)">[<code>asyncio</code>{.xref .py .py-mod .docutils .literal
.notranslate}]{.pre}</a>{.reference
.external} and <a href="https://docs.python.org/3/library/asyncio.html#module-asyncio" title="(in Python v3.12)">[<code>asyncio</code>{.xref .py .py-mod .docutils .literal
.notranslate}]{.pre}</a>{.reference
.external}-powered libraries.
:::</p>
<p>::: {#extending-scrapy .section}
[]{#id2}</p>
<h2 id="extending-scrapyheaderlink"><a class="header" href="#extending-scrapyheaderlink">Extending Scrapy<a href="#extending-scrapy" title="Permalink to this heading">¶</a>{.headerlink}</a></h2>
<p>::: {.toctree-wrapper .compound}
[]{#document-topics/architecture}</p>
<p>::: {#architecture-overview .section}
[]{#topics-architecture}</p>
<h3 id="architecture-overviewheaderlink"><a class="header" href="#architecture-overviewheaderlink">Architecture overview<a href="#architecture-overview" title="Permalink to this heading">¶</a>{.headerlink}</a></h3>
<p>This document describes the architecture of Scrapy and how its
components interact.</p>
<p>::: {#overview .section}</p>
<h4 id="overviewheaderlink"><a class="header" href="#overviewheaderlink">Overview<a href="#overview" title="Permalink to this heading">¶</a>{.headerlink}</a></h4>
<p>The following diagram shows an overview of the Scrapy architecture with
its components and an outline of the data flow that takes place inside
the system (shown by the red arrows). A brief description of the
components is included below with links for more detailed information
about them. The data flow is also described below.
:::</p>
<p>::: {#data-flow .section}
[]{#id1}</p>
<h4 id="data-flowheaderlink"><a class="header" href="#data-flowheaderlink">Data flow<a href="#data-flow" title="Permalink to this heading">¶</a>{.headerlink}</a></h4>
<p><a href="_images/scrapy_architecture_02.png"><img src="_images/scrapy_architecture_02.png" alt="Scrapy architecture" />{style=&quot;width: 700px; height: 470px;&quot;}</a>{.reference
.internal .image-reference}</p>
<p>The data flow in Scrapy is controlled by the execution engine, and goes
like this:</p>
<ol>
<li>
<p>The <a href="#component-engine">[Engine]{.std .std-ref}</a>{.hoverxref .tooltip
.reference .internal} gets the initial Requests to crawl from the
<a href="#component-spiders">[Spider]{.std .std-ref}</a>{.hoverxref .tooltip
.reference .internal}.</p>
</li>
<li>
<p>The <a href="#component-engine">[Engine]{.std .std-ref}</a>{.hoverxref .tooltip
.reference .internal} schedules the Requests in the
<a href="#component-scheduler">[Scheduler]{.std .std-ref}</a>{.hoverxref
.tooltip .reference .internal} and asks for the next Requests to
crawl.</p>
</li>
<li>
<p>The <a href="#component-scheduler">[Scheduler]{.std .std-ref}</a>{.hoverxref
.tooltip .reference .internal} returns the next Requests to the
<a href="#component-engine">[Engine]{.std .std-ref}</a>{.hoverxref .tooltip
.reference .internal}.</p>
</li>
<li>
<p>The <a href="#component-engine">[Engine]{.std .std-ref}</a>{.hoverxref .tooltip
.reference .internal} sends the Requests to the <a href="#component-downloader">[Downloader]{.std
.std-ref}</a>{.hoverxref .tooltip .reference
.internal}, passing through the <a href="#component-downloader-middleware">[Downloader Middlewares]{.std
.std-ref}</a>{.hoverxref .tooltip
.reference .internal} (see <a href="index.html#scrapy.downloadermiddlewares.DownloaderMiddleware.process_request" title="scrapy.downloadermiddlewares.DownloaderMiddleware.process_request">[<code>process_request()</code>{.xref .py .py-meth
.docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal}).</p>
</li>
<li>
<p>Once the page finishes downloading the <a href="#component-downloader">[Downloader]{.std
.std-ref}</a>{.hoverxref .tooltip .reference
.internal} generates a Response (with that page) and sends it to the
Engine, passing through the <a href="#component-downloader-middleware">[Downloader Middlewares]{.std
.std-ref}</a>{.hoverxref .tooltip
.reference .internal} (see <a href="index.html#scrapy.downloadermiddlewares.DownloaderMiddleware.process_response" title="scrapy.downloadermiddlewares.DownloaderMiddleware.process_response">[<code>process_response()</code>{.xref .py .py-meth
.docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal}).</p>
</li>
<li>
<p>The <a href="#component-engine">[Engine]{.std .std-ref}</a>{.hoverxref .tooltip
.reference .internal} receives the Response from the
<a href="#component-downloader">[Downloader]{.std .std-ref}</a>{.hoverxref
.tooltip .reference .internal} and sends it to the <a href="#component-spiders">[Spider]{.std
.std-ref}</a>{.hoverxref .tooltip .reference
.internal} for processing, passing through the <a href="#component-spider-middleware">[Spider
Middleware]{.std .std-ref}</a>{.hoverxref
.tooltip .reference .internal} (see <a href="index.html#scrapy.spidermiddlewares.SpiderMiddleware.process_spider_input" title="scrapy.spidermiddlewares.SpiderMiddleware.process_spider_input">[<code>process_spider_input()</code>{.xref
.py .py-meth .docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal}).</p>
</li>
<li>
<p>The <a href="#component-spiders">[Spider]{.std .std-ref}</a>{.hoverxref
.tooltip .reference .internal} processes the Response and returns
scraped items and new Requests (to follow) to the <a href="#component-engine">[Engine]{.std
.std-ref}</a>{.hoverxref .tooltip .reference
.internal}, passing through the <a href="#component-spider-middleware">[Spider Middleware]{.std
.std-ref}</a>{.hoverxref .tooltip
.reference .internal} (see <a href="index.html#scrapy.spidermiddlewares.SpiderMiddleware.process_spider_output" title="scrapy.spidermiddlewares.SpiderMiddleware.process_spider_output">[<code>process_spider_output()</code>{.xref .py
.py-meth .docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal}).</p>
</li>
<li>
<p>The <a href="#component-engine">[Engine]{.std .std-ref}</a>{.hoverxref .tooltip
.reference .internal} sends processed items to <a href="#component-pipelines">[Item
Pipelines]{.std .std-ref}</a>{.hoverxref .tooltip
.reference .internal}, then send processed Requests to the
<a href="#component-scheduler">[Scheduler]{.std .std-ref}</a>{.hoverxref
.tooltip .reference .internal} and asks for possible next Requests
to crawl.</p>
</li>
<li>
<p>The process repeats (from step 3) until there are no more requests
from the <a href="#component-scheduler">[Scheduler]{.std
.std-ref}</a>{.hoverxref .tooltip .reference
.internal}.
:::</p>
</li>
</ol>
<p>::: {#components .section}</p>
<h4 id="componentsheaderlink"><a class="header" href="#componentsheaderlink">Components<a href="#components" title="Permalink to this heading">¶</a>{.headerlink}</a></h4>
<p>::: {#scrapy-engine .section}
[]{#component-engine}</p>
<h5 id="scrapy-engineheaderlink"><a class="header" href="#scrapy-engineheaderlink">Scrapy Engine<a href="#scrapy-engine" title="Permalink to this heading">¶</a>{.headerlink}</a></h5>
<p>The engine is responsible for controlling the data flow between all
components of the system, and triggering events when certain actions
occur. See the <a href="#data-flow">[Data Flow]{.std .std-ref}</a>{.hoverxref
.tooltip .reference .internal} section above for more details.
:::</p>
<p>::: {#scheduler .section}
[]{#component-scheduler}</p>
<h5 id="schedulerheaderlink-1"><a class="header" href="#schedulerheaderlink-1">Scheduler<a href="#scheduler" title="Permalink to this heading">¶</a>{.headerlink}</a></h5>
<p>The <a href="index.html#topics-scheduler">[scheduler]{.std .std-ref}</a>{.hoverxref
.tooltip .reference .internal} receives requests from the engine and
enqueues them for feeding them later (also to the engine) when the
engine requests them.
:::</p>
<p>::: {#downloader .section}
[]{#component-downloader}</p>
<h5 id="downloaderheaderlink-1"><a class="header" href="#downloaderheaderlink-1">Downloader<a href="#downloader" title="Permalink to this heading">¶</a>{.headerlink}</a></h5>
<p>The Downloader is responsible for fetching web pages and feeding them to
the engine which, in turn, feeds them to the spiders.
:::</p>
<p>::: {#spiders .section}
[]{#component-spiders}</p>
<h5 id="spidersheaderlink-1"><a class="header" href="#spidersheaderlink-1">Spiders<a href="#spiders" title="Permalink to this heading">¶</a>{.headerlink}</a></h5>
<p>Spiders are custom classes written by Scrapy users to parse responses
and extract <a href="index.html#topics-items">[items]{.std .std-ref}</a>{.hoverxref
.tooltip .reference .internal} from them or additional requests to
follow. For more information see <a href="index.html#topics-spiders">[Spiders]{.std
.std-ref}</a>{.hoverxref .tooltip .reference
.internal}.
:::</p>
<p>::: {#item-pipeline .section}
[]{#component-pipelines}</p>
<h5 id="item-pipelineheaderlink-1"><a class="header" href="#item-pipelineheaderlink-1">Item Pipeline<a href="#item-pipeline" title="Permalink to this heading">¶</a>{.headerlink}</a></h5>
<p>The Item Pipeline is responsible for processing the items once they have
been extracted (or scraped) by the spiders. Typical tasks include
cleansing, validation and persistence (like storing the item in a
database). For more information see <a href="index.html#topics-item-pipeline">[Item Pipeline]{.std
.std-ref}</a>{.hoverxref .tooltip
.reference .internal}.
:::</p>
<p>::: {#downloader-middlewares .section}
[]{#component-downloader-middleware}</p>
<h5 id="downloader-middlewaresheaderlink"><a class="header" href="#downloader-middlewaresheaderlink">Downloader middlewares<a href="#downloader-middlewares" title="Permalink to this heading">¶</a>{.headerlink}</a></h5>
<p>Downloader middlewares are specific hooks that sit between the Engine
and the Downloader and process requests when they pass from the Engine
to the Downloader, and responses that pass from Downloader to the
Engine.</p>
<p>Use a Downloader middleware if you need to do one of the following:</p>
<ul>
<li>
<p>process a request just before it is sent to the Downloader (i.e.
right before Scrapy sends the request to the website);</p>
</li>
<li>
<p>change received response before passing it to a spider;</p>
</li>
<li>
<p>send a new Request instead of passing received response to a spider;</p>
</li>
<li>
<p>pass response to a spider without fetching a web page;</p>
</li>
<li>
<p>silently drop some requests.</p>
</li>
</ul>
<p>For more information see <a href="index.html#topics-downloader-middleware">[Downloader Middleware]{.std
.std-ref}</a>{.hoverxref .tooltip
.reference .internal}.
:::</p>
<p>::: {#spider-middlewares .section}
[]{#component-spider-middleware}</p>
<h5 id="spider-middlewaresheaderlink"><a class="header" href="#spider-middlewaresheaderlink">Spider middlewares<a href="#spider-middlewares" title="Permalink to this heading">¶</a>{.headerlink}</a></h5>
<p>Spider middlewares are specific hooks that sit between the Engine and
the Spiders and are able to process spider input (responses) and output
(items and requests).</p>
<p>Use a Spider middleware if you need to</p>
<ul>
<li>
<p>post-process output of spider callbacks - change/add/remove requests
or items;</p>
</li>
<li>
<p>post-process start_requests;</p>
</li>
<li>
<p>handle spider exceptions;</p>
</li>
<li>
<p>call errback instead of callback for some of the requests based on
response content.</p>
</li>
</ul>
<p>For more information see <a href="index.html#topics-spider-middleware">[Spider Middleware]{.std
.std-ref}</a>{.hoverxref .tooltip
.reference .internal}.
:::
:::</p>
<p>::: {#event-driven-networking .section}</p>
<h4 id="event-driven-networkingheaderlink"><a class="header" href="#event-driven-networkingheaderlink">Event-driven networking<a href="#event-driven-networking" title="Permalink to this heading">¶</a>{.headerlink}</a></h4>
<p>Scrapy is written with
<a href="https://twistedmatrix.com/trac/">Twisted</a>{.reference .external}, a
popular event-driven networking framework for Python. Thus, it's
implemented using a non-blocking (aka asynchronous) code for
concurrency.</p>
<p>For more information about asynchronous programming and Twisted see
these links:</p>
<ul>
<li>
<p><a href="https://docs.twisted.org/en/stable/core/howto/defer-intro.html" title="(in Twisted v23.10)">Introduction to
Deferreds</a>{.reference
.external}</p>
</li>
<li>
<p><a href="http://jessenoller.com/blog/2009/02/11/twisted-hello-asynchronous-programming/">Twisted - hello, asynchronous
programming</a>{.reference
.external}</p>
</li>
<li>
<p><a href="http://krondo.com/an-introduction-to-asynchronous-programming-and-twisted/">Twisted Introduction -
Krondo</a>{.reference
.external}
:::
:::</p>
</li>
</ul>
<p>[]{#document-topics/addons}</p>
<p>::: {#add-ons .section}
[]{#topics-addons}</p>
<h3 id="add-onsheaderlink"><a class="header" href="#add-onsheaderlink">Add-ons<a href="#add-ons" title="Permalink to this heading">¶</a>{.headerlink}</a></h3>
<p>Scrapy's add-on system is a framework which unifies managing and
configuring components that extend Scrapy's core functionality, such as
middlewares, extensions, or pipelines. It provides users with a
plug-and-play experience in Scrapy extension management, and grants
extensive configuration control to developers.</p>
<p>::: {#activating-and-configuring-add-ons .section}</p>
<h4 id="activating-and-configuring-add-onsheaderlink"><a class="header" href="#activating-and-configuring-add-onsheaderlink">Activating and configuring add-ons<a href="#activating-and-configuring-add-ons" title="Permalink to this heading">¶</a>{.headerlink}</a></h4>
<p>During <a href="index.html#scrapy.crawler.Crawler" title="scrapy.crawler.Crawler">[<code>Crawler</code>{.xref .py .py-class .docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal} initialization, the list of enabled add-ons is read from your
[<code>ADDONS</code>{.docutils .literal .notranslate}]{.pre} setting.</p>
<p>The [<code>ADDONS</code>{.docutils .literal .notranslate}]{.pre} setting is a dict
in which every key is an add-on class or its import path and the value
is its priority.</p>
<p>This is an example where two add-ons are enabled in a project's
[<code>settings.py</code>{.docutils .literal .notranslate}]{.pre}:</p>
<p>::: {.highlight-default .notranslate}
::: highlight
ADDONS = {
'path.to.someaddon': 0,
SomeAddonClass: 1,
}
:::
:::
:::</p>
<p>::: {#writing-your-own-add-ons .section}</p>
<h4 id="writing-your-own-add-onsheaderlink"><a class="header" href="#writing-your-own-add-onsheaderlink">Writing your own add-ons<a href="#writing-your-own-add-ons" title="Permalink to this heading">¶</a>{.headerlink}</a></h4>
<p>Add-ons are Python classes that include the following method:</p>
<p>[[update_settings]{.pre}]{.sig-name .descname}[(]{.sig-paren}<em>[[settings]{.pre}]{.n}</em>[)]{.sig-paren}<a href="#update_settings" title="Permalink to this definition">¶</a>{.headerlink}</p>
<p>:   This method is called during the initialization of the
<a href="index.html#scrapy.crawler.Crawler" title="scrapy.crawler.Crawler">[<code>Crawler</code>{.xref .py .py-class .docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal}. Here, you should perform dependency checks (e.g. for
external Python libraries) and update the <a href="index.html#scrapy.settings.Settings" title="scrapy.settings.Settings">[<code>Settings</code>{.xref .py
.py-class .docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal} object as wished, e.g. enable components for this add-on
or set required configuration of other extensions.</p>
<pre><code>Parameters

:   **settings** ([[`Settings`{.xref .py .py-class .docutils
    .literal
    .notranslate}]{.pre}](index.html#scrapy.settings.Settings &quot;scrapy.settings.Settings&quot;){.reference
    .internal}) -- The settings object storing Scrapy/component
    configuration
</code></pre>
<p>They can also have the following method:</p>
<p><em>[classmethod]{.pre}[ ]{.w}</em>[[from_crawler]{.pre}]{.sig-name .descname}[(]{.sig-paren}<em>[[cls]{.pre}]{.n}</em>, <em>[[crawler]{.pre}]{.n}</em>[)]{.sig-paren}</p>
<p>:   If present, this class method is called to create an add-on instance
from a <a href="index.html#scrapy.crawler.Crawler" title="scrapy.crawler.Crawler">[<code>Crawler</code>{.xref .py .py-class .docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal}. It must return a new instance of the add-on. The crawler
object provides access to all Scrapy core components like settings
and signals; it is a way for the add-on to access them and hook its
functionality into Scrapy.</p>
<pre><code>Parameters

:   **crawler** ([[`Crawler`{.xref .py .py-class .docutils .literal
    .notranslate}]{.pre}](index.html#scrapy.crawler.Crawler &quot;scrapy.crawler.Crawler&quot;){.reference
    .internal}) -- The crawler that uses this add-on
</code></pre>
<p>The settings set by the add-on should use the [<code>addon</code>{.docutils
.literal .notranslate}]{.pre} priority (see <a href="index.html#populating-settings">[Populating the
settings]{.std .std-ref}</a>{.hoverxref
.tooltip .reference .internal} and
<a href="index.html#scrapy.settings.BaseSettings.set" title="scrapy.settings.BaseSettings.set">[<code>scrapy.settings.BaseSettings.set()</code>{.xref .py .py-func .docutils
.literal
.notranslate}]{.pre}</a>{.reference
.internal}):</p>
<p>::: {.highlight-default .notranslate}
::: highlight
class MyAddon:
def update_settings(self, settings):
settings.set(&quot;DNSCACHE_ENABLED&quot;, True, &quot;addon&quot;)
:::
:::</p>
<p>This allows users to override these settings in the project or spider
configuration. This is not possible with settings that are mutable
objects, such as the dict that is a value of <a href="index.html#std-setting-ITEM_PIPELINES">[<code>ITEM_PIPELINES</code>{.xref
.std .std-setting .docutils .literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal}. In these cases you can provide an
add-on-specific setting that governs whether the add-on will modify
<a href="index.html#std-setting-ITEM_PIPELINES">[<code>ITEM_PIPELINES</code>{.xref .std .std-setting .docutils .literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal}:</p>
<p>::: {.highlight-default .notranslate}
::: highlight
class MyAddon:
def update_settings(self, settings):
if settings.getbool(&quot;MYADDON_ENABLE_PIPELINE&quot;):
settings[&quot;ITEM_PIPELINES&quot;][&quot;path.to.mypipeline&quot;] = 200
:::
:::</p>
<p>If the [<code>update_settings</code>{.docutils .literal .notranslate}]{.pre} method
raises <a href="index.html#scrapy.exceptions.NotConfigured" title="scrapy.exceptions.NotConfigured">[<code>scrapy.exceptions.NotConfigured</code>{.xref .py .py-exc .docutils
.literal
.notranslate}]{.pre}</a>{.reference
.internal}, the add-on will be skipped. This makes it easy to enable an
add-on only when some conditions are met.</p>
<p>::: {#fallbacks .section}</p>
<h5 id="fallbacksheaderlink"><a class="header" href="#fallbacksheaderlink">Fallbacks<a href="#fallbacks" title="Permalink to this heading">¶</a>{.headerlink}</a></h5>
<p>Some components provided by add-ons need to fall back to &quot;default&quot;
implementations, e.g. a custom download handler needs to send the
request that it doesn't handle via the default download handler, or a
stats collector that includes some additional processing but otherwise
uses the default stats collector. And it's possible that a project needs
to use several custom components of the same type, e.g. two custom
download handlers that support different kinds of custom requests and
still need to use the default download handler for other requests. To
make such use cases easier to configure, we recommend that such custom
components should be written in the following way:</p>
<ol>
<li>
<p>The custom component (e.g. [<code>MyDownloadHandler</code>{.docutils .literal
.notranslate}]{.pre}) shouldn't inherit from the default Scrapy one
(e.g.
[<code>scrapy.core.downloader.handlers.http.HTTPDownloadHandler</code>{.docutils
.literal .notranslate}]{.pre}), but instead be able to load the
class of the fallback component from a special setting (e.g.
[<code>MY_FALLBACK_DOWNLOAD_HANDLER</code>{.docutils .literal
.notranslate}]{.pre}), create an instance of it and use it.</p>
</li>
<li>
<p>The add-ons that include these components should read the current
value of the default setting (e.g. [<code>DOWNLOAD_HANDLERS</code>{.docutils
.literal .notranslate}]{.pre}) in their
[<code>update_settings()</code>{.docutils .literal .notranslate}]{.pre}
methods, save that value into the fallback setting
([<code>MY_FALLBACK_DOWNLOAD_HANDLER</code>{.docutils .literal
.notranslate}]{.pre} mentioned earlier) and set the default setting
to the component provided by the add-on (e.g.
[<code>MyDownloadHandler</code>{.docutils .literal .notranslate}]{.pre}). If
the fallback setting is already set by the user, they shouldn't
change it.</p>
</li>
<li>
<p>This way, if there are several add-ons that want to modify the same
setting, all of them will fallback to the component from the
previous one and then to the Scrapy default. The order of that
depends on the priority order in the [<code>ADDONS</code>{.docutils .literal
.notranslate}]{.pre} setting.
:::
:::</p>
</li>
</ol>
<p>::: {#add-on-examples .section}</p>
<h4 id="add-on-examplesheaderlink"><a class="header" href="#add-on-examplesheaderlink">Add-on examples<a href="#add-on-examples" title="Permalink to this heading">¶</a>{.headerlink}</a></h4>
<p>Set some basic configuration:</p>
<p>::: {.highlight-python .notranslate}
::: highlight
class MyAddon:
def update_settings(self, settings):
settings[&quot;ITEM_PIPELINES&quot;][&quot;path.to.mypipeline&quot;] = 200
settings.set(&quot;DNSCACHE_ENABLED&quot;, True, &quot;addon&quot;)
:::
:::</p>
<p>Check dependencies:</p>
<p>::: {.highlight-python .notranslate}
::: highlight
class MyAddon:
def update_settings(self, settings):
try:
import boto
except ImportError:
raise NotConfigured(&quot;MyAddon requires the boto library&quot;)
...
:::
:::</p>
<p>Access the crawler instance:</p>
<p>::: {.highlight-python .notranslate}
::: highlight
class MyAddon:
def <strong>init</strong>(self, crawler) -&gt; None:
super().<strong>init</strong>()
self.crawler = crawler</p>
<pre><code>    @classmethod
    def from_crawler(cls, crawler):
        return cls(crawler)

    def update_settings(self, settings):
        ...
</code></pre>
<p>:::
:::</p>
<p>Use a fallback component:</p>
<p>::: {.highlight-python .notranslate}
::: highlight
from scrapy.core.downloader.handlers.http import HTTPDownloadHandler</p>
<pre><code>FALLBACK_SETTING = &quot;MY_FALLBACK_DOWNLOAD_HANDLER&quot;


class MyHandler:
    lazy = False

    def __init__(self, settings, crawler):
        dhcls = load_object(settings.get(FALLBACK_SETTING))
        self._fallback_handler = create_instance(
            dhcls,
            settings=None,
            crawler=crawler,
        )

    def download_request(self, request, spider):
        if request.meta.get(&quot;my_params&quot;):
            # handle the request
            ...
        else:
            return self._fallback_handler.download_request(request, spider)


class MyAddon:
    def update_settings(self, settings):
        if not settings.get(FALLBACK_SETTING):
            settings.set(
                FALLBACK_SETTING,
                settings.getwithbase(&quot;DOWNLOAD_HANDLERS&quot;)[&quot;https&quot;],
                &quot;addon&quot;,
            )
        settings[&quot;DOWNLOAD_HANDLERS&quot;][&quot;https&quot;] = MyHandler
</code></pre>
<p>:::
:::
:::
:::</p>
<p>[]{#document-topics/downloader-middleware}</p>
<p>::: {#downloader-middleware .section}
[]{#topics-downloader-middleware}</p>
<h3 id="downloader-middlewareheaderlink"><a class="header" href="#downloader-middlewareheaderlink">Downloader Middleware<a href="#downloader-middleware" title="Permalink to this heading">¶</a>{.headerlink}</a></h3>
<p>The downloader middleware is a framework of hooks into Scrapy's
request/response processing. It's a light, low-level system for globally
altering Scrapy's requests and responses.</p>
<p>::: {#activating-a-downloader-middleware .section}
[]{#topics-downloader-middleware-setting}</p>
<h4 id="activating-a-downloader-middlewareheaderlink"><a class="header" href="#activating-a-downloader-middlewareheaderlink">Activating a downloader middleware<a href="#activating-a-downloader-middleware" title="Permalink to this heading">¶</a>{.headerlink}</a></h4>
<p>To activate a downloader middleware component, add it to the
<a href="index.html#std-setting-DOWNLOADER_MIDDLEWARES">[<code>DOWNLOADER_MIDDLEWARES</code>{.xref .std .std-setting .docutils .literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal} setting, which is a dict whose keys are
the middleware class paths and their values are the middleware orders.</p>
<p>Here's an example:</p>
<p>::: {.highlight-python .notranslate}
::: highlight
DOWNLOADER_MIDDLEWARES = {
&quot;myproject.middlewares.CustomDownloaderMiddleware&quot;: 543,
}
:::
:::</p>
<p>The <a href="index.html#std-setting-DOWNLOADER_MIDDLEWARES">[<code>DOWNLOADER_MIDDLEWARES</code>{.xref .std .std-setting .docutils
.literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal} setting is merged with the
<a href="index.html#std-setting-DOWNLOADER_MIDDLEWARES_BASE">[<code>DOWNLOADER_MIDDLEWARES_BASE</code>{.xref .std .std-setting .docutils
.literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal} setting defined in Scrapy (and not meant
to be overridden) and then sorted by order to get the final sorted list
of enabled middlewares: the first middleware is the one closer to the
engine and the last is the one closer to the downloader. In other words,
the <a href="#scrapy.downloadermiddlewares.DownloaderMiddleware.process_request" title="scrapy.downloadermiddlewares.DownloaderMiddleware.process_request">[<code>process_request()</code>{.xref .py .py-meth .docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal} method of each middleware will be invoked in increasing
middleware order (100, 200, 300, ...) and the
<a href="#scrapy.downloadermiddlewares.DownloaderMiddleware.process_response" title="scrapy.downloadermiddlewares.DownloaderMiddleware.process_response">[<code>process_response()</code>{.xref .py .py-meth .docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal} method of each middleware will be invoked in decreasing
order.</p>
<p>To decide which order to assign to your middleware see the
<a href="index.html#std-setting-DOWNLOADER_MIDDLEWARES_BASE">[<code>DOWNLOADER_MIDDLEWARES_BASE</code>{.xref .std .std-setting .docutils
.literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal} setting and pick a value according to
where you want to insert the middleware. The order does matter because
each middleware performs a different action and your middleware could
depend on some previous (or subsequent) middleware being applied.</p>
<p>If you want to disable a built-in middleware (the ones defined in
<a href="index.html#std-setting-DOWNLOADER_MIDDLEWARES_BASE">[<code>DOWNLOADER_MIDDLEWARES_BASE</code>{.xref .std .std-setting .docutils
.literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal} and enabled by default) you must define
it in your project's <a href="index.html#std-setting-DOWNLOADER_MIDDLEWARES">[<code>DOWNLOADER_MIDDLEWARES</code>{.xref .std .std-setting
.docutils .literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal} setting and assign [<code>None</code>{.docutils
.literal .notranslate}]{.pre} as its value. For example, if you want to
disable the user-agent middleware:</p>
<p>::: {.highlight-python .notranslate}
::: highlight
DOWNLOADER_MIDDLEWARES = {
&quot;myproject.middlewares.CustomDownloaderMiddleware&quot;: 543,
&quot;scrapy.downloadermiddlewares.useragent.UserAgentMiddleware&quot;: None,
}
:::
:::</p>
<p>Finally, keep in mind that some middlewares may need to be enabled
through a particular setting. See each middleware documentation for more
info.
:::</p>
<p>::: {#writing-your-own-downloader-middleware .section}
[]{#topics-downloader-middleware-custom}</p>
<h4 id="writing-your-own-downloader-middlewareheaderlink"><a class="header" href="#writing-your-own-downloader-middlewareheaderlink">Writing your own downloader middleware<a href="#writing-your-own-downloader-middleware" title="Permalink to this heading">¶</a>{.headerlink}</a></h4>
<p>Each downloader middleware is a Python class that defines one or more of
the methods defined below.</p>
<p>The main entry point is the [<code>from_crawler</code>{.docutils .literal
.notranslate}]{.pre} class method, which receives a <a href="index.html#scrapy.crawler.Crawler" title="scrapy.crawler.Crawler">[<code>Crawler</code>{.xref
.py .py-class .docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal} instance. The <a href="index.html#scrapy.crawler.Crawler" title="scrapy.crawler.Crawler">[<code>Crawler</code>{.xref .py .py-class .docutils
.literal
.notranslate}]{.pre}</a>{.reference
.internal} object gives you access, for example, to the <a href="index.html#topics-settings">[settings]{.std
.std-ref}</a>{.hoverxref .tooltip .reference
.internal}.</p>
<p>[]{#module-scrapy.downloadermiddlewares .target}</p>
<p><em>[class]{.pre}[ ]{.w}</em>[[scrapy.downloadermiddlewares.]{.pre}]{.sig-prename .descclassname}[[DownloaderMiddleware]{.pre}]{.sig-name .descname}<a href="#scrapy.downloadermiddlewares.DownloaderMiddleware" title="Permalink to this definition">¶</a>{.headerlink}</p>
<p>:   ::: {.admonition .note}
Note</p>
<pre><code>Any of the downloader middleware methods may also return a deferred.
:::

[[process_request]{.pre}]{.sig-name .descname}[(]{.sig-paren}*[[request]{.pre}]{.n}*, *[[spider]{.pre}]{.n}*[)]{.sig-paren}[¶](#scrapy.downloadermiddlewares.DownloaderMiddleware.process_request &quot;Permalink to this definition&quot;){.headerlink}

:   This method is called for each request that goes through the
    download middleware.

    [[`process_request()`{.xref .py .py-meth .docutils .literal
    .notranslate}]{.pre}](#scrapy.downloadermiddlewares.DownloaderMiddleware.process_request &quot;scrapy.downloadermiddlewares.DownloaderMiddleware.process_request&quot;){.reference
    .internal} should either: return [`None`{.docutils .literal
    .notranslate}]{.pre}, return a [`Response`{.xref .py .py-class
    .docutils .literal .notranslate}]{.pre} object, return a
    [[`Request`{.xref .py .py-class .docutils .literal
    .notranslate}]{.pre}](index.html#scrapy.http.Request &quot;scrapy.http.Request&quot;){.reference
    .internal} object, or raise [[`IgnoreRequest`{.xref .py .py-exc
    .docutils .literal
    .notranslate}]{.pre}](index.html#scrapy.exceptions.IgnoreRequest &quot;scrapy.exceptions.IgnoreRequest&quot;){.reference
    .internal}.

    If it returns [`None`{.docutils .literal .notranslate}]{.pre},
    Scrapy will continue processing this request, executing all
    other middlewares until, finally, the appropriate downloader
    handler is called the request performed (and its response
    downloaded).

    If it returns a [[`Response`{.xref .py .py-class .docutils
    .literal
    .notranslate}]{.pre}](index.html#scrapy.http.Response &quot;scrapy.http.Response&quot;){.reference
    .internal} object, Scrapy won't bother calling *any* other
    [[`process_request()`{.xref .py .py-meth .docutils .literal
    .notranslate}]{.pre}](#scrapy.downloadermiddlewares.DownloaderMiddleware.process_request &quot;scrapy.downloadermiddlewares.DownloaderMiddleware.process_request&quot;){.reference
    .internal} or [[`process_exception()`{.xref .py .py-meth
    .docutils .literal
    .notranslate}]{.pre}](#scrapy.downloadermiddlewares.DownloaderMiddleware.process_exception &quot;scrapy.downloadermiddlewares.DownloaderMiddleware.process_exception&quot;){.reference
    .internal} methods, or the appropriate download function; it'll
    return that response. The [[`process_response()`{.xref .py
    .py-meth .docutils .literal
    .notranslate}]{.pre}](#scrapy.downloadermiddlewares.DownloaderMiddleware.process_response &quot;scrapy.downloadermiddlewares.DownloaderMiddleware.process_response&quot;){.reference
    .internal} methods of installed middleware is always called on
    every response.

    If it returns a [`Request`{.xref .py .py-class .docutils
    .literal .notranslate}]{.pre} object, Scrapy will stop calling
    [[`process_request()`{.xref .py .py-meth .docutils .literal
    .notranslate}]{.pre}](#scrapy.downloadermiddlewares.DownloaderMiddleware.process_request &quot;scrapy.downloadermiddlewares.DownloaderMiddleware.process_request&quot;){.reference
    .internal} methods and reschedule the returned request. Once the
    newly returned request is performed, the appropriate middleware
    chain will be called on the downloaded response.

    If it raises an [[`IgnoreRequest`{.xref .py .py-exc .docutils
    .literal
    .notranslate}]{.pre}](index.html#scrapy.exceptions.IgnoreRequest &quot;scrapy.exceptions.IgnoreRequest&quot;){.reference
    .internal} exception, the [[`process_exception()`{.xref .py
    .py-meth .docutils .literal
    .notranslate}]{.pre}](#scrapy.downloadermiddlewares.DownloaderMiddleware.process_exception &quot;scrapy.downloadermiddlewares.DownloaderMiddleware.process_exception&quot;){.reference
    .internal} methods of installed downloader middleware will be
    called. If none of them handle the exception, the errback
    function of the request ([`Request.errback`{.docutils .literal
    .notranslate}]{.pre}) is called. If no code handles the raised
    exception, it is ignored and not logged (unlike other
    exceptions).

    Parameters

    :   -   **request** ([`Request`{.xref .py .py-class .docutils
            .literal .notranslate}]{.pre} object) -- the request
            being processed

        -   **spider** ([[`Spider`{.xref .py .py-class .docutils
            .literal
            .notranslate}]{.pre}](index.html#scrapy.Spider &quot;scrapy.Spider&quot;){.reference
            .internal} object) -- the spider for which this request
            is intended

[[process_response]{.pre}]{.sig-name .descname}[(]{.sig-paren}*[[request]{.pre}]{.n}*, *[[response]{.pre}]{.n}*, *[[spider]{.pre}]{.n}*[)]{.sig-paren}[¶](#scrapy.downloadermiddlewares.DownloaderMiddleware.process_response &quot;Permalink to this definition&quot;){.headerlink}

:   [[`process_response()`{.xref .py .py-meth .docutils .literal
    .notranslate}]{.pre}](#scrapy.downloadermiddlewares.DownloaderMiddleware.process_response &quot;scrapy.downloadermiddlewares.DownloaderMiddleware.process_response&quot;){.reference
    .internal} should either: return a [[`Response`{.xref .py
    .py-class .docutils .literal
    .notranslate}]{.pre}](index.html#scrapy.http.Response &quot;scrapy.http.Response&quot;){.reference
    .internal} object, return a [`Request`{.xref .py .py-class
    .docutils .literal .notranslate}]{.pre} object or raise a
    [[`IgnoreRequest`{.xref .py .py-exc .docutils .literal
    .notranslate}]{.pre}](index.html#scrapy.exceptions.IgnoreRequest &quot;scrapy.exceptions.IgnoreRequest&quot;){.reference
    .internal} exception.

    If it returns a [[`Response`{.xref .py .py-class .docutils
    .literal
    .notranslate}]{.pre}](index.html#scrapy.http.Response &quot;scrapy.http.Response&quot;){.reference
    .internal} (it could be the same given response, or a brand-new
    one), that response will continue to be processed with the
    [[`process_response()`{.xref .py .py-meth .docutils .literal
    .notranslate}]{.pre}](#scrapy.downloadermiddlewares.DownloaderMiddleware.process_response &quot;scrapy.downloadermiddlewares.DownloaderMiddleware.process_response&quot;){.reference
    .internal} of the next middleware in the chain.

    If it returns a [`Request`{.xref .py .py-class .docutils
    .literal .notranslate}]{.pre} object, the middleware chain is
    halted and the returned request is rescheduled to be downloaded
    in the future. This is the same behavior as if a request is
    returned from [[`process_request()`{.xref .py .py-meth .docutils
    .literal
    .notranslate}]{.pre}](#scrapy.downloadermiddlewares.DownloaderMiddleware.process_request &quot;scrapy.downloadermiddlewares.DownloaderMiddleware.process_request&quot;){.reference
    .internal}.

    If it raises an [[`IgnoreRequest`{.xref .py .py-exc .docutils
    .literal
    .notranslate}]{.pre}](index.html#scrapy.exceptions.IgnoreRequest &quot;scrapy.exceptions.IgnoreRequest&quot;){.reference
    .internal} exception, the errback function of the request
    ([`Request.errback`{.docutils .literal .notranslate}]{.pre}) is
    called. If no code handles the raised exception, it is ignored
    and not logged (unlike other exceptions).

    Parameters

    :   -   **request** (is a [`Request`{.xref .py .py-class
            .docutils .literal .notranslate}]{.pre} object) -- the
            request that originated the response

        -   **response** ([[`Response`{.xref .py .py-class .docutils
            .literal
            .notranslate}]{.pre}](index.html#scrapy.http.Response &quot;scrapy.http.Response&quot;){.reference
            .internal} object) -- the response being processed

        -   **spider** ([[`Spider`{.xref .py .py-class .docutils
            .literal
            .notranslate}]{.pre}](index.html#scrapy.Spider &quot;scrapy.Spider&quot;){.reference
            .internal} object) -- the spider for which this response
            is intended

[[process_exception]{.pre}]{.sig-name .descname}[(]{.sig-paren}*[[request]{.pre}]{.n}*, *[[exception]{.pre}]{.n}*, *[[spider]{.pre}]{.n}*[)]{.sig-paren}[¶](#scrapy.downloadermiddlewares.DownloaderMiddleware.process_exception &quot;Permalink to this definition&quot;){.headerlink}

:   Scrapy calls [[`process_exception()`{.xref .py .py-meth
    .docutils .literal
    .notranslate}]{.pre}](#scrapy.downloadermiddlewares.DownloaderMiddleware.process_exception &quot;scrapy.downloadermiddlewares.DownloaderMiddleware.process_exception&quot;){.reference
    .internal} when a download handler or a
    [[`process_request()`{.xref .py .py-meth .docutils .literal
    .notranslate}]{.pre}](#scrapy.downloadermiddlewares.DownloaderMiddleware.process_request &quot;scrapy.downloadermiddlewares.DownloaderMiddleware.process_request&quot;){.reference
    .internal} (from a downloader middleware) raises an exception
    (including an [[`IgnoreRequest`{.xref .py .py-exc .docutils
    .literal
    .notranslate}]{.pre}](index.html#scrapy.exceptions.IgnoreRequest &quot;scrapy.exceptions.IgnoreRequest&quot;){.reference
    .internal} exception)

    [[`process_exception()`{.xref .py .py-meth .docutils .literal
    .notranslate}]{.pre}](#scrapy.downloadermiddlewares.DownloaderMiddleware.process_exception &quot;scrapy.downloadermiddlewares.DownloaderMiddleware.process_exception&quot;){.reference
    .internal} should return: either [`None`{.docutils .literal
    .notranslate}]{.pre}, a [[`Response`{.xref .py .py-class
    .docutils .literal
    .notranslate}]{.pre}](index.html#scrapy.http.Response &quot;scrapy.http.Response&quot;){.reference
    .internal} object, or a [`Request`{.xref .py .py-class .docutils
    .literal .notranslate}]{.pre} object.

    If it returns [`None`{.docutils .literal .notranslate}]{.pre},
    Scrapy will continue processing this exception, executing any
    other [[`process_exception()`{.xref .py .py-meth .docutils
    .literal
    .notranslate}]{.pre}](#scrapy.downloadermiddlewares.DownloaderMiddleware.process_exception &quot;scrapy.downloadermiddlewares.DownloaderMiddleware.process_exception&quot;){.reference
    .internal} methods of installed middleware, until no middleware
    is left and the default exception handling kicks in.

    If it returns a [[`Response`{.xref .py .py-class .docutils
    .literal
    .notranslate}]{.pre}](index.html#scrapy.http.Response &quot;scrapy.http.Response&quot;){.reference
    .internal} object, the [[`process_response()`{.xref .py .py-meth
    .docutils .literal
    .notranslate}]{.pre}](#scrapy.downloadermiddlewares.DownloaderMiddleware.process_response &quot;scrapy.downloadermiddlewares.DownloaderMiddleware.process_response&quot;){.reference
    .internal} method chain of installed middleware is started, and
    Scrapy won't bother calling any other
    [[`process_exception()`{.xref .py .py-meth .docutils .literal
    .notranslate}]{.pre}](#scrapy.downloadermiddlewares.DownloaderMiddleware.process_exception &quot;scrapy.downloadermiddlewares.DownloaderMiddleware.process_exception&quot;){.reference
    .internal} methods of middleware.

    If it returns a [`Request`{.xref .py .py-class .docutils
    .literal .notranslate}]{.pre} object, the returned request is
    rescheduled to be downloaded in the future. This stops the
    execution of [[`process_exception()`{.xref .py .py-meth
    .docutils .literal
    .notranslate}]{.pre}](#scrapy.downloadermiddlewares.DownloaderMiddleware.process_exception &quot;scrapy.downloadermiddlewares.DownloaderMiddleware.process_exception&quot;){.reference
    .internal} methods of the middleware the same as returning a
    response would.

    Parameters

    :   -   **request** (is a [`Request`{.xref .py .py-class
            .docutils .literal .notranslate}]{.pre} object) -- the
            request that generated the exception

        -   **exception** (an [`Exception`{.docutils .literal
            .notranslate}]{.pre} object) -- the raised exception

        -   **spider** ([[`Spider`{.xref .py .py-class .docutils
            .literal
            .notranslate}]{.pre}](index.html#scrapy.Spider &quot;scrapy.Spider&quot;){.reference
            .internal} object) -- the spider for which this request
            is intended

[[from_crawler]{.pre}]{.sig-name .descname}[(]{.sig-paren}*[[cls]{.pre}]{.n}*, *[[crawler]{.pre}]{.n}*[)]{.sig-paren}[¶](#scrapy.downloadermiddlewares.DownloaderMiddleware.from_crawler &quot;Permalink to this definition&quot;){.headerlink}

:   If present, this classmethod is called to create a middleware
    instance from a [[`Crawler`{.xref .py .py-class .docutils
    .literal
    .notranslate}]{.pre}](index.html#scrapy.crawler.Crawler &quot;scrapy.crawler.Crawler&quot;){.reference
    .internal}. It must return a new instance of the middleware.
    Crawler object provides access to all Scrapy core components
    like settings and signals; it is a way for middleware to access
    them and hook its functionality into Scrapy.

    Parameters

    :   **crawler** ([[`Crawler`{.xref .py .py-class .docutils
        .literal
        .notranslate}]{.pre}](index.html#scrapy.crawler.Crawler &quot;scrapy.crawler.Crawler&quot;){.reference
        .internal} object) -- crawler that uses this middleware
</code></pre>
<p>:::</p>
<p>::: {#built-in-downloader-middleware-reference .section}
[]{#topics-downloader-middleware-ref}</p>
<h4 id="built-in-downloader-middleware-referenceheaderlink"><a class="header" href="#built-in-downloader-middleware-referenceheaderlink">Built-in downloader middleware reference<a href="#built-in-downloader-middleware-reference" title="Permalink to this heading">¶</a>{.headerlink}</a></h4>
<p>This page describes all downloader middleware components that come with
Scrapy. For information on how to use them and how to write your own
downloader middleware, see the <a href="#topics-downloader-middleware">[downloader middleware usage guide]{.std
.std-ref}</a>{.hoverxref .tooltip .reference
.internal}.</p>
<p>For a list of the components enabled by default (and their orders) see
the <a href="index.html#std-setting-DOWNLOADER_MIDDLEWARES_BASE">[<code>DOWNLOADER_MIDDLEWARES_BASE</code>{.xref .std .std-setting .docutils
.literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal} setting.</p>
<p>::: {#module-scrapy.downloadermiddlewares.cookies .section}
[]{#cookiesmiddleware}[]{#cookies-mw}</p>
<h5 id="cookiesmiddlewareheaderlink"><a class="header" href="#cookiesmiddlewareheaderlink">CookiesMiddleware<a href="#module-scrapy.downloadermiddlewares.cookies" title="Permalink to this heading">¶</a>{.headerlink}</a></h5>
<p><em>[class]{.pre}[ ]{.w}</em>[[scrapy.downloadermiddlewares.cookies.]{.pre}]{.sig-prename .descclassname}[[CookiesMiddleware]{.pre}]{.sig-name .descname}<a href="_modules/scrapy/downloadermiddlewares/cookies.html#CookiesMiddleware">[[[source]]{.pre}]{.viewcode-link}</a>{.reference .internal}<a href="#scrapy.downloadermiddlewares.cookies.CookiesMiddleware" title="Permalink to this definition">¶</a>{.headerlink}</p>
<p>:   This middleware enables working with sites that require cookies,
such as those that use sessions. It keeps track of cookies sent by
web servers, and sends them back on subsequent requests (from that
spider), just like web browsers do.</p>
<pre><code>::: {.admonition .caution}
Caution

When non-UTF8 encoded byte sequences are passed to a
[`Request`{.xref .py .py-class .docutils .literal
.notranslate}]{.pre}, the [`CookiesMiddleware`{.docutils .literal
.notranslate}]{.pre} will log a warning. Refer to [[Advanced
customization]{.std
.std-ref}](index.html#topics-logging-advanced-customization){.hoverxref
.tooltip .reference .internal} to customize the logging behaviour.
:::

::: {.admonition .caution}
Caution

Cookies set via the [`Cookie`{.docutils .literal
.notranslate}]{.pre} header are not considered by the
[[CookiesMiddleware]{.std .std-ref}](#cookies-mw){.hoverxref
.tooltip .reference .internal}. If you need to set cookies for a
request, use the [`Request.cookies`{.xref .py .py-class .docutils
.literal .notranslate}]{.pre} parameter. This is a known current
limitation that is being worked on.
:::
</code></pre>
<p>The following settings can be used to configure the cookie middleware:</p>
<ul>
<li>
<p><a href="#std-setting-COOKIES_ENABLED">[<code>COOKIES_ENABLED</code>{.xref .std .std-setting .docutils .literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal}</p>
</li>
<li>
<p><a href="#std-setting-COOKIES_DEBUG">[<code>COOKIES_DEBUG</code>{.xref .std .std-setting .docutils .literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal}</p>
</li>
</ul>
<p>::: {#multiple-cookie-sessions-per-spider .section}
[]{#std-reqmeta-cookiejar}</p>
<h6 id="multiple-cookie-sessions-per-spiderheaderlink"><a class="header" href="#multiple-cookie-sessions-per-spiderheaderlink">Multiple cookie sessions per spider<a href="#multiple-cookie-sessions-per-spider" title="Permalink to this heading">¶</a>{.headerlink}</a></h6>
<p>There is support for keeping multiple cookie sessions per spider by
using the <a href="#std-reqmeta-cookiejar">[<code>cookiejar</code>{.xref .std .std-reqmeta .docutils .literal
.notranslate}]{.pre}</a>{.hoverxref .tooltip
.reference .internal} Request meta key. By default it uses a single
cookie jar (session), but you can pass an identifier to use different
ones.</p>
<p>For example:</p>
<p>::: {.highlight-python .notranslate}
::: highlight
for i, url in enumerate(urls):
yield scrapy.Request(url, meta={&quot;cookiejar&quot;: i}, callback=self.parse_page)
:::
:::</p>
<p>Keep in mind that the <a href="#std-reqmeta-cookiejar">[<code>cookiejar</code>{.xref .std .std-reqmeta .docutils
.literal .notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal} meta key is not &quot;sticky&quot;. You need to
keep passing it along on subsequent requests. For example:</p>
<p>::: {.highlight-python .notranslate}
::: highlight
def parse_page(self, response):
# do some processing
return scrapy.Request(
&quot;http://www.example.com/otherpage&quot;,
meta={&quot;cookiejar&quot;: response.meta[&quot;cookiejar&quot;]},
callback=self.parse_other_page,
)
:::
:::
:::</p>
<p>::: {#cookies-enabled .section}
[]{#std-setting-COOKIES_ENABLED}</p>
<h6 id="cookies_enabledheaderlink"><a class="header" href="#cookies_enabledheaderlink">COOKIES_ENABLED<a href="#cookies-enabled" title="Permalink to this heading">¶</a>{.headerlink}</a></h6>
<p>Default: [<code>True</code>{.docutils .literal .notranslate}]{.pre}</p>
<p>Whether to enable the cookies middleware. If disabled, no cookies will
be sent to web servers.</p>
<p>Notice that despite the value of <a href="#std-setting-COOKIES_ENABLED">[<code>COOKIES_ENABLED</code>{.xref .std
.std-setting .docutils .literal
.notranslate}]{.pre}</a>{.hoverxref .tooltip
.reference .internal} setting if [<code>Request.</code>{.docutils .literal
.notranslate}]{.pre}<a href="index.html#std-reqmeta-dont_merge_cookies">[<code>meta['dont_merge_cookies']</code>{.xref .std
.std-reqmeta .docutils .literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal} evaluates to [<code>True</code>{.docutils .literal
.notranslate}]{.pre} the request cookies will <strong>not</strong> be sent to the web
server and received cookies in <a href="index.html#scrapy.http.Response" title="scrapy.http.Response">[<code>Response</code>{.xref .py .py-class
.docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal} will <strong>not</strong> be merged with the existing cookies.</p>
<p>For more detailed information see the [<code>cookies</code>{.docutils .literal
.notranslate}]{.pre} parameter in [<code>Request</code>{.xref .py .py-class
.docutils .literal .notranslate}]{.pre}.
:::</p>
<p>::: {#cookies-debug .section}
[]{#std-setting-COOKIES_DEBUG}</p>
<h6 id="cookies_debugheaderlink"><a class="header" href="#cookies_debugheaderlink">COOKIES_DEBUG<a href="#cookies-debug" title="Permalink to this heading">¶</a>{.headerlink}</a></h6>
<p>Default: [<code>False</code>{.docutils .literal .notranslate}]{.pre}</p>
<p>If enabled, Scrapy will log all cookies sent in requests (i.e.
[<code>Cookie</code>{.docutils .literal .notranslate}]{.pre} header) and all
cookies received in responses (i.e. [<code>Set-Cookie</code>{.docutils .literal
.notranslate}]{.pre} header).</p>
<p>Here's an example of a log with <a href="#std-setting-COOKIES_DEBUG">[<code>COOKIES_DEBUG</code>{.xref .std
.std-setting .docutils .literal
.notranslate}]{.pre}</a>{.hoverxref .tooltip
.reference .internal} enabled:</p>
<p>::: {.highlight-default .notranslate}
::: highlight
2011-04-06 14:35:10-0300 [scrapy.core.engine] INFO: Spider opened
2011-04-06 14:35:10-0300 [scrapy.downloadermiddlewares.cookies] DEBUG: Sending cookies to: &lt;GET http://www.diningcity.com/netherlands/index.html&gt;
Cookie: clientlanguage_nl=en_EN
2011-04-06 14:35:14-0300 [scrapy.downloadermiddlewares.cookies] DEBUG: Received cookies from: &lt;200 http://www.diningcity.com/netherlands/index.html&gt;
Set-Cookie: JSESSIONID=B~FA4DC0C496C8762AE4F1A620EAB34F38; Path=/
Set-Cookie: ip_isocode=US
Set-Cookie: clientlanguage_nl=en_EN; Expires=Thu, 07-Apr-2011 21:21:34 GMT; Path=/
2011-04-06 14:49:50-0300 [scrapy.core.engine] DEBUG: Crawled (200) &lt;GET http://www.diningcity.com/netherlands/index.html&gt; (referer: None)
[...]
:::
:::
:::
:::</p>
<p>::: {#module-scrapy.downloadermiddlewares.defaultheaders .section}
[]{#defaultheadersmiddleware}</p>
<h5 id="defaultheadersmiddlewareheaderlink"><a class="header" href="#defaultheadersmiddlewareheaderlink">DefaultHeadersMiddleware<a href="#module-scrapy.downloadermiddlewares.defaultheaders" title="Permalink to this heading">¶</a>{.headerlink}</a></h5>
<p><em>[class]{.pre}[ ]{.w}</em>[[scrapy.downloadermiddlewares.defaultheaders.]{.pre}]{.sig-prename .descclassname}[[DefaultHeadersMiddleware]{.pre}]{.sig-name .descname}<a href="_modules/scrapy/downloadermiddlewares/defaultheaders.html#DefaultHeadersMiddleware">[[[source]]{.pre}]{.viewcode-link}</a>{.reference .internal}<a href="#scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware" title="Permalink to this definition">¶</a>{.headerlink}</p>
<p>:   This middleware sets all default requests headers specified in the
<a href="index.html#std-setting-DEFAULT_REQUEST_HEADERS">[<code>DEFAULT_REQUEST_HEADERS</code>{.xref .std .std-setting .docutils
.literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal} setting.
:::</p>
<p>::: {#module-scrapy.downloadermiddlewares.downloadtimeout .section}
[]{#downloadtimeoutmiddleware}</p>
<h5 id="downloadtimeoutmiddlewareheaderlink"><a class="header" href="#downloadtimeoutmiddlewareheaderlink">DownloadTimeoutMiddleware<a href="#module-scrapy.downloadermiddlewares.downloadtimeout" title="Permalink to this heading">¶</a>{.headerlink}</a></h5>
<p><em>[class]{.pre}[ ]{.w}</em>[[scrapy.downloadermiddlewares.downloadtimeout.]{.pre}]{.sig-prename .descclassname}[[DownloadTimeoutMiddleware]{.pre}]{.sig-name .descname}<a href="_modules/scrapy/downloadermiddlewares/downloadtimeout.html#DownloadTimeoutMiddleware">[[[source]]{.pre}]{.viewcode-link}</a>{.reference .internal}<a href="#scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware" title="Permalink to this definition">¶</a>{.headerlink}</p>
<p>:   This middleware sets the download timeout for requests specified in
the <a href="index.html#std-setting-DOWNLOAD_TIMEOUT">[<code>DOWNLOAD_TIMEOUT</code>{.xref .std .std-setting .docutils .literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal} setting or [<code>download_timeout</code>{.xref
.py .py-attr .docutils .literal .notranslate}]{.pre} spider
attribute.</p>
<p>::: {.admonition .note}
Note</p>
<p>You can also set download timeout per-request using
<a href="index.html#std-reqmeta-download_timeout">[<code>download_timeout</code>{.xref .std .std-reqmeta .docutils .literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal} Request.meta key; this is supported even
when DownloadTimeoutMiddleware is disabled.
:::
:::</p>
<p>::: {#module-scrapy.downloadermiddlewares.httpauth .section}
[]{#httpauthmiddleware}</p>
<h5 id="httpauthmiddlewareheaderlink"><a class="header" href="#httpauthmiddlewareheaderlink">HttpAuthMiddleware<a href="#module-scrapy.downloadermiddlewares.httpauth" title="Permalink to this heading">¶</a>{.headerlink}</a></h5>
<p><em>[class]{.pre}[ ]{.w}</em>[[scrapy.downloadermiddlewares.httpauth.]{.pre}]{.sig-prename .descclassname}[[HttpAuthMiddleware]{.pre}]{.sig-name .descname}<a href="_modules/scrapy/downloadermiddlewares/httpauth.html#HttpAuthMiddleware">[[[source]]{.pre}]{.viewcode-link}</a>{.reference .internal}<a href="#scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware" title="Permalink to this definition">¶</a>{.headerlink}</p>
<p>:   This middleware authenticates all requests generated from certain
spiders using <a href="https://en.wikipedia.org/wiki/Basic_access_authentication">Basic access
authentication</a>{.reference
.external} (aka. HTTP auth).</p>
<pre><code>To enable HTTP authentication for a spider, set the
[`http_user`{.docutils .literal .notranslate}]{.pre} and
[`http_pass`{.docutils .literal .notranslate}]{.pre} spider
attributes to the authentication data and the
[`http_auth_domain`{.docutils .literal .notranslate}]{.pre} spider
attribute to the domain which requires this authentication (its
subdomains will be also handled in the same way). You can set
[`http_auth_domain`{.docutils .literal .notranslate}]{.pre} to
[`None`{.docutils .literal .notranslate}]{.pre} to enable the
authentication for all requests but you risk leaking your
authentication credentials to unrelated domains.

::: {.admonition .warning}
Warning

In previous Scrapy versions HttpAuthMiddleware sent the
authentication data with all requests, which is a security problem
if the spider makes requests to several different domains. Currently
if the [`http_auth_domain`{.docutils .literal .notranslate}]{.pre}
attribute is not set, the middleware will use the domain of the
first request, which will work for some spiders but not for others.
In the future the middleware will produce an error instead.
:::

Example:

::: {.highlight-python .notranslate}
::: highlight
    from scrapy.spiders import CrawlSpider


    class SomeIntranetSiteSpider(CrawlSpider):
        http_user = &quot;someuser&quot;
        http_pass = &quot;somepass&quot;
        http_auth_domain = &quot;intranet.example.com&quot;
        name = &quot;intranet.example.com&quot;

        # .. rest of the spider code omitted ...
:::
:::
</code></pre>
<p>:::</p>
<p>::: {#module-scrapy.downloadermiddlewares.httpcache .section}
[]{#httpcachemiddleware}</p>
<h5 id="httpcachemiddlewareheaderlink"><a class="header" href="#httpcachemiddlewareheaderlink">HttpCacheMiddleware<a href="#module-scrapy.downloadermiddlewares.httpcache" title="Permalink to this heading">¶</a>{.headerlink}</a></h5>
<p><em>[class]{.pre}[ ]{.w}</em>[[scrapy.downloadermiddlewares.httpcache.]{.pre}]{.sig-prename .descclassname}[[HttpCacheMiddleware]{.pre}]{.sig-name .descname}<a href="_modules/scrapy/downloadermiddlewares/httpcache.html#HttpCacheMiddleware">[[[source]]{.pre}]{.viewcode-link}</a>{.reference .internal}<a href="#scrapy.downloadermiddlewares.httpcache.HttpCacheMiddleware" title="Permalink to this definition">¶</a>{.headerlink}</p>
<p>:   This middleware provides low-level cache to all HTTP requests and
responses. It has to be combined with a cache storage backend as
well as a cache policy.</p>
<pre><code>Scrapy ships with the following HTTP cache storage backends:

&gt; &lt;div&gt;
&gt;
&gt; -   [[Filesystem storage backend (default)]{.std
&gt;     .std-ref}](#httpcache-storage-fs){.hoverxref .tooltip
&gt;     .reference .internal}
&gt;
&gt; -   [[DBM storage backend]{.std
&gt;     .std-ref}](#httpcache-storage-dbm){.hoverxref .tooltip
&gt;     .reference .internal}
&gt;
&gt; &lt;/div&gt;

You can change the HTTP cache storage backend with the
[[`HTTPCACHE_STORAGE`{.xref .std .std-setting .docutils .literal
.notranslate}]{.pre}](#std-setting-HTTPCACHE_STORAGE){.hoverxref
.tooltip .reference .internal} setting. Or you can also [[implement
your own storage backend.]{.std
.std-ref}](#httpcache-storage-custom){.hoverxref .tooltip .reference
.internal}

Scrapy ships with two HTTP cache policies:

&gt; &lt;div&gt;
&gt;
&gt; -   [[RFC2616 policy]{.std
&gt;     .std-ref}](#httpcache-policy-rfc2616){.hoverxref .tooltip
&gt;     .reference .internal}
&gt;
&gt; -   [[Dummy policy (default)]{.std
&gt;     .std-ref}](#httpcache-policy-dummy){.hoverxref .tooltip
&gt;     .reference .internal}
&gt;
&gt; &lt;/div&gt;

You can change the HTTP cache policy with the
[[`HTTPCACHE_POLICY`{.xref .std .std-setting .docutils .literal
.notranslate}]{.pre}](#std-setting-HTTPCACHE_POLICY){.hoverxref
.tooltip .reference .internal} setting. Or you can also implement
your own policy.

You can also avoid caching a response on every policy using
[[`dont_cache`{.xref .std .std-reqmeta .docutils .literal
.notranslate}]{.pre}](#std-reqmeta-dont_cache){.hoverxref .tooltip
.reference .internal} meta key equals [`True`{.docutils .literal
.notranslate}]{.pre}.
</code></pre>
<p>::: {#dummy-policy-default .section}
[]{#httpcache-policy-dummy}</p>
<h6 id="dummy-policy-defaultheaderlink"><a class="header" href="#dummy-policy-defaultheaderlink">Dummy policy (default)<a href="#dummy-policy-default" title="Permalink to this heading">¶</a>{.headerlink}</a></h6>
<p><em>[class]{.pre}[ ]{.w}</em>[[scrapy.extensions.httpcache.]{.pre}]{.sig-prename .descclassname}[[DummyPolicy]{.pre}]{.sig-name .descname}<a href="_modules/scrapy/extensions/httpcache.html#DummyPolicy">[[[source]]{.pre}]{.viewcode-link}</a>{.reference .internal}<a href="#scrapy.extensions.httpcache.DummyPolicy" title="Permalink to this definition">¶</a>{.headerlink}</p>
<p>:   This policy has no awareness of any HTTP Cache-Control directives.
Every request and its corresponding response are cached. When the
same request is seen again, the response is returned without
transferring anything from the Internet.</p>
<pre><code>The Dummy policy is useful for testing spiders faster (without
having to wait for downloads every time) and for trying your spider
offline, when an Internet connection is not available. The goal is
to be able to &quot;replay&quot; a spider run *exactly as it ran before*.
</code></pre>
<p>:::</p>
<p>::: {#rfc2616-policy .section}
[]{#httpcache-policy-rfc2616}</p>
<h6 id="rfc2616-policyheaderlink"><a class="header" href="#rfc2616-policyheaderlink">RFC2616 policy<a href="#rfc2616-policy" title="Permalink to this heading">¶</a>{.headerlink}</a></h6>
<p><em>[class]{.pre}[ ]{.w}</em>[[scrapy.extensions.httpcache.]{.pre}]{.sig-prename .descclassname}[[RFC2616Policy]{.pre}]{.sig-name .descname}<a href="_modules/scrapy/extensions/httpcache.html#RFC2616Policy">[[[source]]{.pre}]{.viewcode-link}</a>{.reference .internal}<a href="#scrapy.extensions.httpcache.RFC2616Policy" title="Permalink to this definition">¶</a>{.headerlink}</p>
<p>:   This policy provides a RFC2616 compliant HTTP cache, i.e. with HTTP
Cache-Control awareness, aimed at production and used in continuous
runs to avoid downloading unmodified data (to save bandwidth and
speed up crawls).</p>
<pre><code>What is implemented:

-   Do not attempt to store responses/requests with
    [`no-store`{.docutils .literal .notranslate}]{.pre}
    cache-control directive set

-   Do not serve responses from cache if [`no-cache`{.docutils
    .literal .notranslate}]{.pre} cache-control directive is set
    even for fresh responses

-   Compute freshness lifetime from [`max-age`{.docutils .literal
    .notranslate}]{.pre} cache-control directive

-   Compute freshness lifetime from [`Expires`{.docutils .literal
    .notranslate}]{.pre} response header

-   Compute freshness lifetime from [`Last-Modified`{.docutils
    .literal .notranslate}]{.pre} response header (heuristic used by
    Firefox)

-   Compute current age from [`Age`{.docutils .literal
    .notranslate}]{.pre} response header

-   Compute current age from [`Date`{.docutils .literal
    .notranslate}]{.pre} header

-   Revalidate stale responses based on [`Last-Modified`{.docutils
    .literal .notranslate}]{.pre} response header

-   Revalidate stale responses based on [`ETag`{.docutils .literal
    .notranslate}]{.pre} response header

-   Set [`Date`{.docutils .literal .notranslate}]{.pre} header for
    any received response missing it

-   Support [`max-stale`{.docutils .literal .notranslate}]{.pre}
    cache-control directive in requests

This allows spiders to be configured with the full RFC2616 cache
policy, but avoid revalidation on a request-by-request basis, while
remaining conformant with the HTTP spec.

Example:

Add [`Cache-Control:`{.docutils .literal
.notranslate}]{.pre}` `{.docutils .literal
.notranslate}[`max-stale=600`{.docutils .literal
.notranslate}]{.pre} to Request headers to accept responses that
have exceeded their expiration time by no more than 600 seconds.

See also: RFC2616, 14.9.3

What is missing:

-   [`Pragma:`{.docutils .literal .notranslate}]{.pre}` `{.docutils
    .literal .notranslate}[`no-cache`{.docutils .literal
    .notranslate}]{.pre} support
    [https://www.w3.org/Protocols/rfc2616/rfc2616-sec14.html#sec14.9.1](https://www.w3.org/Protocols/rfc2616/rfc2616-sec14.html#sec14.9.1){.reference
    .external}

-   [`Vary`{.docutils .literal .notranslate}]{.pre} header support
    [https://www.w3.org/Protocols/rfc2616/rfc2616-sec13.html#sec13.6](https://www.w3.org/Protocols/rfc2616/rfc2616-sec13.html#sec13.6){.reference
    .external}

-   Invalidation after updates or deletes
    [https://www.w3.org/Protocols/rfc2616/rfc2616-sec13.html#sec13.10](https://www.w3.org/Protocols/rfc2616/rfc2616-sec13.html#sec13.10){.reference
    .external}

-   ... probably others ..
</code></pre>
<p>:::</p>
<p>::: {#filesystem-storage-backend-default .section}
[]{#httpcache-storage-fs}</p>
<h6 id="filesystem-storage-backend-defaultheaderlink"><a class="header" href="#filesystem-storage-backend-defaultheaderlink">Filesystem storage backend (default)<a href="#filesystem-storage-backend-default" title="Permalink to this heading">¶</a>{.headerlink}</a></h6>
<p><em>[class]{.pre}[ ]{.w}</em>[[scrapy.extensions.httpcache.]{.pre}]{.sig-prename .descclassname}[[FilesystemCacheStorage]{.pre}]{.sig-name .descname}<a href="_modules/scrapy/extensions/httpcache.html#FilesystemCacheStorage">[[[source]]{.pre}]{.viewcode-link}</a>{.reference .internal}<a href="#scrapy.extensions.httpcache.FilesystemCacheStorage" title="Permalink to this definition">¶</a>{.headerlink}</p>
<p>:   File system storage backend is available for the HTTP cache
middleware.</p>
<pre><code>Each request/response pair is stored in a different directory
containing the following files:

-   [`request_body`{.docutils .literal .notranslate}]{.pre} - the
    plain request body

-   [`request_headers`{.docutils .literal .notranslate}]{.pre} - the
    request headers (in raw HTTP format)

-   [`response_body`{.docutils .literal .notranslate}]{.pre} - the
    plain response body

-   [`response_headers`{.docutils .literal .notranslate}]{.pre} -
    the request headers (in raw HTTP format)

-   [`meta`{.docutils .literal .notranslate}]{.pre} - some metadata
    of this cache resource in Python [`repr()`{.docutils .literal
    .notranslate}]{.pre} format (grep-friendly format)

-   [`pickled_meta`{.docutils .literal .notranslate}]{.pre} - the
    same metadata in [`meta`{.docutils .literal .notranslate}]{.pre}
    but pickled for more efficient deserialization

The directory name is made from the request fingerprint (see
[`scrapy.utils.request.fingerprint`{.docutils .literal
.notranslate}]{.pre}), and one level of subdirectories is used to
avoid creating too many files into the same directory (which is
inefficient in many file systems). An example directory could be:

::: {.highlight-default .notranslate}
::: highlight
    /path/to/cache/dir/example.com/72/72811f648e718090f041317756c03adb0ada46c7
:::
:::
</code></pre>
<p>:::</p>
<p>::: {#dbm-storage-backend .section}
[]{#httpcache-storage-dbm}</p>
<h6 id="dbm-storage-backendheaderlink"><a class="header" href="#dbm-storage-backendheaderlink">DBM storage backend<a href="#dbm-storage-backend" title="Permalink to this heading">¶</a>{.headerlink}</a></h6>
<p><em>[class]{.pre}[ ]{.w}</em>[[scrapy.extensions.httpcache.]{.pre}]{.sig-prename .descclassname}[[DbmCacheStorage]{.pre}]{.sig-name .descname}<a href="_modules/scrapy/extensions/httpcache.html#DbmCacheStorage">[[[source]]{.pre}]{.viewcode-link}</a>{.reference .internal}<a href="#scrapy.extensions.httpcache.DbmCacheStorage" title="Permalink to this definition">¶</a>{.headerlink}</p>
<p>:   A <a href="https://en.wikipedia.org/wiki/Dbm">DBM</a>{.reference .external}
storage backend is also available for the HTTP cache middleware.</p>
<pre><code>By default, it uses the [[`dbm`{.xref .py .py-mod .docutils .literal
.notranslate}]{.pre}](https://docs.python.org/3/library/dbm.html#module-dbm &quot;(in Python v3.12)&quot;){.reference
.external}, but you can change it with the
[[`HTTPCACHE_DBM_MODULE`{.xref .std .std-setting .docutils .literal
.notranslate}]{.pre}](#std-setting-HTTPCACHE_DBM_MODULE){.hoverxref
.tooltip .reference .internal} setting.
</code></pre>
<p>:::</p>
<p>::: {#writing-your-own-storage-backend .section}
[]{#httpcache-storage-custom}</p>
<h6 id="writing-your-own-storage-backendheaderlink"><a class="header" href="#writing-your-own-storage-backendheaderlink">Writing your own storage backend<a href="#writing-your-own-storage-backend" title="Permalink to this heading">¶</a>{.headerlink}</a></h6>
<p>You can implement a cache storage backend by creating a Python class
that defines the methods described below.</p>
<p>[]{#module-scrapy.extensions.httpcache .target}</p>
<p><em>[class]{.pre}[ ]{.w}</em>[[scrapy.extensions.httpcache.]{.pre}]{.sig-prename .descclassname}[[CacheStorage]{.pre}]{.sig-name .descname}<a href="#scrapy.extensions.httpcache.CacheStorage" title="Permalink to this definition">¶</a>{.headerlink}</p>
<p>:</p>
<pre><code>[[open_spider]{.pre}]{.sig-name .descname}[(]{.sig-paren}*[[spider]{.pre}]{.n}*[)]{.sig-paren}[¶](#scrapy.extensions.httpcache.CacheStorage.open_spider &quot;Permalink to this definition&quot;){.headerlink}

:   This method gets called after a spider has been opened for
    crawling. It handles the [[`open_spider`{.xref .std .std-signal
    .docutils .literal
    .notranslate}]{.pre}](index.html#std-signal-spider_opened){.hoverxref
    .tooltip .reference .internal} signal.

    Parameters

    :   **spider** ([[`Spider`{.xref .py .py-class .docutils
        .literal
        .notranslate}]{.pre}](index.html#scrapy.Spider &quot;scrapy.Spider&quot;){.reference
        .internal} object) -- the spider which has been opened

[[close_spider]{.pre}]{.sig-name .descname}[(]{.sig-paren}*[[spider]{.pre}]{.n}*[)]{.sig-paren}[¶](#scrapy.extensions.httpcache.CacheStorage.close_spider &quot;Permalink to this definition&quot;){.headerlink}

:   This method gets called after a spider has been closed. It
    handles the [[`close_spider`{.xref .std .std-signal .docutils
    .literal
    .notranslate}]{.pre}](index.html#std-signal-spider_closed){.hoverxref
    .tooltip .reference .internal} signal.

    Parameters

    :   **spider** ([[`Spider`{.xref .py .py-class .docutils
        .literal
        .notranslate}]{.pre}](index.html#scrapy.Spider &quot;scrapy.Spider&quot;){.reference
        .internal} object) -- the spider which has been closed

[[retrieve_response]{.pre}]{.sig-name .descname}[(]{.sig-paren}*[[spider]{.pre}]{.n}*, *[[request]{.pre}]{.n}*[)]{.sig-paren}[¶](#scrapy.extensions.httpcache.CacheStorage.retrieve_response &quot;Permalink to this definition&quot;){.headerlink}

:   Return response if present in cache, or [`None`{.docutils
    .literal .notranslate}]{.pre} otherwise.

    Parameters

    :   -   **spider** ([[`Spider`{.xref .py .py-class .docutils
            .literal
            .notranslate}]{.pre}](index.html#scrapy.Spider &quot;scrapy.Spider&quot;){.reference
            .internal} object) -- the spider which generated the
            request

        -   **request** ([`Request`{.xref .py .py-class .docutils
            .literal .notranslate}]{.pre} object) -- the request to
            find cached response for

[[store_response]{.pre}]{.sig-name .descname}[(]{.sig-paren}*[[spider]{.pre}]{.n}*, *[[request]{.pre}]{.n}*, *[[response]{.pre}]{.n}*[)]{.sig-paren}[¶](#scrapy.extensions.httpcache.CacheStorage.store_response &quot;Permalink to this definition&quot;){.headerlink}

:   Store the given response in the cache.

    Parameters

    :   -   **spider** ([[`Spider`{.xref .py .py-class .docutils
            .literal
            .notranslate}]{.pre}](index.html#scrapy.Spider &quot;scrapy.Spider&quot;){.reference
            .internal} object) -- the spider for which the response
            is intended

        -   **request** ([`Request`{.xref .py .py-class .docutils
            .literal .notranslate}]{.pre} object) -- the
            corresponding request the spider generated

        -   **response** ([[`Response`{.xref .py .py-class .docutils
            .literal
            .notranslate}]{.pre}](index.html#scrapy.http.Response &quot;scrapy.http.Response&quot;){.reference
            .internal} object) -- the response to store in the cache
</code></pre>
<p>In order to use your storage backend, set:</p>
<ul>
<li><a href="#std-setting-HTTPCACHE_STORAGE">[<code>HTTPCACHE_STORAGE</code>{.xref .std .std-setting .docutils .literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal} to the Python import path of your
custom storage class.
:::</li>
</ul>
<p>::: {#httpcache-middleware-settings .section}</p>
<h6 id="httpcache-middleware-settingsheaderlink"><a class="header" href="#httpcache-middleware-settingsheaderlink">HTTPCache middleware settings<a href="#httpcache-middleware-settings" title="Permalink to this heading">¶</a>{.headerlink}</a></h6>
<p>The [<code>HttpCacheMiddleware</code>{.xref .py .py-class .docutils .literal
.notranslate}]{.pre} can be configured through the following settings:</p>
<p>::: {#httpcache-enabled .section}
[]{#std-setting-HTTPCACHE_ENABLED}HTTPCACHE_ENABLED<a href="#httpcache-enabled" title="Permalink to this heading">¶</a>{.headerlink}</p>
<p>Default: [<code>False</code>{.docutils .literal .notranslate}]{.pre}</p>
<p>Whether the HTTP cache will be enabled.
:::</p>
<p>::: {#httpcache-expiration-secs .section}
[]{#std-setting-HTTPCACHE_EXPIRATION_SECS}HTTPCACHE_EXPIRATION_SECS<a href="#httpcache-expiration-secs" title="Permalink to this heading">¶</a>{.headerlink}</p>
<p>Default: [<code>0</code>{.docutils .literal .notranslate}]{.pre}</p>
<p>Expiration time for cached requests, in seconds.</p>
<p>Cached requests older than this time will be re-downloaded. If zero,
cached requests will never expire.
:::</p>
<p>::: {#httpcache-dir .section}
[]{#std-setting-HTTPCACHE_DIR}HTTPCACHE_DIR<a href="#httpcache-dir" title="Permalink to this heading">¶</a>{.headerlink}</p>
<p>Default: [<code>'httpcache'</code>{.docutils .literal .notranslate}]{.pre}</p>
<p>The directory to use for storing the (low-level) HTTP cache. If empty,
the HTTP cache will be disabled. If a relative path is given, is taken
relative to the project data dir. For more info see: <a href="index.html#topics-project-structure">[Default structure
of Scrapy projects]{.std
.std-ref}</a>{.hoverxref .tooltip
.reference .internal}.
:::</p>
<p>::: {#httpcache-ignore-http-codes .section}
[]{#std-setting-HTTPCACHE_IGNORE_HTTP_CODES}HTTPCACHE_IGNORE_HTTP_CODES<a href="#httpcache-ignore-http-codes" title="Permalink to this heading">¶</a>{.headerlink}</p>
<p>Default: [<code>[]</code>{.docutils .literal .notranslate}]{.pre}</p>
<p>Don't cache response with these HTTP codes.
:::</p>
<p>::: {#httpcache-ignore-missing .section}
[]{#std-setting-HTTPCACHE_IGNORE_MISSING}HTTPCACHE_IGNORE_MISSING<a href="#httpcache-ignore-missing" title="Permalink to this heading">¶</a>{.headerlink}</p>
<p>Default: [<code>False</code>{.docutils .literal .notranslate}]{.pre}</p>
<p>If enabled, requests not found in the cache will be ignored instead of
downloaded.
:::</p>
<p>::: {#httpcache-ignore-schemes .section}
[]{#std-setting-HTTPCACHE_IGNORE_SCHEMES}HTTPCACHE_IGNORE_SCHEMES<a href="#httpcache-ignore-schemes" title="Permalink to this heading">¶</a>{.headerlink}</p>
<p>Default: [<code>['file']</code>{.docutils .literal .notranslate}]{.pre}</p>
<p>Don't cache responses with these URI schemes.
:::</p>
<p>::: {#httpcache-storage .section}
[]{#std-setting-HTTPCACHE_STORAGE}HTTPCACHE_STORAGE<a href="#httpcache-storage" title="Permalink to this heading">¶</a>{.headerlink}</p>
<p>Default:
[<code>'scrapy.extensions.httpcache.FilesystemCacheStorage'</code>{.docutils
.literal .notranslate}]{.pre}</p>
<p>The class which implements the cache storage backend.
:::</p>
<p>::: {#httpcache-dbm-module .section}
[]{#std-setting-HTTPCACHE_DBM_MODULE}HTTPCACHE_DBM_MODULE<a href="#httpcache-dbm-module" title="Permalink to this heading">¶</a>{.headerlink}</p>
<p>Default: [<code>'dbm'</code>{.docutils .literal .notranslate}]{.pre}</p>
<p>The database module to use in the <a href="#httpcache-storage-dbm">[DBM storage backend]{.std
.std-ref}</a>{.hoverxref .tooltip .reference
.internal}. This setting is specific to the DBM backend.
:::</p>
<p>::: {#httpcache-policy .section}
[]{#std-setting-HTTPCACHE_POLICY}HTTPCACHE_POLICY<a href="#httpcache-policy" title="Permalink to this heading">¶</a>{.headerlink}</p>
<p>Default: [<code>'scrapy.extensions.httpcache.DummyPolicy'</code>{.docutils .literal
.notranslate}]{.pre}</p>
<p>The class which implements the cache policy.
:::</p>
<p>::: {#httpcache-gzip .section}
[]{#std-setting-HTTPCACHE_GZIP}HTTPCACHE_GZIP<a href="#httpcache-gzip" title="Permalink to this heading">¶</a>{.headerlink}</p>
<p>Default: [<code>False</code>{.docutils .literal .notranslate}]{.pre}</p>
<p>If enabled, will compress all cached data with gzip. This setting is
specific to the Filesystem backend.
:::</p>
<p>::: {#httpcache-always-store .section}
[]{#std-setting-HTTPCACHE_ALWAYS_STORE}HTTPCACHE_ALWAYS_STORE<a href="#httpcache-always-store" title="Permalink to this heading">¶</a>{.headerlink}</p>
<p>Default: [<code>False</code>{.docutils .literal .notranslate}]{.pre}</p>
<p>If enabled, will cache pages unconditionally.</p>
<p>A spider may wish to have all responses available in the cache, for
future use with [<code>Cache-Control:</code>{.docutils .literal
.notranslate}]{.pre}<code> </code>{.docutils .literal
.notranslate}[<code>max-stale</code>{.docutils .literal .notranslate}]{.pre}, for
instance. The DummyPolicy caches all responses but never revalidates
them, and sometimes a more nuanced policy is desirable.</p>
<p>This setting still respects [<code>Cache-Control:</code>{.docutils .literal
.notranslate}]{.pre}<code> </code>{.docutils .literal
.notranslate}[<code>no-store</code>{.docutils .literal .notranslate}]{.pre}
directives in responses. If you don't want that, filter
[<code>no-store</code>{.docutils .literal .notranslate}]{.pre} out of the
Cache-Control headers in responses you feed to the cache middleware.
:::</p>
<p>::: {#httpcache-ignore-response-cache-controls .section}
[]{#std-setting-HTTPCACHE_IGNORE_RESPONSE_CACHE_CONTROLS}HTTPCACHE_IGNORE_RESPONSE_CACHE_CONTROLS<a href="#httpcache-ignore-response-cache-controls" title="Permalink to this heading">¶</a>{.headerlink}</p>
<p>Default: [<code>[]</code>{.docutils .literal .notranslate}]{.pre}</p>
<p>List of Cache-Control directives in responses to be ignored.</p>
<p>Sites often set &quot;no-store&quot;, &quot;no-cache&quot;, &quot;must-revalidate&quot;, etc., but get
upset at the traffic a spider can generate if it actually respects those
directives. This allows to selectively ignore Cache-Control directives
that are known to be unimportant for the sites being crawled.</p>
<p>We assume that the spider will not issue Cache-Control directives in
requests unless it actually needs them, so directives in requests are
not filtered.
:::
:::
:::</p>
<p>::: {#module-scrapy.downloadermiddlewares.httpcompression .section}
[]{#httpcompressionmiddleware}</p>
<h5 id="httpcompressionmiddlewareheaderlink"><a class="header" href="#httpcompressionmiddlewareheaderlink">HttpCompressionMiddleware<a href="#module-scrapy.downloadermiddlewares.httpcompression" title="Permalink to this heading">¶</a>{.headerlink}</a></h5>
<p><em>[class]{.pre}[ ]{.w}</em>[[scrapy.downloadermiddlewares.httpcompression.]{.pre}]{.sig-prename .descclassname}[[HttpCompressionMiddleware]{.pre}]{.sig-name .descname}<a href="_modules/scrapy/downloadermiddlewares/httpcompression.html#HttpCompressionMiddleware">[[[source]]{.pre}]{.viewcode-link}</a>{.reference .internal}<a href="#scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware" title="Permalink to this definition">¶</a>{.headerlink}</p>
<p>:   This middleware allows compressed (gzip, deflate) traffic to be
sent/received from web sites.</p>
<pre><code>This middleware also supports decoding
[brotli-compressed](https://www.ietf.org/rfc/rfc7932.txt){.reference
.external} as well as
[zstd-compressed](https://www.ietf.org/rfc/rfc8478.txt){.reference
.external} responses, provided that
[brotli](https://pypi.org/project/Brotli/){.reference .external} or
[zstandard](https://pypi.org/project/zstandard/){.reference
.external} is installed, respectively.
</code></pre>
<p>::: {#httpcompressionmiddleware-settings .section}</p>
<h6 id="httpcompressionmiddleware-settingsheaderlink"><a class="header" href="#httpcompressionmiddleware-settingsheaderlink">HttpCompressionMiddleware Settings<a href="#httpcompressionmiddleware-settings" title="Permalink to this heading">¶</a>{.headerlink}</a></h6>
<p>::: {#compression-enabled .section}
[]{#std-setting-COMPRESSION_ENABLED}COMPRESSION_ENABLED<a href="#compression-enabled" title="Permalink to this heading">¶</a>{.headerlink}</p>
<p>Default: [<code>True</code>{.docutils .literal .notranslate}]{.pre}</p>
<p>Whether the Compression middleware will be enabled.
:::
:::
:::</p>
<p>::: {#module-scrapy.downloadermiddlewares.httpproxy .section}
[]{#httpproxymiddleware}</p>
<h5 id="httpproxymiddlewareheaderlink"><a class="header" href="#httpproxymiddlewareheaderlink">HttpProxyMiddleware<a href="#module-scrapy.downloadermiddlewares.httpproxy" title="Permalink to this heading">¶</a>{.headerlink}</a></h5>
<p>[]{#std-reqmeta-proxy .target}</p>
<p><em>[class]{.pre}[ ]{.w}</em>[[scrapy.downloadermiddlewares.httpproxy.]{.pre}]{.sig-prename .descclassname}[[HttpProxyMiddleware]{.pre}]{.sig-name .descname}<a href="_modules/scrapy/downloadermiddlewares/httpproxy.html#HttpProxyMiddleware">[[[source]]{.pre}]{.viewcode-link}</a>{.reference .internal}<a href="#scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware" title="Permalink to this definition">¶</a>{.headerlink}</p>
<p>:   This middleware sets the HTTP proxy to use for requests, by setting
the [<code>proxy</code>{.docutils .literal .notranslate}]{.pre} meta value for
[<code>Request</code>{.xref .py .py-class .docutils .literal
.notranslate}]{.pre} objects.</p>
<pre><code>Like the Python standard library module [[`urllib.request`{.xref .py
.py-mod .docutils .literal
.notranslate}]{.pre}](https://docs.python.org/3/library/urllib.request.html#module-urllib.request &quot;(in Python v3.12)&quot;){.reference
.external}, it obeys the following environment variables:

-   [`http_proxy`{.docutils .literal .notranslate}]{.pre}

-   [`https_proxy`{.docutils .literal .notranslate}]{.pre}

-   [`no_proxy`{.docutils .literal .notranslate}]{.pre}

You can also set the meta key [`proxy`{.docutils .literal
.notranslate}]{.pre} per-request, to a value like
[`http://some_proxy_server:port`{.docutils .literal
.notranslate}]{.pre} or
[`http://username:password@some_proxy_server:port`{.docutils
.literal .notranslate}]{.pre}. Keep in mind this value will take
precedence over [`http_proxy`{.docutils .literal
.notranslate}]{.pre}/[`https_proxy`{.docutils .literal
.notranslate}]{.pre} environment variables, and it will also ignore
[`no_proxy`{.docutils .literal .notranslate}]{.pre} environment
variable.
</code></pre>
<p>:::</p>
<p>::: {#module-scrapy.downloadermiddlewares.redirect .section}
[]{#redirectmiddleware}</p>
<h5 id="redirectmiddlewareheaderlink"><a class="header" href="#redirectmiddlewareheaderlink">RedirectMiddleware<a href="#module-scrapy.downloadermiddlewares.redirect" title="Permalink to this heading">¶</a>{.headerlink}</a></h5>
<p><em>[class]{.pre}[ ]{.w}</em>[[scrapy.downloadermiddlewares.redirect.]{.pre}]{.sig-prename .descclassname}[[RedirectMiddleware]{.pre}]{.sig-name .descname}<a href="_modules/scrapy/downloadermiddlewares/redirect.html#RedirectMiddleware">[[[source]]{.pre}]{.viewcode-link}</a>{.reference .internal}<a href="#scrapy.downloadermiddlewares.redirect.RedirectMiddleware" title="Permalink to this definition">¶</a>{.headerlink}</p>
<p>:   This middleware handles redirection of requests based on response
status.</p>
<p>The urls which the request goes through (while being redirected) can be
found in the [<code>redirect_urls</code>{.docutils .literal .notranslate}]{.pre}
[<code>Request.meta</code>{.xref .py .py-attr .docutils .literal
.notranslate}]{.pre} key.</p>
<p>The reason behind each redirect in <a href="#std-reqmeta-redirect_urls">[<code>redirect_urls</code>{.xref .std
.std-reqmeta .docutils .literal
.notranslate}]{.pre}</a>{.hoverxref .tooltip
.reference .internal} can be found in the [<code>redirect_reasons</code>{.docutils
.literal .notranslate}]{.pre} [<code>Request.meta</code>{.xref .py .py-attr
.docutils .literal .notranslate}]{.pre} key. For example:
[<code>[301,</code>{.docutils .literal .notranslate}]{.pre}<code> </code>{.docutils .literal
.notranslate}[<code>302,</code>{.docutils .literal
.notranslate}]{.pre}<code> </code>{.docutils .literal
.notranslate}[<code>307,</code>{.docutils .literal
.notranslate}]{.pre}<code> </code>{.docutils .literal
.notranslate}[<code>'meta</code>{.docutils .literal
.notranslate}]{.pre}<code> </code>{.docutils .literal
.notranslate}[<code>refresh']</code>{.docutils .literal .notranslate}]{.pre}.</p>
<p>The format of a reason depends on the middleware that handled the
corresponding redirect. For example, <a href="#scrapy.downloadermiddlewares.redirect.RedirectMiddleware" title="scrapy.downloadermiddlewares.redirect.RedirectMiddleware">[<code>RedirectMiddleware</code>{.xref .py
.py-class .docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal} indicates the triggering response status code as an integer,
while <a href="#scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware" title="scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware">[<code>MetaRefreshMiddleware</code>{.xref .py .py-class .docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal} always uses the [<code>'meta</code>{.docutils .literal
.notranslate}]{.pre}<code> </code>{.docutils .literal
.notranslate}[<code>refresh'</code>{.docutils .literal .notranslate}]{.pre} string
as reason.</p>
<p>The <a href="#scrapy.downloadermiddlewares.redirect.RedirectMiddleware" title="scrapy.downloadermiddlewares.redirect.RedirectMiddleware">[<code>RedirectMiddleware</code>{.xref .py .py-class .docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal} can be configured through the following settings (see the
settings documentation for more info):</p>
<ul>
<li>
<p><a href="#std-setting-REDIRECT_ENABLED">[<code>REDIRECT_ENABLED</code>{.xref .std .std-setting .docutils .literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal}</p>
</li>
<li>
<p><a href="#std-setting-REDIRECT_MAX_TIMES">[<code>REDIRECT_MAX_TIMES</code>{.xref .std .std-setting .docutils .literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal}</p>
</li>
</ul>
<p>If [<code>Request.meta</code>{.xref .py .py-attr .docutils .literal
.notranslate}]{.pre} has [<code>dont_redirect</code>{.docutils .literal
.notranslate}]{.pre} key set to True, the request will be ignored by
this middleware.</p>
<p>If you want to handle some redirect status codes in your spider, you can
specify these in the [<code>handle_httpstatus_list</code>{.docutils .literal
.notranslate}]{.pre} spider attribute.</p>
<p>For example, if you want the redirect middleware to ignore 301 and 302
responses (and pass them through to your spider) you can do this:</p>
<p>::: {.highlight-python .notranslate}
::: highlight
class MySpider(CrawlSpider):
handle_httpstatus_list = [301, 302]
:::
:::</p>
<p>The [<code>handle_httpstatus_list</code>{.docutils .literal .notranslate}]{.pre}
key of [<code>Request.meta</code>{.xref .py .py-attr .docutils .literal
.notranslate}]{.pre} can also be used to specify which response codes to
allow on a per-request basis. You can also set the meta key
[<code>handle_httpstatus_all</code>{.docutils .literal .notranslate}]{.pre} to
[<code>True</code>{.docutils .literal .notranslate}]{.pre} if you want to allow any
response code for a request.</p>
<p>::: {#redirectmiddleware-settings .section}</p>
<h6 id="redirectmiddleware-settingsheaderlink"><a class="header" href="#redirectmiddleware-settingsheaderlink">RedirectMiddleware settings<a href="#redirectmiddleware-settings" title="Permalink to this heading">¶</a>{.headerlink}</a></h6>
<p>::: {#redirect-enabled .section}
[]{#std-setting-REDIRECT_ENABLED}REDIRECT_ENABLED<a href="#redirect-enabled" title="Permalink to this heading">¶</a>{.headerlink}</p>
<p>Default: [<code>True</code>{.docutils .literal .notranslate}]{.pre}</p>
<p>Whether the Redirect middleware will be enabled.
:::</p>
<p>::: {#redirect-max-times .section}
[]{#std-setting-REDIRECT_MAX_TIMES}REDIRECT_MAX_TIMES<a href="#redirect-max-times" title="Permalink to this heading">¶</a>{.headerlink}</p>
<p>Default: [<code>20</code>{.docutils .literal .notranslate}]{.pre}</p>
<p>The maximum number of redirections that will be followed for a single
request. After this maximum, the request's response is returned as is.
:::
:::
:::</p>
<p>::: {#metarefreshmiddleware .section}</p>
<h5 id="metarefreshmiddlewareheaderlink"><a class="header" href="#metarefreshmiddlewareheaderlink">MetaRefreshMiddleware<a href="#metarefreshmiddleware" title="Permalink to this heading">¶</a>{.headerlink}</a></h5>
<p><em>[class]{.pre}[ ]{.w}</em>[[scrapy.downloadermiddlewares.redirect.]{.pre}]{.sig-prename .descclassname}[[MetaRefreshMiddleware]{.pre}]{.sig-name .descname}<a href="_modules/scrapy/downloadermiddlewares/redirect.html#MetaRefreshMiddleware">[[[source]]{.pre}]{.viewcode-link}</a>{.reference .internal}<a href="#scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware" title="Permalink to this definition">¶</a>{.headerlink}</p>
<p>:   This middleware handles redirection of requests based on
meta-refresh html tag.</p>
<p>The <a href="#scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware" title="scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware">[<code>MetaRefreshMiddleware</code>{.xref .py .py-class .docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal} can be configured through the following settings (see the
settings documentation for more info):</p>
<ul>
<li>
<p><a href="#std-setting-METAREFRESH_ENABLED">[<code>METAREFRESH_ENABLED</code>{.xref .std .std-setting .docutils .literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal}</p>
</li>
<li>
<p><a href="#std-setting-METAREFRESH_IGNORE_TAGS">[<code>METAREFRESH_IGNORE_TAGS</code>{.xref .std .std-setting .docutils
.literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal}</p>
</li>
<li>
<p><a href="#std-setting-METAREFRESH_MAXDELAY">[<code>METAREFRESH_MAXDELAY</code>{.xref .std .std-setting .docutils .literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal}</p>
</li>
</ul>
<p>This middleware obey <a href="#std-setting-REDIRECT_MAX_TIMES">[<code>REDIRECT_MAX_TIMES</code>{.xref .std .std-setting
.docutils .literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal} setting, <a href="#std-reqmeta-dont_redirect">[<code>dont_redirect</code>{.xref .std
.std-reqmeta .docutils .literal
.notranslate}]{.pre}</a>{.hoverxref .tooltip
.reference .internal}, <a href="#std-reqmeta-redirect_urls">[<code>redirect_urls</code>{.xref .std .std-reqmeta
.docutils .literal
.notranslate}]{.pre}</a>{.hoverxref .tooltip
.reference .internal} and <a href="#std-reqmeta-redirect_reasons">[<code>redirect_reasons</code>{.xref .std .std-reqmeta
.docutils .literal
.notranslate}]{.pre}</a>{.hoverxref .tooltip
.reference .internal} request meta keys as described for
<a href="#scrapy.downloadermiddlewares.redirect.RedirectMiddleware" title="scrapy.downloadermiddlewares.redirect.RedirectMiddleware">[<code>RedirectMiddleware</code>{.xref .py .py-class .docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal}</p>
<p>::: {#metarefreshmiddleware-settings .section}</p>
<h6 id="metarefreshmiddleware-settingsheaderlink"><a class="header" href="#metarefreshmiddleware-settingsheaderlink">MetaRefreshMiddleware settings<a href="#metarefreshmiddleware-settings" title="Permalink to this heading">¶</a>{.headerlink}</a></h6>
<p>::: {#metarefresh-enabled .section}
[]{#std-setting-METAREFRESH_ENABLED}METAREFRESH_ENABLED<a href="#metarefresh-enabled" title="Permalink to this heading">¶</a>{.headerlink}</p>
<p>Default: [<code>True</code>{.docutils .literal .notranslate}]{.pre}</p>
<p>Whether the Meta Refresh middleware will be enabled.
:::</p>
<p>::: {#metarefresh-ignore-tags .section}
[]{#std-setting-METAREFRESH_IGNORE_TAGS}METAREFRESH_IGNORE_TAGS<a href="#metarefresh-ignore-tags" title="Permalink to this heading">¶</a>{.headerlink}</p>
<p>Default: [<code>[]</code>{.docutils .literal .notranslate}]{.pre}</p>
<p>Meta tags within these tags are ignored.</p>
<p>::: versionchanged
[Changed in version 2.0: ]{.versionmodified .changed}The default value
of <a href="#std-setting-METAREFRESH_IGNORE_TAGS">[<code>METAREFRESH_IGNORE_TAGS</code>{.xref .std .std-setting .docutils
.literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal} changed from [<code>['script',</code>{.docutils
.literal .notranslate}]{.pre}<code> </code>{.docutils .literal
.notranslate}[<code>'noscript']</code>{.docutils .literal .notranslate}]{.pre} to
[<code>[]</code>{.docutils .literal .notranslate}]{.pre}.
:::
:::</p>
<p>::: {#metarefresh-maxdelay .section}
[]{#std-setting-METAREFRESH_MAXDELAY}METAREFRESH_MAXDELAY<a href="#metarefresh-maxdelay" title="Permalink to this heading">¶</a>{.headerlink}</p>
<p>Default: [<code>100</code>{.docutils .literal .notranslate}]{.pre}</p>
<p>The maximum meta-refresh delay (in seconds) to follow the redirection.
Some sites use meta-refresh for redirecting to a session expired page,
so we restrict automatic redirection to the maximum delay.
:::
:::
:::</p>
<p>::: {#module-scrapy.downloadermiddlewares.retry .section}
[]{#retrymiddleware}</p>
<h5 id="retrymiddlewareheaderlink"><a class="header" href="#retrymiddlewareheaderlink">RetryMiddleware<a href="#module-scrapy.downloadermiddlewares.retry" title="Permalink to this heading">¶</a>{.headerlink}</a></h5>
<p><em>[class]{.pre}[ ]{.w}</em>[[scrapy.downloadermiddlewares.retry.]{.pre}]{.sig-prename .descclassname}[[RetryMiddleware]{.pre}]{.sig-name .descname}<a href="_modules/scrapy/downloadermiddlewares/retry.html#RetryMiddleware">[[[source]]{.pre}]{.viewcode-link}</a>{.reference .internal}<a href="#scrapy.downloadermiddlewares.retry.RetryMiddleware" title="Permalink to this definition">¶</a>{.headerlink}</p>
<p>:   A middleware to retry failed requests that are potentially caused by
temporary problems such as a connection timeout or HTTP 500 error.</p>
<p>Failed pages are collected on the scraping process and rescheduled at
the end, once the spider has finished crawling all regular (non failed)
pages.</p>
<p>The <a href="#scrapy.downloadermiddlewares.retry.RetryMiddleware" title="scrapy.downloadermiddlewares.retry.RetryMiddleware">[<code>RetryMiddleware</code>{.xref .py .py-class .docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal} can be configured through the following settings (see the
settings documentation for more info):</p>
<ul>
<li>
<p><a href="#std-setting-RETRY_ENABLED">[<code>RETRY_ENABLED</code>{.xref .std .std-setting .docutils .literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal}</p>
</li>
<li>
<p><a href="#std-setting-RETRY_TIMES">[<code>RETRY_TIMES</code>{.xref .std .std-setting .docutils .literal
.notranslate}]{.pre}</a>{.hoverxref .tooltip
.reference .internal}</p>
</li>
<li>
<p><a href="#std-setting-RETRY_HTTP_CODES">[<code>RETRY_HTTP_CODES</code>{.xref .std .std-setting .docutils .literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal}</p>
</li>
<li>
<p><a href="#std-setting-RETRY_EXCEPTIONS">[<code>RETRY_EXCEPTIONS</code>{.xref .std .std-setting .docutils .literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal}</p>
</li>
</ul>
<p>If [<code>Request.meta</code>{.xref .py .py-attr .docutils .literal
.notranslate}]{.pre} has [<code>dont_retry</code>{.docutils .literal
.notranslate}]{.pre} key set to True, the request will be ignored by
this middleware.</p>
<p>To retry requests from a spider callback, you can use the
<a href="#scrapy.downloadermiddlewares.retry.get_retry_request" title="scrapy.downloadermiddlewares.retry.get_retry_request">[<code>get_retry_request()</code>{.xref .py .py-func .docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal} function:</p>
<p>[[scrapy.downloadermiddlewares.retry.]{.pre}]{.sig-prename .descclassname}[[get_retry_request]{.pre}]{.sig-name .descname}[(]{.sig-paren}<em>[[request:]{.pre} [~scrapy.http.request.Request]{.pre}]{.n}</em>, <em>[[*]{.pre}]{.n}</em>, <em>[[spider:]{.pre} [~scrapy.spiders.Spider]{.pre}]{.n}</em>, <em>[[reason:]{.pre} [~typing.Union[str]{.pre}]{.n}</em>, <em>[[Exception]{.pre}]{.n}</em>, <em>[[~typing.Type[Exception]]]{.pre} [=]{.pre} ['unspecified']{.pre}]{.n}</em>, <em>[[max_retry_times:]{.pre} [~typing.Optional[int]]{.pre} [=]{.pre} [None]{.pre}]{.n}</em>, <em>[[priority_adjust:]{.pre} [~typing.Optional[int]]{.pre} [=]{.pre} [None]{.pre}]{.n}</em>, <em>[[logger:]{.pre} [~logging.Logger]{.pre} [=]{.pre} [&lt;Logger]{.pre} [scrapy.downloadermiddlewares.retry]{.pre} [(WARNING)&gt;]{.pre}]{.n}</em>, <em>[[stats_base_key:]{.pre} [str]{.pre} [=]{.pre} ['retry']{.pre}]{.n}</em>[)]{.sig-paren} [[→]{.sig-return-icon} [<a href="https://docs.python.org/3/library/typing.html#typing.Optional" title="(in Python v3.12)">[Optional]{.pre}</a>{.reference .external}[[[]{.pre}]{.p}<a href="index.html#scrapy.http.Request" title="scrapy.http.request.Request">[Request]{.pre}</a>{.reference .internal}[[]]{.pre}]{.p}]{.sig-return-typehint}]{.sig-return}<a href="_modules/scrapy/downloadermiddlewares/retry.html#get_retry_request">[[[source]]{.pre}]{.viewcode-link}</a>{.reference .internal}<a href="#scrapy.downloadermiddlewares.retry.get_retry_request" title="Permalink to this definition">¶</a>{.headerlink}</p>
<p>:   Returns a new [<code>Request</code>{.xref .py .py-class .docutils .literal
.notranslate}]{.pre} object to retry the specified request, or
[<code>None</code>{.docutils .literal .notranslate}]{.pre} if retries of the
specified request have been exhausted.</p>
<pre><code>For example, in a [[`Spider`{.xref .py .py-class .docutils .literal
.notranslate}]{.pre}](index.html#scrapy.Spider &quot;scrapy.Spider&quot;){.reference
.internal} callback, you could use it as follows:

::: {.highlight-default .notranslate}
::: highlight
    def parse(self, response):
        if not response.text:
            new_request_or_none = get_retry_request(
                response.request,
                spider=self,
                reason='empty',
            )
            return new_request_or_none
:::
:::

*spider* is the [[`Spider`{.xref .py .py-class .docutils .literal
.notranslate}]{.pre}](index.html#scrapy.Spider &quot;scrapy.Spider&quot;){.reference
.internal} instance which is asking for the retry request. It is
used to access the [[settings]{.std
.std-ref}](index.html#topics-settings){.hoverxref .tooltip
.reference .internal} and [[stats]{.std
.std-ref}](index.html#topics-stats){.hoverxref .tooltip .reference
.internal}, and to provide extra logging context (see
[[`logging.debug()`{.xref .py .py-func .docutils .literal
.notranslate}]{.pre}](https://docs.python.org/3/library/logging.html#logging.debug &quot;(in Python v3.12)&quot;){.reference
.external}).

*reason* is a string or an [[`Exception`{.xref .py .py-class
.docutils .literal
.notranslate}]{.pre}](https://docs.python.org/3/library/exceptions.html#Exception &quot;(in Python v3.12)&quot;){.reference
.external} object that indicates the reason why the request needs to
be retried. It is used to name retry stats.

*max_retry_times* is a number that determines the maximum number of
times that *request* can be retried. If not specified or
[`None`{.docutils .literal .notranslate}]{.pre}, the number is read
from the [[`max_retry_times`{.xref .std .std-reqmeta .docutils
.literal
.notranslate}]{.pre}](index.html#std-reqmeta-max_retry_times){.hoverxref
.tooltip .reference .internal} meta key of the request. If the
[[`max_retry_times`{.xref .std .std-reqmeta .docutils .literal
.notranslate}]{.pre}](index.html#std-reqmeta-max_retry_times){.hoverxref
.tooltip .reference .internal} meta key is not defined or
[`None`{.docutils .literal .notranslate}]{.pre}, the number is read
from the [[`RETRY_TIMES`{.xref .std .std-setting .docutils .literal
.notranslate}]{.pre}](#std-setting-RETRY_TIMES){.hoverxref .tooltip
.reference .internal} setting.

*priority_adjust* is a number that determines how the priority of
the new request changes in relation to *request*. If not specified,
the number is read from the [[`RETRY_PRIORITY_ADJUST`{.xref .std
.std-setting .docutils .literal
.notranslate}]{.pre}](#std-setting-RETRY_PRIORITY_ADJUST){.hoverxref
.tooltip .reference .internal} setting.

*logger* is the logging.Logger object to be used when logging
messages

*stats_base_key* is a string to be used as the base key for the
retry-related job stats
</code></pre>
<p>::: {#retrymiddleware-settings .section}</p>
<h6 id="retrymiddleware-settingsheaderlink"><a class="header" href="#retrymiddleware-settingsheaderlink">RetryMiddleware Settings<a href="#retrymiddleware-settings" title="Permalink to this heading">¶</a>{.headerlink}</a></h6>
<p>::: {#retry-enabled .section}
[]{#std-setting-RETRY_ENABLED}RETRY_ENABLED<a href="#retry-enabled" title="Permalink to this heading">¶</a>{.headerlink}</p>
<p>Default: [<code>True</code>{.docutils .literal .notranslate}]{.pre}</p>
<p>Whether the Retry middleware will be enabled.
:::</p>
<p>::: {#retry-times .section}
[]{#std-setting-RETRY_TIMES}RETRY_TIMES<a href="#retry-times" title="Permalink to this heading">¶</a>{.headerlink}</p>
<p>Default: [<code>2</code>{.docutils .literal .notranslate}]{.pre}</p>
<p>Maximum number of times to retry, in addition to the first download.</p>
<p>Maximum number of retries can also be specified per-request using
<a href="index.html#std-reqmeta-max_retry_times">[<code>max_retry_times</code>{.xref .std .std-reqmeta .docutils .literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal} attribute of [<code>Request.meta</code>{.xref .py
.py-attr .docutils .literal .notranslate}]{.pre}. When initialized, the
<a href="index.html#std-reqmeta-max_retry_times">[<code>max_retry_times</code>{.xref .std .std-reqmeta .docutils .literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal} meta key takes higher precedence over the
<a href="#std-setting-RETRY_TIMES">[<code>RETRY_TIMES</code>{.xref .std .std-setting .docutils .literal
.notranslate}]{.pre}</a>{.hoverxref .tooltip
.reference .internal} setting.
:::</p>
<p>::: {#retry-http-codes .section}
[]{#std-setting-RETRY_HTTP_CODES}RETRY_HTTP_CODES<a href="#retry-http-codes" title="Permalink to this heading">¶</a>{.headerlink}</p>
<p>Default: [<code>[500,</code>{.docutils .literal .notranslate}]{.pre}<code> </code>{.docutils
.literal .notranslate}[<code>502,</code>{.docutils .literal
.notranslate}]{.pre}<code> </code>{.docutils .literal
.notranslate}[<code>503,</code>{.docutils .literal
.notranslate}]{.pre}<code> </code>{.docutils .literal
.notranslate}[<code>504,</code>{.docutils .literal
.notranslate}]{.pre}<code> </code>{.docutils .literal
.notranslate}[<code>522,</code>{.docutils .literal
.notranslate}]{.pre}<code> </code>{.docutils .literal
.notranslate}[<code>524,</code>{.docutils .literal
.notranslate}]{.pre}<code> </code>{.docutils .literal
.notranslate}[<code>408,</code>{.docutils .literal
.notranslate}]{.pre}<code> </code>{.docutils .literal
.notranslate}[<code>429]</code>{.docutils .literal .notranslate}]{.pre}</p>
<p>Which HTTP response codes to retry. Other errors (DNS lookup issues,
connections lost, etc) are always retried.</p>
<p>In some cases you may want to add 400 to <a href="#std-setting-RETRY_HTTP_CODES">[<code>RETRY_HTTP_CODES</code>{.xref .std
.std-setting .docutils .literal
.notranslate}]{.pre}</a>{.hoverxref .tooltip
.reference .internal} because it is a common code used to indicate
server overload. It is not included by default because HTTP specs say
so.
:::</p>
<p>::: {#retry-exceptions .section}
[]{#std-setting-RETRY_EXCEPTIONS}RETRY_EXCEPTIONS<a href="#retry-exceptions" title="Permalink to this heading">¶</a>{.headerlink}</p>
<p>Default:</p>
<p>::: {.highlight-default .notranslate}
::: highlight
[
'twisted.internet.defer.TimeoutError',
'twisted.internet.error.TimeoutError',
'twisted.internet.error.DNSLookupError',
'twisted.internet.error.ConnectionRefusedError',
'twisted.internet.error.ConnectionDone',
'twisted.internet.error.ConnectError',
'twisted.internet.error.ConnectionLost',
'twisted.internet.error.TCPTimedOutError',
'twisted.web.client.ResponseFailed',
IOError,
'scrapy.core.downloader.handlers.http11.TunnelError',
]
:::
:::</p>
<p>List of exceptions to retry.</p>
<p>Each list entry may be an exception type or its import path as a string.</p>
<p>An exception will not be caught when the exception type is not in
<a href="#std-setting-RETRY_EXCEPTIONS">[<code>RETRY_EXCEPTIONS</code>{.xref .std .std-setting .docutils .literal
.notranslate}]{.pre}</a>{.hoverxref .tooltip
.reference .internal} or when the maximum number of retries for a
request has been exceeded (see <a href="#std-setting-RETRY_TIMES">[<code>RETRY_TIMES</code>{.xref .std .std-setting
.docutils .literal
.notranslate}]{.pre}</a>{.hoverxref .tooltip
.reference .internal}). To learn about uncaught exception propagation,
see <a href="#scrapy.downloadermiddlewares.DownloaderMiddleware.process_exception" title="scrapy.downloadermiddlewares.DownloaderMiddleware.process_exception">[<code>process_exception()</code>{.xref .py .py-meth .docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal}.
:::</p>
<p>::: {#retry-priority-adjust .section}
[]{#std-setting-RETRY_PRIORITY_ADJUST}RETRY_PRIORITY_ADJUST<a href="#retry-priority-adjust" title="Permalink to this heading">¶</a>{.headerlink}</p>
<p>Default: [<code>-1</code>{.docutils .literal .notranslate}]{.pre}</p>
<p>Adjust retry request priority relative to original request:</p>
<ul>
<li>
<p>a positive priority adjust means higher priority.</p>
</li>
<li>
<p><strong>a negative priority adjust (default) means lower priority.</strong>
:::
:::
:::</p>
</li>
</ul>
<p>::: {#module-scrapy.downloadermiddlewares.robotstxt .section}
[]{#robotstxtmiddleware}[]{#topics-dlmw-robots}</p>
<h5 id="robotstxtmiddlewareheaderlink"><a class="header" href="#robotstxtmiddlewareheaderlink">RobotsTxtMiddleware<a href="#module-scrapy.downloadermiddlewares.robotstxt" title="Permalink to this heading">¶</a>{.headerlink}</a></h5>
<p><em>[class]{.pre}[ ]{.w}</em>[[scrapy.downloadermiddlewares.robotstxt.]{.pre}]{.sig-prename .descclassname}[[RobotsTxtMiddleware]{.pre}]{.sig-name .descname}<a href="_modules/scrapy/downloadermiddlewares/robotstxt.html#RobotsTxtMiddleware">[[[source]]{.pre}]{.viewcode-link}</a>{.reference .internal}<a href="#scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware" title="Permalink to this definition">¶</a>{.headerlink}</p>
<p>:   This middleware filters out requests forbidden by the robots.txt
exclusion standard.</p>
<pre><code>To make sure Scrapy respects robots.txt make sure the middleware is
enabled and the [[`ROBOTSTXT_OBEY`{.xref .std .std-setting .docutils
.literal
.notranslate}]{.pre}](index.html#std-setting-ROBOTSTXT_OBEY){.hoverxref
.tooltip .reference .internal} setting is enabled.

The [[`ROBOTSTXT_USER_AGENT`{.xref .std .std-setting .docutils
.literal
.notranslate}]{.pre}](index.html#std-setting-ROBOTSTXT_USER_AGENT){.hoverxref
.tooltip .reference .internal} setting can be used to specify the
user agent string to use for matching in the
[robots.txt](https://www.robotstxt.org/){.reference .external} file.
If it is [`None`{.docutils .literal .notranslate}]{.pre}, the
User-Agent header you are sending with the request or the
[[`USER_AGENT`{.xref .std .std-setting .docutils .literal
.notranslate}]{.pre}](index.html#std-setting-USER_AGENT){.hoverxref
.tooltip .reference .internal} setting (in that order) will be used
for determining the user agent to use in the
[robots.txt](https://www.robotstxt.org/){.reference .external} file.

This middleware has to be combined with a
[robots.txt](https://www.robotstxt.org/){.reference .external}
parser.

Scrapy ships with support for the following
[robots.txt](https://www.robotstxt.org/){.reference .external}
parsers:

-   [[Protego]{.std .std-ref}](#protego-parser){.hoverxref .tooltip
    .reference .internal} (default)

-   [[RobotFileParser]{.std
    .std-ref}](#python-robotfileparser){.hoverxref .tooltip
    .reference .internal}

-   [[Robotexclusionrulesparser]{.std
    .std-ref}](#rerp-parser){.hoverxref .tooltip .reference
    .internal}

-   [[Reppy]{.std .std-ref}](#reppy-parser){.hoverxref .tooltip
    .reference .internal} (deprecated)

You can change the
[robots.txt](https://www.robotstxt.org/){.reference .external}
parser with the [[`ROBOTSTXT_PARSER`{.xref .std .std-setting
.docutils .literal
.notranslate}]{.pre}](index.html#std-setting-ROBOTSTXT_PARSER){.hoverxref
.tooltip .reference .internal} setting. Or you can also [[implement
support for a new parser]{.std
.std-ref}](#support-for-new-robots-parser){.hoverxref .tooltip
.reference .internal}.
</code></pre>
<p>If [<code>Request.meta</code>{.xref .py .py-attr .docutils .literal
.notranslate}]{.pre} has [<code>dont_obey_robotstxt</code>{.docutils .literal
.notranslate}]{.pre} key set to True the request will be ignored by this
middleware even if <a href="index.html#std-setting-ROBOTSTXT_OBEY">[<code>ROBOTSTXT_OBEY</code>{.xref .std .std-setting .docutils
.literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal} is enabled.</p>
<p>Parsers vary in several aspects:</p>
<ul>
<li>
<p>Language of implementation</p>
</li>
<li>
<p>Supported specification</p>
</li>
<li>
<p>Support for wildcard matching</p>
</li>
<li>
<p>Usage of <a href="https://developers.google.com/search/reference/robots_txt#order-of-precedence-for-group-member-lines">length based
rule</a>{.reference
.external}: in particular for [<code>Allow</code>{.docutils .literal
.notranslate}]{.pre} and [<code>Disallow</code>{.docutils .literal
.notranslate}]{.pre} directives, where the most specific rule based
on the length of the path trumps the less specific (shorter) rule</p>
</li>
</ul>
<p>Performance comparison of different parsers is available at <a href="https://github.com/scrapy/scrapy/issues/3969">the
following link</a>{.reference
.external}.</p>
<p>::: {#protego-parser .section}
[]{#id1}</p>
<h6 id="protego-parserheaderlink"><a class="header" href="#protego-parserheaderlink">Protego parser<a href="#protego-parser" title="Permalink to this heading">¶</a>{.headerlink}</a></h6>
<p>Based on <a href="https://github.com/scrapy/protego">Protego</a>{.reference
.external}:</p>
<ul>
<li>
<p>implemented in Python</p>
</li>
<li>
<p>is compliant with <a href="https://developers.google.com/search/reference/robots_txt">Google's Robots.txt
Specification</a>{.reference
.external}</p>
</li>
<li>
<p>supports wildcard matching</p>
</li>
<li>
<p>uses the length based rule</p>
</li>
</ul>
<p>Scrapy uses this parser by default.
:::</p>
<p>::: {#robotfileparser .section}
[]{#python-robotfileparser}</p>
<h6 id="robotfileparserheaderlink"><a class="header" href="#robotfileparserheaderlink">RobotFileParser<a href="#robotfileparser" title="Permalink to this heading">¶</a>{.headerlink}</a></h6>
<p>Based on <a href="https://docs.python.org/3/library/urllib.robotparser.html#urllib.robotparser.RobotFileParser" title="(in Python v3.12)">[<code>RobotFileParser</code>{.xref .py .py-class .docutils .literal
.notranslate}]{.pre}</a>{.reference
.external}:</p>
<ul>
<li>
<p>is Python's built-in
<a href="https://www.robotstxt.org/">robots.txt</a>{.reference .external}
parser</p>
</li>
<li>
<p>is compliant with <a href="https://www.robotstxt.org/norobots-rfc.txt">Martijn Koster's 1996 draft
specification</a>{.reference
.external}</p>
</li>
<li>
<p>lacks support for wildcard matching</p>
</li>
<li>
<p>doesn't use the length based rule</p>
</li>
</ul>
<p>It is faster than Protego and backward-compatible with versions of
Scrapy before 1.8.0.</p>
<p>In order to use this parser, set:</p>
<ul>
<li><a href="index.html#std-setting-ROBOTSTXT_PARSER">[<code>ROBOTSTXT_PARSER</code>{.xref .std .std-setting .docutils .literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal} to
[<code>scrapy.robotstxt.PythonRobotParser</code>{.docutils .literal
.notranslate}]{.pre}
:::</li>
</ul>
<p>::: {#reppy-parser .section}
[]{#id2}</p>
<h6 id="reppy-parserheaderlink"><a class="header" href="#reppy-parserheaderlink">Reppy parser<a href="#reppy-parser" title="Permalink to this heading">¶</a>{.headerlink}</a></h6>
<p>Based on <a href="https://github.com/seomoz/reppy/">Reppy</a>{.reference
.external}:</p>
<ul>
<li>
<p>is a Python wrapper around <a href="https://github.com/seomoz/rep-cpp">Robots Exclusion Protocol Parser for
C++</a>{.reference .external}</p>
</li>
<li>
<p>is compliant with <a href="https://www.robotstxt.org/norobots-rfc.txt">Martijn Koster's 1996 draft
specification</a>{.reference
.external}</p>
</li>
<li>
<p>supports wildcard matching</p>
</li>
<li>
<p>uses the length based rule</p>
</li>
</ul>
<p>Native implementation, provides better speed than Protego.</p>
<p>In order to use this parser:</p>
<ul>
<li>
<p>Install <a href="https://github.com/seomoz/reppy/">Reppy</a>{.reference
.external} by running [<code>pip</code>{.docutils .literal
.notranslate}]{.pre}<code> </code>{.docutils .literal
.notranslate}[<code>install</code>{.docutils .literal
.notranslate}]{.pre}<code> </code>{.docutils .literal
.notranslate}[<code>reppy</code>{.docutils .literal .notranslate}]{.pre}</p>
<blockquote>
<div>
<p>::: {.admonition .warning}
Warning</p>
<p><a href="https://github.com/seomoz/reppy/issues/122">Upstream issue
#122</a>{.reference
.external} prevents reppy usage in Python 3.9+. Because of this
the Reppy parser is deprecated.
:::</p>
</div>
</blockquote>
</li>
<li>
<p>Set <a href="index.html#std-setting-ROBOTSTXT_PARSER">[<code>ROBOTSTXT_PARSER</code>{.xref .std .std-setting .docutils .literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal} setting to
[<code>scrapy.robotstxt.ReppyRobotParser</code>{.docutils .literal
.notranslate}]{.pre}
:::</p>
</li>
</ul>
<p>::: {#robotexclusionrulesparser .section}
[]{#rerp-parser}</p>
<h6 id="robotexclusionrulesparserheaderlink"><a class="header" href="#robotexclusionrulesparserheaderlink">Robotexclusionrulesparser<a href="#robotexclusionrulesparser" title="Permalink to this heading">¶</a>{.headerlink}</a></h6>
<p>Based on
<a href="http://nikitathespider.com/python/rerp/">Robotexclusionrulesparser</a>{.reference
.external}:</p>
<ul>
<li>
<p>implemented in Python</p>
</li>
<li>
<p>is compliant with <a href="https://www.robotstxt.org/norobots-rfc.txt">Martijn Koster's 1996 draft
specification</a>{.reference
.external}</p>
</li>
<li>
<p>supports wildcard matching</p>
</li>
<li>
<p>doesn't use the length based rule</p>
</li>
</ul>
<p>In order to use this parser:</p>
<ul>
<li>
<p>Install
<a href="http://nikitathespider.com/python/rerp/">Robotexclusionrulesparser</a>{.reference
.external} by running [<code>pip</code>{.docutils .literal
.notranslate}]{.pre}<code> </code>{.docutils .literal
.notranslate}[<code>install</code>{.docutils .literal
.notranslate}]{.pre}<code> </code>{.docutils .literal
.notranslate}[<code>robotexclusionrulesparser</code>{.docutils .literal
.notranslate}]{.pre}</p>
</li>
<li>
<p>Set <a href="index.html#std-setting-ROBOTSTXT_PARSER">[<code>ROBOTSTXT_PARSER</code>{.xref .std .std-setting .docutils .literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal} setting to
[<code>scrapy.robotstxt.RerpRobotParser</code>{.docutils .literal
.notranslate}]{.pre}
:::</p>
</li>
</ul>
<p>::: {#implementing-support-for-a-new-parser .section}
[]{#support-for-new-robots-parser}</p>
<h6 id="implementing-support-for-a-new-parserheaderlink"><a class="header" href="#implementing-support-for-a-new-parserheaderlink">Implementing support for a new parser<a href="#implementing-support-for-a-new-parser" title="Permalink to this heading">¶</a>{.headerlink}</a></h6>
<p>You can implement support for a new
<a href="https://www.robotstxt.org/">robots.txt</a>{.reference .external} parser by
subclassing the abstract base class <a href="#scrapy.robotstxt.RobotParser" title="scrapy.robotstxt.RobotParser">[<code>RobotParser</code>{.xref .py .py-class
.docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal} and implementing the methods described below.</p>
<p>[]{#module-scrapy.robotstxt .target}</p>
<p><em>[class]{.pre}[ ]{.w}</em>[[scrapy.robotstxt.]{.pre}]{.sig-prename .descclassname}[[RobotParser]{.pre}]{.sig-name .descname}<a href="_modules/scrapy/robotstxt.html#RobotParser">[[[source]]{.pre}]{.viewcode-link}</a>{.reference .internal}<a href="#scrapy.robotstxt.RobotParser" title="Permalink to this definition">¶</a>{.headerlink}</p>
<p>:</p>
<pre><code>*[abstract]{.pre}[ ]{.w}*[[allowed]{.pre}]{.sig-name .descname}[(]{.sig-paren}*[[url]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[[[Union]{.pre}](https://docs.python.org/3/library/typing.html#typing.Union &quot;(in Python v3.12)&quot;){.reference .external}[[\[]{.pre}]{.p}[[str]{.pre}](https://docs.python.org/3/library/stdtypes.html#str &quot;(in Python v3.12)&quot;){.reference .external}[[,]{.pre}]{.p}[ ]{.w}[[bytes]{.pre}](https://docs.python.org/3/library/stdtypes.html#bytes &quot;(in Python v3.12)&quot;){.reference .external}[[\]]{.pre}]{.p}]{.n}*, *[[user_agent]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[[[Union]{.pre}](https://docs.python.org/3/library/typing.html#typing.Union &quot;(in Python v3.12)&quot;){.reference .external}[[\[]{.pre}]{.p}[[str]{.pre}](https://docs.python.org/3/library/stdtypes.html#str &quot;(in Python v3.12)&quot;){.reference .external}[[,]{.pre}]{.p}[ ]{.w}[[bytes]{.pre}](https://docs.python.org/3/library/stdtypes.html#bytes &quot;(in Python v3.12)&quot;){.reference .external}[[\]]{.pre}]{.p}]{.n}*[)]{.sig-paren} [[→]{.sig-return-icon} [[[bool]{.pre}](https://docs.python.org/3/library/functions.html#bool &quot;(in Python v3.12)&quot;){.reference .external}]{.sig-return-typehint}]{.sig-return}[[[\[source\]]{.pre}]{.viewcode-link}](_modules/scrapy/robotstxt.html#RobotParser.allowed){.reference .internal}[¶](#scrapy.robotstxt.RobotParser.allowed &quot;Permalink to this definition&quot;){.headerlink}

:   Return [`True`{.docutils .literal .notranslate}]{.pre} if
    [`user_agent`{.docutils .literal .notranslate}]{.pre} is allowed
    to crawl [`url`{.docutils .literal .notranslate}]{.pre},
    otherwise return [`False`{.docutils .literal
    .notranslate}]{.pre}.

    Parameters

    :   -   **url**
            ([*str*](https://docs.python.org/3/library/stdtypes.html#str &quot;(in Python v3.12)&quot;){.reference
            .external} *or*
            [*bytes*](https://docs.python.org/3/library/stdtypes.html#bytes &quot;(in Python v3.12)&quot;){.reference
            .external}) -- Absolute URL

        -   **user_agent**
            ([*str*](https://docs.python.org/3/library/stdtypes.html#str &quot;(in Python v3.12)&quot;){.reference
            .external} *or*
            [*bytes*](https://docs.python.org/3/library/stdtypes.html#bytes &quot;(in Python v3.12)&quot;){.reference
            .external}) -- User agent

*[abstract]{.pre}[ ]{.w}[classmethod]{.pre}[ ]{.w}*[[from_crawler]{.pre}]{.sig-name .descname}[(]{.sig-paren}*[[crawler]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[[[Crawler]{.pre}](index.html#scrapy.crawler.Crawler &quot;scrapy.crawler.Crawler&quot;){.reference .internal}]{.n}*, *[[robotstxt_body]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[[[bytes]{.pre}](https://docs.python.org/3/library/stdtypes.html#bytes &quot;(in Python v3.12)&quot;){.reference .external}]{.n}*[)]{.sig-paren} [[→]{.sig-return-icon} [[Self]{.pre}]{.sig-return-typehint}]{.sig-return}[[[\[source\]]{.pre}]{.viewcode-link}](_modules/scrapy/robotstxt.html#RobotParser.from_crawler){.reference .internal}[¶](#scrapy.robotstxt.RobotParser.from_crawler &quot;Permalink to this definition&quot;){.headerlink}

:   Parse the content of a
    [robots.txt](https://www.robotstxt.org/){.reference .external}
    file as bytes. This must be a class method. It must return a new
    instance of the parser backend.

    Parameters

    :   -   **crawler** ([[`Crawler`{.xref .py .py-class .docutils
            .literal
            .notranslate}]{.pre}](index.html#scrapy.crawler.Crawler &quot;scrapy.crawler.Crawler&quot;){.reference
            .internal} instance) -- crawler which made the request

        -   **robotstxt_body**
            ([*bytes*](https://docs.python.org/3/library/stdtypes.html#bytes &quot;(in Python v3.12)&quot;){.reference
            .external}) -- content of a
            [robots.txt](https://www.robotstxt.org/){.reference
            .external} file.
</code></pre>
<p>:::
:::</p>
<p>::: {#module-scrapy.downloadermiddlewares.stats .section}
[]{#downloaderstats}</p>
<h5 id="downloaderstatsheaderlink"><a class="header" href="#downloaderstatsheaderlink">DownloaderStats<a href="#module-scrapy.downloadermiddlewares.stats" title="Permalink to this heading">¶</a>{.headerlink}</a></h5>
<p><em>[class]{.pre}[ ]{.w}</em>[[scrapy.downloadermiddlewares.stats.]{.pre}]{.sig-prename .descclassname}[[DownloaderStats]{.pre}]{.sig-name .descname}<a href="_modules/scrapy/downloadermiddlewares/stats.html#DownloaderStats">[[[source]]{.pre}]{.viewcode-link}</a>{.reference .internal}<a href="#scrapy.downloadermiddlewares.stats.DownloaderStats" title="Permalink to this definition">¶</a>{.headerlink}</p>
<p>:   Middleware that stores stats of all requests, responses and
exceptions that pass through it.</p>
<pre><code>To use this middleware you must enable the
[[`DOWNLOADER_STATS`{.xref .std .std-setting .docutils .literal
.notranslate}]{.pre}](index.html#std-setting-DOWNLOADER_STATS){.hoverxref
.tooltip .reference .internal} setting.
</code></pre>
<p>:::</p>
<p>::: {#module-scrapy.downloadermiddlewares.useragent .section}
[]{#useragentmiddleware}</p>
<h5 id="useragentmiddlewareheaderlink"><a class="header" href="#useragentmiddlewareheaderlink">UserAgentMiddleware<a href="#module-scrapy.downloadermiddlewares.useragent" title="Permalink to this heading">¶</a>{.headerlink}</a></h5>
<p><em>[class]{.pre}[ ]{.w}</em>[[scrapy.downloadermiddlewares.useragent.]{.pre}]{.sig-prename .descclassname}[[UserAgentMiddleware]{.pre}]{.sig-name .descname}<a href="_modules/scrapy/downloadermiddlewares/useragent.html#UserAgentMiddleware">[[[source]]{.pre}]{.viewcode-link}</a>{.reference .internal}<a href="#scrapy.downloadermiddlewares.useragent.UserAgentMiddleware" title="Permalink to this definition">¶</a>{.headerlink}</p>
<p>:   Middleware that allows spiders to override the default user agent.</p>
<pre><code>In order for a spider to override the default user agent, its
[`user_agent`{.docutils .literal .notranslate}]{.pre} attribute must
be set.
</code></pre>
<p>:::</p>
<p>::: {#module-scrapy.downloadermiddlewares.ajaxcrawl .section}
[]{#ajaxcrawlmiddleware}[]{#ajaxcrawl-middleware}</p>
<h5 id="ajaxcrawlmiddlewareheaderlink"><a class="header" href="#ajaxcrawlmiddlewareheaderlink">AjaxCrawlMiddleware<a href="#module-scrapy.downloadermiddlewares.ajaxcrawl" title="Permalink to this heading">¶</a>{.headerlink}</a></h5>
<p><em>[class]{.pre}[ ]{.w}</em>[[scrapy.downloadermiddlewares.ajaxcrawl.]{.pre}]{.sig-prename .descclassname}[[AjaxCrawlMiddleware]{.pre}]{.sig-name .descname}<a href="_modules/scrapy/downloadermiddlewares/ajaxcrawl.html#AjaxCrawlMiddleware">[[[source]]{.pre}]{.viewcode-link}</a>{.reference .internal}<a href="#scrapy.downloadermiddlewares.ajaxcrawl.AjaxCrawlMiddleware" title="Permalink to this definition">¶</a>{.headerlink}</p>
<p>:   Middleware that finds 'AJAX crawlable' page variants based on
meta-fragment html tag. See
<a href="https://developers.google.com/search/docs/ajax-crawling/docs/getting-started">https://developers.google.com/search/docs/ajax-crawling/docs/getting-started</a>{.reference
.external} for more info.</p>
<pre><code>::: {.admonition .note}
Note

Scrapy finds 'AJAX crawlable' pages for URLs like
[`'http://example.com/!#foo=bar'`{.docutils .literal
.notranslate}]{.pre} even without this middleware.
AjaxCrawlMiddleware is necessary when URL doesn't contain
[`'!#'`{.docutils .literal .notranslate}]{.pre}. This is often a
case for 'index' or 'main' website pages.
:::
</code></pre>
<p>::: {#ajaxcrawlmiddleware-settings .section}</p>
<h6 id="ajaxcrawlmiddleware-settingsheaderlink"><a class="header" href="#ajaxcrawlmiddleware-settingsheaderlink">AjaxCrawlMiddleware Settings<a href="#ajaxcrawlmiddleware-settings" title="Permalink to this heading">¶</a>{.headerlink}</a></h6>
<p>::: {#ajaxcrawl-enabled .section}
[]{#std-setting-AJAXCRAWL_ENABLED}AJAXCRAWL_ENABLED<a href="#ajaxcrawl-enabled" title="Permalink to this heading">¶</a>{.headerlink}</p>
<p>Default: [<code>False</code>{.docutils .literal .notranslate}]{.pre}</p>
<p>Whether the AjaxCrawlMiddleware will be enabled. You may want to enable
it for <a href="index.html#topics-broad-crawls">[broad crawls]{.std
.std-ref}</a>{.hoverxref .tooltip
.reference .internal}.
:::
:::</p>
<p>::: {#httpproxymiddleware-settings .section}</p>
<h6 id="httpproxymiddleware-settingsheaderlink"><a class="header" href="#httpproxymiddleware-settingsheaderlink">HttpProxyMiddleware settings<a href="#httpproxymiddleware-settings" title="Permalink to this heading">¶</a>{.headerlink}</a></h6>
<p>[]{#std-setting-HTTPPROXY_ENABLED .target}</p>
<p>::: {#httpproxy-enabled .section}
[]{#std-setting-HTTPPROXY_AUTH_ENCODING}HTTPPROXY_ENABLED<a href="#httpproxy-enabled" title="Permalink to this heading">¶</a>{.headerlink}</p>
<p>Default: [<code>True</code>{.docutils .literal .notranslate}]{.pre}</p>
<p>Whether or not to enable the [<code>HttpProxyMiddleware</code>{.xref .py .py-class
.docutils .literal .notranslate}]{.pre}.
:::</p>
<p>::: {#httpproxy-auth-encoding .section}
HTTPPROXY_AUTH_ENCODING<a href="#httpproxy-auth-encoding" title="Permalink to this heading">¶</a>{.headerlink}</p>
<p>Default: [<code>&quot;latin-1&quot;</code>{.docutils .literal .notranslate}]{.pre}</p>
<p>The default encoding for proxy authentication on
[<code>HttpProxyMiddleware</code>{.xref .py .py-class .docutils .literal
.notranslate}]{.pre}.
:::
:::
:::
:::
:::</p>
<p>[]{#document-topics/spider-middleware}</p>
<p>::: {#spider-middleware .section}
[]{#topics-spider-middleware}</p>
<h3 id="spider-middlewareheaderlink"><a class="header" href="#spider-middlewareheaderlink">Spider Middleware<a href="#spider-middleware" title="Permalink to this heading">¶</a>{.headerlink}</a></h3>
<p>The spider middleware is a framework of hooks into Scrapy's spider
processing mechanism where you can plug custom functionality to process
the responses that are sent to <a href="index.html#topics-spiders">[Spiders]{.std
.std-ref}</a>{.hoverxref .tooltip .reference
.internal} for processing and to process the requests and items that are
generated from spiders.</p>
<p>::: {#activating-a-spider-middleware .section}
[]{#topics-spider-middleware-setting}</p>
<h4 id="activating-a-spider-middlewareheaderlink"><a class="header" href="#activating-a-spider-middlewareheaderlink">Activating a spider middleware<a href="#activating-a-spider-middleware" title="Permalink to this heading">¶</a>{.headerlink}</a></h4>
<p>To activate a spider middleware component, add it to the
<a href="index.html#std-setting-SPIDER_MIDDLEWARES">[<code>SPIDER_MIDDLEWARES</code>{.xref .std .std-setting .docutils .literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal} setting, which is a dict whose keys are
the middleware class path and their values are the middleware orders.</p>
<p>Here's an example:</p>
<p>::: {.highlight-python .notranslate}
::: highlight
SPIDER_MIDDLEWARES = {
&quot;myproject.middlewares.CustomSpiderMiddleware&quot;: 543,
}
:::
:::</p>
<p>The <a href="index.html#std-setting-SPIDER_MIDDLEWARES">[<code>SPIDER_MIDDLEWARES</code>{.xref .std .std-setting .docutils .literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal} setting is merged with the
<a href="index.html#std-setting-SPIDER_MIDDLEWARES_BASE">[<code>SPIDER_MIDDLEWARES_BASE</code>{.xref .std .std-setting .docutils .literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal} setting defined in Scrapy (and not meant
to be overridden) and then sorted by order to get the final sorted list
of enabled middlewares: the first middleware is the one closer to the
engine and the last is the one closer to the spider. In other words, the
<a href="#scrapy.spidermiddlewares.SpiderMiddleware.process_spider_input" title="scrapy.spidermiddlewares.SpiderMiddleware.process_spider_input">[<code>process_spider_input()</code>{.xref .py .py-meth .docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal} method of each middleware will be invoked in increasing
middleware order (100, 200, 300, ...), and the
<a href="#scrapy.spidermiddlewares.SpiderMiddleware.process_spider_output" title="scrapy.spidermiddlewares.SpiderMiddleware.process_spider_output">[<code>process_spider_output()</code>{.xref .py .py-meth .docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal} method of each middleware will be invoked in decreasing
order.</p>
<p>To decide which order to assign to your middleware see the
<a href="index.html#std-setting-SPIDER_MIDDLEWARES_BASE">[<code>SPIDER_MIDDLEWARES_BASE</code>{.xref .std .std-setting .docutils .literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal} setting and pick a value according to
where you want to insert the middleware. The order does matter because
each middleware performs a different action and your middleware could
depend on some previous (or subsequent) middleware being applied.</p>
<p>If you want to disable a builtin middleware (the ones defined in
<a href="index.html#std-setting-SPIDER_MIDDLEWARES_BASE">[<code>SPIDER_MIDDLEWARES_BASE</code>{.xref .std .std-setting .docutils .literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal}, and enabled by default) you must define
it in your project <a href="index.html#std-setting-SPIDER_MIDDLEWARES">[<code>SPIDER_MIDDLEWARES</code>{.xref .std .std-setting
.docutils .literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal} setting and assign [<code>None</code>{.docutils
.literal .notranslate}]{.pre} as its value. For example, if you want to
disable the off-site middleware:</p>
<p>::: {.highlight-python .notranslate}
::: highlight
SPIDER_MIDDLEWARES = {
&quot;myproject.middlewares.CustomSpiderMiddleware&quot;: 543,
&quot;scrapy.spidermiddlewares.offsite.OffsiteMiddleware&quot;: None,
}
:::
:::</p>
<p>Finally, keep in mind that some middlewares may need to be enabled
through a particular setting. See each middleware documentation for more
info.
:::</p>
<p>::: {#writing-your-own-spider-middleware .section}
[]{#custom-spider-middleware}</p>
<h4 id="writing-your-own-spider-middlewareheaderlink"><a class="header" href="#writing-your-own-spider-middlewareheaderlink">Writing your own spider middleware<a href="#writing-your-own-spider-middleware" title="Permalink to this heading">¶</a>{.headerlink}</a></h4>
<p>Each spider middleware is a Python class that defines one or more of the
methods defined below.</p>
<p>The main entry point is the [<code>from_crawler</code>{.docutils .literal
.notranslate}]{.pre} class method, which receives a <a href="index.html#scrapy.crawler.Crawler" title="scrapy.crawler.Crawler">[<code>Crawler</code>{.xref
.py .py-class .docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal} instance. The <a href="index.html#scrapy.crawler.Crawler" title="scrapy.crawler.Crawler">[<code>Crawler</code>{.xref .py .py-class .docutils
.literal
.notranslate}]{.pre}</a>{.reference
.internal} object gives you access, for example, to the <a href="index.html#topics-settings">[settings]{.std
.std-ref}</a>{.hoverxref .tooltip .reference
.internal}.</p>
<p>[]{#module-scrapy.spidermiddlewares .target}</p>
<p><em>[class]{.pre}[ ]{.w}</em>[[scrapy.spidermiddlewares.]{.pre}]{.sig-prename .descclassname}[[SpiderMiddleware]{.pre}]{.sig-name .descname}<a href="#scrapy.spidermiddlewares.SpiderMiddleware" title="Permalink to this definition">¶</a>{.headerlink}</p>
<p>:</p>
<pre><code>[[process_spider_input]{.pre}]{.sig-name .descname}[(]{.sig-paren}*[[response]{.pre}]{.n}*, *[[spider]{.pre}]{.n}*[)]{.sig-paren}[¶](#scrapy.spidermiddlewares.SpiderMiddleware.process_spider_input &quot;Permalink to this definition&quot;){.headerlink}

:   This method is called for each response that goes through the
    spider middleware and into the spider, for processing.

    [[`process_spider_input()`{.xref .py .py-meth .docutils .literal
    .notranslate}]{.pre}](#scrapy.spidermiddlewares.SpiderMiddleware.process_spider_input &quot;scrapy.spidermiddlewares.SpiderMiddleware.process_spider_input&quot;){.reference
    .internal} should return [`None`{.docutils .literal
    .notranslate}]{.pre} or raise an exception.

    If it returns [`None`{.docutils .literal .notranslate}]{.pre},
    Scrapy will continue processing this response, executing all
    other middlewares until, finally, the response is handed to the
    spider for processing.

    If it raises an exception, Scrapy won't bother calling any other
    spider middleware [[`process_spider_input()`{.xref .py .py-meth
    .docutils .literal
    .notranslate}]{.pre}](#scrapy.spidermiddlewares.SpiderMiddleware.process_spider_input &quot;scrapy.spidermiddlewares.SpiderMiddleware.process_spider_input&quot;){.reference
    .internal} and will call the request errback if there is one,
    otherwise it will start the [[`process_spider_exception()`{.xref
    .py .py-meth .docutils .literal
    .notranslate}]{.pre}](#scrapy.spidermiddlewares.SpiderMiddleware.process_spider_exception &quot;scrapy.spidermiddlewares.SpiderMiddleware.process_spider_exception&quot;){.reference
    .internal} chain. The output of the errback is chained back in
    the other direction for [[`process_spider_output()`{.xref .py
    .py-meth .docutils .literal
    .notranslate}]{.pre}](#scrapy.spidermiddlewares.SpiderMiddleware.process_spider_output &quot;scrapy.spidermiddlewares.SpiderMiddleware.process_spider_output&quot;){.reference
    .internal} to process it, or
    [[`process_spider_exception()`{.xref .py .py-meth .docutils
    .literal
    .notranslate}]{.pre}](#scrapy.spidermiddlewares.SpiderMiddleware.process_spider_exception &quot;scrapy.spidermiddlewares.SpiderMiddleware.process_spider_exception&quot;){.reference
    .internal} if it raised an exception.

    Parameters

    :   -   **response** ([[`Response`{.xref .py .py-class .docutils
            .literal
            .notranslate}]{.pre}](index.html#scrapy.http.Response &quot;scrapy.http.Response&quot;){.reference
            .internal} object) -- the response being processed

        -   **spider** ([[`Spider`{.xref .py .py-class .docutils
            .literal
            .notranslate}]{.pre}](index.html#scrapy.Spider &quot;scrapy.Spider&quot;){.reference
            .internal} object) -- the spider for which this response
            is intended

[[process_spider_output]{.pre}]{.sig-name .descname}[(]{.sig-paren}*[[response]{.pre}]{.n}*, *[[result]{.pre}]{.n}*, *[[spider]{.pre}]{.n}*[)]{.sig-paren}[¶](#scrapy.spidermiddlewares.SpiderMiddleware.process_spider_output &quot;Permalink to this definition&quot;){.headerlink}

:   This method is called with the results returned from the Spider,
    after it has processed the response.

    [[`process_spider_output()`{.xref .py .py-meth .docutils
    .literal
    .notranslate}]{.pre}](#scrapy.spidermiddlewares.SpiderMiddleware.process_spider_output &quot;scrapy.spidermiddlewares.SpiderMiddleware.process_spider_output&quot;){.reference
    .internal} must return an iterable of [`Request`{.xref .py
    .py-class .docutils .literal .notranslate}]{.pre} objects and
    [[item objects]{.std
    .std-ref}](index.html#topics-items){.hoverxref .tooltip
    .reference .internal}.

    ::: versionchanged
    [Changed in version 2.7: ]{.versionmodified .changed}This method
    may be defined as an [[asynchronous generator]{.xref .std
    .std-term}](https://docs.python.org/3/glossary.html#term-asynchronous-generator &quot;(in Python v3.12)&quot;){.reference
    .external}, in which case [`result`{.docutils .literal
    .notranslate}]{.pre} is an [[asynchronous iterable]{.xref .std
    .std-term}](https://docs.python.org/3/glossary.html#term-asynchronous-iterable &quot;(in Python v3.12)&quot;){.reference
    .external}.
    :::

    Consider defining this method as an [[asynchronous
    generator]{.xref .std
    .std-term}](https://docs.python.org/3/glossary.html#term-asynchronous-generator &quot;(in Python v3.12)&quot;){.reference
    .external}, which will be a requirement in a future version of
    Scrapy. However, if you plan on sharing your spider middleware
    with other people, consider either [[enforcing Scrapy 2.7]{.std
    .std-ref}](index.html#enforce-component-requirements){.hoverxref
    .tooltip .reference .internal} as a minimum requirement of your
    spider middleware, or [[making your spider middleware
    universal]{.std
    .std-ref}](index.html#universal-spider-middleware){.hoverxref
    .tooltip .reference .internal} so that it works with Scrapy
    versions earlier than Scrapy 2.7.

    Parameters

    :   -   **response** ([[`Response`{.xref .py .py-class .docutils
            .literal
            .notranslate}]{.pre}](index.html#scrapy.http.Response &quot;scrapy.http.Response&quot;){.reference
            .internal} object) -- the response which generated this
            output from the spider

        -   **result** (an iterable of [`Request`{.xref .py
            .py-class .docutils .literal .notranslate}]{.pre}
            objects and [[item objects]{.std
            .std-ref}](index.html#topics-items){.hoverxref .tooltip
            .reference .internal}) -- the result returned by the
            spider

        -   **spider** ([[`Spider`{.xref .py .py-class .docutils
            .literal
            .notranslate}]{.pre}](index.html#scrapy.Spider &quot;scrapy.Spider&quot;){.reference
            .internal} object) -- the spider whose result is being
            processed

[[process_spider_output_async]{.pre}]{.sig-name .descname}[(]{.sig-paren}*[[response]{.pre}]{.n}*, *[[result]{.pre}]{.n}*, *[[spider]{.pre}]{.n}*[)]{.sig-paren}[¶](#scrapy.spidermiddlewares.SpiderMiddleware.process_spider_output_async &quot;Permalink to this definition&quot;){.headerlink}

:   ::: versionadded
    [New in version 2.7.]{.versionmodified .added}
    :::

    If defined, this method must be an [[asynchronous
    generator]{.xref .std
    .std-term}](https://docs.python.org/3/glossary.html#term-asynchronous-generator &quot;(in Python v3.12)&quot;){.reference
    .external}, which will be called instead of
    [[`process_spider_output()`{.xref .py .py-meth .docutils
    .literal
    .notranslate}]{.pre}](#scrapy.spidermiddlewares.SpiderMiddleware.process_spider_output &quot;scrapy.spidermiddlewares.SpiderMiddleware.process_spider_output&quot;){.reference
    .internal} if [`result`{.docutils .literal .notranslate}]{.pre}
    is an [[asynchronous iterable]{.xref .std
    .std-term}](https://docs.python.org/3/glossary.html#term-asynchronous-iterable &quot;(in Python v3.12)&quot;){.reference
    .external}.

[[process_spider_exception]{.pre}]{.sig-name .descname}[(]{.sig-paren}*[[response]{.pre}]{.n}*, *[[exception]{.pre}]{.n}*, *[[spider]{.pre}]{.n}*[)]{.sig-paren}[¶](#scrapy.spidermiddlewares.SpiderMiddleware.process_spider_exception &quot;Permalink to this definition&quot;){.headerlink}

:   This method is called when a spider or
    [[`process_spider_output()`{.xref .py .py-meth .docutils
    .literal
    .notranslate}]{.pre}](#scrapy.spidermiddlewares.SpiderMiddleware.process_spider_output &quot;scrapy.spidermiddlewares.SpiderMiddleware.process_spider_output&quot;){.reference
    .internal} method (from a previous spider middleware) raises an
    exception.

    [[`process_spider_exception()`{.xref .py .py-meth .docutils
    .literal
    .notranslate}]{.pre}](#scrapy.spidermiddlewares.SpiderMiddleware.process_spider_exception &quot;scrapy.spidermiddlewares.SpiderMiddleware.process_spider_exception&quot;){.reference
    .internal} should return either [`None`{.docutils .literal
    .notranslate}]{.pre} or an iterable of [`Request`{.xref .py
    .py-class .docutils .literal .notranslate}]{.pre} or
    [[item]{.std .std-ref}](index.html#topics-items){.hoverxref
    .tooltip .reference .internal} objects.

    If it returns [`None`{.docutils .literal .notranslate}]{.pre},
    Scrapy will continue processing this exception, executing any
    other [[`process_spider_exception()`{.xref .py .py-meth
    .docutils .literal
    .notranslate}]{.pre}](#scrapy.spidermiddlewares.SpiderMiddleware.process_spider_exception &quot;scrapy.spidermiddlewares.SpiderMiddleware.process_spider_exception&quot;){.reference
    .internal} in the following middleware components, until no
    middleware components are left and the exception reaches the
    engine (where it's logged and discarded).

    If it returns an iterable the [[`process_spider_output()`{.xref
    .py .py-meth .docutils .literal
    .notranslate}]{.pre}](#scrapy.spidermiddlewares.SpiderMiddleware.process_spider_output &quot;scrapy.spidermiddlewares.SpiderMiddleware.process_spider_output&quot;){.reference
    .internal} pipeline kicks in, starting from the next spider
    middleware, and no other [[`process_spider_exception()`{.xref
    .py .py-meth .docutils .literal
    .notranslate}]{.pre}](#scrapy.spidermiddlewares.SpiderMiddleware.process_spider_exception &quot;scrapy.spidermiddlewares.SpiderMiddleware.process_spider_exception&quot;){.reference
    .internal} will be called.

    Parameters

    :   -   **response** ([[`Response`{.xref .py .py-class .docutils
            .literal
            .notranslate}]{.pre}](index.html#scrapy.http.Response &quot;scrapy.http.Response&quot;){.reference
            .internal} object) -- the response being processed when
            the exception was raised

        -   **exception** ([[`Exception`{.xref .py .py-exc .docutils
            .literal
            .notranslate}]{.pre}](https://docs.python.org/3/library/exceptions.html#Exception &quot;(in Python v3.12)&quot;){.reference
            .external} object) -- the exception raised

        -   **spider** ([[`Spider`{.xref .py .py-class .docutils
            .literal
            .notranslate}]{.pre}](index.html#scrapy.Spider &quot;scrapy.Spider&quot;){.reference
            .internal} object) -- the spider which raised the
            exception

[[process_start_requests]{.pre}]{.sig-name .descname}[(]{.sig-paren}*[[start_requests]{.pre}]{.n}*, *[[spider]{.pre}]{.n}*[)]{.sig-paren}[¶](#scrapy.spidermiddlewares.SpiderMiddleware.process_start_requests &quot;Permalink to this definition&quot;){.headerlink}

:   This method is called with the start requests of the spider, and
    works similarly to the [[`process_spider_output()`{.xref .py
    .py-meth .docutils .literal
    .notranslate}]{.pre}](#scrapy.spidermiddlewares.SpiderMiddleware.process_spider_output &quot;scrapy.spidermiddlewares.SpiderMiddleware.process_spider_output&quot;){.reference
    .internal} method, except that it doesn't have a response
    associated and must return only requests (not items).

    It receives an iterable (in the [`start_requests`{.docutils
    .literal .notranslate}]{.pre} parameter) and must return another
    iterable of [`Request`{.xref .py .py-class .docutils .literal
    .notranslate}]{.pre} objects.

    ::: {.admonition .note}
    Note

    When implementing this method in your spider middleware, you
    should always return an iterable (that follows the input one)
    and not consume all [`start_requests`{.docutils .literal
    .notranslate}]{.pre} iterator because it can be very large (or
    even unbounded) and cause a memory overflow. The Scrapy engine
    is designed to pull start requests while it has capacity to
    process them, so the start requests iterator can be effectively
    endless where there is some other condition for stopping the
    spider (like a time limit or item/page count).
    :::

    Parameters

    :   -   **start_requests** (an iterable of [`Request`{.xref .py
            .py-class .docutils .literal .notranslate}]{.pre}) --
            the start requests

        -   **spider** ([[`Spider`{.xref .py .py-class .docutils
            .literal
            .notranslate}]{.pre}](index.html#scrapy.Spider &quot;scrapy.Spider&quot;){.reference
            .internal} object) -- the spider to whom the start
            requests belong

[[from_crawler]{.pre}]{.sig-name .descname}[(]{.sig-paren}*[[cls]{.pre}]{.n}*, *[[crawler]{.pre}]{.n}*[)]{.sig-paren}[¶](#scrapy.spidermiddlewares.SpiderMiddleware.from_crawler &quot;Permalink to this definition&quot;){.headerlink}

:   If present, this classmethod is called to create a middleware
    instance from a [[`Crawler`{.xref .py .py-class .docutils
    .literal
    .notranslate}]{.pre}](index.html#scrapy.crawler.Crawler &quot;scrapy.crawler.Crawler&quot;){.reference
    .internal}. It must return a new instance of the middleware.
    Crawler object provides access to all Scrapy core components
    like settings and signals; it is a way for middleware to access
    them and hook its functionality into Scrapy.

    Parameters

    :   **crawler** ([[`Crawler`{.xref .py .py-class .docutils
        .literal
        .notranslate}]{.pre}](index.html#scrapy.crawler.Crawler &quot;scrapy.crawler.Crawler&quot;){.reference
        .internal} object) -- crawler that uses this middleware
</code></pre>
<p>:::</p>
<p>::: {#built-in-spider-middleware-reference .section}
[]{#topics-spider-middleware-ref}</p>
<h4 id="built-in-spider-middleware-referenceheaderlink"><a class="header" href="#built-in-spider-middleware-referenceheaderlink">Built-in spider middleware reference<a href="#built-in-spider-middleware-reference" title="Permalink to this heading">¶</a>{.headerlink}</a></h4>
<p>This page describes all spider middleware components that come with
Scrapy. For information on how to use them and how to write your own
spider middleware, see the <a href="#topics-spider-middleware">[spider middleware usage guide]{.std
.std-ref}</a>{.hoverxref .tooltip .reference
.internal}.</p>
<p>For a list of the components enabled by default (and their orders) see
the <a href="index.html#std-setting-SPIDER_MIDDLEWARES_BASE">[<code>SPIDER_MIDDLEWARES_BASE</code>{.xref .std .std-setting .docutils
.literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal} setting.</p>
<p>::: {#module-scrapy.spidermiddlewares.depth .section}
[]{#depthmiddleware}</p>
<h5 id="depthmiddlewareheaderlink"><a class="header" href="#depthmiddlewareheaderlink">DepthMiddleware<a href="#module-scrapy.spidermiddlewares.depth" title="Permalink to this heading">¶</a>{.headerlink}</a></h5>
<p><em>[class]{.pre}[ ]{.w}</em>[[scrapy.spidermiddlewares.depth.]{.pre}]{.sig-prename .descclassname}[[DepthMiddleware]{.pre}]{.sig-name .descname}<a href="_modules/scrapy/spidermiddlewares/depth.html#DepthMiddleware">[[[source]]{.pre}]{.viewcode-link}</a>{.reference .internal}<a href="#scrapy.spidermiddlewares.depth.DepthMiddleware" title="Permalink to this definition">¶</a>{.headerlink}</p>
<p>:   DepthMiddleware is used for tracking the depth of each Request
inside the site being scraped. It works by setting
[<code>request.meta['depth']</code>{.docutils .literal
.notranslate}]{.pre}<code> </code>{.docutils .literal
.notranslate}[<code>=</code>{.docutils .literal
.notranslate}]{.pre}<code> </code>{.docutils .literal
.notranslate}[<code>0</code>{.docutils .literal .notranslate}]{.pre} whenever
there is no value previously set (usually just the first Request)
and incrementing it by 1 otherwise.</p>
<pre><code>It can be used to limit the maximum depth to scrape, control Request
priority based on their depth, and things like that.

The [[`DepthMiddleware`{.xref .py .py-class .docutils .literal
.notranslate}]{.pre}](#scrapy.spidermiddlewares.depth.DepthMiddleware &quot;scrapy.spidermiddlewares.depth.DepthMiddleware&quot;){.reference
.internal} can be configured through the following settings (see the
settings documentation for more info):

&gt; &lt;div&gt;
&gt;
&gt; -   [[`DEPTH_LIMIT`{.xref .std .std-setting .docutils .literal
&gt;     .notranslate}]{.pre}](index.html#std-setting-DEPTH_LIMIT){.hoverxref
&gt;     .tooltip .reference .internal} - The maximum depth that will
&gt;     be allowed to crawl for any site. If zero, no limit will be
&gt;     imposed.
&gt;
&gt; -   [[`DEPTH_STATS_VERBOSE`{.xref .std .std-setting .docutils
&gt;     .literal
&gt;     .notranslate}]{.pre}](index.html#std-setting-DEPTH_STATS_VERBOSE){.hoverxref
&gt;     .tooltip .reference .internal} - Whether to collect the number
&gt;     of requests for each depth.
&gt;
&gt; -   [[`DEPTH_PRIORITY`{.xref .std .std-setting .docutils .literal
&gt;     .notranslate}]{.pre}](index.html#std-setting-DEPTH_PRIORITY){.hoverxref
&gt;     .tooltip .reference .internal} - Whether to prioritize the
&gt;     requests based on their depth.
&gt;
&gt; &lt;/div&gt;
</code></pre>
<p>:::</p>
<p>::: {#module-scrapy.spidermiddlewares.httperror .section}
[]{#httperrormiddleware}</p>
<h5 id="httperrormiddlewareheaderlink"><a class="header" href="#httperrormiddlewareheaderlink">HttpErrorMiddleware<a href="#module-scrapy.spidermiddlewares.httperror" title="Permalink to this heading">¶</a>{.headerlink}</a></h5>
<p><em>[class]{.pre}[ ]{.w}</em>[[scrapy.spidermiddlewares.httperror.]{.pre}]{.sig-prename .descclassname}[[HttpErrorMiddleware]{.pre}]{.sig-name .descname}<a href="_modules/scrapy/spidermiddlewares/httperror.html#HttpErrorMiddleware">[[[source]]{.pre}]{.viewcode-link}</a>{.reference .internal}<a href="#scrapy.spidermiddlewares.httperror.HttpErrorMiddleware" title="Permalink to this definition">¶</a>{.headerlink}</p>
<p>:   Filter out unsuccessful (erroneous) HTTP responses so that spiders
don't have to deal with them, which (most of the time) imposes an
overhead, consumes more resources, and makes the spider logic more
complex.</p>
<p>According to the <a href="https://www.w3.org/Protocols/rfc2616/rfc2616-sec10.html">HTTP
standard</a>{.reference
.external}, successful responses are those whose status codes are in the
200-300 range.</p>
<p>If you still want to process response codes outside that range, you can
specify which response codes the spider is able to handle using the
[<code>handle_httpstatus_list</code>{.docutils .literal .notranslate}]{.pre} spider
attribute or <a href="#std-setting-HTTPERROR_ALLOWED_CODES">[<code>HTTPERROR_ALLOWED_CODES</code>{.xref .std .std-setting
.docutils .literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal} setting.</p>
<p>For example, if you want your spider to handle 404 responses you can do
this:</p>
<p>::: {.highlight-python .notranslate}
::: highlight
from scrapy.spiders import CrawlSpider</p>
<pre><code>class MySpider(CrawlSpider):
    handle_httpstatus_list = [404]
</code></pre>
<p>:::
:::</p>
<p>[]{#std-reqmeta-handle_httpstatus_list .target}</p>
<p>The [<code>handle_httpstatus_list</code>{.docutils .literal .notranslate}]{.pre}
key of [<code>Request.meta</code>{.xref .py .py-attr .docutils .literal
.notranslate}]{.pre} can also be used to specify which response codes to
allow on a per-request basis. You can also set the meta key
[<code>handle_httpstatus_all</code>{.docutils .literal .notranslate}]{.pre} to
[<code>True</code>{.docutils .literal .notranslate}]{.pre} if you want to allow any
response code for a request, and [<code>False</code>{.docutils .literal
.notranslate}]{.pre} to disable the effects of the
[<code>handle_httpstatus_all</code>{.docutils .literal .notranslate}]{.pre} key.</p>
<p>Keep in mind, however, that it's usually a bad idea to handle non-200
responses, unless you really know what you're doing.</p>
<p>For more information see: <a href="https://www.w3.org/Protocols/rfc2616/rfc2616-sec10.html">HTTP Status Code
Definitions</a>{.reference
.external}.</p>
<p>::: {#httperrormiddleware-settings .section}</p>
<h6 id="httperrormiddleware-settingsheaderlink"><a class="header" href="#httperrormiddleware-settingsheaderlink">HttpErrorMiddleware settings<a href="#httperrormiddleware-settings" title="Permalink to this heading">¶</a>{.headerlink}</a></h6>
<p>::: {#httperror-allowed-codes .section}
[]{#std-setting-HTTPERROR_ALLOWED_CODES}HTTPERROR_ALLOWED_CODES<a href="#httperror-allowed-codes" title="Permalink to this heading">¶</a>{.headerlink}</p>
<p>Default: [<code>[]</code>{.docutils .literal .notranslate}]{.pre}</p>
<p>Pass all responses with non-200 status codes contained in this list.
:::</p>
<p>::: {#httperror-allow-all .section}
[]{#std-setting-HTTPERROR_ALLOW_ALL}HTTPERROR_ALLOW_ALL<a href="#httperror-allow-all" title="Permalink to this heading">¶</a>{.headerlink}</p>
<p>Default: [<code>False</code>{.docutils .literal .notranslate}]{.pre}</p>
<p>Pass all responses, regardless of its status code.
:::
:::
:::</p>
<p>::: {#module-scrapy.spidermiddlewares.offsite .section}
[]{#offsitemiddleware}</p>
<h5 id="offsitemiddlewareheaderlink"><a class="header" href="#offsitemiddlewareheaderlink">OffsiteMiddleware<a href="#module-scrapy.spidermiddlewares.offsite" title="Permalink to this heading">¶</a>{.headerlink}</a></h5>
<p><em>[class]{.pre}[ ]{.w}</em>[[scrapy.spidermiddlewares.offsite.]{.pre}]{.sig-prename .descclassname}[[OffsiteMiddleware]{.pre}]{.sig-name .descname}<a href="_modules/scrapy/spidermiddlewares/offsite.html#OffsiteMiddleware">[[[source]]{.pre}]{.viewcode-link}</a>{.reference .internal}<a href="#scrapy.spidermiddlewares.offsite.OffsiteMiddleware" title="Permalink to this definition">¶</a>{.headerlink}</p>
<p>:   Filters out Requests for URLs outside the domains covered by the
spider.</p>
<pre><code>This middleware filters out every request whose host names aren't in
the spider's [[`allowed_domains`{.xref .py .py-attr .docutils
.literal
.notranslate}]{.pre}](index.html#scrapy.Spider.allowed_domains &quot;scrapy.Spider.allowed_domains&quot;){.reference
.internal} attribute. All subdomains of any domain in the list are
also allowed. E.g. the rule [`www.example.org`{.docutils .literal
.notranslate}]{.pre} will also allow
[`bob.www.example.org`{.docutils .literal .notranslate}]{.pre} but
not [`www2.example.com`{.docutils .literal .notranslate}]{.pre} nor
[`example.com`{.docutils .literal .notranslate}]{.pre}.

When your spider returns a request for a domain not belonging to
those covered by the spider, this middleware will log a debug
message similar to this one:

::: {.highlight-default .notranslate}
::: highlight
    DEBUG: Filtered offsite request to 'www.othersite.com': &lt;GET http://www.othersite.com/some/page.html&gt;
:::
:::

To avoid filling the log with too much noise, it will only print one
of these messages for each new domain filtered. So, for example, if
another request for [`www.othersite.com`{.docutils .literal
.notranslate}]{.pre} is filtered, no log message will be printed.
But if a request for [`someothersite.com`{.docutils .literal
.notranslate}]{.pre} is filtered, a message will be printed (but
only for the first request filtered).

If the spider doesn't define an [[`allowed_domains`{.xref .py
.py-attr .docutils .literal
.notranslate}]{.pre}](index.html#scrapy.Spider.allowed_domains &quot;scrapy.Spider.allowed_domains&quot;){.reference
.internal} attribute, or the attribute is empty, the offsite
middleware will allow all requests.

If the request has the [`dont_filter`{.xref .py .py-attr .docutils
.literal .notranslate}]{.pre} attribute set, the offsite middleware
will allow the request even if its domain is not listed in allowed
domains.
</code></pre>
<p>:::</p>
<p>::: {#module-scrapy.spidermiddlewares.referer .section}
[]{#referermiddleware}</p>
<h5 id="referermiddlewareheaderlink"><a class="header" href="#referermiddlewareheaderlink">RefererMiddleware<a href="#module-scrapy.spidermiddlewares.referer" title="Permalink to this heading">¶</a>{.headerlink}</a></h5>
<p><em>[class]{.pre}[ ]{.w}</em>[[scrapy.spidermiddlewares.referer.]{.pre}]{.sig-prename .descclassname}[[RefererMiddleware]{.pre}]{.sig-name .descname}<a href="_modules/scrapy/spidermiddlewares/referer.html#RefererMiddleware">[[[source]]{.pre}]{.viewcode-link}</a>{.reference .internal}<a href="#scrapy.spidermiddlewares.referer.RefererMiddleware" title="Permalink to this definition">¶</a>{.headerlink}</p>
<p>:   Populates Request [<code>Referer</code>{.docutils .literal .notranslate}]{.pre}
header, based on the URL of the Response which generated it.</p>
<p>::: {#referermiddleware-settings .section}</p>
<h6 id="referermiddleware-settingsheaderlink"><a class="header" href="#referermiddleware-settingsheaderlink">RefererMiddleware settings<a href="#referermiddleware-settings" title="Permalink to this heading">¶</a>{.headerlink}</a></h6>
<p>::: {#referer-enabled .section}
[]{#std-setting-REFERER_ENABLED}REFERER_ENABLED<a href="#referer-enabled" title="Permalink to this heading">¶</a>{.headerlink}</p>
<p>Default: [<code>True</code>{.docutils .literal .notranslate}]{.pre}</p>
<p>Whether to enable referer middleware.
:::</p>
<p>::: {#referrer-policy .section}
[]{#std-setting-REFERRER_POLICY}REFERRER_POLICY<a href="#referrer-policy" title="Permalink to this heading">¶</a>{.headerlink}</p>
<p>Default:
[<code>'scrapy.spidermiddlewares.referer.DefaultReferrerPolicy'</code>{.docutils
.literal .notranslate}]{.pre}</p>
<p><a href="https://www.w3.org/TR/referrer-policy">Referrer Policy</a>{.reference
.external} to apply when populating Request &quot;Referer&quot; header.</p>
<p>::: {.admonition .note}
Note</p>
<p>You can also set the Referrer Policy per request, using the special
[<code>&quot;referrer_policy&quot;</code>{.docutils .literal .notranslate}]{.pre}
<a href="index.html#topics-request-meta">[Request.meta]{.std
.std-ref}</a>{.hoverxref .tooltip
.reference .internal} key, with the same acceptable values as for the
[<code>REFERRER_POLICY</code>{.docutils .literal .notranslate}]{.pre} setting.
:::</p>
<p>::: {#acceptable-values-for-referrer-policy .section}
Acceptable values for
REFERRER_POLICY<a href="#acceptable-values-for-referrer-policy" title="Permalink to this heading">¶</a>{.headerlink}</p>
<ul>
<li>
<p>either a path to a
[<code>scrapy.spidermiddlewares.referer.ReferrerPolicy</code>{.docutils
.literal .notranslate}]{.pre} subclass --- a custom policy or one of
the built-in ones (see classes below),</p>
</li>
<li>
<p>or one of the standard W3C-defined string values,</p>
</li>
<li>
<p>or the special [<code>&quot;scrapy-default&quot;</code>{.docutils .literal
.notranslate}]{.pre}.</p>
</li>
</ul>
<p>+-----------------------+----------------------------------------------+
| String value          | Class name (as a string)                     |
+=======================+==============================================+
| [<code>&quot;scrap              | [[</code>scrapy.spidermidd                         |
| y-default&quot;<code>{.docutils | lewares.referer.DefaultReferrerPolicy</code>{.xref |
| .literal              | .py .py-class .docutils .literal             |
| .notranslate}]{.pre}  | .notranslate}]                               |
| (default)             | {.pre}](#scrapy.spidermiddlewares.referer.De |
|                       | faultReferrerPolicy &quot;scrapy.spidermiddleware |
|                       | s.referer.DefaultReferrerPolicy&quot;){.reference |
|                       | .internal}                                   |
+-----------------------+----------------------------------------------+
| [&quot;no-refer            | [[<code>scrapy.spide                              | | rer&quot;](https://www.w3. | rmiddlewares.referer.NoReferrerPolicy</code>{.xref |
| org/TR/referrer-polic | .py .py-class .docutils .literal             |
| y/#referrer-policy-no | .not                                         |
| -referrer){.reference | ranslate}]{.pre}](#scrapy.spidermiddlewares. |
| .external}            | referer.NoReferrerPolicy &quot;scrapy.spidermiddl |
|                       | ewares.referer.NoReferrerPolicy&quot;){.reference |
|                       | .internal}                                   |
+-----------------------+----------------------------------------------+
| [&quot;no-referrer-when-   | [[<code>scrapy.spidermiddlewares.                 | | downgrade&quot;](https://w | referer.NoReferrerWhenDowngradePolicy</code>{.xref |
| ww.w3.org/TR/referrer | .py .py-class .docutils .literal             |
| -policy/#referrer-pol | .notranslate}]{.pre}](#scrapy.               |
| icy-no-referrer-when- | spidermiddlewares.referer.NoReferrerWhenDown |
| downgrade){.reference | gradePolicy &quot;scrapy.spidermiddlewares.refere |
| .external}            | r.NoReferrerWhenDowngradePolicy&quot;){.reference |
|                       | .internal}                                   |
+-----------------------+----------------------------------------------+
| [&quot;same-ori            | [[<code>scrapy.spide                              | | gin&quot;](https://www.w3. | rmiddlewares.referer.SameOriginPolicy</code>{.xref |
| org/TR/referrer-polic | .py .py-class .docutils .literal             |
| y/#referrer-policy-sa | .not                                         |
| me-origin){.reference | ranslate}]{.pre}](#scrapy.spidermiddlewares. |
| .external}            | referer.SameOriginPolicy &quot;scrapy.spidermiddl |
|                       | ewares.referer.SameOriginPolicy&quot;){.reference |
|                       | .internal}                                   |
+-----------------------+----------------------------------------------+
| [&quot;origin&quot;](https://ww | [[<code>scrapy.s                                  | | w.w3.org/TR/referrer- | pidermiddlewares.referer.OriginPolicy</code>{.xref |
| policy/#referrer-poli | .py .py-class .docutils .literal             |
| cy-origin){.reference | .notranslate}]{.pre}](#scrapy.spidermidd     |
| .external}            | lewares.referer.OriginPolicy &quot;scrapy.spiderm |
|                       | iddlewares.referer.OriginPolicy&quot;){.reference |
|                       | .internal}                                   |
+-----------------------+----------------------------------------------+
| [&quot;strict-origi        | [[<code>scrapy.spiderm                            | | n&quot;](https://www.w3.or | iddlewares.referer.StrictOriginPolicy</code>{.xref |
| g/TR/referrer-policy/ | .py .py-class .docutils .literal             |
| #referrer-policy-stri | .notrans                                     |
| ct-origin){.reference | late}]{.pre}](#scrapy.spidermiddlewares.refe |
| .external}            | rer.StrictOriginPolicy &quot;scrapy.spidermiddlew |
|                       | ares.referer.StrictOriginPolicy&quot;){.reference |
|                       | .internal}                                   |
+-----------------------+----------------------------------------------+
| [&quot;origin-when-c       | [[<code>scrapy.spidermiddleware                   | | ross-origin&quot;](https:/ | s.referer.OriginWhenCrossOriginPolicy</code>{.xref |
| /www.w3.org/TR/referr | .py .py-class .docutils .literal             |
| er-policy/#referrer-p | .notranslate}]{.pre}](#scr                   |
| olicy-origin-when-cro | apy.spidermiddlewares.referer.OriginWhenCros |
| ss-origin){.reference | sOriginPolicy &quot;scrapy.spidermiddlewares.refe |
| .external}            | rer.OriginWhenCrossOriginPolicy&quot;){.reference |
|                       | .internal}                                   |
+-----------------------+----------------------------------------------+
| [&quot;strict              | [[<code>scrapy.spidermiddlewares.refe             | | -origin-when-cross-or | rer.StrictOriginWhenCrossOriginPolicy</code>{.xref |
| igin&quot;](https://www.w3 | .py .py-class .docutils .literal             |
| .org/TR/referrer-poli | .notranslate}]{.pre}](#scrapy.spidermi       |
| cy/#referrer-policy-s | ddlewares.referer.StrictOriginWhenCrossOrigi |
| trict-origin-when-cro | nPolicy &quot;scrapy.spidermiddlewares.referer.St |
| ss-origin){.reference | rictOriginWhenCrossOriginPolicy&quot;){.reference |
| .external}            | .internal}                                   |
+-----------------------+----------------------------------------------+
| [&quot;unsafe              | [[<code>scrapy.spid                               | | -url&quot;](https://www.w3 | ermiddlewares.referer.UnsafeUrlPolicy</code>{.xref |
| .org/TR/referrer-poli | .py .py-class .docutils .literal             |
| cy/#referrer-policy-u | .n                                           |
| nsafe-url){.reference | otranslate}]{.pre}](#scrapy.spidermiddleware |
| .external}            | s.referer.UnsafeUrlPolicy &quot;scrapy.spidermidd |
|                       | lewares.referer.UnsafeUrlPolicy&quot;){.reference |
|                       | .internal}                                   |
+-----------------------+----------------------------------------------+</p>
<p><em>[class]{.pre}[ ]{.w}</em>[[scrapy.spidermiddlewares.referer.]{.pre}]{.sig-prename .descclassname}[[DefaultReferrerPolicy]{.pre}]{.sig-name .descname}<a href="_modules/scrapy/spidermiddlewares/referer.html#DefaultReferrerPolicy">[[[source]]{.pre}]{.viewcode-link}</a>{.reference .internal}<a href="#scrapy.spidermiddlewares.referer.DefaultReferrerPolicy" title="Permalink to this definition">¶</a>{.headerlink}</p>
<p>:   A variant of &quot;no-referrer-when-downgrade&quot;, with the addition that
&quot;Referer&quot; is not sent if the parent request was using
[<code>file://</code>{.docutils .literal .notranslate}]{.pre} or
[<code>s3://</code>{.docutils .literal .notranslate}]{.pre} scheme.</p>
<p>::: {.admonition .warning}
Warning</p>
<p>Scrapy's default referrer policy --- just like
<a href="https://www.w3.org/TR/referrer-policy/#referrer-policy-no-referrer-when-downgrade">&quot;no-referrer-when-downgrade&quot;</a>{.reference
.external}, the W3C-recommended value for browsers --- will send a
non-empty &quot;Referer&quot; header from any [<code>http(s)://</code>{.docutils .literal
.notranslate}]{.pre} to any [<code>https://</code>{.docutils .literal
.notranslate}]{.pre} URL, even if the domain is different.</p>
<p><a href="https://www.w3.org/TR/referrer-policy/#referrer-policy-same-origin">&quot;same-origin&quot;</a>{.reference
.external} may be a better choice if you want to remove referrer
information for cross-domain requests.
:::</p>
<p><em>[class]{.pre}[ ]{.w}</em>[[scrapy.spidermiddlewares.referer.]{.pre}]{.sig-prename .descclassname}[[NoReferrerPolicy]{.pre}]{.sig-name .descname}<a href="_modules/scrapy/spidermiddlewares/referer.html#NoReferrerPolicy">[[[source]]{.pre}]{.viewcode-link}</a>{.reference .internal}<a href="#scrapy.spidermiddlewares.referer.NoReferrerPolicy" title="Permalink to this definition">¶</a>{.headerlink}</p>
<p>:   <a href="https://www.w3.org/TR/referrer-policy/#referrer-policy-no-referrer">https://www.w3.org/TR/referrer-policy/#referrer-policy-no-referrer</a>{.reference
.external}</p>
<pre><code>The simplest policy is &quot;no-referrer&quot;, which specifies that no
referrer information is to be sent along with requests made from a
particular request client to any origin. The header will be omitted
entirely.
</code></pre>
<pre><code class="language-{=html}">&lt;!-- --&gt;
</code></pre>
<p><em>[class]{.pre}[ ]{.w}</em>[[scrapy.spidermiddlewares.referer.]{.pre}]{.sig-prename .descclassname}[[NoReferrerWhenDowngradePolicy]{.pre}]{.sig-name .descname}<a href="_modules/scrapy/spidermiddlewares/referer.html#NoReferrerWhenDowngradePolicy">[[[source]]{.pre}]{.viewcode-link}</a>{.reference .internal}<a href="#scrapy.spidermiddlewares.referer.NoReferrerWhenDowngradePolicy" title="Permalink to this definition">¶</a>{.headerlink}</p>
<p>:   <a href="https://www.w3.org/TR/referrer-policy/#referrer-policy-no-referrer-when-downgrade">https://www.w3.org/TR/referrer-policy/#referrer-policy-no-referrer-when-downgrade</a>{.reference
.external}</p>
<pre><code>The &quot;no-referrer-when-downgrade&quot; policy sends a full URL along with
requests from a TLS-protected environment settings object to a
potentially trustworthy URL, and requests from clients which are not
TLS-protected to any origin.

Requests from TLS-protected clients to non-potentially trustworthy
URLs, on the other hand, will contain no referrer information. A
Referer HTTP header will not be sent.

This is a user agent's default behavior, if no policy is otherwise
specified.
</code></pre>
<p>::: {.admonition .note}
Note</p>
<p>&quot;no-referrer-when-downgrade&quot; policy is the W3C-recommended default, and
is used by major web browsers.</p>
<p>However, it is NOT Scrapy's default referrer policy (see
<a href="#scrapy.spidermiddlewares.referer.DefaultReferrerPolicy" title="scrapy.spidermiddlewares.referer.DefaultReferrerPolicy">[<code>DefaultReferrerPolicy</code>{.xref .py .py-class .docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal}).
:::</p>
<p><em>[class]{.pre}[ ]{.w}</em>[[scrapy.spidermiddlewares.referer.]{.pre}]{.sig-prename .descclassname}[[SameOriginPolicy]{.pre}]{.sig-name .descname}<a href="_modules/scrapy/spidermiddlewares/referer.html#SameOriginPolicy">[[[source]]{.pre}]{.viewcode-link}</a>{.reference .internal}<a href="#scrapy.spidermiddlewares.referer.SameOriginPolicy" title="Permalink to this definition">¶</a>{.headerlink}</p>
<p>:   <a href="https://www.w3.org/TR/referrer-policy/#referrer-policy-same-origin">https://www.w3.org/TR/referrer-policy/#referrer-policy-same-origin</a>{.reference
.external}</p>
<pre><code>The &quot;same-origin&quot; policy specifies that a full URL, stripped for use
as a referrer, is sent as referrer information when making
same-origin requests from a particular request client.

Cross-origin requests, on the other hand, will contain no referrer
information. A Referer HTTP header will not be sent.
</code></pre>
<pre><code class="language-{=html}">&lt;!-- --&gt;
</code></pre>
<p><em>[class]{.pre}[ ]{.w}</em>[[scrapy.spidermiddlewares.referer.]{.pre}]{.sig-prename .descclassname}[[OriginPolicy]{.pre}]{.sig-name .descname}<a href="_modules/scrapy/spidermiddlewares/referer.html#OriginPolicy">[[[source]]{.pre}]{.viewcode-link}</a>{.reference .internal}<a href="#scrapy.spidermiddlewares.referer.OriginPolicy" title="Permalink to this definition">¶</a>{.headerlink}</p>
<p>:   <a href="https://www.w3.org/TR/referrer-policy/#referrer-policy-origin">https://www.w3.org/TR/referrer-policy/#referrer-policy-origin</a>{.reference
.external}</p>
<pre><code>The &quot;origin&quot; policy specifies that only the ASCII serialization of
the origin of the request client is sent as referrer information
when making both same-origin requests and cross-origin requests from
a particular request client.
</code></pre>
<pre><code class="language-{=html}">&lt;!-- --&gt;
</code></pre>
<p><em>[class]{.pre}[ ]{.w}</em>[[scrapy.spidermiddlewares.referer.]{.pre}]{.sig-prename .descclassname}[[StrictOriginPolicy]{.pre}]{.sig-name .descname}<a href="_modules/scrapy/spidermiddlewares/referer.html#StrictOriginPolicy">[[[source]]{.pre}]{.viewcode-link}</a>{.reference .internal}<a href="#scrapy.spidermiddlewares.referer.StrictOriginPolicy" title="Permalink to this definition">¶</a>{.headerlink}</p>
<p>:   <a href="https://www.w3.org/TR/referrer-policy/#referrer-policy-strict-origin">https://www.w3.org/TR/referrer-policy/#referrer-policy-strict-origin</a>{.reference
.external}</p>
<pre><code>The &quot;strict-origin&quot; policy sends the ASCII serialization of the
origin of the request client when making requests: - from a
TLS-protected environment settings object to a potentially
trustworthy URL, and - from non-TLS-protected environment settings
objects to any origin.

Requests from TLS-protected request clients to non- potentially
trustworthy URLs, on the other hand, will contain no referrer
information. A Referer HTTP header will not be sent.
</code></pre>
<pre><code class="language-{=html}">&lt;!-- --&gt;
</code></pre>
<p><em>[class]{.pre}[ ]{.w}</em>[[scrapy.spidermiddlewares.referer.]{.pre}]{.sig-prename .descclassname}[[OriginWhenCrossOriginPolicy]{.pre}]{.sig-name .descname}<a href="_modules/scrapy/spidermiddlewares/referer.html#OriginWhenCrossOriginPolicy">[[[source]]{.pre}]{.viewcode-link}</a>{.reference .internal}<a href="#scrapy.spidermiddlewares.referer.OriginWhenCrossOriginPolicy" title="Permalink to this definition">¶</a>{.headerlink}</p>
<p>:   <a href="https://www.w3.org/TR/referrer-policy/#referrer-policy-origin-when-cross-origin">https://www.w3.org/TR/referrer-policy/#referrer-policy-origin-when-cross-origin</a>{.reference
.external}</p>
<pre><code>The &quot;origin-when-cross-origin&quot; policy specifies that a full URL,
stripped for use as a referrer, is sent as referrer information when
making same-origin requests from a particular request client, and
only the ASCII serialization of the origin of the request client is
sent as referrer information when making cross-origin requests from
a particular request client.
</code></pre>
<pre><code class="language-{=html}">&lt;!-- --&gt;
</code></pre>
<p><em>[class]{.pre}[ ]{.w}</em>[[scrapy.spidermiddlewares.referer.]{.pre}]{.sig-prename .descclassname}[[StrictOriginWhenCrossOriginPolicy]{.pre}]{.sig-name .descname}<a href="_modules/scrapy/spidermiddlewares/referer.html#StrictOriginWhenCrossOriginPolicy">[[[source]]{.pre}]{.viewcode-link}</a>{.reference .internal}<a href="#scrapy.spidermiddlewares.referer.StrictOriginWhenCrossOriginPolicy" title="Permalink to this definition">¶</a>{.headerlink}</p>
<p>:   <a href="https://www.w3.org/TR/referrer-policy/#referrer-policy-strict-origin-when-cross-origin">https://www.w3.org/TR/referrer-policy/#referrer-policy-strict-origin-when-cross-origin</a>{.reference
.external}</p>
<pre><code>The &quot;strict-origin-when-cross-origin&quot; policy specifies that a full
URL, stripped for use as a referrer, is sent as referrer information
when making same-origin requests from a particular request client,
and only the ASCII serialization of the origin of the request client
when making cross-origin requests:

-   from a TLS-protected environment settings object to a
    potentially trustworthy URL, and

-   from non-TLS-protected environment settings objects to any
    origin.

Requests from TLS-protected clients to non- potentially trustworthy
URLs, on the other hand, will contain no referrer information. A
Referer HTTP header will not be sent.
</code></pre>
<pre><code class="language-{=html}">&lt;!-- --&gt;
</code></pre>
<p><em>[class]{.pre}[ ]{.w}</em>[[scrapy.spidermiddlewares.referer.]{.pre}]{.sig-prename .descclassname}[[UnsafeUrlPolicy]{.pre}]{.sig-name .descname}<a href="_modules/scrapy/spidermiddlewares/referer.html#UnsafeUrlPolicy">[[[source]]{.pre}]{.viewcode-link}</a>{.reference .internal}<a href="#scrapy.spidermiddlewares.referer.UnsafeUrlPolicy" title="Permalink to this definition">¶</a>{.headerlink}</p>
<p>:   <a href="https://www.w3.org/TR/referrer-policy/#referrer-policy-unsafe-url">https://www.w3.org/TR/referrer-policy/#referrer-policy-unsafe-url</a>{.reference
.external}</p>
<pre><code>The &quot;unsafe-url&quot; policy specifies that a full URL, stripped for use
as a referrer, is sent along with both cross-origin requests and
same-origin requests made from a particular request client.

Note: The policy's name doesn't lie; it is unsafe. This policy will
leak origins and paths from TLS-protected resources to insecure
origins. Carefully consider the impact of setting such a policy for
potentially sensitive documents.
</code></pre>
<p>::: {.admonition .warning}
Warning</p>
<p>&quot;unsafe-url&quot; policy is NOT recommended.
:::
:::
:::
:::
:::</p>
<p>::: {#module-scrapy.spidermiddlewares.urllength .section}
[]{#urllengthmiddleware}</p>
<h5 id="urllengthmiddlewareheaderlink"><a class="header" href="#urllengthmiddlewareheaderlink">UrlLengthMiddleware<a href="#module-scrapy.spidermiddlewares.urllength" title="Permalink to this heading">¶</a>{.headerlink}</a></h5>
<p><em>[class]{.pre}[ ]{.w}</em>[[scrapy.spidermiddlewares.urllength.]{.pre}]{.sig-prename .descclassname}[[UrlLengthMiddleware]{.pre}]{.sig-name .descname}<a href="_modules/scrapy/spidermiddlewares/urllength.html#UrlLengthMiddleware">[[[source]]{.pre}]{.viewcode-link}</a>{.reference .internal}<a href="#scrapy.spidermiddlewares.urllength.UrlLengthMiddleware" title="Permalink to this definition">¶</a>{.headerlink}</p>
<p>:   Filters out requests with URLs longer than URLLENGTH_LIMIT</p>
<pre><code>The [[`UrlLengthMiddleware`{.xref .py .py-class .docutils .literal
.notranslate}]{.pre}](#scrapy.spidermiddlewares.urllength.UrlLengthMiddleware &quot;scrapy.spidermiddlewares.urllength.UrlLengthMiddleware&quot;){.reference
.internal} can be configured through the following settings (see the
settings documentation for more info):

&gt; &lt;div&gt;
&gt;
&gt; -   [[`URLLENGTH_LIMIT`{.xref .std .std-setting .docutils .literal
&gt;     .notranslate}]{.pre}](index.html#std-setting-URLLENGTH_LIMIT){.hoverxref
&gt;     .tooltip .reference .internal} - The maximum URL length to
&gt;     allow for crawled URLs.
&gt;
&gt; &lt;/div&gt;
</code></pre>
<p>:::
:::
:::</p>
<p>[]{#document-topics/extensions}</p>
<p>::: {#extensions .section}
[]{#topics-extensions}</p>
<h3 id="extensionsheaderlink-1"><a class="header" href="#extensionsheaderlink-1">Extensions<a href="#extensions" title="Permalink to this heading">¶</a>{.headerlink}</a></h3>
<p>The extensions framework provides a mechanism for inserting your own
custom functionality into Scrapy.</p>
<p>Extensions are just regular classes.</p>
<p>::: {#extension-settings .section}</p>
<h4 id="extension-settingsheaderlink"><a class="header" href="#extension-settingsheaderlink">Extension settings<a href="#extension-settings" title="Permalink to this heading">¶</a>{.headerlink}</a></h4>
<p>Extensions use the <a href="index.html#topics-settings">[Scrapy settings]{.std
.std-ref}</a>{.hoverxref .tooltip .reference
.internal} to manage their settings, just like any other Scrapy code.</p>
<p>It is customary for extensions to prefix their settings with their own
name, to avoid collision with existing (and future) extensions. For
example, a hypothetical extension to handle <a href="https://en.wikipedia.org/wiki/Sitemaps">Google
Sitemaps</a>{.reference .external}
would use settings like [<code>GOOGLESITEMAP_ENABLED</code>{.docutils .literal
.notranslate}]{.pre}, [<code>GOOGLESITEMAP_DEPTH</code>{.docutils .literal
.notranslate}]{.pre}, and so on.
:::</p>
<p>::: {#loading-activating-extensions .section}</p>
<h4 id="loading--activating-extensionsheaderlink"><a class="header" href="#loading--activating-extensionsheaderlink">Loading &amp; activating extensions<a href="#loading-activating-extensions" title="Permalink to this heading">¶</a>{.headerlink}</a></h4>
<p>Extensions are loaded and activated at startup by instantiating a single
instance of the extension class per spider being run. All the extension
initialization code must be performed in the class [<code>__init__</code>{.docutils
.literal .notranslate}]{.pre} method.</p>
<p>To make an extension available, add it to the <a href="index.html#std-setting-EXTENSIONS">[<code>EXTENSIONS</code>{.xref .std
.std-setting .docutils .literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal} setting in your Scrapy settings. In
<a href="index.html#std-setting-EXTENSIONS">[<code>EXTENSIONS</code>{.xref .std .std-setting .docutils .literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal}, each extension is represented by a
string: the full Python path to the extension's class name. For example:</p>
<p>::: {.highlight-python .notranslate}
::: highlight
EXTENSIONS = {
&quot;scrapy.extensions.corestats.CoreStats&quot;: 500,
&quot;scrapy.extensions.telnet.TelnetConsole&quot;: 500,
}
:::
:::</p>
<p>As you can see, the <a href="index.html#std-setting-EXTENSIONS">[<code>EXTENSIONS</code>{.xref .std .std-setting .docutils
.literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal} setting is a dict where the keys are the
extension paths, and their values are the orders, which define the
extension <em>loading</em> order. The <a href="index.html#std-setting-EXTENSIONS">[<code>EXTENSIONS</code>{.xref .std .std-setting
.docutils .literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal} setting is merged with the
<a href="index.html#std-setting-EXTENSIONS_BASE">[<code>EXTENSIONS_BASE</code>{.xref .std .std-setting .docutils .literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal} setting defined in Scrapy (and not meant
to be overridden) and then sorted by order to get the final sorted list
of enabled extensions.</p>
<p>As extensions typically do not depend on each other, their loading order
is irrelevant in most cases. This is why the <a href="index.html#std-setting-EXTENSIONS_BASE">[<code>EXTENSIONS_BASE</code>{.xref
.std .std-setting .docutils .literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal} setting defines all extensions with the
same order ([<code>0</code>{.docutils .literal .notranslate}]{.pre}). However, this
feature can be exploited if you need to add an extension which depends
on other extensions already loaded.
:::</p>
<p>::: {#available-enabled-and-disabled-extensions .section}</p>
<h4 id="available-enabled-and-disabled-extensionsheaderlink"><a class="header" href="#available-enabled-and-disabled-extensionsheaderlink">Available, enabled and disabled extensions<a href="#available-enabled-and-disabled-extensions" title="Permalink to this heading">¶</a>{.headerlink}</a></h4>
<p>Not all available extensions will be enabled. Some of them usually
depend on a particular setting. For example, the HTTP Cache extension is
available by default but disabled unless the <a href="index.html#std-setting-HTTPCACHE_ENABLED">[<code>HTTPCACHE_ENABLED</code>{.xref
.std .std-setting .docutils .literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal} setting is set.
:::</p>
<p>::: {#disabling-an-extension .section}</p>
<h4 id="disabling-an-extensionheaderlink"><a class="header" href="#disabling-an-extensionheaderlink">Disabling an extension<a href="#disabling-an-extension" title="Permalink to this heading">¶</a>{.headerlink}</a></h4>
<p>In order to disable an extension that comes enabled by default (i.e.
those included in the <a href="index.html#std-setting-EXTENSIONS_BASE">[<code>EXTENSIONS_BASE</code>{.xref .std .std-setting
.docutils .literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal} setting) you must set its order to
[<code>None</code>{.docutils .literal .notranslate}]{.pre}. For example:</p>
<p>::: {.highlight-python .notranslate}
::: highlight
EXTENSIONS = {
&quot;scrapy.extensions.corestats.CoreStats&quot;: None,
}
:::
:::
:::</p>
<p>::: {#writing-your-own-extension .section}</p>
<h4 id="writing-your-own-extensionheaderlink"><a class="header" href="#writing-your-own-extensionheaderlink">Writing your own extension<a href="#writing-your-own-extension" title="Permalink to this heading">¶</a>{.headerlink}</a></h4>
<p>Each extension is a Python class. The main entry point for a Scrapy
extension (this also includes middlewares and pipelines) is the
[<code>from_crawler</code>{.docutils .literal .notranslate}]{.pre} class method
which receives a [<code>Crawler</code>{.docutils .literal .notranslate}]{.pre}
instance. Through the Crawler object you can access settings, signals,
stats, and also control the crawling behaviour.</p>
<p>Typically, extensions connect to <a href="index.html#topics-signals">[signals]{.std
.std-ref}</a>{.hoverxref .tooltip .reference
.internal} and perform tasks triggered by them.</p>
<p>Finally, if the [<code>from_crawler</code>{.docutils .literal .notranslate}]{.pre}
method raises the <a href="index.html#scrapy.exceptions.NotConfigured" title="scrapy.exceptions.NotConfigured">[<code>NotConfigured</code>{.xref .py .py-exc .docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal} exception, the extension will be disabled. Otherwise, the
extension will be enabled.</p>
<p>::: {#sample-extension .section}</p>
<h5 id="sample-extensionheaderlink"><a class="header" href="#sample-extensionheaderlink">Sample extension<a href="#sample-extension" title="Permalink to this heading">¶</a>{.headerlink}</a></h5>
<p>Here we will implement a simple extension to illustrate the concepts
described in the previous section. This extension will log a message
every time:</p>
<ul>
<li>
<p>a spider is opened</p>
</li>
<li>
<p>a spider is closed</p>
</li>
<li>
<p>a specific number of items are scraped</p>
</li>
</ul>
<p>The extension will be enabled through the [<code>MYEXT_ENABLED</code>{.docutils
.literal .notranslate}]{.pre} setting and the number of items will be
specified through the [<code>MYEXT_ITEMCOUNT</code>{.docutils .literal
.notranslate}]{.pre} setting.</p>
<p>Here is the code of such extension:</p>
<p>::: {.highlight-python .notranslate}
::: highlight
import logging
from scrapy import signals
from scrapy.exceptions import NotConfigured</p>
<pre><code>logger = logging.getLogger(__name__)


class SpiderOpenCloseLogging:
    def __init__(self, item_count):
        self.item_count = item_count
        self.items_scraped = 0

    @classmethod
    def from_crawler(cls, crawler):
        # first check if the extension should be enabled and raise
        # NotConfigured otherwise
        if not crawler.settings.getbool(&quot;MYEXT_ENABLED&quot;):
            raise NotConfigured

        # get the number of items from settings
        item_count = crawler.settings.getint(&quot;MYEXT_ITEMCOUNT&quot;, 1000)

        # instantiate the extension object
        ext = cls(item_count)

        # connect the extension object to signals
        crawler.signals.connect(ext.spider_opened, signal=signals.spider_opened)
        crawler.signals.connect(ext.spider_closed, signal=signals.spider_closed)
        crawler.signals.connect(ext.item_scraped, signal=signals.item_scraped)

        # return the extension object
        return ext

    def spider_opened(self, spider):
        logger.info(&quot;opened spider %s&quot;, spider.name)

    def spider_closed(self, spider):
        logger.info(&quot;closed spider %s&quot;, spider.name)

    def item_scraped(self, item, spider):
        self.items_scraped += 1
        if self.items_scraped % self.item_count == 0:
            logger.info(&quot;scraped %d items&quot;, self.items_scraped)
</code></pre>
<p>:::
:::
:::
:::</p>
<p>::: {#built-in-extensions-reference .section}
[]{#topics-extensions-ref}</p>
<h4 id="built-in-extensions-referenceheaderlink"><a class="header" href="#built-in-extensions-referenceheaderlink">Built-in extensions reference<a href="#built-in-extensions-reference" title="Permalink to this heading">¶</a>{.headerlink}</a></h4>
<p>::: {#general-purpose-extensions .section}</p>
<h5 id="general-purpose-extensionsheaderlink"><a class="header" href="#general-purpose-extensionsheaderlink">General purpose extensions<a href="#general-purpose-extensions" title="Permalink to this heading">¶</a>{.headerlink}</a></h5>
<p>::: {#module-scrapy.extensions.logstats .section}
[]{#log-stats-extension}</p>
<h6 id="log-stats-extensionheaderlink"><a class="header" href="#log-stats-extensionheaderlink">Log Stats extension<a href="#module-scrapy.extensions.logstats" title="Permalink to this heading">¶</a>{.headerlink}</a></h6>
<p><em>[class]{.pre}[ ]{.w}</em>[[scrapy.extensions.logstats.]{.pre}]{.sig-prename .descclassname}[[LogStats]{.pre}]{.sig-name .descname}<a href="_modules/scrapy/extensions/logstats.html#LogStats">[[[source]]{.pre}]{.viewcode-link}</a>{.reference .internal}<a href="#scrapy.extensions.logstats.LogStats" title="Permalink to this definition">¶</a>{.headerlink}</p>
<p>:</p>
<p>Log basic stats like crawled pages and scraped items.
:::</p>
<p>::: {#module-scrapy.extensions.corestats .section}
[]{#core-stats-extension}</p>
<h6 id="core-stats-extensionheaderlink"><a class="header" href="#core-stats-extensionheaderlink">Core Stats extension<a href="#module-scrapy.extensions.corestats" title="Permalink to this heading">¶</a>{.headerlink}</a></h6>
<p><em>[class]{.pre}[ ]{.w}</em>[[scrapy.extensions.corestats.]{.pre}]{.sig-prename .descclassname}[[CoreStats]{.pre}]{.sig-name .descname}<a href="_modules/scrapy/extensions/corestats.html#CoreStats">[[[source]]{.pre}]{.viewcode-link}</a>{.reference .internal}<a href="#scrapy.extensions.corestats.CoreStats" title="Permalink to this definition">¶</a>{.headerlink}</p>
<p>:</p>
<p>Enable the collection of core statistics, provided the stats collection
is enabled (see <a href="index.html#topics-stats">[Stats Collection]{.std
.std-ref}</a>{.hoverxref .tooltip .reference
.internal}).
:::</p>
<p>::: {#module-scrapy.extensions.telnet .section}
[]{#telnet-console-extension}[]{#topics-extensions-ref-telnetconsole}</p>
<h6 id="telnet-console-extensionheaderlink"><a class="header" href="#telnet-console-extensionheaderlink">Telnet console extension<a href="#module-scrapy.extensions.telnet" title="Permalink to this heading">¶</a>{.headerlink}</a></h6>
<p><em>[class]{.pre}[ ]{.w}</em>[[scrapy.extensions.telnet.]{.pre}]{.sig-prename .descclassname}[[TelnetConsole]{.pre}]{.sig-name .descname}<a href="_modules/scrapy/extensions/telnet.html#TelnetConsole">[[[source]]{.pre}]{.viewcode-link}</a>{.reference .internal}<a href="#scrapy.extensions.telnet.TelnetConsole" title="Permalink to this definition">¶</a>{.headerlink}</p>
<p>:</p>
<p>Provides a telnet console for getting into a Python interpreter inside
the currently running Scrapy process, which can be very useful for
debugging.</p>
<p>The telnet console must be enabled by the
<a href="index.html#std-setting-TELNETCONSOLE_ENABLED">[<code>TELNETCONSOLE_ENABLED</code>{.xref .std .std-setting .docutils .literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal} setting, and the server will listen in
the port specified in <a href="index.html#std-setting-TELNETCONSOLE_PORT">[<code>TELNETCONSOLE_PORT</code>{.xref .std .std-setting
.docutils .literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal}.
:::</p>
<p>::: {#module-scrapy.extensions.memusage .section}
[]{#memory-usage-extension}[]{#topics-extensions-ref-memusage}</p>
<h6 id="memory-usage-extensionheaderlink"><a class="header" href="#memory-usage-extensionheaderlink">Memory usage extension<a href="#module-scrapy.extensions.memusage" title="Permalink to this heading">¶</a>{.headerlink}</a></h6>
<p><em>[class]{.pre}[ ]{.w}</em>[[scrapy.extensions.memusage.]{.pre}]{.sig-prename .descclassname}[[MemoryUsage]{.pre}]{.sig-name .descname}<a href="_modules/scrapy/extensions/memusage.html#MemoryUsage">[[[source]]{.pre}]{.viewcode-link}</a>{.reference .internal}<a href="#scrapy.extensions.memusage.MemoryUsage" title="Permalink to this definition">¶</a>{.headerlink}</p>
<p>:</p>
<p>::: {.admonition .note}
Note</p>
<p>This extension does not work in Windows.
:::</p>
<p>Monitors the memory used by the Scrapy process that runs the spider and:</p>
<ol>
<li>
<p>sends a notification e-mail when it exceeds a certain value</p>
</li>
<li>
<p>closes the spider when it exceeds a certain value</p>
</li>
</ol>
<p>The notification e-mails can be triggered when a certain warning value
is reached (<a href="index.html#std-setting-MEMUSAGE_WARNING_MB">[<code>MEMUSAGE_WARNING_MB</code>{.xref .std .std-setting .docutils
.literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal}) and when the maximum value is reached
(<a href="index.html#std-setting-MEMUSAGE_LIMIT_MB">[<code>MEMUSAGE_LIMIT_MB</code>{.xref .std .std-setting .docutils .literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal}) which will also cause the spider to be
closed and the Scrapy process to be terminated.</p>
<p>This extension is enabled by the <a href="index.html#std-setting-MEMUSAGE_ENABLED">[<code>MEMUSAGE_ENABLED</code>{.xref .std
.std-setting .docutils .literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal} setting and can be configured with the
following settings:</p>
<ul>
<li>
<p><a href="index.html#std-setting-MEMUSAGE_LIMIT_MB">[<code>MEMUSAGE_LIMIT_MB</code>{.xref .std .std-setting .docutils .literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal}</p>
</li>
<li>
<p><a href="index.html#std-setting-MEMUSAGE_WARNING_MB">[<code>MEMUSAGE_WARNING_MB</code>{.xref .std .std-setting .docutils .literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal}</p>
</li>
<li>
<p><a href="index.html#std-setting-MEMUSAGE_NOTIFY_MAIL">[<code>MEMUSAGE_NOTIFY_MAIL</code>{.xref .std .std-setting .docutils .literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal}</p>
</li>
<li>
<p><a href="index.html#std-setting-MEMUSAGE_CHECK_INTERVAL_SECONDS">[<code>MEMUSAGE_CHECK_INTERVAL_SECONDS</code>{.xref .std .std-setting
.docutils .literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal}
:::</p>
</li>
</ul>
<p>::: {#module-scrapy.extensions.memdebug .section}
[]{#memory-debugger-extension}</p>
<h6 id="memory-debugger-extensionheaderlink"><a class="header" href="#memory-debugger-extensionheaderlink">Memory debugger extension<a href="#module-scrapy.extensions.memdebug" title="Permalink to this heading">¶</a>{.headerlink}</a></h6>
<p><em>[class]{.pre}[ ]{.w}</em>[[scrapy.extensions.memdebug.]{.pre}]{.sig-prename .descclassname}[[MemoryDebugger]{.pre}]{.sig-name .descname}<a href="_modules/scrapy/extensions/memdebug.html#MemoryDebugger">[[[source]]{.pre}]{.viewcode-link}</a>{.reference .internal}<a href="#scrapy.extensions.memdebug.MemoryDebugger" title="Permalink to this definition">¶</a>{.headerlink}</p>
<p>:</p>
<p>An extension for debugging memory usage. It collects information about:</p>
<ul>
<li>
<p>objects uncollected by the Python garbage collector</p>
</li>
<li>
<p>objects left alive that shouldn't. For more info, see <a href="index.html#topics-leaks-trackrefs">[Debugging
memory leaks with trackref]{.std
.std-ref}</a>{.hoverxref .tooltip
.reference .internal}</p>
</li>
</ul>
<p>To enable this extension, turn on the <a href="index.html#std-setting-MEMDEBUG_ENABLED">[<code>MEMDEBUG_ENABLED</code>{.xref .std
.std-setting .docutils .literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal} setting. The info will be stored in the
stats.
:::</p>
<p>::: {#module-scrapy.extensions.closespider .section}
[]{#close-spider-extension}</p>
<h6 id="close-spider-extensionheaderlink"><a class="header" href="#close-spider-extensionheaderlink">Close spider extension<a href="#module-scrapy.extensions.closespider" title="Permalink to this heading">¶</a>{.headerlink}</a></h6>
<p><em>[class]{.pre}[ ]{.w}</em>[[scrapy.extensions.closespider.]{.pre}]{.sig-prename .descclassname}[[CloseSpider]{.pre}]{.sig-name .descname}<a href="_modules/scrapy/extensions/closespider.html#CloseSpider">[[[source]]{.pre}]{.viewcode-link}</a>{.reference .internal}<a href="#scrapy.extensions.closespider.CloseSpider" title="Permalink to this definition">¶</a>{.headerlink}</p>
<p>:</p>
<p>Closes a spider automatically when some conditions are met, using a
specific closing reason for each condition.</p>
<p>The conditions for closing a spider can be configured through the
following settings:</p>
<ul>
<li>
<p><a href="#std-setting-CLOSESPIDER_TIMEOUT">[<code>CLOSESPIDER_TIMEOUT</code>{.xref .std .std-setting .docutils .literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal}</p>
</li>
<li>
<p><a href="#std-setting-CLOSESPIDER_TIMEOUT_NO_ITEM">[<code>CLOSESPIDER_TIMEOUT_NO_ITEM</code>{.xref .std .std-setting .docutils
.literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal}</p>
</li>
<li>
<p><a href="#std-setting-CLOSESPIDER_ITEMCOUNT">[<code>CLOSESPIDER_ITEMCOUNT</code>{.xref .std .std-setting .docutils .literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal}</p>
</li>
<li>
<p><a href="#std-setting-CLOSESPIDER_PAGECOUNT">[<code>CLOSESPIDER_PAGECOUNT</code>{.xref .std .std-setting .docutils .literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal}</p>
</li>
<li>
<p><a href="#std-setting-CLOSESPIDER_ERRORCOUNT">[<code>CLOSESPIDER_ERRORCOUNT</code>{.xref .std .std-setting .docutils
.literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal}</p>
</li>
</ul>
<p>::: {.admonition .note}
Note</p>
<p>When a certain closing condition is met, requests which are currently in
the downloader queue (up to <a href="index.html#std-setting-CONCURRENT_REQUESTS">[<code>CONCURRENT_REQUESTS</code>{.xref .std
.std-setting .docutils .literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal} requests) are still processed.
:::</p>
<p>::: {#closespider-timeout .section}
[]{#std-setting-CLOSESPIDER_TIMEOUT}CLOSESPIDER_TIMEOUT<a href="#closespider-timeout" title="Permalink to this heading">¶</a>{.headerlink}</p>
<p>Default: [<code>0</code>{.docutils .literal .notranslate}]{.pre}</p>
<p>An integer which specifies a number of seconds. If the spider remains
open for more than that number of second, it will be automatically
closed with the reason [<code>closespider_timeout</code>{.docutils .literal
.notranslate}]{.pre}. If zero (or non set), spiders won't be closed by
timeout.
:::</p>
<p>::: {#closespider-timeout-no-item .section}
[]{#std-setting-CLOSESPIDER_TIMEOUT_NO_ITEM}CLOSESPIDER_TIMEOUT_NO_ITEM<a href="#closespider-timeout-no-item" title="Permalink to this heading">¶</a>{.headerlink}</p>
<p>Default: [<code>0</code>{.docutils .literal .notranslate}]{.pre}</p>
<p>An integer which specifies a number of seconds. If the spider has not
produced any items in the last number of seconds, it will be closed with
the reason [<code>closespider_timeout_no_item</code>{.docutils .literal
.notranslate}]{.pre}. If zero (or non set), spiders won't be closed
regardless if it hasn't produced any items.
:::</p>
<p>::: {#closespider-itemcount .section}
[]{#std-setting-CLOSESPIDER_ITEMCOUNT}CLOSESPIDER_ITEMCOUNT<a href="#closespider-itemcount" title="Permalink to this heading">¶</a>{.headerlink}</p>
<p>Default: [<code>0</code>{.docutils .literal .notranslate}]{.pre}</p>
<p>An integer which specifies a number of items. If the spider scrapes more
than that amount and those items are passed by the item pipeline, the
spider will be closed with the reason [<code>closespider_itemcount</code>{.docutils
.literal .notranslate}]{.pre}. If zero (or non set), spiders won't be
closed by number of passed items.
:::</p>
<p>::: {#closespider-pagecount .section}
[]{#std-setting-CLOSESPIDER_PAGECOUNT}CLOSESPIDER_PAGECOUNT<a href="#closespider-pagecount" title="Permalink to this heading">¶</a>{.headerlink}</p>
<p>Default: [<code>0</code>{.docutils .literal .notranslate}]{.pre}</p>
<p>An integer which specifies the maximum number of responses to crawl. If
the spider crawls more than that, the spider will be closed with the
reason [<code>closespider_pagecount</code>{.docutils .literal .notranslate}]{.pre}.
If zero (or non set), spiders won't be closed by number of crawled
responses.
:::</p>
<p>::: {#closespider-errorcount .section}
[]{#std-setting-CLOSESPIDER_ERRORCOUNT}CLOSESPIDER_ERRORCOUNT<a href="#closespider-errorcount" title="Permalink to this heading">¶</a>{.headerlink}</p>
<p>Default: [<code>0</code>{.docutils .literal .notranslate}]{.pre}</p>
<p>An integer which specifies the maximum number of errors to receive
before closing the spider. If the spider generates more than that number
of errors, it will be closed with the reason
[<code>closespider_errorcount</code>{.docutils .literal .notranslate}]{.pre}. If
zero (or non set), spiders won't be closed by number of errors.
:::
:::</p>
<p>::: {#module-scrapy.extensions.statsmailer .section}
[]{#statsmailer-extension}</p>
<h6 id="statsmailer-extensionheaderlink"><a class="header" href="#statsmailer-extensionheaderlink">StatsMailer extension<a href="#module-scrapy.extensions.statsmailer" title="Permalink to this heading">¶</a>{.headerlink}</a></h6>
<p><em>[class]{.pre}[ ]{.w}</em>[[scrapy.extensions.statsmailer.]{.pre}]{.sig-prename .descclassname}[[StatsMailer]{.pre}]{.sig-name .descname}<a href="_modules/scrapy/extensions/statsmailer.html#StatsMailer">[[[source]]{.pre}]{.viewcode-link}</a>{.reference .internal}<a href="#scrapy.extensions.statsmailer.StatsMailer" title="Permalink to this definition">¶</a>{.headerlink}</p>
<p>:</p>
<p>This simple extension can be used to send a notification e-mail every
time a domain has finished scraping, including the Scrapy stats
collected. The email will be sent to all recipients specified in the
<a href="index.html#std-setting-STATSMAILER_RCPTS">[<code>STATSMAILER_RCPTS</code>{.xref .std .std-setting .docutils .literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal} setting.</p>
<p>Emails can be sent using the <a href="index.html#scrapy.mail.MailSender" title="scrapy.mail.MailSender">[<code>MailSender</code>{.xref .py .py-class
.docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal} class. To see a full list of parameters, including examples
on how to instantiate <a href="index.html#scrapy.mail.MailSender" title="scrapy.mail.MailSender">[<code>MailSender</code>{.xref .py .py-class .docutils
.literal
.notranslate}]{.pre}</a>{.reference
.internal} and use mail settings, see <a href="index.html#topics-email">[Sending e-mail]{.std
.std-ref}</a>{.hoverxref .tooltip .reference
.internal}.</p>
<p>[]{#module-scrapy.extensions.debug
.target}[]{#module-scrapy.extensions.periodic_log .target}
:::</p>
<p>::: {#periodic-log-extension .section}</p>
<h6 id="periodic-log-extensionheaderlink"><a class="header" href="#periodic-log-extensionheaderlink">Periodic log extension<a href="#periodic-log-extension" title="Permalink to this heading">¶</a>{.headerlink}</a></h6>
<p><em>[class]{.pre}[ ]{.w}</em>[[scrapy.extensions.periodic_log.]{.pre}]{.sig-prename .descclassname}[[PeriodicLog]{.pre}]{.sig-name .descname}<a href="_modules/scrapy/extensions/periodic_log.html#PeriodicLog">[[[source]]{.pre}]{.viewcode-link}</a>{.reference .internal}<a href="#scrapy.extensions.periodic_log.PeriodicLog" title="Permalink to this definition">¶</a>{.headerlink}</p>
<p>:</p>
<p>This extension periodically logs rich stat data as a JSON object:</p>
<p>::: {.highlight-default .notranslate}
::: highlight
2023-08-04 02:30:57 [scrapy.extensions.logstats] INFO: Crawled 976 pages (at 162 pages/min), scraped 925 items (at 161 items/min)
2023-08-04 02:30:57 [scrapy.extensions.periodic_log] INFO: {
&quot;delta&quot;: {
&quot;downloader/request_bytes&quot;: 55582,
&quot;downloader/request_count&quot;: 162,
&quot;downloader/request_method_count/GET&quot;: 162,
&quot;downloader/response_bytes&quot;: 618133,
&quot;downloader/response_count&quot;: 162,
&quot;downloader/response_status_count/200&quot;: 162,
&quot;item_scraped_count&quot;: 161
},
&quot;stats&quot;: {
&quot;downloader/request_bytes&quot;: 338243,
&quot;downloader/request_count&quot;: 992,
&quot;downloader/request_method_count/GET&quot;: 992,
&quot;downloader/response_bytes&quot;: 3836736,
&quot;downloader/response_count&quot;: 976,
&quot;downloader/response_status_count/200&quot;: 976,
&quot;item_scraped_count&quot;: 925,
&quot;log_count/INFO&quot;: 21,
&quot;log_count/WARNING&quot;: 1,
&quot;scheduler/dequeued&quot;: 992,
&quot;scheduler/dequeued/memory&quot;: 992,
&quot;scheduler/enqueued&quot;: 1050,
&quot;scheduler/enqueued/memory&quot;: 1050
},
&quot;time&quot;: {
&quot;elapsed&quot;: 360.008903,
&quot;log_interval&quot;: 60.0,
&quot;log_interval_real&quot;: 60.006694,
&quot;start_time&quot;: &quot;2023-08-03 23:24:57&quot;,
&quot;utcnow&quot;: &quot;2023-08-03 23:30:57&quot;
}
}
:::
:::</p>
<p>This extension logs the following configurable sections:</p>
<ul>
<li>
<p>[<code>&quot;delta&quot;</code>{.docutils .literal .notranslate}]{.pre} shows how some
numeric stats have changed since the last stats log message.</p>
<p>The <a href="#std-setting-PERIODIC_LOG_DELTA">[<code>PERIODIC_LOG_DELTA</code>{.xref .std .std-setting .docutils
.literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal} setting determines the target stats.
They must have [<code>int</code>{.docutils .literal .notranslate}]{.pre} or
[<code>float</code>{.docutils .literal .notranslate}]{.pre} values.</p>
</li>
<li>
<p>[<code>&quot;stats&quot;</code>{.docutils .literal .notranslate}]{.pre} shows the current
value of some stats.</p>
<p>The <a href="#std-setting-PERIODIC_LOG_STATS">[<code>PERIODIC_LOG_STATS</code>{.xref .std .std-setting .docutils
.literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal} setting determines the target stats.</p>
</li>
<li>
<p>[<code>&quot;time&quot;</code>{.docutils .literal .notranslate}]{.pre} shows detailed
timing data.</p>
<p>The <a href="#std-setting-PERIODIC_LOG_TIMING_ENABLED">[<code>PERIODIC_LOG_TIMING_ENABLED</code>{.xref .std .std-setting
.docutils .literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal} setting determines whether or not to
show this section.</p>
</li>
</ul>
<p>This extension logs data at the start, then on a fixed time interval
configurable through the <a href="index.html#std-setting-LOGSTATS_INTERVAL">[<code>LOGSTATS_INTERVAL</code>{.xref .std .std-setting
.docutils .literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal} setting, and finally right before the
crawl ends.</p>
<p>Example extension configuration:</p>
<p>::: {.highlight-python .notranslate}
::: highlight
custom_settings = {
&quot;LOG_LEVEL&quot;: &quot;INFO&quot;,
&quot;PERIODIC_LOG_STATS&quot;: {
&quot;include&quot;: [&quot;downloader/&quot;, &quot;scheduler/&quot;, &quot;log_count/&quot;, &quot;item_scraped_count/&quot;],
},
&quot;PERIODIC_LOG_DELTA&quot;: {&quot;include&quot;: [&quot;downloader/&quot;]},
&quot;PERIODIC_LOG_TIMING_ENABLED&quot;: True,
&quot;EXTENSIONS&quot;: {
&quot;scrapy.extensions.periodic_log.PeriodicLog&quot;: 0,
},
}
:::
:::</p>
<p>::: {#periodic-log-delta .section}
[]{#std-setting-PERIODIC_LOG_DELTA}PERIODIC_LOG_DELTA<a href="#periodic-log-delta" title="Permalink to this heading">¶</a>{.headerlink}</p>
<p>Default: [<code>None</code>{.docutils .literal .notranslate}]{.pre}</p>
<ul>
<li>
<p>[<code>&quot;PERIODIC_LOG_DELTA&quot;:</code>{.docutils .literal
.notranslate}]{.pre}<code> </code>{.docutils .literal
.notranslate}[<code>True</code>{.docutils .literal .notranslate}]{.pre} - show
deltas for all [<code>int</code>{.docutils .literal .notranslate}]{.pre} and
[<code>float</code>{.docutils .literal .notranslate}]{.pre} stat values.</p>
</li>
<li>
<p>[<code>&quot;PERIODIC_LOG_DELTA&quot;:</code>{.docutils .literal
.notranslate}]{.pre}<code> </code>{.docutils .literal
.notranslate}[<code>{&quot;include&quot;:</code>{.docutils .literal
.notranslate}]{.pre}<code> </code>{.docutils .literal
.notranslate}[<code>[&quot;downloader/&quot;,</code>{.docutils .literal
.notranslate}]{.pre}<code> </code>{.docutils .literal
.notranslate}[<code>&quot;scheduler/&quot;]}</code>{.docutils .literal
.notranslate}]{.pre} - show deltas for stats with names containing
any configured substring.</p>
</li>
<li>
<p>[<code>&quot;PERIODIC_LOG_DELTA&quot;:</code>{.docutils .literal
.notranslate}]{.pre}<code> </code>{.docutils .literal
.notranslate}[<code>{&quot;exclude&quot;:</code>{.docutils .literal
.notranslate}]{.pre}<code> </code>{.docutils .literal
.notranslate}[<code>[&quot;downloader/&quot;]}</code>{.docutils .literal
.notranslate}]{.pre} - show deltas for all stats with names not
containing any configured substring.
:::</p>
</li>
</ul>
<p>::: {#periodic-log-stats .section}
[]{#std-setting-PERIODIC_LOG_STATS}PERIODIC_LOG_STATS<a href="#periodic-log-stats" title="Permalink to this heading">¶</a>{.headerlink}</p>
<p>Default: [<code>None</code>{.docutils .literal .notranslate}]{.pre}</p>
<ul>
<li>
<p>[<code>&quot;PERIODIC_LOG_STATS&quot;:</code>{.docutils .literal
.notranslate}]{.pre}<code> </code>{.docutils .literal
.notranslate}[<code>True</code>{.docutils .literal .notranslate}]{.pre} - show
the current value of all stats.</p>
</li>
<li>
<p>[<code>&quot;PERIODIC_LOG_STATS&quot;:</code>{.docutils .literal
.notranslate}]{.pre}<code> </code>{.docutils .literal
.notranslate}[<code>{&quot;include&quot;:</code>{.docutils .literal
.notranslate}]{.pre}<code> </code>{.docutils .literal
.notranslate}[<code>[&quot;downloader/&quot;,</code>{.docutils .literal
.notranslate}]{.pre}<code> </code>{.docutils .literal
.notranslate}[<code>&quot;scheduler/&quot;]}</code>{.docutils .literal
.notranslate}]{.pre} - show current values for stats with names
containing any configured substring.</p>
</li>
<li>
<p>[<code>&quot;PERIODIC_LOG_STATS&quot;:</code>{.docutils .literal
.notranslate}]{.pre}<code> </code>{.docutils .literal
.notranslate}[<code>{&quot;exclude&quot;:</code>{.docutils .literal
.notranslate}]{.pre}<code> </code>{.docutils .literal
.notranslate}[<code>[&quot;downloader/&quot;]}</code>{.docutils .literal
.notranslate}]{.pre} - show current values for all stats with names
not containing any configured substring.
:::</p>
</li>
</ul>
<p>::: {#periodic-log-timing-enabled .section}
[]{#std-setting-PERIODIC_LOG_TIMING_ENABLED}PERIODIC_LOG_TIMING_ENABLED<a href="#periodic-log-timing-enabled" title="Permalink to this heading">¶</a>{.headerlink}</p>
<p>Default: [<code>False</code>{.docutils .literal .notranslate}]{.pre}</p>
<p>[<code>True</code>{.docutils .literal .notranslate}]{.pre} enables logging of
timing data (i.e. the [<code>&quot;time&quot;</code>{.docutils .literal .notranslate}]{.pre}
section).
:::
:::
:::</p>
<p>::: {#debugging-extensions .section}</p>
<h5 id="debugging-extensionsheaderlink"><a class="header" href="#debugging-extensionsheaderlink">Debugging extensions<a href="#debugging-extensions" title="Permalink to this heading">¶</a>{.headerlink}</a></h5>
<p>::: {#stack-trace-dump-extension .section}</p>
<h6 id="stack-trace-dump-extensionheaderlink"><a class="header" href="#stack-trace-dump-extensionheaderlink">Stack trace dump extension<a href="#stack-trace-dump-extension" title="Permalink to this heading">¶</a>{.headerlink}</a></h6>
<p><em>[class]{.pre}[ ]{.w}</em>[[scrapy.extensions.periodic_log.]{.pre}]{.sig-prename .descclassname}[[StackTraceDump]{.pre}]{.sig-name .descname}<a href="#scrapy.extensions.periodic_log.StackTraceDump" title="Permalink to this definition">¶</a>{.headerlink}</p>
<p>:</p>
<p>Dumps information about the running process when a
<a href="https://en.wikipedia.org/wiki/SIGQUIT">SIGQUIT</a>{.reference .external}
or
<a href="https://en.wikipedia.org/wiki/SIGUSR1_and_SIGUSR2">SIGUSR2</a>{.reference
.external} signal is received. The information dumped is the following:</p>
<ol>
<li>
<p>engine status (using
[<code>scrapy.utils.engine.get_engine_status()</code>{.docutils .literal
.notranslate}]{.pre})</p>
</li>
<li>
<p>live references (see <a href="index.html#topics-leaks-trackrefs">[Debugging memory leaks with trackref]{.std
.std-ref}</a>{.hoverxref .tooltip
.reference .internal})</p>
</li>
<li>
<p>stack trace of all threads</p>
</li>
</ol>
<p>After the stack trace and engine status is dumped, the Scrapy process
continues running normally.</p>
<p>This extension only works on POSIX-compliant platforms (i.e. not
Windows), because the
<a href="https://en.wikipedia.org/wiki/SIGQUIT">SIGQUIT</a>{.reference .external}
and
<a href="https://en.wikipedia.org/wiki/SIGUSR1_and_SIGUSR2">SIGUSR2</a>{.reference
.external} signals are not available on Windows.</p>
<p>There are at least two ways to send Scrapy the
<a href="https://en.wikipedia.org/wiki/SIGQUIT">SIGQUIT</a>{.reference .external}
signal:</p>
<ol>
<li>
<p>By pressing Ctrl-while a Scrapy process is running (Linux only?)</p>
</li>
<li>
<p>By running this command (assuming [<code>&lt;pid&gt;</code>{.docutils .literal
.notranslate}]{.pre} is the process id of the Scrapy process):</p>
<p>::: {.highlight-default .notranslate}
::: highlight
kill -QUIT <pid>
:::
:::
:::</p>
</li>
</ol>
<p>::: {#debugger-extension .section}</p>
<h6 id="debugger-extensionheaderlink"><a class="header" href="#debugger-extensionheaderlink">Debugger extension<a href="#debugger-extension" title="Permalink to this heading">¶</a>{.headerlink}</a></h6>
<p><em>[class]{.pre}[ ]{.w}</em>[[scrapy.extensions.periodic_log.]{.pre}]{.sig-prename .descclassname}[[Debugger]{.pre}]{.sig-name .descname}<a href="#scrapy.extensions.periodic_log.Debugger" title="Permalink to this definition">¶</a>{.headerlink}</p>
<p>:</p>
<p>Invokes a <a href="https://docs.python.org/3/library/pdb.html" title="(in Python v3.12)">[Python debugger]{.xref .std
.std-doc}</a>{.reference
.external} inside a running Scrapy process when a
<a href="https://en.wikipedia.org/wiki/SIGUSR1_and_SIGUSR2">SIGUSR2</a>{.reference
.external} signal is received. After the debugger is exited, the Scrapy
process continues running normally.</p>
<p>For more info see <a href="https://pythonconquerstheuniverse.wordpress.com/2009/09/10/debugging-in-python/">Debugging in
Python</a>{.reference
.external}.</p>
<p>This extension only works on POSIX-compliant platforms (i.e. not
Windows).
:::
:::
:::
:::</p>
<p>[]{#document-topics/signals}</p>
<p>::: {#signals .section}
[]{#topics-signals}</p>
<h3 id="signalsheaderlink"><a class="header" href="#signalsheaderlink">Signals<a href="#signals" title="Permalink to this heading">¶</a>{.headerlink}</a></h3>
<p>Scrapy uses signals extensively to notify when certain events occur. You
can catch some of those signals in your Scrapy project (using an
<a href="index.html#topics-extensions">[extension]{.std .std-ref}</a>{.hoverxref
.tooltip .reference .internal}, for example) to perform additional tasks
or extend Scrapy to add functionality not provided out of the box.</p>
<p>Even though signals provide several arguments, the handlers that catch
them don't need to accept all of them - the signal dispatching mechanism
will only deliver the arguments that the handler receives.</p>
<p>You can connect to signals (or send your own) through the <a href="index.html#topics-api-signals">[Signals
API]{.std .std-ref}</a>{.hoverxref .tooltip
.reference .internal}.</p>
<p>Here is a simple example showing how you can catch signals and perform
some action:</p>
<p>::: {.highlight-python .notranslate}
::: highlight
from scrapy import signals
from scrapy import Spider</p>
<pre><code>class DmozSpider(Spider):
    name = &quot;dmoz&quot;
    allowed_domains = [&quot;dmoz.org&quot;]
    start_urls = [
        &quot;http://www.dmoz.org/Computers/Programming/Languages/Python/Books/&quot;,
        &quot;http://www.dmoz.org/Computers/Programming/Languages/Python/Resources/&quot;,
    ]

    @classmethod
    def from_crawler(cls, crawler, *args, **kwargs):
        spider = super(DmozSpider, cls).from_crawler(crawler, *args, **kwargs)
        crawler.signals.connect(spider.spider_closed, signal=signals.spider_closed)
        return spider

    def spider_closed(self, spider):
        spider.logger.info(&quot;Spider closed: %s&quot;, spider.name)

    def parse(self, response):
        pass
</code></pre>
<p>:::
:::</p>
<p>::: {#deferred-signal-handlers .section}
[]{#signal-deferred}</p>
<h4 id="deferred-signal-handlersheaderlink"><a class="header" href="#deferred-signal-handlersheaderlink">Deferred signal handlers<a href="#deferred-signal-handlers" title="Permalink to this heading">¶</a>{.headerlink}</a></h4>
<p>Some signals support returning <a href="https://docs.twisted.org/en/stable/api/twisted.internet.defer.Deferred.html" title="(in Twisted)">[<code>Deferred</code>{.xref .py .py-class
.docutils .literal
.notranslate}]{.pre}</a>{.reference
.external} or <a href="https://docs.python.org/3/glossary.html#term-awaitable" title="(in Python v3.12)">[awaitable objects]{.xref .std
.std-term}</a>{.reference
.external} from their handlers, allowing you to run asynchronous code
that does not block Scrapy. If a signal handler returns one of these
objects, Scrapy waits for that asynchronous operation to finish.</p>
<p>Let's take an example using <a href="index.html#topics-coroutines">[coroutines]{.std
.std-ref}</a>{.hoverxref .tooltip .reference
.internal}:</p>
<p>::: {.highlight-python .notranslate}
::: highlight
import scrapy</p>
<pre><code>class SignalSpider(scrapy.Spider):
    name = &quot;signals&quot;
    start_urls = [&quot;https://quotes.toscrape.com/page/1/&quot;]

    @classmethod
    def from_crawler(cls, crawler, *args, **kwargs):
        spider = super(SignalSpider, cls).from_crawler(crawler, *args, **kwargs)
        crawler.signals.connect(spider.item_scraped, signal=signals.item_scraped)
        return spider

    async def item_scraped(self, item):
        # Send the scraped item to the server
        response = await treq.post(
            &quot;http://example.com/post&quot;,
            json.dumps(item).encode(&quot;ascii&quot;),
            headers={b&quot;Content-Type&quot;: [b&quot;application/json&quot;]},
        )

        return response

    def parse(self, response):
        for quote in response.css(&quot;div.quote&quot;):
            yield {
                &quot;text&quot;: quote.css(&quot;span.text::text&quot;).get(),
                &quot;author&quot;: quote.css(&quot;small.author::text&quot;).get(),
                &quot;tags&quot;: quote.css(&quot;div.tags a.tag::text&quot;).getall(),
            }
</code></pre>
<p>:::
:::</p>
<p>See the <a href="#topics-signals-ref">[Built-in signals reference]{.std
.std-ref}</a>{.hoverxref .tooltip .reference
.internal} below to know which signals support <a href="https://docs.twisted.org/en/stable/api/twisted.internet.defer.Deferred.html" title="(in Twisted)">[<code>Deferred</code>{.xref .py
.py-class .docutils .literal
.notranslate}]{.pre}</a>{.reference
.external} and <a href="https://docs.python.org/3/glossary.html#term-awaitable" title="(in Python v3.12)">[awaitable objects]{.xref .std
.std-term}</a>{.reference
.external}.
:::</p>
<p>::: {#module-scrapy.signals .section}
[]{#built-in-signals-reference}[]{#topics-signals-ref}</p>
<h4 id="built-in-signals-referenceheaderlink"><a class="header" href="#built-in-signals-referenceheaderlink">Built-in signals reference<a href="#module-scrapy.signals" title="Permalink to this heading">¶</a>{.headerlink}</a></h4>
<p>Here's the list of Scrapy built-in signals and their meaning.</p>
<p>::: {#engine-signals .section}</p>
<h5 id="engine-signalsheaderlink"><a class="header" href="#engine-signalsheaderlink">Engine signals<a href="#engine-signals" title="Permalink to this heading">¶</a>{.headerlink}</a></h5>
<p>::: {#engine-started .section}</p>
<h6 id="engine_startedheaderlink"><a class="header" href="#engine_startedheaderlink">engine_started<a href="#engine-started" title="Permalink to this heading">¶</a>{.headerlink}</a></h6>
<p>[]{#std-signal-engine_started .target}</p>
<p>[[scrapy.signals.]{.pre}]{.sig-prename .descclassname}[[engine_started]{.pre}]{.sig-name .descname}[(]{.sig-paren}[)]{.sig-paren}<a href="#scrapy.signals.engine_started" title="Permalink to this definition">¶</a>{.headerlink}</p>
<p>:   Sent when the Scrapy engine has started crawling.</p>
<pre><code>This signal supports returning deferreds from its handlers.
</code></pre>
<p>::: {.admonition .note}
Note</p>
<p>This signal may be fired <em>after</em> the <a href="#std-signal-spider_opened">[<code>spider_opened</code>{.xref .std
.std-signal .docutils .literal
.notranslate}]{.pre}</a>{.hoverxref .tooltip
.reference .internal} signal, depending on how the spider was started.
So <strong>don't</strong> rely on this signal getting fired before
<a href="#std-signal-spider_opened">[<code>spider_opened</code>{.xref .std .std-signal .docutils .literal
.notranslate}]{.pre}</a>{.hoverxref .tooltip
.reference .internal}.
:::
:::</p>
<p>::: {#engine-stopped .section}</p>
<h6 id="engine_stoppedheaderlink"><a class="header" href="#engine_stoppedheaderlink">engine_stopped<a href="#engine-stopped" title="Permalink to this heading">¶</a>{.headerlink}</a></h6>
<p>[]{#std-signal-engine_stopped .target}</p>
<p>[[scrapy.signals.]{.pre}]{.sig-prename .descclassname}[[engine_stopped]{.pre}]{.sig-name .descname}[(]{.sig-paren}[)]{.sig-paren}<a href="#scrapy.signals.engine_stopped" title="Permalink to this definition">¶</a>{.headerlink}</p>
<p>:   Sent when the Scrapy engine is stopped (for example, when a crawling
process has finished).</p>
<pre><code>This signal supports returning deferreds from its handlers.
</code></pre>
<p>:::
:::</p>
<p>::: {#item-signals .section}</p>
<h5 id="item-signalsheaderlink"><a class="header" href="#item-signalsheaderlink">Item signals<a href="#item-signals" title="Permalink to this heading">¶</a>{.headerlink}</a></h5>
<p>::: {.admonition .note}
Note</p>
<p>As at max <a href="index.html#std-setting-CONCURRENT_ITEMS">[<code>CONCURRENT_ITEMS</code>{.xref .std .std-setting .docutils
.literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal} items are processed in parallel, many
deferreds are fired together using <a href="https://docs.twisted.org/en/stable/api/twisted.internet.defer.DeferredList.html" title="(in Twisted)">[<code>DeferredList</code>{.xref .py .py-class
.docutils .literal
.notranslate}]{.pre}</a>{.reference
.external}. Hence the next batch waits for the <a href="https://docs.twisted.org/en/stable/api/twisted.internet.defer.DeferredList.html" title="(in Twisted)">[<code>DeferredList</code>{.xref
.py .py-class .docutils .literal
.notranslate}]{.pre}</a>{.reference
.external} to fire and then runs the respective item signal handler for
the next batch of scraped items.
:::</p>
<p>::: {#item-scraped .section}</p>
<h6 id="item_scrapedheaderlink"><a class="header" href="#item_scrapedheaderlink">item_scraped<a href="#item-scraped" title="Permalink to this heading">¶</a>{.headerlink}</a></h6>
<p>[]{#std-signal-item_scraped .target}</p>
<p>[[scrapy.signals.]{.pre}]{.sig-prename .descclassname}[[item_scraped]{.pre}]{.sig-name .descname}[(]{.sig-paren}<em>[[item]{.pre}]{.n}</em>, <em>[[response]{.pre}]{.n}</em>, <em>[[spider]{.pre}]{.n}</em>[)]{.sig-paren}<a href="#scrapy.signals.item_scraped" title="Permalink to this definition">¶</a>{.headerlink}</p>
<p>:   Sent when an item has been scraped, after it has passed all the
<a href="index.html#topics-item-pipeline">[Item Pipeline]{.std
.std-ref}</a>{.hoverxref .tooltip
.reference .internal} stages (without being dropped).</p>
<pre><code>This signal supports returning deferreds from its handlers.

Parameters

:   -   **item** ([[item object]{.std
        .std-ref}](index.html#item-types){.hoverxref .tooltip
        .reference .internal}) -- the scraped item

    -   **spider** ([[`Spider`{.xref .py .py-class .docutils
        .literal
        .notranslate}]{.pre}](index.html#scrapy.Spider &quot;scrapy.Spider&quot;){.reference
        .internal} object) -- the spider which scraped the item

    -   **response** ([[`Response`{.xref .py .py-class .docutils
        .literal
        .notranslate}]{.pre}](index.html#scrapy.http.Response &quot;scrapy.http.Response&quot;){.reference
        .internal} object) -- the response from where the item was
        scraped
</code></pre>
<p>:::</p>
<p>::: {#item-dropped .section}</p>
<h6 id="item_droppedheaderlink"><a class="header" href="#item_droppedheaderlink">item_dropped<a href="#item-dropped" title="Permalink to this heading">¶</a>{.headerlink}</a></h6>
<p>[]{#std-signal-item_dropped .target}</p>
<p>[[scrapy.signals.]{.pre}]{.sig-prename .descclassname}[[item_dropped]{.pre}]{.sig-name .descname}[(]{.sig-paren}<em>[[item]{.pre}]{.n}</em>, <em>[[response]{.pre}]{.n}</em>, <em>[[exception]{.pre}]{.n}</em>, <em>[[spider]{.pre}]{.n}</em>[)]{.sig-paren}<a href="#scrapy.signals.item_dropped" title="Permalink to this definition">¶</a>{.headerlink}</p>
<p>:   Sent after an item has been dropped from the <a href="index.html#topics-item-pipeline">[Item Pipeline]{.std
.std-ref}</a>{.hoverxref .tooltip
.reference .internal} when some stage raised a <a href="index.html#scrapy.exceptions.DropItem" title="scrapy.exceptions.DropItem">[<code>DropItem</code>{.xref
.py .py-exc .docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal} exception.</p>
<pre><code>This signal supports returning deferreds from its handlers.

Parameters

:   -   **item** ([[item object]{.std
        .std-ref}](index.html#item-types){.hoverxref .tooltip
        .reference .internal}) -- the item dropped from the [[Item
        Pipeline]{.std
        .std-ref}](index.html#topics-item-pipeline){.hoverxref
        .tooltip .reference .internal}

    -   **spider** ([[`Spider`{.xref .py .py-class .docutils
        .literal
        .notranslate}]{.pre}](index.html#scrapy.Spider &quot;scrapy.Spider&quot;){.reference
        .internal} object) -- the spider which scraped the item

    -   **response** ([[`Response`{.xref .py .py-class .docutils
        .literal
        .notranslate}]{.pre}](index.html#scrapy.http.Response &quot;scrapy.http.Response&quot;){.reference
        .internal} object) -- the response from where the item was
        dropped

    -   **exception** ([[`DropItem`{.xref .py .py-exc .docutils
        .literal
        .notranslate}]{.pre}](index.html#scrapy.exceptions.DropItem &quot;scrapy.exceptions.DropItem&quot;){.reference
        .internal} exception) -- the exception (which must be a
        [[`DropItem`{.xref .py .py-exc .docutils .literal
        .notranslate}]{.pre}](index.html#scrapy.exceptions.DropItem &quot;scrapy.exceptions.DropItem&quot;){.reference
        .internal} subclass) which caused the item to be dropped
</code></pre>
<p>:::</p>
<p>::: {#item-error .section}</p>
<h6 id="item_errorheaderlink"><a class="header" href="#item_errorheaderlink">item_error<a href="#item-error" title="Permalink to this heading">¶</a>{.headerlink}</a></h6>
<p>[]{#std-signal-item_error .target}</p>
<p>[[scrapy.signals.]{.pre}]{.sig-prename .descclassname}[[item_error]{.pre}]{.sig-name .descname}[(]{.sig-paren}<em>[[item]{.pre}]{.n}</em>, <em>[[response]{.pre}]{.n}</em>, <em>[[spider]{.pre}]{.n}</em>, <em>[[failure]{.pre}]{.n}</em>[)]{.sig-paren}<a href="#scrapy.signals.item_error" title="Permalink to this definition">¶</a>{.headerlink}</p>
<p>:   Sent when a <a href="index.html#topics-item-pipeline">[Item Pipeline]{.std
.std-ref}</a>{.hoverxref .tooltip
.reference .internal} generates an error (i.e. raises an exception),
except <a href="index.html#scrapy.exceptions.DropItem" title="scrapy.exceptions.DropItem">[<code>DropItem</code>{.xref .py .py-exc .docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal} exception.</p>
<pre><code>This signal supports returning deferreds from its handlers.

Parameters

:   -   **item** ([[item object]{.std
        .std-ref}](index.html#item-types){.hoverxref .tooltip
        .reference .internal}) -- the item that caused the error in
        the [[Item Pipeline]{.std
        .std-ref}](index.html#topics-item-pipeline){.hoverxref
        .tooltip .reference .internal}

    -   **response** ([[`Response`{.xref .py .py-class .docutils
        .literal
        .notranslate}]{.pre}](index.html#scrapy.http.Response &quot;scrapy.http.Response&quot;){.reference
        .internal} object) -- the response being processed when the
        exception was raised

    -   **spider** ([[`Spider`{.xref .py .py-class .docutils
        .literal
        .notranslate}]{.pre}](index.html#scrapy.Spider &quot;scrapy.Spider&quot;){.reference
        .internal} object) -- the spider which raised the exception

    -   **failure**
        ([*twisted.python.failure.Failure*](https://docs.twisted.org/en/stable/api/twisted.python.failure.Failure.html &quot;(in Twisted)&quot;){.reference
        .external}) -- the exception raised
</code></pre>
<p>:::
:::</p>
<p>::: {#spider-signals .section}</p>
<h5 id="spider-signalsheaderlink"><a class="header" href="#spider-signalsheaderlink">Spider signals<a href="#spider-signals" title="Permalink to this heading">¶</a>{.headerlink}</a></h5>
<p>::: {#spider-closed .section}</p>
<h6 id="spider_closedheaderlink"><a class="header" href="#spider_closedheaderlink">spider_closed<a href="#spider-closed" title="Permalink to this heading">¶</a>{.headerlink}</a></h6>
<p>[]{#std-signal-spider_closed .target}</p>
<p>[[scrapy.signals.]{.pre}]{.sig-prename .descclassname}[[spider_closed]{.pre}]{.sig-name .descname}[(]{.sig-paren}<em>[[spider]{.pre}]{.n}</em>, <em>[[reason]{.pre}]{.n}</em>[)]{.sig-paren}<a href="#scrapy.signals.spider_closed" title="Permalink to this definition">¶</a>{.headerlink}</p>
<p>:   Sent after a spider has been closed. This can be used to release
per-spider resources reserved on <a href="#std-signal-spider_opened">[<code>spider_opened</code>{.xref .std
.std-signal .docutils .literal
.notranslate}]{.pre}</a>{.hoverxref .tooltip
.reference .internal}.</p>
<pre><code>This signal supports returning deferreds from its handlers.

Parameters

:   -   **spider** ([[`Spider`{.xref .py .py-class .docutils
        .literal
        .notranslate}]{.pre}](index.html#scrapy.Spider &quot;scrapy.Spider&quot;){.reference
        .internal} object) -- the spider which has been closed

    -   **reason**
        ([*str*](https://docs.python.org/3/library/stdtypes.html#str &quot;(in Python v3.12)&quot;){.reference
        .external}) -- a string which describes the reason why the
        spider was closed. If it was closed because the spider has
        completed scraping, the reason is [`'finished'`{.docutils
        .literal .notranslate}]{.pre}. Otherwise, if the spider was
        manually closed by calling the [`close_spider`{.docutils
        .literal .notranslate}]{.pre} engine method, then the reason
        is the one passed in the [`reason`{.docutils .literal
        .notranslate}]{.pre} argument of that method (which defaults
        to [`'cancelled'`{.docutils .literal .notranslate}]{.pre}).
        If the engine was shutdown (for example, by hitting Ctrl-C
        to stop it) the reason will be [`'shutdown'`{.docutils
        .literal .notranslate}]{.pre}.
</code></pre>
<p>:::</p>
<p>::: {#spider-opened .section}</p>
<h6 id="spider_openedheaderlink"><a class="header" href="#spider_openedheaderlink">spider_opened<a href="#spider-opened" title="Permalink to this heading">¶</a>{.headerlink}</a></h6>
<p>[]{#std-signal-spider_opened .target}</p>
<p>[[scrapy.signals.]{.pre}]{.sig-prename .descclassname}[[spider_opened]{.pre}]{.sig-name .descname}[(]{.sig-paren}<em>[[spider]{.pre}]{.n}</em>[)]{.sig-paren}<a href="#scrapy.signals.spider_opened" title="Permalink to this definition">¶</a>{.headerlink}</p>
<p>:   Sent after a spider has been opened for crawling. This is typically
used to reserve per-spider resources, but can be used for any task
that needs to be performed when a spider is opened.</p>
<pre><code>This signal supports returning deferreds from its handlers.

Parameters

:   **spider** ([[`Spider`{.xref .py .py-class .docutils .literal
    .notranslate}]{.pre}](index.html#scrapy.Spider &quot;scrapy.Spider&quot;){.reference
    .internal} object) -- the spider which has been opened
</code></pre>
<p>:::</p>
<p>::: {#spider-idle .section}</p>
<h6 id="spider_idleheaderlink"><a class="header" href="#spider_idleheaderlink">spider_idle<a href="#spider-idle" title="Permalink to this heading">¶</a>{.headerlink}</a></h6>
<p>[]{#std-signal-spider_idle .target}</p>
<p>[[scrapy.signals.]{.pre}]{.sig-prename .descclassname}[[spider_idle]{.pre}]{.sig-name .descname}[(]{.sig-paren}<em>[[spider]{.pre}]{.n}</em>[)]{.sig-paren}<a href="#scrapy.signals.spider_idle" title="Permalink to this definition">¶</a>{.headerlink}</p>
<p>:   Sent when a spider has gone idle, which means the spider has no
further:</p>
<pre><code>&gt; &lt;div&gt;
&gt;
&gt; -   requests waiting to be downloaded
&gt;
&gt; -   requests scheduled
&gt;
&gt; -   items being processed in the item pipeline
&gt;
&gt; &lt;/div&gt;

If the idle state persists after all handlers of this signal have
finished, the engine starts closing the spider. After the spider has
finished closing, the [[`spider_closed`{.xref .std .std-signal
.docutils .literal
.notranslate}]{.pre}](#std-signal-spider_closed){.hoverxref .tooltip
.reference .internal} signal is sent.

You may raise a [[`DontCloseSpider`{.xref .py .py-exc .docutils
.literal
.notranslate}]{.pre}](index.html#scrapy.exceptions.DontCloseSpider &quot;scrapy.exceptions.DontCloseSpider&quot;){.reference
.internal} exception to prevent the spider from being closed.

Alternatively, you may raise a [[`CloseSpider`{.xref .py .py-exc
.docutils .literal
.notranslate}]{.pre}](index.html#scrapy.exceptions.CloseSpider &quot;scrapy.exceptions.CloseSpider&quot;){.reference
.internal} exception to provide a custom spider closing reason. An
idle handler is the perfect place to put some code that assesses the
final spider results and update the final closing reason accordingly
(e.g. setting it to 'too_few_results' instead of 'finished').

This signal does not support returning deferreds from its handlers.

Parameters

:   **spider** ([[`Spider`{.xref .py .py-class .docutils .literal
    .notranslate}]{.pre}](index.html#scrapy.Spider &quot;scrapy.Spider&quot;){.reference
    .internal} object) -- the spider which has gone idle
</code></pre>
<p>::: {.admonition .note}
Note</p>
<p>Scheduling some requests in your <a href="#std-signal-spider_idle">[<code>spider_idle</code>{.xref .std .std-signal
.docutils .literal
.notranslate}]{.pre}</a>{.hoverxref .tooltip
.reference .internal} handler does <strong>not</strong> guarantee that it can prevent
the spider from being closed, although it sometimes can. That's because
the spider may still remain idle if all the scheduled requests are
rejected by the scheduler (e.g. filtered due to duplication).
:::
:::</p>
<p>::: {#spider-error .section}</p>
<h6 id="spider_errorheaderlink"><a class="header" href="#spider_errorheaderlink">spider_error<a href="#spider-error" title="Permalink to this heading">¶</a>{.headerlink}</a></h6>
<p>[]{#std-signal-spider_error .target}</p>
<p>[[scrapy.signals.]{.pre}]{.sig-prename .descclassname}[[spider_error]{.pre}]{.sig-name .descname}[(]{.sig-paren}<em>[[failure]{.pre}]{.n}</em>, <em>[[response]{.pre}]{.n}</em>, <em>[[spider]{.pre}]{.n}</em>[)]{.sig-paren}<a href="#scrapy.signals.spider_error" title="Permalink to this definition">¶</a>{.headerlink}</p>
<p>:   Sent when a spider callback generates an error (i.e. raises an
exception).</p>
<pre><code>This signal does not support returning deferreds from its handlers.

Parameters

:   -   **failure**
        ([*twisted.python.failure.Failure*](https://docs.twisted.org/en/stable/api/twisted.python.failure.Failure.html &quot;(in Twisted)&quot;){.reference
        .external}) -- the exception raised

    -   **response** ([[`Response`{.xref .py .py-class .docutils
        .literal
        .notranslate}]{.pre}](index.html#scrapy.http.Response &quot;scrapy.http.Response&quot;){.reference
        .internal} object) -- the response being processed when the
        exception was raised

    -   **spider** ([[`Spider`{.xref .py .py-class .docutils
        .literal
        .notranslate}]{.pre}](index.html#scrapy.Spider &quot;scrapy.Spider&quot;){.reference
        .internal} object) -- the spider which raised the exception
</code></pre>
<p>:::</p>
<p>::: {#feed-slot-closed .section}</p>
<h6 id="feed_slot_closedheaderlink"><a class="header" href="#feed_slot_closedheaderlink">feed_slot_closed<a href="#feed-slot-closed" title="Permalink to this heading">¶</a>{.headerlink}</a></h6>
<p>[]{#std-signal-feed_slot_closed .target}</p>
<p>[[scrapy.signals.]{.pre}]{.sig-prename .descclassname}[[feed_slot_closed]{.pre}]{.sig-name .descname}[(]{.sig-paren}<em>[[slot]{.pre}]{.n}</em>[)]{.sig-paren}<a href="#scrapy.signals.feed_slot_closed" title="Permalink to this definition">¶</a>{.headerlink}</p>
<p>:   Sent when a <a href="index.html#topics-feed-exports">[feed exports]{.std
.std-ref}</a>{.hoverxref .tooltip
.reference .internal} slot is closed.</p>
<pre><code>This signal supports returning deferreds from its handlers.

Parameters

:   **slot** (*scrapy.extensions.feedexport.FeedSlot*) -- the slot
    closed
</code></pre>
<p>:::</p>
<p>::: {#feed-exporter-closed .section}</p>
<h6 id="feed_exporter_closedheaderlink"><a class="header" href="#feed_exporter_closedheaderlink">feed_exporter_closed<a href="#feed-exporter-closed" title="Permalink to this heading">¶</a>{.headerlink}</a></h6>
<p>[]{#std-signal-feed_exporter_closed .target}</p>
<p>[[scrapy.signals.]{.pre}]{.sig-prename .descclassname}[[feed_exporter_closed]{.pre}]{.sig-name .descname}[(]{.sig-paren}[)]{.sig-paren}<a href="#scrapy.signals.feed_exporter_closed" title="Permalink to this definition">¶</a>{.headerlink}</p>
<p>:   Sent when the <a href="index.html#topics-feed-exports">[feed exports]{.std
.std-ref}</a>{.hoverxref .tooltip
.reference .internal} extension is closed, during the handling of
the <a href="#std-signal-spider_closed">[<code>spider_closed</code>{.xref .std .std-signal .docutils .literal
.notranslate}]{.pre}</a>{.hoverxref .tooltip
.reference .internal} signal by the extension, after all feed
exporting has been handled.</p>
<pre><code>This signal supports returning deferreds from its handlers.
</code></pre>
<p>:::
:::</p>
<p>::: {#request-signals .section}</p>
<h5 id="request-signalsheaderlink"><a class="header" href="#request-signalsheaderlink">Request signals<a href="#request-signals" title="Permalink to this heading">¶</a>{.headerlink}</a></h5>
<p>::: {#request-scheduled .section}</p>
<h6 id="request_scheduledheaderlink"><a class="header" href="#request_scheduledheaderlink">request_scheduled<a href="#request-scheduled" title="Permalink to this heading">¶</a>{.headerlink}</a></h6>
<p>[]{#std-signal-request_scheduled .target}</p>
<p>[[scrapy.signals.]{.pre}]{.sig-prename .descclassname}[[request_scheduled]{.pre}]{.sig-name .descname}[(]{.sig-paren}<em>[[request]{.pre}]{.n}</em>, <em>[[spider]{.pre}]{.n}</em>[)]{.sig-paren}<a href="#scrapy.signals.request_scheduled" title="Permalink to this definition">¶</a>{.headerlink}</p>
<p>:   Sent when the engine schedules a [<code>Request</code>{.xref .py .py-class
.docutils .literal .notranslate}]{.pre}, to be downloaded later.</p>
<pre><code>This signal does not support returning deferreds from its handlers.

Parameters

:   -   **request** ([`Request`{.xref .py .py-class .docutils
        .literal .notranslate}]{.pre} object) -- the request that
        reached the scheduler

    -   **spider** ([[`Spider`{.xref .py .py-class .docutils
        .literal
        .notranslate}]{.pre}](index.html#scrapy.Spider &quot;scrapy.Spider&quot;){.reference
        .internal} object) -- the spider that yielded the request
</code></pre>
<p>:::</p>
<p>::: {#request-dropped .section}</p>
<h6 id="request_droppedheaderlink"><a class="header" href="#request_droppedheaderlink">request_dropped<a href="#request-dropped" title="Permalink to this heading">¶</a>{.headerlink}</a></h6>
<p>[]{#std-signal-request_dropped .target}</p>
<p>[[scrapy.signals.]{.pre}]{.sig-prename .descclassname}[[request_dropped]{.pre}]{.sig-name .descname}[(]{.sig-paren}<em>[[request]{.pre}]{.n}</em>, <em>[[spider]{.pre}]{.n}</em>[)]{.sig-paren}<a href="#scrapy.signals.request_dropped" title="Permalink to this definition">¶</a>{.headerlink}</p>
<p>:   Sent when a [<code>Request</code>{.xref .py .py-class .docutils .literal
.notranslate}]{.pre}, scheduled by the engine to be downloaded
later, is rejected by the scheduler.</p>
<pre><code>This signal does not support returning deferreds from its handlers.

Parameters

:   -   **request** ([`Request`{.xref .py .py-class .docutils
        .literal .notranslate}]{.pre} object) -- the request that
        reached the scheduler

    -   **spider** ([[`Spider`{.xref .py .py-class .docutils
        .literal
        .notranslate}]{.pre}](index.html#scrapy.Spider &quot;scrapy.Spider&quot;){.reference
        .internal} object) -- the spider that yielded the request
</code></pre>
<p>:::</p>
<p>::: {#request-reached-downloader .section}</p>
<h6 id="request_reached_downloaderheaderlink"><a class="header" href="#request_reached_downloaderheaderlink">request_reached_downloader<a href="#request-reached-downloader" title="Permalink to this heading">¶</a>{.headerlink}</a></h6>
<p>[]{#std-signal-request_reached_downloader .target}</p>
<p>[[scrapy.signals.]{.pre}]{.sig-prename .descclassname}[[request_reached_downloader]{.pre}]{.sig-name .descname}[(]{.sig-paren}<em>[[request]{.pre}]{.n}</em>, <em>[[spider]{.pre}]{.n}</em>[)]{.sig-paren}<a href="#scrapy.signals.request_reached_downloader" title="Permalink to this definition">¶</a>{.headerlink}</p>
<p>:   Sent when a [<code>Request</code>{.xref .py .py-class .docutils .literal
.notranslate}]{.pre} reached downloader.</p>
<pre><code>This signal does not support returning deferreds from its handlers.

Parameters

:   -   **request** ([`Request`{.xref .py .py-class .docutils
        .literal .notranslate}]{.pre} object) -- the request that
        reached downloader

    -   **spider** ([[`Spider`{.xref .py .py-class .docutils
        .literal
        .notranslate}]{.pre}](index.html#scrapy.Spider &quot;scrapy.Spider&quot;){.reference
        .internal} object) -- the spider that yielded the request
</code></pre>
<p>:::</p>
<p>::: {#request-left-downloader .section}</p>
<h6 id="request_left_downloaderheaderlink"><a class="header" href="#request_left_downloaderheaderlink">request_left_downloader<a href="#request-left-downloader" title="Permalink to this heading">¶</a>{.headerlink}</a></h6>
<p>[]{#std-signal-request_left_downloader .target}</p>
<p>[[scrapy.signals.]{.pre}]{.sig-prename .descclassname}[[request_left_downloader]{.pre}]{.sig-name .descname}[(]{.sig-paren}<em>[[request]{.pre}]{.n}</em>, <em>[[spider]{.pre}]{.n}</em>[)]{.sig-paren}<a href="#scrapy.signals.request_left_downloader" title="Permalink to this definition">¶</a>{.headerlink}</p>
<p>:   ::: versionadded
[New in version 2.0.]{.versionmodified .added}
:::</p>
<pre><code>Sent when a [`Request`{.xref .py .py-class .docutils .literal
.notranslate}]{.pre} leaves the downloader, even in case of failure.

This signal does not support returning deferreds from its handlers.

Parameters

:   -   **request** ([`Request`{.xref .py .py-class .docutils
        .literal .notranslate}]{.pre} object) -- the request that
        reached the downloader

    -   **spider** ([[`Spider`{.xref .py .py-class .docutils
        .literal
        .notranslate}]{.pre}](index.html#scrapy.Spider &quot;scrapy.Spider&quot;){.reference
        .internal} object) -- the spider that yielded the request
</code></pre>
<p>:::</p>
<p>::: {#bytes-received .section}</p>
<h6 id="bytes_receivedheaderlink"><a class="header" href="#bytes_receivedheaderlink">bytes_received<a href="#bytes-received" title="Permalink to this heading">¶</a>{.headerlink}</a></h6>
<p>::: versionadded
[New in version 2.2.]{.versionmodified .added}
:::</p>
<p>[]{#std-signal-bytes_received .target}</p>
<p>[[scrapy.signals.]{.pre}]{.sig-prename .descclassname}[[bytes_received]{.pre}]{.sig-name .descname}[(]{.sig-paren}<em>[[data]{.pre}]{.n}</em>, <em>[[request]{.pre}]{.n}</em>, <em>[[spider]{.pre}]{.n}</em>[)]{.sig-paren}<a href="#scrapy.signals.bytes_received" title="Permalink to this definition">¶</a>{.headerlink}</p>
<p>:   Sent by the HTTP 1.1 and S3 download handlers when a group of bytes
is received for a specific request. This signal might be fired
multiple times for the same request, with partial data each time.
For instance, a possible scenario for a 25 kb response would be two
signals fired with 10 kb of data, and a final one with 5 kb of data.</p>
<pre><code>Handlers for this signal can stop the download of a response while
it is in progress by raising the [[`StopDownload`{.xref .py .py-exc
.docutils .literal
.notranslate}]{.pre}](index.html#scrapy.exceptions.StopDownload &quot;scrapy.exceptions.StopDownload&quot;){.reference
.internal} exception. Please refer to the [[Stopping the download of
a Response]{.std
.std-ref}](index.html#topics-stop-response-download){.hoverxref
.tooltip .reference .internal} topic for additional information and
examples.

This signal does not support returning deferreds from its handlers.

Parameters

:   -   **data** ([[`bytes`{.xref .py .py-class .docutils .literal
        .notranslate}]{.pre}](https://docs.python.org/3/library/stdtypes.html#bytes &quot;(in Python v3.12)&quot;){.reference
        .external} object) -- the data received by the download
        handler

    -   **request** ([`Request`{.xref .py .py-class .docutils
        .literal .notranslate}]{.pre} object) -- the request that
        generated the download

    -   **spider** ([[`Spider`{.xref .py .py-class .docutils
        .literal
        .notranslate}]{.pre}](index.html#scrapy.Spider &quot;scrapy.Spider&quot;){.reference
        .internal} object) -- the spider associated with the
        response
</code></pre>
<p>:::</p>
<p>::: {#headers-received .section}</p>
<h6 id="headers_receivedheaderlink"><a class="header" href="#headers_receivedheaderlink">headers_received<a href="#headers-received" title="Permalink to this heading">¶</a>{.headerlink}</a></h6>
<p>::: versionadded
[New in version 2.5.]{.versionmodified .added}
:::</p>
<p>[]{#std-signal-headers_received .target}</p>
<p>[[scrapy.signals.]{.pre}]{.sig-prename .descclassname}[[headers_received]{.pre}]{.sig-name .descname}[(]{.sig-paren}<em>[[headers]{.pre}]{.n}</em>, <em>[[body_length]{.pre}]{.n}</em>, <em>[[request]{.pre}]{.n}</em>, <em>[[spider]{.pre}]{.n}</em>[)]{.sig-paren}<a href="#scrapy.signals.headers_received" title="Permalink to this definition">¶</a>{.headerlink}</p>
<p>:   Sent by the HTTP 1.1 and S3 download handlers when the response
headers are available for a given request, before downloading any
additional content.</p>
<pre><code>Handlers for this signal can stop the download of a response while
it is in progress by raising the [[`StopDownload`{.xref .py .py-exc
.docutils .literal
.notranslate}]{.pre}](index.html#scrapy.exceptions.StopDownload &quot;scrapy.exceptions.StopDownload&quot;){.reference
.internal} exception. Please refer to the [[Stopping the download of
a Response]{.std
.std-ref}](index.html#topics-stop-response-download){.hoverxref
.tooltip .reference .internal} topic for additional information and
examples.

This signal does not support returning deferreds from its handlers.

Parameters

:   -   **headers** ([`scrapy.http.headers.Headers`{.xref .py
        .py-class .docutils .literal .notranslate}]{.pre} object) --
        the headers received by the download handler

    -   **body_length** (int) -- expected size of the response body,
        in bytes

    -   **request** ([`Request`{.xref .py .py-class .docutils
        .literal .notranslate}]{.pre} object) -- the request that
        generated the download

    -   **spider** ([[`Spider`{.xref .py .py-class .docutils
        .literal
        .notranslate}]{.pre}](index.html#scrapy.Spider &quot;scrapy.Spider&quot;){.reference
        .internal} object) -- the spider associated with the
        response
</code></pre>
<p>:::
:::</p>
<p>::: {#response-signals .section}</p>
<h5 id="response-signalsheaderlink"><a class="header" href="#response-signalsheaderlink">Response signals<a href="#response-signals" title="Permalink to this heading">¶</a>{.headerlink}</a></h5>
<p>::: {#response-received .section}</p>
<h6 id="response_receivedheaderlink"><a class="header" href="#response_receivedheaderlink">response_received<a href="#response-received" title="Permalink to this heading">¶</a>{.headerlink}</a></h6>
<p>[]{#std-signal-response_received .target}</p>
<p>[[scrapy.signals.]{.pre}]{.sig-prename .descclassname}[[response_received]{.pre}]{.sig-name .descname}[(]{.sig-paren}<em>[[response]{.pre}]{.n}</em>, <em>[[request]{.pre}]{.n}</em>, <em>[[spider]{.pre}]{.n}</em>[)]{.sig-paren}<a href="#scrapy.signals.response_received" title="Permalink to this definition">¶</a>{.headerlink}</p>
<p>:   Sent when the engine receives a new <a href="index.html#scrapy.http.Response" title="scrapy.http.Response">[<code>Response</code>{.xref .py .py-class
.docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal} from the downloader.</p>
<pre><code>This signal does not support returning deferreds from its handlers.

Parameters

:   -   **response** ([[`Response`{.xref .py .py-class .docutils
        .literal
        .notranslate}]{.pre}](index.html#scrapy.http.Response &quot;scrapy.http.Response&quot;){.reference
        .internal} object) -- the response received

    -   **request** ([`Request`{.xref .py .py-class .docutils
        .literal .notranslate}]{.pre} object) -- the request that
        generated the response

    -   **spider** ([[`Spider`{.xref .py .py-class .docutils
        .literal
        .notranslate}]{.pre}](index.html#scrapy.Spider &quot;scrapy.Spider&quot;){.reference
        .internal} object) -- the spider for which the response is
        intended
</code></pre>
<p>::: {.admonition .note}
Note</p>
<p>The [<code>request</code>{.docutils .literal .notranslate}]{.pre} argument might
not contain the original request that reached the downloader, if a
<a href="index.html#topics-downloader-middleware">[Downloader Middleware]{.std
.std-ref}</a>{.hoverxref .tooltip
.reference .internal} modifies the <a href="index.html#scrapy.http.Response" title="scrapy.http.Response">[<code>Response</code>{.xref .py .py-class
.docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal} object and sets a specific [<code>request</code>{.docutils .literal
.notranslate}]{.pre} attribute.
:::
:::</p>
<p>::: {#response-downloaded .section}</p>
<h6 id="response_downloadedheaderlink"><a class="header" href="#response_downloadedheaderlink">response_downloaded<a href="#response-downloaded" title="Permalink to this heading">¶</a>{.headerlink}</a></h6>
<p>[]{#std-signal-response_downloaded .target}</p>
<p>[[scrapy.signals.]{.pre}]{.sig-prename .descclassname}[[response_downloaded]{.pre}]{.sig-name .descname}[(]{.sig-paren}<em>[[response]{.pre}]{.n}</em>, <em>[[request]{.pre}]{.n}</em>, <em>[[spider]{.pre}]{.n}</em>[)]{.sig-paren}<a href="#scrapy.signals.response_downloaded" title="Permalink to this definition">¶</a>{.headerlink}</p>
<p>:   Sent by the downloader right after a [<code>HTTPResponse</code>{.docutils
.literal .notranslate}]{.pre} is downloaded.</p>
<pre><code>This signal does not support returning deferreds from its handlers.

Parameters

:   -   **response** ([[`Response`{.xref .py .py-class .docutils
        .literal
        .notranslate}]{.pre}](index.html#scrapy.http.Response &quot;scrapy.http.Response&quot;){.reference
        .internal} object) -- the response downloaded

    -   **request** ([`Request`{.xref .py .py-class .docutils
        .literal .notranslate}]{.pre} object) -- the request that
        generated the response

    -   **spider** ([[`Spider`{.xref .py .py-class .docutils
        .literal
        .notranslate}]{.pre}](index.html#scrapy.Spider &quot;scrapy.Spider&quot;){.reference
        .internal} object) -- the spider for which the response is
        intended
</code></pre>
<p>:::
:::
:::
:::</p>
<p>[]{#document-topics/scheduler}</p>
<p>::: {#module-scrapy.core.scheduler .section}
[]{#scheduler}[]{#topics-scheduler}</p>
<h3 id="schedulerheaderlink-2"><a class="header" href="#schedulerheaderlink-2">Scheduler<a href="#module-scrapy.core.scheduler" title="Permalink to this heading">¶</a>{.headerlink}</a></h3>
<p>The scheduler component receives requests from the <a href="index.html#component-engine">[engine]{.std
.std-ref}</a>{.hoverxref .tooltip .reference
.internal} and stores them into persistent and/or non-persistent data
structures. It also gets those requests and feeds them back to the
engine when it asks for a next request to be downloaded.</p>
<p>::: {#overriding-the-default-scheduler .section}</p>
<h4 id="overriding-the-default-schedulerheaderlink"><a class="header" href="#overriding-the-default-schedulerheaderlink">Overriding the default scheduler<a href="#overriding-the-default-scheduler" title="Permalink to this heading">¶</a>{.headerlink}</a></h4>
<p>You can use your own custom scheduler class by supplying its full Python
path in the <a href="index.html#std-setting-SCHEDULER">[<code>SCHEDULER</code>{.xref .std .std-setting .docutils .literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal} setting.
:::</p>
<p>::: {#minimal-scheduler-interface .section}</p>
<h4 id="minimal-scheduler-interfaceheaderlink"><a class="header" href="#minimal-scheduler-interfaceheaderlink">Minimal scheduler interface<a href="#minimal-scheduler-interface" title="Permalink to this heading">¶</a>{.headerlink}</a></h4>
<p><em>[class]{.pre}[ ]{.w}</em>[[scrapy.core.scheduler.]{.pre}]{.sig-prename .descclassname}[[BaseScheduler]{.pre}]{.sig-name .descname}<a href="_modules/scrapy/core/scheduler.html#BaseScheduler">[[[source]]{.pre}]{.viewcode-link}</a>{.reference .internal}<a href="#scrapy.core.scheduler.BaseScheduler" title="Permalink to this definition">¶</a>{.headerlink}</p>
<p>:   The scheduler component is responsible for storing requests received
from the engine, and feeding them back upon request (also to the
engine).</p>
<pre><code>The original sources of said requests are:

-   Spider: [`start_requests`{.docutils .literal
    .notranslate}]{.pre} method, requests created for URLs in the
    [`start_urls`{.docutils .literal .notranslate}]{.pre} attribute,
    request callbacks

-   Spider middleware: [`process_spider_output`{.docutils .literal
    .notranslate}]{.pre} and [`process_spider_exception`{.docutils
    .literal .notranslate}]{.pre} methods

-   Downloader middleware: [`process_request`{.docutils .literal
    .notranslate}]{.pre}, [`process_response`{.docutils .literal
    .notranslate}]{.pre} and [`process_exception`{.docutils .literal
    .notranslate}]{.pre} methods

The order in which the scheduler returns its stored requests (via
the [`next_request`{.docutils .literal .notranslate}]{.pre} method)
plays a great part in determining the order in which those requests
are downloaded.

The methods defined in this class constitute the minimal interface
that the Scrapy engine will interact with.

[[close]{.pre}]{.sig-name .descname}[(]{.sig-paren}*[[reason]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[[[str]{.pre}](https://docs.python.org/3/library/stdtypes.html#str &quot;(in Python v3.12)&quot;){.reference .external}]{.n}*[)]{.sig-paren} [[→]{.sig-return-icon} [[[Optional]{.pre}](https://docs.python.org/3/library/typing.html#typing.Optional &quot;(in Python v3.12)&quot;){.reference .external}[[\[]{.pre}]{.p}[[Deferred]{.pre}](https://docs.twisted.org/en/stable/api/twisted.internet.defer.Deferred.html &quot;(in Twisted)&quot;){.reference .external}[[\]]{.pre}]{.p}]{.sig-return-typehint}]{.sig-return}[[[\[source\]]{.pre}]{.viewcode-link}](_modules/scrapy/core/scheduler.html#BaseScheduler.close){.reference .internal}[¶](#scrapy.core.scheduler.BaseScheduler.close &quot;Permalink to this definition&quot;){.headerlink}

:   Called when the spider is closed by the engine. It receives the
    reason why the crawl finished as argument and it's useful to
    execute cleaning code.

    Parameters

    :   **reason** ([[`str`{.xref .py .py-class .docutils .literal
        .notranslate}]{.pre}](https://docs.python.org/3/library/stdtypes.html#str &quot;(in Python v3.12)&quot;){.reference
        .external}) -- a string which describes the reason why the
        spider was closed

*[abstract]{.pre}[ ]{.w}*[[enqueue_request]{.pre}]{.sig-name .descname}[(]{.sig-paren}*[[request]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[[[Request]{.pre}](index.html#scrapy.http.Request &quot;scrapy.http.request.Request&quot;){.reference .internal}]{.n}*[)]{.sig-paren} [[→]{.sig-return-icon} [[[bool]{.pre}](https://docs.python.org/3/library/functions.html#bool &quot;(in Python v3.12)&quot;){.reference .external}]{.sig-return-typehint}]{.sig-return}[[[\[source\]]{.pre}]{.viewcode-link}](_modules/scrapy/core/scheduler.html#BaseScheduler.enqueue_request){.reference .internal}[¶](#scrapy.core.scheduler.BaseScheduler.enqueue_request &quot;Permalink to this definition&quot;){.headerlink}

:   Process a request received by the engine.

    Return [`True`{.docutils .literal .notranslate}]{.pre} if the
    request is stored correctly, [`False`{.docutils .literal
    .notranslate}]{.pre} otherwise.

    If [`False`{.docutils .literal .notranslate}]{.pre}, the engine
    will fire a [`request_dropped`{.docutils .literal
    .notranslate}]{.pre} signal, and will not make further attempts
    to schedule the request at a later time. For reference, the
    default Scrapy scheduler returns [`False`{.docutils .literal
    .notranslate}]{.pre} when the request is rejected by the
    dupefilter.

*[classmethod]{.pre}[ ]{.w}*[[from_crawler]{.pre}]{.sig-name .descname}[(]{.sig-paren}*[[crawler]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[[[Crawler]{.pre}](index.html#scrapy.crawler.Crawler &quot;scrapy.crawler.Crawler&quot;){.reference .internal}]{.n}*[)]{.sig-paren} [[→]{.sig-return-icon} [[Self]{.pre}]{.sig-return-typehint}]{.sig-return}[[[\[source\]]{.pre}]{.viewcode-link}](_modules/scrapy/core/scheduler.html#BaseScheduler.from_crawler){.reference .internal}[¶](#scrapy.core.scheduler.BaseScheduler.from_crawler &quot;Permalink to this definition&quot;){.headerlink}

:   Factory method which receives the current [[`Crawler`{.xref .py
    .py-class .docutils .literal
    .notranslate}]{.pre}](index.html#scrapy.crawler.Crawler &quot;scrapy.crawler.Crawler&quot;){.reference
    .internal} object as argument.

*[abstract]{.pre}[ ]{.w}*[[has_pending_requests]{.pre}]{.sig-name .descname}[(]{.sig-paren}[)]{.sig-paren} [[→]{.sig-return-icon} [[[bool]{.pre}](https://docs.python.org/3/library/functions.html#bool &quot;(in Python v3.12)&quot;){.reference .external}]{.sig-return-typehint}]{.sig-return}[[[\[source\]]{.pre}]{.viewcode-link}](_modules/scrapy/core/scheduler.html#BaseScheduler.has_pending_requests){.reference .internal}[¶](#scrapy.core.scheduler.BaseScheduler.has_pending_requests &quot;Permalink to this definition&quot;){.headerlink}

:   [`True`{.docutils .literal .notranslate}]{.pre} if the scheduler
    has enqueued requests, [`False`{.docutils .literal
    .notranslate}]{.pre} otherwise

*[abstract]{.pre}[ ]{.w}*[[next_request]{.pre}]{.sig-name .descname}[(]{.sig-paren}[)]{.sig-paren} [[→]{.sig-return-icon} [[[Optional]{.pre}](https://docs.python.org/3/library/typing.html#typing.Optional &quot;(in Python v3.12)&quot;){.reference .external}[[\[]{.pre}]{.p}[[Request]{.pre}](index.html#scrapy.http.Request &quot;scrapy.http.request.Request&quot;){.reference .internal}[[\]]{.pre}]{.p}]{.sig-return-typehint}]{.sig-return}[[[\[source\]]{.pre}]{.viewcode-link}](_modules/scrapy/core/scheduler.html#BaseScheduler.next_request){.reference .internal}[¶](#scrapy.core.scheduler.BaseScheduler.next_request &quot;Permalink to this definition&quot;){.headerlink}

:   Return the next [[`Request`{.xref .py .py-class .docutils
    .literal
    .notranslate}]{.pre}](index.html#scrapy.http.Request &quot;scrapy.http.Request&quot;){.reference
    .internal} to be processed, or [`None`{.docutils .literal
    .notranslate}]{.pre} to indicate that there are no requests to
    be considered ready at the moment.

    Returning [`None`{.docutils .literal .notranslate}]{.pre}
    implies that no request from the scheduler will be sent to the
    downloader in the current reactor cycle. The engine will
    continue calling [`next_request`{.docutils .literal
    .notranslate}]{.pre} until [`has_pending_requests`{.docutils
    .literal .notranslate}]{.pre} is [`False`{.docutils .literal
    .notranslate}]{.pre}.

[[open]{.pre}]{.sig-name .descname}[(]{.sig-paren}*[[spider]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[[[Spider]{.pre}](index.html#scrapy.spiders.Spider &quot;scrapy.spiders.Spider&quot;){.reference .internal}]{.n}*[)]{.sig-paren} [[→]{.sig-return-icon} [[[Optional]{.pre}](https://docs.python.org/3/library/typing.html#typing.Optional &quot;(in Python v3.12)&quot;){.reference .external}[[\[]{.pre}]{.p}[[Deferred]{.pre}](https://docs.twisted.org/en/stable/api/twisted.internet.defer.Deferred.html &quot;(in Twisted)&quot;){.reference .external}[[\]]{.pre}]{.p}]{.sig-return-typehint}]{.sig-return}[[[\[source\]]{.pre}]{.viewcode-link}](_modules/scrapy/core/scheduler.html#BaseScheduler.open){.reference .internal}[¶](#scrapy.core.scheduler.BaseScheduler.open &quot;Permalink to this definition&quot;){.headerlink}

:   Called when the spider is opened by the engine. It receives the
    spider instance as argument and it's useful to execute
    initialization code.

    Parameters

    :   **spider** ([[`Spider`{.xref .py .py-class .docutils
        .literal
        .notranslate}]{.pre}](index.html#scrapy.spiders.Spider &quot;scrapy.spiders.Spider&quot;){.reference
        .internal}) -- the spider object for the current crawl
</code></pre>
<p>:::</p>
<p>::: {#default-scrapy-scheduler .section}</p>
<h4 id="default-scrapy-schedulerheaderlink"><a class="header" href="#default-scrapy-schedulerheaderlink">Default Scrapy scheduler<a href="#default-scrapy-scheduler" title="Permalink to this heading">¶</a>{.headerlink}</a></h4>
<p><em>[class]{.pre}[ ]{.w}</em>[[scrapy.core.scheduler.]{.pre}]{.sig-prename .descclassname}[[Scheduler]{.pre}]{.sig-name .descname}[(]{.sig-paren}<em>[[dupefilter]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[[BaseDupeFilter]{.pre}]{.n}</em>, <em>[[jobdir]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[<a href="https://docs.python.org/3/library/typing.html#typing.Optional" title="(in Python v3.12)">[Optional]{.pre}</a>{.reference .external}[[[]{.pre}]{.p}<a href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.12)">[str]{.pre}</a>{.reference .external}[[]]{.pre}]{.p}]{.n}[ ]{.w}[[=]{.pre}]{.o}[ ]{.w}[[None]{.pre}]{.default_value}</em>, <em>[[dqclass]{.pre}]{.n}[[=]{.pre}]{.o}[[None]{.pre}]{.default_value}</em>, <em>[[mqclass]{.pre}]{.n}[[=]{.pre}]{.o}[[None]{.pre}]{.default_value}</em>, <em>[[logunser]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[<a href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.12)">[bool]{.pre}</a>{.reference .external}]{.n}[ ]{.w}[[=]{.pre}]{.o}[ ]{.w}[[False]{.pre}]{.default_value}</em>, <em>[[stats]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[<a href="https://docs.python.org/3/library/typing.html#typing.Optional" title="(in Python v3.12)">[Optional]{.pre}</a>{.reference .external}[[[]{.pre}]{.p}<a href="index.html#scrapy.statscollectors.StatsCollector" title="scrapy.statscollectors.StatsCollector">[StatsCollector]{.pre}</a>{.reference .internal}[[]]{.pre}]{.p}]{.n}[ ]{.w}[[=]{.pre}]{.o}[ ]{.w}[[None]{.pre}]{.default_value}</em>, <em>[[pqclass]{.pre}]{.n}[[=]{.pre}]{.o}[[None]{.pre}]{.default_value}</em>, <em>[[crawler]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[<a href="https://docs.python.org/3/library/typing.html#typing.Optional" title="(in Python v3.12)">[Optional]{.pre}</a>{.reference .external}[[[]{.pre}]{.p}<a href="index.html#scrapy.crawler.Crawler" title="scrapy.crawler.Crawler">[Crawler]{.pre}</a>{.reference .internal}[[]]{.pre}]{.p}]{.n}[ ]{.w}[[=]{.pre}]{.o}[ ]{.w}[[None]{.pre}]{.default_value}</em>[)]{.sig-paren}<a href="_modules/scrapy/core/scheduler.html#Scheduler">[[[source]]{.pre}]{.viewcode-link}</a>{.reference .internal}<a href="#scrapy.core.scheduler.Scheduler" title="Permalink to this definition">¶</a>{.headerlink}</p>
<p>:   Default Scrapy scheduler. This implementation also handles
duplication filtering via the <a href="index.html#std-setting-DUPEFILTER_CLASS">[<code>dupefilter</code>{.xref .std .std-setting
.docutils .literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal}.</p>
<pre><code>This scheduler stores requests into several priority queues (defined
by the [[`SCHEDULER_PRIORITY_QUEUE`{.xref .std .std-setting
.docutils .literal
.notranslate}]{.pre}](index.html#std-setting-SCHEDULER_PRIORITY_QUEUE){.hoverxref
.tooltip .reference .internal} setting). In turn, said priority
queues are backed by either memory or disk based queues
(respectively defined by the [[`SCHEDULER_MEMORY_QUEUE`{.xref .std
.std-setting .docutils .literal
.notranslate}]{.pre}](index.html#std-setting-SCHEDULER_MEMORY_QUEUE){.hoverxref
.tooltip .reference .internal} and [[`SCHEDULER_DISK_QUEUE`{.xref
.std .std-setting .docutils .literal
.notranslate}]{.pre}](index.html#std-setting-SCHEDULER_DISK_QUEUE){.hoverxref
.tooltip .reference .internal} settings).

Request prioritization is almost entirely delegated to the priority
queue. The only prioritization performed by this scheduler is using
the disk-based queue if present (i.e. if the [[`JOBDIR`{.xref .std
.std-setting .docutils .literal
.notranslate}]{.pre}](index.html#std-setting-JOBDIR){.hoverxref
.tooltip .reference .internal} setting is defined) and falling back
to the memory-based queue if a serialization error occurs. If the
disk queue is not present, the memory one is used directly.

Parameters

:   -   **dupefilter** ([`scrapy.dupefilters.BaseDupeFilter`{.xref
        .py .py-class .docutils .literal .notranslate}]{.pre}
        instance or similar: any class that implements the
        BaseDupeFilter interface) -- An object responsible for
        checking and filtering duplicate requests. The value for the
        [[`DUPEFILTER_CLASS`{.xref .std .std-setting .docutils
        .literal
        .notranslate}]{.pre}](index.html#std-setting-DUPEFILTER_CLASS){.hoverxref
        .tooltip .reference .internal} setting is used by default.

    -   **jobdir** ([[`str`{.xref .py .py-class .docutils .literal
        .notranslate}]{.pre}](https://docs.python.org/3/library/stdtypes.html#str &quot;(in Python v3.12)&quot;){.reference
        .external} or [`None`{.docutils .literal
        .notranslate}]{.pre}) -- The path of a directory to be used
        for persisting the crawl's state. The value for the
        [[`JOBDIR`{.xref .std .std-setting .docutils .literal
        .notranslate}]{.pre}](index.html#std-setting-JOBDIR){.hoverxref
        .tooltip .reference .internal} setting is used by default.
        See [[Jobs: pausing and resuming crawls]{.std
        .std-ref}](index.html#topics-jobs){.hoverxref .tooltip
        .reference .internal}.

    -   **dqclass** (*class*) -- A class to be used as persistent
        request queue. The value for the
        [[`SCHEDULER_DISK_QUEUE`{.xref .std .std-setting .docutils
        .literal
        .notranslate}]{.pre}](index.html#std-setting-SCHEDULER_DISK_QUEUE){.hoverxref
        .tooltip .reference .internal} setting is used by default.

    -   **mqclass** (*class*) -- A class to be used as
        non-persistent request queue. The value for the
        [[`SCHEDULER_MEMORY_QUEUE`{.xref .std .std-setting .docutils
        .literal
        .notranslate}]{.pre}](index.html#std-setting-SCHEDULER_MEMORY_QUEUE){.hoverxref
        .tooltip .reference .internal} setting is used by default.

    -   **logunser**
        ([*bool*](https://docs.python.org/3/library/functions.html#bool &quot;(in Python v3.12)&quot;){.reference
        .external}) -- A boolean that indicates whether or not
        unserializable requests should be logged. The value for the
        [[`SCHEDULER_DEBUG`{.xref .std .std-setting .docutils
        .literal
        .notranslate}]{.pre}](index.html#std-setting-SCHEDULER_DEBUG){.hoverxref
        .tooltip .reference .internal} setting is used by default.

    -   **stats** ([[`scrapy.statscollectors.StatsCollector`{.xref
        .py .py-class .docutils .literal
        .notranslate}]{.pre}](index.html#scrapy.statscollectors.StatsCollector &quot;scrapy.statscollectors.StatsCollector&quot;){.reference
        .internal} instance or similar: any class that implements
        the StatsCollector interface) -- A stats collector object to
        record stats about the request scheduling process. The value
        for the [[`STATS_CLASS`{.xref .std .std-setting .docutils
        .literal
        .notranslate}]{.pre}](index.html#std-setting-STATS_CLASS){.hoverxref
        .tooltip .reference .internal} setting is used by default.

    -   **pqclass** (*class*) -- A class to be used as priority
        queue for requests. The value for the
        [[`SCHEDULER_PRIORITY_QUEUE`{.xref .std .std-setting
        .docutils .literal
        .notranslate}]{.pre}](index.html#std-setting-SCHEDULER_PRIORITY_QUEUE){.hoverxref
        .tooltip .reference .internal} setting is used by default.

    -   **crawler** ([[`scrapy.crawler.Crawler`{.xref .py .py-class
        .docutils .literal
        .notranslate}]{.pre}](index.html#scrapy.crawler.Crawler &quot;scrapy.crawler.Crawler&quot;){.reference
        .internal}) -- The crawler object corresponding to the
        current crawl.

[[\_\_len\_\_]{.pre}]{.sig-name .descname}[(]{.sig-paren}[)]{.sig-paren} [[→]{.sig-return-icon} [[[int]{.pre}](https://docs.python.org/3/library/functions.html#int &quot;(in Python v3.12)&quot;){.reference .external}]{.sig-return-typehint}]{.sig-return}[[[\[source\]]{.pre}]{.viewcode-link}](_modules/scrapy/core/scheduler.html#Scheduler.__len__){.reference .internal}[¶](#scrapy.core.scheduler.Scheduler.__len__ &quot;Permalink to this definition&quot;){.headerlink}

:   Return the total amount of enqueued requests

[[close]{.pre}]{.sig-name .descname}[(]{.sig-paren}*[[reason]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[[[str]{.pre}](https://docs.python.org/3/library/stdtypes.html#str &quot;(in Python v3.12)&quot;){.reference .external}]{.n}*[)]{.sig-paren} [[→]{.sig-return-icon} [[[Optional]{.pre}](https://docs.python.org/3/library/typing.html#typing.Optional &quot;(in Python v3.12)&quot;){.reference .external}[[\[]{.pre}]{.p}[[Deferred]{.pre}](https://docs.twisted.org/en/stable/api/twisted.internet.defer.Deferred.html &quot;(in Twisted)&quot;){.reference .external}[[\]]{.pre}]{.p}]{.sig-return-typehint}]{.sig-return}[[[\[source\]]{.pre}]{.viewcode-link}](_modules/scrapy/core/scheduler.html#Scheduler.close){.reference .internal}[¶](#scrapy.core.scheduler.Scheduler.close &quot;Permalink to this definition&quot;){.headerlink}

:   1.  dump pending requests to disk if there is a disk queue

    2.  return the result of the dupefilter's [`close`{.docutils
        .literal .notranslate}]{.pre} method

[[enqueue_request]{.pre}]{.sig-name .descname}[(]{.sig-paren}*[[request]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[[[Request]{.pre}](index.html#scrapy.http.Request &quot;scrapy.http.request.Request&quot;){.reference .internal}]{.n}*[)]{.sig-paren} [[→]{.sig-return-icon} [[[bool]{.pre}](https://docs.python.org/3/library/functions.html#bool &quot;(in Python v3.12)&quot;){.reference .external}]{.sig-return-typehint}]{.sig-return}[[[\[source\]]{.pre}]{.viewcode-link}](_modules/scrapy/core/scheduler.html#Scheduler.enqueue_request){.reference .internal}[¶](#scrapy.core.scheduler.Scheduler.enqueue_request &quot;Permalink to this definition&quot;){.headerlink}

:   Unless the received request is filtered out by the Dupefilter,
    attempt to push it into the disk queue, falling back to pushing
    it into the memory queue.

    Increment the appropriate stats, such as:
    [`scheduler/enqueued`{.docutils .literal .notranslate}]{.pre},
    [`scheduler/enqueued/disk`{.docutils .literal
    .notranslate}]{.pre}, [`scheduler/enqueued/memory`{.docutils
    .literal .notranslate}]{.pre}.

    Return [`True`{.docutils .literal .notranslate}]{.pre} if the
    request was stored successfully, [`False`{.docutils .literal
    .notranslate}]{.pre} otherwise.

*[classmethod]{.pre}[ ]{.w}*[[from_crawler]{.pre}]{.sig-name .descname}[(]{.sig-paren}*[[crawler]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[[[Crawler]{.pre}](index.html#scrapy.crawler.Crawler &quot;scrapy.crawler.Crawler&quot;){.reference .internal}]{.n}*[)]{.sig-paren} [[→]{.sig-return-icon} [[SchedulerTV]{.pre}]{.sig-return-typehint}]{.sig-return}[[[\[source\]]{.pre}]{.viewcode-link}](_modules/scrapy/core/scheduler.html#Scheduler.from_crawler){.reference .internal}[¶](#scrapy.core.scheduler.Scheduler.from_crawler &quot;Permalink to this definition&quot;){.headerlink}

:   Factory method, initializes the scheduler with arguments taken
    from the crawl settings

[[has_pending_requests]{.pre}]{.sig-name .descname}[(]{.sig-paren}[)]{.sig-paren} [[→]{.sig-return-icon} [[[bool]{.pre}](https://docs.python.org/3/library/functions.html#bool &quot;(in Python v3.12)&quot;){.reference .external}]{.sig-return-typehint}]{.sig-return}[[[\[source\]]{.pre}]{.viewcode-link}](_modules/scrapy/core/scheduler.html#Scheduler.has_pending_requests){.reference .internal}[¶](#scrapy.core.scheduler.Scheduler.has_pending_requests &quot;Permalink to this definition&quot;){.headerlink}

:   [`True`{.docutils .literal .notranslate}]{.pre} if the scheduler
    has enqueued requests, [`False`{.docutils .literal
    .notranslate}]{.pre} otherwise

[[next_request]{.pre}]{.sig-name .descname}[(]{.sig-paren}[)]{.sig-paren} [[→]{.sig-return-icon} [[[Optional]{.pre}](https://docs.python.org/3/library/typing.html#typing.Optional &quot;(in Python v3.12)&quot;){.reference .external}[[\[]{.pre}]{.p}[[Request]{.pre}](index.html#scrapy.http.Request &quot;scrapy.http.request.Request&quot;){.reference .internal}[[\]]{.pre}]{.p}]{.sig-return-typehint}]{.sig-return}[[[\[source\]]{.pre}]{.viewcode-link}](_modules/scrapy/core/scheduler.html#Scheduler.next_request){.reference .internal}[¶](#scrapy.core.scheduler.Scheduler.next_request &quot;Permalink to this definition&quot;){.headerlink}

:   Return a [[`Request`{.xref .py .py-class .docutils .literal
    .notranslate}]{.pre}](index.html#scrapy.http.Request &quot;scrapy.http.Request&quot;){.reference
    .internal} object from the memory queue, falling back to the
    disk queue if the memory queue is empty. Return
    [`None`{.docutils .literal .notranslate}]{.pre} if there are no
    more enqueued requests.

    Increment the appropriate stats, such as:
    [`scheduler/dequeued`{.docutils .literal .notranslate}]{.pre},
    [`scheduler/dequeued/disk`{.docutils .literal
    .notranslate}]{.pre}, [`scheduler/dequeued/memory`{.docutils
    .literal .notranslate}]{.pre}.

[[open]{.pre}]{.sig-name .descname}[(]{.sig-paren}*[[spider]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[[[Spider]{.pre}](index.html#scrapy.spiders.Spider &quot;scrapy.spiders.Spider&quot;){.reference .internal}]{.n}*[)]{.sig-paren} [[→]{.sig-return-icon} [[[Optional]{.pre}](https://docs.python.org/3/library/typing.html#typing.Optional &quot;(in Python v3.12)&quot;){.reference .external}[[\[]{.pre}]{.p}[[Deferred]{.pre}](https://docs.twisted.org/en/stable/api/twisted.internet.defer.Deferred.html &quot;(in Twisted)&quot;){.reference .external}[[\]]{.pre}]{.p}]{.sig-return-typehint}]{.sig-return}[[[\[source\]]{.pre}]{.viewcode-link}](_modules/scrapy/core/scheduler.html#Scheduler.open){.reference .internal}[¶](#scrapy.core.scheduler.Scheduler.open &quot;Permalink to this definition&quot;){.headerlink}

:   1.  initialize the memory queue

    2.  initialize the disk queue if the [`jobdir`{.docutils
        .literal .notranslate}]{.pre} attribute is a valid directory

    3.  return the result of the dupefilter's [`open`{.docutils
        .literal .notranslate}]{.pre} method
</code></pre>
<p>:::
:::</p>
<p>[]{#document-topics/exporters}</p>
<p>::: {#module-scrapy.exporters .section}
[]{#item-exporters}[]{#topics-exporters}</p>
<h3 id="item-exportersheaderlink"><a class="header" href="#item-exportersheaderlink">Item Exporters<a href="#module-scrapy.exporters" title="Permalink to this heading">¶</a>{.headerlink}</a></h3>
<p>Once you have scraped your items, you often want to persist or export
those items, to use the data in some other application. That is, after
all, the whole purpose of the scraping process.</p>
<p>For this purpose Scrapy provides a collection of Item Exporters for
different output formats, such as XML, CSV or JSON.</p>
<p>::: {#using-item-exporters .section}</p>
<h4 id="using-item-exportersheaderlink"><a class="header" href="#using-item-exportersheaderlink">Using Item Exporters<a href="#using-item-exporters" title="Permalink to this heading">¶</a>{.headerlink}</a></h4>
<p>If you are in a hurry, and just want to use an Item Exporter to output
scraped data see the <a href="index.html#topics-feed-exports">[Feed exports]{.std
.std-ref}</a>{.hoverxref .tooltip
.reference .internal}. Otherwise, if you want to know how Item Exporters
work or need more custom functionality (not covered by the default
exports), continue reading below.</p>
<p>In order to use an Item Exporter, you must instantiate it with its
required args. Each Item Exporter requires different arguments, so check
each exporter documentation to be sure, in <a href="#topics-exporters-reference">[Built-in Item Exporters
reference]{.std .std-ref}</a>{.hoverxref
.tooltip .reference .internal}. After you have instantiated your
exporter, you have to:</p>
<p>1. call the method <a href="#scrapy.exporters.BaseItemExporter.start_exporting" title="scrapy.exporters.BaseItemExporter.start_exporting">[<code>start_exporting()</code>{.xref .py .py-meth .docutils
.literal
.notranslate}]{.pre}</a>{.reference
.internal} in order to signal the beginning of the exporting process</p>
<p>2. call the <a href="#scrapy.exporters.BaseItemExporter.export_item" title="scrapy.exporters.BaseItemExporter.export_item">[<code>export_item()</code>{.xref .py .py-meth .docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal} method for each item you want to export</p>
<p>3. and finally call the <a href="#scrapy.exporters.BaseItemExporter.finish_exporting" title="scrapy.exporters.BaseItemExporter.finish_exporting">[<code>finish_exporting()</code>{.xref .py .py-meth
.docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal} to signal the end of the exporting process</p>
<p>Here you can see an <a href="index.html#document-topics/item-pipeline">[Item
Pipeline]{.doc}</a>{.reference
.internal} which uses multiple Item Exporters to group scraped items to
different files according to the value of one of their fields:</p>
<p>::: {.highlight-python .notranslate}
::: highlight
from itemadapter import ItemAdapter
from scrapy.exporters import XmlItemExporter</p>
<pre><code>class PerYearXmlExportPipeline:
    &quot;&quot;&quot;Distribute items across multiple XML files according to their 'year' field&quot;&quot;&quot;

    def open_spider(self, spider):
        self.year_to_exporter = {}

    def close_spider(self, spider):
        for exporter, xml_file in self.year_to_exporter.values():
            exporter.finish_exporting()
            xml_file.close()

    def _exporter_for_item(self, item):
        adapter = ItemAdapter(item)
        year = adapter[&quot;year&quot;]
        if year not in self.year_to_exporter:
            xml_file = open(f&quot;{year}.xml&quot;, &quot;wb&quot;)
            exporter = XmlItemExporter(xml_file)
            exporter.start_exporting()
            self.year_to_exporter[year] = (exporter, xml_file)
        return self.year_to_exporter[year][0]

    def process_item(self, item, spider):
        exporter = self._exporter_for_item(item)
        exporter.export_item(item)
        return item
</code></pre>
<p>:::
:::
:::</p>
<p>::: {#serialization-of-item-fields .section}
[]{#topics-exporters-field-serialization}</p>
<h4 id="serialization-of-item-fieldsheaderlink"><a class="header" href="#serialization-of-item-fieldsheaderlink">Serialization of item fields<a href="#serialization-of-item-fields" title="Permalink to this heading">¶</a>{.headerlink}</a></h4>
<p>By default, the field values are passed unmodified to the underlying
serialization library, and the decision of how to serialize them is
delegated to each particular serialization library.</p>
<p>However, you can customize how each field value is serialized <em>before it
is passed to the serialization library</em>.</p>
<p>There are two ways to customize how a field will be serialized, which
are described next.</p>
<p>::: {#declaring-a-serializer-in-the-field .section}
[]{#topics-exporters-serializers}</p>
<h5 id="1-declaring-a-serializer-in-the-fieldheaderlink"><a class="header" href="#1-declaring-a-serializer-in-the-fieldheaderlink">1. Declaring a serializer in the field<a href="#declaring-a-serializer-in-the-field" title="Permalink to this heading">¶</a>{.headerlink}</a></h5>
<p>If you use [<code>Item</code>{.xref .py .py-class .docutils .literal
.notranslate}]{.pre} you can declare a serializer in the <a href="index.html#topics-items-fields">[field
metadata]{.std .std-ref}</a>{.hoverxref
.tooltip .reference .internal}. The serializer must be a callable which
receives a value and returns its serialized form.</p>
<p>Example:</p>
<p>::: {.highlight-python .notranslate}
::: highlight
import scrapy</p>
<pre><code>def serialize_price(value):
    return f&quot;$ {str(value)}&quot;


class Product(scrapy.Item):
    name = scrapy.Field()
    price = scrapy.Field(serializer=serialize_price)
</code></pre>
<p>:::
:::
:::</p>
<p>::: {#overriding-the-serialize-field-method .section}</p>
<h5 id="2-overriding-the-serialize_field-methodheaderlink"><a class="header" href="#2-overriding-the-serialize_field-methodheaderlink">2. Overriding the serialize_field() method<a href="#overriding-the-serialize-field-method" title="Permalink to this heading">¶</a>{.headerlink}</a></h5>
<p>You can also override the <a href="#scrapy.exporters.BaseItemExporter.serialize_field" title="scrapy.exporters.BaseItemExporter.serialize_field">[<code>serialize_field()</code>{.xref .py .py-meth
.docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal} method to customize how your field value will be exported.</p>
<p>Make sure you call the base class <a href="#scrapy.exporters.BaseItemExporter.serialize_field" title="scrapy.exporters.BaseItemExporter.serialize_field">[<code>serialize_field()</code>{.xref .py
.py-meth .docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal} method after your custom code.</p>
<p>Example:</p>
<p>::: {.highlight-python .notranslate}
::: highlight
from scrapy.exporters import XmlItemExporter</p>
<pre><code>class ProductXmlExporter(XmlItemExporter):
    def serialize_field(self, field, name, value):
        if name == &quot;price&quot;:
            return f&quot;$ {str(value)}&quot;
        return super().serialize_field(field, name, value)
</code></pre>
<p>:::
:::
:::
:::</p>
<p>::: {#built-in-item-exporters-reference .section}
[]{#topics-exporters-reference}</p>
<h4 id="built-in-item-exporters-referenceheaderlink"><a class="header" href="#built-in-item-exporters-referenceheaderlink">Built-in Item Exporters reference<a href="#built-in-item-exporters-reference" title="Permalink to this heading">¶</a>{.headerlink}</a></h4>
<p>Here is a list of the Item Exporters bundled with Scrapy. Some of them
contain output examples, which assume you're exporting these two items:</p>
<p>::: {.highlight-python .notranslate}
::: highlight
Item(name=&quot;Color TV&quot;, price=&quot;1200&quot;)
Item(name=&quot;DVD player&quot;, price=&quot;200&quot;)
:::
:::</p>
<p>::: {#baseitemexporter .section}</p>
<h5 id="baseitemexporterheaderlink"><a class="header" href="#baseitemexporterheaderlink">BaseItemExporter<a href="#baseitemexporter" title="Permalink to this heading">¶</a>{.headerlink}</a></h5>
<p><em>[class]{.pre}[ ]{.w}</em>[[scrapy.exporters.]{.pre}]{.sig-prename .descclassname}[[BaseItemExporter]{.pre}]{.sig-name .descname}[(]{.sig-paren}<em>[[fields_to_export]{.pre}]{.n}[[=]{.pre}]{.o}[[None]{.pre}]{.default_value}</em>, <em>[[export_empty_fields]{.pre}]{.n}[[=]{.pre}]{.o}[[False]{.pre}]{.default_value}</em>, <em>[[encoding]{.pre}]{.n}[[=]{.pre}]{.o}[['utf-8']{.pre}]{.default_value}</em>, <em>[[indent]{.pre}]{.n}[[=]{.pre}]{.o}[[0]{.pre}]{.default_value}</em>, <em>[[dont_fail]{.pre}]{.n}[[=]{.pre}]{.o}[[False]{.pre}]{.default_value}</em>[)]{.sig-paren}<a href="_modules/scrapy/exporters.html#BaseItemExporter">[[[source]]{.pre}]{.viewcode-link}</a>{.reference .internal}<a href="#scrapy.exporters.BaseItemExporter" title="Permalink to this definition">¶</a>{.headerlink}</p>
<p>:   This is the (abstract) base class for all Item Exporters. It
provides support for common features used by all (concrete) Item
Exporters, such as defining what fields to export, whether to export
empty fields, or which encoding to use.</p>
<pre><code>These features can be configured through the [`__init__`{.docutils
.literal .notranslate}]{.pre} method arguments which populate their
respective instance attributes: [[`fields_to_export`{.xref .py
.py-attr .docutils .literal
.notranslate}]{.pre}](#scrapy.exporters.BaseItemExporter.fields_to_export &quot;scrapy.exporters.BaseItemExporter.fields_to_export&quot;){.reference
.internal}, [[`export_empty_fields`{.xref .py .py-attr .docutils
.literal
.notranslate}]{.pre}](#scrapy.exporters.BaseItemExporter.export_empty_fields &quot;scrapy.exporters.BaseItemExporter.export_empty_fields&quot;){.reference
.internal}, [[`encoding`{.xref .py .py-attr .docutils .literal
.notranslate}]{.pre}](#scrapy.exporters.BaseItemExporter.encoding &quot;scrapy.exporters.BaseItemExporter.encoding&quot;){.reference
.internal}, [[`indent`{.xref .py .py-attr .docutils .literal
.notranslate}]{.pre}](#scrapy.exporters.BaseItemExporter.indent &quot;scrapy.exporters.BaseItemExporter.indent&quot;){.reference
.internal}.

::: versionadded
[New in version 2.0: ]{.versionmodified .added}The *dont_fail*
parameter.
:::

[[export_item]{.pre}]{.sig-name .descname}[(]{.sig-paren}*[[item]{.pre}]{.n}*[)]{.sig-paren}[[[\[source\]]{.pre}]{.viewcode-link}](_modules/scrapy/exporters.html#BaseItemExporter.export_item){.reference .internal}[¶](#scrapy.exporters.BaseItemExporter.export_item &quot;Permalink to this definition&quot;){.headerlink}

:   Exports the given item. This method must be implemented in
    subclasses.

[[serialize_field]{.pre}]{.sig-name .descname}[(]{.sig-paren}*[[field]{.pre}]{.n}*, *[[name]{.pre}]{.n}*, *[[value]{.pre}]{.n}*[)]{.sig-paren}[[[\[source\]]{.pre}]{.viewcode-link}](_modules/scrapy/exporters.html#BaseItemExporter.serialize_field){.reference .internal}[¶](#scrapy.exporters.BaseItemExporter.serialize_field &quot;Permalink to this definition&quot;){.headerlink}

:   Return the serialized value for the given field. You can
    override this method (in your custom Item Exporters) if you want
    to control how a particular field or value will be
    serialized/exported.

    By default, this method looks for a serializer [[declared in the
    item field]{.std
    .std-ref}](#topics-exporters-serializers){.hoverxref .tooltip
    .reference .internal} and returns the result of applying that
    serializer to the value. If no serializer is found, it returns
    the value unchanged.

    Parameters

    :   -   **field** ([`Field`{.xref .py .py-class .docutils
            .literal .notranslate}]{.pre} object or a [[`dict`{.xref
            .py .py-class .docutils .literal
            .notranslate}]{.pre}](https://docs.python.org/3/library/stdtypes.html#dict &quot;(in Python v3.12)&quot;){.reference
            .external} instance) -- the field being serialized. If
            the source [[item object]{.std
            .std-ref}](index.html#item-types){.hoverxref .tooltip
            .reference .internal} does not define field metadata,
            *field* is an empty [[`dict`{.xref .py .py-class
            .docutils .literal
            .notranslate}]{.pre}](https://docs.python.org/3/library/stdtypes.html#dict &quot;(in Python v3.12)&quot;){.reference
            .external}.

        -   **name**
            ([*str*](https://docs.python.org/3/library/stdtypes.html#str &quot;(in Python v3.12)&quot;){.reference
            .external}) -- the name of the field being serialized

        -   **value** -- the value being serialized

[[start_exporting]{.pre}]{.sig-name .descname}[(]{.sig-paren}[)]{.sig-paren}[[[\[source\]]{.pre}]{.viewcode-link}](_modules/scrapy/exporters.html#BaseItemExporter.start_exporting){.reference .internal}[¶](#scrapy.exporters.BaseItemExporter.start_exporting &quot;Permalink to this definition&quot;){.headerlink}

:   Signal the beginning of the exporting process. Some exporters
    may use this to generate some required header (for example, the
    [[`XmlItemExporter`{.xref .py .py-class .docutils .literal
    .notranslate}]{.pre}](#scrapy.exporters.XmlItemExporter &quot;scrapy.exporters.XmlItemExporter&quot;){.reference
    .internal}). You must call this method before exporting any
    items.

[[finish_exporting]{.pre}]{.sig-name .descname}[(]{.sig-paren}[)]{.sig-paren}[[[\[source\]]{.pre}]{.viewcode-link}](_modules/scrapy/exporters.html#BaseItemExporter.finish_exporting){.reference .internal}[¶](#scrapy.exporters.BaseItemExporter.finish_exporting &quot;Permalink to this definition&quot;){.headerlink}

:   Signal the end of the exporting process. Some exporters may use
    this to generate some required footer (for example, the
    [[`XmlItemExporter`{.xref .py .py-class .docutils .literal
    .notranslate}]{.pre}](#scrapy.exporters.XmlItemExporter &quot;scrapy.exporters.XmlItemExporter&quot;){.reference
    .internal}). You must always call this method after you have no
    more items to export.

[[fields_to_export]{.pre}]{.sig-name .descname}[¶](#scrapy.exporters.BaseItemExporter.fields_to_export &quot;Permalink to this definition&quot;){.headerlink}

:   Fields to export, their order [1](#id3){#id1 .footnote-reference
    .brackets} and their output names.

    Possible values are:

    -   [`None`{.docutils .literal .notranslate}]{.pre} (all fields
        [2](#id4){#id2 .footnote-reference .brackets}, default)

    -   A list of fields:

        ::: {.highlight-default .notranslate}
        ::: highlight
            ['field1', 'field2']
        :::
        :::

    -   A dict where keys are fields and values are output names:

        ::: {.highlight-default .notranslate}
        ::: highlight
            {'field1': 'Field 1', 'field2': 'Field 2'}
        :::
        :::

    [[1](#id1){.fn-backref}]{.brackets}

    :   Not all exporters respect the specified field order.

    [[2](#id2){.fn-backref}]{.brackets}

    :   When using [[item objects]{.std
        .std-ref}](index.html#item-types){.hoverxref .tooltip
        .reference .internal} that do not expose all their possible
        fields, exporters that do not support exporting a different
        subset of fields per item will only export the fields found
        in the first item exported.

[[export_empty_fields]{.pre}]{.sig-name .descname}[¶](#scrapy.exporters.BaseItemExporter.export_empty_fields &quot;Permalink to this definition&quot;){.headerlink}

:   Whether to include empty/unpopulated item fields in the exported
    data. Defaults to [`False`{.docutils .literal
    .notranslate}]{.pre}. Some exporters (like
    [[`CsvItemExporter`{.xref .py .py-class .docutils .literal
    .notranslate}]{.pre}](#scrapy.exporters.CsvItemExporter &quot;scrapy.exporters.CsvItemExporter&quot;){.reference
    .internal}) ignore this attribute and always export all empty
    fields.

    This option is ignored for dict items.

[[encoding]{.pre}]{.sig-name .descname}[¶](#scrapy.exporters.BaseItemExporter.encoding &quot;Permalink to this definition&quot;){.headerlink}

:   The output character encoding.

[[indent]{.pre}]{.sig-name .descname}[¶](#scrapy.exporters.BaseItemExporter.indent &quot;Permalink to this definition&quot;){.headerlink}

:   Amount of spaces used to indent the output on each level.
    Defaults to [`0`{.docutils .literal .notranslate}]{.pre}.

    -   [`indent=None`{.docutils .literal .notranslate}]{.pre}
        selects the most compact representation, all items in the
        same line with no indentation

    -   [`indent&lt;=0`{.docutils .literal .notranslate}]{.pre} each
        item on its own line, no indentation

    -   [`indent&gt;0`{.docutils .literal .notranslate}]{.pre} each
        item on its own line, indented with the provided numeric
        value
</code></pre>
<p>:::</p>
<p>::: {#pythonitemexporter .section}</p>
<h5 id="pythonitemexporterheaderlink"><a class="header" href="#pythonitemexporterheaderlink">PythonItemExporter<a href="#pythonitemexporter" title="Permalink to this heading">¶</a>{.headerlink}</a></h5>
<p><em>[class]{.pre}[ ]{.w}</em>[[scrapy.exporters.]{.pre}]{.sig-prename .descclassname}[[PythonItemExporter]{.pre}]{.sig-name .descname}[(]{.sig-paren}<em>[[*]{.pre}]{.o}</em>, <em>[[dont_fail]{.pre}]{.n}[[=]{.pre}]{.o}[[False]{.pre}]{.default_value}</em>, <em>[[**]{.pre}]{.o}[[kwargs]{.pre}]{.n}</em>[)]{.sig-paren}<a href="_modules/scrapy/exporters.html#PythonItemExporter">[[[source]]{.pre}]{.viewcode-link}</a>{.reference .internal}<a href="#scrapy.exporters.PythonItemExporter" title="Permalink to this definition">¶</a>{.headerlink}</p>
<p>:   This is a base class for item exporters that extends
<a href="#scrapy.exporters.BaseItemExporter" title="scrapy.exporters.BaseItemExporter">[<code>BaseItemExporter</code>{.xref .py .py-class .docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal} with support for nested items.</p>
<pre><code>It serializes items to built-in Python types, so that any
serialization library (e.g. [[`json`{.xref .py .py-mod .docutils
.literal
.notranslate}]{.pre}](https://docs.python.org/3/library/json.html#module-json &quot;(in Python v3.12)&quot;){.reference
.external} or
[msgpack](https://pypi.org/project/msgpack/){.reference .external})
can be used on top of it.
</code></pre>
<p>:::</p>
<p>::: {#xmlitemexporter .section}</p>
<h5 id="xmlitemexporterheaderlink"><a class="header" href="#xmlitemexporterheaderlink">XmlItemExporter<a href="#xmlitemexporter" title="Permalink to this heading">¶</a>{.headerlink}</a></h5>
<p><em>[class]{.pre}[ ]{.w}</em>[[scrapy.exporters.]{.pre}]{.sig-prename .descclassname}[[XmlItemExporter]{.pre}]{.sig-name .descname}[(]{.sig-paren}<em>[[file]{.pre}]{.n}</em>, <em>[[item_element]{.pre}]{.n}[[=]{.pre}]{.o}[['item']{.pre}]{.default_value}</em>, <em>[[root_element]{.pre}]{.n}[[=]{.pre}]{.o}[['items']{.pre}]{.default_value}</em>, <em>[[**]{.pre}]{.o}[[kwargs]{.pre}]{.n}</em>[)]{.sig-paren}<a href="_modules/scrapy/exporters.html#XmlItemExporter">[[[source]]{.pre}]{.viewcode-link}</a>{.reference .internal}<a href="#scrapy.exporters.XmlItemExporter" title="Permalink to this definition">¶</a>{.headerlink}</p>
<p>:   Exports items in XML format to the specified file object.</p>
<pre><code>Parameters

:   -   **file** -- the file-like object to use for exporting the
        data. Its [`write`{.docutils .literal .notranslate}]{.pre}
        method should accept [`bytes`{.docutils .literal
        .notranslate}]{.pre} (a disk file opened in binary mode, a
        [`io.BytesIO`{.docutils .literal .notranslate}]{.pre}
        object, etc)

    -   **root_element**
        ([*str*](https://docs.python.org/3/library/stdtypes.html#str &quot;(in Python v3.12)&quot;){.reference
        .external}) -- The name of root element in the exported XML.

    -   **item_element**
        ([*str*](https://docs.python.org/3/library/stdtypes.html#str &quot;(in Python v3.12)&quot;){.reference
        .external}) -- The name of each item element in the exported
        XML.

The additional keyword arguments of this [`__init__`{.docutils
.literal .notranslate}]{.pre} method are passed to the
[[`BaseItemExporter`{.xref .py .py-class .docutils .literal
.notranslate}]{.pre}](#scrapy.exporters.BaseItemExporter &quot;scrapy.exporters.BaseItemExporter&quot;){.reference
.internal} [`__init__`{.docutils .literal .notranslate}]{.pre}
method.

A typical output of this exporter would be:

::: {.highlight-none .notranslate}
::: highlight
    &lt;?xml version=&quot;1.0&quot; encoding=&quot;utf-8&quot;?&gt;
    &lt;items&gt;
      &lt;item&gt;
        &lt;name&gt;Color TV&lt;/name&gt;
        &lt;price&gt;1200&lt;/price&gt;
     &lt;/item&gt;
      &lt;item&gt;
        &lt;name&gt;DVD player&lt;/name&gt;
        &lt;price&gt;200&lt;/price&gt;
     &lt;/item&gt;
    &lt;/items&gt;
:::
:::

Unless overridden in the [`serialize_field()`{.xref .py .py-meth
.docutils .literal .notranslate}]{.pre} method, multi-valued fields
are exported by serializing each value inside a [`&lt;value&gt;`{.docutils
.literal .notranslate}]{.pre} element. This is for convenience, as
multi-valued fields are very common.

For example, the item:

::: {.highlight-none .notranslate}
::: highlight
    Item(name=['John', 'Doe'], age='23')
:::
:::

Would be serialized as:

::: {.highlight-none .notranslate}
::: highlight
    &lt;?xml version=&quot;1.0&quot; encoding=&quot;utf-8&quot;?&gt;
    &lt;items&gt;
      &lt;item&gt;
        &lt;name&gt;
          &lt;value&gt;John&lt;/value&gt;
          &lt;value&gt;Doe&lt;/value&gt;
        &lt;/name&gt;
        &lt;age&gt;23&lt;/age&gt;
      &lt;/item&gt;
    &lt;/items&gt;
:::
:::
</code></pre>
<p>:::</p>
<p>::: {#csvitemexporter .section}</p>
<h5 id="csvitemexporterheaderlink"><a class="header" href="#csvitemexporterheaderlink">CsvItemExporter<a href="#csvitemexporter" title="Permalink to this heading">¶</a>{.headerlink}</a></h5>
<p><em>[class]{.pre}[ ]{.w}</em>[[scrapy.exporters.]{.pre}]{.sig-prename .descclassname}[[CsvItemExporter]{.pre}]{.sig-name .descname}[(]{.sig-paren}<em>[[file]{.pre}]{.n}</em>, <em>[[include_headers_line]{.pre}]{.n}[[=]{.pre}]{.o}[[True]{.pre}]{.default_value}</em>, <em>[[join_multivalued]{.pre}]{.n}[[=]{.pre}]{.o}[[',']{.pre}]{.default_value}</em>, <em>[[errors]{.pre}]{.n}[[=]{.pre}]{.o}[[None]{.pre}]{.default_value}</em>, <em>[[**]{.pre}]{.o}[[kwargs]{.pre}]{.n}</em>[)]{.sig-paren}<a href="_modules/scrapy/exporters.html#CsvItemExporter">[[[source]]{.pre}]{.viewcode-link}</a>{.reference .internal}<a href="#scrapy.exporters.CsvItemExporter" title="Permalink to this definition">¶</a>{.headerlink}</p>
<p>:   Exports items in CSV format to the given file-like object. If the
[<code>fields_to_export</code>{.xref .py .py-attr .docutils .literal
.notranslate}]{.pre} attribute is set, it will be used to define the
CSV columns, their order and their column names. The
[<code>export_empty_fields</code>{.xref .py .py-attr .docutils .literal
.notranslate}]{.pre} attribute has no effect on this exporter.</p>
<pre><code>Parameters

:   -   **file** -- the file-like object to use for exporting the
        data. Its [`write`{.docutils .literal .notranslate}]{.pre}
        method should accept [`bytes`{.docutils .literal
        .notranslate}]{.pre} (a disk file opened in binary mode, a
        [`io.BytesIO`{.docutils .literal .notranslate}]{.pre}
        object, etc)

    -   **include_headers_line**
        ([*str*](https://docs.python.org/3/library/stdtypes.html#str &quot;(in Python v3.12)&quot;){.reference
        .external}) -- If enabled, makes the exporter output a
        header line with the field names taken from
        [[`BaseItemExporter.fields_to_export`{.xref .py .py-attr
        .docutils .literal
        .notranslate}]{.pre}](#scrapy.exporters.BaseItemExporter.fields_to_export &quot;scrapy.exporters.BaseItemExporter.fields_to_export&quot;){.reference
        .internal} or the first exported item fields.

    -   **join_multivalued** -- The char (or chars) that will be
        used for joining multi-valued fields, if found.

    -   **errors**
        ([*str*](https://docs.python.org/3/library/stdtypes.html#str &quot;(in Python v3.12)&quot;){.reference
        .external}) -- The optional string that specifies how
        encoding and decoding errors are to be handled. For more
        information see [[`io.TextIOWrapper`{.xref .py .py-class
        .docutils .literal
        .notranslate}]{.pre}](https://docs.python.org/3/library/io.html#io.TextIOWrapper &quot;(in Python v3.12)&quot;){.reference
        .external}.

The additional keyword arguments of this [`__init__`{.docutils
.literal .notranslate}]{.pre} method are passed to the
[[`BaseItemExporter`{.xref .py .py-class .docutils .literal
.notranslate}]{.pre}](#scrapy.exporters.BaseItemExporter &quot;scrapy.exporters.BaseItemExporter&quot;){.reference
.internal} [`__init__`{.docutils .literal .notranslate}]{.pre}
method, and the leftover arguments to the [[`csv.writer()`{.xref .py
.py-func .docutils .literal
.notranslate}]{.pre}](https://docs.python.org/3/library/csv.html#csv.writer &quot;(in Python v3.12)&quot;){.reference
.external} function, so you can use any [[`csv.writer()`{.xref .py
.py-func .docutils .literal
.notranslate}]{.pre}](https://docs.python.org/3/library/csv.html#csv.writer &quot;(in Python v3.12)&quot;){.reference
.external} function argument to customize this exporter.

A typical output of this exporter would be:

::: {.highlight-none .notranslate}
::: highlight
    product,price
    Color TV,1200
    DVD player,200
:::
:::
</code></pre>
<p>:::</p>
<p>::: {#pickleitemexporter .section}</p>
<h5 id="pickleitemexporterheaderlink"><a class="header" href="#pickleitemexporterheaderlink">PickleItemExporter<a href="#pickleitemexporter" title="Permalink to this heading">¶</a>{.headerlink}</a></h5>
<p><em>[class]{.pre}[ ]{.w}</em>[[scrapy.exporters.]{.pre}]{.sig-prename .descclassname}[[PickleItemExporter]{.pre}]{.sig-name .descname}[(]{.sig-paren}<em>[[file]{.pre}]{.n}</em>, <em>[[protocol]{.pre}]{.n}[[=]{.pre}]{.o}[[0]{.pre}]{.default_value}</em>, <em>[[**]{.pre}]{.o}[[kwargs]{.pre}]{.n}</em>[)]{.sig-paren}<a href="_modules/scrapy/exporters.html#PickleItemExporter">[[[source]]{.pre}]{.viewcode-link}</a>{.reference .internal}<a href="#scrapy.exporters.PickleItemExporter" title="Permalink to this definition">¶</a>{.headerlink}</p>
<p>:   Exports items in pickle format to the given file-like object.</p>
<pre><code>Parameters

:   -   **file** -- the file-like object to use for exporting the
        data. Its [`write`{.docutils .literal .notranslate}]{.pre}
        method should accept [`bytes`{.docutils .literal
        .notranslate}]{.pre} (a disk file opened in binary mode, a
        [`io.BytesIO`{.docutils .literal .notranslate}]{.pre}
        object, etc)

    -   **protocol**
        ([*int*](https://docs.python.org/3/library/functions.html#int &quot;(in Python v3.12)&quot;){.reference
        .external}) -- The pickle protocol to use.

For more information, see [[`pickle`{.xref .py .py-mod .docutils
.literal
.notranslate}]{.pre}](https://docs.python.org/3/library/pickle.html#module-pickle &quot;(in Python v3.12)&quot;){.reference
.external}.

The additional keyword arguments of this [`__init__`{.docutils
.literal .notranslate}]{.pre} method are passed to the
[[`BaseItemExporter`{.xref .py .py-class .docutils .literal
.notranslate}]{.pre}](#scrapy.exporters.BaseItemExporter &quot;scrapy.exporters.BaseItemExporter&quot;){.reference
.internal} [`__init__`{.docutils .literal .notranslate}]{.pre}
method.

Pickle isn't a human readable format, so no output examples are
provided.
</code></pre>
<p>:::</p>
<p>::: {#pprintitemexporter .section}</p>
<h5 id="pprintitemexporterheaderlink"><a class="header" href="#pprintitemexporterheaderlink">PprintItemExporter<a href="#pprintitemexporter" title="Permalink to this heading">¶</a>{.headerlink}</a></h5>
<p><em>[class]{.pre}[ ]{.w}</em>[[scrapy.exporters.]{.pre}]{.sig-prename .descclassname}[[PprintItemExporter]{.pre}]{.sig-name .descname}[(]{.sig-paren}<em>[[file]{.pre}]{.n}</em>, <em>[[**]{.pre}]{.o}[[kwargs]{.pre}]{.n}</em>[)]{.sig-paren}<a href="_modules/scrapy/exporters.html#PprintItemExporter">[[[source]]{.pre}]{.viewcode-link}</a>{.reference .internal}<a href="#scrapy.exporters.PprintItemExporter" title="Permalink to this definition">¶</a>{.headerlink}</p>
<p>:   Exports items in pretty print format to the specified file object.</p>
<pre><code>Parameters

:   **file** -- the file-like object to use for exporting the data.
    Its [`write`{.docutils .literal .notranslate}]{.pre} method
    should accept [`bytes`{.docutils .literal .notranslate}]{.pre}
    (a disk file opened in binary mode, a [`io.BytesIO`{.docutils
    .literal .notranslate}]{.pre} object, etc)

The additional keyword arguments of this [`__init__`{.docutils
.literal .notranslate}]{.pre} method are passed to the
[[`BaseItemExporter`{.xref .py .py-class .docutils .literal
.notranslate}]{.pre}](#scrapy.exporters.BaseItemExporter &quot;scrapy.exporters.BaseItemExporter&quot;){.reference
.internal} [`__init__`{.docutils .literal .notranslate}]{.pre}
method.

A typical output of this exporter would be:

::: {.highlight-none .notranslate}
::: highlight
    {'name': 'Color TV', 'price': '1200'}
    {'name': 'DVD player', 'price': '200'}
:::
:::

Longer lines (when present) are pretty-formatted.
</code></pre>
<p>:::</p>
<p>::: {#jsonitemexporter .section}</p>
<h5 id="jsonitemexporterheaderlink"><a class="header" href="#jsonitemexporterheaderlink">JsonItemExporter<a href="#jsonitemexporter" title="Permalink to this heading">¶</a>{.headerlink}</a></h5>
<p><em>[class]{.pre}[ ]{.w}</em>[[scrapy.exporters.]{.pre}]{.sig-prename .descclassname}[[JsonItemExporter]{.pre}]{.sig-name .descname}[(]{.sig-paren}<em>[[file]{.pre}]{.n}</em>, <em>[[**]{.pre}]{.o}[[kwargs]{.pre}]{.n}</em>[)]{.sig-paren}<a href="_modules/scrapy/exporters.html#JsonItemExporter">[[[source]]{.pre}]{.viewcode-link}</a>{.reference .internal}<a href="#scrapy.exporters.JsonItemExporter" title="Permalink to this definition">¶</a>{.headerlink}</p>
<p>:   Exports items in JSON format to the specified file-like object,
writing all objects as a list of objects. The additional
[<code>__init__</code>{.docutils .literal .notranslate}]{.pre} method arguments
are passed to the <a href="#scrapy.exporters.BaseItemExporter" title="scrapy.exporters.BaseItemExporter">[<code>BaseItemExporter</code>{.xref .py .py-class .docutils
.literal
.notranslate}]{.pre}</a>{.reference
.internal} [<code>__init__</code>{.docutils .literal .notranslate}]{.pre}
method, and the leftover arguments to the <a href="https://docs.python.org/3/library/json.html#json.JSONEncoder" title="(in Python v3.12)">[<code>JSONEncoder</code>{.xref .py
.py-class .docutils .literal
.notranslate}]{.pre}</a>{.reference
.external} [<code>__init__</code>{.docutils .literal .notranslate}]{.pre}
method, so you can use any <a href="https://docs.python.org/3/library/json.html#json.JSONEncoder" title="(in Python v3.12)">[<code>JSONEncoder</code>{.xref .py .py-class
.docutils .literal
.notranslate}]{.pre}</a>{.reference
.external} [<code>__init__</code>{.docutils .literal .notranslate}]{.pre}
method argument to customize this exporter.</p>
<pre><code>Parameters

:   **file** -- the file-like object to use for exporting the data.
    Its [`write`{.docutils .literal .notranslate}]{.pre} method
    should accept [`bytes`{.docutils .literal .notranslate}]{.pre}
    (a disk file opened in binary mode, a [`io.BytesIO`{.docutils
    .literal .notranslate}]{.pre} object, etc)

A typical output of this exporter would be:

::: {.highlight-none .notranslate}
::: highlight
    [{&quot;name&quot;: &quot;Color TV&quot;, &quot;price&quot;: &quot;1200&quot;},
    {&quot;name&quot;: &quot;DVD player&quot;, &quot;price&quot;: &quot;200&quot;}]
:::
:::

::: {#json-with-large-data .admonition .warning}
Warning

JSON is very simple and flexible serialization format, but it
doesn't scale well for large amounts of data since incremental (aka.
stream-mode) parsing is not well supported (if at all) among JSON
parsers (on any language), and most of them just parse the entire
object in memory. If you want the power and simplicity of JSON with
a more stream-friendly format, consider using
[[`JsonLinesItemExporter`{.xref .py .py-class .docutils .literal
.notranslate}]{.pre}](#scrapy.exporters.JsonLinesItemExporter &quot;scrapy.exporters.JsonLinesItemExporter&quot;){.reference
.internal} instead, or splitting the output in multiple chunks.
:::
</code></pre>
<p>:::</p>
<p>::: {#jsonlinesitemexporter .section}</p>
<h5 id="jsonlinesitemexporterheaderlink"><a class="header" href="#jsonlinesitemexporterheaderlink">JsonLinesItemExporter<a href="#jsonlinesitemexporter" title="Permalink to this heading">¶</a>{.headerlink}</a></h5>
<p><em>[class]{.pre}[ ]{.w}</em>[[scrapy.exporters.]{.pre}]{.sig-prename .descclassname}[[JsonLinesItemExporter]{.pre}]{.sig-name .descname}[(]{.sig-paren}<em>[[file]{.pre}]{.n}</em>, <em>[[**]{.pre}]{.o}[[kwargs]{.pre}]{.n}</em>[)]{.sig-paren}<a href="_modules/scrapy/exporters.html#JsonLinesItemExporter">[[[source]]{.pre}]{.viewcode-link}</a>{.reference .internal}<a href="#scrapy.exporters.JsonLinesItemExporter" title="Permalink to this definition">¶</a>{.headerlink}</p>
<p>:   Exports items in JSON format to the specified file-like object,
writing one JSON-encoded item per line. The additional
[<code>__init__</code>{.docutils .literal .notranslate}]{.pre} method arguments
are passed to the <a href="#scrapy.exporters.BaseItemExporter" title="scrapy.exporters.BaseItemExporter">[<code>BaseItemExporter</code>{.xref .py .py-class .docutils
.literal
.notranslate}]{.pre}</a>{.reference
.internal} [<code>__init__</code>{.docutils .literal .notranslate}]{.pre}
method, and the leftover arguments to the <a href="https://docs.python.org/3/library/json.html#json.JSONEncoder" title="(in Python v3.12)">[<code>JSONEncoder</code>{.xref .py
.py-class .docutils .literal
.notranslate}]{.pre}</a>{.reference
.external} [<code>__init__</code>{.docutils .literal .notranslate}]{.pre}
method, so you can use any <a href="https://docs.python.org/3/library/json.html#json.JSONEncoder" title="(in Python v3.12)">[<code>JSONEncoder</code>{.xref .py .py-class
.docutils .literal
.notranslate}]{.pre}</a>{.reference
.external} [<code>__init__</code>{.docutils .literal .notranslate}]{.pre}
method argument to customize this exporter.</p>
<pre><code>Parameters

:   **file** -- the file-like object to use for exporting the data.
    Its [`write`{.docutils .literal .notranslate}]{.pre} method
    should accept [`bytes`{.docutils .literal .notranslate}]{.pre}
    (a disk file opened in binary mode, a [`io.BytesIO`{.docutils
    .literal .notranslate}]{.pre} object, etc)

A typical output of this exporter would be:

::: {.highlight-none .notranslate}
::: highlight
    {&quot;name&quot;: &quot;Color TV&quot;, &quot;price&quot;: &quot;1200&quot;}
    {&quot;name&quot;: &quot;DVD player&quot;, &quot;price&quot;: &quot;200&quot;}
:::
:::

Unlike the one produced by [[`JsonItemExporter`{.xref .py .py-class
.docutils .literal
.notranslate}]{.pre}](#scrapy.exporters.JsonItemExporter &quot;scrapy.exporters.JsonItemExporter&quot;){.reference
.internal}, the format produced by this exporter is well suited for
serializing large amounts of data.
</code></pre>
<p>:::</p>
<p>::: {#marshalitemexporter .section}</p>
<h5 id="marshalitemexporterheaderlink"><a class="header" href="#marshalitemexporterheaderlink">MarshalItemExporter<a href="#marshalitemexporter" title="Permalink to this heading">¶</a>{.headerlink}</a></h5>
<p><em>[class]{.pre}[ ]{.w}</em>[[scrapy.exporters.]{.pre}]{.sig-prename .descclassname}[[MarshalItemExporter]{.pre}]{.sig-name .descname}[(]{.sig-paren}<em>[[file]{.pre}]{.n}</em>, <em>[[**]{.pre}]{.o}[[kwargs]{.pre}]{.n}</em>[)]{.sig-paren}<a href="_modules/scrapy/exporters.html#MarshalItemExporter">[[[source]]{.pre}]{.viewcode-link}</a>{.reference .internal}<a href="#scrapy.exporters.MarshalItemExporter" title="Permalink to this definition">¶</a>{.headerlink}</p>
<p>:   Exports items in a Python-specific binary format (see
<a href="https://docs.python.org/3/library/marshal.html#module-marshal" title="(in Python v3.12)">[<code>marshal</code>{.xref .py .py-mod .docutils .literal
.notranslate}]{.pre}</a>{.reference
.external}).</p>
<pre><code>Parameters

:   **file** -- The file-like object to use for exporting the data.
    Its [`write`{.docutils .literal .notranslate}]{.pre} method
    should accept [[`bytes`{.xref .py .py-class .docutils .literal
    .notranslate}]{.pre}](https://docs.python.org/3/library/stdtypes.html#bytes &quot;(in Python v3.12)&quot;){.reference
    .external} (a disk file opened in binary mode, a
    [[`BytesIO`{.xref .py .py-class .docutils .literal
    .notranslate}]{.pre}](https://docs.python.org/3/library/io.html#io.BytesIO &quot;(in Python v3.12)&quot;){.reference
    .external} object, etc)
</code></pre>
<p>:::
:::
:::</p>
<p>[]{#document-topics/components}</p>
<p>::: {#components .section}
[]{#topics-components}</p>
<h3 id="componentsheaderlink-1"><a class="header" href="#componentsheaderlink-1">Components<a href="#components" title="Permalink to this heading">¶</a>{.headerlink}</a></h3>
<p>A Scrapy component is any class whose objects are created using
[<code>scrapy.utils.misc.create_instance()</code>{.xref .py .py-func .docutils
.literal .notranslate}]{.pre}.</p>
<p>That includes the classes that you may assign to the following settings:</p>
<ul>
<li>
<p><a href="index.html#std-setting-DNS_RESOLVER">[<code>DNS_RESOLVER</code>{.xref .std .std-setting .docutils .literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal}</p>
</li>
<li>
<p><a href="index.html#std-setting-DOWNLOAD_HANDLERS">[<code>DOWNLOAD_HANDLERS</code>{.xref .std .std-setting .docutils .literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal}</p>
</li>
<li>
<p><a href="index.html#std-setting-DOWNLOADER_CLIENTCONTEXTFACTORY">[<code>DOWNLOADER_CLIENTCONTEXTFACTORY</code>{.xref .std .std-setting
.docutils .literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal}</p>
</li>
<li>
<p><a href="index.html#std-setting-DOWNLOADER_MIDDLEWARES">[<code>DOWNLOADER_MIDDLEWARES</code>{.xref .std .std-setting .docutils
.literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal}</p>
</li>
<li>
<p><a href="index.html#std-setting-DUPEFILTER_CLASS">[<code>DUPEFILTER_CLASS</code>{.xref .std .std-setting .docutils .literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal}</p>
</li>
<li>
<p><a href="index.html#std-setting-EXTENSIONS">[<code>EXTENSIONS</code>{.xref .std .std-setting .docutils .literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal}</p>
</li>
<li>
<p><a href="index.html#std-setting-FEED_EXPORTERS">[<code>FEED_EXPORTERS</code>{.xref .std .std-setting .docutils .literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal}</p>
</li>
<li>
<p><a href="index.html#std-setting-FEED_STORAGES">[<code>FEED_STORAGES</code>{.xref .std .std-setting .docutils .literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal}</p>
</li>
<li>
<p><a href="index.html#std-setting-ITEM_PIPELINES">[<code>ITEM_PIPELINES</code>{.xref .std .std-setting .docutils .literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal}</p>
</li>
<li>
<p><a href="index.html#std-setting-SCHEDULER">[<code>SCHEDULER</code>{.xref .std .std-setting .docutils .literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal}</p>
</li>
<li>
<p><a href="index.html#std-setting-SCHEDULER_DISK_QUEUE">[<code>SCHEDULER_DISK_QUEUE</code>{.xref .std .std-setting .docutils .literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal}</p>
</li>
<li>
<p><a href="index.html#std-setting-SCHEDULER_MEMORY_QUEUE">[<code>SCHEDULER_MEMORY_QUEUE</code>{.xref .std .std-setting .docutils
.literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal}</p>
</li>
<li>
<p><a href="index.html#std-setting-SCHEDULER_PRIORITY_QUEUE">[<code>SCHEDULER_PRIORITY_QUEUE</code>{.xref .std .std-setting .docutils
.literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal}</p>
</li>
<li>
<p><a href="index.html#std-setting-SPIDER_MIDDLEWARES">[<code>SPIDER_MIDDLEWARES</code>{.xref .std .std-setting .docutils .literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal}</p>
</li>
</ul>
<p>Third-party Scrapy components may also let you define additional Scrapy
components, usually configurable through <a href="index.html#topics-settings">[settings]{.std
.std-ref}</a>{.hoverxref .tooltip .reference
.internal}, to modify their behavior.</p>
<p>::: {#enforcing-component-requirements .section}
[]{#enforce-component-requirements}</p>
<h4 id="enforcing-component-requirementsheaderlink"><a class="header" href="#enforcing-component-requirementsheaderlink">Enforcing component requirements<a href="#enforcing-component-requirements" title="Permalink to this heading">¶</a>{.headerlink}</a></h4>
<p>Sometimes, your components may only be intended to work under certain
conditions. For example, they may require a minimum version of Scrapy to
work as intended, or they may require certain settings to have specific
values.</p>
<p>In addition to describing those conditions in the documentation of your
component, it is a good practice to raise an exception from the
[<code>__init__</code>{.docutils .literal .notranslate}]{.pre} method of your
component if those conditions are not met at run time.</p>
<p>In the case of <a href="index.html#topics-downloader-middleware">[downloader middlewares]{.std
.std-ref}</a>{.hoverxref .tooltip
.reference .internal}, <a href="index.html#topics-extensions">[extensions]{.std
.std-ref}</a>{.hoverxref .tooltip .reference
.internal}, <a href="index.html#topics-item-pipeline">[item pipelines]{.std
.std-ref}</a>{.hoverxref .tooltip
.reference .internal}, and <a href="index.html#topics-spider-middleware">[spider middlewares]{.std
.std-ref}</a>{.hoverxref .tooltip
.reference .internal}, you should raise
<a href="index.html#scrapy.exceptions.NotConfigured" title="scrapy.exceptions.NotConfigured">[<code>scrapy.exceptions.NotConfigured</code>{.xref .py .py-exc .docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal}, passing a description of the issue as a parameter to the
exception so that it is printed in the logs, for the user to see. For
other components, feel free to raise whatever other exception feels
right to you; for example, <a href="https://docs.python.org/3/library/exceptions.html#RuntimeError" title="(in Python v3.12)">[<code>RuntimeError</code>{.xref .py .py-exc .docutils
.literal
.notranslate}]{.pre}</a>{.reference
.external} would make sense for a Scrapy version mismatch, while
<a href="https://docs.python.org/3/library/exceptions.html#ValueError" title="(in Python v3.12)">[<code>ValueError</code>{.xref .py .py-exc .docutils .literal
.notranslate}]{.pre}</a>{.reference
.external} may be better if the issue is the value of a setting.</p>
<p>If your requirement is a minimum Scrapy version, you may use
[<code>scrapy.__version__</code>{.xref .py .py-attr .docutils .literal
.notranslate}]{.pre} to enforce your requirement. For example:</p>
<p>::: {.highlight-python .notranslate}
::: highlight
from packaging.version import parse as parse_version</p>
<pre><code>import scrapy


class MyComponent:
    def __init__(self):
        if parse_version(scrapy.__version__) &lt; parse_version(&quot;2.7&quot;):
            raise RuntimeError(
                f&quot;{MyComponent.__qualname__} requires Scrapy 2.7 or &quot;
                f&quot;later, which allow defining the process_spider_output &quot;
                f&quot;method of spider middlewares as an asynchronous &quot;
                f&quot;generator.&quot;
            )
</code></pre>
<p>:::
:::
:::
:::</p>
<p>[]{#document-topics/api}</p>
<p>::: {#core-api .section}
[]{#topics-api}</p>
<h3 id="core-apiheaderlink"><a class="header" href="#core-apiheaderlink">Core API<a href="#core-api" title="Permalink to this heading">¶</a>{.headerlink}</a></h3>
<p>This section documents the Scrapy core API, and it's intended for
developers of extensions and middlewares.</p>
<p>::: {#crawler-api .section}
[]{#topics-api-crawler}</p>
<h4 id="crawler-apiheaderlink"><a class="header" href="#crawler-apiheaderlink">Crawler API<a href="#crawler-api" title="Permalink to this heading">¶</a>{.headerlink}</a></h4>
<p>The main entry point to Scrapy API is the <a href="#scrapy.crawler.Crawler" title="scrapy.crawler.Crawler">[<code>Crawler</code>{.xref .py
.py-class .docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal} object, passed to extensions through the
[<code>from_crawler</code>{.docutils .literal .notranslate}]{.pre} class method.
This object provides access to all Scrapy core components, and it's the
only way for extensions to access them and hook their functionality into
Scrapy.</p>
<p>[]{#module-scrapy.crawler .target}</p>
<p>The Extension Manager is responsible for loading and keeping track of
installed extensions and it's configured through the
<a href="index.html#std-setting-EXTENSIONS">[<code>EXTENSIONS</code>{.xref .std .std-setting .docutils .literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal} setting which contains a dictionary of
all available extensions and their order similar to how you <a href="index.html#topics-downloader-middleware-setting">[configure
the downloader middlewares]{.std
.std-ref}</a>{.hoverxref
.tooltip .reference .internal}.</p>
<p><em>[class]{.pre}[ ]{.w}</em>[[scrapy.crawler.]{.pre}]{.sig-prename .descclassname}[[Crawler]{.pre}]{.sig-name .descname}[(]{.sig-paren}<em>[[spidercls]{.pre}]{.n}</em>, <em>[[settings]{.pre}]{.n}</em>[)]{.sig-paren}<a href="_modules/scrapy/crawler.html#Crawler">[[[source]]{.pre}]{.viewcode-link}</a>{.reference .internal}<a href="#scrapy.crawler.Crawler" title="Permalink to this definition">¶</a>{.headerlink}</p>
<p>:   The Crawler object must be instantiated with a
<a href="index.html#scrapy.Spider" title="scrapy.Spider">[<code>scrapy.Spider</code>{.xref .py .py-class .docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal} subclass and a <a href="#scrapy.settings.Settings" title="scrapy.settings.Settings">[<code>scrapy.settings.Settings</code>{.xref .py
.py-class .docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal} object.</p>
<pre><code>[[request_fingerprinter]{.pre}]{.sig-name .descname}[¶](#scrapy.crawler.Crawler.request_fingerprinter &quot;Permalink to this definition&quot;){.headerlink}

:   The request fingerprint builder of this crawler.

    This is used from extensions and middlewares to build short,
    unique identifiers for requests. See [[Request
    fingerprints]{.std
    .std-ref}](index.html#request-fingerprints){.hoverxref .tooltip
    .reference .internal}.

[[settings]{.pre}]{.sig-name .descname}[¶](#scrapy.crawler.Crawler.settings &quot;Permalink to this definition&quot;){.headerlink}

:   The settings manager of this crawler.

    This is used by extensions &amp; middlewares to access the Scrapy
    settings of this crawler.

    For an introduction on Scrapy settings see [[Settings]{.std
    .std-ref}](index.html#topics-settings){.hoverxref .tooltip
    .reference .internal}.

    For the API see [[`Settings`{.xref .py .py-class .docutils
    .literal
    .notranslate}]{.pre}](#scrapy.settings.Settings &quot;scrapy.settings.Settings&quot;){.reference
    .internal} class.

[[signals]{.pre}]{.sig-name .descname}[¶](#scrapy.crawler.Crawler.signals &quot;Permalink to this definition&quot;){.headerlink}

:   The signals manager of this crawler.

    This is used by extensions &amp; middlewares to hook themselves into
    Scrapy functionality.

    For an introduction on signals see [[Signals]{.std
    .std-ref}](index.html#topics-signals){.hoverxref .tooltip
    .reference .internal}.

    For the API see [[`SignalManager`{.xref .py .py-class .docutils
    .literal
    .notranslate}]{.pre}](#scrapy.signalmanager.SignalManager &quot;scrapy.signalmanager.SignalManager&quot;){.reference
    .internal} class.

[[stats]{.pre}]{.sig-name .descname}[¶](#scrapy.crawler.Crawler.stats &quot;Permalink to this definition&quot;){.headerlink}

:   The stats collector of this crawler.

    This is used from extensions &amp; middlewares to record stats of
    their behaviour, or access stats collected by other extensions.

    For an introduction on stats collection see [[Stats
    Collection]{.std .std-ref}](index.html#topics-stats){.hoverxref
    .tooltip .reference .internal}.

    For the API see [[`StatsCollector`{.xref .py .py-class .docutils
    .literal
    .notranslate}]{.pre}](#scrapy.statscollectors.StatsCollector &quot;scrapy.statscollectors.StatsCollector&quot;){.reference
    .internal} class.

[[extensions]{.pre}]{.sig-name .descname}[¶](#scrapy.crawler.Crawler.extensions &quot;Permalink to this definition&quot;){.headerlink}

:   The extension manager that keeps track of enabled extensions.

    Most extensions won't need to access this attribute.

    For an introduction on extensions and a list of available
    extensions on Scrapy see [[Extensions]{.std
    .std-ref}](index.html#topics-extensions){.hoverxref .tooltip
    .reference .internal}.

[[engine]{.pre}]{.sig-name .descname}[¶](#scrapy.crawler.Crawler.engine &quot;Permalink to this definition&quot;){.headerlink}

:   The execution engine, which coordinates the core crawling logic
    between the scheduler, downloader and spiders.

    Some extension may want to access the Scrapy engine, to inspect
    or modify the downloader and scheduler behaviour, although this
    is an advanced use and this API is not yet stable.

[[spider]{.pre}]{.sig-name .descname}[¶](#scrapy.crawler.Crawler.spider &quot;Permalink to this definition&quot;){.headerlink}

:   Spider currently being crawled. This is an instance of the
    spider class provided while constructing the crawler, and it is
    created after the arguments given in the [[`crawl()`{.xref .py
    .py-meth .docutils .literal
    .notranslate}]{.pre}](#scrapy.crawler.Crawler.crawl &quot;scrapy.crawler.Crawler.crawl&quot;){.reference
    .internal} method.

[[crawl]{.pre}]{.sig-name .descname}[(]{.sig-paren}*[[\*]{.pre}]{.o}[[args]{.pre}]{.n}*, *[[\*\*]{.pre}]{.o}[[kwargs]{.pre}]{.n}*[)]{.sig-paren}[[[\[source\]]{.pre}]{.viewcode-link}](_modules/scrapy/crawler.html#Crawler.crawl){.reference .internal}[¶](#scrapy.crawler.Crawler.crawl &quot;Permalink to this definition&quot;){.headerlink}

:   Starts the crawler by instantiating its spider class with the
    given [`args`{.docutils .literal .notranslate}]{.pre} and
    [`kwargs`{.docutils .literal .notranslate}]{.pre} arguments,
    while setting the execution engine in motion. Should be called
    only once.

    Returns a deferred that is fired when the crawl is finished.

[[stop]{.pre}]{.sig-name .descname}[(]{.sig-paren}[)]{.sig-paren} [[→]{.sig-return-icon} [[[Generator]{.pre}](https://docs.python.org/3/library/typing.html#typing.Generator &quot;(in Python v3.12)&quot;){.reference .external}[[\[]{.pre}]{.p}[[Deferred]{.pre}](https://docs.twisted.org/en/stable/api/twisted.internet.defer.Deferred.html &quot;(in Twisted)&quot;){.reference .external}[[,]{.pre}]{.p}[ ]{.w}[[Any]{.pre}](https://docs.python.org/3/library/typing.html#typing.Any &quot;(in Python v3.12)&quot;){.reference .external}[[,]{.pre}]{.p}[ ]{.w}[[None]{.pre}](https://docs.python.org/3/library/constants.html#None &quot;(in Python v3.12)&quot;){.reference .external}[[\]]{.pre}]{.p}]{.sig-return-typehint}]{.sig-return}[[[\[source\]]{.pre}]{.viewcode-link}](_modules/scrapy/crawler.html#Crawler.stop){.reference .internal}[¶](#scrapy.crawler.Crawler.stop &quot;Permalink to this definition&quot;){.headerlink}

:   Starts a graceful stop of the crawler and returns a deferred
    that is fired when the crawler is stopped.
</code></pre>
<pre><code class="language-{=html}">&lt;!-- --&gt;
</code></pre>
<p><em>[class]{.pre}[ ]{.w}</em>[[scrapy.crawler.]{.pre}]{.sig-prename .descclassname}[[CrawlerRunner]{.pre}]{.sig-name .descname}[(]{.sig-paren}<em>[[settings]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[<a href="https://docs.python.org/3/library/typing.html#typing.Optional" title="(in Python v3.12)">[Optional]{.pre}</a>{.reference .external}[[[]{.pre}]{.p}<a href="https://docs.python.org/3/library/typing.html#typing.Union" title="(in Python v3.12)">[Union]{.pre}</a>{.reference .external}[[[]{.pre}]{.p}<a href="https://docs.python.org/3/library/typing.html#typing.Dict" title="(in Python v3.12)">[Dict]{.pre}</a>{.reference .external}[[[]{.pre}]{.p}<a href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.12)">[str]{.pre}</a>{.reference .external}[[,]{.pre}]{.p}[ ]{.w}<a href="https://docs.python.org/3/library/typing.html#typing.Any" title="(in Python v3.12)">[Any]{.pre}</a>{.reference .external}[[]]{.pre}]{.p}[[,]{.pre}]{.p}[ ]{.w}<a href="index.html#scrapy.settings.Settings" title="scrapy.settings.Settings">[Settings]{.pre}</a>{.reference .internal}[[]]{.pre}]{.p}[[]]{.pre}]{.p}]{.n}[ ]{.w}[[=]{.pre}]{.o}[ ]{.w}[[None]{.pre}]{.default_value}</em>[)]{.sig-paren}<a href="_modules/scrapy/crawler.html#CrawlerRunner">[[[source]]{.pre}]{.viewcode-link}</a>{.reference .internal}<a href="#scrapy.crawler.CrawlerRunner" title="Permalink to this definition">¶</a>{.headerlink}</p>
<p>:   This is a convenient helper class that keeps track of, manages and
runs crawlers inside an already setup <a href="https://docs.twisted.org/en/stable/api/twisted.internet.reactor.html" title="(in Twisted)">[<code>reactor</code>{.xref .py .py-mod
.docutils .literal
.notranslate}]{.pre}</a>{.reference
.external}.</p>
<pre><code>The CrawlerRunner object must be instantiated with a
[[`Settings`{.xref .py .py-class .docutils .literal
.notranslate}]{.pre}](#scrapy.settings.Settings &quot;scrapy.settings.Settings&quot;){.reference
.internal} object.

This class shouldn't be needed (since Scrapy is responsible of using
it accordingly) unless writing scripts that manually handle the
crawling process. See [[Run Scrapy from a script]{.std
.std-ref}](index.html#run-from-script){.hoverxref .tooltip
.reference .internal} for an example.

[[crawl]{.pre}]{.sig-name .descname}[(]{.sig-paren}*[[crawler_or_spidercls]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[[[Union]{.pre}](https://docs.python.org/3/library/typing.html#typing.Union &quot;(in Python v3.12)&quot;){.reference .external}[[\[]{.pre}]{.p}[[Type]{.pre}](https://docs.python.org/3/library/typing.html#typing.Type &quot;(in Python v3.12)&quot;){.reference .external}[[\[]{.pre}]{.p}[[Spider]{.pre}](index.html#scrapy.spiders.Spider &quot;scrapy.spiders.Spider&quot;){.reference .internal}[[\]]{.pre}]{.p}[[,]{.pre}]{.p}[ ]{.w}[[str]{.pre}](https://docs.python.org/3/library/stdtypes.html#str &quot;(in Python v3.12)&quot;){.reference .external}[[,]{.pre}]{.p}[ ]{.w}[[Crawler]{.pre}](index.html#scrapy.crawler.Crawler &quot;scrapy.crawler.Crawler&quot;){.reference .internal}[[\]]{.pre}]{.p}]{.n}*, *[[\*]{.pre}]{.o}[[args]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[[[Any]{.pre}](https://docs.python.org/3/library/typing.html#typing.Any &quot;(in Python v3.12)&quot;){.reference .external}]{.n}*, *[[\*\*]{.pre}]{.o}[[kwargs]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[[[Any]{.pre}](https://docs.python.org/3/library/typing.html#typing.Any &quot;(in Python v3.12)&quot;){.reference .external}]{.n}*[)]{.sig-paren} [[→]{.sig-return-icon} [[[Deferred]{.pre}](https://docs.twisted.org/en/stable/api/twisted.internet.defer.Deferred.html &quot;(in Twisted)&quot;){.reference .external}]{.sig-return-typehint}]{.sig-return}[[[\[source\]]{.pre}]{.viewcode-link}](_modules/scrapy/crawler.html#CrawlerRunner.crawl){.reference .internal}[¶](#scrapy.crawler.CrawlerRunner.crawl &quot;Permalink to this definition&quot;){.headerlink}

:   Run a crawler with the provided arguments.

    It will call the given Crawler's [[`crawl()`{.xref .py .py-meth
    .docutils .literal
    .notranslate}]{.pre}](#scrapy.crawler.Crawler.crawl &quot;scrapy.crawler.Crawler.crawl&quot;){.reference
    .internal} method, while keeping track of it so it can be
    stopped later.

    If [`crawler_or_spidercls`{.docutils .literal
    .notranslate}]{.pre} isn't a [[`Crawler`{.xref .py .py-class
    .docutils .literal
    .notranslate}]{.pre}](#scrapy.crawler.Crawler &quot;scrapy.crawler.Crawler&quot;){.reference
    .internal} instance, this method will try to create one using
    this parameter as the spider class given to it.

    Returns a deferred that is fired when the crawling is finished.

    Parameters

    :   -   **crawler_or_spidercls** ([[`Crawler`{.xref .py
            .py-class .docutils .literal
            .notranslate}]{.pre}](#scrapy.crawler.Crawler &quot;scrapy.crawler.Crawler&quot;){.reference
            .internal} instance, [[`Spider`{.xref .py .py-class
            .docutils .literal
            .notranslate}]{.pre}](index.html#scrapy.spiders.Spider &quot;scrapy.spiders.Spider&quot;){.reference
            .internal} subclass or string) -- already created
            crawler, or a spider class or spider's name inside the
            project to create it

        -   **args** -- arguments to initialize the spider

        -   **kwargs** -- keyword arguments to initialize the spider

*[property]{.pre}[ ]{.w}*[[crawlers]{.pre}]{.sig-name .descname}[¶](#scrapy.crawler.CrawlerRunner.crawlers &quot;Permalink to this definition&quot;){.headerlink}

:   Set of [[`crawlers`{.xref .py .py-class .docutils .literal
    .notranslate}]{.pre}](#scrapy.crawler.Crawler &quot;scrapy.crawler.Crawler&quot;){.reference
    .internal} started by [[`crawl()`{.xref .py .py-meth .docutils
    .literal
    .notranslate}]{.pre}](#scrapy.crawler.CrawlerRunner.crawl &quot;scrapy.crawler.CrawlerRunner.crawl&quot;){.reference
    .internal} and managed by this class.

[[create_crawler]{.pre}]{.sig-name .descname}[(]{.sig-paren}*[[crawler_or_spidercls]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[[[Union]{.pre}](https://docs.python.org/3/library/typing.html#typing.Union &quot;(in Python v3.12)&quot;){.reference .external}[[\[]{.pre}]{.p}[[Type]{.pre}](https://docs.python.org/3/library/typing.html#typing.Type &quot;(in Python v3.12)&quot;){.reference .external}[[\[]{.pre}]{.p}[[Spider]{.pre}](index.html#scrapy.spiders.Spider &quot;scrapy.spiders.Spider&quot;){.reference .internal}[[\]]{.pre}]{.p}[[,]{.pre}]{.p}[ ]{.w}[[str]{.pre}](https://docs.python.org/3/library/stdtypes.html#str &quot;(in Python v3.12)&quot;){.reference .external}[[,]{.pre}]{.p}[ ]{.w}[[Crawler]{.pre}](index.html#scrapy.crawler.Crawler &quot;scrapy.crawler.Crawler&quot;){.reference .internal}[[\]]{.pre}]{.p}]{.n}*[)]{.sig-paren} [[→]{.sig-return-icon} [[[Crawler]{.pre}](index.html#scrapy.crawler.Crawler &quot;scrapy.crawler.Crawler&quot;){.reference .internal}]{.sig-return-typehint}]{.sig-return}[[[\[source\]]{.pre}]{.viewcode-link}](_modules/scrapy/crawler.html#CrawlerRunner.create_crawler){.reference .internal}[¶](#scrapy.crawler.CrawlerRunner.create_crawler &quot;Permalink to this definition&quot;){.headerlink}

:   Return a [[`Crawler`{.xref .py .py-class .docutils .literal
    .notranslate}]{.pre}](#scrapy.crawler.Crawler &quot;scrapy.crawler.Crawler&quot;){.reference
    .internal} object.

    -   If [`crawler_or_spidercls`{.docutils .literal
        .notranslate}]{.pre} is a Crawler, it is returned as-is.

    -   If [`crawler_or_spidercls`{.docutils .literal
        .notranslate}]{.pre} is a Spider subclass, a new Crawler is
        constructed for it.

    -   If [`crawler_or_spidercls`{.docutils .literal
        .notranslate}]{.pre} is a string, this function finds a
        spider with this name in a Scrapy project (using spider
        loader), then creates a Crawler instance for it.

[[join]{.pre}]{.sig-name .descname}[(]{.sig-paren}[)]{.sig-paren}[[[\[source\]]{.pre}]{.viewcode-link}](_modules/scrapy/crawler.html#CrawlerRunner.join){.reference .internal}[¶](#scrapy.crawler.CrawlerRunner.join &quot;Permalink to this definition&quot;){.headerlink}

:   Returns a deferred that is fired when all managed
    [[`crawlers`{.xref .py .py-attr .docutils .literal
    .notranslate}]{.pre}](#scrapy.crawler.CrawlerRunner.crawlers &quot;scrapy.crawler.CrawlerRunner.crawlers&quot;){.reference
    .internal} have completed their executions.

[[stop]{.pre}]{.sig-name .descname}[(]{.sig-paren}[)]{.sig-paren} [[→]{.sig-return-icon} [[[Deferred]{.pre}](https://docs.twisted.org/en/stable/api/twisted.internet.defer.Deferred.html &quot;(in Twisted)&quot;){.reference .external}]{.sig-return-typehint}]{.sig-return}[[[\[source\]]{.pre}]{.viewcode-link}](_modules/scrapy/crawler.html#CrawlerRunner.stop){.reference .internal}[¶](#scrapy.crawler.CrawlerRunner.stop &quot;Permalink to this definition&quot;){.headerlink}

:   Stops simultaneously all the crawling jobs taking place.

    Returns a deferred that is fired when they all have ended.
</code></pre>
<pre><code class="language-{=html}">&lt;!-- --&gt;
</code></pre>
<p><em>[class]{.pre}[ ]{.w}</em>[[scrapy.crawler.]{.pre}]{.sig-prename .descclassname}[[CrawlerProcess]{.pre}]{.sig-name .descname}[(]{.sig-paren}<em>[[settings]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[<a href="https://docs.python.org/3/library/typing.html#typing.Optional" title="(in Python v3.12)">[Optional]{.pre}</a>{.reference .external}[[[]{.pre}]{.p}<a href="https://docs.python.org/3/library/typing.html#typing.Union" title="(in Python v3.12)">[Union]{.pre}</a>{.reference .external}[[[]{.pre}]{.p}<a href="https://docs.python.org/3/library/typing.html#typing.Dict" title="(in Python v3.12)">[Dict]{.pre}</a>{.reference .external}[[[]{.pre}]{.p}<a href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.12)">[str]{.pre}</a>{.reference .external}[[,]{.pre}]{.p}[ ]{.w}<a href="https://docs.python.org/3/library/typing.html#typing.Any" title="(in Python v3.12)">[Any]{.pre}</a>{.reference .external}[[]]{.pre}]{.p}[[,]{.pre}]{.p}[ ]{.w}<a href="index.html#scrapy.settings.Settings" title="scrapy.settings.Settings">[Settings]{.pre}</a>{.reference .internal}[[]]{.pre}]{.p}[[]]{.pre}]{.p}]{.n}[ ]{.w}[[=]{.pre}]{.o}[ ]{.w}[[None]{.pre}]{.default_value}</em>, <em>[[install_root_handler]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[<a href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.12)">[bool]{.pre}</a>{.reference .external}]{.n}[ ]{.w}[[=]{.pre}]{.o}[ ]{.w}[[True]{.pre}]{.default_value}</em>[)]{.sig-paren}<a href="_modules/scrapy/crawler.html#CrawlerProcess">[[[source]]{.pre}]{.viewcode-link}</a>{.reference .internal}<a href="#scrapy.crawler.CrawlerProcess" title="Permalink to this definition">¶</a>{.headerlink}</p>
<p>:   Bases: <a href="#scrapy.crawler.CrawlerRunner" title="scrapy.crawler.CrawlerRunner">[<code>CrawlerRunner</code>{.xref .py .py-class .docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal}</p>
<pre><code>A class to run multiple scrapy crawlers in a process simultaneously.

This class extends [[`CrawlerRunner`{.xref .py .py-class .docutils
.literal
.notranslate}]{.pre}](#scrapy.crawler.CrawlerRunner &quot;scrapy.crawler.CrawlerRunner&quot;){.reference
.internal} by adding support for starting a [[`reactor`{.xref .py
.py-mod .docutils .literal
.notranslate}]{.pre}](https://docs.twisted.org/en/stable/api/twisted.internet.reactor.html &quot;(in Twisted)&quot;){.reference
.external} and handling shutdown signals, like the keyboard
interrupt command Ctrl-C. It also configures top-level logging.

This utility should be a better fit than [[`CrawlerRunner`{.xref .py
.py-class .docutils .literal
.notranslate}]{.pre}](#scrapy.crawler.CrawlerRunner &quot;scrapy.crawler.CrawlerRunner&quot;){.reference
.internal} if you aren't running another [[`reactor`{.xref .py
.py-mod .docutils .literal
.notranslate}]{.pre}](https://docs.twisted.org/en/stable/api/twisted.internet.reactor.html &quot;(in Twisted)&quot;){.reference
.external} within your application.

The CrawlerProcess object must be instantiated with a
[[`Settings`{.xref .py .py-class .docutils .literal
.notranslate}]{.pre}](#scrapy.settings.Settings &quot;scrapy.settings.Settings&quot;){.reference
.internal} object.

Parameters

:   **install_root_handler** -- whether to install root logging
    handler (default: True)

This class shouldn't be needed (since Scrapy is responsible of using
it accordingly) unless writing scripts that manually handle the
crawling process. See [[Run Scrapy from a script]{.std
.std-ref}](index.html#run-from-script){.hoverxref .tooltip
.reference .internal} for an example.

[[crawl]{.pre}]{.sig-name .descname}[(]{.sig-paren}*[[crawler_or_spidercls]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[[[Union]{.pre}](https://docs.python.org/3/library/typing.html#typing.Union &quot;(in Python v3.12)&quot;){.reference .external}[[\[]{.pre}]{.p}[[Type]{.pre}](https://docs.python.org/3/library/typing.html#typing.Type &quot;(in Python v3.12)&quot;){.reference .external}[[\[]{.pre}]{.p}[[Spider]{.pre}](index.html#scrapy.spiders.Spider &quot;scrapy.spiders.Spider&quot;){.reference .internal}[[\]]{.pre}]{.p}[[,]{.pre}]{.p}[ ]{.w}[[str]{.pre}](https://docs.python.org/3/library/stdtypes.html#str &quot;(in Python v3.12)&quot;){.reference .external}[[,]{.pre}]{.p}[ ]{.w}[[Crawler]{.pre}](index.html#scrapy.crawler.Crawler &quot;scrapy.crawler.Crawler&quot;){.reference .internal}[[\]]{.pre}]{.p}]{.n}*, *[[\*]{.pre}]{.o}[[args]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[[[Any]{.pre}](https://docs.python.org/3/library/typing.html#typing.Any &quot;(in Python v3.12)&quot;){.reference .external}]{.n}*, *[[\*\*]{.pre}]{.o}[[kwargs]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[[[Any]{.pre}](https://docs.python.org/3/library/typing.html#typing.Any &quot;(in Python v3.12)&quot;){.reference .external}]{.n}*[)]{.sig-paren} [[→]{.sig-return-icon} [[[Deferred]{.pre}](https://docs.twisted.org/en/stable/api/twisted.internet.defer.Deferred.html &quot;(in Twisted)&quot;){.reference .external}]{.sig-return-typehint}]{.sig-return}[¶](#scrapy.crawler.CrawlerProcess.crawl &quot;Permalink to this definition&quot;){.headerlink}

:   Run a crawler with the provided arguments.

    It will call the given Crawler's [[`crawl()`{.xref .py .py-meth
    .docutils .literal
    .notranslate}]{.pre}](#scrapy.crawler.Crawler.crawl &quot;scrapy.crawler.Crawler.crawl&quot;){.reference
    .internal} method, while keeping track of it so it can be
    stopped later.

    If [`crawler_or_spidercls`{.docutils .literal
    .notranslate}]{.pre} isn't a [[`Crawler`{.xref .py .py-class
    .docutils .literal
    .notranslate}]{.pre}](#scrapy.crawler.Crawler &quot;scrapy.crawler.Crawler&quot;){.reference
    .internal} instance, this method will try to create one using
    this parameter as the spider class given to it.

    Returns a deferred that is fired when the crawling is finished.

    Parameters

    :   -   **crawler_or_spidercls** ([[`Crawler`{.xref .py
            .py-class .docutils .literal
            .notranslate}]{.pre}](#scrapy.crawler.Crawler &quot;scrapy.crawler.Crawler&quot;){.reference
            .internal} instance, [[`Spider`{.xref .py .py-class
            .docutils .literal
            .notranslate}]{.pre}](index.html#scrapy.spiders.Spider &quot;scrapy.spiders.Spider&quot;){.reference
            .internal} subclass or string) -- already created
            crawler, or a spider class or spider's name inside the
            project to create it

        -   **args** -- arguments to initialize the spider

        -   **kwargs** -- keyword arguments to initialize the spider

*[property]{.pre}[ ]{.w}*[[crawlers]{.pre}]{.sig-name .descname}[¶](#scrapy.crawler.CrawlerProcess.crawlers &quot;Permalink to this definition&quot;){.headerlink}

:   Set of [[`crawlers`{.xref .py .py-class .docutils .literal
    .notranslate}]{.pre}](#scrapy.crawler.Crawler &quot;scrapy.crawler.Crawler&quot;){.reference
    .internal} started by [[`crawl()`{.xref .py .py-meth .docutils
    .literal
    .notranslate}]{.pre}](#scrapy.crawler.CrawlerProcess.crawl &quot;scrapy.crawler.CrawlerProcess.crawl&quot;){.reference
    .internal} and managed by this class.

[[create_crawler]{.pre}]{.sig-name .descname}[(]{.sig-paren}*[[crawler_or_spidercls]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[[[Union]{.pre}](https://docs.python.org/3/library/typing.html#typing.Union &quot;(in Python v3.12)&quot;){.reference .external}[[\[]{.pre}]{.p}[[Type]{.pre}](https://docs.python.org/3/library/typing.html#typing.Type &quot;(in Python v3.12)&quot;){.reference .external}[[\[]{.pre}]{.p}[[Spider]{.pre}](index.html#scrapy.spiders.Spider &quot;scrapy.spiders.Spider&quot;){.reference .internal}[[\]]{.pre}]{.p}[[,]{.pre}]{.p}[ ]{.w}[[str]{.pre}](https://docs.python.org/3/library/stdtypes.html#str &quot;(in Python v3.12)&quot;){.reference .external}[[,]{.pre}]{.p}[ ]{.w}[[Crawler]{.pre}](index.html#scrapy.crawler.Crawler &quot;scrapy.crawler.Crawler&quot;){.reference .internal}[[\]]{.pre}]{.p}]{.n}*[)]{.sig-paren} [[→]{.sig-return-icon} [[[Crawler]{.pre}](index.html#scrapy.crawler.Crawler &quot;scrapy.crawler.Crawler&quot;){.reference .internal}]{.sig-return-typehint}]{.sig-return}[¶](#scrapy.crawler.CrawlerProcess.create_crawler &quot;Permalink to this definition&quot;){.headerlink}

:   Return a [[`Crawler`{.xref .py .py-class .docutils .literal
    .notranslate}]{.pre}](#scrapy.crawler.Crawler &quot;scrapy.crawler.Crawler&quot;){.reference
    .internal} object.

    -   If [`crawler_or_spidercls`{.docutils .literal
        .notranslate}]{.pre} is a Crawler, it is returned as-is.

    -   If [`crawler_or_spidercls`{.docutils .literal
        .notranslate}]{.pre} is a Spider subclass, a new Crawler is
        constructed for it.

    -   If [`crawler_or_spidercls`{.docutils .literal
        .notranslate}]{.pre} is a string, this function finds a
        spider with this name in a Scrapy project (using spider
        loader), then creates a Crawler instance for it.

[[join]{.pre}]{.sig-name .descname}[(]{.sig-paren}[)]{.sig-paren}[¶](#scrapy.crawler.CrawlerProcess.join &quot;Permalink to this definition&quot;){.headerlink}

:   Returns a deferred that is fired when all managed
    [[`crawlers`{.xref .py .py-attr .docutils .literal
    .notranslate}]{.pre}](#scrapy.crawler.CrawlerProcess.crawlers &quot;scrapy.crawler.CrawlerProcess.crawlers&quot;){.reference
    .internal} have completed their executions.

[[start]{.pre}]{.sig-name .descname}[(]{.sig-paren}*[[stop_after_crawl]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[[[bool]{.pre}](https://docs.python.org/3/library/functions.html#bool &quot;(in Python v3.12)&quot;){.reference .external}]{.n}[ ]{.w}[[=]{.pre}]{.o}[ ]{.w}[[True]{.pre}]{.default_value}*, *[[install_signal_handlers]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[[[bool]{.pre}](https://docs.python.org/3/library/functions.html#bool &quot;(in Python v3.12)&quot;){.reference .external}]{.n}[ ]{.w}[[=]{.pre}]{.o}[ ]{.w}[[True]{.pre}]{.default_value}*[)]{.sig-paren} [[→]{.sig-return-icon} [[[None]{.pre}](https://docs.python.org/3/library/constants.html#None &quot;(in Python v3.12)&quot;){.reference .external}]{.sig-return-typehint}]{.sig-return}[[[\[source\]]{.pre}]{.viewcode-link}](_modules/scrapy/crawler.html#CrawlerProcess.start){.reference .internal}[¶](#scrapy.crawler.CrawlerProcess.start &quot;Permalink to this definition&quot;){.headerlink}

:   This method starts a [[`reactor`{.xref .py .py-mod .docutils
    .literal
    .notranslate}]{.pre}](https://docs.twisted.org/en/stable/api/twisted.internet.reactor.html &quot;(in Twisted)&quot;){.reference
    .external}, adjusts its pool size to
    [[`REACTOR_THREADPOOL_MAXSIZE`{.xref .std .std-setting .docutils
    .literal
    .notranslate}]{.pre}](index.html#std-setting-REACTOR_THREADPOOL_MAXSIZE){.hoverxref
    .tooltip .reference .internal}, and installs a DNS cache based
    on [[`DNSCACHE_ENABLED`{.xref .std .std-setting .docutils
    .literal
    .notranslate}]{.pre}](index.html#std-setting-DNSCACHE_ENABLED){.hoverxref
    .tooltip .reference .internal} and [[`DNSCACHE_SIZE`{.xref .std
    .std-setting .docutils .literal
    .notranslate}]{.pre}](index.html#std-setting-DNSCACHE_SIZE){.hoverxref
    .tooltip .reference .internal}.

    If [`stop_after_crawl`{.docutils .literal .notranslate}]{.pre}
    is True, the reactor will be stopped after all crawlers have
    finished, using [[`join()`{.xref .py .py-meth .docutils .literal
    .notranslate}]{.pre}](#scrapy.crawler.CrawlerProcess.join &quot;scrapy.crawler.CrawlerProcess.join&quot;){.reference
    .internal}.

    Parameters

    :   -   **stop_after_crawl**
            ([*bool*](https://docs.python.org/3/library/functions.html#bool &quot;(in Python v3.12)&quot;){.reference
            .external}) -- stop or not the reactor when all crawlers
            have finished

        -   **install_signal_handlers**
            ([*bool*](https://docs.python.org/3/library/functions.html#bool &quot;(in Python v3.12)&quot;){.reference
            .external}) -- whether to install the OS signal handlers
            from Twisted and Scrapy (default: True)

[[stop]{.pre}]{.sig-name .descname}[(]{.sig-paren}[)]{.sig-paren} [[→]{.sig-return-icon} [[[Deferred]{.pre}](https://docs.twisted.org/en/stable/api/twisted.internet.defer.Deferred.html &quot;(in Twisted)&quot;){.reference .external}]{.sig-return-typehint}]{.sig-return}[¶](#scrapy.crawler.CrawlerProcess.stop &quot;Permalink to this definition&quot;){.headerlink}

:   Stops simultaneously all the crawling jobs taking place.

    Returns a deferred that is fired when they all have ended.
</code></pre>
<p>:::</p>
<p>::: {#module-scrapy.settings .section}
[]{#settings-api}[]{#topics-api-settings}</p>
<h4 id="settings-apiheaderlink"><a class="header" href="#settings-apiheaderlink">Settings API<a href="#module-scrapy.settings" title="Permalink to this heading">¶</a>{.headerlink}</a></h4>
<p>[[scrapy.settings.]{.pre}]{.sig-prename .descclassname}[[SETTINGS_PRIORITIES]{.pre}]{.sig-name .descname}<a href="#scrapy.settings.SETTINGS_PRIORITIES" title="Permalink to this definition">¶</a>{.headerlink}</p>
<p>:   Dictionary that sets the key name and priority level of the default
settings priorities used in Scrapy.</p>
<pre><code>Each item defines a settings entry point, giving it a code name for
identification and an integer priority. Greater priorities take more
precedence over lesser ones when setting and retrieving values in
the [[`Settings`{.xref .py .py-class .docutils .literal
.notranslate}]{.pre}](#scrapy.settings.Settings &quot;scrapy.settings.Settings&quot;){.reference
.internal} class.

::: {.highlight-python .notranslate}
::: highlight
    SETTINGS_PRIORITIES = {
        &quot;default&quot;: 0,
        &quot;command&quot;: 10,
        &quot;addon&quot;: 15,
        &quot;project&quot;: 20,
        &quot;spider&quot;: 30,
        &quot;cmdline&quot;: 40,
    }
:::
:::

For a detailed explanation on each settings sources, see:
[[Settings]{.std .std-ref}](index.html#topics-settings){.hoverxref
.tooltip .reference .internal}.
</code></pre>
<pre><code class="language-{=html}">&lt;!-- --&gt;
</code></pre>
<p>[[scrapy.settings.]{.pre}]{.sig-prename .descclassname}[[get_settings_priority]{.pre}]{.sig-name .descname}[(]{.sig-paren}<em>[[priority]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[<a href="https://docs.python.org/3/library/typing.html#typing.Union" title="(in Python v3.12)">[Union]{.pre}</a>{.reference .external}[[[]{.pre}]{.p}<a href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.12)">[int]{.pre}</a>{.reference .external}[[,]{.pre}]{.p}[ ]{.w}<a href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.12)">[str]{.pre}</a>{.reference .external}[[]]{.pre}]{.p}]{.n}</em>[)]{.sig-paren} [[→]{.sig-return-icon} [<a href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.12)">[int]{.pre}</a>{.reference .external}]{.sig-return-typehint}]{.sig-return}<a href="_modules/scrapy/settings.html#get_settings_priority">[[[source]]{.pre}]{.viewcode-link}</a>{.reference .internal}<a href="#scrapy.settings.get_settings_priority" title="Permalink to this definition">¶</a>{.headerlink}</p>
<p>:   Small helper function that looks up a given string priority in the
<a href="#scrapy.settings.SETTINGS_PRIORITIES" title="scrapy.settings.SETTINGS_PRIORITIES">[<code>SETTINGS_PRIORITIES</code>{.xref .py .py-attr .docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal} dictionary and returns its numerical value, or directly
returns a given numerical priority.</p>
<pre><code class="language-{=html}">&lt;!-- --&gt;
</code></pre>
<p><em>[class]{.pre}[ ]{.w}</em>[[scrapy.settings.]{.pre}]{.sig-prename .descclassname}[[Settings]{.pre}]{.sig-name .descname}[(]{.sig-paren}<em>[[values]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[[_SettingsInputT]{.pre}]{.n}[ ]{.w}[[=]{.pre}]{.o}[ ]{.w}[[None]{.pre}]{.default_value}</em>, <em>[[priority]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[[Union]{.pre}[[[]{.pre}]{.p}<a href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.12)">[int]{.pre}</a>{.reference .external}[[,]{.pre}]{.p}[ ]{.w}<a href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.12)">[str]{.pre}</a>{.reference .external}[[]]{.pre}]{.p}]{.n}[ ]{.w}[[=]{.pre}]{.o}[ ]{.w}[['project']{.pre}]{.default_value}</em>[)]{.sig-paren}<a href="_modules/scrapy/settings.html#Settings">[[[source]]{.pre}]{.viewcode-link}</a>{.reference .internal}<a href="#scrapy.settings.Settings" title="Permalink to this definition">¶</a>{.headerlink}</p>
<p>:   Bases: <a href="#scrapy.settings.BaseSettings" title="scrapy.settings.BaseSettings">[<code>BaseSettings</code>{.xref .py .py-class .docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal}</p>
<pre><code>This object stores Scrapy settings for the configuration of internal
components, and can be used for any further customization.

It is a direct subclass and supports all methods of
[[`BaseSettings`{.xref .py .py-class .docutils .literal
.notranslate}]{.pre}](#scrapy.settings.BaseSettings &quot;scrapy.settings.BaseSettings&quot;){.reference
.internal}. Additionally, after instantiation of this class, the new
object will have the global default settings described on [[Built-in
settings reference]{.std
.std-ref}](index.html#topics-settings-ref){.hoverxref .tooltip
.reference .internal} already populated.
</code></pre>
<pre><code class="language-{=html}">&lt;!-- --&gt;
</code></pre>
<p><em>[class]{.pre}[ ]{.w}</em>[[scrapy.settings.]{.pre}]{.sig-prename .descclassname}[[BaseSettings]{.pre}]{.sig-name .descname}[(]{.sig-paren}<em>[[values]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[[_SettingsInputT]{.pre}]{.n}[ ]{.w}[[=]{.pre}]{.o}[ ]{.w}[[None]{.pre}]{.default_value}</em>, <em>[[priority]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[[Union]{.pre}[[[]{.pre}]{.p}<a href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.12)">[int]{.pre}</a>{.reference .external}[[,]{.pre}]{.p}[ ]{.w}<a href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.12)">[str]{.pre}</a>{.reference .external}[[]]{.pre}]{.p}]{.n}[ ]{.w}[[=]{.pre}]{.o}[ ]{.w}[['project']{.pre}]{.default_value}</em>[)]{.sig-paren}<a href="_modules/scrapy/settings.html#BaseSettings">[[[source]]{.pre}]{.viewcode-link}</a>{.reference .internal}<a href="#scrapy.settings.BaseSettings" title="Permalink to this definition">¶</a>{.headerlink}</p>
<p>:   Instances of this class behave like dictionaries, but store
priorities along with their [<code>(key,</code>{.docutils .literal
.notranslate}]{.pre}<code> </code>{.docutils .literal
.notranslate}[<code>value)</code>{.docutils .literal .notranslate}]{.pre}
pairs, and can be frozen (i.e. marked immutable).</p>
<pre><code>Key-value entries can be passed on initialization with the
[`values`{.docutils .literal .notranslate}]{.pre} argument, and they
would take the [`priority`{.docutils .literal .notranslate}]{.pre}
level (unless [`values`{.docutils .literal .notranslate}]{.pre} is
already an instance of [[`BaseSettings`{.xref .py .py-class
.docutils .literal
.notranslate}]{.pre}](#scrapy.settings.BaseSettings &quot;scrapy.settings.BaseSettings&quot;){.reference
.internal}, in which case the existing priority levels will be
kept). If the [`priority`{.docutils .literal .notranslate}]{.pre}
argument is a string, the priority name will be looked up in
[[`SETTINGS_PRIORITIES`{.xref .py .py-attr .docutils .literal
.notranslate}]{.pre}](#scrapy.settings.SETTINGS_PRIORITIES &quot;scrapy.settings.SETTINGS_PRIORITIES&quot;){.reference
.internal}. Otherwise, a specific integer should be provided.

Once the object is created, new settings can be loaded or updated
with the [[`set()`{.xref .py .py-meth .docutils .literal
.notranslate}]{.pre}](#scrapy.settings.BaseSettings.set &quot;scrapy.settings.BaseSettings.set&quot;){.reference
.internal} method, and can be accessed with the square bracket
notation of dictionaries, or with the [[`get()`{.xref .py .py-meth
.docutils .literal
.notranslate}]{.pre}](#scrapy.settings.BaseSettings.get &quot;scrapy.settings.BaseSettings.get&quot;){.reference
.internal} method of the instance and its value conversion variants.
When requesting a stored key, the value with the highest priority
will be retrieved.

[[copy]{.pre}]{.sig-name .descname}[(]{.sig-paren}[)]{.sig-paren} [[→]{.sig-return-icon} [[Self]{.pre}]{.sig-return-typehint}]{.sig-return}[[[\[source\]]{.pre}]{.viewcode-link}](_modules/scrapy/settings.html#BaseSettings.copy){.reference .internal}[¶](#scrapy.settings.BaseSettings.copy &quot;Permalink to this definition&quot;){.headerlink}

:   Make a deep copy of current settings.

    This method returns a new instance of the [[`Settings`{.xref .py
    .py-class .docutils .literal
    .notranslate}]{.pre}](#scrapy.settings.Settings &quot;scrapy.settings.Settings&quot;){.reference
    .internal} class, populated with the same values and their
    priorities.

    Modifications to the new object won't be reflected on the
    original settings.

[[copy_to_dict]{.pre}]{.sig-name .descname}[(]{.sig-paren}[)]{.sig-paren} [[→]{.sig-return-icon} [[[Dict]{.pre}](https://docs.python.org/3/library/typing.html#typing.Dict &quot;(in Python v3.12)&quot;){.reference .external}[[\[]{.pre}]{.p}[[Optional]{.pre}](https://docs.python.org/3/library/typing.html#typing.Optional &quot;(in Python v3.12)&quot;){.reference .external}[[\[]{.pre}]{.p}[[Union]{.pre}](https://docs.python.org/3/library/typing.html#typing.Union &quot;(in Python v3.12)&quot;){.reference .external}[[\[]{.pre}]{.p}[[bool]{.pre}](https://docs.python.org/3/library/functions.html#bool &quot;(in Python v3.12)&quot;){.reference .external}[[,]{.pre}]{.p}[ ]{.w}[[float]{.pre}](https://docs.python.org/3/library/functions.html#float &quot;(in Python v3.12)&quot;){.reference .external}[[,]{.pre}]{.p}[ ]{.w}[[int]{.pre}](https://docs.python.org/3/library/functions.html#int &quot;(in Python v3.12)&quot;){.reference .external}[[,]{.pre}]{.p}[ ]{.w}[[str]{.pre}](https://docs.python.org/3/library/stdtypes.html#str &quot;(in Python v3.12)&quot;){.reference .external}[[\]]{.pre}]{.p}[[\]]{.pre}]{.p}[[,]{.pre}]{.p}[ ]{.w}[[Any]{.pre}](https://docs.python.org/3/library/typing.html#typing.Any &quot;(in Python v3.12)&quot;){.reference .external}[[\]]{.pre}]{.p}]{.sig-return-typehint}]{.sig-return}[[[\[source\]]{.pre}]{.viewcode-link}](_modules/scrapy/settings.html#BaseSettings.copy_to_dict){.reference .internal}[¶](#scrapy.settings.BaseSettings.copy_to_dict &quot;Permalink to this definition&quot;){.headerlink}

:   Make a copy of current settings and convert to a dict.

    This method returns a new dict populated with the same values
    and their priorities as the current settings.

    Modifications to the returned dict won't be reflected on the
    original settings.

    This method can be useful for example for printing settings in
    Scrapy shell.

[[freeze]{.pre}]{.sig-name .descname}[(]{.sig-paren}[)]{.sig-paren} [[→]{.sig-return-icon} [[[None]{.pre}](https://docs.python.org/3/library/constants.html#None &quot;(in Python v3.12)&quot;){.reference .external}]{.sig-return-typehint}]{.sig-return}[[[\[source\]]{.pre}]{.viewcode-link}](_modules/scrapy/settings.html#BaseSettings.freeze){.reference .internal}[¶](#scrapy.settings.BaseSettings.freeze &quot;Permalink to this definition&quot;){.headerlink}

:   Disable further changes to the current settings.

    After calling this method, the present state of the settings
    will become immutable. Trying to change values through the
    [[`set()`{.xref .py .py-meth .docutils .literal
    .notranslate}]{.pre}](#scrapy.settings.BaseSettings.set &quot;scrapy.settings.BaseSettings.set&quot;){.reference
    .internal} method and its variants won't be possible and will be
    alerted.

[[frozencopy]{.pre}]{.sig-name .descname}[(]{.sig-paren}[)]{.sig-paren} [[→]{.sig-return-icon} [[Self]{.pre}]{.sig-return-typehint}]{.sig-return}[[[\[source\]]{.pre}]{.viewcode-link}](_modules/scrapy/settings.html#BaseSettings.frozencopy){.reference .internal}[¶](#scrapy.settings.BaseSettings.frozencopy &quot;Permalink to this definition&quot;){.headerlink}

:   Return an immutable copy of the current settings.

    Alias for a [[`freeze()`{.xref .py .py-meth .docutils .literal
    .notranslate}]{.pre}](#scrapy.settings.BaseSettings.freeze &quot;scrapy.settings.BaseSettings.freeze&quot;){.reference
    .internal} call in the object returned by [[`copy()`{.xref .py
    .py-meth .docutils .literal
    .notranslate}]{.pre}](#scrapy.settings.BaseSettings.copy &quot;scrapy.settings.BaseSettings.copy&quot;){.reference
    .internal}.

[[get]{.pre}]{.sig-name .descname}[(]{.sig-paren}*[[name]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[[[Optional]{.pre}](https://docs.python.org/3/library/typing.html#typing.Optional &quot;(in Python v3.12)&quot;){.reference .external}[[\[]{.pre}]{.p}[[Union]{.pre}](https://docs.python.org/3/library/typing.html#typing.Union &quot;(in Python v3.12)&quot;){.reference .external}[[\[]{.pre}]{.p}[[bool]{.pre}](https://docs.python.org/3/library/functions.html#bool &quot;(in Python v3.12)&quot;){.reference .external}[[,]{.pre}]{.p}[ ]{.w}[[float]{.pre}](https://docs.python.org/3/library/functions.html#float &quot;(in Python v3.12)&quot;){.reference .external}[[,]{.pre}]{.p}[ ]{.w}[[int]{.pre}](https://docs.python.org/3/library/functions.html#int &quot;(in Python v3.12)&quot;){.reference .external}[[,]{.pre}]{.p}[ ]{.w}[[str]{.pre}](https://docs.python.org/3/library/stdtypes.html#str &quot;(in Python v3.12)&quot;){.reference .external}[[\]]{.pre}]{.p}[[\]]{.pre}]{.p}]{.n}*, *[[default]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[[[Any]{.pre}](https://docs.python.org/3/library/typing.html#typing.Any &quot;(in Python v3.12)&quot;){.reference .external}]{.n}[ ]{.w}[[=]{.pre}]{.o}[ ]{.w}[[None]{.pre}]{.default_value}*[)]{.sig-paren} [[→]{.sig-return-icon} [[[Any]{.pre}](https://docs.python.org/3/library/typing.html#typing.Any &quot;(in Python v3.12)&quot;){.reference .external}]{.sig-return-typehint}]{.sig-return}[[[\[source\]]{.pre}]{.viewcode-link}](_modules/scrapy/settings.html#BaseSettings.get){.reference .internal}[¶](#scrapy.settings.BaseSettings.get &quot;Permalink to this definition&quot;){.headerlink}

:   Get a setting value without affecting its original type.

    Parameters

    :   -   **name**
            ([*str*](https://docs.python.org/3/library/stdtypes.html#str &quot;(in Python v3.12)&quot;){.reference
            .external}) -- the setting name

        -   **default**
            ([*object*](https://docs.python.org/3/library/functions.html#object &quot;(in Python v3.12)&quot;){.reference
            .external}) -- the value to return if no setting is
            found

[[getbool]{.pre}]{.sig-name .descname}[(]{.sig-paren}*[[name]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[[[Optional]{.pre}](https://docs.python.org/3/library/typing.html#typing.Optional &quot;(in Python v3.12)&quot;){.reference .external}[[\[]{.pre}]{.p}[[Union]{.pre}](https://docs.python.org/3/library/typing.html#typing.Union &quot;(in Python v3.12)&quot;){.reference .external}[[\[]{.pre}]{.p}[[bool]{.pre}](https://docs.python.org/3/library/functions.html#bool &quot;(in Python v3.12)&quot;){.reference .external}[[,]{.pre}]{.p}[ ]{.w}[[float]{.pre}](https://docs.python.org/3/library/functions.html#float &quot;(in Python v3.12)&quot;){.reference .external}[[,]{.pre}]{.p}[ ]{.w}[[int]{.pre}](https://docs.python.org/3/library/functions.html#int &quot;(in Python v3.12)&quot;){.reference .external}[[,]{.pre}]{.p}[ ]{.w}[[str]{.pre}](https://docs.python.org/3/library/stdtypes.html#str &quot;(in Python v3.12)&quot;){.reference .external}[[\]]{.pre}]{.p}[[\]]{.pre}]{.p}]{.n}*, *[[default]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[[[bool]{.pre}](https://docs.python.org/3/library/functions.html#bool &quot;(in Python v3.12)&quot;){.reference .external}]{.n}[ ]{.w}[[=]{.pre}]{.o}[ ]{.w}[[False]{.pre}]{.default_value}*[)]{.sig-paren} [[→]{.sig-return-icon} [[[bool]{.pre}](https://docs.python.org/3/library/functions.html#bool &quot;(in Python v3.12)&quot;){.reference .external}]{.sig-return-typehint}]{.sig-return}[[[\[source\]]{.pre}]{.viewcode-link}](_modules/scrapy/settings.html#BaseSettings.getbool){.reference .internal}[¶](#scrapy.settings.BaseSettings.getbool &quot;Permalink to this definition&quot;){.headerlink}

:   Get a setting value as a boolean.

    [`1`{.docutils .literal .notranslate}]{.pre}, [`'1'`{.docutils
    .literal .notranslate}]{.pre}, True\` and [`'True'`{.docutils
    .literal .notranslate}]{.pre} return [`True`{.docutils .literal
    .notranslate}]{.pre}, while [`0`{.docutils .literal
    .notranslate}]{.pre}, [`'0'`{.docutils .literal
    .notranslate}]{.pre}, [`False`{.docutils .literal
    .notranslate}]{.pre}, [`'False'`{.docutils .literal
    .notranslate}]{.pre} and [`None`{.docutils .literal
    .notranslate}]{.pre} return [`False`{.docutils .literal
    .notranslate}]{.pre}.

    For example, settings populated through environment variables
    set to [`'0'`{.docutils .literal .notranslate}]{.pre} will
    return [`False`{.docutils .literal .notranslate}]{.pre} when
    using this method.

    Parameters

    :   -   **name**
            ([*str*](https://docs.python.org/3/library/stdtypes.html#str &quot;(in Python v3.12)&quot;){.reference
            .external}) -- the setting name

        -   **default**
            ([*object*](https://docs.python.org/3/library/functions.html#object &quot;(in Python v3.12)&quot;){.reference
            .external}) -- the value to return if no setting is
            found

[[getdict]{.pre}]{.sig-name .descname}[(]{.sig-paren}*[[name]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[[[Optional]{.pre}](https://docs.python.org/3/library/typing.html#typing.Optional &quot;(in Python v3.12)&quot;){.reference .external}[[\[]{.pre}]{.p}[[Union]{.pre}](https://docs.python.org/3/library/typing.html#typing.Union &quot;(in Python v3.12)&quot;){.reference .external}[[\[]{.pre}]{.p}[[bool]{.pre}](https://docs.python.org/3/library/functions.html#bool &quot;(in Python v3.12)&quot;){.reference .external}[[,]{.pre}]{.p}[ ]{.w}[[float]{.pre}](https://docs.python.org/3/library/functions.html#float &quot;(in Python v3.12)&quot;){.reference .external}[[,]{.pre}]{.p}[ ]{.w}[[int]{.pre}](https://docs.python.org/3/library/functions.html#int &quot;(in Python v3.12)&quot;){.reference .external}[[,]{.pre}]{.p}[ ]{.w}[[str]{.pre}](https://docs.python.org/3/library/stdtypes.html#str &quot;(in Python v3.12)&quot;){.reference .external}[[\]]{.pre}]{.p}[[\]]{.pre}]{.p}]{.n}*, *[[default]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[[[Optional]{.pre}](https://docs.python.org/3/library/typing.html#typing.Optional &quot;(in Python v3.12)&quot;){.reference .external}[[\[]{.pre}]{.p}[[Dict]{.pre}](https://docs.python.org/3/library/typing.html#typing.Dict &quot;(in Python v3.12)&quot;){.reference .external}[[\[]{.pre}]{.p}[[Any]{.pre}](https://docs.python.org/3/library/typing.html#typing.Any &quot;(in Python v3.12)&quot;){.reference .external}[[,]{.pre}]{.p}[ ]{.w}[[Any]{.pre}](https://docs.python.org/3/library/typing.html#typing.Any &quot;(in Python v3.12)&quot;){.reference .external}[[\]]{.pre}]{.p}[[\]]{.pre}]{.p}]{.n}[ ]{.w}[[=]{.pre}]{.o}[ ]{.w}[[None]{.pre}]{.default_value}*[)]{.sig-paren} [[→]{.sig-return-icon} [[[Dict]{.pre}](https://docs.python.org/3/library/typing.html#typing.Dict &quot;(in Python v3.12)&quot;){.reference .external}[[\[]{.pre}]{.p}[[Any]{.pre}](https://docs.python.org/3/library/typing.html#typing.Any &quot;(in Python v3.12)&quot;){.reference .external}[[,]{.pre}]{.p}[ ]{.w}[[Any]{.pre}](https://docs.python.org/3/library/typing.html#typing.Any &quot;(in Python v3.12)&quot;){.reference .external}[[\]]{.pre}]{.p}]{.sig-return-typehint}]{.sig-return}[[[\[source\]]{.pre}]{.viewcode-link}](_modules/scrapy/settings.html#BaseSettings.getdict){.reference .internal}[¶](#scrapy.settings.BaseSettings.getdict &quot;Permalink to this definition&quot;){.headerlink}

:   Get a setting value as a dictionary. If the setting original
    type is a dictionary, a copy of it will be returned. If it is a
    string it will be evaluated as a JSON dictionary. In the case
    that it is a [[`BaseSettings`{.xref .py .py-class .docutils
    .literal
    .notranslate}]{.pre}](#scrapy.settings.BaseSettings &quot;scrapy.settings.BaseSettings&quot;){.reference
    .internal} instance itself, it will be converted to a
    dictionary, containing all its current settings values as they
    would be returned by [[`get()`{.xref .py .py-meth .docutils
    .literal
    .notranslate}]{.pre}](#scrapy.settings.BaseSettings.get &quot;scrapy.settings.BaseSettings.get&quot;){.reference
    .internal}, and losing all information about priority and
    mutability.

    Parameters

    :   -   **name**
            ([*str*](https://docs.python.org/3/library/stdtypes.html#str &quot;(in Python v3.12)&quot;){.reference
            .external}) -- the setting name

        -   **default**
            ([*object*](https://docs.python.org/3/library/functions.html#object &quot;(in Python v3.12)&quot;){.reference
            .external}) -- the value to return if no setting is
            found

[[getdictorlist]{.pre}]{.sig-name .descname}[(]{.sig-paren}*[[name]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[[[Optional]{.pre}](https://docs.python.org/3/library/typing.html#typing.Optional &quot;(in Python v3.12)&quot;){.reference .external}[[\[]{.pre}]{.p}[[Union]{.pre}](https://docs.python.org/3/library/typing.html#typing.Union &quot;(in Python v3.12)&quot;){.reference .external}[[\[]{.pre}]{.p}[[bool]{.pre}](https://docs.python.org/3/library/functions.html#bool &quot;(in Python v3.12)&quot;){.reference .external}[[,]{.pre}]{.p}[ ]{.w}[[float]{.pre}](https://docs.python.org/3/library/functions.html#float &quot;(in Python v3.12)&quot;){.reference .external}[[,]{.pre}]{.p}[ ]{.w}[[int]{.pre}](https://docs.python.org/3/library/functions.html#int &quot;(in Python v3.12)&quot;){.reference .external}[[,]{.pre}]{.p}[ ]{.w}[[str]{.pre}](https://docs.python.org/3/library/stdtypes.html#str &quot;(in Python v3.12)&quot;){.reference .external}[[\]]{.pre}]{.p}[[\]]{.pre}]{.p}]{.n}*, *[[default]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[[[Optional]{.pre}](https://docs.python.org/3/library/typing.html#typing.Optional &quot;(in Python v3.12)&quot;){.reference .external}[[\[]{.pre}]{.p}[[Union]{.pre}](https://docs.python.org/3/library/typing.html#typing.Union &quot;(in Python v3.12)&quot;){.reference .external}[[\[]{.pre}]{.p}[[Dict]{.pre}](https://docs.python.org/3/library/typing.html#typing.Dict &quot;(in Python v3.12)&quot;){.reference .external}[[\[]{.pre}]{.p}[[Any]{.pre}](https://docs.python.org/3/library/typing.html#typing.Any &quot;(in Python v3.12)&quot;){.reference .external}[[,]{.pre}]{.p}[ ]{.w}[[Any]{.pre}](https://docs.python.org/3/library/typing.html#typing.Any &quot;(in Python v3.12)&quot;){.reference .external}[[\]]{.pre}]{.p}[[,]{.pre}]{.p}[ ]{.w}[[List]{.pre}](https://docs.python.org/3/library/typing.html#typing.List &quot;(in Python v3.12)&quot;){.reference .external}[[\[]{.pre}]{.p}[[Any]{.pre}](https://docs.python.org/3/library/typing.html#typing.Any &quot;(in Python v3.12)&quot;){.reference .external}[[\]]{.pre}]{.p}[[,]{.pre}]{.p}[ ]{.w}[[Tuple]{.pre}](https://docs.python.org/3/library/typing.html#typing.Tuple &quot;(in Python v3.12)&quot;){.reference .external}[[\[]{.pre}]{.p}[[Any]{.pre}](https://docs.python.org/3/library/typing.html#typing.Any &quot;(in Python v3.12)&quot;){.reference .external}[[\]]{.pre}]{.p}[[\]]{.pre}]{.p}[[\]]{.pre}]{.p}]{.n}[ ]{.w}[[=]{.pre}]{.o}[ ]{.w}[[None]{.pre}]{.default_value}*[)]{.sig-paren} [[→]{.sig-return-icon} [[[Union]{.pre}](https://docs.python.org/3/library/typing.html#typing.Union &quot;(in Python v3.12)&quot;){.reference .external}[[\[]{.pre}]{.p}[[Dict]{.pre}](https://docs.python.org/3/library/typing.html#typing.Dict &quot;(in Python v3.12)&quot;){.reference .external}[[\[]{.pre}]{.p}[[Any]{.pre}](https://docs.python.org/3/library/typing.html#typing.Any &quot;(in Python v3.12)&quot;){.reference .external}[[,]{.pre}]{.p}[ ]{.w}[[Any]{.pre}](https://docs.python.org/3/library/typing.html#typing.Any &quot;(in Python v3.12)&quot;){.reference .external}[[\]]{.pre}]{.p}[[,]{.pre}]{.p}[ ]{.w}[[List]{.pre}](https://docs.python.org/3/library/typing.html#typing.List &quot;(in Python v3.12)&quot;){.reference .external}[[\[]{.pre}]{.p}[[Any]{.pre}](https://docs.python.org/3/library/typing.html#typing.Any &quot;(in Python v3.12)&quot;){.reference .external}[[\]]{.pre}]{.p}[[\]]{.pre}]{.p}]{.sig-return-typehint}]{.sig-return}[[[\[source\]]{.pre}]{.viewcode-link}](_modules/scrapy/settings.html#BaseSettings.getdictorlist){.reference .internal}[¶](#scrapy.settings.BaseSettings.getdictorlist &quot;Permalink to this definition&quot;){.headerlink}

:   Get a setting value as either a [[`dict`{.xref .py .py-class
    .docutils .literal
    .notranslate}]{.pre}](https://docs.python.org/3/library/stdtypes.html#dict &quot;(in Python v3.12)&quot;){.reference
    .external} or a [[`list`{.xref .py .py-class .docutils .literal
    .notranslate}]{.pre}](https://docs.python.org/3/library/stdtypes.html#list &quot;(in Python v3.12)&quot;){.reference
    .external}.

    If the setting is already a dict or a list, a copy of it will be
    returned.

    If it is a string it will be evaluated as JSON, or as a
    comma-separated list of strings as a fallback.

    For example, settings populated from the command line will
    return:

    -   [`{'key1':`{.docutils .literal
        .notranslate}]{.pre}` `{.docutils .literal
        .notranslate}[`'value1',`{.docutils .literal
        .notranslate}]{.pre}` `{.docutils .literal
        .notranslate}[`'key2':`{.docutils .literal
        .notranslate}]{.pre}` `{.docutils .literal
        .notranslate}[`'value2'}`{.docutils .literal
        .notranslate}]{.pre} if set to [`'{&quot;key1&quot;:`{.docutils
        .literal .notranslate}]{.pre}` `{.docutils .literal
        .notranslate}[`&quot;value1&quot;,`{.docutils .literal
        .notranslate}]{.pre}` `{.docutils .literal
        .notranslate}[`&quot;key2&quot;:`{.docutils .literal
        .notranslate}]{.pre}` `{.docutils .literal
        .notranslate}[`&quot;value2&quot;}'`{.docutils .literal
        .notranslate}]{.pre}

    -   [`['one',`{.docutils .literal
        .notranslate}]{.pre}` `{.docutils .literal
        .notranslate}[`'two']`{.docutils .literal
        .notranslate}]{.pre} if set to [`'[&quot;one&quot;,`{.docutils
        .literal .notranslate}]{.pre}` `{.docutils .literal
        .notranslate}[`&quot;two&quot;]'`{.docutils .literal
        .notranslate}]{.pre} or [`'one,two'`{.docutils .literal
        .notranslate}]{.pre}

    Parameters

    :   -   **name** (*string*) -- the setting name

        -   **default** (*any*) -- the value to return if no setting
            is found

[[getfloat]{.pre}]{.sig-name .descname}[(]{.sig-paren}*[[name]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[[[Optional]{.pre}](https://docs.python.org/3/library/typing.html#typing.Optional &quot;(in Python v3.12)&quot;){.reference .external}[[\[]{.pre}]{.p}[[Union]{.pre}](https://docs.python.org/3/library/typing.html#typing.Union &quot;(in Python v3.12)&quot;){.reference .external}[[\[]{.pre}]{.p}[[bool]{.pre}](https://docs.python.org/3/library/functions.html#bool &quot;(in Python v3.12)&quot;){.reference .external}[[,]{.pre}]{.p}[ ]{.w}[[float]{.pre}](https://docs.python.org/3/library/functions.html#float &quot;(in Python v3.12)&quot;){.reference .external}[[,]{.pre}]{.p}[ ]{.w}[[int]{.pre}](https://docs.python.org/3/library/functions.html#int &quot;(in Python v3.12)&quot;){.reference .external}[[,]{.pre}]{.p}[ ]{.w}[[str]{.pre}](https://docs.python.org/3/library/stdtypes.html#str &quot;(in Python v3.12)&quot;){.reference .external}[[\]]{.pre}]{.p}[[\]]{.pre}]{.p}]{.n}*, *[[default]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[[[float]{.pre}](https://docs.python.org/3/library/functions.html#float &quot;(in Python v3.12)&quot;){.reference .external}]{.n}[ ]{.w}[[=]{.pre}]{.o}[ ]{.w}[[0.0]{.pre}]{.default_value}*[)]{.sig-paren} [[→]{.sig-return-icon} [[[float]{.pre}](https://docs.python.org/3/library/functions.html#float &quot;(in Python v3.12)&quot;){.reference .external}]{.sig-return-typehint}]{.sig-return}[[[\[source\]]{.pre}]{.viewcode-link}](_modules/scrapy/settings.html#BaseSettings.getfloat){.reference .internal}[¶](#scrapy.settings.BaseSettings.getfloat &quot;Permalink to this definition&quot;){.headerlink}

:   Get a setting value as a float.

    Parameters

    :   -   **name**
            ([*str*](https://docs.python.org/3/library/stdtypes.html#str &quot;(in Python v3.12)&quot;){.reference
            .external}) -- the setting name

        -   **default**
            ([*object*](https://docs.python.org/3/library/functions.html#object &quot;(in Python v3.12)&quot;){.reference
            .external}) -- the value to return if no setting is
            found

[[getint]{.pre}]{.sig-name .descname}[(]{.sig-paren}*[[name]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[[[Optional]{.pre}](https://docs.python.org/3/library/typing.html#typing.Optional &quot;(in Python v3.12)&quot;){.reference .external}[[\[]{.pre}]{.p}[[Union]{.pre}](https://docs.python.org/3/library/typing.html#typing.Union &quot;(in Python v3.12)&quot;){.reference .external}[[\[]{.pre}]{.p}[[bool]{.pre}](https://docs.python.org/3/library/functions.html#bool &quot;(in Python v3.12)&quot;){.reference .external}[[,]{.pre}]{.p}[ ]{.w}[[float]{.pre}](https://docs.python.org/3/library/functions.html#float &quot;(in Python v3.12)&quot;){.reference .external}[[,]{.pre}]{.p}[ ]{.w}[[int]{.pre}](https://docs.python.org/3/library/functions.html#int &quot;(in Python v3.12)&quot;){.reference .external}[[,]{.pre}]{.p}[ ]{.w}[[str]{.pre}](https://docs.python.org/3/library/stdtypes.html#str &quot;(in Python v3.12)&quot;){.reference .external}[[\]]{.pre}]{.p}[[\]]{.pre}]{.p}]{.n}*, *[[default]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[[[int]{.pre}](https://docs.python.org/3/library/functions.html#int &quot;(in Python v3.12)&quot;){.reference .external}]{.n}[ ]{.w}[[=]{.pre}]{.o}[ ]{.w}[[0]{.pre}]{.default_value}*[)]{.sig-paren} [[→]{.sig-return-icon} [[[int]{.pre}](https://docs.python.org/3/library/functions.html#int &quot;(in Python v3.12)&quot;){.reference .external}]{.sig-return-typehint}]{.sig-return}[[[\[source\]]{.pre}]{.viewcode-link}](_modules/scrapy/settings.html#BaseSettings.getint){.reference .internal}[¶](#scrapy.settings.BaseSettings.getint &quot;Permalink to this definition&quot;){.headerlink}

:   Get a setting value as an int.

    Parameters

    :   -   **name**
            ([*str*](https://docs.python.org/3/library/stdtypes.html#str &quot;(in Python v3.12)&quot;){.reference
            .external}) -- the setting name

        -   **default**
            ([*object*](https://docs.python.org/3/library/functions.html#object &quot;(in Python v3.12)&quot;){.reference
            .external}) -- the value to return if no setting is
            found

[[getlist]{.pre}]{.sig-name .descname}[(]{.sig-paren}*[[name]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[[[Optional]{.pre}](https://docs.python.org/3/library/typing.html#typing.Optional &quot;(in Python v3.12)&quot;){.reference .external}[[\[]{.pre}]{.p}[[Union]{.pre}](https://docs.python.org/3/library/typing.html#typing.Union &quot;(in Python v3.12)&quot;){.reference .external}[[\[]{.pre}]{.p}[[bool]{.pre}](https://docs.python.org/3/library/functions.html#bool &quot;(in Python v3.12)&quot;){.reference .external}[[,]{.pre}]{.p}[ ]{.w}[[float]{.pre}](https://docs.python.org/3/library/functions.html#float &quot;(in Python v3.12)&quot;){.reference .external}[[,]{.pre}]{.p}[ ]{.w}[[int]{.pre}](https://docs.python.org/3/library/functions.html#int &quot;(in Python v3.12)&quot;){.reference .external}[[,]{.pre}]{.p}[ ]{.w}[[str]{.pre}](https://docs.python.org/3/library/stdtypes.html#str &quot;(in Python v3.12)&quot;){.reference .external}[[\]]{.pre}]{.p}[[\]]{.pre}]{.p}]{.n}*, *[[default]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[[[Optional]{.pre}](https://docs.python.org/3/library/typing.html#typing.Optional &quot;(in Python v3.12)&quot;){.reference .external}[[\[]{.pre}]{.p}[[List]{.pre}](https://docs.python.org/3/library/typing.html#typing.List &quot;(in Python v3.12)&quot;){.reference .external}[[\[]{.pre}]{.p}[[Any]{.pre}](https://docs.python.org/3/library/typing.html#typing.Any &quot;(in Python v3.12)&quot;){.reference .external}[[\]]{.pre}]{.p}[[\]]{.pre}]{.p}]{.n}[ ]{.w}[[=]{.pre}]{.o}[ ]{.w}[[None]{.pre}]{.default_value}*[)]{.sig-paren} [[→]{.sig-return-icon} [[[List]{.pre}](https://docs.python.org/3/library/typing.html#typing.List &quot;(in Python v3.12)&quot;){.reference .external}[[\[]{.pre}]{.p}[[Any]{.pre}](https://docs.python.org/3/library/typing.html#typing.Any &quot;(in Python v3.12)&quot;){.reference .external}[[\]]{.pre}]{.p}]{.sig-return-typehint}]{.sig-return}[[[\[source\]]{.pre}]{.viewcode-link}](_modules/scrapy/settings.html#BaseSettings.getlist){.reference .internal}[¶](#scrapy.settings.BaseSettings.getlist &quot;Permalink to this definition&quot;){.headerlink}

:   Get a setting value as a list. If the setting original type is a
    list, a copy of it will be returned. If it's a string it will be
    split by &quot;,&quot;.

    For example, settings populated through environment variables
    set to [`'one,two'`{.docutils .literal .notranslate}]{.pre} will
    return a list \['one', 'two'\] when using this method.

    Parameters

    :   -   **name**
            ([*str*](https://docs.python.org/3/library/stdtypes.html#str &quot;(in Python v3.12)&quot;){.reference
            .external}) -- the setting name

        -   **default**
            ([*object*](https://docs.python.org/3/library/functions.html#object &quot;(in Python v3.12)&quot;){.reference
            .external}) -- the value to return if no setting is
            found

[[getpriority]{.pre}]{.sig-name .descname}[(]{.sig-paren}*[[name]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[[[Optional]{.pre}](https://docs.python.org/3/library/typing.html#typing.Optional &quot;(in Python v3.12)&quot;){.reference .external}[[\[]{.pre}]{.p}[[Union]{.pre}](https://docs.python.org/3/library/typing.html#typing.Union &quot;(in Python v3.12)&quot;){.reference .external}[[\[]{.pre}]{.p}[[bool]{.pre}](https://docs.python.org/3/library/functions.html#bool &quot;(in Python v3.12)&quot;){.reference .external}[[,]{.pre}]{.p}[ ]{.w}[[float]{.pre}](https://docs.python.org/3/library/functions.html#float &quot;(in Python v3.12)&quot;){.reference .external}[[,]{.pre}]{.p}[ ]{.w}[[int]{.pre}](https://docs.python.org/3/library/functions.html#int &quot;(in Python v3.12)&quot;){.reference .external}[[,]{.pre}]{.p}[ ]{.w}[[str]{.pre}](https://docs.python.org/3/library/stdtypes.html#str &quot;(in Python v3.12)&quot;){.reference .external}[[\]]{.pre}]{.p}[[\]]{.pre}]{.p}]{.n}*[)]{.sig-paren} [[→]{.sig-return-icon} [[[Optional]{.pre}](https://docs.python.org/3/library/typing.html#typing.Optional &quot;(in Python v3.12)&quot;){.reference .external}[[\[]{.pre}]{.p}[[int]{.pre}](https://docs.python.org/3/library/functions.html#int &quot;(in Python v3.12)&quot;){.reference .external}[[\]]{.pre}]{.p}]{.sig-return-typehint}]{.sig-return}[[[\[source\]]{.pre}]{.viewcode-link}](_modules/scrapy/settings.html#BaseSettings.getpriority){.reference .internal}[¶](#scrapy.settings.BaseSettings.getpriority &quot;Permalink to this definition&quot;){.headerlink}

:   Return the current numerical priority value of a setting, or
    [`None`{.docutils .literal .notranslate}]{.pre} if the given
    [`name`{.docutils .literal .notranslate}]{.pre} does not exist.

    Parameters

    :   **name**
        ([*str*](https://docs.python.org/3/library/stdtypes.html#str &quot;(in Python v3.12)&quot;){.reference
        .external}) -- the setting name

[[getwithbase]{.pre}]{.sig-name .descname}[(]{.sig-paren}*[[name]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[[[Optional]{.pre}](https://docs.python.org/3/library/typing.html#typing.Optional &quot;(in Python v3.12)&quot;){.reference .external}[[\[]{.pre}]{.p}[[Union]{.pre}](https://docs.python.org/3/library/typing.html#typing.Union &quot;(in Python v3.12)&quot;){.reference .external}[[\[]{.pre}]{.p}[[bool]{.pre}](https://docs.python.org/3/library/functions.html#bool &quot;(in Python v3.12)&quot;){.reference .external}[[,]{.pre}]{.p}[ ]{.w}[[float]{.pre}](https://docs.python.org/3/library/functions.html#float &quot;(in Python v3.12)&quot;){.reference .external}[[,]{.pre}]{.p}[ ]{.w}[[int]{.pre}](https://docs.python.org/3/library/functions.html#int &quot;(in Python v3.12)&quot;){.reference .external}[[,]{.pre}]{.p}[ ]{.w}[[str]{.pre}](https://docs.python.org/3/library/stdtypes.html#str &quot;(in Python v3.12)&quot;){.reference .external}[[\]]{.pre}]{.p}[[\]]{.pre}]{.p}]{.n}*[)]{.sig-paren} [[→]{.sig-return-icon} [[[BaseSettings]{.pre}](index.html#scrapy.settings.BaseSettings &quot;scrapy.settings.BaseSettings&quot;){.reference .internal}]{.sig-return-typehint}]{.sig-return}[[[\[source\]]{.pre}]{.viewcode-link}](_modules/scrapy/settings.html#BaseSettings.getwithbase){.reference .internal}[¶](#scrapy.settings.BaseSettings.getwithbase &quot;Permalink to this definition&quot;){.headerlink}

:   Get a composition of a dictionary-like setting and its \_BASE
    counterpart.

    Parameters

    :   **name**
        ([*str*](https://docs.python.org/3/library/stdtypes.html#str &quot;(in Python v3.12)&quot;){.reference
        .external}) -- name of the dictionary-like setting

[[maxpriority]{.pre}]{.sig-name .descname}[(]{.sig-paren}[)]{.sig-paren} [[→]{.sig-return-icon} [[[int]{.pre}](https://docs.python.org/3/library/functions.html#int &quot;(in Python v3.12)&quot;){.reference .external}]{.sig-return-typehint}]{.sig-return}[[[\[source\]]{.pre}]{.viewcode-link}](_modules/scrapy/settings.html#BaseSettings.maxpriority){.reference .internal}[¶](#scrapy.settings.BaseSettings.maxpriority &quot;Permalink to this definition&quot;){.headerlink}

:   Return the numerical value of the highest priority present
    throughout all settings, or the numerical value for
    [`default`{.docutils .literal .notranslate}]{.pre} from
    [[`SETTINGS_PRIORITIES`{.xref .py .py-attr .docutils .literal
    .notranslate}]{.pre}](#scrapy.settings.SETTINGS_PRIORITIES &quot;scrapy.settings.SETTINGS_PRIORITIES&quot;){.reference
    .internal} if there are no settings stored.

[[pop]{.pre}]{.sig-name .descname}[(]{.sig-paren}*[[k]{.pre}]{.n}*[\[]{.optional}, *[[d]{.pre}]{.n}*[\]]{.optional}[)]{.sig-paren} [[→]{.sig-return-icon} [[v,]{.pre} [remove]{.pre} [specified]{.pre} [key]{.pre} [and]{.pre} [return]{.pre} [the]{.pre} [corresponding]{.pre} [value.]{.pre}]{.sig-return-typehint}]{.sig-return}[[[\[source\]]{.pre}]{.viewcode-link}](_modules/scrapy/settings.html#BaseSettings.pop){.reference .internal}[¶](#scrapy.settings.BaseSettings.pop &quot;Permalink to this definition&quot;){.headerlink}

:   If key is not found, d is returned if given, otherwise KeyError
    is raised.

[[set]{.pre}]{.sig-name .descname}[(]{.sig-paren}*[[name]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[[[Optional]{.pre}](https://docs.python.org/3/library/typing.html#typing.Optional &quot;(in Python v3.12)&quot;){.reference .external}[[\[]{.pre}]{.p}[[Union]{.pre}](https://docs.python.org/3/library/typing.html#typing.Union &quot;(in Python v3.12)&quot;){.reference .external}[[\[]{.pre}]{.p}[[bool]{.pre}](https://docs.python.org/3/library/functions.html#bool &quot;(in Python v3.12)&quot;){.reference .external}[[,]{.pre}]{.p}[ ]{.w}[[float]{.pre}](https://docs.python.org/3/library/functions.html#float &quot;(in Python v3.12)&quot;){.reference .external}[[,]{.pre}]{.p}[ ]{.w}[[int]{.pre}](https://docs.python.org/3/library/functions.html#int &quot;(in Python v3.12)&quot;){.reference .external}[[,]{.pre}]{.p}[ ]{.w}[[str]{.pre}](https://docs.python.org/3/library/stdtypes.html#str &quot;(in Python v3.12)&quot;){.reference .external}[[\]]{.pre}]{.p}[[\]]{.pre}]{.p}]{.n}*, *[[value]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[[[Any]{.pre}](https://docs.python.org/3/library/typing.html#typing.Any &quot;(in Python v3.12)&quot;){.reference .external}]{.n}*, *[[priority]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[[[Union]{.pre}](https://docs.python.org/3/library/typing.html#typing.Union &quot;(in Python v3.12)&quot;){.reference .external}[[\[]{.pre}]{.p}[[int]{.pre}](https://docs.python.org/3/library/functions.html#int &quot;(in Python v3.12)&quot;){.reference .external}[[,]{.pre}]{.p}[ ]{.w}[[str]{.pre}](https://docs.python.org/3/library/stdtypes.html#str &quot;(in Python v3.12)&quot;){.reference .external}[[\]]{.pre}]{.p}]{.n}[ ]{.w}[[=]{.pre}]{.o}[ ]{.w}[[\'project\']{.pre}]{.default_value}*[)]{.sig-paren} [[→]{.sig-return-icon} [[[None]{.pre}](https://docs.python.org/3/library/constants.html#None &quot;(in Python v3.12)&quot;){.reference .external}]{.sig-return-typehint}]{.sig-return}[[[\[source\]]{.pre}]{.viewcode-link}](_modules/scrapy/settings.html#BaseSettings.set){.reference .internal}[¶](#scrapy.settings.BaseSettings.set &quot;Permalink to this definition&quot;){.headerlink}

:   Store a key/value attribute with a given priority.

    Settings should be populated *before* configuring the Crawler
    object (through the [`configure()`{.xref .py .py-meth .docutils
    .literal .notranslate}]{.pre} method), otherwise they won't have
    any effect.

    Parameters

    :   -   **name**
            ([*str*](https://docs.python.org/3/library/stdtypes.html#str &quot;(in Python v3.12)&quot;){.reference
            .external}) -- the setting name

        -   **value**
            ([*object*](https://docs.python.org/3/library/functions.html#object &quot;(in Python v3.12)&quot;){.reference
            .external}) -- the value to associate with the setting

        -   **priority**
            ([*str*](https://docs.python.org/3/library/stdtypes.html#str &quot;(in Python v3.12)&quot;){.reference
            .external} *or*
            [*int*](https://docs.python.org/3/library/functions.html#int &quot;(in Python v3.12)&quot;){.reference
            .external}) -- the priority of the setting. Should be a
            key of [[`SETTINGS_PRIORITIES`{.xref .py .py-attr
            .docutils .literal
            .notranslate}]{.pre}](#scrapy.settings.SETTINGS_PRIORITIES &quot;scrapy.settings.SETTINGS_PRIORITIES&quot;){.reference
            .internal} or an integer

[[setdefault]{.pre}]{.sig-name .descname}[(]{.sig-paren}*[[k]{.pre}]{.n}*[\[]{.optional}, *[[d]{.pre}]{.n}*[\]]{.optional}[)]{.sig-paren} [[→]{.sig-return-icon} [[D.get(k,d),]{.pre} [also]{.pre} [set]{.pre} [D\[k\]=d]{.pre} [if]{.pre} [k]{.pre} [not]{.pre} [in]{.pre} [D]{.pre}]{.sig-return-typehint}]{.sig-return}[[[\[source\]]{.pre}]{.viewcode-link}](_modules/scrapy/settings.html#BaseSettings.setdefault){.reference .internal}[¶](#scrapy.settings.BaseSettings.setdefault &quot;Permalink to this definition&quot;){.headerlink}

:   

[[setmodule]{.pre}]{.sig-name .descname}[(]{.sig-paren}*[[module]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[[[Union]{.pre}](https://docs.python.org/3/library/typing.html#typing.Union &quot;(in Python v3.12)&quot;){.reference .external}[[\[]{.pre}]{.p}[module]{.pre}[[,]{.pre}]{.p}[ ]{.w}[[str]{.pre}](https://docs.python.org/3/library/stdtypes.html#str &quot;(in Python v3.12)&quot;){.reference .external}[[\]]{.pre}]{.p}]{.n}*, *[[priority]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[[[Union]{.pre}](https://docs.python.org/3/library/typing.html#typing.Union &quot;(in Python v3.12)&quot;){.reference .external}[[\[]{.pre}]{.p}[[int]{.pre}](https://docs.python.org/3/library/functions.html#int &quot;(in Python v3.12)&quot;){.reference .external}[[,]{.pre}]{.p}[ ]{.w}[[str]{.pre}](https://docs.python.org/3/library/stdtypes.html#str &quot;(in Python v3.12)&quot;){.reference .external}[[\]]{.pre}]{.p}]{.n}[ ]{.w}[[=]{.pre}]{.o}[ ]{.w}[[\'project\']{.pre}]{.default_value}*[)]{.sig-paren} [[→]{.sig-return-icon} [[[None]{.pre}](https://docs.python.org/3/library/constants.html#None &quot;(in Python v3.12)&quot;){.reference .external}]{.sig-return-typehint}]{.sig-return}[[[\[source\]]{.pre}]{.viewcode-link}](_modules/scrapy/settings.html#BaseSettings.setmodule){.reference .internal}[¶](#scrapy.settings.BaseSettings.setmodule &quot;Permalink to this definition&quot;){.headerlink}

:   Store settings from a module with a given priority.

    This is a helper function that calls [[`set()`{.xref .py
    .py-meth .docutils .literal
    .notranslate}]{.pre}](#scrapy.settings.BaseSettings.set &quot;scrapy.settings.BaseSettings.set&quot;){.reference
    .internal} for every globally declared uppercase variable of
    [`module`{.docutils .literal .notranslate}]{.pre} with the
    provided [`priority`{.docutils .literal .notranslate}]{.pre}.

    Parameters

    :   -   **module**
            ([*types.ModuleType*](https://docs.python.org/3/library/types.html#types.ModuleType &quot;(in Python v3.12)&quot;){.reference
            .external} *or*
            [*str*](https://docs.python.org/3/library/stdtypes.html#str &quot;(in Python v3.12)&quot;){.reference
            .external}) -- the module or the path of the module

        -   **priority**
            ([*str*](https://docs.python.org/3/library/stdtypes.html#str &quot;(in Python v3.12)&quot;){.reference
            .external} *or*
            [*int*](https://docs.python.org/3/library/functions.html#int &quot;(in Python v3.12)&quot;){.reference
            .external}) -- the priority of the settings. Should be a
            key of [[`SETTINGS_PRIORITIES`{.xref .py .py-attr
            .docutils .literal
            .notranslate}]{.pre}](#scrapy.settings.SETTINGS_PRIORITIES &quot;scrapy.settings.SETTINGS_PRIORITIES&quot;){.reference
            .internal} or an integer

[[update]{.pre}]{.sig-name .descname}[(]{.sig-paren}*[[values]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[[\_SettingsInputT]{.pre}]{.n}*, *[[priority]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[[Union]{.pre}[[\[]{.pre}]{.p}[[int]{.pre}](https://docs.python.org/3/library/functions.html#int &quot;(in Python v3.12)&quot;){.reference .external}[[,]{.pre}]{.p}[ ]{.w}[[str]{.pre}](https://docs.python.org/3/library/stdtypes.html#str &quot;(in Python v3.12)&quot;){.reference .external}[[\]]{.pre}]{.p}]{.n}[ ]{.w}[[=]{.pre}]{.o}[ ]{.w}[[\'project\']{.pre}]{.default_value}*[)]{.sig-paren} [[→]{.sig-return-icon} [[[None]{.pre}](https://docs.python.org/3/library/constants.html#None &quot;(in Python v3.12)&quot;){.reference .external}]{.sig-return-typehint}]{.sig-return}[[[\[source\]]{.pre}]{.viewcode-link}](_modules/scrapy/settings.html#BaseSettings.update){.reference .internal}[¶](#scrapy.settings.BaseSettings.update &quot;Permalink to this definition&quot;){.headerlink}

:   Store key/value pairs with a given priority.

    This is a helper function that calls [[`set()`{.xref .py
    .py-meth .docutils .literal
    .notranslate}]{.pre}](#scrapy.settings.BaseSettings.set &quot;scrapy.settings.BaseSettings.set&quot;){.reference
    .internal} for every item of [`values`{.docutils .literal
    .notranslate}]{.pre} with the provided [`priority`{.docutils
    .literal .notranslate}]{.pre}.

    If [`values`{.docutils .literal .notranslate}]{.pre} is a
    string, it is assumed to be JSON-encoded and parsed into a dict
    with [`json.loads()`{.docutils .literal .notranslate}]{.pre}
    first. If it is a [[`BaseSettings`{.xref .py .py-class .docutils
    .literal
    .notranslate}]{.pre}](#scrapy.settings.BaseSettings &quot;scrapy.settings.BaseSettings&quot;){.reference
    .internal} instance, the per-key priorities will be used and the
    [`priority`{.docutils .literal .notranslate}]{.pre} parameter
    ignored. This allows inserting/updating settings with different
    priorities with a single command.

    Parameters

    :   -   **values** (dict or string or [[`BaseSettings`{.xref .py
            .py-class .docutils .literal
            .notranslate}]{.pre}](#scrapy.settings.BaseSettings &quot;scrapy.settings.BaseSettings&quot;){.reference
            .internal}) -- the settings names and values

        -   **priority**
            ([*str*](https://docs.python.org/3/library/stdtypes.html#str &quot;(in Python v3.12)&quot;){.reference
            .external} *or*
            [*int*](https://docs.python.org/3/library/functions.html#int &quot;(in Python v3.12)&quot;){.reference
            .external}) -- the priority of the settings. Should be a
            key of [[`SETTINGS_PRIORITIES`{.xref .py .py-attr
            .docutils .literal
            .notranslate}]{.pre}](#scrapy.settings.SETTINGS_PRIORITIES &quot;scrapy.settings.SETTINGS_PRIORITIES&quot;){.reference
            .internal} or an integer
</code></pre>
<p>:::</p>
<p>::: {#module-scrapy.spiderloader .section}
[]{#spiderloader-api}[]{#topics-api-spiderloader}</p>
<h4 id="spiderloader-apiheaderlink"><a class="header" href="#spiderloader-apiheaderlink">SpiderLoader API<a href="#module-scrapy.spiderloader" title="Permalink to this heading">¶</a>{.headerlink}</a></h4>
<p><em>[class]{.pre}[ ]{.w}</em>[[scrapy.spiderloader.]{.pre}]{.sig-prename .descclassname}[[SpiderLoader]{.pre}]{.sig-name .descname}<a href="_modules/scrapy/spiderloader.html#SpiderLoader">[[[source]]{.pre}]{.viewcode-link}</a>{.reference .internal}<a href="#scrapy.spiderloader.SpiderLoader" title="Permalink to this definition">¶</a>{.headerlink}</p>
<p>:   This class is in charge of retrieving and handling the spider
classes defined across the project.</p>
<pre><code>Custom spider loaders can be employed by specifying their path in
the [[`SPIDER_LOADER_CLASS`{.xref .std .std-setting .docutils
.literal
.notranslate}]{.pre}](index.html#std-setting-SPIDER_LOADER_CLASS){.hoverxref
.tooltip .reference .internal} project setting. They must fully
implement the [`scrapy.interfaces.ISpiderLoader`{.xref .py .py-class
.docutils .literal .notranslate}]{.pre} interface to guarantee an
errorless execution.

[[from_settings]{.pre}]{.sig-name .descname}[(]{.sig-paren}*[[settings]{.pre}]{.n}*[)]{.sig-paren}[[[\[source\]]{.pre}]{.viewcode-link}](_modules/scrapy/spiderloader.html#SpiderLoader.from_settings){.reference .internal}[¶](#scrapy.spiderloader.SpiderLoader.from_settings &quot;Permalink to this definition&quot;){.headerlink}

:   This class method is used by Scrapy to create an instance of the
    class. It's called with the current project settings, and it
    loads the spiders found recursively in the modules of the
    [[`SPIDER_MODULES`{.xref .std .std-setting .docutils .literal
    .notranslate}]{.pre}](index.html#std-setting-SPIDER_MODULES){.hoverxref
    .tooltip .reference .internal} setting.

    Parameters

    :   **settings** ([[`Settings`{.xref .py .py-class .docutils
        .literal
        .notranslate}]{.pre}](#scrapy.settings.Settings &quot;scrapy.settings.Settings&quot;){.reference
        .internal} instance) -- project settings

[[load]{.pre}]{.sig-name .descname}[(]{.sig-paren}*[[spider_name]{.pre}]{.n}*[)]{.sig-paren}[[[\[source\]]{.pre}]{.viewcode-link}](_modules/scrapy/spiderloader.html#SpiderLoader.load){.reference .internal}[¶](#scrapy.spiderloader.SpiderLoader.load &quot;Permalink to this definition&quot;){.headerlink}

:   Get the Spider class with the given name. It'll look into the
    previously loaded spiders for a spider class with name
    [`spider_name`{.docutils .literal .notranslate}]{.pre} and will
    raise a KeyError if not found.

    Parameters

    :   **spider_name**
        ([*str*](https://docs.python.org/3/library/stdtypes.html#str &quot;(in Python v3.12)&quot;){.reference
        .external}) -- spider class name

[[list]{.pre}]{.sig-name .descname}[(]{.sig-paren}[)]{.sig-paren}[[[\[source\]]{.pre}]{.viewcode-link}](_modules/scrapy/spiderloader.html#SpiderLoader.list){.reference .internal}[¶](#scrapy.spiderloader.SpiderLoader.list &quot;Permalink to this definition&quot;){.headerlink}

:   Get the names of the available spiders in the project.

[[find_by_request]{.pre}]{.sig-name .descname}[(]{.sig-paren}*[[request]{.pre}]{.n}*[)]{.sig-paren}[[[\[source\]]{.pre}]{.viewcode-link}](_modules/scrapy/spiderloader.html#SpiderLoader.find_by_request){.reference .internal}[¶](#scrapy.spiderloader.SpiderLoader.find_by_request &quot;Permalink to this definition&quot;){.headerlink}

:   List the spiders' names that can handle the given request. Will
    try to match the request's url against the domains of the
    spiders.

    Parameters

    :   **request** ([`Request`{.xref .py .py-class .docutils
        .literal .notranslate}]{.pre} instance) -- queried request
</code></pre>
<p>:::</p>
<p>::: {#module-scrapy.signalmanager .section}
[]{#signals-api}[]{#topics-api-signals}</p>
<h4 id="signals-apiheaderlink"><a class="header" href="#signals-apiheaderlink">Signals API<a href="#module-scrapy.signalmanager" title="Permalink to this heading">¶</a>{.headerlink}</a></h4>
<p><em>[class]{.pre}[ ]{.w}</em>[[scrapy.signalmanager.]{.pre}]{.sig-prename .descclassname}[[SignalManager]{.pre}]{.sig-name .descname}[(]{.sig-paren}<em>[[sender]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[<a href="https://docs.python.org/3/library/typing.html#typing.Any" title="(in Python v3.12)">[Any]{.pre}</a>{.reference .external}]{.n}[ ]{.w}[[=]{.pre}]{.o}[ ]{.w}[[_Anonymous]{.pre}]{.default_value}</em>[)]{.sig-paren}<a href="_modules/scrapy/signalmanager.html#SignalManager">[[[source]]{.pre}]{.viewcode-link}</a>{.reference .internal}<a href="#scrapy.signalmanager.SignalManager" title="Permalink to this definition">¶</a>{.headerlink}</p>
<p>:</p>
<pre><code>[[connect]{.pre}]{.sig-name .descname}[(]{.sig-paren}*[[receiver]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[[[Any]{.pre}](https://docs.python.org/3/library/typing.html#typing.Any &quot;(in Python v3.12)&quot;){.reference .external}]{.n}*, *[[signal]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[[[Any]{.pre}](https://docs.python.org/3/library/typing.html#typing.Any &quot;(in Python v3.12)&quot;){.reference .external}]{.n}*, *[[\*\*]{.pre}]{.o}[[kwargs]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[[[Any]{.pre}](https://docs.python.org/3/library/typing.html#typing.Any &quot;(in Python v3.12)&quot;){.reference .external}]{.n}*[)]{.sig-paren} [[→]{.sig-return-icon} [[[None]{.pre}](https://docs.python.org/3/library/constants.html#None &quot;(in Python v3.12)&quot;){.reference .external}]{.sig-return-typehint}]{.sig-return}[[[\[source\]]{.pre}]{.viewcode-link}](_modules/scrapy/signalmanager.html#SignalManager.connect){.reference .internal}[¶](#scrapy.signalmanager.SignalManager.connect &quot;Permalink to this definition&quot;){.headerlink}

:   Connect a receiver function to a signal.

    The signal can be any object, although Scrapy comes with some
    predefined signals that are documented in the [[Signals]{.std
    .std-ref}](index.html#topics-signals){.hoverxref .tooltip
    .reference .internal} section.

    Parameters

    :   -   **receiver**
            ([*collections.abc.Callable*](https://docs.python.org/3/library/collections.abc.html#collections.abc.Callable &quot;(in Python v3.12)&quot;){.reference
            .external}) -- the function to be connected

        -   **signal**
            ([*object*](https://docs.python.org/3/library/functions.html#object &quot;(in Python v3.12)&quot;){.reference
            .external}) -- the signal to connect to

[[disconnect]{.pre}]{.sig-name .descname}[(]{.sig-paren}*[[receiver]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[[[Any]{.pre}](https://docs.python.org/3/library/typing.html#typing.Any &quot;(in Python v3.12)&quot;){.reference .external}]{.n}*, *[[signal]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[[[Any]{.pre}](https://docs.python.org/3/library/typing.html#typing.Any &quot;(in Python v3.12)&quot;){.reference .external}]{.n}*, *[[\*\*]{.pre}]{.o}[[kwargs]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[[[Any]{.pre}](https://docs.python.org/3/library/typing.html#typing.Any &quot;(in Python v3.12)&quot;){.reference .external}]{.n}*[)]{.sig-paren} [[→]{.sig-return-icon} [[[None]{.pre}](https://docs.python.org/3/library/constants.html#None &quot;(in Python v3.12)&quot;){.reference .external}]{.sig-return-typehint}]{.sig-return}[[[\[source\]]{.pre}]{.viewcode-link}](_modules/scrapy/signalmanager.html#SignalManager.disconnect){.reference .internal}[¶](#scrapy.signalmanager.SignalManager.disconnect &quot;Permalink to this definition&quot;){.headerlink}

:   Disconnect a receiver function from a signal. This has the
    opposite effect of the [[`connect()`{.xref .py .py-meth
    .docutils .literal
    .notranslate}]{.pre}](#scrapy.signalmanager.SignalManager.connect &quot;scrapy.signalmanager.SignalManager.connect&quot;){.reference
    .internal} method, and the arguments are the same.

[[disconnect_all]{.pre}]{.sig-name .descname}[(]{.sig-paren}*[[signal]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[[[Any]{.pre}](https://docs.python.org/3/library/typing.html#typing.Any &quot;(in Python v3.12)&quot;){.reference .external}]{.n}*, *[[\*\*]{.pre}]{.o}[[kwargs]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[[[Any]{.pre}](https://docs.python.org/3/library/typing.html#typing.Any &quot;(in Python v3.12)&quot;){.reference .external}]{.n}*[)]{.sig-paren} [[→]{.sig-return-icon} [[[None]{.pre}](https://docs.python.org/3/library/constants.html#None &quot;(in Python v3.12)&quot;){.reference .external}]{.sig-return-typehint}]{.sig-return}[[[\[source\]]{.pre}]{.viewcode-link}](_modules/scrapy/signalmanager.html#SignalManager.disconnect_all){.reference .internal}[¶](#scrapy.signalmanager.SignalManager.disconnect_all &quot;Permalink to this definition&quot;){.headerlink}

:   Disconnect all receivers from the given signal.

    Parameters

    :   **signal**
        ([*object*](https://docs.python.org/3/library/functions.html#object &quot;(in Python v3.12)&quot;){.reference
        .external}) -- the signal to disconnect from

[[send_catch_log]{.pre}]{.sig-name .descname}[(]{.sig-paren}*[[signal]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[[[Any]{.pre}](https://docs.python.org/3/library/typing.html#typing.Any &quot;(in Python v3.12)&quot;){.reference .external}]{.n}*, *[[\*\*]{.pre}]{.o}[[kwargs]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[[[Any]{.pre}](https://docs.python.org/3/library/typing.html#typing.Any &quot;(in Python v3.12)&quot;){.reference .external}]{.n}*[)]{.sig-paren} [[→]{.sig-return-icon} [[[List]{.pre}](https://docs.python.org/3/library/typing.html#typing.List &quot;(in Python v3.12)&quot;){.reference .external}[[\[]{.pre}]{.p}[[Tuple]{.pre}](https://docs.python.org/3/library/typing.html#typing.Tuple &quot;(in Python v3.12)&quot;){.reference .external}[[\[]{.pre}]{.p}[[Any]{.pre}](https://docs.python.org/3/library/typing.html#typing.Any &quot;(in Python v3.12)&quot;){.reference .external}[[,]{.pre}]{.p}[ ]{.w}[[Any]{.pre}](https://docs.python.org/3/library/typing.html#typing.Any &quot;(in Python v3.12)&quot;){.reference .external}[[\]]{.pre}]{.p}[[\]]{.pre}]{.p}]{.sig-return-typehint}]{.sig-return}[[[\[source\]]{.pre}]{.viewcode-link}](_modules/scrapy/signalmanager.html#SignalManager.send_catch_log){.reference .internal}[¶](#scrapy.signalmanager.SignalManager.send_catch_log &quot;Permalink to this definition&quot;){.headerlink}

:   Send a signal, catch exceptions and log them.

    The keyword arguments are passed to the signal handlers
    (connected through the [[`connect()`{.xref .py .py-meth
    .docutils .literal
    .notranslate}]{.pre}](#scrapy.signalmanager.SignalManager.connect &quot;scrapy.signalmanager.SignalManager.connect&quot;){.reference
    .internal} method).

[[send_catch_log_deferred]{.pre}]{.sig-name .descname}[(]{.sig-paren}*[[signal]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[[[Any]{.pre}](https://docs.python.org/3/library/typing.html#typing.Any &quot;(in Python v3.12)&quot;){.reference .external}]{.n}*, *[[\*\*]{.pre}]{.o}[[kwargs]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[[[Any]{.pre}](https://docs.python.org/3/library/typing.html#typing.Any &quot;(in Python v3.12)&quot;){.reference .external}]{.n}*[)]{.sig-paren} [[→]{.sig-return-icon} [[[Deferred]{.pre}](https://docs.twisted.org/en/stable/api/twisted.internet.defer.Deferred.html &quot;(in Twisted)&quot;){.reference .external}]{.sig-return-typehint}]{.sig-return}[[[\[source\]]{.pre}]{.viewcode-link}](_modules/scrapy/signalmanager.html#SignalManager.send_catch_log_deferred){.reference .internal}[¶](#scrapy.signalmanager.SignalManager.send_catch_log_deferred &quot;Permalink to this definition&quot;){.headerlink}

:   Like [[`send_catch_log()`{.xref .py .py-meth .docutils .literal
    .notranslate}]{.pre}](#scrapy.signalmanager.SignalManager.send_catch_log &quot;scrapy.signalmanager.SignalManager.send_catch_log&quot;){.reference
    .internal} but supports returning [[`Deferred`{.xref .py
    .py-class .docutils .literal
    .notranslate}]{.pre}](https://docs.twisted.org/en/stable/api/twisted.internet.defer.Deferred.html &quot;(in Twisted)&quot;){.reference
    .external} objects from signal handlers.

    Returns a Deferred that gets fired once all signal handlers
    deferreds were fired. Send a signal, catch exceptions and log
    them.

    The keyword arguments are passed to the signal handlers
    (connected through the [[`connect()`{.xref .py .py-meth
    .docutils .literal
    .notranslate}]{.pre}](#scrapy.signalmanager.SignalManager.connect &quot;scrapy.signalmanager.SignalManager.connect&quot;){.reference
    .internal} method).
</code></pre>
<p>:::</p>
<p>::: {#stats-collector-api .section}
[]{#topics-api-stats}</p>
<h4 id="stats-collector-apiheaderlink"><a class="header" href="#stats-collector-apiheaderlink">Stats Collector API<a href="#stats-collector-api" title="Permalink to this heading">¶</a>{.headerlink}</a></h4>
<p>There are several Stats Collectors available under the
<a href="#module-scrapy.statscollectors" title="scrapy.statscollectors: Stats Collectors">[<code>scrapy.statscollectors</code>{.xref .py .py-mod .docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal} module and they all implement the Stats Collector API defined
by the <a href="#scrapy.statscollectors.StatsCollector" title="scrapy.statscollectors.StatsCollector">[<code>StatsCollector</code>{.xref .py .py-class .docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal} class (which they all inherit from).</p>
<p>[]{#module-scrapy.statscollectors .target}</p>
<p><em>[class]{.pre}[ ]{.w}</em>[[scrapy.statscollectors.]{.pre}]{.sig-prename .descclassname}[[StatsCollector]{.pre}]{.sig-name .descname}<a href="_modules/scrapy/statscollectors.html#StatsCollector">[[[source]]{.pre}]{.viewcode-link}</a>{.reference .internal}<a href="#scrapy.statscollectors.StatsCollector" title="Permalink to this definition">¶</a>{.headerlink}</p>
<p>:</p>
<pre><code>[[get_value]{.pre}]{.sig-name .descname}[(]{.sig-paren}*[[key]{.pre}]{.n}*, *[[default]{.pre}]{.n}[[=]{.pre}]{.o}[[None]{.pre}]{.default_value}*[)]{.sig-paren}[[[\[source\]]{.pre}]{.viewcode-link}](_modules/scrapy/statscollectors.html#StatsCollector.get_value){.reference .internal}[¶](#scrapy.statscollectors.StatsCollector.get_value &quot;Permalink to this definition&quot;){.headerlink}

:   Return the value for the given stats key or default if it
    doesn't exist.

[[get_stats]{.pre}]{.sig-name .descname}[(]{.sig-paren}[)]{.sig-paren}[[[\[source\]]{.pre}]{.viewcode-link}](_modules/scrapy/statscollectors.html#StatsCollector.get_stats){.reference .internal}[¶](#scrapy.statscollectors.StatsCollector.get_stats &quot;Permalink to this definition&quot;){.headerlink}

:   Get all stats from the currently running spider as a dict.

[[set_value]{.pre}]{.sig-name .descname}[(]{.sig-paren}*[[key]{.pre}]{.n}*, *[[value]{.pre}]{.n}*[)]{.sig-paren}[[[\[source\]]{.pre}]{.viewcode-link}](_modules/scrapy/statscollectors.html#StatsCollector.set_value){.reference .internal}[¶](#scrapy.statscollectors.StatsCollector.set_value &quot;Permalink to this definition&quot;){.headerlink}

:   Set the given value for the given stats key.

[[set_stats]{.pre}]{.sig-name .descname}[(]{.sig-paren}*[[stats]{.pre}]{.n}*[)]{.sig-paren}[[[\[source\]]{.pre}]{.viewcode-link}](_modules/scrapy/statscollectors.html#StatsCollector.set_stats){.reference .internal}[¶](#scrapy.statscollectors.StatsCollector.set_stats &quot;Permalink to this definition&quot;){.headerlink}

:   Override the current stats with the dict passed in
    [`stats`{.docutils .literal .notranslate}]{.pre} argument.

[[inc_value]{.pre}]{.sig-name .descname}[(]{.sig-paren}*[[key]{.pre}]{.n}*, *[[count]{.pre}]{.n}[[=]{.pre}]{.o}[[1]{.pre}]{.default_value}*, *[[start]{.pre}]{.n}[[=]{.pre}]{.o}[[0]{.pre}]{.default_value}*[)]{.sig-paren}[[[\[source\]]{.pre}]{.viewcode-link}](_modules/scrapy/statscollectors.html#StatsCollector.inc_value){.reference .internal}[¶](#scrapy.statscollectors.StatsCollector.inc_value &quot;Permalink to this definition&quot;){.headerlink}

:   Increment the value of the given stats key, by the given count,
    assuming the start value given (when it's not set).

[[max_value]{.pre}]{.sig-name .descname}[(]{.sig-paren}*[[key]{.pre}]{.n}*, *[[value]{.pre}]{.n}*[)]{.sig-paren}[[[\[source\]]{.pre}]{.viewcode-link}](_modules/scrapy/statscollectors.html#StatsCollector.max_value){.reference .internal}[¶](#scrapy.statscollectors.StatsCollector.max_value &quot;Permalink to this definition&quot;){.headerlink}

:   Set the given value for the given key only if current value for
    the same key is lower than value. If there is no current value
    for the given key, the value is always set.

[[min_value]{.pre}]{.sig-name .descname}[(]{.sig-paren}*[[key]{.pre}]{.n}*, *[[value]{.pre}]{.n}*[)]{.sig-paren}[[[\[source\]]{.pre}]{.viewcode-link}](_modules/scrapy/statscollectors.html#StatsCollector.min_value){.reference .internal}[¶](#scrapy.statscollectors.StatsCollector.min_value &quot;Permalink to this definition&quot;){.headerlink}

:   Set the given value for the given key only if current value for
    the same key is greater than value. If there is no current value
    for the given key, the value is always set.

[[clear_stats]{.pre}]{.sig-name .descname}[(]{.sig-paren}[)]{.sig-paren}[[[\[source\]]{.pre}]{.viewcode-link}](_modules/scrapy/statscollectors.html#StatsCollector.clear_stats){.reference .internal}[¶](#scrapy.statscollectors.StatsCollector.clear_stats &quot;Permalink to this definition&quot;){.headerlink}

:   Clear all stats.

The following methods are not part of the stats collection api but
instead used when implementing custom stats collectors:

[[open_spider]{.pre}]{.sig-name .descname}[(]{.sig-paren}*[[spider]{.pre}]{.n}*[)]{.sig-paren}[[[\[source\]]{.pre}]{.viewcode-link}](_modules/scrapy/statscollectors.html#StatsCollector.open_spider){.reference .internal}[¶](#scrapy.statscollectors.StatsCollector.open_spider &quot;Permalink to this definition&quot;){.headerlink}

:   Open the given spider for stats collection.

[[close_spider]{.pre}]{.sig-name .descname}[(]{.sig-paren}*[[spider]{.pre}]{.n}*[)]{.sig-paren}[[[\[source\]]{.pre}]{.viewcode-link}](_modules/scrapy/statscollectors.html#StatsCollector.close_spider){.reference .internal}[¶](#scrapy.statscollectors.StatsCollector.close_spider &quot;Permalink to this definition&quot;){.headerlink}

:   Close the given spider. After this is called, no more specific
    stats can be accessed or collected.
</code></pre>
<p>:::
:::
:::</p>
<p><a href="index.html#document-topics/architecture">[Architecture overview]{.doc}</a>{.reference .internal}</p>
<p>:   Understand the Scrapy architecture.</p>
<p><a href="index.html#document-topics/addons">[Add-ons]{.doc}</a>{.reference .internal}</p>
<p>:   Enable and configure third-party extensions.</p>
<p><a href="index.html#document-topics/downloader-middleware">[Downloader Middleware]{.doc}</a>{.reference .internal}</p>
<p>:   Customize how pages get requested and downloaded.</p>
<p><a href="index.html#document-topics/spider-middleware">[Spider Middleware]{.doc}</a>{.reference .internal}</p>
<p>:   Customize the input and output of your spiders.</p>
<p><a href="index.html#document-topics/extensions">[Extensions]{.doc}</a>{.reference .internal}</p>
<p>:   Extend Scrapy with your custom functionality</p>
<p><a href="index.html#document-topics/signals">[Signals]{.doc}</a>{.reference .internal}</p>
<p>:   See all available signals and how to work with them.</p>
<p><a href="index.html#document-topics/scheduler">[Scheduler]{.doc}</a>{.reference .internal}</p>
<p>:   Understand the scheduler component.</p>
<p><a href="index.html#document-topics/exporters">[Item Exporters]{.doc}</a>{.reference .internal}</p>
<p>:   Quickly export your scraped items to a file (XML, CSV, etc).</p>
<p><a href="index.html#document-topics/components">[Components]{.doc}</a>{.reference .internal}</p>
<p>:   Learn the common API and some good practices when building custom
Scrapy components.</p>
<p><a href="index.html#document-topics/api">[Core API]{.doc}</a>{.reference .internal}</p>
<p>:   Use it on extensions and middlewares to extend Scrapy functionality.
:::</p>
<p>::: {#all-the-rest .section}</p>
<h2 id="all-the-restheaderlink"><a class="header" href="#all-the-restheaderlink">All the rest<a href="#all-the-rest" title="Permalink to this heading">¶</a>{.headerlink}</a></h2>
<p>::: {.toctree-wrapper .compound}
[]{#document-news}</p>
<p>::: {#release-notes .section}
[]{#news}</p>
<h3 id="release-notesheaderlink"><a class="header" href="#release-notesheaderlink">Release notes<a href="#release-notes" title="Permalink to this heading">¶</a>{.headerlink}</a></h3>
<p>::: {#scrapy-2-11-0-2023-09-18 .section}
[]{#release-2-11-0}</p>
<h4 id="scrapy-2110-2023-09-18headerlink"><a class="header" href="#scrapy-2110-2023-09-18headerlink">Scrapy 2.11.0 (2023-09-18)<a href="#scrapy-2-11-0-2023-09-18" title="Permalink to this heading">¶</a>{.headerlink}</a></h4>
<p>Highlights:</p>
<ul>
<li>
<p>Spiders can now modify <a href="index.html#topics-settings">[settings]{.std
.std-ref}</a>{.hoverxref .tooltip
.reference .internal} in their <a href="index.html#scrapy.Spider.from_crawler" title="scrapy.Spider.from_crawler">[<code>from_crawler()</code>{.xref .py .py-meth
.docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal} methods, e.g. based on <a href="index.html#spiderargs">[spider arguments]{.std
.std-ref}</a>{.hoverxref .tooltip .reference
.internal}.</p>
</li>
<li>
<p>Periodic logging of stats.</p>
</li>
</ul>
<p>::: {#backward-incompatible-changes .section}</p>
<h5 id="backward-incompatible-changesheaderlink"><a class="header" href="#backward-incompatible-changesheaderlink">Backward-incompatible changes<a href="#backward-incompatible-changes" title="Permalink to this heading">¶</a>{.headerlink}</a></h5>
<ul>
<li>
<p>Most of the initialization of <a href="index.html#scrapy.crawler.Crawler" title="scrapy.crawler.Crawler">[<code>scrapy.crawler.Crawler</code>{.xref .py
.py-class .docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal} instances is now done in <a href="index.html#scrapy.crawler.Crawler.crawl" title="scrapy.crawler.Crawler.crawl">[<code>crawl()</code>{.xref .py .py-meth
.docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal}, so the state of instances before that method is called
is now different compared to older Scrapy versions. We do not
recommend using the <a href="index.html#scrapy.crawler.Crawler" title="scrapy.crawler.Crawler">[<code>Crawler</code>{.xref .py .py-class .docutils
.literal
.notranslate}]{.pre}</a>{.reference
.internal} instances before <a href="index.html#scrapy.crawler.Crawler.crawl" title="scrapy.crawler.Crawler.crawl">[<code>crawl()</code>{.xref .py .py-meth .docutils
.literal
.notranslate}]{.pre}</a>{.reference
.internal} is called. (<a href="https://github.com/scrapy/scrapy/issues/6038">issue
6038</a>{.reference
.external})</p>
</li>
<li>
<p><a href="index.html#scrapy.Spider.from_crawler" title="scrapy.Spider.from_crawler">[<code>scrapy.Spider.from_crawler()</code>{.xref .py .py-meth .docutils
.literal
.notranslate}]{.pre}</a>{.reference
.internal} is now called before the initialization of various
components previously initialized in
[<code>scrapy.crawler.Crawler.__init__()</code>{.xref .py .py-meth .docutils
.literal .notranslate}]{.pre} and before the settings are finalized
and frozen. This change was needed to allow changing the settings in
<a href="index.html#scrapy.Spider.from_crawler" title="scrapy.Spider.from_crawler">[<code>scrapy.Spider.from_crawler()</code>{.xref .py .py-meth .docutils
.literal
.notranslate}]{.pre}</a>{.reference
.internal}. If you want to access the final setting values and the
initialized <a href="index.html#scrapy.crawler.Crawler" title="scrapy.crawler.Crawler">[<code>Crawler</code>{.xref .py .py-class .docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal} attributes in the spider code as early as possible you
can do this in <a href="index.html#scrapy.Spider.start_requests" title="scrapy.Spider.start_requests">[<code>start_requests()</code>{.xref .py .py-meth .docutils
.literal
.notranslate}]{.pre}</a>{.reference
.internal} or in a handler of the <a href="index.html#std-signal-engine_started">[<code>engine_started</code>{.xref .std
.std-signal .docutils .literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal} signal. (<a href="https://github.com/scrapy/scrapy/issues/6038">issue
6038</a>{.reference
.external})</p>
</li>
<li>
<p>The <a href="index.html#scrapy.http.TextResponse.json" title="scrapy.http.TextResponse.json">[<code>TextResponse.json</code>{.xref .py .py-meth .docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal} method now requires the response to be in a valid JSON
encoding (UTF-8, UTF-16, or UTF-32). If you need to deal with JSON
documents in an invalid encoding, use
[<code>json.loads(response.text)</code>{.docutils .literal .notranslate}]{.pre}
instead. (<a href="https://github.com/scrapy/scrapy/issues/6016">issue
6016</a>{.reference
.external})</p>
</li>
<li>
<p><a href="index.html#scrapy.exporters.PythonItemExporter" title="scrapy.exporters.PythonItemExporter">[<code>PythonItemExporter</code>{.xref .py .py-class .docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal} used the binary output by default but it no longer does.
(<a href="https://github.com/scrapy/scrapy/issues/6006">issue
6006</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/6007">issue
6007</a>{.reference
.external})
:::</p>
</li>
</ul>
<p>::: {#deprecation-removals .section}</p>
<h5 id="deprecation-removalsheaderlink"><a class="header" href="#deprecation-removalsheaderlink">Deprecation removals<a href="#deprecation-removals" title="Permalink to this heading">¶</a>{.headerlink}</a></h5>
<ul>
<li>
<p>Removed the binary export mode of <a href="index.html#scrapy.exporters.PythonItemExporter" title="scrapy.exporters.PythonItemExporter">[<code>PythonItemExporter</code>{.xref .py
.py-class .docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal}, deprecated in Scrapy 1.1.0. (<a href="https://github.com/scrapy/scrapy/issues/6006">issue
6006</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/6007">issue
6007</a>{.reference
.external})</p>
<p>::: {.admonition .note}
Note</p>
<p>If you are using this Scrapy version on Scrapy Cloud with a stack
that includes an older Scrapy version and get a &quot;TypeError:
Unexpected options: binary&quot; error, you may need to add
[<code>scrapinghub-entrypoint-scrapy</code>{.docutils .literal
.notranslate}]{.pre}<code> </code>{.docutils .literal
.notranslate}[<code>&gt;=</code>{.docutils .literal
.notranslate}]{.pre}<code> </code>{.docutils .literal
.notranslate}[<code>0.14.1</code>{.docutils .literal .notranslate}]{.pre} to
your project requirements or switch to a stack that includes Scrapy
2.11.
:::</p>
</li>
<li>
<p>Removed the [<code>CrawlerRunner.spiders</code>{.docutils .literal
.notranslate}]{.pre} attribute, deprecated in Scrapy 1.0.0, use
[<code>CrawlerRunner.spider_loader</code>{.xref .py .py-attr .docutils .literal
.notranslate}]{.pre} instead. (<a href="https://github.com/scrapy/scrapy/issues/6010">issue
6010</a>{.reference
.external})
:::</p>
</li>
</ul>
<p>::: {#deprecations .section}</p>
<h5 id="deprecationsheaderlink"><a class="header" href="#deprecationsheaderlink">Deprecations<a href="#deprecations" title="Permalink to this heading">¶</a>{.headerlink}</a></h5>
<ul>
<li>Running <a href="index.html#scrapy.crawler.Crawler.crawl" title="scrapy.crawler.Crawler.crawl">[<code>crawl()</code>{.xref .py .py-meth .docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal} more than once on the same
<a href="index.html#scrapy.crawler.Crawler" title="scrapy.crawler.Crawler">[<code>scrapy.crawler.Crawler</code>{.xref .py .py-class .docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal} instance is now deprecated. (<a href="https://github.com/scrapy/scrapy/issues/1587">issue
1587</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/6040">issue
6040</a>{.reference
.external})
:::</li>
</ul>
<p>::: {#new-features .section}</p>
<h5 id="new-featuresheaderlink"><a class="header" href="#new-featuresheaderlink">New features<a href="#new-features" title="Permalink to this heading">¶</a>{.headerlink}</a></h5>
<ul>
<li>
<p>Spiders can now modify settings in their <a href="index.html#scrapy.Spider.from_crawler" title="scrapy.Spider.from_crawler">[<code>from_crawler()</code>{.xref
.py .py-meth .docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal} method, e.g. based on <a href="index.html#spiderargs">[spider arguments]{.std
.std-ref}</a>{.hoverxref .tooltip .reference
.internal}. (<a href="https://github.com/scrapy/scrapy/issues/1305">issue
1305</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/1580">issue
1580</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/2392">issue
2392</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/3663">issue
3663</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/6038">issue
6038</a>{.reference
.external})</p>
</li>
<li>
<p>Added the <a href="index.html#scrapy.extensions.periodic_log.PeriodicLog" title="scrapy.extensions.periodic_log.PeriodicLog">[<code>PeriodicLog</code>{.xref .py .py-class .docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal} extension which can be enabled to log stats and/or their
differences periodically. (<a href="https://github.com/scrapy/scrapy/issues/5926">issue
5926</a>{.reference
.external})</p>
</li>
<li>
<p>Optimized the memory usage in <a href="index.html#scrapy.http.TextResponse.json" title="scrapy.http.TextResponse.json">[<code>TextResponse.json</code>{.xref .py
.py-meth .docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal} by removing unnecessary body decoding. (<a href="https://github.com/scrapy/scrapy/issues/5968">issue
5968</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/6016">issue
6016</a>{.reference
.external})</p>
</li>
<li>
<p>Links to [<code>.webp</code>{.docutils .literal .notranslate}]{.pre} files are
now ignored by <a href="index.html#topics-link-extractors">[link extractors]{.std
.std-ref}</a>{.hoverxref .tooltip
.reference .internal}. (<a href="https://github.com/scrapy/scrapy/issues/6021">issue
6021</a>{.reference
.external})
:::</p>
</li>
</ul>
<p>::: {#bug-fixes .section}</p>
<h5 id="bug-fixesheaderlink"><a class="header" href="#bug-fixesheaderlink">Bug fixes<a href="#bug-fixes" title="Permalink to this heading">¶</a>{.headerlink}</a></h5>
<ul>
<li>
<p>Fixed logging enabled add-ons. (<a href="https://github.com/scrapy/scrapy/issues/6036">issue
6036</a>{.reference
.external})</p>
</li>
<li>
<p>Fixed <a href="index.html#scrapy.mail.MailSender" title="scrapy.mail.MailSender">[<code>MailSender</code>{.xref .py .py-class .docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal} producing invalid message bodies when the
[<code>charset</code>{.docutils .literal .notranslate}]{.pre} argument is
passed to <a href="index.html#scrapy.mail.MailSender.send" title="scrapy.mail.MailSender.send">[<code>send()</code>{.xref .py .py-meth .docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal}. (<a href="https://github.com/scrapy/scrapy/issues/5096">issue
5096</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/5118">issue
5118</a>{.reference
.external})</p>
</li>
<li>
<p>Fixed an exception when accessing
[<code>self.EXCEPTIONS_TO_RETRY</code>{.docutils .literal .notranslate}]{.pre}
from a subclass of <a href="index.html#scrapy.downloadermiddlewares.retry.RetryMiddleware" title="scrapy.downloadermiddlewares.retry.RetryMiddleware">[<code>RetryMiddleware</code>{.xref .py .py-class .docutils
.literal
.notranslate}]{.pre}</a>{.reference
.internal}. (<a href="https://github.com/scrapy/scrapy/issues/6049">issue
6049</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/6050">issue
6050</a>{.reference
.external})</p>
</li>
<li>
<p><a href="index.html#scrapy.settings.BaseSettings.getdictorlist" title="scrapy.settings.BaseSettings.getdictorlist">[<code>scrapy.settings.BaseSettings.getdictorlist()</code>{.xref .py .py-meth
.docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal}, used to parse <a href="index.html#std-setting-FEED_EXPORT_FIELDS">[<code>FEED_EXPORT_FIELDS</code>{.xref .std
.std-setting .docutils .literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal}, now handles tuple values. (<a href="https://github.com/scrapy/scrapy/issues/6011">issue
6011</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/6013">issue
6013</a>{.reference
.external})</p>
</li>
<li>
<p>Calls to [<code>datetime.utcnow()</code>{.docutils .literal
.notranslate}]{.pre}, no longer recommended to be used, have been
replaced with calls to [<code>datetime.now()</code>{.docutils .literal
.notranslate}]{.pre} with a timezone. (<a href="https://github.com/scrapy/scrapy/issues/6014">issue
6014</a>{.reference
.external})
:::</p>
</li>
</ul>
<p>::: {#documentation .section}</p>
<h5 id="documentationheaderlink"><a class="header" href="#documentationheaderlink">Documentation<a href="#documentation" title="Permalink to this heading">¶</a>{.headerlink}</a></h5>
<ul>
<li>Updated a deprecated function call in a pipeline example. (<a href="https://github.com/scrapy/scrapy/issues/6008">issue
6008</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/6009">issue
6009</a>{.reference
.external})
:::</li>
</ul>
<p>::: {#quality-assurance .section}</p>
<h5 id="quality-assuranceheaderlink"><a class="header" href="#quality-assuranceheaderlink">Quality assurance<a href="#quality-assurance" title="Permalink to this heading">¶</a>{.headerlink}</a></h5>
<ul>
<li>
<p>Extended typing hints. (<a href="https://github.com/scrapy/scrapy/issues/6003">issue
6003</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/6005">issue
6005</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/6031">issue
6031</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/6034">issue
6034</a>{.reference
.external})</p>
</li>
<li>
<p>Pinned <a href="https://github.com/google/brotli">brotli</a>{.reference
.external} to 1.0.9 for the PyPy tests as 1.1.0 breaks them. (<a href="https://github.com/scrapy/scrapy/issues/6044">issue
6044</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/6045">issue
6045</a>{.reference
.external})</p>
</li>
<li>
<p>Other CI and pre-commit improvements. (<a href="https://github.com/scrapy/scrapy/issues/6002">issue
6002</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/6013">issue
6013</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/6046">issue
6046</a>{.reference
.external})
:::
:::</p>
</li>
</ul>
<p>::: {#scrapy-2-10-1-2023-08-30 .section}
[]{#release-2-10-1}</p>
<h4 id="scrapy-2101-2023-08-30headerlink"><a class="header" href="#scrapy-2101-2023-08-30headerlink">Scrapy 2.10.1 (2023-08-30)<a href="#scrapy-2-10-1-2023-08-30" title="Permalink to this heading">¶</a>{.headerlink}</a></h4>
<p>Marked [<code>Twisted</code>{.docutils .literal .notranslate}]{.pre}<code> </code>{.docutils
.literal .notranslate}[<code>&gt;=</code>{.docutils .literal
.notranslate}]{.pre}<code> </code>{.docutils .literal
.notranslate}[<code>23.8.0</code>{.docutils .literal .notranslate}]{.pre} as
unsupported. (<a href="https://github.com/scrapy/scrapy/issues/6024">issue
6024</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/6026">issue
6026</a>{.reference
.external})
:::</p>
<p>::: {#scrapy-2-10-0-2023-08-04 .section}
[]{#release-2-10-0}</p>
<h4 id="scrapy-2100-2023-08-04headerlink"><a class="header" href="#scrapy-2100-2023-08-04headerlink">Scrapy 2.10.0 (2023-08-04)<a href="#scrapy-2-10-0-2023-08-04" title="Permalink to this heading">¶</a>{.headerlink}</a></h4>
<p>Highlights:</p>
<ul>
<li>
<p>Added Python 3.12 support, dropped Python 3.7 support.</p>
</li>
<li>
<p>The new add-ons framework simplifies configuring 3rd-party
components that support it.</p>
</li>
<li>
<p>Exceptions to retry can now be configured.</p>
</li>
<li>
<p>Many fixes and improvements for feed exports.</p>
</li>
</ul>
<p>::: {#modified-requirements .section}</p>
<h5 id="modified-requirementsheaderlink"><a class="header" href="#modified-requirementsheaderlink">Modified requirements<a href="#modified-requirements" title="Permalink to this heading">¶</a>{.headerlink}</a></h5>
<ul>
<li>
<p>Dropped support for Python 3.7. (<a href="https://github.com/scrapy/scrapy/issues/5953">issue
5953</a>{.reference
.external})</p>
</li>
<li>
<p>Added support for the upcoming Python 3.12. (<a href="https://github.com/scrapy/scrapy/issues/5984">issue
5984</a>{.reference
.external})</p>
</li>
<li>
<p>Minimum versions increased for these dependencies:</p>
<ul>
<li>
<p><a href="https://lxml.de/">lxml</a>{.reference .external}: 4.3.0 → 4.4.1</p>
</li>
<li>
<p><a href="https://cryptography.io/en/latest/">cryptography</a>{.reference
.external}: 3.4.6 → 36.0.0</p>
</li>
</ul>
</li>
<li>
<p>[<code>pkg_resources</code>{.docutils .literal .notranslate}]{.pre} is no
longer used. (<a href="https://github.com/scrapy/scrapy/issues/5956">issue
5956</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/5958">issue
5958</a>{.reference
.external})</p>
</li>
<li>
<p><a href="https://github.com/boto/boto3">boto3</a>{.reference .external} is now
recommended instead of
<a href="https://github.com/boto/botocore">botocore</a>{.reference .external}
for exporting to S3. (<a href="https://github.com/scrapy/scrapy/issues/5833">issue
5833</a>{.reference
.external}).
:::</p>
</li>
</ul>
<p>::: {#id1 .section}</p>
<h5 id="backward-incompatible-changesheaderlink-1"><a class="header" href="#backward-incompatible-changesheaderlink-1">Backward-incompatible changes<a href="#id1" title="Permalink to this heading">¶</a>{.headerlink}</a></h5>
<ul>
<li>The value of the <a href="index.html#std-setting-FEED_STORE_EMPTY">[<code>FEED_STORE_EMPTY</code>{.xref .std .std-setting
.docutils .literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal} setting is now [<code>True</code>{.docutils
.literal .notranslate}]{.pre} instead of [<code>False</code>{.docutils .literal
.notranslate}]{.pre}. In earlier Scrapy versions empty files were
created even when this setting was [<code>False</code>{.docutils .literal
.notranslate}]{.pre} (which was a bug that is now fixed), so the new
default should keep the old behavior. (<a href="https://github.com/scrapy/scrapy/issues/872">issue
872</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/5847">issue
5847</a>{.reference
.external})
:::</li>
</ul>
<p>::: {#id2 .section}</p>
<h5 id="deprecation-removalsheaderlink-1"><a class="header" href="#deprecation-removalsheaderlink-1">Deprecation removals<a href="#id2" title="Permalink to this heading">¶</a>{.headerlink}</a></h5>
<ul>
<li>
<p>When a function is assigned to the <a href="index.html#std-setting-FEED_URI_PARAMS">[<code>FEED_URI_PARAMS</code>{.xref .std
.std-setting .docutils .literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal} setting, returning [<code>None</code>{.docutils
.literal .notranslate}]{.pre} or modifying the [<code>params</code>{.docutils
.literal .notranslate}]{.pre} input parameter, deprecated in Scrapy
2.6, is no longer supported. (<a href="https://github.com/scrapy/scrapy/issues/5994">issue
5994</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/5996">issue
5996</a>{.reference
.external})</p>
</li>
<li>
<p>The [<code>scrapy.utils.reqser</code>{.docutils .literal .notranslate}]{.pre}
module, deprecated in Scrapy 2.6, is removed. (<a href="https://github.com/scrapy/scrapy/issues/5994">issue
5994</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/5996">issue
5996</a>{.reference
.external})</p>
</li>
<li>
<p>The [<code>scrapy.squeues</code>{.docutils .literal .notranslate}]{.pre}
classes [<code>PickleFifoDiskQueueNonRequest</code>{.docutils .literal
.notranslate}]{.pre}, [<code>PickleLifoDiskQueueNonRequest</code>{.docutils
.literal .notranslate}]{.pre},
[<code>MarshalFifoDiskQueueNonRequest</code>{.docutils .literal
.notranslate}]{.pre}, and
[<code>MarshalLifoDiskQueueNonRequest</code>{.docutils .literal
.notranslate}]{.pre}, deprecated in Scrapy 2.6, are removed. (<a href="https://github.com/scrapy/scrapy/issues/5994">issue
5994</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/5996">issue
5996</a>{.reference
.external})</p>
</li>
<li>
<p>The property [<code>open_spiders</code>{.docutils .literal .notranslate}]{.pre}
and the methods [<code>has_capacity</code>{.docutils .literal
.notranslate}]{.pre} and [<code>schedule</code>{.docutils .literal
.notranslate}]{.pre} of [<code>scrapy.core.engine.ExecutionEngine</code>{.xref
.py .py-class .docutils .literal .notranslate}]{.pre}, deprecated in
Scrapy 2.6, are removed. (<a href="https://github.com/scrapy/scrapy/issues/5994">issue
5994</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/5998">issue
5998</a>{.reference
.external})</p>
</li>
<li>
<p>Passing a [<code>spider</code>{.docutils .literal .notranslate}]{.pre} argument
to the [<code>spider_is_idle()</code>{.xref .py .py-meth .docutils .literal
.notranslate}]{.pre}, [<code>crawl()</code>{.xref .py .py-meth .docutils
.literal .notranslate}]{.pre} and [<code>download()</code>{.xref .py .py-meth
.docutils .literal .notranslate}]{.pre} methods of
[<code>scrapy.core.engine.ExecutionEngine</code>{.xref .py .py-class .docutils
.literal .notranslate}]{.pre}, deprecated in Scrapy 2.6, is no
longer supported. (<a href="https://github.com/scrapy/scrapy/issues/5994">issue
5994</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/5998">issue
5998</a>{.reference
.external})
:::</p>
</li>
</ul>
<p>::: {#id3 .section}</p>
<h5 id="deprecationsheaderlink-1"><a class="header" href="#deprecationsheaderlink-1">Deprecations<a href="#id3" title="Permalink to this heading">¶</a>{.headerlink}</a></h5>
<ul>
<li>
<p>[<code>scrapy.utils.datatypes.CaselessDict</code>{.xref .py .py-class .docutils
.literal .notranslate}]{.pre} is deprecated, use
[<code>scrapy.utils.datatypes.CaseInsensitiveDict</code>{.xref .py .py-class
.docutils .literal .notranslate}]{.pre} instead. (<a href="https://github.com/scrapy/scrapy/issues/5146">issue
5146</a>{.reference
.external})</p>
</li>
<li>
<p>Passing the [<code>custom</code>{.docutils .literal .notranslate}]{.pre}
argument to [<code>scrapy.utils.conf.build_component_list()</code>{.xref .py
.py-func .docutils .literal .notranslate}]{.pre} is deprecated, it
was used in the past to merge [<code>FOO</code>{.docutils .literal
.notranslate}]{.pre} and [<code>FOO_BASE</code>{.docutils .literal
.notranslate}]{.pre} setting values but now Scrapy uses
<a href="index.html#scrapy.settings.BaseSettings.getwithbase" title="scrapy.settings.BaseSettings.getwithbase">[<code>scrapy.settings.BaseSettings.getwithbase()</code>{.xref .py .py-func
.docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal} to do the same. Code that uses this argument and cannot
be switched to [<code>getwithbase()</code>{.docutils .literal
.notranslate}]{.pre} can be switched to merging the values
explicitly. (<a href="https://github.com/scrapy/scrapy/issues/5726">issue
5726</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/5923">issue
5923</a>{.reference
.external})
:::</p>
</li>
</ul>
<p>::: {#id4 .section}</p>
<h5 id="new-featuresheaderlink-1"><a class="header" href="#new-featuresheaderlink-1">New features<a href="#id4" title="Permalink to this heading">¶</a>{.headerlink}</a></h5>
<ul>
<li>
<p>Added support for <a href="index.html#topics-addons">[Scrapy add-ons]{.std
.std-ref}</a>{.hoverxref .tooltip .reference
.internal}. (<a href="https://github.com/scrapy/scrapy/issues/5950">issue
5950</a>{.reference
.external})</p>
</li>
<li>
<p>Added the <a href="index.html#std-setting-RETRY_EXCEPTIONS">[<code>RETRY_EXCEPTIONS</code>{.xref .std .std-setting .docutils
.literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal} setting that configures which
exceptions will be retried by <a href="index.html#scrapy.downloadermiddlewares.retry.RetryMiddleware" title="scrapy.downloadermiddlewares.retry.RetryMiddleware">[<code>RetryMiddleware</code>{.xref .py
.py-class .docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal}. (<a href="https://github.com/scrapy/scrapy/issues/2701">issue
2701</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/5929">issue
5929</a>{.reference
.external})</p>
</li>
<li>
<p>Added the possiiblity to close the spider if no items were produced
in the specified time, configured by
<a href="index.html#std-setting-CLOSESPIDER_TIMEOUT_NO_ITEM">[<code>CLOSESPIDER_TIMEOUT_NO_ITEM</code>{.xref .std .std-setting .docutils
.literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal}. (<a href="https://github.com/scrapy/scrapy/issues/5979">issue
5979</a>{.reference
.external})</p>
</li>
<li>
<p>Added support for the <a href="index.html#std-setting-AWS_REGION_NAME">[<code>AWS_REGION_NAME</code>{.xref .std .std-setting
.docutils .literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal} setting to feed exports. (<a href="https://github.com/scrapy/scrapy/issues/5980">issue
5980</a>{.reference
.external})</p>
</li>
<li>
<p>Added support for using <a href="https://docs.python.org/3/library/pathlib.html#pathlib.Path" title="(in Python v3.12)">[<code>pathlib.Path</code>{.xref .py .py-class
.docutils .literal
.notranslate}]{.pre}</a>{.reference
.external} objects that refer to absolute Windows paths in the
<a href="index.html#std-setting-FEEDS">[<code>FEEDS</code>{.xref .std .std-setting .docutils .literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal} setting. (<a href="https://github.com/scrapy/scrapy/issues/5939">issue
5939</a>{.reference
.external})
:::</p>
</li>
</ul>
<p>::: {#id5 .section}</p>
<h5 id="bug-fixesheaderlink-1"><a class="header" href="#bug-fixesheaderlink-1">Bug fixes<a href="#id5" title="Permalink to this heading">¶</a>{.headerlink}</a></h5>
<ul>
<li>
<p>Fixed creating empty feeds even with
[<code>FEED_STORE_EMPTY=False</code>{.docutils .literal .notranslate}]{.pre}.
(<a href="https://github.com/scrapy/scrapy/issues/872">issue 872</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/5847">issue
5847</a>{.reference
.external})</p>
</li>
<li>
<p>Fixed using absolute Windows paths when specifying output files.
(<a href="https://github.com/scrapy/scrapy/issues/5969">issue
5969</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/5971">issue
5971</a>{.reference
.external})</p>
</li>
<li>
<p>Fixed problems with uploading large files to S3 by switching to
multipart uploads (requires
<a href="https://github.com/boto/boto3">boto3</a>{.reference .external}).
(<a href="https://github.com/scrapy/scrapy/issues/960">issue 960</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/5735">issue
5735</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/5833">issue
5833</a>{.reference
.external})</p>
</li>
<li>
<p>Fixed the JSON exporter writing extra commas when some exceptions
occur. (<a href="https://github.com/scrapy/scrapy/issues/3090">issue
3090</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/5952">issue
5952</a>{.reference
.external})</p>
</li>
<li>
<p>Fixed the &quot;read of closed file&quot; error in the CSV exporter. (<a href="https://github.com/scrapy/scrapy/issues/5043">issue
5043</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/5705">issue
5705</a>{.reference
.external})</p>
</li>
<li>
<p>Fixed an error when a component added by the class object throws
<a href="index.html#scrapy.exceptions.NotConfigured" title="scrapy.exceptions.NotConfigured">[<code>NotConfigured</code>{.xref .py .py-exc .docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal} with a message. (<a href="https://github.com/scrapy/scrapy/issues/5950">issue
5950</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/5992">issue
5992</a>{.reference
.external})</p>
</li>
<li>
<p>Added the missing <a href="index.html#scrapy.settings.BaseSettings.pop" title="scrapy.settings.BaseSettings.pop">[<code>scrapy.settings.BaseSettings.pop()</code>{.xref .py
.py-meth .docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal} method. (<a href="https://github.com/scrapy/scrapy/issues/5959">issue
5959</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/5960">issue
5960</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/5963">issue
5963</a>{.reference
.external})</p>
</li>
<li>
<p>Added [<code>CaseInsensitiveDict</code>{.xref .py .py-class .docutils .literal
.notranslate}]{.pre} as a replacement for [<code>CaselessDict</code>{.xref .py
.py-class .docutils .literal .notranslate}]{.pre} that fixes some
API inconsistencies. (<a href="https://github.com/scrapy/scrapy/issues/5146">issue
5146</a>{.reference
.external})
:::</p>
</li>
</ul>
<p>::: {#id6 .section}</p>
<h5 id="documentationheaderlink-1"><a class="header" href="#documentationheaderlink-1">Documentation<a href="#id6" title="Permalink to this heading">¶</a>{.headerlink}</a></h5>
<ul>
<li>
<p>Documented <a href="index.html#scrapy.Spider.update_settings" title="scrapy.Spider.update_settings">[<code>scrapy.Spider.update_settings()</code>{.xref .py .py-meth
.docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal}. (<a href="https://github.com/scrapy/scrapy/issues/5745">issue
5745</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/5846">issue
5846</a>{.reference
.external})</p>
</li>
<li>
<p>Documented possible problems with early Twisted reactor installation
and their solutions. (<a href="https://github.com/scrapy/scrapy/issues/5981">issue
5981</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/6000">issue
6000</a>{.reference
.external})</p>
</li>
<li>
<p>Added examples of making additional requests in callbacks. (<a href="https://github.com/scrapy/scrapy/issues/5927">issue
5927</a>{.reference
.external})</p>
</li>
<li>
<p>Improved the feed export docs. (<a href="https://github.com/scrapy/scrapy/issues/5579">issue
5579</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/5931">issue
5931</a>{.reference
.external})</p>
</li>
<li>
<p>Clarified the docs about request objects on redirection. (<a href="https://github.com/scrapy/scrapy/issues/5707">issue
5707</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/5937">issue
5937</a>{.reference
.external})
:::</p>
</li>
</ul>
<p>::: {#id7 .section}</p>
<h5 id="quality-assuranceheaderlink-1"><a class="header" href="#quality-assuranceheaderlink-1">Quality assurance<a href="#id7" title="Permalink to this heading">¶</a>{.headerlink}</a></h5>
<ul>
<li>
<p>Added support for running tests against the installed Scrapy
version. (<a href="https://github.com/scrapy/scrapy/issues/4914">issue
4914</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/5949">issue
5949</a>{.reference
.external})</p>
</li>
<li>
<p>Extended typing hints. (<a href="https://github.com/scrapy/scrapy/issues/5925">issue
5925</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/5977">issue
5977</a>{.reference
.external})</p>
</li>
<li>
<p>Fixed the
[<code>test_utils_asyncio.AsyncioTest.test_set_asyncio_event_loop</code>{.docutils
.literal .notranslate}]{.pre} test. (<a href="https://github.com/scrapy/scrapy/issues/5951">issue
5951</a>{.reference
.external})</p>
</li>
<li>
<p>Fixed the
[<code>test_feedexport.BatchDeliveriesTest.test_batch_path_differ</code>{.docutils
.literal .notranslate}]{.pre} test on Windows. (<a href="https://github.com/scrapy/scrapy/issues/5847">issue
5847</a>{.reference
.external})</p>
</li>
<li>
<p>Enabled CI runs for Python 3.11 on Windows. (<a href="https://github.com/scrapy/scrapy/issues/5999">issue
5999</a>{.reference
.external})</p>
</li>
<li>
<p>Simplified skipping tests that depend on [<code>uvloop</code>{.docutils
.literal .notranslate}]{.pre}. (<a href="https://github.com/scrapy/scrapy/issues/5984">issue
5984</a>{.reference
.external})</p>
</li>
<li>
<p>Fixed the [<code>extra-deps-pinned</code>{.docutils .literal
.notranslate}]{.pre} tox env. (<a href="https://github.com/scrapy/scrapy/issues/5948">issue
5948</a>{.reference
.external})</p>
</li>
<li>
<p>Implemented cleanups. (<a href="https://github.com/scrapy/scrapy/issues/5965">issue
5965</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/5986">issue
5986</a>{.reference
.external})
:::
:::</p>
</li>
</ul>
<p>::: {#scrapy-2-9-0-2023-05-08 .section}
[]{#release-2-9-0}</p>
<h4 id="scrapy-290-2023-05-08headerlink"><a class="header" href="#scrapy-290-2023-05-08headerlink">Scrapy 2.9.0 (2023-05-08)<a href="#scrapy-2-9-0-2023-05-08" title="Permalink to this heading">¶</a>{.headerlink}</a></h4>
<p>Highlights:</p>
<ul>
<li>
<p>Per-domain download settings.</p>
</li>
<li>
<p>Compatibility with new
<a href="https://cryptography.io/en/latest/">cryptography</a>{.reference
.external} and new
<a href="https://github.com/scrapy/parsel">parsel</a>{.reference .external}.</p>
</li>
<li>
<p>JMESPath selectors from the new
<a href="https://github.com/scrapy/parsel">parsel</a>{.reference .external}.</p>
</li>
<li>
<p>Bug fixes.</p>
</li>
</ul>
<p>::: {#id8 .section}</p>
<h5 id="deprecationsheaderlink-2"><a class="header" href="#deprecationsheaderlink-2">Deprecations<a href="#id8" title="Permalink to this heading">¶</a>{.headerlink}</a></h5>
<ul>
<li>[<code>scrapy.extensions.feedexport._FeedSlot</code>{.xref .py .py-class
.docutils .literal .notranslate}]{.pre} is renamed to
[<code>scrapy.extensions.feedexport.FeedSlot</code>{.xref .py .py-class
.docutils .literal .notranslate}]{.pre} and the old name is
deprecated. (<a href="https://github.com/scrapy/scrapy/issues/5876">issue
5876</a>{.reference
.external})
:::</li>
</ul>
<p>::: {#id9 .section}</p>
<h5 id="new-featuresheaderlink-2"><a class="header" href="#new-featuresheaderlink-2">New features<a href="#id9" title="Permalink to this heading">¶</a>{.headerlink}</a></h5>
<ul>
<li>
<p>Settings corresponding to <a href="index.html#std-setting-DOWNLOAD_DELAY">[<code>DOWNLOAD_DELAY</code>{.xref .std .std-setting
.docutils .literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal},
<a href="index.html#std-setting-CONCURRENT_REQUESTS_PER_DOMAIN">[<code>CONCURRENT_REQUESTS_PER_DOMAIN</code>{.xref .std .std-setting .docutils
.literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal} and
<a href="index.html#std-setting-RANDOMIZE_DOWNLOAD_DELAY">[<code>RANDOMIZE_DOWNLOAD_DELAY</code>{.xref .std .std-setting .docutils
.literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal} can now be set on a per-domain basis
via the new <a href="index.html#std-setting-DOWNLOAD_SLOTS">[<code>DOWNLOAD_SLOTS</code>{.xref .std .std-setting .docutils
.literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal} setting. (<a href="https://github.com/scrapy/scrapy/issues/5328">issue
5328</a>{.reference
.external})</p>
</li>
<li>
<p>Added [<code>TextResponse.jmespath()</code>{.xref .py .py-meth .docutils
.literal .notranslate}]{.pre}, a shortcut for JMESPath selectors
available since
<a href="https://github.com/scrapy/parsel">parsel</a>{.reference .external}
1.8.1. (<a href="https://github.com/scrapy/scrapy/issues/5894">issue
5894</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/5915">issue
5915</a>{.reference
.external})</p>
</li>
<li>
<p>Added <a href="index.html#std-signal-feed_slot_closed">[<code>feed_slot_closed</code>{.xref .std .std-signal .docutils .literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal} and <a href="index.html#std-signal-feed_exporter_closed">[<code>feed_exporter_closed</code>{.xref
.std .std-signal .docutils .literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal} signals. (<a href="https://github.com/scrapy/scrapy/issues/5876">issue
5876</a>{.reference
.external})</p>
</li>
<li>
<p>Added [<code>scrapy.utils.request.request_to_curl()</code>{.xref .py .py-func
.docutils .literal .notranslate}]{.pre}, a function to produce a
curl command from a [<code>Request</code>{.xref .py .py-class .docutils
.literal .notranslate}]{.pre} object. (<a href="https://github.com/scrapy/scrapy/issues/5892">issue
5892</a>{.reference
.external})</p>
</li>
<li>
<p>Values of <a href="index.html#std-setting-FILES_STORE">[<code>FILES_STORE</code>{.xref .std .std-setting .docutils .literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal} and <a href="index.html#std-setting-IMAGES_STORE">[<code>IMAGES_STORE</code>{.xref .std
.std-setting .docutils .literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal} can now be <a href="https://docs.python.org/3/library/pathlib.html#pathlib.Path" title="(in Python v3.12)">[<code>pathlib.Path</code>{.xref .py
.py-class .docutils .literal
.notranslate}]{.pre}</a>{.reference
.external} instances. (<a href="https://github.com/scrapy/scrapy/issues/5801">issue
5801</a>{.reference
.external})
:::</p>
</li>
</ul>
<p>::: {#id10 .section}</p>
<h5 id="bug-fixesheaderlink-2"><a class="header" href="#bug-fixesheaderlink-2">Bug fixes<a href="#id10" title="Permalink to this heading">¶</a>{.headerlink}</a></h5>
<ul>
<li>
<p>Fixed a warning with Parsel 1.8.1+. (<a href="https://github.com/scrapy/scrapy/issues/5903">issue
5903</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/5918">issue
5918</a>{.reference
.external})</p>
</li>
<li>
<p>Fixed an error when using feed postprocessing with S3 storage.
(<a href="https://github.com/scrapy/scrapy/issues/5500">issue
5500</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/5581">issue
5581</a>{.reference
.external})</p>
</li>
<li>
<p>Added the missing
<a href="index.html#scrapy.settings.BaseSettings.setdefault" title="scrapy.settings.BaseSettings.setdefault">[<code>scrapy.settings.BaseSettings.setdefault()</code>{.xref .py .py-meth
.docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal} method. (<a href="https://github.com/scrapy/scrapy/issues/5811">issue
5811</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/5821">issue
5821</a>{.reference
.external})</p>
</li>
<li>
<p>Fixed an error when using
<a href="https://cryptography.io/en/latest/">cryptography</a>{.reference
.external} 40.0.0+ and
<a href="index.html#std-setting-DOWNLOADER_CLIENT_TLS_VERBOSE_LOGGING">[<code>DOWNLOADER_CLIENT_TLS_VERBOSE_LOGGING</code>{.xref .std .std-setting
.docutils .literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal} is enabled. (<a href="https://github.com/scrapy/scrapy/issues/5857">issue
5857</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/5858">issue
5858</a>{.reference
.external})</p>
</li>
<li>
<p>The checksums returned by <a href="index.html#scrapy.pipelines.files.FilesPipeline" title="scrapy.pipelines.files.FilesPipeline">[<code>FilesPipeline</code>{.xref .py .py-class
.docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal} for files on Google Cloud Storage are no longer
Base64-encoded. (<a href="https://github.com/scrapy/scrapy/issues/5874">issue
5874</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/5891">issue
5891</a>{.reference
.external})</p>
</li>
<li>
<p>[<code>scrapy.utils.request.request_from_curl()</code>{.xref .py .py-func
.docutils .literal .notranslate}]{.pre} now supports $-prefixed
string values for the curl [<code>--data-raw</code>{.docutils .literal
.notranslate}]{.pre} argument, which are produced by browsers for
data that includes certain symbols. (<a href="https://github.com/scrapy/scrapy/issues/5899">issue
5899</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/5901">issue
5901</a>{.reference
.external})</p>
</li>
<li>
<p>The <a href="index.html#std-command-parse">[<code>parse</code>{.xref .std .std-command .docutils .literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal} command now also works with async
generator callbacks. (<a href="https://github.com/scrapy/scrapy/issues/5819">issue
5819</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/5824">issue
5824</a>{.reference
.external})</p>
</li>
<li>
<p>The <a href="index.html#std-command-genspider">[<code>genspider</code>{.xref .std .std-command .docutils .literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal} command now properly works with HTTPS
URLs. (<a href="https://github.com/scrapy/scrapy/issues/3553">issue
3553</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/5808">issue
5808</a>{.reference
.external})</p>
</li>
<li>
<p>Improved handling of asyncio loops. (<a href="https://github.com/scrapy/scrapy/issues/5831">issue
5831</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/5832">issue
5832</a>{.reference
.external})</p>
</li>
<li>
<p><a href="index.html#scrapy.linkextractors.lxmlhtml.LxmlLinkExtractor" title="scrapy.linkextractors.lxmlhtml.LxmlLinkExtractor">[<code>LinkExtractor</code>{.xref .py .py-class .docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal} now skips certain malformed URLs instead of raising an
exception. (<a href="https://github.com/scrapy/scrapy/issues/5881">issue
5881</a>{.reference
.external})</p>
</li>
<li>
<p>[<code>scrapy.utils.python.get_func_args()</code>{.xref .py .py-func .docutils
.literal .notranslate}]{.pre} now supports more types of callables.
(<a href="https://github.com/scrapy/scrapy/issues/5872">issue
5872</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/5885">issue
5885</a>{.reference
.external})</p>
</li>
<li>
<p>Fixed an error when processing non-UTF8 values of
[<code>Content-Type</code>{.docutils .literal .notranslate}]{.pre} headers.
(<a href="https://github.com/scrapy/scrapy/issues/5914">issue
5914</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/5917">issue
5917</a>{.reference
.external})</p>
</li>
<li>
<p>Fixed an error breaking user handling of send failures in
<a href="index.html#scrapy.mail.MailSender.send" title="scrapy.mail.MailSender.send">[<code>scrapy.mail.MailSender.send()</code>{.xref .py .py-meth .docutils
.literal
.notranslate}]{.pre}</a>{.reference
.internal}. (<a href="https://github.com/scrapy/scrapy/issues/1611">issue
1611</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/5880">issue
5880</a>{.reference
.external})
:::</p>
</li>
</ul>
<p>::: {#id11 .section}</p>
<h5 id="documentationheaderlink-2"><a class="header" href="#documentationheaderlink-2">Documentation<a href="#id11" title="Permalink to this heading">¶</a>{.headerlink}</a></h5>
<ul>
<li>
<p>Expanded contributing docs. (<a href="https://github.com/scrapy/scrapy/issues/5109">issue
5109</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/5851">issue
5851</a>{.reference
.external})</p>
</li>
<li>
<p>Added
<a href="https://github.com/adamchainz/blacken-docs">blacken-docs</a>{.reference
.external} to pre-commit and reformatted the docs with it. (<a href="https://github.com/scrapy/scrapy/issues/5813">issue
5813</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/5816">issue
5816</a>{.reference
.external})</p>
</li>
<li>
<p>Fixed a JS issue. (<a href="https://github.com/scrapy/scrapy/issues/5875">issue
5875</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/5877">issue
5877</a>{.reference
.external})</p>
</li>
<li>
<p>Fixed [<code>make</code>{.docutils .literal .notranslate}]{.pre}<code> </code>{.docutils
.literal .notranslate}[<code>htmlview</code>{.docutils .literal
.notranslate}]{.pre}. (<a href="https://github.com/scrapy/scrapy/issues/5878">issue
5878</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/5879">issue
5879</a>{.reference
.external})</p>
</li>
<li>
<p>Fixed typos and other small errors. (<a href="https://github.com/scrapy/scrapy/issues/5827">issue
5827</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/5839">issue
5839</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/5883">issue
5883</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/5890">issue
5890</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/5895">issue
5895</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/5904">issue
5904</a>{.reference
.external})
:::</p>
</li>
</ul>
<p>::: {#id12 .section}</p>
<h5 id="quality-assuranceheaderlink-2"><a class="header" href="#quality-assuranceheaderlink-2">Quality assurance<a href="#id12" title="Permalink to this heading">¶</a>{.headerlink}</a></h5>
<ul>
<li>
<p>Extended typing hints. (<a href="https://github.com/scrapy/scrapy/issues/5805">issue
5805</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/5889">issue
5889</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/5896">issue
5896</a>{.reference
.external})</p>
</li>
<li>
<p>Tests for most of the examples in the docs are now run as a part of
CI, found problems were fixed. (<a href="https://github.com/scrapy/scrapy/issues/5816">issue
5816</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/5826">issue
5826</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/5919">issue
5919</a>{.reference
.external})</p>
</li>
<li>
<p>Removed usage of deprecated Python classes. (<a href="https://github.com/scrapy/scrapy/issues/5849">issue
5849</a>{.reference
.external})</p>
</li>
<li>
<p>Silenced [<code>include-ignored</code>{.docutils .literal .notranslate}]{.pre}
warnings from coverage. (<a href="https://github.com/scrapy/scrapy/issues/5820">issue
5820</a>{.reference
.external})</p>
</li>
<li>
<p>Fixed a random failure of the
[<code>test_feedexport.test_batch_path_differ</code>{.docutils .literal
.notranslate}]{.pre} test. (<a href="https://github.com/scrapy/scrapy/issues/5855">issue
5855</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/5898">issue
5898</a>{.reference
.external})</p>
</li>
<li>
<p>Updated docstrings to match output produced by
<a href="https://github.com/scrapy/parsel">parsel</a>{.reference .external}
1.8.1 so that they don't cause test failures. (<a href="https://github.com/scrapy/scrapy/issues/5902">issue
5902</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/5919">issue
5919</a>{.reference
.external})</p>
</li>
<li>
<p>Other CI and pre-commit improvements. (<a href="https://github.com/scrapy/scrapy/issues/5802">issue
5802</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/5823">issue
5823</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/5908">issue
5908</a>{.reference
.external})
:::
:::</p>
</li>
</ul>
<p>::: {#scrapy-2-8-0-2023-02-02 .section}
[]{#release-2-8-0}</p>
<h4 id="scrapy-280-2023-02-02headerlink"><a class="header" href="#scrapy-280-2023-02-02headerlink">Scrapy 2.8.0 (2023-02-02)<a href="#scrapy-2-8-0-2023-02-02" title="Permalink to this heading">¶</a>{.headerlink}</a></h4>
<p>This is a maintenance release, with minor features, bug fixes, and
cleanups.</p>
<p>::: {#id13 .section}</p>
<h5 id="deprecation-removalsheaderlink-2"><a class="header" href="#deprecation-removalsheaderlink-2">Deprecation removals<a href="#id13" title="Permalink to this heading">¶</a>{.headerlink}</a></h5>
<ul>
<li>
<p>The [<code>scrapy.utils.gz.read1</code>{.docutils .literal .notranslate}]{.pre}
function, deprecated in Scrapy 2.0, has now been removed. Use the
<a href="https://docs.python.org/3/library/io.html#io.BufferedIOBase.read1" title="(in Python v3.12)">[<code>read1()</code>{.xref .py .py-meth .docutils .literal
.notranslate}]{.pre}</a>{.reference
.external} method of <a href="https://docs.python.org/3/library/gzip.html#gzip.GzipFile" title="(in Python v3.12)">[<code>GzipFile</code>{.xref .py .py-class .docutils
.literal
.notranslate}]{.pre}</a>{.reference
.external} instead. (<a href="https://github.com/scrapy/scrapy/issues/5719">issue
5719</a>{.reference
.external})</p>
</li>
<li>
<p>The [<code>scrapy.utils.python.to_native_str</code>{.docutils .literal
.notranslate}]{.pre} function, deprecated in Scrapy 2.0, has now
been removed. Use [<code>scrapy.utils.python.to_unicode()</code>{.xref .py
.py-func .docutils .literal .notranslate}]{.pre} instead. (<a href="https://github.com/scrapy/scrapy/issues/5719">issue
5719</a>{.reference
.external})</p>
</li>
<li>
<p>The [<code>scrapy.utils.python.MutableChain.next</code>{.docutils .literal
.notranslate}]{.pre} method, deprecated in Scrapy 2.0, has now been
removed. Use [<code>__next__()</code>{.xref .py .py-meth .docutils .literal
.notranslate}]{.pre} instead. (<a href="https://github.com/scrapy/scrapy/issues/5719">issue
5719</a>{.reference
.external})</p>
</li>
<li>
<p>The [<code>scrapy.linkextractors.FilteringLinkExtractor</code>{.docutils
.literal .notranslate}]{.pre} class, deprecated in Scrapy 2.0, has
now been removed. Use <a href="index.html#scrapy.linkextractors.lxmlhtml.LxmlLinkExtractor" title="scrapy.linkextractors.lxmlhtml.LxmlLinkExtractor">[<code>LinkExtractor</code>{.xref .py .py-class
.docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal} instead. (<a href="https://github.com/scrapy/scrapy/issues/5720">issue
5720</a>{.reference
.external})</p>
</li>
<li>
<p>Support for using environment variables prefixed with
[<code>SCRAPY_</code>{.docutils .literal .notranslate}]{.pre} to override
settings, deprecated in Scrapy 2.0, has now been removed. (<a href="https://github.com/scrapy/scrapy/issues/5724">issue
5724</a>{.reference
.external})</p>
</li>
<li>
<p>Support for the [<code>noconnect</code>{.docutils .literal .notranslate}]{.pre}
query string argument in proxy URLs, deprecated in Scrapy 2.0, has
now been removed. We expect proxies that used to need it to work
fine without it. (<a href="https://github.com/scrapy/scrapy/issues/5731">issue
5731</a>{.reference
.external})</p>
</li>
<li>
<p>The [<code>scrapy.utils.python.retry_on_eintr</code>{.docutils .literal
.notranslate}]{.pre} function, deprecated in Scrapy 2.3, has now
been removed. (<a href="https://github.com/scrapy/scrapy/issues/5719">issue
5719</a>{.reference
.external})</p>
</li>
<li>
<p>The [<code>scrapy.utils.python.WeakKeyCache</code>{.docutils .literal
.notranslate}]{.pre} class, deprecated in Scrapy 2.4, has now been
removed. (<a href="https://github.com/scrapy/scrapy/issues/5719">issue
5719</a>{.reference
.external})</p>
</li>
<li>
<p>The [<code>scrapy.utils.boto.is_botocore()</code>{.docutils .literal
.notranslate}]{.pre} function, deprecated in Scrapy 2.4, has now
been removed. (<a href="https://github.com/scrapy/scrapy/issues/5719">issue
5719</a>{.reference
.external})
:::</p>
</li>
</ul>
<p>::: {#id14 .section}</p>
<h5 id="deprecationsheaderlink-3"><a class="header" href="#deprecationsheaderlink-3">Deprecations<a href="#id14" title="Permalink to this heading">¶</a>{.headerlink}</a></h5>
<ul>
<li>
<p>[<code>scrapy.pipelines.images.NoimagesDrop</code>{.xref .py .py-exc .docutils
.literal .notranslate}]{.pre} is now deprecated. (<a href="https://github.com/scrapy/scrapy/issues/5368">issue
5368</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/5489">issue
5489</a>{.reference
.external})</p>
</li>
<li>
<p>[<code>ImagesPipeline.convert_image</code>{.xref .py .py-meth .docutils
.literal .notranslate}]{.pre} must now accept a
[<code>response_body</code>{.docutils .literal .notranslate}]{.pre} parameter.
(<a href="https://github.com/scrapy/scrapy/issues/3055">issue
3055</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/3689">issue
3689</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/4753">issue
4753</a>{.reference
.external})
:::</p>
</li>
</ul>
<p>::: {#id15 .section}</p>
<h5 id="new-featuresheaderlink-3"><a class="header" href="#new-featuresheaderlink-3">New features<a href="#id15" title="Permalink to this heading">¶</a>{.headerlink}</a></h5>
<ul>
<li>
<p>Applied <a href="https://black.readthedocs.io/en/stable/">black</a>{.reference
.external} coding style to files generated with the
<a href="index.html#std-command-genspider">[<code>genspider</code>{.xref .std .std-command .docutils .literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal} and <a href="index.html#std-command-startproject">[<code>startproject</code>{.xref .std
.std-command .docutils .literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal} commands. (<a href="https://github.com/scrapy/scrapy/issues/5809">issue
5809</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/5814">issue
5814</a>{.reference
.external})</p>
</li>
<li>
<p><a href="index.html#std-setting-FEED_EXPORT_ENCODING">[<code>FEED_EXPORT_ENCODING</code>{.xref .std .std-setting .docutils .literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal} is now set to [<code>&quot;utf-8&quot;</code>{.docutils
.literal .notranslate}]{.pre} in the [<code>settings.py</code>{.docutils
.literal .notranslate}]{.pre} file that the <a href="index.html#std-command-startproject">[<code>startproject</code>{.xref
.std .std-command .docutils .literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal} command generates. With this value,
JSON exports won't force the use of escape sequences for non-ASCII
characters. (<a href="https://github.com/scrapy/scrapy/issues/5797">issue
5797</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/5800">issue
5800</a>{.reference
.external})</p>
</li>
<li>
<p>The <a href="index.html#scrapy.extensions.memusage.MemoryUsage" title="scrapy.extensions.memusage.MemoryUsage">[<code>MemoryUsage</code>{.xref .py .py-class .docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal} extension now logs the peak memory usage during checks,
and the binary unit MiB is now used to avoid confusion. (<a href="https://github.com/scrapy/scrapy/issues/5717">issue
5717</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/5722">issue
5722</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/5727">issue
5727</a>{.reference
.external})</p>
</li>
<li>
<p>The [<code>callback</code>{.docutils .literal .notranslate}]{.pre} parameter of
<a href="index.html#scrapy.http.Request" title="scrapy.http.Request">[<code>Request</code>{.xref .py .py-class .docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal} can now be set to
<a href="index.html#scrapy.http.request.NO_CALLBACK" title="scrapy.http.request.NO_CALLBACK">[<code>scrapy.http.request.NO_CALLBACK()</code>{.xref .py .py-func .docutils
.literal
.notranslate}]{.pre}</a>{.reference
.internal}, to distinguish it from [<code>None</code>{.docutils .literal
.notranslate}]{.pre}, as the latter indicates that the default
spider callback (<a href="index.html#scrapy.Spider.parse" title="scrapy.Spider.parse">[<code>parse()</code>{.xref .py .py-meth .docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal}) is to be used. (<a href="https://github.com/scrapy/scrapy/issues/5798">issue
5798</a>{.reference
.external})
:::</p>
</li>
</ul>
<p>::: {#id16 .section}</p>
<h5 id="bug-fixesheaderlink-3"><a class="header" href="#bug-fixesheaderlink-3">Bug fixes<a href="#id16" title="Permalink to this heading">¶</a>{.headerlink}</a></h5>
<ul>
<li>
<p>Enabled unsafe legacy SSL renegotiation to fix access to some
outdated websites. (<a href="https://github.com/scrapy/scrapy/issues/5491">issue
5491</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/5790">issue
5790</a>{.reference
.external})</p>
</li>
<li>
<p>Fixed STARTTLS-based email delivery not working with Twisted 21.2.0
and better. (<a href="https://github.com/scrapy/scrapy/issues/5386">issue
5386</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/5406">issue
5406</a>{.reference
.external})</p>
</li>
<li>
<p>Fixed the [<code>finish_exporting()</code>{.xref .py .py-meth .docutils
.literal .notranslate}]{.pre} method of <a href="index.html#topics-exporters">[item exporters]{.std
.std-ref}</a>{.hoverxref .tooltip
.reference .internal} not being called for empty files. (<a href="https://github.com/scrapy/scrapy/issues/5537">issue
5537</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/5758">issue
5758</a>{.reference
.external})</p>
</li>
<li>
<p>Fixed HTTP/2 responses getting only the last value for a header when
multiple headers with the same name are received. (<a href="https://github.com/scrapy/scrapy/issues/5777">issue
5777</a>{.reference
.external})</p>
</li>
<li>
<p>Fixed an exception raised by the <a href="index.html#std-command-shell">[<code>shell</code>{.xref .std .std-command
.docutils .literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal} command on some cases when <a href="index.html#using-asyncio">[using
asyncio]{.std .std-ref}</a>{.hoverxref
.tooltip .reference .internal}. (<a href="https://github.com/scrapy/scrapy/issues/5740">issue
5740</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/5742">issue
5742</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/5748">issue
5748</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/5759">issue
5759</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/5760">issue
5760</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/5771">issue
5771</a>{.reference
.external})</p>
</li>
<li>
<p>When using <a href="index.html#scrapy.spiders.CrawlSpider" title="scrapy.spiders.CrawlSpider">[<code>CrawlSpider</code>{.xref .py .py-class .docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal}, callback keyword arguments ([<code>cb_kwargs</code>{.docutils
.literal .notranslate}]{.pre}) added to a request in the
[<code>process_request</code>{.docutils .literal .notranslate}]{.pre} callback
of a <a href="index.html#scrapy.spiders.Rule" title="scrapy.spiders.Rule">[<code>Rule</code>{.xref .py .py-class .docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal} will no longer be ignored. (<a href="https://github.com/scrapy/scrapy/issues/5699">issue
5699</a>{.reference
.external})</p>
</li>
<li>
<p>The <a href="index.html#images-pipeline">[images pipeline]{.std
.std-ref}</a>{.hoverxref .tooltip
.reference .internal} no longer re-encodes JPEG files. (<a href="https://github.com/scrapy/scrapy/issues/3055">issue
3055</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/3689">issue
3689</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/4753">issue
4753</a>{.reference
.external})</p>
</li>
<li>
<p>Fixed the handling of transparent WebP images by the <a href="index.html#images-pipeline">[images
pipeline]{.std .std-ref}</a>{.hoverxref
.tooltip .reference .internal}. (<a href="https://github.com/scrapy/scrapy/issues/3072">issue
3072</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/5766">issue
5766</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/5767">issue
5767</a>{.reference
.external})</p>
</li>
<li>
<p>[<code>scrapy.shell.inspect_response()</code>{.xref .py .py-func .docutils
.literal .notranslate}]{.pre} no longer inhibits [<code>SIGINT</code>{.docutils
.literal .notranslate}]{.pre} (Ctrl+C). (<a href="https://github.com/scrapy/scrapy/issues/2918">issue
2918</a>{.reference
.external})</p>
</li>
<li>
<p><a href="index.html#scrapy.linkextractors.lxmlhtml.LxmlLinkExtractor" title="scrapy.linkextractors.lxmlhtml.LxmlLinkExtractor">[<code>LinkExtractor</code>{.xref .py .py-class .docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal} with [<code>unique=False</code>{.docutils .literal
.notranslate}]{.pre} no longer filters out links that have identical
URL <em>and</em> text. (<a href="https://github.com/scrapy/scrapy/issues/3798">issue
3798</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/3799">issue
3799</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/4695">issue
4695</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/5458">issue
5458</a>{.reference
.external})</p>
</li>
<li>
<p><a href="index.html#scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware" title="scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware">[<code>RobotsTxtMiddleware</code>{.xref .py .py-class .docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal} now ignores URL protocols that do not support
[<code>robots.txt</code>{.docutils .literal .notranslate}]{.pre}
([<code>data://</code>{.docutils .literal .notranslate}]{.pre},
[<code>file://</code>{.docutils .literal .notranslate}]{.pre}). (<a href="https://github.com/scrapy/scrapy/issues/5807">issue
5807</a>{.reference
.external})</p>
</li>
<li>
<p>Silenced the [<code>filelock</code>{.docutils .literal .notranslate}]{.pre}
debug log messages introduced in Scrapy 2.6. (<a href="https://github.com/scrapy/scrapy/issues/5753">issue
5753</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/5754">issue
5754</a>{.reference
.external})</p>
</li>
<li>
<p>Fixed the output of [<code>scrapy</code>{.docutils .literal
.notranslate}]{.pre}<code> </code>{.docutils .literal
.notranslate}[<code>-h</code>{.docutils .literal .notranslate}]{.pre} showing
an unintended [<code>**commands**</code>{.docutils .literal
.notranslate}]{.pre} line. (<a href="https://github.com/scrapy/scrapy/issues/5709">issue
5709</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/5711">issue
5711</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/5712">issue
5712</a>{.reference
.external})</p>
</li>
<li>
<p>Made the active project indication in the output of <a href="index.html#topics-commands">[commands]{.std
.std-ref}</a>{.hoverxref .tooltip
.reference .internal} more clear. (<a href="https://github.com/scrapy/scrapy/issues/5715">issue
5715</a>{.reference
.external})
:::</p>
</li>
</ul>
<p>::: {#id17 .section}</p>
<h5 id="documentationheaderlink-3"><a class="header" href="#documentationheaderlink-3">Documentation<a href="#id17" title="Permalink to this heading">¶</a>{.headerlink}</a></h5>
<ul>
<li>
<p>Documented how to <a href="index.html#debug-vscode">[debug spiders from Visual Studio Code]{.std
.std-ref}</a>{.hoverxref .tooltip .reference
.internal}. (<a href="https://github.com/scrapy/scrapy/issues/5721">issue
5721</a>{.reference
.external})</p>
</li>
<li>
<p>Documented how <a href="index.html#std-setting-DOWNLOAD_DELAY">[<code>DOWNLOAD_DELAY</code>{.xref .std .std-setting .docutils
.literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal} affects per-domain concurrency.
(<a href="https://github.com/scrapy/scrapy/issues/5083">issue
5083</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/5540">issue
5540</a>{.reference
.external})</p>
</li>
<li>
<p>Improved consistency. (<a href="https://github.com/scrapy/scrapy/issues/5761">issue
5761</a>{.reference
.external})</p>
</li>
<li>
<p>Fixed typos. (<a href="https://github.com/scrapy/scrapy/issues/5714">issue
5714</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/5744">issue
5744</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/5764">issue
5764</a>{.reference
.external})
:::</p>
</li>
</ul>
<p>::: {#id18 .section}</p>
<h5 id="quality-assuranceheaderlink-3"><a class="header" href="#quality-assuranceheaderlink-3">Quality assurance<a href="#id18" title="Permalink to this heading">¶</a>{.headerlink}</a></h5>
<ul>
<li>
<p>Applied <a href="index.html#coding-style">[black coding style]{.std
.std-ref}</a>{.hoverxref .tooltip .reference
.internal}, sorted import statements, and introduced
<a href="index.html#scrapy-pre-commit">[pre-commit]{.std
.std-ref}</a>{.hoverxref .tooltip
.reference .internal}. (<a href="https://github.com/scrapy/scrapy/issues/4654">issue
4654</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/4658">issue
4658</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/5734">issue
5734</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/5737">issue
5737</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/5806">issue
5806</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/5810">issue
5810</a>{.reference
.external})</p>
</li>
<li>
<p>Switched from <a href="https://docs.python.org/3/library/os.path.html#module-os.path" title="(in Python v3.12)">[<code>os.path</code>{.xref .py .py-mod .docutils .literal
.notranslate}]{.pre}</a>{.reference
.external} to <a href="https://docs.python.org/3/library/pathlib.html#module-pathlib" title="(in Python v3.12)">[<code>pathlib</code>{.xref .py .py-mod .docutils .literal
.notranslate}]{.pre}</a>{.reference
.external}. (<a href="https://github.com/scrapy/scrapy/issues/4916">issue
4916</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/4497">issue
4497</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/5682">issue
5682</a>{.reference
.external})</p>
</li>
<li>
<p>Addressed many issues reported by Pylint. (<a href="https://github.com/scrapy/scrapy/issues/5677">issue
5677</a>{.reference
.external})</p>
</li>
<li>
<p>Improved code readability. (<a href="https://github.com/scrapy/scrapy/issues/5736">issue
5736</a>{.reference
.external})</p>
</li>
<li>
<p>Improved package metadata. (<a href="https://github.com/scrapy/scrapy/issues/5768">issue
5768</a>{.reference
.external})</p>
</li>
<li>
<p>Removed direct invocations of [<code>setup.py</code>{.docutils .literal
.notranslate}]{.pre}. (<a href="https://github.com/scrapy/scrapy/issues/5774">issue
5774</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/5776">issue
5776</a>{.reference
.external})</p>
</li>
<li>
<p>Removed unnecessary <a href="https://docs.python.org/3/library/collections.html#collections.OrderedDict" title="(in Python v3.12)">[<code>OrderedDict</code>{.xref .py .py-class .docutils
.literal
.notranslate}]{.pre}</a>{.reference
.external} usages. (<a href="https://github.com/scrapy/scrapy/issues/5795">issue
5795</a>{.reference
.external})</p>
</li>
<li>
<p>Removed unnecessary [<code>__str__</code>{.docutils .literal
.notranslate}]{.pre} definitions. (<a href="https://github.com/scrapy/scrapy/issues/5150">issue
5150</a>{.reference
.external})</p>
</li>
<li>
<p>Removed obsolete code and comments. (<a href="https://github.com/scrapy/scrapy/issues/5725">issue
5725</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/5729">issue
5729</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/5730">issue
5730</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/5732">issue
5732</a>{.reference
.external})</p>
</li>
<li>
<p>Fixed test and CI issues. (<a href="https://github.com/scrapy/scrapy/issues/5749">issue
5749</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/5750">issue
5750</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/5756">issue
5756</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/5762">issue
5762</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/5765">issue
5765</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/5780">issue
5780</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/5781">issue
5781</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/5782">issue
5782</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/5783">issue
5783</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/5785">issue
5785</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/5786">issue
5786</a>{.reference
.external})
:::
:::</p>
</li>
</ul>
<p>::: {#scrapy-2-7-1-2022-11-02 .section}
[]{#release-2-7-1}</p>
<h4 id="scrapy-271-2022-11-02headerlink"><a class="header" href="#scrapy-271-2022-11-02headerlink">Scrapy 2.7.1 (2022-11-02)<a href="#scrapy-2-7-1-2022-11-02" title="Permalink to this heading">¶</a>{.headerlink}</a></h4>
<p>::: {#id19 .section}</p>
<h5 id="new-featuresheaderlink-4"><a class="header" href="#new-featuresheaderlink-4">New features<a href="#id19" title="Permalink to this heading">¶</a>{.headerlink}</a></h5>
<ul>
<li>Relaxed the restriction introduced in 2.6.2 so that the
[<code>Proxy-Authorization</code>{.docutils .literal .notranslate}]{.pre}
header can again be set explicitly, as long as the proxy URL in the
<a href="index.html#std-reqmeta-proxy">[<code>proxy</code>{.xref .std .std-reqmeta .docutils .literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal} metadata has no other credentials,
and for as long as that proxy URL remains the same; this restores
compatibility with scrapy-zyte-smartproxy 2.1.0 and older (<a href="https://github.com/scrapy/scrapy/issues/5626">issue
5626</a>{.reference
.external}).
:::</li>
</ul>
<p>::: {#id20 .section}</p>
<h5 id="bug-fixesheaderlink-4"><a class="header" href="#bug-fixesheaderlink-4">Bug fixes<a href="#id20" title="Permalink to this heading">¶</a>{.headerlink}</a></h5>
<ul>
<li>
<p>Using [<code>-O</code>{.docutils .literal
.notranslate}]{.pre}/[<code>--overwrite-output</code>{.docutils .literal
.notranslate}]{.pre} and [<code>-t</code>{.docutils .literal
.notranslate}]{.pre}/[<code>--output-format</code>{.docutils .literal
.notranslate}]{.pre} options together now produces an error instead
of ignoring the former option (<a href="https://github.com/scrapy/scrapy/issues/5516">issue
5516</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/5605">issue
5605</a>{.reference
.external}).</p>
</li>
<li>
<p>Replaced deprecated <a href="https://docs.python.org/3/library/asyncio.html#module-asyncio" title="(in Python v3.12)">[<code>asyncio</code>{.xref .py .py-mod .docutils .literal
.notranslate}]{.pre}</a>{.reference
.external} APIs that implicitly use the current event loop with code
that explicitly requests a loop from the event loop policy (<a href="https://github.com/scrapy/scrapy/issues/5685">issue
5685</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/5689">issue
5689</a>{.reference
.external}).</p>
</li>
<li>
<p>Fixed uses of deprecated Scrapy APIs in Scrapy itself (<a href="https://github.com/scrapy/scrapy/issues/5588">issue
5588</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/5589">issue
5589</a>{.reference
.external}).</p>
</li>
<li>
<p>Fixed uses of a deprecated Pillow API (<a href="https://github.com/scrapy/scrapy/issues/5684">issue
5684</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/5692">issue
5692</a>{.reference
.external}).</p>
</li>
<li>
<p>Improved code that checks if generators return values, so that it no
longer fails on decorated methods and partial methods (<a href="https://github.com/scrapy/scrapy/issues/5323">issue
5323</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/5592">issue
5592</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/5599">issue
5599</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/5691">issue
5691</a>{.reference
.external}).
:::</p>
</li>
</ul>
<p>::: {#id21 .section}</p>
<h5 id="documentationheaderlink-4"><a class="header" href="#documentationheaderlink-4">Documentation<a href="#id21" title="Permalink to this heading">¶</a>{.headerlink}</a></h5>
<ul>
<li>
<p>Upgraded the Code of Conduct to Contributor Covenant v2.1 (<a href="https://github.com/scrapy/scrapy/issues/5698">issue
5698</a>{.reference
.external}).</p>
</li>
<li>
<p>Fixed typos (<a href="https://github.com/scrapy/scrapy/issues/5681">issue
5681</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/5694">issue
5694</a>{.reference
.external}).
:::</p>
</li>
</ul>
<p>::: {#id22 .section}</p>
<h5 id="quality-assuranceheaderlink-4"><a class="header" href="#quality-assuranceheaderlink-4">Quality assurance<a href="#id22" title="Permalink to this heading">¶</a>{.headerlink}</a></h5>
<ul>
<li>
<p>Re-enabled some erroneously disabled flake8 checks (<a href="https://github.com/scrapy/scrapy/issues/5688">issue
5688</a>{.reference
.external}).</p>
</li>
<li>
<p>Ignored harmless deprecation warnings from <a href="https://docs.python.org/3/library/typing.html#module-typing" title="(in Python v3.12)">[<code>typing</code>{.xref .py
.py-mod .docutils .literal
.notranslate}]{.pre}</a>{.reference
.external} in tests (<a href="https://github.com/scrapy/scrapy/issues/5686">issue
5686</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/5697">issue
5697</a>{.reference
.external}).</p>
</li>
<li>
<p>Modernized our CI configuration (<a href="https://github.com/scrapy/scrapy/issues/5695">issue
5695</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/5696">issue
5696</a>{.reference
.external}).
:::
:::</p>
</li>
</ul>
<p>::: {#scrapy-2-7-0-2022-10-17 .section}
[]{#release-2-7-0}</p>
<h4 id="scrapy-270-2022-10-17headerlink"><a class="header" href="#scrapy-270-2022-10-17headerlink">Scrapy 2.7.0 (2022-10-17)<a href="#scrapy-2-7-0-2022-10-17" title="Permalink to this heading">¶</a>{.headerlink}</a></h4>
<p>Highlights:</p>
<ul>
<li>
<p>Added Python 3.11 support, dropped Python 3.6 support</p>
</li>
<li>
<p>Improved support for <a href="index.html#topics-coroutines">[asynchronous callbacks]{.std
.std-ref}</a>{.hoverxref .tooltip
.reference .internal}</p>
</li>
<li>
<p><a href="index.html#using-asyncio">[Asyncio support]{.std
.std-ref}</a>{.hoverxref .tooltip .reference
.internal} is enabled by default on new projects</p>
</li>
<li>
<p>Output names of item fields can now be arbitrary strings</p>
</li>
<li>
<p>Centralized <a href="index.html#request-fingerprints">[request fingerprinting]{.std
.std-ref}</a>{.hoverxref .tooltip
.reference .internal} configuration is now possible</p>
</li>
</ul>
<p>::: {#id23 .section}</p>
<h5 id="modified-requirementsheaderlink-1"><a class="header" href="#modified-requirementsheaderlink-1">Modified requirements<a href="#id23" title="Permalink to this heading">¶</a>{.headerlink}</a></h5>
<p>Python 3.7 or greater is now required; support for Python 3.6 has been
dropped. Support for the upcoming Python 3.11 has been added.</p>
<p>The minimum required version of some dependencies has changed as well:</p>
<ul>
<li>
<p><a href="https://lxml.de/">lxml</a>{.reference .external}: 3.5.0 → 4.3.0</p>
</li>
<li>
<p><a href="https://python-pillow.org/">Pillow</a>{.reference .external} (<a href="index.html#images-pipeline">[images
pipeline]{.std .std-ref}</a>{.hoverxref
.tooltip .reference .internal}): 4.0.0 → 7.1.0</p>
</li>
<li>
<p><a href="https://zopeinterface.readthedocs.io/en/latest/">zope.interface</a>{.reference
.external}: 5.0.0 → 5.1.0</p>
</li>
</ul>
<p>(<a href="https://github.com/scrapy/scrapy/issues/5512">issue 5512</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/5514">issue
5514</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/5524">issue
5524</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/5563">issue
5563</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/5664">issue
5664</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/5670">issue
5670</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/5678">issue
5678</a>{.reference
.external})
:::</p>
<p>::: {#id24 .section}</p>
<h5 id="deprecationsheaderlink-4"><a class="header" href="#deprecationsheaderlink-4">Deprecations<a href="#id24" title="Permalink to this heading">¶</a>{.headerlink}</a></h5>
<ul>
<li>
<p><a href="index.html#scrapy.pipelines.images.ImagesPipeline.thumb_path" title="scrapy.pipelines.images.ImagesPipeline.thumb_path">[<code>ImagesPipeline.thumb_path</code>{.xref .py .py-meth .docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal} must now accept an [<code>item</code>{.docutils .literal
.notranslate}]{.pre} parameter (<a href="https://github.com/scrapy/scrapy/issues/5504">issue
5504</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/5508">issue
5508</a>{.reference
.external}).</p>
</li>
<li>
<p>The [<code>scrapy.downloadermiddlewares.decompression</code>{.docutils .literal
.notranslate}]{.pre} module is now deprecated (<a href="https://github.com/scrapy/scrapy/issues/5546">issue
5546</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/5547">issue
5547</a>{.reference
.external}).
:::</p>
</li>
</ul>
<p>::: {#id25 .section}</p>
<h5 id="new-featuresheaderlink-5"><a class="header" href="#new-featuresheaderlink-5">New features<a href="#id25" title="Permalink to this heading">¶</a>{.headerlink}</a></h5>
<ul>
<li>
<p>The <a href="index.html#scrapy.spidermiddlewares.SpiderMiddleware.process_spider_output" title="scrapy.spidermiddlewares.SpiderMiddleware.process_spider_output">[<code>process_spider_output()</code>{.xref .py .py-meth .docutils
.literal
.notranslate}]{.pre}</a>{.reference
.internal} method of <a href="index.html#topics-spider-middleware">[spider middlewares]{.std
.std-ref}</a>{.hoverxref .tooltip
.reference .internal} can now be defined as an <a href="https://docs.python.org/3/glossary.html#term-asynchronous-generator" title="(in Python v3.12)">[asynchronous
generator]{.xref .std
.std-term}</a>{.reference
.external} (<a href="https://github.com/scrapy/scrapy/issues/4978">issue
4978</a>{.reference
.external}).</p>
</li>
<li>
<p>The output of [<code>Request</code>{.xref .py .py-class .docutils .literal
.notranslate}]{.pre} callbacks defined as <a href="index.html#topics-coroutines">[coroutines]{.std
.std-ref}</a>{.hoverxref .tooltip
.reference .internal} is now processed asynchronously (<a href="https://github.com/scrapy/scrapy/issues/4978">issue
4978</a>{.reference
.external}).</p>
</li>
<li>
<p>[<code>CrawlSpider</code>{.xref .py .py-class .docutils .literal
.notranslate}]{.pre} now supports <a href="index.html#topics-coroutines">[asynchronous callbacks]{.std
.std-ref}</a>{.hoverxref .tooltip
.reference .internal} (<a href="https://github.com/scrapy/scrapy/issues/5657">issue
5657</a>{.reference
.external}).</p>
</li>
<li>
<p>New projects created with the <a href="index.html#std-command-startproject">[<code>startproject</code>{.xref .std
.std-command .docutils .literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal} command have <a href="index.html#using-asyncio">[asyncio support]{.std
.std-ref}</a>{.hoverxref .tooltip .reference
.internal} enabled by default (<a href="https://github.com/scrapy/scrapy/issues/5590">issue
5590</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/5679">issue
5679</a>{.reference
.external}).</p>
</li>
<li>
<p>The <a href="index.html#std-setting-FEED_EXPORT_FIELDS">[<code>FEED_EXPORT_FIELDS</code>{.xref .std .std-setting .docutils
.literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal} setting can now be defined as a
dictionary to customize the output name of item fields, lifting the
restriction that required output names to be valid Python
identifiers, e.g. preventing them to have whitespace (<a href="https://github.com/scrapy/scrapy/issues/1008">issue
1008</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/3266">issue
3266</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/3696">issue
3696</a>{.reference
.external}).</p>
</li>
<li>
<p>You can now customize <a href="index.html#request-fingerprints">[request fingerprinting]{.std
.std-ref}</a>{.hoverxref .tooltip
.reference .internal} through the new
<a href="index.html#std-setting-REQUEST_FINGERPRINTER_CLASS">[<code>REQUEST_FINGERPRINTER_CLASS</code>{.xref .std .std-setting .docutils
.literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal} setting, instead of having to change
it on every Scrapy component that relies on request fingerprinting
(<a href="https://github.com/scrapy/scrapy/issues/900">issue 900</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/3420">issue
3420</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/4113">issue
4113</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/4762">issue
4762</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/4524">issue
4524</a>{.reference
.external}).</p>
</li>
<li>
<p>[<code>jsonl</code>{.docutils .literal .notranslate}]{.pre} is now supported
and encouraged as a file extension for <a href="https://jsonlines.org/">JSON
Lines</a>{.reference .external} files (<a href="https://github.com/scrapy/scrapy/issues/4848">issue
4848</a>{.reference
.external}).</p>
</li>
<li>
<p><a href="index.html#scrapy.pipelines.images.ImagesPipeline.thumb_path" title="scrapy.pipelines.images.ImagesPipeline.thumb_path">[<code>ImagesPipeline.thumb_path</code>{.xref .py .py-meth .docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal} now receives the source <a href="index.html#topics-items">[item]{.std
.std-ref}</a>{.hoverxref .tooltip .reference
.internal} (<a href="https://github.com/scrapy/scrapy/issues/5504">issue
5504</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/5508">issue
5508</a>{.reference
.external}).
:::</p>
</li>
</ul>
<p>::: {#id26 .section}</p>
<h5 id="bug-fixesheaderlink-5"><a class="header" href="#bug-fixesheaderlink-5">Bug fixes<a href="#id26" title="Permalink to this heading">¶</a>{.headerlink}</a></h5>
<ul>
<li>
<p>When using Google Cloud Storage with a <a href="index.html#topics-media-pipeline">[media pipeline]{.std
.std-ref}</a>{.hoverxref .tooltip
.reference .internal}, <a href="index.html#std-setting-FILES_EXPIRES">[<code>FILES_EXPIRES</code>{.xref .std .std-setting
.docutils .literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal} now also works when
<a href="index.html#std-setting-FILES_STORE">[<code>FILES_STORE</code>{.xref .std .std-setting .docutils .literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal} does not point at the root of your
Google Cloud Storage bucket (<a href="https://github.com/scrapy/scrapy/issues/5317">issue
5317</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/5318">issue
5318</a>{.reference
.external}).</p>
</li>
<li>
<p>The <a href="index.html#std-command-parse">[<code>parse</code>{.xref .std .std-command .docutils .literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal} command now supports <a href="index.html#topics-coroutines">[asynchronous
callbacks]{.std .std-ref}</a>{.hoverxref
.tooltip .reference .internal} (<a href="https://github.com/scrapy/scrapy/issues/5424">issue
5424</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/5577">issue
5577</a>{.reference
.external}).</p>
</li>
<li>
<p>When using the <a href="index.html#std-command-parse">[<code>parse</code>{.xref .std .std-command .docutils .literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal} command with a URL for which there is
no available spider, an exception is no longer raised (<a href="https://github.com/scrapy/scrapy/issues/3264">issue
3264</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/3265">issue
3265</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/5375">issue
5375</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/5376">issue
5376</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/5497">issue
5497</a>{.reference
.external}).</p>
</li>
<li>
<p><a href="index.html#scrapy.http.TextResponse" title="scrapy.http.TextResponse">[<code>TextResponse</code>{.xref .py .py-class .docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal} now gives higher priority to the <a href="https://en.wikipedia.org/wiki/Byte_order_mark">byte order
mark</a>{.reference
.external} when determining the text encoding of the response body,
following the <a href="https://html.spec.whatwg.org/multipage/parsing.html#determining-the-character-encoding">HTML living
standard</a>{.reference
.external} (<a href="https://github.com/scrapy/scrapy/issues/5601">issue
5601</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/5611">issue
5611</a>{.reference
.external}).</p>
</li>
<li>
<p>MIME sniffing takes the response body into account in FTP and
HTTP/1.0 requests, as well as in cached requests (<a href="https://github.com/scrapy/scrapy/issues/4873">issue
4873</a>{.reference
.external}).</p>
</li>
<li>
<p>MIME sniffing now detects valid HTML 5 documents even if the
[<code>html</code>{.docutils .literal .notranslate}]{.pre} tag is missing
(<a href="https://github.com/scrapy/scrapy/issues/4873">issue
4873</a>{.reference
.external}).</p>
</li>
<li>
<p>An exception is now raised if <a href="index.html#std-setting-ASYNCIO_EVENT_LOOP">[<code>ASYNCIO_EVENT_LOOP</code>{.xref .std
.std-setting .docutils .literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal} has a value that does not match the
asyncio event loop actually installed (<a href="https://github.com/scrapy/scrapy/issues/5529">issue
5529</a>{.reference
.external}).</p>
</li>
<li>
<p>Fixed [<code>Headers.getlist</code>{.xref .py .py-meth .docutils .literal
.notranslate}]{.pre} returning only the last header (<a href="https://github.com/scrapy/scrapy/issues/5515">issue
5515</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/5526">issue
5526</a>{.reference
.external}).</p>
</li>
<li>
<p>Fixed <a href="index.html#scrapy.linkextractors.lxmlhtml.LxmlLinkExtractor" title="scrapy.linkextractors.lxmlhtml.LxmlLinkExtractor">[<code>LinkExtractor</code>{.xref .py .py-class .docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal} not ignoring the [<code>tar.gz</code>{.docutils .literal
.notranslate}]{.pre} file extension by default (<a href="https://github.com/scrapy/scrapy/issues/1837">issue
1837</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/2067">issue
2067</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/4066">issue
4066</a>{.reference
.external})
:::</p>
</li>
</ul>
<p>::: {#id27 .section}</p>
<h5 id="documentationheaderlink-5"><a class="header" href="#documentationheaderlink-5">Documentation<a href="#id27" title="Permalink to this heading">¶</a>{.headerlink}</a></h5>
<ul>
<li>
<p>Clarified the return type of <a href="index.html#scrapy.Spider.parse" title="scrapy.Spider.parse">[<code>Spider.parse</code>{.xref .py .py-meth
.docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal} (<a href="https://github.com/scrapy/scrapy/issues/5602">issue
5602</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/5608">issue
5608</a>{.reference
.external}).</p>
</li>
<li>
<p>To enable <a href="index.html#scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware" title="scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware">[<code>HttpCompressionMiddleware</code>{.xref .py .py-class
.docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal} to do <a href="https://www.ietf.org/rfc/rfc7932.txt">brotli
compression</a>{.reference
.external}, installing
<a href="https://github.com/google/brotli">brotli</a>{.reference .external} is
now recommended instead of installing
<a href="https://github.com/python-hyper/brotlipy/">brotlipy</a>{.reference
.external}, as the former provides a more recent version of brotli.</p>
</li>
<li>
<p><a href="index.html#topics-signals">[Signal documentation]{.std
.std-ref}</a>{.hoverxref .tooltip .reference
.internal} now mentions <a href="index.html#topics-coroutines">[coroutine support]{.std
.std-ref}</a>{.hoverxref .tooltip
.reference .internal} and uses it in code examples (<a href="https://github.com/scrapy/scrapy/issues/4852">issue
4852</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/5358">issue
5358</a>{.reference
.external}).</p>
</li>
<li>
<p><a href="index.html#bans">[Avoiding getting banned]{.std
.std-ref}</a>{.hoverxref .tooltip .reference
.internal} now recommends <a href="https://commoncrawl.org/">Common
Crawl</a>{.reference .external} instead of
<a href="http://www.googleguide.com/cached_pages.html">Google
cache</a>{.reference
.external} (<a href="https://github.com/scrapy/scrapy/issues/3582">issue
3582</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/5432">issue
5432</a>{.reference
.external}).</p>
</li>
<li>
<p>The new <a href="index.html#topics-components">[Components]{.std
.std-ref}</a>{.hoverxref .tooltip
.reference .internal} topic covers enforcing requirements on Scrapy
components, like <a href="index.html#topics-downloader-middleware">[downloader middlewares]{.std
.std-ref}</a>{.hoverxref
.tooltip .reference .internal}, <a href="index.html#topics-extensions">[extensions]{.std
.std-ref}</a>{.hoverxref .tooltip
.reference .internal}, <a href="index.html#topics-item-pipeline">[item pipelines]{.std
.std-ref}</a>{.hoverxref .tooltip
.reference .internal}, <a href="index.html#topics-spider-middleware">[spider middlewares]{.std
.std-ref}</a>{.hoverxref .tooltip
.reference .internal}, and more; <a href="index.html#enforce-asyncio-requirement">[Enforcing asyncio as a
requirement]{.std
.std-ref}</a>{.hoverxref
.tooltip .reference .internal} has also been added (<a href="https://github.com/scrapy/scrapy/issues/4978">issue
4978</a>{.reference
.external}).</p>
</li>
<li>
<p><a href="index.html#topics-settings">[Settings]{.std .std-ref}</a>{.hoverxref
.tooltip .reference .internal} now indicates that setting values
must be <a href="https://docs.python.org/3/library/pickle.html#pickle-picklable" title="(in Python v3.12)">[picklable]{.xref .std
.std-ref}</a>{.reference
.external} (<a href="https://github.com/scrapy/scrapy/issues/5607">issue
5607</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/5629">issue
5629</a>{.reference
.external}).</p>
</li>
<li>
<p>Removed outdated documentation (<a href="https://github.com/scrapy/scrapy/issues/5446">issue
5446</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/5373">issue
5373</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/5369">issue
5369</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/5370">issue
5370</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/5554">issue
5554</a>{.reference
.external}).</p>
</li>
<li>
<p>Fixed typos (<a href="https://github.com/scrapy/scrapy/issues/5442">issue
5442</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/5455">issue
5455</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/5457">issue
5457</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/5461">issue
5461</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/5538">issue
5538</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/5553">issue
5553</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/5558">issue
5558</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/5624">issue
5624</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/5631">issue
5631</a>{.reference
.external}).</p>
</li>
<li>
<p>Fixed other issues (<a href="https://github.com/scrapy/scrapy/issues/5283">issue
5283</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/5284">issue
5284</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/5559">issue
5559</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/5567">issue
5567</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/5648">issue
5648</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/5659">issue
5659</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/5665">issue
5665</a>{.reference
.external}).
:::</p>
</li>
</ul>
<p>::: {#id28 .section}</p>
<h5 id="quality-assuranceheaderlink-5"><a class="header" href="#quality-assuranceheaderlink-5">Quality assurance<a href="#id28" title="Permalink to this heading">¶</a>{.headerlink}</a></h5>
<ul>
<li>
<p>Added a continuous integration job to run <a href="https://twine.readthedocs.io/en/stable/#twine-check">twine
check</a>{.reference
.external} (<a href="https://github.com/scrapy/scrapy/issues/5655">issue
5655</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/5656">issue
5656</a>{.reference
.external}).</p>
</li>
<li>
<p>Addressed test issues and warnings (<a href="https://github.com/scrapy/scrapy/issues/5560">issue
5560</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/5561">issue
5561</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/5612">issue
5612</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/5617">issue
5617</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/5639">issue
5639</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/5645">issue
5645</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/5662">issue
5662</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/5671">issue
5671</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/5675">issue
5675</a>{.reference
.external}).</p>
</li>
<li>
<p>Cleaned up code (<a href="https://github.com/scrapy/scrapy/issues/4991">issue
4991</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/4995">issue
4995</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/5451">issue
5451</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/5487">issue
5487</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/5542">issue
5542</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/5667">issue
5667</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/5668">issue
5668</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/5672">issue
5672</a>{.reference
.external}).</p>
</li>
<li>
<p>Applied minor code improvements (<a href="https://github.com/scrapy/scrapy/issues/5661">issue
5661</a>{.reference
.external}).
:::
:::</p>
</li>
</ul>
<p>::: {#scrapy-2-6-3-2022-09-27 .section}
[]{#release-2-6-3}</p>
<h4 id="scrapy-263-2022-09-27headerlink"><a class="header" href="#scrapy-263-2022-09-27headerlink">Scrapy 2.6.3 (2022-09-27)<a href="#scrapy-2-6-3-2022-09-27" title="Permalink to this heading">¶</a>{.headerlink}</a></h4>
<ul>
<li>
<p>Added support for
<a href="https://www.pyopenssl.org/en/stable/">pyOpenSSL</a>{.reference
.external} 22.1.0, removing support for SSLv3 (<a href="https://github.com/scrapy/scrapy/issues/5634">issue
5634</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/5635">issue
5635</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/5636">issue
5636</a>{.reference
.external}).</p>
</li>
<li>
<p>Upgraded the minimum versions of the following dependencies:</p>
<ul>
<li>
<p><a href="https://cryptography.io/en/latest/">cryptography</a>{.reference
.external}: 2.0 → 3.3</p>
</li>
<li>
<p><a href="https://www.pyopenssl.org/en/stable/">pyOpenSSL</a>{.reference
.external}: 16.2.0 → 21.0.0</p>
</li>
<li>
<p><a href="https://service-identity.readthedocs.io/en/stable/">service_identity</a>{.reference
.external}: 16.0.0 → 18.1.0</p>
</li>
<li>
<p><a href="https://twistedmatrix.com/trac/">Twisted</a>{.reference
.external}: 17.9.0 → 18.9.0</p>
</li>
<li>
<p><a href="https://zopeinterface.readthedocs.io/en/latest/">zope.interface</a>{.reference
.external}: 4.1.3 → 5.0.0</p>
</li>
</ul>
<p>(<a href="https://github.com/scrapy/scrapy/issues/5621">issue
5621</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/5632">issue
5632</a>{.reference
.external})</p>
</li>
<li>
<p>Fixes test and documentation issues (<a href="https://github.com/scrapy/scrapy/issues/5612">issue
5612</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/5617">issue
5617</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/5631">issue
5631</a>{.reference
.external}).
:::</p>
</li>
</ul>
<p>::: {#scrapy-2-6-2-2022-07-25 .section}
[]{#release-2-6-2}</p>
<h4 id="scrapy-262-2022-07-25headerlink"><a class="header" href="#scrapy-262-2022-07-25headerlink">Scrapy 2.6.2 (2022-07-25)<a href="#scrapy-2-6-2-2022-07-25" title="Permalink to this heading">¶</a>{.headerlink}</a></h4>
<p><strong>Security bug fix:</strong></p>
<ul>
<li>
<p>When <a href="index.html#scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware" title="scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware">[<code>HttpProxyMiddleware</code>{.xref .py .py-class .docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal} processes a request with <a href="index.html#std-reqmeta-proxy">[<code>proxy</code>{.xref .std
.std-reqmeta .docutils .literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal} metadata, and that <a href="index.html#std-reqmeta-proxy">[<code>proxy</code>{.xref
.std .std-reqmeta .docutils .literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal} metadata includes proxy credentials,
<a href="index.html#scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware" title="scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware">[<code>HttpProxyMiddleware</code>{.xref .py .py-class .docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal} sets the [<code>Proxy-Authorization</code>{.docutils .literal
.notranslate}]{.pre} header, but only if that header is not already
set.</p>
<p>There are third-party proxy-rotation downloader middlewares that set
different <a href="index.html#std-reqmeta-proxy">[<code>proxy</code>{.xref .std .std-reqmeta .docutils .literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal} metadata every time they process a
request.</p>
<p>Because of request retries and redirects, the same request can be
processed by downloader middlewares more than once, including both
<a href="index.html#scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware" title="scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware">[<code>HttpProxyMiddleware</code>{.xref .py .py-class .docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal} and any third-party proxy-rotation downloader middleware.</p>
<p>These third-party proxy-rotation downloader middlewares could change
the <a href="index.html#std-reqmeta-proxy">[<code>proxy</code>{.xref .std .std-reqmeta .docutils .literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal} metadata of a request to a new value,
but fail to remove the [<code>Proxy-Authorization</code>{.docutils .literal
.notranslate}]{.pre} header from the previous value of the
<a href="index.html#std-reqmeta-proxy">[<code>proxy</code>{.xref .std .std-reqmeta .docutils .literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal} metadata, causing the credentials of
one proxy to be sent to a different proxy.</p>
<p>To prevent the unintended leaking of proxy credentials, the behavior
of <a href="index.html#scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware" title="scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware">[<code>HttpProxyMiddleware</code>{.xref .py .py-class .docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal} is now as follows when processing a request:</p>
<ul>
<li>
<p>If the request being processed defines <a href="index.html#std-reqmeta-proxy">[<code>proxy</code>{.xref .std
.std-reqmeta .docutils .literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal} metadata that includes
credentials, the [<code>Proxy-Authorization</code>{.docutils .literal
.notranslate}]{.pre} header is always updated to feature those
credentials.</p>
</li>
<li>
<p>If the request being processed defines <a href="index.html#std-reqmeta-proxy">[<code>proxy</code>{.xref .std
.std-reqmeta .docutils .literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal} metadata without credentials, the
[<code>Proxy-Authorization</code>{.docutils .literal .notranslate}]{.pre}
header is removed <em>unless</em> it was originally defined for the
same proxy URL.</p>
<p>To remove proxy credentials while keeping the same proxy URL,
remove the [<code>Proxy-Authorization</code>{.docutils .literal
.notranslate}]{.pre} header.</p>
</li>
<li>
<p>If the request has no <a href="index.html#std-reqmeta-proxy">[<code>proxy</code>{.xref .std .std-reqmeta
.docutils .literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal} metadata, or that metadata is a
falsy value (e.g. [<code>None</code>{.docutils .literal
.notranslate}]{.pre}), the [<code>Proxy-Authorization</code>{.docutils
.literal .notranslate}]{.pre} header is removed.</p>
<p>It is no longer possible to set a proxy URL through the
<a href="index.html#std-reqmeta-proxy">[<code>proxy</code>{.xref .std .std-reqmeta .docutils .literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal} metadata but set the credentials
through the [<code>Proxy-Authorization</code>{.docutils .literal
.notranslate}]{.pre} header. Set proxy credentials through the
<a href="index.html#std-reqmeta-proxy">[<code>proxy</code>{.xref .std .std-reqmeta .docutils .literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal} metadata instead.</p>
</li>
</ul>
</li>
</ul>
<p>Also fixes the following regressions introduced in 2.6.0:</p>
<ul>
<li>
<p><a href="index.html#scrapy.crawler.CrawlerProcess" title="scrapy.crawler.CrawlerProcess">[<code>CrawlerProcess</code>{.xref .py .py-class .docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal} supports again crawling multiple spiders (<a href="https://github.com/scrapy/scrapy/issues/5435">issue
5435</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/5436">issue
5436</a>{.reference
.external})</p>
</li>
<li>
<p>Installing a Twisted reactor before Scrapy does (e.g. importing
<a href="https://docs.twisted.org/en/stable/api/twisted.internet.reactor.html" title="(in Twisted)">[<code>twisted.internet.reactor</code>{.xref .py .py-mod .docutils .literal
.notranslate}]{.pre}</a>{.reference
.external} somewhere at the module level) no longer prevents Scrapy
from starting, as long as a different reactor is not specified in
<a href="index.html#std-setting-TWISTED_REACTOR">[<code>TWISTED_REACTOR</code>{.xref .std .std-setting .docutils .literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal} (<a href="https://github.com/scrapy/scrapy/issues/5525">issue
5525</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/5528">issue
5528</a>{.reference
.external})</p>
</li>
<li>
<p>Fixed an exception that was being logged after the spider finished
under certain conditions (<a href="https://github.com/scrapy/scrapy/issues/5437">issue
5437</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/5440">issue
5440</a>{.reference
.external})</p>
</li>
<li>
<p>The [<code>--output</code>{.docutils .literal
.notranslate}]{.pre}/[<code>-o</code>{.docutils .literal .notranslate}]{.pre}
command-line parameter supports again a value starting with a hyphen
(<a href="https://github.com/scrapy/scrapy/issues/5444">issue
5444</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/5445">issue
5445</a>{.reference
.external})</p>
</li>
<li>
<p>The [<code>scrapy</code>{.docutils .literal .notranslate}]{.pre}<code> </code>{.docutils
.literal .notranslate}[<code>parse</code>{.docutils .literal
.notranslate}]{.pre}<code> </code>{.docutils .literal
.notranslate}[<code>-h</code>{.docutils .literal .notranslate}]{.pre} command
no longer throws an error (<a href="https://github.com/scrapy/scrapy/issues/5481">issue
5481</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/5482">issue
5482</a>{.reference
.external})
:::</p>
</li>
</ul>
<p>::: {#scrapy-2-6-1-2022-03-01 .section}
[]{#release-2-6-1}</p>
<h4 id="scrapy-261-2022-03-01headerlink"><a class="header" href="#scrapy-261-2022-03-01headerlink">Scrapy 2.6.1 (2022-03-01)<a href="#scrapy-2-6-1-2022-03-01" title="Permalink to this heading">¶</a>{.headerlink}</a></h4>
<p>Fixes a regression introduced in 2.6.0 that would unset the request
method when following redirects.
:::</p>
<p>::: {#scrapy-2-6-0-2022-03-01 .section}
[]{#release-2-6-0}</p>
<h4 id="scrapy-260-2022-03-01headerlink"><a class="header" href="#scrapy-260-2022-03-01headerlink">Scrapy 2.6.0 (2022-03-01)<a href="#scrapy-2-6-0-2022-03-01" title="Permalink to this heading">¶</a>{.headerlink}</a></h4>
<p>Highlights:</p>
<ul>
<li>
<p><a href="#security-fixes">[Security fixes for cookie handling]{.std
.std-ref}</a>{.hoverxref .tooltip .reference
.internal}</p>
</li>
<li>
<p>Python 3.10 support</p>
</li>
<li>
<p><a href="index.html#using-asyncio">[asyncio support]{.std
.std-ref}</a>{.hoverxref .tooltip .reference
.internal} is no longer considered experimental, and works
out-of-the-box on Windows regardless of your Python version</p>
</li>
<li>
<p>Feed exports now support <a href="https://docs.python.org/3/library/pathlib.html#pathlib.Path" title="(in Python v3.12)">[<code>pathlib.Path</code>{.xref .py .py-class
.docutils .literal
.notranslate}]{.pre}</a>{.reference
.external} output paths and per-feed <a href="index.html#item-filter">[item filtering]{.std
.std-ref}</a>{.hoverxref .tooltip .reference
.internal} and <a href="index.html#post-processing">[post-processing]{.std
.std-ref}</a>{.hoverxref .tooltip
.reference .internal}</p>
</li>
</ul>
<p>::: {#security-bug-fixes .section}
[]{#security-fixes}</p>
<h5 id="security-bug-fixesheaderlink"><a class="header" href="#security-bug-fixesheaderlink">Security bug fixes<a href="#security-bug-fixes" title="Permalink to this heading">¶</a>{.headerlink}</a></h5>
<ul>
<li>
<p>When a <a href="index.html#scrapy.http.Request" title="scrapy.http.Request">[<code>Request</code>{.xref .py .py-class .docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal} object with cookies defined gets a redirect response
causing a new <a href="index.html#scrapy.http.Request" title="scrapy.http.Request">[<code>Request</code>{.xref .py .py-class .docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal} object to be scheduled, the cookies defined in the
original <a href="index.html#scrapy.http.Request" title="scrapy.http.Request">[<code>Request</code>{.xref .py .py-class .docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal} object are no longer copied into the new
<a href="index.html#scrapy.http.Request" title="scrapy.http.Request">[<code>Request</code>{.xref .py .py-class .docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal} object.</p>
<p>If you manually set the [<code>Cookie</code>{.docutils .literal
.notranslate}]{.pre} header on a <a href="index.html#scrapy.http.Request" title="scrapy.http.Request">[<code>Request</code>{.xref .py .py-class
.docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal} object and the domain name of the redirect URL is not an
exact match for the domain of the URL of the original
<a href="index.html#scrapy.http.Request" title="scrapy.http.Request">[<code>Request</code>{.xref .py .py-class .docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal} object, your [<code>Cookie</code>{.docutils .literal
.notranslate}]{.pre} header is now dropped from the new
<a href="index.html#scrapy.http.Request" title="scrapy.http.Request">[<code>Request</code>{.xref .py .py-class .docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal} object.</p>
<p>The old behavior could be exploited by an attacker to gain access to
your cookies. Please, see the <a href="https://github.com/scrapy/scrapy/security/advisories/GHSA-cjvr-mfj7-j4j8">cjvr-mfj7-j4j8 security
advisory</a>{.reference
.external} for more information.</p>
<p>::: {.admonition .note}
Note</p>
<p>It is still possible to enable the sharing of cookies between
different domains with a shared domain suffix (e.g.
[<code>example.com</code>{.docutils .literal .notranslate}]{.pre} and any
subdomain) by defining the shared domain suffix (e.g.
[<code>example.com</code>{.docutils .literal .notranslate}]{.pre}) as the
cookie domain when defining your cookies. See the documentation of
the <a href="index.html#scrapy.http.Request" title="scrapy.http.Request">[<code>Request</code>{.xref .py .py-class .docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal} class for more information.
:::</p>
</li>
<li>
<p>When the domain of a cookie, either received in the
[<code>Set-Cookie</code>{.docutils .literal .notranslate}]{.pre} header of a
response or defined in a <a href="index.html#scrapy.http.Request" title="scrapy.http.Request">[<code>Request</code>{.xref .py .py-class .docutils
.literal
.notranslate}]{.pre}</a>{.reference
.internal} object, is set to a <a href="https://publicsuffix.org/">public
suffix</a>{.reference .external}, the cookie
is now ignored unless the cookie domain is the same as the request
domain.</p>
<p>The old behavior could be exploited by an attacker to inject cookies
from a controlled domain into your cookiejar that could be sent to
other domains not controlled by the attacker. Please, see the
<a href="https://github.com/scrapy/scrapy/security/advisories/GHSA-mfjm-vh54-3f96">mfjm-vh54-3f96 security
advisory</a>{.reference
.external} for more information.
:::</p>
</li>
</ul>
<p>::: {#id29 .section}</p>
<h5 id="modified-requirementsheaderlink-2"><a class="header" href="#modified-requirementsheaderlink-2">Modified requirements<a href="#id29" title="Permalink to this heading">¶</a>{.headerlink}</a></h5>
<ul>
<li>The <a href="https://pypi.org/project/h2/">h2</a>{.reference .external}
dependency is now optional, only needed to <a href="index.html#http2">[enable HTTP/2
support]{.std .std-ref}</a>{.hoverxref .tooltip
.reference .internal}. (<a href="https://github.com/scrapy/scrapy/issues/5113">issue
5113</a>{.reference
.external})
:::</li>
</ul>
<p>::: {#id30 .section}</p>
<h5 id="backward-incompatible-changesheaderlink-2"><a class="header" href="#backward-incompatible-changesheaderlink-2">Backward-incompatible changes<a href="#id30" title="Permalink to this heading">¶</a>{.headerlink}</a></h5>
<ul>
<li>
<p>The [<code>formdata</code>{.docutils .literal .notranslate}]{.pre} parameter of
[<code>FormRequest</code>{.xref .py .py-class .docutils .literal
.notranslate}]{.pre}, if specified for a non-POST request, now
overrides the URL query string, instead of being appended to it.
(<a href="https://github.com/scrapy/scrapy/issues/2919">issue
2919</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/3579">issue
3579</a>{.reference
.external})</p>
</li>
<li>
<p>When a function is assigned to the <a href="index.html#std-setting-FEED_URI_PARAMS">[<code>FEED_URI_PARAMS</code>{.xref .std
.std-setting .docutils .literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal} setting, now the return value of that
function, and not the [<code>params</code>{.docutils .literal
.notranslate}]{.pre} input parameter, will determine the feed URI
parameters, unless that return value is [<code>None</code>{.docutils .literal
.notranslate}]{.pre}. (<a href="https://github.com/scrapy/scrapy/issues/4962">issue
4962</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/4966">issue
4966</a>{.reference
.external})</p>
</li>
<li>
<p>In [<code>scrapy.core.engine.ExecutionEngine</code>{.xref .py .py-class
.docutils .literal .notranslate}]{.pre}, methods [<code>crawl()</code>{.xref
.py .py-meth .docutils .literal .notranslate}]{.pre},
[<code>download()</code>{.xref .py .py-meth .docutils .literal
.notranslate}]{.pre}, [<code>schedule()</code>{.xref .py .py-meth .docutils
.literal .notranslate}]{.pre}, and [<code>spider_is_idle()</code>{.xref .py
.py-meth .docutils .literal .notranslate}]{.pre} now raise
<a href="https://docs.python.org/3/library/exceptions.html#RuntimeError" title="(in Python v3.12)">[<code>RuntimeError</code>{.xref .py .py-exc .docutils .literal
.notranslate}]{.pre}</a>{.reference
.external} if called before [<code>open_spider()</code>{.xref .py .py-meth
.docutils .literal .notranslate}]{.pre}. (<a href="https://github.com/scrapy/scrapy/issues/5090">issue
5090</a>{.reference
.external})</p>
<p>These methods used to assume that [<code>ExecutionEngine.slot</code>{.xref .py
.py-attr .docutils .literal .notranslate}]{.pre} had been defined by
a prior call to [<code>open_spider()</code>{.xref .py .py-meth .docutils
.literal .notranslate}]{.pre}, so they were raising
<a href="https://docs.python.org/3/library/exceptions.html#AttributeError" title="(in Python v3.12)">[<code>AttributeError</code>{.xref .py .py-exc .docutils .literal
.notranslate}]{.pre}</a>{.reference
.external} instead.</p>
</li>
<li>
<p>If the API of the configured <a href="index.html#topics-scheduler">[scheduler]{.std
.std-ref}</a>{.hoverxref .tooltip
.reference .internal} does not meet expectations,
<a href="https://docs.python.org/3/library/exceptions.html#TypeError" title="(in Python v3.12)">[<code>TypeError</code>{.xref .py .py-exc .docutils .literal
.notranslate}]{.pre}</a>{.reference
.external} is now raised at startup time. Before, other exceptions
would be raised at run time. (<a href="https://github.com/scrapy/scrapy/issues/3559">issue
3559</a>{.reference
.external})</p>
</li>
<li>
<p>The [<code>_encoding</code>{.docutils .literal .notranslate}]{.pre} field of
serialized <a href="index.html#scrapy.http.Request" title="scrapy.http.Request">[<code>Request</code>{.xref .py .py-class .docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal} objects is now named [<code>encoding</code>{.docutils .literal
.notranslate}]{.pre}, in line with all other fields (<a href="https://github.com/scrapy/scrapy/issues/5130">issue
5130</a>{.reference
.external})
:::</p>
</li>
</ul>
<p>::: {#id31 .section}</p>
<h5 id="deprecation-removalsheaderlink-3"><a class="header" href="#deprecation-removalsheaderlink-3">Deprecation removals<a href="#id31" title="Permalink to this heading">¶</a>{.headerlink}</a></h5>
<ul>
<li>
<p>[<code>scrapy.http.TextResponse.body_as_unicode</code>{.docutils .literal
.notranslate}]{.pre}, deprecated in Scrapy 2.2, has now been
removed. (<a href="https://github.com/scrapy/scrapy/issues/5393">issue
5393</a>{.reference
.external})</p>
</li>
<li>
<p>[<code>scrapy.item.BaseItem</code>{.docutils .literal .notranslate}]{.pre},
deprecated in Scrapy 2.2, has now been removed. (<a href="https://github.com/scrapy/scrapy/issues/5398">issue
5398</a>{.reference
.external})</p>
</li>
<li>
<p>[<code>scrapy.item.DictItem</code>{.docutils .literal .notranslate}]{.pre},
deprecated in Scrapy 1.8, has now been removed. (<a href="https://github.com/scrapy/scrapy/issues/5398">issue
5398</a>{.reference
.external})</p>
</li>
<li>
<p>[<code>scrapy.Spider.make_requests_from_url</code>{.docutils .literal
.notranslate}]{.pre}, deprecated in Scrapy 1.4, has now been
removed. (<a href="https://github.com/scrapy/scrapy/issues/4178">issue
4178</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/4356">issue
4356</a>{.reference
.external})
:::</p>
</li>
</ul>
<p>::: {#id32 .section}</p>
<h5 id="deprecationsheaderlink-5"><a class="header" href="#deprecationsheaderlink-5">Deprecations<a href="#id32" title="Permalink to this heading">¶</a>{.headerlink}</a></h5>
<ul>
<li>
<p>When a function is assigned to the <a href="index.html#std-setting-FEED_URI_PARAMS">[<code>FEED_URI_PARAMS</code>{.xref .std
.std-setting .docutils .literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal} setting, returning [<code>None</code>{.docutils
.literal .notranslate}]{.pre} or modifying the [<code>params</code>{.docutils
.literal .notranslate}]{.pre} input parameter is now deprecated.
Return a new dictionary instead. (<a href="https://github.com/scrapy/scrapy/issues/4962">issue
4962</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/4966">issue
4966</a>{.reference
.external})</p>
</li>
<li>
<p>[<code>scrapy.utils.reqser</code>{.xref .py .py-mod .docutils .literal
.notranslate}]{.pre} is deprecated. (<a href="https://github.com/scrapy/scrapy/issues/5130">issue
5130</a>{.reference
.external})</p>
<ul>
<li>
<p>Instead of [<code>request_to_dict()</code>{.xref .py .py-func .docutils
.literal .notranslate}]{.pre}, use the new
<a href="index.html#scrapy.http.Request.to_dict" title="scrapy.http.Request.to_dict">[<code>Request.to_dict</code>{.xref .py .py-meth .docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal} method.</p>
</li>
<li>
<p>Instead of [<code>request_from_dict()</code>{.xref .py .py-func .docutils
.literal .notranslate}]{.pre}, use the new
<a href="index.html#scrapy.utils.request.request_from_dict" title="scrapy.utils.request.request_from_dict">[<code>scrapy.utils.request.request_from_dict()</code>{.xref .py .py-func
.docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal} function.</p>
</li>
</ul>
</li>
<li>
<p>In [<code>scrapy.squeues</code>{.xref .py .py-mod .docutils .literal
.notranslate}]{.pre}, the following queue classes are deprecated:
[<code>PickleFifoDiskQueueNonRequest</code>{.xref .py .py-class .docutils
.literal .notranslate}]{.pre},
[<code>PickleLifoDiskQueueNonRequest</code>{.xref .py .py-class .docutils
.literal .notranslate}]{.pre},
[<code>MarshalFifoDiskQueueNonRequest</code>{.xref .py .py-class .docutils
.literal .notranslate}]{.pre}, and
[<code>MarshalLifoDiskQueueNonRequest</code>{.xref .py .py-class .docutils
.literal .notranslate}]{.pre}. You should instead use:
[<code>PickleFifoDiskQueue</code>{.xref .py .py-class .docutils .literal
.notranslate}]{.pre}, [<code>PickleLifoDiskQueue</code>{.xref .py .py-class
.docutils .literal .notranslate}]{.pre},
[<code>MarshalFifoDiskQueue</code>{.xref .py .py-class .docutils .literal
.notranslate}]{.pre}, and [<code>MarshalLifoDiskQueue</code>{.xref .py
.py-class .docutils .literal .notranslate}]{.pre}. (<a href="https://github.com/scrapy/scrapy/issues/5117">issue
5117</a>{.reference
.external})</p>
</li>
<li>
<p>Many aspects of [<code>scrapy.core.engine.ExecutionEngine</code>{.xref .py
.py-class .docutils .literal .notranslate}]{.pre} that come from a
time when this class could handle multiple <a href="index.html#scrapy.Spider" title="scrapy.Spider">[<code>Spider</code>{.xref .py
.py-class .docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal} objects at a time have been deprecated. (<a href="https://github.com/scrapy/scrapy/issues/5090">issue
5090</a>{.reference
.external})</p>
<ul>
<li>
<p>The [<code>has_capacity()</code>{.xref .py .py-meth .docutils .literal
.notranslate}]{.pre} method is deprecated.</p>
</li>
<li>
<p>The [<code>schedule()</code>{.xref .py .py-meth .docutils .literal
.notranslate}]{.pre} method is deprecated, use [<code>crawl()</code>{.xref
.py .py-meth .docutils .literal .notranslate}]{.pre} or
[<code>download()</code>{.xref .py .py-meth .docutils .literal
.notranslate}]{.pre} instead.</p>
</li>
<li>
<p>The [<code>open_spiders</code>{.xref .py .py-attr .docutils .literal
.notranslate}]{.pre} attribute is deprecated, use
[<code>spider</code>{.xref .py .py-attr .docutils .literal
.notranslate}]{.pre} instead.</p>
</li>
<li>
<p>The [<code>spider</code>{.docutils .literal .notranslate}]{.pre} parameter
is deprecated for the following methods:</p>
<ul>
<li>
<p>[<code>spider_is_idle()</code>{.xref .py .py-meth .docutils .literal
.notranslate}]{.pre}</p>
</li>
<li>
<p>[<code>crawl()</code>{.xref .py .py-meth .docutils .literal
.notranslate}]{.pre}</p>
</li>
<li>
<p>[<code>download()</code>{.xref .py .py-meth .docutils .literal
.notranslate}]{.pre}</p>
</li>
</ul>
<p>Instead, call [<code>open_spider()</code>{.xref .py .py-meth .docutils
.literal .notranslate}]{.pre} first to set the <a href="index.html#scrapy.Spider" title="scrapy.Spider">[<code>Spider</code>{.xref
.py .py-class .docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal} object.
:::</p>
</li>
</ul>
</li>
</ul>
<p>::: {#id33 .section}</p>
<h5 id="new-featuresheaderlink-6"><a class="header" href="#new-featuresheaderlink-6">New features<a href="#id33" title="Permalink to this heading">¶</a>{.headerlink}</a></h5>
<ul>
<li>
<p>You can now use <a href="index.html#item-filter">[item filtering]{.std
.std-ref}</a>{.hoverxref .tooltip .reference
.internal} to control which items are exported to each output feed.
(<a href="https://github.com/scrapy/scrapy/issues/4575">issue
4575</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/5178">issue
5178</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/5161">issue
5161</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/5203">issue
5203</a>{.reference
.external})</p>
</li>
<li>
<p>You can now apply <a href="index.html#post-processing">[post-processing]{.std
.std-ref}</a>{.hoverxref .tooltip
.reference .internal} to feeds, and <a href="index.html#builtin-plugins">[built-in post-processing
plugins]{.std .std-ref}</a>{.hoverxref
.tooltip .reference .internal} are provided for output file
compression. (<a href="https://github.com/scrapy/scrapy/issues/2174">issue
2174</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/5168">issue
5168</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/5190">issue
5190</a>{.reference
.external})</p>
</li>
<li>
<p>The <a href="index.html#std-setting-FEEDS">[<code>FEEDS</code>{.xref .std .std-setting .docutils .literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal} setting now supports
<a href="https://docs.python.org/3/library/pathlib.html#pathlib.Path" title="(in Python v3.12)">[<code>pathlib.Path</code>{.xref .py .py-class .docutils .literal
.notranslate}]{.pre}</a>{.reference
.external} objects as keys. (<a href="https://github.com/scrapy/scrapy/issues/5383">issue
5383</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/5384">issue
5384</a>{.reference
.external})</p>
</li>
<li>
<p>Enabling <a href="index.html#using-asyncio">[asyncio]{.std
.std-ref}</a>{.hoverxref .tooltip .reference
.internal} while using Windows and Python 3.8 or later will
automatically switch the asyncio event loop to one that allows
Scrapy to work. See <a href="index.html#asyncio-windows">[Windows-specific notes]{.std
.std-ref}</a>{.hoverxref .tooltip
.reference .internal}. (<a href="https://github.com/scrapy/scrapy/issues/4976">issue
4976</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/5315">issue
5315</a>{.reference
.external})</p>
</li>
<li>
<p>The <a href="index.html#std-command-genspider">[<code>genspider</code>{.xref .std .std-command .docutils .literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal} command now supports a start URL
instead of a domain name. (<a href="https://github.com/scrapy/scrapy/issues/4439">issue
4439</a>{.reference
.external})</p>
</li>
<li>
<p>[<code>scrapy.utils.defer</code>{.xref .py .py-mod .docutils .literal
.notranslate}]{.pre} gained 2 new functions,
<a href="index.html#scrapy.utils.defer.deferred_to_future" title="scrapy.utils.defer.deferred_to_future">[<code>deferred_to_future()</code>{.xref .py .py-func .docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal} and <a href="index.html#scrapy.utils.defer.maybe_deferred_to_future" title="scrapy.utils.defer.maybe_deferred_to_future">[<code>maybe_deferred_to_future()</code>{.xref .py .py-func
.docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal}, to help <a href="index.html#asyncio-await-dfd">[await on Deferreds when using the asyncio
reactor]{.std .std-ref}</a>{.hoverxref
.tooltip .reference .internal}. (<a href="https://github.com/scrapy/scrapy/issues/5288">issue
5288</a>{.reference
.external})</p>
</li>
<li>
<p><a href="index.html#topics-feed-storage-s3">[Amazon S3 feed export storage]{.std
.std-ref}</a>{.hoverxref .tooltip
.reference .internal} gained support for <a href="https://docs.aws.amazon.com/general/latest/gr/aws-sec-cred-types.html#temporary-access-keys">temporary security
credentials</a>{.reference
.external} (<a href="index.html#std-setting-AWS_SESSION_TOKEN">[<code>AWS_SESSION_TOKEN</code>{.xref .std .std-setting .docutils
.literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal}) and endpoint customization
(<a href="index.html#std-setting-AWS_ENDPOINT_URL">[<code>AWS_ENDPOINT_URL</code>{.xref .std .std-setting .docutils .literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal}). (<a href="https://github.com/scrapy/scrapy/issues/4998">issue
4998</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/5210">issue
5210</a>{.reference
.external})</p>
</li>
<li>
<p>New <a href="index.html#std-setting-LOG_FILE_APPEND">[<code>LOG_FILE_APPEND</code>{.xref .std .std-setting .docutils .literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal} setting to allow truncating the log
file. (<a href="https://github.com/scrapy/scrapy/issues/5279">issue
5279</a>{.reference
.external})</p>
</li>
<li>
<p>[<code>Request.cookies</code>{.xref .py .py-attr .docutils .literal
.notranslate}]{.pre} values that are <a href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.12)">[<code>bool</code>{.xref .py .py-class
.docutils .literal
.notranslate}]{.pre}</a>{.reference
.external}, <a href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.12)">[<code>float</code>{.xref .py .py-class .docutils .literal
.notranslate}]{.pre}</a>{.reference
.external} or <a href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.12)">[<code>int</code>{.xref .py .py-class .docutils .literal
.notranslate}]{.pre}</a>{.reference
.external} are cast to <a href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.12)">[<code>str</code>{.xref .py .py-class .docutils
.literal
.notranslate}]{.pre}</a>{.reference
.external}. (<a href="https://github.com/scrapy/scrapy/issues/5252">issue
5252</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/5253">issue
5253</a>{.reference
.external})</p>
</li>
<li>
<p>You may now raise <a href="index.html#scrapy.exceptions.CloseSpider" title="scrapy.exceptions.CloseSpider">[<code>CloseSpider</code>{.xref .py .py-exc .docutils
.literal
.notranslate}]{.pre}</a>{.reference
.internal} from a handler of the <a href="index.html#std-signal-spider_idle">[<code>spider_idle</code>{.xref .std
.std-signal .docutils .literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal} signal to customize the reason why
the spider is stopping. (<a href="https://github.com/scrapy/scrapy/issues/5191">issue
5191</a>{.reference
.external})</p>
</li>
<li>
<p>When using <a href="index.html#scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware" title="scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware">[<code>HttpProxyMiddleware</code>{.xref .py .py-class .docutils
.literal
.notranslate}]{.pre}</a>{.reference
.internal}, the proxy URL for non-HTTPS HTTP/1.1 requests no longer
needs to include a URL scheme. (<a href="https://github.com/scrapy/scrapy/issues/4505">issue
4505</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/4649">issue
4649</a>{.reference
.external})</p>
</li>
<li>
<p>All built-in queues now expose a [<code>peek</code>{.docutils .literal
.notranslate}]{.pre} method that returns the next queue object (like
[<code>pop</code>{.docutils .literal .notranslate}]{.pre}) but does not remove
the returned object from the queue. (<a href="https://github.com/scrapy/scrapy/issues/5112">issue
5112</a>{.reference
.external})</p>
<p>If the underlying queue does not support peeking (e.g. because you
are not using [<code>queuelib</code>{.docutils .literal .notranslate}]{.pre}
1.6.1 or later), the [<code>peek</code>{.docutils .literal .notranslate}]{.pre}
method raises <a href="https://docs.python.org/3/library/exceptions.html#NotImplementedError" title="(in Python v3.12)">[<code>NotImplementedError</code>{.xref .py .py-exc .docutils
.literal
.notranslate}]{.pre}</a>{.reference
.external}.</p>
</li>
<li>
<p><a href="index.html#scrapy.http.Request" title="scrapy.http.Request">[<code>Request</code>{.xref .py .py-class .docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal} and <a href="index.html#scrapy.http.Response" title="scrapy.http.Response">[<code>Response</code>{.xref .py .py-class .docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal} now have an [<code>attributes</code>{.docutils .literal
.notranslate}]{.pre} attribute that makes subclassing easier. For
<a href="index.html#scrapy.http.Request" title="scrapy.http.Request">[<code>Request</code>{.xref .py .py-class .docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal}, it also allows subclasses to work with
<a href="index.html#scrapy.utils.request.request_from_dict" title="scrapy.utils.request.request_from_dict">[<code>scrapy.utils.request.request_from_dict()</code>{.xref .py .py-func
.docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal}. (<a href="https://github.com/scrapy/scrapy/issues/1877">issue
1877</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/5130">issue
5130</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/5218">issue
5218</a>{.reference
.external})</p>
</li>
<li>
<p>The <a href="index.html#scrapy.core.scheduler.BaseScheduler.open" title="scrapy.core.scheduler.BaseScheduler.open">[<code>open()</code>{.xref .py .py-meth .docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal} and <a href="index.html#scrapy.core.scheduler.BaseScheduler.close" title="scrapy.core.scheduler.BaseScheduler.close">[<code>close()</code>{.xref .py .py-meth .docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal} methods of the <a href="index.html#topics-scheduler">[scheduler]{.std
.std-ref}</a>{.hoverxref .tooltip
.reference .internal} are now optional. (<a href="https://github.com/scrapy/scrapy/issues/3559">issue
3559</a>{.reference
.external})</p>
</li>
<li>
<p>HTTP/1.1 [<code>TunnelError</code>{.xref .py .py-exc .docutils .literal
.notranslate}]{.pre} exceptions now only truncate response bodies
longer than 1000 characters, instead of those longer than 32
characters, making it easier to debug such errors. (<a href="https://github.com/scrapy/scrapy/issues/4881">issue
4881</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/5007">issue
5007</a>{.reference
.external})</p>
</li>
<li>
<p><a href="index.html#scrapy.loader.ItemLoader" title="scrapy.loader.ItemLoader">[<code>ItemLoader</code>{.xref .py .py-class .docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal} now supports non-text responses. (<a href="https://github.com/scrapy/scrapy/issues/5145">issue
5145</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/5269">issue
5269</a>{.reference
.external})
:::</p>
</li>
</ul>
<p>::: {#id34 .section}</p>
<h5 id="bug-fixesheaderlink-6"><a class="header" href="#bug-fixesheaderlink-6">Bug fixes<a href="#id34" title="Permalink to this heading">¶</a>{.headerlink}</a></h5>
<ul>
<li>
<p>The <a href="index.html#std-setting-TWISTED_REACTOR">[<code>TWISTED_REACTOR</code>{.xref .std .std-setting .docutils .literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal} and <a href="index.html#std-setting-ASYNCIO_EVENT_LOOP">[<code>ASYNCIO_EVENT_LOOP</code>{.xref .std
.std-setting .docutils .literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal} settings are no longer ignored if
defined in <a href="index.html#scrapy.Spider.custom_settings" title="scrapy.Spider.custom_settings">[<code>custom_settings</code>{.xref .py .py-attr .docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal}. (<a href="https://github.com/scrapy/scrapy/issues/4485">issue
4485</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/5352">issue
5352</a>{.reference
.external})</p>
</li>
<li>
<p>Removed a module-level Twisted reactor import that could prevent
<a href="index.html#using-asyncio">[using the asyncio reactor]{.std
.std-ref}</a>{.hoverxref .tooltip .reference
.internal}. (<a href="https://github.com/scrapy/scrapy/issues/5357">issue
5357</a>{.reference
.external})</p>
</li>
<li>
<p>The <a href="index.html#std-command-startproject">[<code>startproject</code>{.xref .std .std-command .docutils .literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal} command works with existing folders
again. (<a href="https://github.com/scrapy/scrapy/issues/4665">issue
4665</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/4676">issue
4676</a>{.reference
.external})</p>
</li>
<li>
<p>The <a href="index.html#std-setting-FEED_URI_PARAMS">[<code>FEED_URI_PARAMS</code>{.xref .std .std-setting .docutils .literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal} setting now behaves as documented.
(<a href="https://github.com/scrapy/scrapy/issues/4962">issue
4962</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/4966">issue
4966</a>{.reference
.external})</p>
</li>
<li>
<p>[<code>Request.cb_kwargs</code>{.xref .py .py-attr .docutils .literal
.notranslate}]{.pre} once again allows the [<code>callback</code>{.docutils
.literal .notranslate}]{.pre} keyword. (<a href="https://github.com/scrapy/scrapy/issues/5237">issue
5237</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/5251">issue
5251</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/5264">issue
5264</a>{.reference
.external})</p>
</li>
<li>
<p>Made [<code>scrapy.utils.response.open_in_browser()</code>{.xref .py .py-func
.docutils .literal .notranslate}]{.pre} support more complex HTML.
(<a href="https://github.com/scrapy/scrapy/issues/5319">issue
5319</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/5320">issue
5320</a>{.reference
.external})</p>
</li>
<li>
<p>Fixed <a href="index.html#scrapy.spiders.CSVFeedSpider.quotechar" title="scrapy.spiders.CSVFeedSpider.quotechar">[<code>CSVFeedSpider.quotechar</code>{.xref .py .py-attr .docutils
.literal
.notranslate}]{.pre}</a>{.reference
.internal} being interpreted as the CSV file encoding. (<a href="https://github.com/scrapy/scrapy/issues/5391">issue
5391</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/5394">issue
5394</a>{.reference
.external})</p>
</li>
<li>
<p>Added missing
<a href="https://pypi.org/project/setuptools/">setuptools</a>{.reference
.external} to the list of dependencies. (<a href="https://github.com/scrapy/scrapy/issues/5122">issue
5122</a>{.reference
.external})</p>
</li>
<li>
<p><a href="index.html#scrapy.linkextractors.lxmlhtml.LxmlLinkExtractor" title="scrapy.linkextractors.lxmlhtml.LxmlLinkExtractor">[<code>LinkExtractor</code>{.xref .py .py-class .docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal} now also works as expected with links that have
comma-separated [<code>rel</code>{.docutils .literal .notranslate}]{.pre}
attribute values including [<code>nofollow</code>{.docutils .literal
.notranslate}]{.pre}. (<a href="https://github.com/scrapy/scrapy/issues/5225">issue
5225</a>{.reference
.external})</p>
</li>
<li>
<p>Fixed a <a href="https://docs.python.org/3/library/exceptions.html#TypeError" title="(in Python v3.12)">[<code>TypeError</code>{.xref .py .py-exc .docutils .literal
.notranslate}]{.pre}</a>{.reference
.external} that could be raised during <a href="index.html#topics-feed-exports">[feed export]{.std
.std-ref}</a>{.hoverxref .tooltip
.reference .internal} parameter parsing. (<a href="https://github.com/scrapy/scrapy/issues/5359">issue
5359</a>{.reference
.external})
:::</p>
</li>
</ul>
<p>::: {#id35 .section}</p>
<h5 id="documentationheaderlink-6"><a class="header" href="#documentationheaderlink-6">Documentation<a href="#id35" title="Permalink to this heading">¶</a>{.headerlink}</a></h5>
<ul>
<li>
<p><a href="index.html#using-asyncio">[asyncio support]{.std
.std-ref}</a>{.hoverxref .tooltip .reference
.internal} is no longer considered experimental. (<a href="https://github.com/scrapy/scrapy/issues/5332">issue
5332</a>{.reference
.external})</p>
</li>
<li>
<p>Included <a href="index.html#asyncio-windows">[Windows-specific help for asyncio usage]{.std
.std-ref}</a>{.hoverxref .tooltip
.reference .internal}. (<a href="https://github.com/scrapy/scrapy/issues/4976">issue
4976</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/5315">issue
5315</a>{.reference
.external})</p>
</li>
<li>
<p>Rewrote <a href="index.html#topics-headless-browsing">[Using a headless browser]{.std
.std-ref}</a>{.hoverxref .tooltip
.reference .internal} with up-to-date best practices. (<a href="https://github.com/scrapy/scrapy/issues/4484">issue
4484</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/4613">issue
4613</a>{.reference
.external})</p>
</li>
<li>
<p>Documented <a href="index.html#topics-file-naming">[local file naming in media pipelines]{.std
.std-ref}</a>{.hoverxref .tooltip
.reference .internal}. (<a href="https://github.com/scrapy/scrapy/issues/5069">issue
5069</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/5152">issue
5152</a>{.reference
.external})</p>
</li>
<li>
<p><a href="index.html#faq">[Frequently Asked Questions]{.std
.std-ref}</a>{.hoverxref .tooltip .reference .internal}
now covers spider file name collision issues. (<a href="https://github.com/scrapy/scrapy/issues/2680">issue
2680</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/3669">issue
3669</a>{.reference
.external})</p>
</li>
<li>
<p>Provided better context and instructions to disable the
<a href="index.html#std-setting-URLLENGTH_LIMIT">[<code>URLLENGTH_LIMIT</code>{.xref .std .std-setting .docutils .literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal} setting. (<a href="https://github.com/scrapy/scrapy/issues/5135">issue
5135</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/5250">issue
5250</a>{.reference
.external})</p>
</li>
<li>
<p>Documented that <a href="index.html#reppy-parser">[Reppy parser]{.std
.std-ref}</a>{.hoverxref .tooltip .reference
.internal} does not support Python 3.9+. (<a href="https://github.com/scrapy/scrapy/issues/5226">issue
5226</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/5231">issue
5231</a>{.reference
.external})</p>
</li>
<li>
<p>Documented <a href="index.html#topics-scheduler">[the scheduler component]{.std
.std-ref}</a>{.hoverxref .tooltip
.reference .internal}. (<a href="https://github.com/scrapy/scrapy/issues/3537">issue
3537</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/3559">issue
3559</a>{.reference
.external})</p>
</li>
<li>
<p>Documented the method used by <a href="index.html#topics-media-pipeline">[media pipelines]{.std
.std-ref}</a>{.hoverxref .tooltip
.reference .internal} to <a href="index.html#file-expiration">[determine if a file has expired]{.std
.std-ref}</a>{.hoverxref .tooltip
.reference .internal}. (<a href="https://github.com/scrapy/scrapy/issues/5120">issue
5120</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/5254">issue
5254</a>{.reference
.external})</p>
</li>
<li>
<p><a href="index.html#run-multiple-spiders">[Running multiple spiders in the same process]{.std
.std-ref}</a>{.hoverxref .tooltip
.reference .internal} now features
[<code>scrapy.utils.project.get_project_settings()</code>{.xref .py .py-func
.docutils .literal .notranslate}]{.pre} usage. (<a href="https://github.com/scrapy/scrapy/issues/5070">issue
5070</a>{.reference
.external})</p>
</li>
<li>
<p><a href="index.html#run-multiple-spiders">[Running multiple spiders in the same process]{.std
.std-ref}</a>{.hoverxref .tooltip
.reference .internal} now covers what happens when you define
different per-spider values for some settings that cannot differ at
run time. (<a href="https://github.com/scrapy/scrapy/issues/4485">issue
4485</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/5352">issue
5352</a>{.reference
.external})</p>
</li>
<li>
<p>Extended the documentation of the <a href="index.html#scrapy.extensions.statsmailer.StatsMailer" title="scrapy.extensions.statsmailer.StatsMailer">[<code>StatsMailer</code>{.xref .py
.py-class .docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal} extension. (<a href="https://github.com/scrapy/scrapy/issues/5199">issue
5199</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/5217">issue
5217</a>{.reference
.external})</p>
</li>
<li>
<p>Added <a href="index.html#std-setting-JOBDIR">[<code>JOBDIR</code>{.xref .std .std-setting .docutils .literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal} to <a href="index.html#topics-settings">[Settings]{.std
.std-ref}</a>{.hoverxref .tooltip
.reference .internal}. (<a href="https://github.com/scrapy/scrapy/issues/5173">issue
5173</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/5224">issue
5224</a>{.reference
.external})</p>
</li>
<li>
<p>Documented [<code>Spider.attribute</code>{.xref .py .py-attr .docutils .literal
.notranslate}]{.pre}. (<a href="https://github.com/scrapy/scrapy/issues/5174">issue
5174</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/5244">issue
5244</a>{.reference
.external})</p>
</li>
<li>
<p>Documented <a href="index.html#scrapy.http.TextResponse.urljoin" title="scrapy.http.TextResponse.urljoin">[<code>TextResponse.urljoin</code>{.xref .py .py-attr .docutils
.literal
.notranslate}]{.pre}</a>{.reference
.internal}. (<a href="https://github.com/scrapy/scrapy/issues/1582">issue
1582</a>{.reference
.external})</p>
</li>
<li>
<p>Added the [<code>body_length</code>{.docutils .literal .notranslate}]{.pre}
parameter to the documented signature of the
<a href="index.html#std-signal-headers_received">[<code>headers_received</code>{.xref .std .std-signal .docutils .literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal} signal. (<a href="https://github.com/scrapy/scrapy/issues/5270">issue
5270</a>{.reference
.external})</p>
</li>
<li>
<p>Clarified <a href="index.html#scrapy.selector.SelectorList.get" title="scrapy.selector.SelectorList.get">[<code>SelectorList.get</code>{.xref .py .py-meth .docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal} usage in the <a href="index.html#intro-tutorial">[tutorial]{.std
.std-ref}</a>{.hoverxref .tooltip .reference
.internal}. (<a href="https://github.com/scrapy/scrapy/issues/5256">issue
5256</a>{.reference
.external})</p>
</li>
<li>
<p>The documentation now features the shortest import path of classes
with multiple import paths. (<a href="https://github.com/scrapy/scrapy/issues/2733">issue
2733</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/5099">issue
5099</a>{.reference
.external})</p>
</li>
<li>
<p>[<code>quotes.toscrape.com</code>{.docutils .literal .notranslate}]{.pre}
references now use HTTPS instead of HTTP. (<a href="https://github.com/scrapy/scrapy/issues/5395">issue
5395</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/5396">issue
5396</a>{.reference
.external})</p>
</li>
<li>
<p>Added a link to <a href="https://discord.gg/mv3yErfpvq">our Discord
server</a>{.reference .external} to
<a href="index.html#getting-help">[Getting help]{.std .std-ref}</a>{.hoverxref
.tooltip .reference .internal}. (<a href="https://github.com/scrapy/scrapy/issues/5421">issue
5421</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/5422">issue
5422</a>{.reference
.external})</p>
</li>
<li>
<p>The pronunciation of the project name is now <a href="index.html#intro-overview">[officially]{.std
.std-ref}</a>{.hoverxref .tooltip .reference
.internal} /ˈskreɪpaɪ/. (<a href="https://github.com/scrapy/scrapy/issues/5280">issue
5280</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/5281">issue
5281</a>{.reference
.external})</p>
</li>
<li>
<p>Added the Scrapy logo to the README. (<a href="https://github.com/scrapy/scrapy/issues/5255">issue
5255</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/5258">issue
5258</a>{.reference
.external})</p>
</li>
<li>
<p>Fixed issues and implemented minor improvements. (<a href="https://github.com/scrapy/scrapy/issues/3155">issue
3155</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/4335">issue
4335</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/5074">issue
5074</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/5098">issue
5098</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/5134">issue
5134</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/5180">issue
5180</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/5194">issue
5194</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/5239">issue
5239</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/5266">issue
5266</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/5271">issue
5271</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/5273">issue
5273</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/5274">issue
5274</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/5276">issue
5276</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/5347">issue
5347</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/5356">issue
5356</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/5414">issue
5414</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/5415">issue
5415</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/5416">issue
5416</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/5419">issue
5419</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/5420">issue
5420</a>{.reference
.external})
:::</p>
</li>
</ul>
<p>::: {#id36 .section}</p>
<h5 id="quality-assuranceheaderlink-6"><a class="header" href="#quality-assuranceheaderlink-6">Quality Assurance<a href="#id36" title="Permalink to this heading">¶</a>{.headerlink}</a></h5>
<ul>
<li>
<p>Added support for Python 3.10. (<a href="https://github.com/scrapy/scrapy/issues/5212">issue
5212</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/5221">issue
5221</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/5265">issue
5265</a>{.reference
.external})</p>
</li>
<li>
<p>Significantly reduced memory usage by
[<code>scrapy.utils.response.response_httprepr()</code>{.xref .py .py-func
.docutils .literal .notranslate}]{.pre}, used by the
<a href="index.html#scrapy.downloadermiddlewares.stats.DownloaderStats" title="scrapy.downloadermiddlewares.stats.DownloaderStats">[<code>DownloaderStats</code>{.xref .py .py-class .docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal} downloader middleware, which is enabled by default.
(<a href="https://github.com/scrapy/scrapy/issues/4964">issue
4964</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/4972">issue
4972</a>{.reference
.external})</p>
</li>
<li>
<p>Removed uses of the deprecated <a href="https://docs.python.org/3/library/optparse.html#module-optparse" title="(in Python v3.12)">[<code>optparse</code>{.xref .py .py-mod
.docutils .literal
.notranslate}]{.pre}</a>{.reference
.external} module. (<a href="https://github.com/scrapy/scrapy/issues/5366">issue
5366</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/5374">issue
5374</a>{.reference
.external})</p>
</li>
<li>
<p>Extended typing hints. (<a href="https://github.com/scrapy/scrapy/issues/5077">issue
5077</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/5090">issue
5090</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/5100">issue
5100</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/5108">issue
5108</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/5171">issue
5171</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/5215">issue
5215</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/5334">issue
5334</a>{.reference
.external})</p>
</li>
<li>
<p>Improved tests, fixed CI issues, removed unused code. (<a href="https://github.com/scrapy/scrapy/issues/5094">issue
5094</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/5157">issue
5157</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/5162">issue
5162</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/5198">issue
5198</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/5207">issue
5207</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/5208">issue
5208</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/5229">issue
5229</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/5298">issue
5298</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/5299">issue
5299</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/5310">issue
5310</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/5316">issue
5316</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/5333">issue
5333</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/5388">issue
5388</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/5389">issue
5389</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/5400">issue
5400</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/5401">issue
5401</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/5404">issue
5404</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/5405">issue
5405</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/5407">issue
5407</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/5410">issue
5410</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/5412">issue
5412</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/5425">issue
5425</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/5427">issue
5427</a>{.reference
.external})</p>
</li>
<li>
<p>Implemented improvements for contributors. (<a href="https://github.com/scrapy/scrapy/issues/5080">issue
5080</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/5082">issue
5082</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/5177">issue
5177</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/5200">issue
5200</a>{.reference
.external})</p>
</li>
<li>
<p>Implemented cleanups. (<a href="https://github.com/scrapy/scrapy/issues/5095">issue
5095</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/5106">issue
5106</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/5209">issue
5209</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/5228">issue
5228</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/5235">issue
5235</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/5245">issue
5245</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/5246">issue
5246</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/5292">issue
5292</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/5314">issue
5314</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/5322">issue
5322</a>{.reference
.external})
:::
:::</p>
</li>
</ul>
<p>::: {#scrapy-2-5-1-2021-10-05 .section}
[]{#release-2-5-1}</p>
<h4 id="scrapy-251-2021-10-05headerlink"><a class="header" href="#scrapy-251-2021-10-05headerlink">Scrapy 2.5.1 (2021-10-05)<a href="#scrapy-2-5-1-2021-10-05" title="Permalink to this heading">¶</a>{.headerlink}</a></h4>
<ul>
<li>
<p><strong>Security bug fix:</strong></p>
<p>If you use <a href="index.html#scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware" title="scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware">[<code>HttpAuthMiddleware</code>{.xref .py .py-class .docutils
.literal
.notranslate}]{.pre}</a>{.reference
.internal} (i.e. the [<code>http_user</code>{.docutils .literal
.notranslate}]{.pre} and [<code>http_pass</code>{.docutils .literal
.notranslate}]{.pre} spider attributes) for HTTP authentication, any
request exposes your credentials to the request target.</p>
<p>To prevent unintended exposure of authentication credentials to
unintended domains, you must now additionally set a new, additional
spider attribute, [<code>http_auth_domain</code>{.docutils .literal
.notranslate}]{.pre}, and point it to the specific domain to which
the authentication credentials must be sent.</p>
<p>If the [<code>http_auth_domain</code>{.docutils .literal .notranslate}]{.pre}
spider attribute is not set, the domain of the first request will be
considered the HTTP authentication target, and authentication
credentials will only be sent in requests targeting that domain.</p>
<p>If you need to send the same HTTP authentication credentials to
multiple domains, you can use
<a href="https://w3lib.readthedocs.io/en/latest/w3lib.html#w3lib.http.basic_auth_header" title="(in w3lib v2.1)">[<code>w3lib.http.basic_auth_header()</code>{.xref .py .py-func .docutils
.literal
.notranslate}]{.pre}</a>{.reference
.external} instead to set the value of the
[<code>Authorization</code>{.docutils .literal .notranslate}]{.pre} header of
your requests.</p>
<p>If you <em>really</em> want your spider to send the same HTTP
authentication credentials to any domain, set the
[<code>http_auth_domain</code>{.docutils .literal .notranslate}]{.pre} spider
attribute to [<code>None</code>{.docutils .literal .notranslate}]{.pre}.</p>
<p>Finally, if you are a user of
<a href="https://github.com/scrapy-plugins/scrapy-splash">scrapy-splash</a>{.reference
.external}, know that this version of Scrapy breaks compatibility
with scrapy-splash 0.7.2 and earlier. You will need to upgrade
scrapy-splash to a greater version for it to continue to work.
:::</p>
</li>
</ul>
<p>::: {#scrapy-2-5-0-2021-04-06 .section}
[]{#release-2-5-0}</p>
<h4 id="scrapy-250-2021-04-06headerlink"><a class="header" href="#scrapy-250-2021-04-06headerlink">Scrapy 2.5.0 (2021-04-06)<a href="#scrapy-2-5-0-2021-04-06" title="Permalink to this heading">¶</a>{.headerlink}</a></h4>
<p>Highlights:</p>
<ul>
<li>
<p>Official Python 3.9 support</p>
</li>
<li>
<p>Experimental <a href="index.html#http2">[HTTP/2 support]{.std
.std-ref}</a>{.hoverxref .tooltip .reference
.internal}</p>
</li>
<li>
<p>New <a href="index.html#scrapy.downloadermiddlewares.retry.get_retry_request" title="scrapy.downloadermiddlewares.retry.get_retry_request">[<code>get_retry_request()</code>{.xref .py .py-func .docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal} function to retry requests from spider callbacks</p>
</li>
<li>
<p>New <a href="index.html#scrapy.signals.headers_received" title="scrapy.signals.headers_received">[<code>headers_received</code>{.xref .py .py-class .docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal} signal that allows stopping downloads early</p>
</li>
<li>
<p>New <a href="index.html#scrapy.http.Response.protocol" title="scrapy.http.Response.protocol">[<code>Response.protocol</code>{.xref .py .py-class .docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal} attribute</p>
</li>
</ul>
<p>::: {#id37 .section}</p>
<h5 id="deprecation-removalsheaderlink-4"><a class="header" href="#deprecation-removalsheaderlink-4">Deprecation removals<a href="#id37" title="Permalink to this heading">¶</a>{.headerlink}</a></h5>
<ul>
<li>
<p>Removed all code that <a href="#id96">[was deprecated in 1.7.0]{.std
.std-ref}</a>{.hoverxref .tooltip .reference .internal} and had
not <a href="#id45">[already been removed in 2.4.0]{.std
.std-ref}</a>{.hoverxref .tooltip .reference .internal}. (<a href="https://github.com/scrapy/scrapy/issues/4901">issue
4901</a>{.reference
.external})</p>
</li>
<li>
<p>Removed support for the
[<code>SCRAPY_PICKLED_SETTINGS_TO_OVERRIDE</code>{.docutils .literal
.notranslate}]{.pre} environment variable, <a href="#id88">[deprecated in
1.8.0]{.std .std-ref}</a>{.hoverxref .tooltip .reference
.internal}. (<a href="https://github.com/scrapy/scrapy/issues/4912">issue
4912</a>{.reference
.external})
:::</p>
</li>
</ul>
<p>::: {#id38 .section}</p>
<h5 id="deprecationsheaderlink-6"><a class="header" href="#deprecationsheaderlink-6">Deprecations<a href="#id38" title="Permalink to this heading">¶</a>{.headerlink}</a></h5>
<ul>
<li>The [<code>scrapy.utils.py36</code>{.xref .py .py-mod .docutils .literal
.notranslate}]{.pre} module is now deprecated in favor of
[<code>scrapy.utils.asyncgen</code>{.xref .py .py-mod .docutils .literal
.notranslate}]{.pre}. (<a href="https://github.com/scrapy/scrapy/issues/4900">issue
4900</a>{.reference
.external})
:::</li>
</ul>
<p>::: {#id39 .section}</p>
<h5 id="new-featuresheaderlink-7"><a class="header" href="#new-featuresheaderlink-7">New features<a href="#id39" title="Permalink to this heading">¶</a>{.headerlink}</a></h5>
<ul>
<li>
<p>Experimental <a href="index.html#http2">[HTTP/2 support]{.std
.std-ref}</a>{.hoverxref .tooltip .reference
.internal} through a new download handler that can be assigned to
the [<code>https</code>{.docutils .literal .notranslate}]{.pre} protocol in the
<a href="index.html#std-setting-DOWNLOAD_HANDLERS">[<code>DOWNLOAD_HANDLERS</code>{.xref .std .std-setting .docutils .literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal} setting. (<a href="https://github.com/scrapy/scrapy/issues/1854">issue
1854</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/4769">issue
4769</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/5058">issue
5058</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/5059">issue
5059</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/5066">issue
5066</a>{.reference
.external})</p>
</li>
<li>
<p>The new
<a href="index.html#scrapy.downloadermiddlewares.retry.get_retry_request" title="scrapy.downloadermiddlewares.retry.get_retry_request">[<code>scrapy.downloadermiddlewares.retry.get_retry_request()</code>{.xref .py
.py-func .docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal} function may be used from spider callbacks or middlewares
to handle the retrying of a request beyond the scenarios that
<a href="index.html#scrapy.downloadermiddlewares.retry.RetryMiddleware" title="scrapy.downloadermiddlewares.retry.RetryMiddleware">[<code>RetryMiddleware</code>{.xref .py .py-class .docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal} supports. (<a href="https://github.com/scrapy/scrapy/issues/3590">issue
3590</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/3685">issue
3685</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/4902">issue
4902</a>{.reference
.external})</p>
</li>
<li>
<p>The new <a href="index.html#scrapy.signals.headers_received" title="scrapy.signals.headers_received">[<code>headers_received</code>{.xref .py .py-class .docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal} signal gives early access to response headers and allows
<a href="index.html#topics-stop-response-download">[stopping downloads]{.std
.std-ref}</a>{.hoverxref
.tooltip .reference .internal}. (<a href="https://github.com/scrapy/scrapy/issues/1772">issue
1772</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/4897">issue
4897</a>{.reference
.external})</p>
</li>
<li>
<p>The new <a href="index.html#scrapy.http.Response.protocol" title="scrapy.http.Response.protocol">[<code>Response.protocol</code>{.xref .py .py-attr .docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal} attribute gives access to the string that identifies the
protocol used to download a response. (<a href="https://github.com/scrapy/scrapy/issues/4878">issue
4878</a>{.reference
.external})</p>
</li>
<li>
<p><a href="index.html#topics-stats">[Stats]{.std .std-ref}</a>{.hoverxref
.tooltip .reference .internal} now include the following entries
that indicate the number of successes and failures in storing
<a href="index.html#topics-feed-exports">[feeds]{.std .std-ref}</a>{.hoverxref
.tooltip .reference .internal}:</p>
<p>::: {.highlight-default .notranslate}
::: highlight
feedexport/success_count/<storage type>
feedexport/failed_count/<storage type>
:::
:::</p>
<p>Where [<code>&lt;storage</code>{.docutils .literal
.notranslate}]{.pre}<code> </code>{.docutils .literal
.notranslate}[<code>type&gt;</code>{.docutils .literal .notranslate}]{.pre} is the
feed storage backend class name, such as [<code>FileFeedStorage</code>{.xref
.py .py-class .docutils .literal .notranslate}]{.pre} or
[<code>FTPFeedStorage</code>{.xref .py .py-class .docutils .literal
.notranslate}]{.pre}.</p>
<p>(<a href="https://github.com/scrapy/scrapy/issues/3947">issue
3947</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/4850">issue
4850</a>{.reference
.external})</p>
</li>
<li>
<p>The <a href="index.html#scrapy.spidermiddlewares.urllength.UrlLengthMiddleware" title="scrapy.spidermiddlewares.urllength.UrlLengthMiddleware">[<code>UrlLengthMiddleware</code>{.xref .py .py-class .docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal} spider middleware now logs ignored URLs with
[<code>INFO</code>{.docutils .literal .notranslate}]{.pre} <a href="https://docs.python.org/3/library/logging.html#levels" title="(in Python v3.12)">[logging
level]{.xref .std
.std-ref}</a>{.reference
.external} instead of [<code>DEBUG</code>{.docutils .literal
.notranslate}]{.pre}, and it now includes the following entry into
<a href="index.html#topics-stats">[stats]{.std .std-ref}</a>{.hoverxref
.tooltip .reference .internal} to keep track of the number of
ignored URLs:</p>
<p>::: {.highlight-default .notranslate}
::: highlight
urllength/request_ignored_count
:::
:::</p>
<p>(<a href="https://github.com/scrapy/scrapy/issues/5036">issue
5036</a>{.reference
.external})</p>
</li>
<li>
<p>The <a href="index.html#scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware" title="scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware">[<code>HttpCompressionMiddleware</code>{.xref .py .py-class .docutils
.literal
.notranslate}]{.pre}</a>{.reference
.internal} downloader middleware now logs the number of decompressed
responses and the total count of resulting bytes:</p>
<p>::: {.highlight-default .notranslate}
::: highlight
httpcompression/response_bytes
httpcompression/response_count
:::
:::</p>
<p>(<a href="https://github.com/scrapy/scrapy/issues/4797">issue
4797</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/4799">issue
4799</a>{.reference
.external})
:::</p>
</li>
</ul>
<p>::: {#id40 .section}</p>
<h5 id="bug-fixesheaderlink-7"><a class="header" href="#bug-fixesheaderlink-7">Bug fixes<a href="#id40" title="Permalink to this heading">¶</a>{.headerlink}</a></h5>
<ul>
<li>
<p>Fixed installation on PyPy installing PyDispatcher in addition to
PyPyDispatcher, which could prevent Scrapy from working depending on
which package got imported. (<a href="https://github.com/scrapy/scrapy/issues/4710">issue
4710</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/4814">issue
4814</a>{.reference
.external})</p>
</li>
<li>
<p>When inspecting a callback to check if it is a generator that also
returns a value, an exception is no longer raised if the callback
has a docstring with lower indentation than the following code.
(<a href="https://github.com/scrapy/scrapy/issues/4477">issue
4477</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/4935">issue
4935</a>{.reference
.external})</p>
</li>
<li>
<p>The
<a href="https://tools.ietf.org/html/rfc2616#section-14.13">Content-Length</a>{.reference
.external} header is no longer omitted from responses when using the
default, HTTP/1.1 download handler (see <a href="index.html#std-setting-DOWNLOAD_HANDLERS">[<code>DOWNLOAD_HANDLERS</code>{.xref
.std .std-setting .docutils .literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal}). (<a href="https://github.com/scrapy/scrapy/issues/5009">issue
5009</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/5034">issue
5034</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/5045">issue
5045</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/5057">issue
5057</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/5062">issue
5062</a>{.reference
.external})</p>
</li>
<li>
<p>Setting the <a href="index.html#std-reqmeta-handle_httpstatus_all">[<code>handle_httpstatus_all</code>{.xref .std .std-reqmeta
.docutils .literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal} request meta key to
[<code>False</code>{.docutils .literal .notranslate}]{.pre} now has the same
effect as not setting it at all, instead of having the same effect
as setting it to [<code>True</code>{.docutils .literal .notranslate}]{.pre}.
(<a href="https://github.com/scrapy/scrapy/issues/3851">issue
3851</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/4694">issue
4694</a>{.reference
.external})
:::</p>
</li>
</ul>
<p>::: {#id41 .section}</p>
<h5 id="documentationheaderlink-7"><a class="header" href="#documentationheaderlink-7">Documentation<a href="#id41" title="Permalink to this heading">¶</a>{.headerlink}</a></h5>
<ul>
<li>
<p>Added instructions to <a href="index.html#intro-install-windows">[install Scrapy in Windows using pip]{.std
.std-ref}</a>{.hoverxref .tooltip
.reference .internal}. (<a href="https://github.com/scrapy/scrapy/issues/4715">issue
4715</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/4736">issue
4736</a>{.reference
.external})</p>
</li>
<li>
<p>Logging documentation now includes <a href="index.html#topics-logging-advanced-customization">[additional ways to filter
logs]{.std
.std-ref}</a>{.hoverxref
.tooltip .reference .internal}. (<a href="https://github.com/scrapy/scrapy/issues/4216">issue
4216</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/4257">issue
4257</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/4965">issue
4965</a>{.reference
.external})</p>
</li>
<li>
<p>Covered how to deal with long lists of allowed domains in the
<a href="index.html#faq">[FAQ]{.std .std-ref}</a>{.hoverxref .tooltip
.reference .internal}. (<a href="https://github.com/scrapy/scrapy/issues/2263">issue
2263</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/3667">issue
3667</a>{.reference
.external})</p>
</li>
<li>
<p>Covered
<a href="https://github.com/scrapy/scrapy-bench">scrapy-bench</a>{.reference
.external} in <a href="index.html#benchmarking">[Benchmarking]{.std
.std-ref}</a>{.hoverxref .tooltip .reference
.internal}. (<a href="https://github.com/scrapy/scrapy/issues/4996">issue
4996</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/5016">issue
5016</a>{.reference
.external})</p>
</li>
<li>
<p>Clarified that one <a href="index.html#topics-extensions">[extension]{.std
.std-ref}</a>{.hoverxref .tooltip
.reference .internal} instance is created per crawler. (<a href="https://github.com/scrapy/scrapy/issues/5014">issue
5014</a>{.reference
.external})</p>
</li>
<li>
<p>Fixed some errors in examples. (<a href="https://github.com/scrapy/scrapy/issues/4829">issue
4829</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/4830">issue
4830</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/4907">issue
4907</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/4909">issue
4909</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/5008">issue
5008</a>{.reference
.external})</p>
</li>
<li>
<p>Fixed some external links, typos, and so on. (<a href="https://github.com/scrapy/scrapy/issues/4892">issue
4892</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/4899">issue
4899</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/4936">issue
4936</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/4942">issue
4942</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/5005">issue
5005</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/5063">issue
5063</a>{.reference
.external})</p>
</li>
<li>
<p>The <a href="index.html#topics-request-meta">[list of Request.meta keys]{.std
.std-ref}</a>{.hoverxref .tooltip
.reference .internal} is now sorted alphabetically. (<a href="https://github.com/scrapy/scrapy/issues/5061">issue
5061</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/5065">issue
5065</a>{.reference
.external})</p>
</li>
<li>
<p>Updated references to Scrapinghub, which is now called Zyte. (<a href="https://github.com/scrapy/scrapy/issues/4973">issue
4973</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/5072">issue
5072</a>{.reference
.external})</p>
</li>
<li>
<p>Added a mention to contributors in the README. (<a href="https://github.com/scrapy/scrapy/issues/4956">issue
4956</a>{.reference
.external})</p>
</li>
<li>
<p>Reduced the top margin of lists. (<a href="https://github.com/scrapy/scrapy/issues/4974">issue
4974</a>{.reference
.external})
:::</p>
</li>
</ul>
<p>::: {#id42 .section}</p>
<h5 id="quality-assuranceheaderlink-7"><a class="header" href="#quality-assuranceheaderlink-7">Quality Assurance<a href="#id42" title="Permalink to this heading">¶</a>{.headerlink}</a></h5>
<ul>
<li>
<p>Made Python 3.9 support official (<a href="https://github.com/scrapy/scrapy/issues/4757">issue
4757</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/4759">issue
4759</a>{.reference
.external})</p>
</li>
<li>
<p>Extended typing hints (<a href="https://github.com/scrapy/scrapy/issues/4895">issue
4895</a>{.reference
.external})</p>
</li>
<li>
<p>Fixed deprecated uses of the Twisted API. (<a href="https://github.com/scrapy/scrapy/issues/4940">issue
4940</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/4950">issue
4950</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/5073">issue
5073</a>{.reference
.external})</p>
</li>
<li>
<p>Made our tests run with the new pip resolver. (<a href="https://github.com/scrapy/scrapy/issues/4710">issue
4710</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/4814">issue
4814</a>{.reference
.external})</p>
</li>
<li>
<p>Added tests to ensure that <a href="index.html#coroutine-support">[coroutine support]{.std
.std-ref}</a>{.hoverxref .tooltip
.reference .internal} is tested. (<a href="https://github.com/scrapy/scrapy/issues/4987">issue
4987</a>{.reference
.external})</p>
</li>
<li>
<p>Migrated from Travis CI to GitHub Actions. (<a href="https://github.com/scrapy/scrapy/issues/4924">issue
4924</a>{.reference
.external})</p>
</li>
<li>
<p>Fixed CI issues. (<a href="https://github.com/scrapy/scrapy/issues/4986">issue
4986</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/5020">issue
5020</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/5022">issue
5022</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/5027">issue
5027</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/5052">issue
5052</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/5053">issue
5053</a>{.reference
.external})</p>
</li>
<li>
<p>Implemented code refactorings, style fixes and cleanups. (<a href="https://github.com/scrapy/scrapy/issues/4911">issue
4911</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/4982">issue
4982</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/5001">issue
5001</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/5002">issue
5002</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/5076">issue
5076</a>{.reference
.external})
:::
:::</p>
</li>
</ul>
<p>::: {#scrapy-2-4-1-2020-11-17 .section}
[]{#release-2-4-1}</p>
<h4 id="scrapy-241-2020-11-17headerlink"><a class="header" href="#scrapy-241-2020-11-17headerlink">Scrapy 2.4.1 (2020-11-17)<a href="#scrapy-2-4-1-2020-11-17" title="Permalink to this heading">¶</a>{.headerlink}</a></h4>
<ul>
<li>
<p>Fixed <a href="index.html#topics-feed-exports">[feed exports]{.std
.std-ref}</a>{.hoverxref .tooltip
.reference .internal} overwrite support (<a href="https://github.com/scrapy/scrapy/issues/4845">issue
4845</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/4857">issue
4857</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/4859">issue
4859</a>{.reference
.external})</p>
</li>
<li>
<p>Fixed the AsyncIO event loop handling, which could make code hang
(<a href="https://github.com/scrapy/scrapy/issues/4855">issue
4855</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/4872">issue
4872</a>{.reference
.external})</p>
</li>
<li>
<p>Fixed the IPv6-capable DNS resolver [<code>CachingHostnameResolver</code>{.xref
.py .py-class .docutils .literal .notranslate}]{.pre} for download
handlers that call <a href="https://docs.twisted.org/en/stable/api/twisted.internet.interfaces.IReactorCore.html#resolve" title="(in Twisted)">[<code>reactor.resolve</code>{.xref .py .py-meth .docutils
.literal
.notranslate}]{.pre}</a>{.reference
.external} (<a href="https://github.com/scrapy/scrapy/issues/4802">issue
4802</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/4803">issue
4803</a>{.reference
.external})</p>
</li>
<li>
<p>Fixed the output of the <a href="index.html#std-command-genspider">[<code>genspider</code>{.xref .std .std-command
.docutils .literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal} command showing placeholders instead
of the import path of the generated spider module (<a href="https://github.com/scrapy/scrapy/issues/4874">issue
4874</a>{.reference
.external})</p>
</li>
<li>
<p>Migrated Windows CI from Azure Pipelines to GitHub Actions (<a href="https://github.com/scrapy/scrapy/issues/4869">issue
4869</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/4876">issue
4876</a>{.reference
.external})
:::</p>
</li>
</ul>
<p>::: {#scrapy-2-4-0-2020-10-11 .section}
[]{#release-2-4-0}</p>
<h4 id="scrapy-240-2020-10-11headerlink"><a class="header" href="#scrapy-240-2020-10-11headerlink">Scrapy 2.4.0 (2020-10-11)<a href="#scrapy-2-4-0-2020-10-11" title="Permalink to this heading">¶</a>{.headerlink}</a></h4>
<p>Highlights:</p>
<ul>
<li>
<p>Python 3.5 support has been dropped.</p>
</li>
<li>
<p>The [<code>file_path</code>{.docutils .literal .notranslate}]{.pre} method of
<a href="index.html#topics-media-pipeline">[media pipelines]{.std
.std-ref}</a>{.hoverxref .tooltip
.reference .internal} can now access the source <a href="index.html#topics-items">[item]{.std
.std-ref}</a>{.hoverxref .tooltip .reference
.internal}.</p>
<p>This allows you to set a download file path based on item data.</p>
</li>
<li>
<p>The new [<code>item_export_kwargs</code>{.docutils .literal
.notranslate}]{.pre} key of the <a href="index.html#std-setting-FEEDS">[<code>FEEDS</code>{.xref .std .std-setting
.docutils .literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal} setting allows to define keyword
parameters to pass to <a href="index.html#topics-exporters">[item exporter classes]{.std
.std-ref}</a>{.hoverxref .tooltip
.reference .internal}</p>
</li>
<li>
<p>You can now choose whether <a href="index.html#topics-feed-exports">[feed exports]{.std
.std-ref}</a>{.hoverxref .tooltip
.reference .internal} overwrite or append to the output file.</p>
<p>For example, when using the <a href="index.html#std-command-crawl">[<code>crawl</code>{.xref .std .std-command
.docutils .literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal} or <a href="index.html#std-command-runspider">[<code>runspider</code>{.xref .std
.std-command .docutils .literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal} commands, you can use the
[<code>-O</code>{.docutils .literal .notranslate}]{.pre} option instead of
[<code>-o</code>{.docutils .literal .notranslate}]{.pre} to overwrite the
output file.</p>
</li>
<li>
<p>Zstd-compressed responses are now supported if
<a href="https://pypi.org/project/zstandard/">zstandard</a>{.reference
.external} is installed.</p>
</li>
<li>
<p>In settings, where the import path of a class is required, it is now
possible to pass a class object instead.</p>
</li>
</ul>
<p>::: {#id43 .section}</p>
<h5 id="modified-requirementsheaderlink-3"><a class="header" href="#modified-requirementsheaderlink-3">Modified requirements<a href="#id43" title="Permalink to this heading">¶</a>{.headerlink}</a></h5>
<ul>
<li>
<p>Python 3.6 or greater is now required; support for Python 3.5 has
been dropped</p>
<p>As a result:</p>
<ul>
<li>
<p>When using PyPy, PyPy 7.2.0 or greater <a href="index.html#faq-python-versions">[is now required]{.std
.std-ref}</a>{.hoverxref .tooltip
.reference .internal}</p>
</li>
<li>
<p>For Amazon S3 storage support in <a href="index.html#topics-feed-storage-s3">[feed exports]{.std
.std-ref}</a>{.hoverxref
.tooltip .reference .internal} or <a href="index.html#media-pipelines-s3">[media pipelines]{.std
.std-ref}</a>{.hoverxref .tooltip
.reference .internal},
<a href="https://github.com/boto/botocore">botocore</a>{.reference
.external} 1.4.87 or greater is now required</p>
</li>
<li>
<p>To use the <a href="index.html#images-pipeline">[images pipeline]{.std
.std-ref}</a>{.hoverxref .tooltip
.reference .internal},
<a href="https://python-pillow.org/">Pillow</a>{.reference .external} 4.0.0
or greater is now required</p>
</li>
</ul>
<p>(<a href="https://github.com/scrapy/scrapy/issues/4718">issue
4718</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/4732">issue
4732</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/4733">issue
4733</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/4742">issue
4742</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/4743">issue
4743</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/4764">issue
4764</a>{.reference
.external})
:::</p>
</li>
</ul>
<p>::: {#id44 .section}</p>
<h5 id="backward-incompatible-changesheaderlink-3"><a class="header" href="#backward-incompatible-changesheaderlink-3">Backward-incompatible changes<a href="#id44" title="Permalink to this heading">¶</a>{.headerlink}</a></h5>
<ul>
<li>
<p><a href="index.html#scrapy.downloadermiddlewares.cookies.CookiesMiddleware" title="scrapy.downloadermiddlewares.cookies.CookiesMiddleware">[<code>CookiesMiddleware</code>{.xref .py .py-class .docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal} once again discards cookies defined in
<a href="index.html#scrapy.http.Request.headers" title="scrapy.http.Request.headers">[<code>Request.headers</code>{.xref .py .py-attr .docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal}.</p>
<p>We decided to revert this bug fix, introduced in Scrapy 2.2.0,
because it was reported that the current implementation could break
existing code.</p>
<p>If you need to set cookies for a request, use the
<a href="index.html#scrapy.http.Request" title="scrapy.http.Request">[<code>Request.cookies</code>{.xref .py .py-class .docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal} parameter.</p>
<p>A future version of Scrapy will include a new, better implementation
of the reverted bug fix.</p>
<p>(<a href="https://github.com/scrapy/scrapy/issues/4717">issue
4717</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/4823">issue
4823</a>{.reference
.external})
:::</p>
</li>
</ul>
<p>::: {#id45 .section}
[]{#id46}</p>
<h5 id="deprecation-removalsheaderlink-5"><a class="header" href="#deprecation-removalsheaderlink-5">Deprecation removals<a href="#id45" title="Permalink to this heading">¶</a>{.headerlink}</a></h5>
<ul>
<li>
<p>[<code>scrapy.extensions.feedexport.S3FeedStorage</code>{.xref .py .py-class
.docutils .literal .notranslate}]{.pre} no longer reads the values
of [<code>access_key</code>{.docutils .literal .notranslate}]{.pre} and
[<code>secret_key</code>{.docutils .literal .notranslate}]{.pre} from the
running project settings when they are not passed to its
[<code>__init__</code>{.docutils .literal .notranslate}]{.pre} method; you must
either pass those parameters to its [<code>__init__</code>{.docutils .literal
.notranslate}]{.pre} method or use
[<code>S3FeedStorage.from_crawler</code>{.xref .py .py-class .docutils .literal
.notranslate}]{.pre} (<a href="https://github.com/scrapy/scrapy/issues/4356">issue
4356</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/4411">issue
4411</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/4688">issue
4688</a>{.reference
.external})</p>
</li>
<li>
<p>[<code>Rule.process_request</code>{.xref .py .py-attr .docutils .literal
.notranslate}]{.pre} no longer admits callables which expect a
single [<code>request</code>{.docutils .literal .notranslate}]{.pre} parameter,
rather than both [<code>request</code>{.docutils .literal .notranslate}]{.pre}
and [<code>response</code>{.docutils .literal .notranslate}]{.pre} (<a href="https://github.com/scrapy/scrapy/issues/4818">issue
4818</a>{.reference
.external})
:::</p>
</li>
</ul>
<p>::: {#id47 .section}</p>
<h5 id="deprecationsheaderlink-7"><a class="header" href="#deprecationsheaderlink-7">Deprecations<a href="#id47" title="Permalink to this heading">¶</a>{.headerlink}</a></h5>
<ul>
<li>
<p>In custom <a href="index.html#topics-media-pipeline">[media pipelines]{.std
.std-ref}</a>{.hoverxref .tooltip
.reference .internal}, signatures that do not accept a keyword-only
[<code>item</code>{.docutils .literal .notranslate}]{.pre} parameter in any of
the methods that <a href="#media-pipeline-item-parameter">[now support this parameter]{.std
.std-ref}</a>{.hoverxref .tooltip
.reference .internal} are now deprecated (<a href="https://github.com/scrapy/scrapy/issues/4628">issue
4628</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/4686">issue
4686</a>{.reference
.external})</p>
</li>
<li>
<p>In custom <a href="index.html#topics-feed-storage">[feed storage backend classes]{.std
.std-ref}</a>{.hoverxref .tooltip
.reference .internal}, [<code>__init__</code>{.docutils .literal
.notranslate}]{.pre} method signatures that do not accept a
keyword-only [<code>feed_options</code>{.docutils .literal .notranslate}]{.pre}
parameter are now deprecated (<a href="https://github.com/scrapy/scrapy/issues/547">issue
547</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/716">issue
716</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/4512">issue
4512</a>{.reference
.external})</p>
</li>
<li>
<p>The [<code>scrapy.utils.python.WeakKeyCache</code>{.xref .py .py-class
.docutils .literal .notranslate}]{.pre} class is now deprecated
(<a href="https://github.com/scrapy/scrapy/issues/4684">issue
4684</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/4701">issue
4701</a>{.reference
.external})</p>
</li>
<li>
<p>The [<code>scrapy.utils.boto.is_botocore()</code>{.xref .py .py-func .docutils
.literal .notranslate}]{.pre} function is now deprecated, use
[<code>scrapy.utils.boto.is_botocore_available()</code>{.xref .py .py-func
.docutils .literal .notranslate}]{.pre} instead (<a href="https://github.com/scrapy/scrapy/issues/4734">issue
4734</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/4776">issue
4776</a>{.reference
.external})
:::</p>
</li>
</ul>
<p>::: {#id48 .section}</p>
<h5 id="new-featuresheaderlink-8"><a class="header" href="#new-featuresheaderlink-8">New features<a href="#id48" title="Permalink to this heading">¶</a>{.headerlink}</a></h5>
<ul>
<li>
<p>The following methods of <a href="index.html#topics-media-pipeline">[media pipelines]{.std
.std-ref}</a>{.hoverxref .tooltip
.reference .internal} now accept an [<code>item</code>{.docutils .literal
.notranslate}]{.pre} keyword-only parameter containing the source
<a href="index.html#topics-items">[item]{.std .std-ref}</a>{.hoverxref .tooltip
.reference .internal}:</p>
<ul>
<li>
<p>In <a href="index.html#scrapy.pipelines.files.FilesPipeline" title="scrapy.pipelines.files.FilesPipeline">[<code>scrapy.pipelines.files.FilesPipeline</code>{.xref .py .py-class
.docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal}:</p>
<ul>
<li>
<p>[<code>file_downloaded()</code>{.xref .py .py-meth .docutils .literal
.notranslate}]{.pre}</p>
</li>
<li>
<p><a href="index.html#scrapy.pipelines.files.FilesPipeline.file_path" title="scrapy.pipelines.files.FilesPipeline.file_path">[<code>file_path()</code>{.xref .py .py-meth .docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal}</p>
</li>
<li>
<p>[<code>media_downloaded()</code>{.xref .py .py-meth .docutils .literal
.notranslate}]{.pre}</p>
</li>
<li>
<p>[<code>media_to_download()</code>{.xref .py .py-meth .docutils .literal
.notranslate}]{.pre}</p>
</li>
</ul>
</li>
<li>
<p>In <a href="index.html#scrapy.pipelines.images.ImagesPipeline" title="scrapy.pipelines.images.ImagesPipeline">[<code>scrapy.pipelines.images.ImagesPipeline</code>{.xref .py
.py-class .docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal}:</p>
<ul>
<li>
<p>[<code>file_downloaded()</code>{.xref .py .py-meth .docutils .literal
.notranslate}]{.pre}</p>
</li>
<li>
<p><a href="index.html#scrapy.pipelines.images.ImagesPipeline.file_path" title="scrapy.pipelines.images.ImagesPipeline.file_path">[<code>file_path()</code>{.xref .py .py-meth .docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal}</p>
</li>
<li>
<p>[<code>get_images()</code>{.xref .py .py-meth .docutils .literal
.notranslate}]{.pre}</p>
</li>
<li>
<p>[<code>image_downloaded()</code>{.xref .py .py-meth .docutils .literal
.notranslate}]{.pre}</p>
</li>
<li>
<p>[<code>media_downloaded()</code>{.xref .py .py-meth .docutils .literal
.notranslate}]{.pre}</p>
</li>
<li>
<p>[<code>media_to_download()</code>{.xref .py .py-meth .docutils .literal
.notranslate}]{.pre}</p>
</li>
</ul>
</li>
</ul>
<p>(<a href="https://github.com/scrapy/scrapy/issues/4628">issue
4628</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/4686">issue
4686</a>{.reference
.external})</p>
</li>
<li>
<p>The new [<code>item_export_kwargs</code>{.docutils .literal
.notranslate}]{.pre} key of the <a href="index.html#std-setting-FEEDS">[<code>FEEDS</code>{.xref .std .std-setting
.docutils .literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal} setting allows to define keyword
parameters to pass to <a href="index.html#topics-exporters">[item exporter classes]{.std
.std-ref}</a>{.hoverxref .tooltip
.reference .internal} (<a href="https://github.com/scrapy/scrapy/issues/4606">issue
4606</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/4768">issue
4768</a>{.reference
.external})</p>
</li>
<li>
<p><a href="index.html#topics-feed-exports">[Feed exports]{.std
.std-ref}</a>{.hoverxref .tooltip
.reference .internal} gained overwrite support:</p>
<ul>
<li>
<p>When using the <a href="index.html#std-command-crawl">[<code>crawl</code>{.xref .std .std-command .docutils
.literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal} or <a href="index.html#std-command-runspider">[<code>runspider</code>{.xref .std
.std-command .docutils .literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal} commands, you can use the
[<code>-O</code>{.docutils .literal .notranslate}]{.pre} option instead of
[<code>-o</code>{.docutils .literal .notranslate}]{.pre} to overwrite the
output file</p>
</li>
<li>
<p>You can use the [<code>overwrite</code>{.docutils .literal
.notranslate}]{.pre} key in the <a href="index.html#std-setting-FEEDS">[<code>FEEDS</code>{.xref .std
.std-setting .docutils .literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal} setting to configure whether to
overwrite the output file ([<code>True</code>{.docutils .literal
.notranslate}]{.pre}) or append to its content
([<code>False</code>{.docutils .literal .notranslate}]{.pre})</p>
</li>
<li>
<p>The [<code>__init__</code>{.docutils .literal .notranslate}]{.pre} and
[<code>from_crawler</code>{.docutils .literal .notranslate}]{.pre} methods
of <a href="index.html#topics-feed-storage">[feed storage backend classes]{.std
.std-ref}</a>{.hoverxref .tooltip
.reference .internal} now receive a new keyword-only parameter,
[<code>feed_options</code>{.docutils .literal .notranslate}]{.pre}, which
is a dictionary of <a href="index.html#feed-options">[feed options]{.std
.std-ref}</a>{.hoverxref .tooltip
.reference .internal}</p>
</li>
</ul>
<p>(<a href="https://github.com/scrapy/scrapy/issues/547">issue 547</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/716">issue
716</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/4512">issue
4512</a>{.reference
.external})</p>
</li>
<li>
<p>Zstd-compressed responses are now supported if
<a href="https://pypi.org/project/zstandard/">zstandard</a>{.reference
.external} is installed (<a href="https://github.com/scrapy/scrapy/issues/4831">issue
4831</a>{.reference
.external})</p>
</li>
<li>
<p>In settings, where the import path of a class is required, it is now
possible to pass a class object instead (<a href="https://github.com/scrapy/scrapy/issues/3870">issue
3870</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/3873">issue
3873</a>{.reference
.external}).</p>
<p>This includes also settings where only part of its value is made of
an import path, such as <a href="index.html#std-setting-DOWNLOADER_MIDDLEWARES">[<code>DOWNLOADER_MIDDLEWARES</code>{.xref .std
.std-setting .docutils .literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal} or <a href="index.html#std-setting-DOWNLOAD_HANDLERS">[<code>DOWNLOAD_HANDLERS</code>{.xref .std
.std-setting .docutils .literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal}.</p>
</li>
<li>
<p><a href="index.html#topics-downloader-middleware">[Downloader middlewares]{.std
.std-ref}</a>{.hoverxref
.tooltip .reference .internal} can now override
<a href="index.html#scrapy.http.Response.request" title="scrapy.http.Response.request">[<code>response.request</code>{.xref .py .py-class .docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal}.</p>
<p>If a <a href="index.html#topics-downloader-middleware">[downloader middleware]{.std
.std-ref}</a>{.hoverxref
.tooltip .reference .internal} returns a <a href="index.html#scrapy.http.Response" title="scrapy.http.Response">[<code>Response</code>{.xref .py
.py-class .docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal} object from <a href="index.html#scrapy.downloadermiddlewares.DownloaderMiddleware.process_response" title="scrapy.downloadermiddlewares.DownloaderMiddleware.process_response">[<code>process_response()</code>{.xref .py .py-meth
.docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal} or <a href="index.html#scrapy.downloadermiddlewares.DownloaderMiddleware.process_exception" title="scrapy.downloadermiddlewares.DownloaderMiddleware.process_exception">[<code>process_exception()</code>{.xref .py .py-meth .docutils
.literal
.notranslate}]{.pre}</a>{.reference
.internal} with a custom <a href="index.html#scrapy.http.Request" title="scrapy.http.Request">[<code>Request</code>{.xref .py .py-class .docutils
.literal
.notranslate}]{.pre}</a>{.reference
.internal} object assigned to <a href="index.html#scrapy.http.Response.request" title="scrapy.http.Response.request">[<code>response.request</code>{.xref .py
.py-class .docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal}:</p>
<ul>
<li>
<p>The response is handled by the callback of that custom
<a href="index.html#scrapy.http.Request" title="scrapy.http.Request">[<code>Request</code>{.xref .py .py-class .docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal} object, instead of being handled by the callback of
the original <a href="index.html#scrapy.http.Request" title="scrapy.http.Request">[<code>Request</code>{.xref .py .py-class .docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal} object</p>
</li>
<li>
<p>That custom <a href="index.html#scrapy.http.Request" title="scrapy.http.Request">[<code>Request</code>{.xref .py .py-class .docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal} object is now sent as the [<code>request</code>{.docutils
.literal .notranslate}]{.pre} argument to the
<a href="index.html#std-signal-response_received">[<code>response_received</code>{.xref .std .std-signal .docutils .literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal} signal, instead of the original
<a href="index.html#scrapy.http.Request" title="scrapy.http.Request">[<code>Request</code>{.xref .py .py-class .docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal} object</p>
</li>
</ul>
<p>(<a href="https://github.com/scrapy/scrapy/issues/4529">issue
4529</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/4632">issue
4632</a>{.reference
.external})</p>
</li>
<li>
<p>When using the <a href="index.html#topics-feed-storage-ftp">[FTP feed storage backend]{.std
.std-ref}</a>{.hoverxref .tooltip
.reference .internal}:</p>
<ul>
<li>
<p>It is now possible to set the new [<code>overwrite</code>{.docutils
.literal .notranslate}]{.pre} <a href="index.html#feed-options">[feed option]{.std
.std-ref}</a>{.hoverxref .tooltip
.reference .internal} to [<code>False</code>{.docutils .literal
.notranslate}]{.pre} to append to an existing file instead of
overwriting it</p>
</li>
<li>
<p>The FTP password can now be omitted if it is not necessary</p>
</li>
</ul>
<p>(<a href="https://github.com/scrapy/scrapy/issues/547">issue 547</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/716">issue
716</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/4512">issue
4512</a>{.reference
.external})</p>
</li>
<li>
<p>The [<code>__init__</code>{.docutils .literal .notranslate}]{.pre} method of
<a href="index.html#scrapy.exporters.CsvItemExporter" title="scrapy.exporters.CsvItemExporter">[<code>CsvItemExporter</code>{.xref .py .py-class .docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal} now supports an [<code>errors</code>{.docutils .literal
.notranslate}]{.pre} parameter to indicate how to handle encoding
errors (<a href="https://github.com/scrapy/scrapy/issues/4755">issue
4755</a>{.reference
.external})</p>
</li>
<li>
<p>When <a href="index.html#using-asyncio">[using asyncio]{.std
.std-ref}</a>{.hoverxref .tooltip .reference
.internal}, it is now possible to <a href="index.html#using-custom-loops">[set a custom asyncio loop]{.std
.std-ref}</a>{.hoverxref .tooltip
.reference .internal} (<a href="https://github.com/scrapy/scrapy/issues/4306">issue
4306</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/4414">issue
4414</a>{.reference
.external})</p>
</li>
<li>
<p>Serialized requests (see <a href="index.html#topics-jobs">[Jobs: pausing and resuming crawls]{.std
.std-ref}</a>{.hoverxref .tooltip .reference
.internal}) now support callbacks that are spider methods that
delegate on other callable (<a href="https://github.com/scrapy/scrapy/issues/4756">issue
4756</a>{.reference
.external})</p>
</li>
<li>
<p>When a response is larger than <a href="index.html#std-setting-DOWNLOAD_MAXSIZE">[<code>DOWNLOAD_MAXSIZE</code>{.xref .std
.std-setting .docutils .literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal}, the logged message is now a warning,
instead of an error (<a href="https://github.com/scrapy/scrapy/issues/3874">issue
3874</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/3886">issue
3886</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/4752">issue
4752</a>{.reference
.external})
:::</p>
</li>
</ul>
<p>::: {#id49 .section}</p>
<h5 id="bug-fixesheaderlink-8"><a class="header" href="#bug-fixesheaderlink-8">Bug fixes<a href="#id49" title="Permalink to this heading">¶</a>{.headerlink}</a></h5>
<ul>
<li>
<p>The <a href="index.html#std-command-genspider">[<code>genspider</code>{.xref .std .std-command .docutils .literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal} command no longer overwrites existing
files unless the [<code>--force</code>{.docutils .literal .notranslate}]{.pre}
option is used (<a href="https://github.com/scrapy/scrapy/issues/4561">issue
4561</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/4616">issue
4616</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/4623">issue
4623</a>{.reference
.external})</p>
</li>
<li>
<p>Cookies with an empty value are no longer considered invalid cookies
(<a href="https://github.com/scrapy/scrapy/issues/4772">issue
4772</a>{.reference
.external})</p>
</li>
<li>
<p>The <a href="index.html#std-command-runspider">[<code>runspider</code>{.xref .std .std-command .docutils .literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal} command now supports files with the
[<code>.pyw</code>{.docutils .literal .notranslate}]{.pre} file extension
(<a href="https://github.com/scrapy/scrapy/issues/4643">issue
4643</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/4646">issue
4646</a>{.reference
.external})</p>
</li>
<li>
<p>The <a href="index.html#scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware" title="scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware">[<code>HttpProxyMiddleware</code>{.xref .py .py-class .docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal} middleware now simply ignores unsupported proxy values
(<a href="https://github.com/scrapy/scrapy/issues/3331">issue
3331</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/4778">issue
4778</a>{.reference
.external})</p>
</li>
<li>
<p>Checks for generator callbacks with a [<code>return</code>{.docutils .literal
.notranslate}]{.pre} statement no longer warn about
[<code>return</code>{.docutils .literal .notranslate}]{.pre} statements in
nested functions (<a href="https://github.com/scrapy/scrapy/issues/4720">issue
4720</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/4721">issue
4721</a>{.reference
.external})</p>
</li>
<li>
<p>The system file mode creation mask no longer affects the permissions
of files generated using the <a href="index.html#std-command-startproject">[<code>startproject</code>{.xref .std
.std-command .docutils .literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal} command (<a href="https://github.com/scrapy/scrapy/issues/4722">issue
4722</a>{.reference
.external})</p>
</li>
<li>
<p>[<code>scrapy.utils.iterators.xmliter()</code>{.xref .py .py-func .docutils
.literal .notranslate}]{.pre} now supports namespaced node names
(<a href="https://github.com/scrapy/scrapy/issues/861">issue 861</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/4746">issue
4746</a>{.reference
.external})</p>
</li>
<li>
<p>[<code>Request</code>{.xref .py .py-class .docutils .literal
.notranslate}]{.pre} objects can now have [<code>about:</code>{.docutils
.literal .notranslate}]{.pre} URLs, which can work when using a
headless browser (<a href="https://github.com/scrapy/scrapy/issues/4835">issue
4835</a>{.reference
.external})
:::</p>
</li>
</ul>
<p>::: {#id50 .section}</p>
<h5 id="documentationheaderlink-8"><a class="header" href="#documentationheaderlink-8">Documentation<a href="#id50" title="Permalink to this heading">¶</a>{.headerlink}</a></h5>
<ul>
<li>
<p>The <a href="index.html#std-setting-FEED_URI_PARAMS">[<code>FEED_URI_PARAMS</code>{.xref .std .std-setting .docutils .literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal} setting is now documented (<a href="https://github.com/scrapy/scrapy/issues/4671">issue
4671</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/4724">issue
4724</a>{.reference
.external})</p>
</li>
<li>
<p>Improved the documentation of <a href="index.html#topics-link-extractors">[link extractors]{.std
.std-ref}</a>{.hoverxref .tooltip
.reference .internal} with an usage example from a spider callback
and reference documentation for the <a href="index.html#scrapy.link.Link" title="scrapy.link.Link">[<code>Link</code>{.xref .py .py-class
.docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal} class (<a href="https://github.com/scrapy/scrapy/issues/4751">issue
4751</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/4775">issue
4775</a>{.reference
.external})</p>
</li>
<li>
<p>Clarified the impact of <a href="index.html#std-setting-CONCURRENT_REQUESTS">[<code>CONCURRENT_REQUESTS</code>{.xref .std
.std-setting .docutils .literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal} when using the <a href="index.html#scrapy.extensions.closespider.CloseSpider" title="scrapy.extensions.closespider.CloseSpider">[<code>CloseSpider</code>{.xref
.py .py-class .docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal} extension (<a href="https://github.com/scrapy/scrapy/issues/4836">issue
4836</a>{.reference
.external})</p>
</li>
<li>
<p>Removed references to Python 2's [<code>unicode</code>{.docutils .literal
.notranslate}]{.pre} type (<a href="https://github.com/scrapy/scrapy/issues/4547">issue
4547</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/4703">issue
4703</a>{.reference
.external})</p>
</li>
<li>
<p>We now have an <a href="index.html#deprecation-policy">[official deprecation policy]{.std
.std-ref}</a>{.hoverxref .tooltip
.reference .internal} (<a href="https://github.com/scrapy/scrapy/issues/4705">issue
4705</a>{.reference
.external})</p>
</li>
<li>
<p>Our <a href="index.html#documentation-policies">[documentation policies]{.std
.std-ref}</a>{.hoverxref .tooltip
.reference .internal} now cover usage of Sphinx's
<a href="https://www.sphinx-doc.org/en/master/usage/restructuredtext/directives.html#directive-versionadded" title="(in Sphinx v7.3.0)">[<code>versionadded</code>{.xref .rst .rst-dir .docutils .literal
.notranslate}]{.pre}</a>{.reference
.external} and <a href="https://www.sphinx-doc.org/en/master/usage/restructuredtext/directives.html#directive-versionchanged" title="(in Sphinx v7.3.0)">[<code>versionchanged</code>{.xref .rst .rst-dir .docutils
.literal
.notranslate}]{.pre}</a>{.reference
.external} directives, and we have removed usages referencing Scrapy
1.4.0 and earlier versions (<a href="https://github.com/scrapy/scrapy/issues/3971">issue
3971</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/4310">issue
4310</a>{.reference
.external})</p>
</li>
<li>
<p>Other documentation cleanups (<a href="https://github.com/scrapy/scrapy/issues/4090">issue
4090</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/4782">issue
4782</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/4800">issue
4800</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/4801">issue
4801</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/4809">issue
4809</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/4816">issue
4816</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/4825">issue
4825</a>{.reference
.external})
:::</p>
</li>
</ul>
<p>::: {#id51 .section}</p>
<h5 id="quality-assuranceheaderlink-8"><a class="header" href="#quality-assuranceheaderlink-8">Quality assurance<a href="#id51" title="Permalink to this heading">¶</a>{.headerlink}</a></h5>
<ul>
<li>
<p>Extended typing hints (<a href="https://github.com/scrapy/scrapy/issues/4243">issue
4243</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/4691">issue
4691</a>{.reference
.external})</p>
</li>
<li>
<p>Added tests for the <a href="index.html#std-command-check">[<code>check</code>{.xref .std .std-command .docutils
.literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal} command (<a href="https://github.com/scrapy/scrapy/issues/4663">issue
4663</a>{.reference
.external})</p>
</li>
<li>
<p>Fixed test failures on Debian (<a href="https://github.com/scrapy/scrapy/issues/4726">issue
4726</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/4727">issue
4727</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/4735">issue
4735</a>{.reference
.external})</p>
</li>
<li>
<p>Improved Windows test coverage (<a href="https://github.com/scrapy/scrapy/issues/4723">issue
4723</a>{.reference
.external})</p>
</li>
<li>
<p>Switched to <a href="https://docs.python.org/3/reference/lexical_analysis.html#f-strings" title="(in Python v3.12)">[formatted string literals]{.xref .std
.std-ref}</a>{.reference
.external} where possible (<a href="https://github.com/scrapy/scrapy/issues/4307">issue
4307</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/4324">issue
4324</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/4672">issue
4672</a>{.reference
.external})</p>
</li>
<li>
<p>Modernized [<code>super()</code>{.xref .py .py-func .docutils .literal
.notranslate}]{.pre} usage (<a href="https://github.com/scrapy/scrapy/issues/4707">issue
4707</a>{.reference
.external})</p>
</li>
<li>
<p>Other code and test cleanups (<a href="https://github.com/scrapy/scrapy/issues/1790">issue
1790</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/3288">issue
3288</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/4165">issue
4165</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/4564">issue
4564</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/4651">issue
4651</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/4714">issue
4714</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/4738">issue
4738</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/4745">issue
4745</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/4747">issue
4747</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/4761">issue
4761</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/4765">issue
4765</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/4804">issue
4804</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/4817">issue
4817</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/4820">issue
4820</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/4822">issue
4822</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/4839">issue
4839</a>{.reference
.external})
:::
:::</p>
</li>
</ul>
<p>::: {#scrapy-2-3-0-2020-08-04 .section}
[]{#release-2-3-0}</p>
<h4 id="scrapy-230-2020-08-04headerlink"><a class="header" href="#scrapy-230-2020-08-04headerlink">Scrapy 2.3.0 (2020-08-04)<a href="#scrapy-2-3-0-2020-08-04" title="Permalink to this heading">¶</a>{.headerlink}</a></h4>
<p>Highlights:</p>
<ul>
<li>
<p><a href="index.html#topics-feed-exports">[Feed exports]{.std
.std-ref}</a>{.hoverxref .tooltip
.reference .internal} now support <a href="index.html#topics-feed-storage-gcs">[Google Cloud Storage]{.std
.std-ref}</a>{.hoverxref .tooltip
.reference .internal} as a storage backend</p>
</li>
<li>
<p>The new <a href="index.html#std-setting-FEED_EXPORT_BATCH_ITEM_COUNT">[<code>FEED_EXPORT_BATCH_ITEM_COUNT</code>{.xref .std .std-setting
.docutils .literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal} setting allows to deliver output
items in batches of up to the specified number of items.</p>
<p>It also serves as a workaround for <a href="index.html#delayed-file-delivery">[delayed file delivery]{.std
.std-ref}</a>{.hoverxref .tooltip
.reference .internal}, which causes Scrapy to only start item
delivery after the crawl has finished when using certain storage
backends (<a href="index.html#topics-feed-storage-s3">[S3]{.std
.std-ref}</a>{.hoverxref .tooltip
.reference .internal}, <a href="index.html#topics-feed-storage-ftp">[FTP]{.std
.std-ref}</a>{.hoverxref .tooltip
.reference .internal}, and now <a href="index.html#topics-feed-storage-gcs">[GCS]{.std
.std-ref}</a>{.hoverxref .tooltip
.reference .internal}).</p>
</li>
<li>
<p>The base implementation of <a href="index.html#topics-loaders">[item loaders]{.std
.std-ref}</a>{.hoverxref .tooltip .reference
.internal} has been moved into a separate library,
<a href="https://itemloaders.readthedocs.io/en/latest/index.html" title="(in itemloaders)">[itemloaders]{.xref .std
.std-doc}</a>{.reference
.external}, allowing usage from outside Scrapy and a separate
release schedule</p>
</li>
</ul>
<p>::: {#id52 .section}</p>
<h5 id="deprecation-removalsheaderlink-6"><a class="header" href="#deprecation-removalsheaderlink-6">Deprecation removals<a href="#id52" title="Permalink to this heading">¶</a>{.headerlink}</a></h5>
<ul>
<li>
<p>Removed the following classes and their parent modules from
[<code>scrapy.linkextractors</code>{.docutils .literal .notranslate}]{.pre}:</p>
<ul>
<li>
<p>[<code>htmlparser.HtmlParserLinkExtractor</code>{.docutils .literal
.notranslate}]{.pre}</p>
</li>
<li>
<p>[<code>regex.RegexLinkExtractor</code>{.docutils .literal
.notranslate}]{.pre}</p>
</li>
<li>
<p>[<code>sgml.BaseSgmlLinkExtractor</code>{.docutils .literal
.notranslate}]{.pre}</p>
</li>
<li>
<p>[<code>sgml.SgmlLinkExtractor</code>{.docutils .literal
.notranslate}]{.pre}</p>
</li>
</ul>
<p>Use <a href="index.html#scrapy.linkextractors.lxmlhtml.LxmlLinkExtractor" title="scrapy.linkextractors.lxmlhtml.LxmlLinkExtractor">[<code>LinkExtractor</code>{.xref .py .py-class .docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal} instead (<a href="https://github.com/scrapy/scrapy/issues/4356">issue
4356</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/4679">issue
4679</a>{.reference
.external})
:::</p>
</li>
</ul>
<p>::: {#id53 .section}</p>
<h5 id="deprecationsheaderlink-8"><a class="header" href="#deprecationsheaderlink-8">Deprecations<a href="#id53" title="Permalink to this heading">¶</a>{.headerlink}</a></h5>
<ul>
<li>The [<code>scrapy.utils.python.retry_on_eintr</code>{.docutils .literal
.notranslate}]{.pre} function is now deprecated (<a href="https://github.com/scrapy/scrapy/issues/4683">issue
4683</a>{.reference
.external})
:::</li>
</ul>
<p>::: {#id54 .section}</p>
<h5 id="new-featuresheaderlink-9"><a class="header" href="#new-featuresheaderlink-9">New features<a href="#id54" title="Permalink to this heading">¶</a>{.headerlink}</a></h5>
<ul>
<li>
<p><a href="index.html#topics-feed-exports">[Feed exports]{.std
.std-ref}</a>{.hoverxref .tooltip
.reference .internal} support <a href="index.html#topics-feed-storage-gcs">[Google Cloud Storage]{.std
.std-ref}</a>{.hoverxref .tooltip
.reference .internal} (<a href="https://github.com/scrapy/scrapy/issues/685">issue
685</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/3608">issue
3608</a>{.reference
.external})</p>
</li>
<li>
<p>New <a href="index.html#std-setting-FEED_EXPORT_BATCH_ITEM_COUNT">[<code>FEED_EXPORT_BATCH_ITEM_COUNT</code>{.xref .std .std-setting
.docutils .literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal} setting for batch deliveries (<a href="https://github.com/scrapy/scrapy/issues/4250">issue
4250</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/4434">issue
4434</a>{.reference
.external})</p>
</li>
<li>
<p>The <a href="index.html#std-command-parse">[<code>parse</code>{.xref .std .std-command .docutils .literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal} command now allows specifying an
output file (<a href="https://github.com/scrapy/scrapy/issues/4317">issue
4317</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/4377">issue
4377</a>{.reference
.external})</p>
</li>
<li>
<p><a href="index.html#scrapy.http.Request.from_curl" title="scrapy.http.Request.from_curl">[<code>Request.from_curl</code>{.xref .py .py-meth .docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal} and <a href="index.html#scrapy.utils.curl.curl_to_request_kwargs" title="scrapy.utils.curl.curl_to_request_kwargs">[<code>curl_to_request_kwargs()</code>{.xref .py .py-func
.docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal} now also support [<code>--data-raw</code>{.docutils .literal
.notranslate}]{.pre} (<a href="https://github.com/scrapy/scrapy/issues/4612">issue
4612</a>{.reference
.external})</p>
</li>
<li>
<p>A [<code>parse</code>{.docutils .literal .notranslate}]{.pre} callback may now
be used in built-in spider subclasses, such as <a href="index.html#scrapy.spiders.CrawlSpider" title="scrapy.spiders.CrawlSpider">[<code>CrawlSpider</code>{.xref
.py .py-class .docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal} (<a href="https://github.com/scrapy/scrapy/issues/712">issue
712</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/732">issue
732</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/781">issue
781</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/4254">issue
4254</a>{.reference
.external} )
:::</p>
</li>
</ul>
<p>::: {#id55 .section}</p>
<h5 id="bug-fixesheaderlink-9"><a class="header" href="#bug-fixesheaderlink-9">Bug fixes<a href="#id55" title="Permalink to this heading">¶</a>{.headerlink}</a></h5>
<ul>
<li>
<p>Fixed the <a href="index.html#topics-feed-format-csv">[CSV exporting]{.std
.std-ref}</a>{.hoverxref .tooltip
.reference .internal} of <a href="index.html#dataclass-items">[dataclass items]{.std
.std-ref}</a>{.hoverxref .tooltip
.reference .internal} and <a href="index.html#attrs-items">[attr.s items]{.std
.std-ref}</a>{.hoverxref .tooltip .reference
.internal} (<a href="https://github.com/scrapy/scrapy/issues/4667">issue
4667</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/4668">issue
4668</a>{.reference
.external})</p>
</li>
<li>
<p><a href="index.html#scrapy.http.Request.from_curl" title="scrapy.http.Request.from_curl">[<code>Request.from_curl</code>{.xref .py .py-meth .docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal} and <a href="index.html#scrapy.utils.curl.curl_to_request_kwargs" title="scrapy.utils.curl.curl_to_request_kwargs">[<code>curl_to_request_kwargs()</code>{.xref .py .py-func
.docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal} now set the request method to [<code>POST</code>{.docutils .literal
.notranslate}]{.pre} when a request body is specified and no request
method is specified (<a href="https://github.com/scrapy/scrapy/issues/4612">issue
4612</a>{.reference
.external})</p>
</li>
<li>
<p>The processing of ANSI escape sequences in enabled in Windows
10.0.14393 and later, where it is required for colored output
(<a href="https://github.com/scrapy/scrapy/issues/4393">issue
4393</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/4403">issue
4403</a>{.reference
.external})
:::</p>
</li>
</ul>
<p>::: {#id56 .section}</p>
<h5 id="documentationheaderlink-9"><a class="header" href="#documentationheaderlink-9">Documentation<a href="#id56" title="Permalink to this heading">¶</a>{.headerlink}</a></h5>
<ul>
<li>
<p>Updated the <a href="https://www.openssl.org/docs/manmaster/man1/openssl-ciphers.html#CIPHER-LIST-FORMAT">OpenSSL cipher list
format</a>{.reference
.external} link in the documentation about the
<a href="index.html#std-setting-DOWNLOADER_CLIENT_TLS_CIPHERS">[<code>DOWNLOADER_CLIENT_TLS_CIPHERS</code>{.xref .std .std-setting .docutils
.literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal} setting (<a href="https://github.com/scrapy/scrapy/issues/4653">issue
4653</a>{.reference
.external})</p>
</li>
<li>
<p>Simplified the code example in <a href="index.html#topics-loaders-dataclass">[Working with dataclass items]{.std
.std-ref}</a>{.hoverxref .tooltip
.reference .internal} (<a href="https://github.com/scrapy/scrapy/issues/4652">issue
4652</a>{.reference
.external})
:::</p>
</li>
</ul>
<p>::: {#id57 .section}</p>
<h5 id="quality-assuranceheaderlink-9"><a class="header" href="#quality-assuranceheaderlink-9">Quality assurance<a href="#id57" title="Permalink to this heading">¶</a>{.headerlink}</a></h5>
<ul>
<li>
<p>The base implementation of <a href="index.html#topics-loaders">[item loaders]{.std
.std-ref}</a>{.hoverxref .tooltip .reference
.internal} has been moved into <a href="https://itemloaders.readthedocs.io/en/latest/index.html" title="(in itemloaders)">[itemloaders]{.xref .std
.std-doc}</a>{.reference
.external} (<a href="https://github.com/scrapy/scrapy/issues/4005">issue
4005</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/4516">issue
4516</a>{.reference
.external})</p>
</li>
<li>
<p>Fixed a silenced error in some scheduler tests (<a href="https://github.com/scrapy/scrapy/issues/4644">issue
4644</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/4645">issue
4645</a>{.reference
.external})</p>
</li>
<li>
<p>Renewed the localhost certificate used for SSL tests (<a href="https://github.com/scrapy/scrapy/issues/4650">issue
4650</a>{.reference
.external})</p>
</li>
<li>
<p>Removed cookie-handling code specific to Python 2 (<a href="https://github.com/scrapy/scrapy/issues/4682">issue
4682</a>{.reference
.external})</p>
</li>
<li>
<p>Stopped using Python 2 unicode literal syntax (<a href="https://github.com/scrapy/scrapy/issues/4704">issue
4704</a>{.reference
.external})</p>
</li>
<li>
<p>Stopped using a backlash for line continuation (<a href="https://github.com/scrapy/scrapy/issues/4673">issue
4673</a>{.reference
.external})</p>
</li>
<li>
<p>Removed unneeded entries from the MyPy exception list (<a href="https://github.com/scrapy/scrapy/issues/4690">issue
4690</a>{.reference
.external})</p>
</li>
<li>
<p>Automated tests now pass on Windows as part of our continuous
integration system (<a href="https://github.com/scrapy/scrapy/issues/4458">issue
4458</a>{.reference
.external})</p>
</li>
<li>
<p>Automated tests now pass on the latest PyPy version for supported
Python versions in our continuous integration system (<a href="https://github.com/scrapy/scrapy/issues/4504">issue
4504</a>{.reference
.external})
:::
:::</p>
</li>
</ul>
<p>::: {#scrapy-2-2-1-2020-07-17 .section}
[]{#release-2-2-1}</p>
<h4 id="scrapy-221-2020-07-17headerlink"><a class="header" href="#scrapy-221-2020-07-17headerlink">Scrapy 2.2.1 (2020-07-17)<a href="#scrapy-2-2-1-2020-07-17" title="Permalink to this heading">¶</a>{.headerlink}</a></h4>
<ul>
<li>The <a href="index.html#std-command-startproject">[<code>startproject</code>{.xref .std .std-command .docutils .literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal} command no longer makes unintended
changes to the permissions of files in the destination folder, such
as removing execution permissions (<a href="https://github.com/scrapy/scrapy/issues/4662">issue
4662</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/4666">issue
4666</a>{.reference
.external})
:::</li>
</ul>
<p>::: {#scrapy-2-2-0-2020-06-24 .section}
[]{#release-2-2-0}</p>
<h4 id="scrapy-220-2020-06-24headerlink"><a class="header" href="#scrapy-220-2020-06-24headerlink">Scrapy 2.2.0 (2020-06-24)<a href="#scrapy-2-2-0-2020-06-24" title="Permalink to this heading">¶</a>{.headerlink}</a></h4>
<p>Highlights:</p>
<ul>
<li>
<p>Python 3.5.2+ is required now</p>
</li>
<li>
<p><a href="index.html#dataclass-items">[dataclass objects]{.std
.std-ref}</a>{.hoverxref .tooltip
.reference .internal} and <a href="index.html#attrs-items">[attrs objects]{.std
.std-ref}</a>{.hoverxref .tooltip .reference
.internal} are now valid <a href="index.html#item-types">[item types]{.std
.std-ref}</a>{.hoverxref .tooltip .reference
.internal}</p>
</li>
<li>
<p>New <a href="index.html#scrapy.http.TextResponse.json" title="scrapy.http.TextResponse.json">[<code>TextResponse.json</code>{.xref .py .py-meth .docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal} method</p>
</li>
<li>
<p>New <a href="index.html#std-signal-bytes_received">[<code>bytes_received</code>{.xref .std .std-signal .docutils .literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal} signal that allows canceling response
download</p>
</li>
<li>
<p><a href="index.html#scrapy.downloadermiddlewares.cookies.CookiesMiddleware" title="scrapy.downloadermiddlewares.cookies.CookiesMiddleware">[<code>CookiesMiddleware</code>{.xref .py .py-class .docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal} fixes</p>
</li>
</ul>
<p>::: {#id58 .section}</p>
<h5 id="backward-incompatible-changesheaderlink-4"><a class="header" href="#backward-incompatible-changesheaderlink-4">Backward-incompatible changes<a href="#id58" title="Permalink to this heading">¶</a>{.headerlink}</a></h5>
<ul>
<li>Support for Python 3.5.0 and 3.5.1 has been dropped; Scrapy now
refuses to run with a Python version lower than 3.5.2, which
introduced <a href="https://docs.python.org/3/library/typing.html#typing.Type" title="(in Python v3.12)">[<code>typing.Type</code>{.xref .py .py-class .docutils .literal
.notranslate}]{.pre}</a>{.reference
.external} (<a href="https://github.com/scrapy/scrapy/issues/4615">issue
4615</a>{.reference
.external})
:::</li>
</ul>
<p>::: {#id59 .section}</p>
<h5 id="deprecationsheaderlink-9"><a class="header" href="#deprecationsheaderlink-9">Deprecations<a href="#id59" title="Permalink to this heading">¶</a>{.headerlink}</a></h5>
<ul>
<li>
<p>[<code>TextResponse.body_as_unicode</code>{.xref .py .py-meth .docutils
.literal .notranslate}]{.pre} is now deprecated, use
<a href="index.html#scrapy.http.TextResponse.text" title="scrapy.http.TextResponse.text">[<code>TextResponse.text</code>{.xref .py .py-attr .docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal} instead (<a href="https://github.com/scrapy/scrapy/issues/4546">issue
4546</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/4555">issue
4555</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/4579">issue
4579</a>{.reference
.external})</p>
</li>
<li>
<p>[<code>scrapy.item.BaseItem</code>{.xref .py .py-class .docutils .literal
.notranslate}]{.pre} is now deprecated, use
[<code>scrapy.item.Item</code>{.xref .py .py-class .docutils .literal
.notranslate}]{.pre} instead (<a href="https://github.com/scrapy/scrapy/issues/4534">issue
4534</a>{.reference
.external})
:::</p>
</li>
</ul>
<p>::: {#id60 .section}</p>
<h5 id="new-featuresheaderlink-10"><a class="header" href="#new-featuresheaderlink-10">New features<a href="#id60" title="Permalink to this heading">¶</a>{.headerlink}</a></h5>
<ul>
<li>
<p><a href="index.html#dataclass-items">[dataclass objects]{.std
.std-ref}</a>{.hoverxref .tooltip
.reference .internal} and <a href="index.html#attrs-items">[attrs objects]{.std
.std-ref}</a>{.hoverxref .tooltip .reference
.internal} are now valid <a href="index.html#item-types">[item types]{.std
.std-ref}</a>{.hoverxref .tooltip .reference
.internal}, and a new
<a href="https://github.com/scrapy/itemadapter">itemadapter</a>{.reference
.external} library makes it easy to write code that <a href="index.html#supporting-item-types">[supports any
item type]{.std
.std-ref}</a>{.hoverxref .tooltip
.reference .internal} (<a href="https://github.com/scrapy/scrapy/issues/2749">issue
2749</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/2807">issue
2807</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/3761">issue
3761</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/3881">issue
3881</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/4642">issue
4642</a>{.reference
.external})</p>
</li>
<li>
<p>A new <a href="index.html#scrapy.http.TextResponse.json" title="scrapy.http.TextResponse.json">[<code>TextResponse.json</code>{.xref .py .py-meth .docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal} method allows to deserialize JSON responses (<a href="https://github.com/scrapy/scrapy/issues/2444">issue
2444</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/4460">issue
4460</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/4574">issue
4574</a>{.reference
.external})</p>
</li>
<li>
<p>A new <a href="index.html#std-signal-bytes_received">[<code>bytes_received</code>{.xref .std .std-signal .docutils .literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal} signal allows monitoring response
download progress and <a href="index.html#topics-stop-response-download">[stopping downloads]{.std
.std-ref}</a>{.hoverxref
.tooltip .reference .internal} (<a href="https://github.com/scrapy/scrapy/issues/4205">issue
4205</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/4559">issue
4559</a>{.reference
.external})</p>
</li>
<li>
<p>The dictionaries in the result list of a <a href="index.html#topics-media-pipeline">[media pipeline]{.std
.std-ref}</a>{.hoverxref .tooltip
.reference .internal} now include a new key, [<code>status</code>{.docutils
.literal .notranslate}]{.pre}, which indicates if the file was
downloaded or, if the file was not downloaded, why it was not
downloaded; see <a href="index.html#scrapy.pipelines.files.FilesPipeline.get_media_requests" title="scrapy.pipelines.files.FilesPipeline.get_media_requests">[<code>FilesPipeline.get_media_requests</code>{.xref .py
.py-meth .docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal} for more information (<a href="https://github.com/scrapy/scrapy/issues/2893">issue
2893</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/4486">issue
4486</a>{.reference
.external})</p>
</li>
<li>
<p>When using <a href="index.html#media-pipeline-gcs">[Google Cloud Storage]{.std
.std-ref}</a>{.hoverxref .tooltip
.reference .internal} for a <a href="index.html#topics-media-pipeline">[media pipeline]{.std
.std-ref}</a>{.hoverxref .tooltip
.reference .internal}, a warning is now logged if the configured
credentials do not grant the required permissions (<a href="https://github.com/scrapy/scrapy/issues/4346">issue
4346</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/4508">issue
4508</a>{.reference
.external})</p>
</li>
<li>
<p><a href="index.html#topics-link-extractors">[Link extractors]{.std
.std-ref}</a>{.hoverxref .tooltip
.reference .internal} are now serializable, as long as you do not
use <a href="https://docs.python.org/3/reference/expressions.html#lambda" title="(in Python v3.12)">[lambdas]{.xref .std
.std-ref}</a>{.reference
.external} for parameters; for example, you can now pass link
extractors in <a href="index.html#scrapy.http.Request.cb_kwargs" title="scrapy.http.Request.cb_kwargs">[<code>Request.cb_kwargs</code>{.xref .py .py-attr .docutils
.literal
.notranslate}]{.pre}</a>{.reference
.internal} or <a href="index.html#scrapy.http.Request.meta" title="scrapy.http.Request.meta">[<code>Request.meta</code>{.xref .py .py-attr .docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal} when <a href="index.html#topics-jobs">[persisting scheduled requests]{.std
.std-ref}</a>{.hoverxref .tooltip .reference
.internal} (<a href="https://github.com/scrapy/scrapy/issues/4554">issue
4554</a>{.reference
.external})</p>
</li>
<li>
<p>Upgraded the <a href="https://docs.python.org/3/library/pickle.html#pickle-protocols" title="(in Python v3.12)">[pickle protocol]{.xref .std
.std-ref}</a>{.reference
.external} that Scrapy uses from protocol 2 to protocol 4, improving
serialization capabilities and performance (<a href="https://github.com/scrapy/scrapy/issues/4135">issue
4135</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/4541">issue
4541</a>{.reference
.external})</p>
</li>
<li>
<p>[<code>scrapy.utils.misc.create_instance()</code>{.xref .py .py-func .docutils
.literal .notranslate}]{.pre} now raises a <a href="https://docs.python.org/3/library/exceptions.html#TypeError" title="(in Python v3.12)">[<code>TypeError</code>{.xref .py
.py-exc .docutils .literal
.notranslate}]{.pre}</a>{.reference
.external} exception if the resulting instance is [<code>None</code>{.docutils
.literal .notranslate}]{.pre} (<a href="https://github.com/scrapy/scrapy/issues/4528">issue
4528</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/4532">issue
4532</a>{.reference
.external})
:::</p>
</li>
</ul>
<p>::: {#id61 .section}</p>
<h5 id="bug-fixesheaderlink-10"><a class="header" href="#bug-fixesheaderlink-10">Bug fixes<a href="#id61" title="Permalink to this heading">¶</a>{.headerlink}</a></h5>
<ul>
<li>
<p><a href="index.html#scrapy.downloadermiddlewares.cookies.CookiesMiddleware" title="scrapy.downloadermiddlewares.cookies.CookiesMiddleware">[<code>CookiesMiddleware</code>{.xref .py .py-class .docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal} no longer discards cookies defined in
<a href="index.html#scrapy.http.Request.headers" title="scrapy.http.Request.headers">[<code>Request.headers</code>{.xref .py .py-attr .docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal} (<a href="https://github.com/scrapy/scrapy/issues/1992">issue
1992</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/2400">issue
2400</a>{.reference
.external})</p>
</li>
<li>
<p><a href="index.html#scrapy.downloadermiddlewares.cookies.CookiesMiddleware" title="scrapy.downloadermiddlewares.cookies.CookiesMiddleware">[<code>CookiesMiddleware</code>{.xref .py .py-class .docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal} no longer re-encodes cookies defined as <a href="https://docs.python.org/3/library/stdtypes.html#bytes" title="(in Python v3.12)">[<code>bytes</code>{.xref
.py .py-class .docutils .literal
.notranslate}]{.pre}</a>{.reference
.external} in the [<code>cookies</code>{.docutils .literal .notranslate}]{.pre}
parameter of the [<code>__init__</code>{.docutils .literal .notranslate}]{.pre}
method of <a href="index.html#scrapy.http.Request" title="scrapy.http.Request">[<code>Request</code>{.xref .py .py-class .docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal} (<a href="https://github.com/scrapy/scrapy/issues/2400">issue
2400</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/3575">issue
3575</a>{.reference
.external})</p>
</li>
<li>
<p>When <a href="index.html#std-setting-FEEDS">[<code>FEEDS</code>{.xref .std .std-setting .docutils .literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal} defines multiple URIs,
<a href="index.html#std-setting-FEED_STORE_EMPTY">[<code>FEED_STORE_EMPTY</code>{.xref .std .std-setting .docutils .literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal} is [<code>False</code>{.docutils .literal
.notranslate}]{.pre} and the crawl yields no items, Scrapy no longer
stops feed exports after the first URI (<a href="https://github.com/scrapy/scrapy/issues/4621">issue
4621</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/4626">issue
4626</a>{.reference
.external})</p>
</li>
<li>
<p><a href="index.html#scrapy.spiders.Spider" title="scrapy.spiders.Spider">[<code>Spider</code>{.xref .py .py-class .docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal} callbacks defined using <a href="index.html#document-topics/coroutines">[coroutine
syntax]{.doc}</a>{.reference
.internal} no longer need to return an iterable, and may instead
return a <a href="index.html#scrapy.http.Request" title="scrapy.http.Request">[<code>Request</code>{.xref .py .py-class .docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal} object, an <a href="index.html#topics-items">[item]{.std
.std-ref}</a>{.hoverxref .tooltip .reference
.internal}, or [<code>None</code>{.docutils .literal .notranslate}]{.pre}
(<a href="https://github.com/scrapy/scrapy/issues/4609">issue
4609</a>{.reference
.external})</p>
</li>
<li>
<p>The <a href="index.html#std-command-startproject">[<code>startproject</code>{.xref .std .std-command .docutils .literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal} command now ensures that the
generated project folders and files have the right permissions
(<a href="https://github.com/scrapy/scrapy/issues/4604">issue
4604</a>{.reference
.external})</p>
</li>
<li>
<p>Fix a <a href="https://docs.python.org/3/library/exceptions.html#KeyError" title="(in Python v3.12)">[<code>KeyError</code>{.xref .py .py-exc .docutils .literal
.notranslate}]{.pre}</a>{.reference
.external} exception being sometimes raised from
[<code>scrapy.utils.datatypes.LocalWeakReferencedCache</code>{.xref .py
.py-class .docutils .literal .notranslate}]{.pre} (<a href="https://github.com/scrapy/scrapy/issues/4597">issue
4597</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/4599">issue
4599</a>{.reference
.external})</p>
</li>
<li>
<p>When <a href="index.html#std-setting-FEEDS">[<code>FEEDS</code>{.xref .std .std-setting .docutils .literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal} defines multiple URIs, log messages
about items being stored now contain information from the
corresponding feed, instead of always containing information about
only one of the feeds (<a href="https://github.com/scrapy/scrapy/issues/4619">issue
4619</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/4629">issue
4629</a>{.reference
.external})
:::</p>
</li>
</ul>
<p>::: {#id62 .section}</p>
<h5 id="documentationheaderlink-10"><a class="header" href="#documentationheaderlink-10">Documentation<a href="#id62" title="Permalink to this heading">¶</a>{.headerlink}</a></h5>
<ul>
<li>
<p>Added a new section about <a href="index.html#errback-cb-kwargs">[accessing cb_kwargs from errbacks]{.std
.std-ref}</a>{.hoverxref .tooltip
.reference .internal} (<a href="https://github.com/scrapy/scrapy/issues/4598">issue
4598</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/4634">issue
4634</a>{.reference
.external})</p>
</li>
<li>
<p>Covered <a href="https://github.com/Nykakin/chompjs">chompjs</a>{.reference
.external} in <a href="index.html#topics-parsing-javascript">[Parsing JavaScript code]{.std
.std-ref}</a>{.hoverxref .tooltip
.reference .internal} (<a href="https://github.com/scrapy/scrapy/issues/4556">issue
4556</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/4562">issue
4562</a>{.reference
.external})</p>
</li>
<li>
<p>Removed from
<a href="index.html#document-topics/coroutines">[Coroutines]{.doc}</a>{.reference
.internal} the warning about the API being experimental (<a href="https://github.com/scrapy/scrapy/issues/4511">issue
4511</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/4513">issue
4513</a>{.reference
.external})</p>
</li>
<li>
<p>Removed references to unsupported versions of <a href="https://docs.twisted.org/en/stable/index.html" title="(in Twisted v23.10)">[Twisted]{.xref .std
.std-doc}</a>{.reference
.external} (<a href="https://github.com/scrapy/scrapy/issues/4533">issue
4533</a>{.reference
.external})</p>
</li>
<li>
<p>Updated the description of the <a href="index.html#screenshotpipeline">[screenshot pipeline example]{.std
.std-ref}</a>{.hoverxref .tooltip
.reference .internal}, which now uses <a href="index.html#document-topics/coroutines">[coroutine
syntax]{.doc}</a>{.reference
.internal} instead of returning a <a href="https://docs.twisted.org/en/stable/api/twisted.internet.defer.Deferred.html" title="(in Twisted)">[<code>Deferred</code>{.xref .py .py-class
.docutils .literal
.notranslate}]{.pre}</a>{.reference
.external} (<a href="https://github.com/scrapy/scrapy/issues/4514">issue
4514</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/4593">issue
4593</a>{.reference
.external})</p>
</li>
<li>
<p>Removed a misleading import line from the
<a href="index.html#scrapy.utils.log.configure_logging" title="scrapy.utils.log.configure_logging">[<code>scrapy.utils.log.configure_logging()</code>{.xref .py .py-func
.docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal} code example (<a href="https://github.com/scrapy/scrapy/issues/4510">issue
4510</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/4587">issue
4587</a>{.reference
.external})</p>
</li>
<li>
<p>The display-on-hover behavior of internal documentation references
now also covers links to <a href="index.html#topics-commands">[commands]{.std
.std-ref}</a>{.hoverxref .tooltip
.reference .internal}, <a href="index.html#scrapy.http.Request.meta" title="scrapy.http.Request.meta">[<code>Request.meta</code>{.xref .py .py-attr .docutils
.literal
.notranslate}]{.pre}</a>{.reference
.internal} keys, <a href="index.html#topics-settings">[settings]{.std
.std-ref}</a>{.hoverxref .tooltip
.reference .internal} and <a href="index.html#topics-signals">[signals]{.std
.std-ref}</a>{.hoverxref .tooltip .reference
.internal} (<a href="https://github.com/scrapy/scrapy/issues/4495">issue
4495</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/4563">issue
4563</a>{.reference
.external})</p>
</li>
<li>
<p>It is again possible to download the documentation for offline
reading (<a href="https://github.com/scrapy/scrapy/issues/4578">issue
4578</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/4585">issue
4585</a>{.reference
.external})</p>
</li>
<li>
<p>Removed backslashes preceding [<code>*args</code>{.docutils .literal
.notranslate}]{.pre} and [<code>**kwargs</code>{.docutils .literal
.notranslate}]{.pre} in some function and method signatures (<a href="https://github.com/scrapy/scrapy/issues/4592">issue
4592</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/4596">issue
4596</a>{.reference
.external})
:::</p>
</li>
</ul>
<p>::: {#id63 .section}</p>
<h5 id="quality-assuranceheaderlink-10"><a class="header" href="#quality-assuranceheaderlink-10">Quality assurance<a href="#id63" title="Permalink to this heading">¶</a>{.headerlink}</a></h5>
<ul>
<li>
<p>Adjusted the code base further to our <a href="index.html#coding-style">[style guidelines]{.std
.std-ref}</a>{.hoverxref .tooltip .reference
.internal} (<a href="https://github.com/scrapy/scrapy/issues/4237">issue
4237</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/4525">issue
4525</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/4538">issue
4538</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/4539">issue
4539</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/4540">issue
4540</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/4542">issue
4542</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/4543">issue
4543</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/4544">issue
4544</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/4545">issue
4545</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/4557">issue
4557</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/4558">issue
4558</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/4566">issue
4566</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/4568">issue
4568</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/4572">issue
4572</a>{.reference
.external})</p>
</li>
<li>
<p>Removed remnants of Python 2 support (<a href="https://github.com/scrapy/scrapy/issues/4550">issue
4550</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/4553">issue
4553</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/4568">issue
4568</a>{.reference
.external})</p>
</li>
<li>
<p>Improved code sharing between the <a href="index.html#std-command-crawl">[<code>crawl</code>{.xref .std .std-command
.docutils .literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal} and <a href="index.html#std-command-runspider">[<code>runspider</code>{.xref .std
.std-command .docutils .literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal} commands (<a href="https://github.com/scrapy/scrapy/issues/4548">issue
4548</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/4552">issue
4552</a>{.reference
.external})</p>
</li>
<li>
<p>Replaced [<code>chain(*iterable)</code>{.docutils .literal .notranslate}]{.pre}
with [<code>chain.from_iterable(iterable)</code>{.docutils .literal
.notranslate}]{.pre} (<a href="https://github.com/scrapy/scrapy/issues/4635">issue
4635</a>{.reference
.external})</p>
</li>
<li>
<p>You may now run the <a href="https://docs.python.org/3/library/asyncio.html#module-asyncio" title="(in Python v3.12)">[<code>asyncio</code>{.xref .py .py-mod .docutils .literal
.notranslate}]{.pre}</a>{.reference
.external} tests with Tox on any Python version (<a href="https://github.com/scrapy/scrapy/issues/4521">issue
4521</a>{.reference
.external})</p>
</li>
<li>
<p>Updated test requirements to reflect an incompatibility with pytest
5.4 and 5.4.1 (<a href="https://github.com/scrapy/scrapy/issues/4588">issue
4588</a>{.reference
.external})</p>
</li>
<li>
<p>Improved <a href="index.html#scrapy.spiderloader.SpiderLoader" title="scrapy.spiderloader.SpiderLoader">[<code>SpiderLoader</code>{.xref .py .py-class .docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal} test coverage for scenarios involving duplicate spider
names (<a href="https://github.com/scrapy/scrapy/issues/4549">issue
4549</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/4560">issue
4560</a>{.reference
.external})</p>
</li>
<li>
<p>Configured Travis CI to also run the tests with Python 3.5.2 (<a href="https://github.com/scrapy/scrapy/issues/4518">issue
4518</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/4615">issue
4615</a>{.reference
.external})</p>
</li>
<li>
<p>Added a <a href="https://www.pylint.org/">Pylint</a>{.reference .external} job
to Travis CI (<a href="https://github.com/scrapy/scrapy/issues/3727">issue
3727</a>{.reference
.external})</p>
</li>
<li>
<p>Added a <a href="http://mypy-lang.org/">Mypy</a>{.reference .external} job to
Travis CI (<a href="https://github.com/scrapy/scrapy/issues/4637">issue
4637</a>{.reference
.external})</p>
</li>
<li>
<p>Made use of set literals in tests (<a href="https://github.com/scrapy/scrapy/issues/4573">issue
4573</a>{.reference
.external})</p>
</li>
<li>
<p>Cleaned up the Travis CI configuration (<a href="https://github.com/scrapy/scrapy/issues/4517">issue
4517</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/4519">issue
4519</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/4522">issue
4522</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/4537">issue
4537</a>{.reference
.external})
:::
:::</p>
</li>
</ul>
<p>::: {#scrapy-2-1-0-2020-04-24 .section}
[]{#release-2-1-0}</p>
<h4 id="scrapy-210-2020-04-24headerlink"><a class="header" href="#scrapy-210-2020-04-24headerlink">Scrapy 2.1.0 (2020-04-24)<a href="#scrapy-2-1-0-2020-04-24" title="Permalink to this heading">¶</a>{.headerlink}</a></h4>
<p>Highlights:</p>
<ul>
<li>
<p>New <a href="index.html#std-setting-FEEDS">[<code>FEEDS</code>{.xref .std .std-setting .docutils .literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal} setting to export to multiple feeds</p>
</li>
<li>
<p>New <a href="index.html#scrapy.http.Response.ip_address" title="scrapy.http.Response.ip_address">[<code>Response.ip_address</code>{.xref .py .py-attr .docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal} attribute</p>
</li>
</ul>
<p>::: {#id64 .section}</p>
<h5 id="backward-incompatible-changesheaderlink-5"><a class="header" href="#backward-incompatible-changesheaderlink-5">Backward-incompatible changes<a href="#id64" title="Permalink to this heading">¶</a>{.headerlink}</a></h5>
<ul>
<li>
<p><a href="https://docs.python.org/3/library/exceptions.html#AssertionError" title="(in Python v3.12)">[<code>AssertionError</code>{.xref .py .py-exc .docutils .literal
.notranslate}]{.pre}</a>{.reference
.external} exceptions triggered by <a href="https://docs.python.org/3/reference/simple_stmts.html#assert" title="(in Python v3.12)">[assert]{.xref .std
.std-ref}</a>{.reference
.external} statements have been replaced by new exception types, to
support running Python in optimized mode (see <a href="https://docs.python.org/3/using/cmdline.html#cmdoption-O" title="(in Python v3.12)">[<code>-O</code>{.xref .std
.std-option .docutils .literal
.notranslate}]{.pre}</a>{.reference
.external}) without changing Scrapy's behavior in any unexpected
ways.</p>
<p>If you catch an <a href="https://docs.python.org/3/library/exceptions.html#AssertionError" title="(in Python v3.12)">[<code>AssertionError</code>{.xref .py .py-exc .docutils
.literal
.notranslate}]{.pre}</a>{.reference
.external} exception from Scrapy, update your code to catch the
corresponding new exception.</p>
<p>(<a href="https://github.com/scrapy/scrapy/issues/4440">issue
4440</a>{.reference
.external})
:::</p>
</li>
</ul>
<p>::: {#id65 .section}</p>
<h5 id="deprecation-removalsheaderlink-7"><a class="header" href="#deprecation-removalsheaderlink-7">Deprecation removals<a href="#id65" title="Permalink to this heading">¶</a>{.headerlink}</a></h5>
<ul>
<li>
<p>The [<code>LOG_UNSERIALIZABLE_REQUESTS</code>{.docutils .literal
.notranslate}]{.pre} setting is no longer supported, use
<a href="index.html#std-setting-SCHEDULER_DEBUG">[<code>SCHEDULER_DEBUG</code>{.xref .std .std-setting .docutils .literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal} instead (<a href="https://github.com/scrapy/scrapy/issues/4385">issue
4385</a>{.reference
.external})</p>
</li>
<li>
<p>The [<code>REDIRECT_MAX_METAREFRESH_DELAY</code>{.docutils .literal
.notranslate}]{.pre} setting is no longer supported, use
<a href="index.html#std-setting-METAREFRESH_MAXDELAY">[<code>METAREFRESH_MAXDELAY</code>{.xref .std .std-setting .docutils .literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal} instead (<a href="https://github.com/scrapy/scrapy/issues/4385">issue
4385</a>{.reference
.external})</p>
</li>
<li>
<p>The [<code>ChunkedTransferMiddleware</code>{.xref .py .py-class .docutils
.literal .notranslate}]{.pre} middleware has been removed, including
the entire [<code>scrapy.downloadermiddlewares.chunked</code>{.xref .py
.py-class .docutils .literal .notranslate}]{.pre} module; chunked
transfers work out of the box (<a href="https://github.com/scrapy/scrapy/issues/4431">issue
4431</a>{.reference
.external})</p>
</li>
<li>
<p>The [<code>spiders</code>{.docutils .literal .notranslate}]{.pre} property has
been removed from <a href="index.html#scrapy.crawler.Crawler" title="scrapy.crawler.Crawler">[<code>Crawler</code>{.xref .py .py-class .docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal}, use [<code>CrawlerRunner.spider_loader</code>{.xref .py .py-class
.docutils .literal .notranslate}]{.pre} or instantiate
<a href="index.html#std-setting-SPIDER_LOADER_CLASS">[<code>SPIDER_LOADER_CLASS</code>{.xref .std .std-setting .docutils .literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal} with your settings instead (<a href="https://github.com/scrapy/scrapy/issues/4398">issue
4398</a>{.reference
.external})</p>
</li>
<li>
<p>The [<code>MultiValueDict</code>{.docutils .literal .notranslate}]{.pre},
[<code>MultiValueDictKeyError</code>{.docutils .literal .notranslate}]{.pre},
and [<code>SiteNode</code>{.docutils .literal .notranslate}]{.pre} classes have
been removed from [<code>scrapy.utils.datatypes</code>{.xref .py .py-mod
.docutils .literal .notranslate}]{.pre} (<a href="https://github.com/scrapy/scrapy/issues/4400">issue
4400</a>{.reference
.external})
:::</p>
</li>
</ul>
<p>::: {#id66 .section}</p>
<h5 id="deprecationsheaderlink-10"><a class="header" href="#deprecationsheaderlink-10">Deprecations<a href="#id66" title="Permalink to this heading">¶</a>{.headerlink}</a></h5>
<ul>
<li>The [<code>FEED_FORMAT</code>{.docutils .literal .notranslate}]{.pre} and
[<code>FEED_URI</code>{.docutils .literal .notranslate}]{.pre} settings have
been deprecated in favor of the new <a href="index.html#std-setting-FEEDS">[<code>FEEDS</code>{.xref .std
.std-setting .docutils .literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal} setting (<a href="https://github.com/scrapy/scrapy/issues/1336">issue
1336</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/3858">issue
3858</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/4507">issue
4507</a>{.reference
.external})
:::</li>
</ul>
<p>::: {#id67 .section}</p>
<h5 id="new-featuresheaderlink-11"><a class="header" href="#new-featuresheaderlink-11">New features<a href="#id67" title="Permalink to this heading">¶</a>{.headerlink}</a></h5>
<ul>
<li>
<p>A new setting, <a href="index.html#std-setting-FEEDS">[<code>FEEDS</code>{.xref .std .std-setting .docutils .literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal}, allows configuring multiple output
feeds with different settings each (<a href="https://github.com/scrapy/scrapy/issues/1336">issue
1336</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/3858">issue
3858</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/4507">issue
4507</a>{.reference
.external})</p>
</li>
<li>
<p>The <a href="index.html#std-command-crawl">[<code>crawl</code>{.xref .std .std-command .docutils .literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal} and <a href="index.html#std-command-runspider">[<code>runspider</code>{.xref .std
.std-command .docutils .literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal} commands now support multiple
[<code>-o</code>{.docutils .literal .notranslate}]{.pre} parameters (<a href="https://github.com/scrapy/scrapy/issues/1336">issue
1336</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/3858">issue
3858</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/4507">issue
4507</a>{.reference
.external})</p>
</li>
<li>
<p>The <a href="index.html#std-command-crawl">[<code>crawl</code>{.xref .std .std-command .docutils .literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal} and <a href="index.html#std-command-runspider">[<code>runspider</code>{.xref .std
.std-command .docutils .literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal} commands now support specifying an
output format by appending [<code>:&lt;format&gt;</code>{.docutils .literal
.notranslate}]{.pre} to the output file (<a href="https://github.com/scrapy/scrapy/issues/1336">issue
1336</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/3858">issue
3858</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/4507">issue
4507</a>{.reference
.external})</p>
</li>
<li>
<p>The new <a href="index.html#scrapy.http.Response.ip_address" title="scrapy.http.Response.ip_address">[<code>Response.ip_address</code>{.xref .py .py-attr .docutils
.literal
.notranslate}]{.pre}</a>{.reference
.internal} attribute gives access to the IP address that originated
a response (<a href="https://github.com/scrapy/scrapy/issues/3903">issue
3903</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/3940">issue
3940</a>{.reference
.external})</p>
</li>
<li>
<p>A warning is now issued when a value in [<code>allowed_domains</code>{.xref .py
.py-attr .docutils .literal .notranslate}]{.pre} includes a port
(<a href="https://github.com/scrapy/scrapy/issues/50">issue 50</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/3198">issue
3198</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/4413">issue
4413</a>{.reference
.external})</p>
</li>
<li>
<p>Zsh completion now excludes used option aliases from the completion
list (<a href="https://github.com/scrapy/scrapy/issues/4438">issue
4438</a>{.reference
.external})
:::</p>
</li>
</ul>
<p>::: {#id68 .section}</p>
<h5 id="bug-fixesheaderlink-11"><a class="header" href="#bug-fixesheaderlink-11">Bug fixes<a href="#id68" title="Permalink to this heading">¶</a>{.headerlink}</a></h5>
<ul>
<li>
<p><a href="index.html#request-serialization">[Request serialization]{.std
.std-ref}</a>{.hoverxref .tooltip
.reference .internal} no longer breaks for callbacks that are spider
attributes which are assigned a function with a different name
(<a href="https://github.com/scrapy/scrapy/issues/4500">issue
4500</a>{.reference
.external})</p>
</li>
<li>
<p>[<code>None</code>{.docutils .literal .notranslate}]{.pre} values in
[<code>allowed_domains</code>{.xref .py .py-attr .docutils .literal
.notranslate}]{.pre} no longer cause a <a href="https://docs.python.org/3/library/exceptions.html#TypeError" title="(in Python v3.12)">[<code>TypeError</code>{.xref .py
.py-exc .docutils .literal
.notranslate}]{.pre}</a>{.reference
.external} exception (<a href="https://github.com/scrapy/scrapy/issues/4410">issue
4410</a>{.reference
.external})</p>
</li>
<li>
<p>Zsh completion no longer allows options after arguments (<a href="https://github.com/scrapy/scrapy/issues/4438">issue
4438</a>{.reference
.external})</p>
</li>
<li>
<p>zope.interface 5.0.0 and later versions are now supported (<a href="https://github.com/scrapy/scrapy/issues/4447">issue
4447</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/4448">issue
4448</a>{.reference
.external})</p>
</li>
<li>
<p>[<code>Spider.make_requests_from_url</code>{.docutils .literal
.notranslate}]{.pre}, deprecated in Scrapy 1.4.0, now issues a
warning when used (<a href="https://github.com/scrapy/scrapy/issues/4412">issue
4412</a>{.reference
.external})
:::</p>
</li>
</ul>
<p>::: {#id69 .section}</p>
<h5 id="documentationheaderlink-11"><a class="header" href="#documentationheaderlink-11">Documentation<a href="#id69" title="Permalink to this heading">¶</a>{.headerlink}</a></h5>
<ul>
<li>
<p>Improved the documentation about signals that allow their handlers
to return a <a href="https://docs.twisted.org/en/stable/api/twisted.internet.defer.Deferred.html" title="(in Twisted)">[<code>Deferred</code>{.xref .py .py-class .docutils .literal
.notranslate}]{.pre}</a>{.reference
.external} (<a href="https://github.com/scrapy/scrapy/issues/4295">issue
4295</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/4390">issue
4390</a>{.reference
.external})</p>
</li>
<li>
<p>Our PyPI entry now includes links for our documentation, our source
code repository and our issue tracker (<a href="https://github.com/scrapy/scrapy/issues/4456">issue
4456</a>{.reference
.external})</p>
</li>
<li>
<p>Covered the
<a href="https://michael-shub.github.io/curl2scrapy/">curl2scrapy</a>{.reference
.external} service in the documentation (<a href="https://github.com/scrapy/scrapy/issues/4206">issue
4206</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/4455">issue
4455</a>{.reference
.external})</p>
</li>
<li>
<p>Removed references to the Guppy library, which only works in Python
2 (<a href="https://github.com/scrapy/scrapy/issues/4285">issue
4285</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/4343">issue
4343</a>{.reference
.external})</p>
</li>
<li>
<p>Extended use of InterSphinx to link to Python 3 documentation
(<a href="https://github.com/scrapy/scrapy/issues/4444">issue
4444</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/4445">issue
4445</a>{.reference
.external})</p>
</li>
<li>
<p>Added support for Sphinx 3.0 and later (<a href="https://github.com/scrapy/scrapy/issues/4475">issue
4475</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/4480">issue
4480</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/4496">issue
4496</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/4503">issue
4503</a>{.reference
.external})
:::</p>
</li>
</ul>
<p>::: {#id70 .section}</p>
<h5 id="quality-assuranceheaderlink-11"><a class="header" href="#quality-assuranceheaderlink-11">Quality assurance<a href="#id70" title="Permalink to this heading">¶</a>{.headerlink}</a></h5>
<ul>
<li>
<p>Removed warnings about using old, removed settings (<a href="https://github.com/scrapy/scrapy/issues/4404">issue
4404</a>{.reference
.external})</p>
</li>
<li>
<p>Removed a warning about importing <a href="https://docs.twisted.org/en/stable/api/twisted.internet.testing.StringTransport.html" title="(in Twisted)">[<code>StringTransport</code>{.xref .py
.py-class .docutils .literal
.notranslate}]{.pre}</a>{.reference
.external} from [<code>twisted.test.proto_helpers</code>{.docutils .literal
.notranslate}]{.pre} in Twisted 19.7.0 or newer (<a href="https://github.com/scrapy/scrapy/issues/4409">issue
4409</a>{.reference
.external})</p>
</li>
<li>
<p>Removed outdated Debian package build files (<a href="https://github.com/scrapy/scrapy/issues/4384">issue
4384</a>{.reference
.external})</p>
</li>
<li>
<p>Removed <a href="https://docs.python.org/3/library/functions.html#object" title="(in Python v3.12)">[<code>object</code>{.xref .py .py-class .docutils .literal
.notranslate}]{.pre}</a>{.reference
.external} usage as a base class (<a href="https://github.com/scrapy/scrapy/issues/4430">issue
4430</a>{.reference
.external})</p>
</li>
<li>
<p>Removed code that added support for old versions of Twisted that we
no longer support (<a href="https://github.com/scrapy/scrapy/issues/4472">issue
4472</a>{.reference
.external})</p>
</li>
<li>
<p>Fixed code style issues (<a href="https://github.com/scrapy/scrapy/issues/4468">issue
4468</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/4469">issue
4469</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/4471">issue
4471</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/4481">issue
4481</a>{.reference
.external})</p>
</li>
<li>
<p>Removed <a href="https://docs.twisted.org/en/stable/api/twisted.internet.defer.html#returnValue" title="(in Twisted)">[<code>twisted.internet.defer.returnValue()</code>{.xref .py .py-func
.docutils .literal
.notranslate}]{.pre}</a>{.reference
.external} calls (<a href="https://github.com/scrapy/scrapy/issues/4443">issue
4443</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/4446">issue
4446</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/4489">issue
4489</a>{.reference
.external})
:::
:::</p>
</li>
</ul>
<p>::: {#scrapy-2-0-1-2020-03-18 .section}
[]{#release-2-0-1}</p>
<h4 id="scrapy-201-2020-03-18headerlink"><a class="header" href="#scrapy-201-2020-03-18headerlink">Scrapy 2.0.1 (2020-03-18)<a href="#scrapy-2-0-1-2020-03-18" title="Permalink to this heading">¶</a>{.headerlink}</a></h4>
<ul>
<li>
<p><a href="index.html#scrapy.http.Response.follow_all" title="scrapy.http.Response.follow_all">[<code>Response.follow_all</code>{.xref .py .py-meth .docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal} now supports an empty URL iterable as input (<a href="https://github.com/scrapy/scrapy/issues/4408">issue
4408</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/4420">issue
4420</a>{.reference
.external})</p>
</li>
<li>
<p>Removed top-level <a href="https://docs.twisted.org/en/stable/api/twisted.internet.reactor.html" title="(in Twisted)">[<code>reactor</code>{.xref .py .py-mod .docutils .literal
.notranslate}]{.pre}</a>{.reference
.external} imports to prevent errors about the wrong Twisted reactor
being installed when setting a different Twisted reactor using
<a href="index.html#std-setting-TWISTED_REACTOR">[<code>TWISTED_REACTOR</code>{.xref .std .std-setting .docutils .literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal} (<a href="https://github.com/scrapy/scrapy/issues/4401">issue
4401</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/4406">issue
4406</a>{.reference
.external})</p>
</li>
<li>
<p>Fixed tests (<a href="https://github.com/scrapy/scrapy/issues/4422">issue
4422</a>{.reference
.external})
:::</p>
</li>
</ul>
<p>::: {#scrapy-2-0-0-2020-03-03 .section}
[]{#release-2-0-0}</p>
<h4 id="scrapy-200-2020-03-03headerlink"><a class="header" href="#scrapy-200-2020-03-03headerlink">Scrapy 2.0.0 (2020-03-03)<a href="#scrapy-2-0-0-2020-03-03" title="Permalink to this heading">¶</a>{.headerlink}</a></h4>
<p>Highlights:</p>
<ul>
<li>
<p>Python 2 support has been removed</p>
</li>
<li>
<p><a href="index.html#document-topics/coroutines">[Partial]{.doc}</a>{.reference
.internal} <a href="https://docs.python.org/3/reference/compound_stmts.html#async" title="(in Python v3.12)">[coroutine syntax]{.xref .std
.std-ref}</a>{.reference
.external} support and
<a href="index.html#document-topics/asyncio">[experimental]{.doc}</a>{.reference
.internal} <a href="https://docs.python.org/3/library/asyncio.html#module-asyncio" title="(in Python v3.12)">[<code>asyncio</code>{.xref .py .py-mod .docutils .literal
.notranslate}]{.pre}</a>{.reference
.external} support</p>
</li>
<li>
<p>New <a href="index.html#scrapy.http.Response.follow_all" title="scrapy.http.Response.follow_all">[<code>Response.follow_all</code>{.xref .py .py-meth .docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal} method</p>
</li>
<li>
<p><a href="index.html#media-pipeline-ftp">[FTP support]{.std
.std-ref}</a>{.hoverxref .tooltip
.reference .internal} for media pipelines</p>
</li>
<li>
<p>New <a href="index.html#scrapy.http.Response.certificate" title="scrapy.http.Response.certificate">[<code>Response.certificate</code>{.xref .py .py-attr .docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal} attribute</p>
</li>
<li>
<p>IPv6 support through <a href="index.html#std-setting-DNS_RESOLVER">[<code>DNS_RESOLVER</code>{.xref .std .std-setting
.docutils .literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal}</p>
</li>
</ul>
<p>::: {#id71 .section}</p>
<h5 id="backward-incompatible-changesheaderlink-6"><a class="header" href="#backward-incompatible-changesheaderlink-6">Backward-incompatible changes<a href="#id71" title="Permalink to this heading">¶</a>{.headerlink}</a></h5>
<ul>
<li>
<p>Python 2 support has been removed, following <a href="https://www.python.org/doc/sunset-python-2/">Python 2 end-of-life
on January 1,
2020</a>{.reference
.external} (<a href="https://github.com/scrapy/scrapy/issues/4091">issue
4091</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/4114">issue
4114</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/4115">issue
4115</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/4121">issue
4121</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/4138">issue
4138</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/4231">issue
4231</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/4242">issue
4242</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/4304">issue
4304</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/4309">issue
4309</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/4373">issue
4373</a>{.reference
.external})</p>
</li>
<li>
<p>Retry gaveups (see <a href="index.html#std-setting-RETRY_TIMES">[<code>RETRY_TIMES</code>{.xref .std .std-setting .docutils
.literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal}) are now logged as errors instead of
as debug information (<a href="https://github.com/scrapy/scrapy/issues/3171">issue
3171</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/3566">issue
3566</a>{.reference
.external})</p>
</li>
<li>
<p>File extensions that <a href="index.html#scrapy.linkextractors.lxmlhtml.LxmlLinkExtractor" title="scrapy.linkextractors.lxmlhtml.LxmlLinkExtractor">[<code>LinkExtractor</code>{.xref .py .py-class .docutils
.literal
.notranslate}]{.pre}</a>{.reference
.internal} ignores by default now also include [<code>7z</code>{.docutils
.literal .notranslate}]{.pre}, [<code>7zip</code>{.docutils .literal
.notranslate}]{.pre}, [<code>apk</code>{.docutils .literal
.notranslate}]{.pre}, [<code>bz2</code>{.docutils .literal
.notranslate}]{.pre}, [<code>cdr</code>{.docutils .literal
.notranslate}]{.pre}, [<code>dmg</code>{.docutils .literal
.notranslate}]{.pre}, [<code>ico</code>{.docutils .literal
.notranslate}]{.pre}, [<code>iso</code>{.docutils .literal
.notranslate}]{.pre}, [<code>tar</code>{.docutils .literal
.notranslate}]{.pre}, [<code>tar.gz</code>{.docutils .literal
.notranslate}]{.pre}, [<code>webm</code>{.docutils .literal
.notranslate}]{.pre}, and [<code>xz</code>{.docutils .literal
.notranslate}]{.pre} (<a href="https://github.com/scrapy/scrapy/issues/1837">issue
1837</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/2067">issue
2067</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/4066">issue
4066</a>{.reference
.external})</p>
</li>
<li>
<p>The <a href="index.html#std-setting-METAREFRESH_IGNORE_TAGS">[<code>METAREFRESH_IGNORE_TAGS</code>{.xref .std .std-setting .docutils
.literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal} setting is now an empty list by
default, following web browser behavior (<a href="https://github.com/scrapy/scrapy/issues/3844">issue
3844</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/4311">issue
4311</a>{.reference
.external})</p>
</li>
<li>
<p>The <a href="index.html#scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware" title="scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware">[<code>HttpCompressionMiddleware</code>{.xref .py .py-class .docutils
.literal
.notranslate}]{.pre}</a>{.reference
.internal} now includes spaces after commas in the value of the
[<code>Accept-Encoding</code>{.docutils .literal .notranslate}]{.pre} header
that it sets, following web browser behavior (<a href="https://github.com/scrapy/scrapy/issues/4293">issue
4293</a>{.reference
.external})</p>
</li>
<li>
<p>The [<code>__init__</code>{.docutils .literal .notranslate}]{.pre} method of
custom download handlers (see <a href="index.html#std-setting-DOWNLOAD_HANDLERS">[<code>DOWNLOAD_HANDLERS</code>{.xref .std
.std-setting .docutils .literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal}) or subclasses of the following
downloader handlers no longer receives a [<code>settings</code>{.docutils
.literal .notranslate}]{.pre} parameter:</p>
<ul>
<li>
<p>[<code>scrapy.core.downloader.handlers.datauri.DataURIDownloadHandler</code>{.xref
.py .py-class .docutils .literal .notranslate}]{.pre}</p>
</li>
<li>
<p>[<code>scrapy.core.downloader.handlers.file.FileDownloadHandler</code>{.xref
.py .py-class .docutils .literal .notranslate}]{.pre}</p>
</li>
</ul>
<p>Use the [<code>from_settings</code>{.docutils .literal .notranslate}]{.pre} or
[<code>from_crawler</code>{.docutils .literal .notranslate}]{.pre} class
methods to expose such a parameter to your custom download handlers.</p>
<p>(<a href="https://github.com/scrapy/scrapy/issues/4126">issue
4126</a>{.reference
.external})</p>
</li>
<li>
<p>We have refactored the <a href="index.html#scrapy.core.scheduler.Scheduler" title="scrapy.core.scheduler.Scheduler">[<code>scrapy.core.scheduler.Scheduler</code>{.xref .py
.py-class .docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal} class and related queue classes (see
<a href="index.html#std-setting-SCHEDULER_PRIORITY_QUEUE">[<code>SCHEDULER_PRIORITY_QUEUE</code>{.xref .std .std-setting .docutils
.literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal}, <a href="index.html#std-setting-SCHEDULER_DISK_QUEUE">[<code>SCHEDULER_DISK_QUEUE</code>{.xref .std
.std-setting .docutils .literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal} and <a href="index.html#std-setting-SCHEDULER_MEMORY_QUEUE">[<code>SCHEDULER_MEMORY_QUEUE</code>{.xref
.std .std-setting .docutils .literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal}) to make it easier to implement
custom scheduler queue classes. See <a href="#scheduler-queue-changes">[Changes to scheduler queue
classes]{.std .std-ref}</a>{.hoverxref
.tooltip .reference .internal} below for details.</p>
</li>
<li>
<p>Overridden settings are now logged in a different format. This is
more in line with similar information logged at startup (<a href="https://github.com/scrapy/scrapy/issues/4199">issue
4199</a>{.reference
.external})
:::</p>
</li>
</ul>
<p>::: {#id72 .section}</p>
<h5 id="deprecation-removalsheaderlink-8"><a class="header" href="#deprecation-removalsheaderlink-8">Deprecation removals<a href="#id72" title="Permalink to this heading">¶</a>{.headerlink}</a></h5>
<ul>
<li>
<p>The <a href="index.html#topics-shell">[Scrapy shell]{.std
.std-ref}</a>{.hoverxref .tooltip .reference
.internal} no longer provides a sel proxy object, use
[<code>response.selector</code>{.xref .py .py-meth .docutils .literal
.notranslate}]{.pre} instead (<a href="https://github.com/scrapy/scrapy/issues/4347">issue
4347</a>{.reference
.external})</p>
</li>
<li>
<p>LevelDB support has been removed (<a href="https://github.com/scrapy/scrapy/issues/4112">issue
4112</a>{.reference
.external})</p>
</li>
<li>
<p>The following functions have been removed from
[<code>scrapy.utils.python</code>{.xref .py .py-mod .docutils .literal
.notranslate}]{.pre}: [<code>isbinarytext</code>{.docutils .literal
.notranslate}]{.pre}, [<code>is_writable</code>{.docutils .literal
.notranslate}]{.pre}, [<code>setattr_default</code>{.docutils .literal
.notranslate}]{.pre}, [<code>stringify_dict</code>{.docutils .literal
.notranslate}]{.pre} (<a href="https://github.com/scrapy/scrapy/issues/4362">issue
4362</a>{.reference
.external})
:::</p>
</li>
</ul>
<p>::: {#id73 .section}</p>
<h5 id="deprecationsheaderlink-11"><a class="header" href="#deprecationsheaderlink-11">Deprecations<a href="#id73" title="Permalink to this heading">¶</a>{.headerlink}</a></h5>
<ul>
<li>
<p>Using environment variables prefixed with [<code>SCRAPY_</code>{.docutils
.literal .notranslate}]{.pre} to override settings is deprecated
(<a href="https://github.com/scrapy/scrapy/issues/4300">issue
4300</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/4374">issue
4374</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/4375">issue
4375</a>{.reference
.external})</p>
</li>
<li>
<p>[<code>scrapy.linkextractors.FilteringLinkExtractor</code>{.xref .py .py-class
.docutils .literal .notranslate}]{.pre} is deprecated, use
<a href="index.html#scrapy.linkextractors.lxmlhtml.LxmlLinkExtractor" title="scrapy.linkextractors.lxmlhtml.LxmlLinkExtractor">[<code>scrapy.linkextractors.LinkExtractor</code>{.xref .py .py-class
.docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal} instead (<a href="https://github.com/scrapy/scrapy/issues/4045">issue
4045</a>{.reference
.external})</p>
</li>
<li>
<p>The [<code>noconnect</code>{.docutils .literal .notranslate}]{.pre} query
string argument of proxy URLs is deprecated and should be removed
from proxy URLs (<a href="https://github.com/scrapy/scrapy/issues/4198">issue
4198</a>{.reference
.external})</p>
</li>
<li>
<p>The [<code>next</code>{.xref .py .py-meth .docutils .literal
.notranslate}]{.pre} method of
[<code>scrapy.utils.python.MutableChain</code>{.xref .py .py-class .docutils
.literal .notranslate}]{.pre} is deprecated, use the global
<a href="https://docs.python.org/3/library/functions.html#next" title="(in Python v3.12)">[<code>next()</code>{.xref .py .py-func .docutils .literal
.notranslate}]{.pre}</a>{.reference
.external} function or [<code>MutableChain.__next__</code>{.xref .py .py-meth
.docutils .literal .notranslate}]{.pre} instead (<a href="https://github.com/scrapy/scrapy/issues/4153">issue
4153</a>{.reference
.external})
:::</p>
</li>
</ul>
<p>::: {#id74 .section}</p>
<h5 id="new-featuresheaderlink-12"><a class="header" href="#new-featuresheaderlink-12">New features<a href="#id74" title="Permalink to this heading">¶</a>{.headerlink}</a></h5>
<ul>
<li>
<p>Added <a href="index.html#document-topics/coroutines">[partial
support]{.doc}</a>{.reference
.internal} for Python's <a href="https://docs.python.org/3/reference/compound_stmts.html#async" title="(in Python v3.12)">[coroutine syntax]{.xref .std
.std-ref}</a>{.reference
.external} and <a href="index.html#document-topics/asyncio">[experimental
support]{.doc}</a>{.reference
.internal} for <a href="https://docs.python.org/3/library/asyncio.html#module-asyncio" title="(in Python v3.12)">[<code>asyncio</code>{.xref .py .py-mod .docutils .literal
.notranslate}]{.pre}</a>{.reference
.external} and <a href="https://docs.python.org/3/library/asyncio.html#module-asyncio" title="(in Python v3.12)">[<code>asyncio</code>{.xref .py .py-mod .docutils .literal
.notranslate}]{.pre}</a>{.reference
.external}-powered libraries (<a href="https://github.com/scrapy/scrapy/issues/4010">issue
4010</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/4259">issue
4259</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/4269">issue
4269</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/4270">issue
4270</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/4271">issue
4271</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/4316">issue
4316</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/4318">issue
4318</a>{.reference
.external})</p>
</li>
<li>
<p>The new <a href="index.html#scrapy.http.Response.follow_all" title="scrapy.http.Response.follow_all">[<code>Response.follow_all</code>{.xref .py .py-meth .docutils
.literal
.notranslate}]{.pre}</a>{.reference
.internal} method offers the same functionality as
<a href="index.html#scrapy.http.Response.follow" title="scrapy.http.Response.follow">[<code>Response.follow</code>{.xref .py .py-meth .docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal} but supports an iterable of URLs as input and returns an
iterable of requests (<a href="https://github.com/scrapy/scrapy/issues/2582">issue
2582</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/4057">issue
4057</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/4286">issue
4286</a>{.reference
.external})</p>
</li>
<li>
<p><a href="index.html#topics-media-pipeline">[Media pipelines]{.std
.std-ref}</a>{.hoverxref .tooltip
.reference .internal} now support <a href="index.html#media-pipeline-ftp">[FTP storage]{.std
.std-ref}</a>{.hoverxref .tooltip
.reference .internal} (<a href="https://github.com/scrapy/scrapy/issues/3928">issue
3928</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/3961">issue
3961</a>{.reference
.external})</p>
</li>
<li>
<p>The new <a href="index.html#scrapy.http.Response.certificate" title="scrapy.http.Response.certificate">[<code>Response.certificate</code>{.xref .py .py-attr .docutils
.literal
.notranslate}]{.pre}</a>{.reference
.internal} attribute exposes the SSL certificate of the server as a
<a href="https://docs.twisted.org/en/stable/api/twisted.internet.ssl.Certificate.html" title="(in Twisted)">[<code>twisted.internet.ssl.Certificate</code>{.xref .py .py-class .docutils
.literal
.notranslate}]{.pre}</a>{.reference
.external} object for HTTPS responses (<a href="https://github.com/scrapy/scrapy/issues/2726">issue
2726</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/4054">issue
4054</a>{.reference
.external})</p>
</li>
<li>
<p>A new <a href="index.html#std-setting-DNS_RESOLVER">[<code>DNS_RESOLVER</code>{.xref .std .std-setting .docutils .literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal} setting allows enabling IPv6 support
(<a href="https://github.com/scrapy/scrapy/issues/1031">issue
1031</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/4227">issue
4227</a>{.reference
.external})</p>
</li>
<li>
<p>A new <a href="index.html#std-setting-SCRAPER_SLOT_MAX_ACTIVE_SIZE">[<code>SCRAPER_SLOT_MAX_ACTIVE_SIZE</code>{.xref .std .std-setting
.docutils .literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal} setting allows configuring the
existing soft limit that pauses request downloads when the total
response data being processed is too high (<a href="https://github.com/scrapy/scrapy/issues/1410">issue
1410</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/3551">issue
3551</a>{.reference
.external})</p>
</li>
<li>
<p>A new <a href="index.html#std-setting-TWISTED_REACTOR">[<code>TWISTED_REACTOR</code>{.xref .std .std-setting .docutils .literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal} setting allows customizing the
<a href="https://docs.twisted.org/en/stable/api/twisted.internet.reactor.html" title="(in Twisted)">[<code>reactor</code>{.xref .py .py-mod .docutils .literal
.notranslate}]{.pre}</a>{.reference
.external} that Scrapy uses, allowing to <a href="index.html#document-topics/asyncio">[enable asyncio
support]{.doc}</a>{.reference
.internal} or deal with a <a href="index.html#faq-specific-reactor">[common macOS issue]{.std
.std-ref}</a>{.hoverxref .tooltip
.reference .internal} (<a href="https://github.com/scrapy/scrapy/issues/2905">issue
2905</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/4294">issue
4294</a>{.reference
.external})</p>
</li>
<li>
<p>Scheduler disk and memory queues may now use the class methods
[<code>from_crawler</code>{.docutils .literal .notranslate}]{.pre} or
[<code>from_settings</code>{.docutils .literal .notranslate}]{.pre} (<a href="https://github.com/scrapy/scrapy/issues/3884">issue
3884</a>{.reference
.external})</p>
</li>
<li>
<p>The new <a href="index.html#scrapy.http.Response.cb_kwargs" title="scrapy.http.Response.cb_kwargs">[<code>Response.cb_kwargs</code>{.xref .py .py-attr .docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal} attribute serves as a shortcut for
<a href="index.html#scrapy.http.Request.cb_kwargs" title="scrapy.http.Request.cb_kwargs">[<code>Response.request.cb_kwargs</code>{.xref .py .py-attr .docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal} (<a href="https://github.com/scrapy/scrapy/issues/4331">issue
4331</a>{.reference
.external})</p>
</li>
<li>
<p><a href="index.html#scrapy.http.Response.follow" title="scrapy.http.Response.follow">[<code>Response.follow</code>{.xref .py .py-meth .docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal} now supports a [<code>flags</code>{.docutils .literal
.notranslate}]{.pre} parameter, for consistency with
<a href="index.html#scrapy.http.Request" title="scrapy.http.Request">[<code>Request</code>{.xref .py .py-class .docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal} (<a href="https://github.com/scrapy/scrapy/issues/4277">issue
4277</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/4279">issue
4279</a>{.reference
.external})</p>
</li>
<li>
<p><a href="index.html#topics-loaders-processors">[Item loader processors]{.std
.std-ref}</a>{.hoverxref .tooltip
.reference .internal} can now be regular functions, they no longer
need to be methods (<a href="https://github.com/scrapy/scrapy/issues/3899">issue
3899</a>{.reference
.external})</p>
</li>
<li>
<p><a href="index.html#scrapy.spiders.Rule" title="scrapy.spiders.Rule">[<code>Rule</code>{.xref .py .py-class .docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal} now accepts an [<code>errback</code>{.docutils .literal
.notranslate}]{.pre} parameter (<a href="https://github.com/scrapy/scrapy/issues/4000">issue
4000</a>{.reference
.external})</p>
</li>
<li>
<p><a href="index.html#scrapy.http.Request" title="scrapy.http.Request">[<code>Request</code>{.xref .py .py-class .docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal} no longer requires a [<code>callback</code>{.docutils .literal
.notranslate}]{.pre} parameter when an [<code>errback</code>{.docutils .literal
.notranslate}]{.pre} parameter is specified (<a href="https://github.com/scrapy/scrapy/issues/3586">issue
3586</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/4008">issue
4008</a>{.reference
.external})</p>
</li>
<li>
<p><a href="index.html#scrapy.logformatter.LogFormatter" title="scrapy.logformatter.LogFormatter">[<code>LogFormatter</code>{.xref .py .py-class .docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal} now supports some additional methods:</p>
<ul>
<li>
<p><a href="index.html#scrapy.logformatter.LogFormatter.download_error" title="scrapy.logformatter.LogFormatter.download_error">[<code>download_error</code>{.xref .py .py-class .docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal} for download errors</p>
</li>
<li>
<p><a href="index.html#scrapy.logformatter.LogFormatter.item_error" title="scrapy.logformatter.LogFormatter.item_error">[<code>item_error</code>{.xref .py .py-class .docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal} for exceptions raised during item processing by
<a href="index.html#topics-item-pipeline">[item pipelines]{.std
.std-ref}</a>{.hoverxref .tooltip
.reference .internal}</p>
</li>
<li>
<p><a href="index.html#scrapy.logformatter.LogFormatter.spider_error" title="scrapy.logformatter.LogFormatter.spider_error">[<code>spider_error</code>{.xref .py .py-class .docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal} for exceptions raised from <a href="index.html#topics-spiders">[spider callbacks]{.std
.std-ref}</a>{.hoverxref .tooltip
.reference .internal}</p>
</li>
</ul>
<p>(<a href="https://github.com/scrapy/scrapy/issues/374">issue 374</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/3986">issue
3986</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/3989">issue
3989</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/4176">issue
4176</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/4188">issue
4188</a>{.reference
.external})</p>
</li>
<li>
<p>The [<code>FEED_URI</code>{.xref .std .std-setting .docutils .literal
.notranslate}]{.pre} setting now supports <a href="https://docs.python.org/3/library/pathlib.html#pathlib.Path" title="(in Python v3.12)">[<code>pathlib.Path</code>{.xref .py
.py-class .docutils .literal
.notranslate}]{.pre}</a>{.reference
.external} values (<a href="https://github.com/scrapy/scrapy/issues/3731">issue
3731</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/4074">issue
4074</a>{.reference
.external})</p>
</li>
<li>
<p>A new <a href="index.html#std-signal-request_left_downloader">[<code>request_left_downloader</code>{.xref .std .std-signal .docutils
.literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal} signal is sent when a request leaves
the downloader (<a href="https://github.com/scrapy/scrapy/issues/4303">issue
4303</a>{.reference
.external})</p>
</li>
<li>
<p>Scrapy logs a warning when it detects a request callback or errback
that uses [<code>yield</code>{.docutils .literal .notranslate}]{.pre} but also
returns a value, since the returned value would be lost (<a href="https://github.com/scrapy/scrapy/issues/3484">issue
3484</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/3869">issue
3869</a>{.reference
.external})</p>
</li>
<li>
<p><a href="index.html#scrapy.spiders.Spider" title="scrapy.spiders.Spider">[<code>Spider</code>{.xref .py .py-class .docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal} objects now raise an <a href="https://docs.python.org/3/library/exceptions.html#AttributeError" title="(in Python v3.12)">[<code>AttributeError</code>{.xref .py .py-exc
.docutils .literal
.notranslate}]{.pre}</a>{.reference
.external} exception if they do not have a [<code>start_urls</code>{.xref .py
.py-class .docutils .literal .notranslate}]{.pre} attribute nor
reimplement [<code>start_requests</code>{.xref .py .py-class .docutils .literal
.notranslate}]{.pre}, but have a [<code>start_url</code>{.docutils .literal
.notranslate}]{.pre} attribute (<a href="https://github.com/scrapy/scrapy/issues/4133">issue
4133</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/4170">issue
4170</a>{.reference
.external})</p>
</li>
<li>
<p><a href="index.html#scrapy.exporters.BaseItemExporter" title="scrapy.exporters.BaseItemExporter">[<code>BaseItemExporter</code>{.xref .py .py-class .docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal} subclasses may now use
[<code>super().__init__(**kwargs)</code>{.docutils .literal
.notranslate}]{.pre} instead of [<code>self._configure(kwargs)</code>{.docutils
.literal .notranslate}]{.pre} in their [<code>__init__</code>{.docutils
.literal .notranslate}]{.pre} method, passing
[<code>dont_fail=True</code>{.docutils .literal .notranslate}]{.pre} to the
parent [<code>__init__</code>{.docutils .literal .notranslate}]{.pre} method if
needed, and accessing [<code>kwargs</code>{.docutils .literal
.notranslate}]{.pre} at [<code>self._kwargs</code>{.docutils .literal
.notranslate}]{.pre} after calling their parent
[<code>__init__</code>{.docutils .literal .notranslate}]{.pre} method (<a href="https://github.com/scrapy/scrapy/issues/4193">issue
4193</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/4370">issue
4370</a>{.reference
.external})</p>
</li>
<li>
<p>A new [<code>keep_fragments</code>{.docutils .literal .notranslate}]{.pre}
parameter of [<code>scrapy.utils.request.request_fingerprint</code>{.docutils
.literal .notranslate}]{.pre} allows to generate different
fingerprints for requests with different fragments in their URL
(<a href="https://github.com/scrapy/scrapy/issues/4104">issue
4104</a>{.reference
.external})</p>
</li>
<li>
<p>Download handlers (see <a href="index.html#std-setting-DOWNLOAD_HANDLERS">[<code>DOWNLOAD_HANDLERS</code>{.xref .std .std-setting
.docutils .literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal}) may now use the
[<code>from_settings</code>{.docutils .literal .notranslate}]{.pre} and
[<code>from_crawler</code>{.docutils .literal .notranslate}]{.pre} class
methods that other Scrapy components already supported (<a href="https://github.com/scrapy/scrapy/issues/4126">issue
4126</a>{.reference
.external})</p>
</li>
<li>
<p>[<code>scrapy.utils.python.MutableChain.__iter__</code>{.xref .py .py-class
.docutils .literal .notranslate}]{.pre} now returns
[<code>self</code>{.docutils .literal .notranslate}]{.pre}, <a href="https://lgtm.com/rules/4850080/">allowing it to be
used as a sequence</a>{.reference
.external} (<a href="https://github.com/scrapy/scrapy/issues/4153">issue
4153</a>{.reference
.external})
:::</p>
</li>
</ul>
<p>::: {#id75 .section}</p>
<h5 id="bug-fixesheaderlink-12"><a class="header" href="#bug-fixesheaderlink-12">Bug fixes<a href="#id75" title="Permalink to this heading">¶</a>{.headerlink}</a></h5>
<ul>
<li>
<p>The <a href="index.html#std-command-crawl">[<code>crawl</code>{.xref .std .std-command .docutils .literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal} command now also exits with exit code
1 when an exception happens before the crawling starts (<a href="https://github.com/scrapy/scrapy/issues/4175">issue
4175</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/4207">issue
4207</a>{.reference
.external})</p>
</li>
<li>
<p><a href="index.html#scrapy.linkextractors.lxmlhtml.LxmlLinkExtractor.extract_links" title="scrapy.linkextractors.lxmlhtml.LxmlLinkExtractor.extract_links">[<code>LinkExtractor.extract_links</code>{.xref .py .py-class .docutils
.literal
.notranslate}]{.pre}</a>{.reference
.internal} no longer re-encodes the query string or URLs from
non-UTF-8 responses in UTF-8 (<a href="https://github.com/scrapy/scrapy/issues/998">issue
998</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/1403">issue
1403</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/1949">issue
1949</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/4321">issue
4321</a>{.reference
.external})</p>
</li>
<li>
<p>The first spider middleware (see <a href="index.html#std-setting-SPIDER_MIDDLEWARES">[<code>SPIDER_MIDDLEWARES</code>{.xref .std
.std-setting .docutils .literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal}) now also processes exceptions raised
from callbacks that are generators (<a href="https://github.com/scrapy/scrapy/issues/4260">issue
4260</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/4272">issue
4272</a>{.reference
.external})</p>
</li>
<li>
<p>Redirects to URLs starting with 3 slashes ([<code>///</code>{.docutils .literal
.notranslate}]{.pre}) are now supported (<a href="https://github.com/scrapy/scrapy/issues/4032">issue
4032</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/4042">issue
4042</a>{.reference
.external})</p>
</li>
<li>
<p><a href="index.html#scrapy.http.Request" title="scrapy.http.Request">[<code>Request</code>{.xref .py .py-class .docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal} no longer accepts strings as [<code>url</code>{.docutils .literal
.notranslate}]{.pre} simply because they have a colon (<a href="https://github.com/scrapy/scrapy/issues/2552">issue
2552</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/4094">issue
4094</a>{.reference
.external})</p>
</li>
<li>
<p>The correct encoding is now used for attach names in
<a href="index.html#scrapy.mail.MailSender" title="scrapy.mail.MailSender">[<code>MailSender</code>{.xref .py .py-class .docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal} (<a href="https://github.com/scrapy/scrapy/issues/4229">issue
4229</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/4239">issue
4239</a>{.reference
.external})</p>
</li>
<li>
<p>[<code>RFPDupeFilter</code>{.xref .py .py-class .docutils .literal
.notranslate}]{.pre}, the default <a href="index.html#std-setting-DUPEFILTER_CLASS">[<code>DUPEFILTER_CLASS</code>{.xref .std
.std-setting .docutils .literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal}, no longer writes an extra
[<code>\r</code>{.docutils .literal .notranslate}]{.pre} character on each line
in Windows, which made the size of the [<code>requests.seen</code>{.docutils
.literal .notranslate}]{.pre} file unnecessarily large on that
platform (<a href="https://github.com/scrapy/scrapy/issues/4283">issue
4283</a>{.reference
.external})</p>
</li>
<li>
<p>Z shell auto-completion now looks for [<code>.html</code>{.docutils .literal
.notranslate}]{.pre} files, not [<code>.http</code>{.docutils .literal
.notranslate}]{.pre} files, and covers the [<code>-h</code>{.docutils .literal
.notranslate}]{.pre} command-line switch (<a href="https://github.com/scrapy/scrapy/issues/4122">issue
4122</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/4291">issue
4291</a>{.reference
.external})</p>
</li>
<li>
<p>Adding items to a [<code>scrapy.utils.datatypes.LocalCache</code>{.xref .py
.py-class .docutils .literal .notranslate}]{.pre} object without a
[<code>limit</code>{.docutils .literal .notranslate}]{.pre} defined no longer
raises a <a href="https://docs.python.org/3/library/exceptions.html#TypeError" title="(in Python v3.12)">[<code>TypeError</code>{.xref .py .py-exc .docutils .literal
.notranslate}]{.pre}</a>{.reference
.external} exception (<a href="https://github.com/scrapy/scrapy/issues/4123">issue
4123</a>{.reference
.external})</p>
</li>
<li>
<p>Fixed a typo in the message of the <a href="https://docs.python.org/3/library/exceptions.html#ValueError" title="(in Python v3.12)">[<code>ValueError</code>{.xref .py .py-exc
.docutils .literal
.notranslate}]{.pre}</a>{.reference
.external} exception raised when
[<code>scrapy.utils.misc.create_instance()</code>{.xref .py .py-func .docutils
.literal .notranslate}]{.pre} gets both [<code>settings</code>{.docutils
.literal .notranslate}]{.pre} and [<code>crawler</code>{.docutils .literal
.notranslate}]{.pre} set to [<code>None</code>{.docutils .literal
.notranslate}]{.pre} (<a href="https://github.com/scrapy/scrapy/issues/4128">issue
4128</a>{.reference
.external})
:::</p>
</li>
</ul>
<p>::: {#id76 .section}</p>
<h5 id="documentationheaderlink-12"><a class="header" href="#documentationheaderlink-12">Documentation<a href="#id76" title="Permalink to this heading">¶</a>{.headerlink}</a></h5>
<ul>
<li>
<p>API documentation now links to an online, syntax-highlighted view of
the corresponding source code (<a href="https://github.com/scrapy/scrapy/issues/4148">issue
4148</a>{.reference
.external})</p>
</li>
<li>
<p>Links to unexisting documentation pages now allow access to the
sidebar (<a href="https://github.com/scrapy/scrapy/issues/4152">issue
4152</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/4169">issue
4169</a>{.reference
.external})</p>
</li>
<li>
<p>Cross-references within our documentation now display a tooltip when
hovered (<a href="https://github.com/scrapy/scrapy/issues/4173">issue
4173</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/4183">issue
4183</a>{.reference
.external})</p>
</li>
<li>
<p>Improved the documentation about
<a href="index.html#scrapy.linkextractors.lxmlhtml.LxmlLinkExtractor.extract_links" title="scrapy.linkextractors.lxmlhtml.LxmlLinkExtractor.extract_links">[<code>LinkExtractor.extract_links</code>{.xref .py .py-meth .docutils
.literal
.notranslate}]{.pre}</a>{.reference
.internal} and simplified <a href="index.html#topics-link-extractors">[Link Extractors]{.std
.std-ref}</a>{.hoverxref .tooltip
.reference .internal} (<a href="https://github.com/scrapy/scrapy/issues/4045">issue
4045</a>{.reference
.external})</p>
</li>
<li>
<p>Clarified how <a href="index.html#scrapy.loader.ItemLoader.item" title="scrapy.loader.ItemLoader.item">[<code>ItemLoader.item</code>{.xref .py .py-class .docutils
.literal
.notranslate}]{.pre}</a>{.reference
.internal} works (<a href="https://github.com/scrapy/scrapy/issues/3574">issue
3574</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/4099">issue
4099</a>{.reference
.external})</p>
</li>
<li>
<p>Clarified that <a href="https://docs.python.org/3/library/logging.html#logging.basicConfig" title="(in Python v3.12)">[<code>logging.basicConfig()</code>{.xref .py .py-func
.docutils .literal
.notranslate}]{.pre}</a>{.reference
.external} should not be used when also using
<a href="index.html#scrapy.crawler.CrawlerProcess" title="scrapy.crawler.CrawlerProcess">[<code>CrawlerProcess</code>{.xref .py .py-class .docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal} (<a href="https://github.com/scrapy/scrapy/issues/2149">issue
2149</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/2352">issue
2352</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/3146">issue
3146</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/3960">issue
3960</a>{.reference
.external})</p>
</li>
<li>
<p>Clarified the requirements for <a href="index.html#scrapy.http.Request" title="scrapy.http.Request">[<code>Request</code>{.xref .py .py-class
.docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal} objects <a href="index.html#request-serialization">[when using persistence]{.std
.std-ref}</a>{.hoverxref .tooltip
.reference .internal} (<a href="https://github.com/scrapy/scrapy/issues/4124">issue
4124</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/4139">issue
4139</a>{.reference
.external})</p>
</li>
<li>
<p>Clarified how to install a <a href="index.html#media-pipeline-example">[custom image pipeline]{.std
.std-ref}</a>{.hoverxref .tooltip
.reference .internal} (<a href="https://github.com/scrapy/scrapy/issues/4034">issue
4034</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/4252">issue
4252</a>{.reference
.external})</p>
</li>
<li>
<p>Fixed the signatures of the [<code>file_path</code>{.docutils .literal
.notranslate}]{.pre} method in <a href="index.html#topics-media-pipeline">[media pipeline]{.std
.std-ref}</a>{.hoverxref .tooltip
.reference .internal} examples (<a href="https://github.com/scrapy/scrapy/issues/4290">issue
4290</a>{.reference
.external})</p>
</li>
<li>
<p>Covered a backward-incompatible change in Scrapy 1.7.0 affecting
custom <a href="index.html#scrapy.core.scheduler.Scheduler" title="scrapy.core.scheduler.Scheduler">[<code>scrapy.core.scheduler.Scheduler</code>{.xref .py .py-class
.docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal} subclasses (<a href="https://github.com/scrapy/scrapy/issues/4274">issue
4274</a>{.reference
.external})</p>
</li>
<li>
<p>Improved the [<code>README.rst</code>{.docutils .literal .notranslate}]{.pre}
and [<code>CODE_OF_CONDUCT.md</code>{.docutils .literal .notranslate}]{.pre}
files (<a href="https://github.com/scrapy/scrapy/issues/4059">issue
4059</a>{.reference
.external})</p>
</li>
<li>
<p>Documentation examples are now checked as part of our test suite and
we have fixed some of the issues detected (<a href="https://github.com/scrapy/scrapy/issues/4142">issue
4142</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/4146">issue
4146</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/4171">issue
4171</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/4184">issue
4184</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/4190">issue
4190</a>{.reference
.external})</p>
</li>
<li>
<p>Fixed logic issues, broken links and typos (<a href="https://github.com/scrapy/scrapy/issues/4247">issue
4247</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/4258">issue
4258</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/4282">issue
4282</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/4288">issue
4288</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/4305">issue
4305</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/4308">issue
4308</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/4323">issue
4323</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/4338">issue
4338</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/4359">issue
4359</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/4361">issue
4361</a>{.reference
.external})</p>
</li>
<li>
<p>Improved consistency when referring to the [<code>__init__</code>{.docutils
.literal .notranslate}]{.pre} method of an object (<a href="https://github.com/scrapy/scrapy/issues/4086">issue
4086</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/4088">issue
4088</a>{.reference
.external})</p>
</li>
<li>
<p>Fixed an inconsistency between code and output in <a href="index.html#intro-overview">[Scrapy at a
glance]{.std .std-ref}</a>{.hoverxref
.tooltip .reference .internal} (<a href="https://github.com/scrapy/scrapy/issues/4213">issue
4213</a>{.reference
.external})</p>
</li>
<li>
<p>Extended <a href="https://www.sphinx-doc.org/en/master/usage/extensions/intersphinx.html#module-sphinx.ext.intersphinx" title="(in Sphinx v7.3.0)">[<code>intersphinx</code>{.xref .py .py-mod .docutils .literal
.notranslate}]{.pre}</a>{.reference
.external} usage (<a href="https://github.com/scrapy/scrapy/issues/4147">issue
4147</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/4172">issue
4172</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/4185">issue
4185</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/4194">issue
4194</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/4197">issue
4197</a>{.reference
.external})</p>
</li>
<li>
<p>We now use a recent version of Python to build the documentation
(<a href="https://github.com/scrapy/scrapy/issues/4140">issue
4140</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/4249">issue
4249</a>{.reference
.external})</p>
</li>
<li>
<p>Cleaned up documentation (<a href="https://github.com/scrapy/scrapy/issues/4143">issue
4143</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/4275">issue
4275</a>{.reference
.external})
:::</p>
</li>
</ul>
<p>::: {#id77 .section}</p>
<h5 id="quality-assuranceheaderlink-12"><a class="header" href="#quality-assuranceheaderlink-12">Quality assurance<a href="#id77" title="Permalink to this heading">¶</a>{.headerlink}</a></h5>
<ul>
<li>
<p>Re-enabled proxy [<code>CONNECT</code>{.docutils .literal .notranslate}]{.pre}
tests (<a href="https://github.com/scrapy/scrapy/issues/2545">issue
2545</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/4114">issue
4114</a>{.reference
.external})</p>
</li>
<li>
<p>Added <a href="https://bandit.readthedocs.io/">Bandit</a>{.reference .external}
security checks to our test suite (<a href="https://github.com/scrapy/scrapy/issues/4162">issue
4162</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/4181">issue
4181</a>{.reference
.external})</p>
</li>
<li>
<p>Added <a href="https://flake8.pycqa.org/en/latest/">Flake8</a>{.reference
.external} style checks to our test suite and applied many of the
corresponding changes (<a href="https://github.com/scrapy/scrapy/issues/3944">issue
3944</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/3945">issue
3945</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/4137">issue
4137</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/4157">issue
4157</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/4167">issue
4167</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/4174">issue
4174</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/4186">issue
4186</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/4195">issue
4195</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/4238">issue
4238</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/4246">issue
4246</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/4355">issue
4355</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/4360">issue
4360</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/4365">issue
4365</a>{.reference
.external})</p>
</li>
<li>
<p>Improved test coverage (<a href="https://github.com/scrapy/scrapy/issues/4097">issue
4097</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/4218">issue
4218</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/4236">issue
4236</a>{.reference
.external})</p>
</li>
<li>
<p>Started reporting slowest tests, and improved the performance of
some of them (<a href="https://github.com/scrapy/scrapy/issues/4163">issue
4163</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/4164">issue
4164</a>{.reference
.external})</p>
</li>
<li>
<p>Fixed broken tests and refactored some tests (<a href="https://github.com/scrapy/scrapy/issues/4014">issue
4014</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/4095">issue
4095</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/4244">issue
4244</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/4268">issue
4268</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/4372">issue
4372</a>{.reference
.external})</p>
</li>
<li>
<p>Modified the <a href="https://tox.wiki/en/latest/index.html" title="(in Python v4.11)">[tox]{.xref .std
.std-doc}</a>{.reference
.external} configuration to allow running tests with any Python
version, run <a href="https://bandit.readthedocs.io/">Bandit</a>{.reference
.external} and
<a href="https://flake8.pycqa.org/en/latest/">Flake8</a>{.reference .external}
tests by default, and enforce a minimum tox version programmatically
(<a href="https://github.com/scrapy/scrapy/issues/4179">issue
4179</a>{.reference
.external})</p>
</li>
<li>
<p>Cleaned up code (<a href="https://github.com/scrapy/scrapy/issues/3937">issue
3937</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/4208">issue
4208</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/4209">issue
4209</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/4210">issue
4210</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/4212">issue
4212</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/4369">issue
4369</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/4376">issue
4376</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/4378">issue
4378</a>{.reference
.external})
:::</p>
</li>
</ul>
<p>::: {#changes-to-scheduler-queue-classes .section}
[]{#scheduler-queue-changes}</p>
<h5 id="changes-to-scheduler-queue-classesheaderlink"><a class="header" href="#changes-to-scheduler-queue-classesheaderlink">Changes to scheduler queue classes<a href="#changes-to-scheduler-queue-classes" title="Permalink to this heading">¶</a>{.headerlink}</a></h5>
<p>The following changes may impact any custom queue classes of all types:</p>
<ul>
<li>The [<code>push</code>{.docutils .literal .notranslate}]{.pre} method no longer
receives a second positional parameter containing
[<code>request.priority</code>{.docutils .literal
.notranslate}]{.pre}<code> </code>{.docutils .literal
.notranslate}[<code>*</code>{.docutils .literal
.notranslate}]{.pre}<code> </code>{.docutils .literal
.notranslate}[<code>-1</code>{.docutils .literal .notranslate}]{.pre}. If you
need that value, get it from the first positional parameter,
[<code>request</code>{.docutils .literal .notranslate}]{.pre}, instead, or use
the new [<code>priority()</code>{.xref .py .py-meth .docutils .literal
.notranslate}]{.pre} method in
[<code>scrapy.core.scheduler.ScrapyPriorityQueue</code>{.xref .py .py-class
.docutils .literal .notranslate}]{.pre} subclasses.</li>
</ul>
<p>The following changes may impact custom priority queue classes:</p>
<ul>
<li>
<p>In the [<code>__init__</code>{.docutils .literal .notranslate}]{.pre} method or
the [<code>from_crawler</code>{.docutils .literal .notranslate}]{.pre} or
[<code>from_settings</code>{.docutils .literal .notranslate}]{.pre} class
methods:</p>
<ul>
<li>
<p>The parameter that used to contain a factory function,
[<code>qfactory</code>{.docutils .literal .notranslate}]{.pre}, is now
passed as a keyword parameter named
[<code>downstream_queue_cls</code>{.docutils .literal .notranslate}]{.pre}.</p>
</li>
<li>
<p>A new keyword parameter has been added: [<code>key</code>{.docutils
.literal .notranslate}]{.pre}. It is a string that is always an
empty string for memory queues and indicates the
[<code>JOB_DIR</code>{.xref .std .std-setting .docutils .literal
.notranslate}]{.pre} value for disk queues.</p>
</li>
<li>
<p>The parameter for disk queues that contains data from the
previous crawl, [<code>startprios</code>{.docutils .literal
.notranslate}]{.pre} or [<code>slot_startprios</code>{.docutils .literal
.notranslate}]{.pre}, is now passed as a keyword parameter named
[<code>startprios</code>{.docutils .literal .notranslate}]{.pre}.</p>
</li>
<li>
<p>The [<code>serialize</code>{.docutils .literal .notranslate}]{.pre}
parameter is no longer passed. The disk queue class must take
care of request serialization on its own before writing to disk,
using the [<code>request_to_dict()</code>{.xref .py .py-func .docutils
.literal .notranslate}]{.pre} and [<code>request_from_dict()</code>{.xref
.py .py-func .docutils .literal .notranslate}]{.pre} functions
from the [<code>scrapy.utils.reqser</code>{.xref .py .py-mod .docutils
.literal .notranslate}]{.pre} module.</p>
</li>
</ul>
</li>
</ul>
<p>The following changes may impact custom disk and memory queue classes:</p>
<ul>
<li>The signature of the [<code>__init__</code>{.docutils .literal
.notranslate}]{.pre} method is now [<code>__init__(self,</code>{.docutils
.literal .notranslate}]{.pre}<code> </code>{.docutils .literal
.notranslate}[<code>crawler,</code>{.docutils .literal
.notranslate}]{.pre}<code> </code>{.docutils .literal
.notranslate}[<code>key)</code>{.docutils .literal .notranslate}]{.pre}.</li>
</ul>
<p>The following changes affect specifically the
[<code>ScrapyPriorityQueue</code>{.xref .py .py-class .docutils .literal
.notranslate}]{.pre} and [<code>DownloaderAwarePriorityQueue</code>{.xref .py
.py-class .docutils .literal .notranslate}]{.pre} classes from
<a href="index.html#module-scrapy.core.scheduler" title="scrapy.core.scheduler">[<code>scrapy.core.scheduler</code>{.xref .py .py-mod .docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal} and may affect subclasses:</p>
<ul>
<li>
<p>In the [<code>__init__</code>{.docutils .literal .notranslate}]{.pre} method,
most of the changes described above apply.</p>
<p>[<code>__init__</code>{.docutils .literal .notranslate}]{.pre} may still
receive all parameters as positional parameters, however:</p>
<ul>
<li>
<p>[<code>downstream_queue_cls</code>{.docutils .literal .notranslate}]{.pre},
which replaced [<code>qfactory</code>{.docutils .literal
.notranslate}]{.pre}, must be instantiated differently.</p>
<p>[<code>qfactory</code>{.docutils .literal .notranslate}]{.pre} was
instantiated with a priority value (integer).</p>
<p>Instances of [<code>downstream_queue_cls</code>{.docutils .literal
.notranslate}]{.pre} should be created using the new
[<code>ScrapyPriorityQueue.qfactory</code>{.xref .py .py-meth .docutils
.literal .notranslate}]{.pre} or
[<code>DownloaderAwarePriorityQueue.pqfactory</code>{.xref .py .py-meth
.docutils .literal .notranslate}]{.pre} methods.</p>
</li>
<li>
<p>The new [<code>key</code>{.docutils .literal .notranslate}]{.pre} parameter
displaced the [<code>startprios</code>{.docutils .literal
.notranslate}]{.pre} parameter 1 position to the right.</p>
</li>
</ul>
</li>
<li>
<p>The following class attributes have been added:</p>
<ul>
<li>
<p>[<code>crawler</code>{.xref .py .py-attr .docutils .literal
.notranslate}]{.pre}</p>
</li>
<li>
<p>[<code>downstream_queue_cls</code>{.xref .py .py-attr .docutils .literal
.notranslate}]{.pre} (details above)</p>
</li>
<li>
<p>[<code>key</code>{.xref .py .py-attr .docutils .literal
.notranslate}]{.pre} (details above)</p>
</li>
</ul>
</li>
<li>
<p>The [<code>serialize</code>{.docutils .literal .notranslate}]{.pre} attribute
has been removed (details above)</p>
</li>
</ul>
<p>The following changes affect specifically the
[<code>ScrapyPriorityQueue</code>{.xref .py .py-class .docutils .literal
.notranslate}]{.pre} class and may affect subclasses:</p>
<ul>
<li>
<p>A new [<code>priority()</code>{.xref .py .py-meth .docutils .literal
.notranslate}]{.pre} method has been added which, given a request,
returns [<code>request.priority</code>{.docutils .literal
.notranslate}]{.pre}<code> </code>{.docutils .literal
.notranslate}[<code>*</code>{.docutils .literal
.notranslate}]{.pre}<code> </code>{.docutils .literal
.notranslate}[<code>-1</code>{.docutils .literal .notranslate}]{.pre}.</p>
<p>It is used in [<code>push()</code>{.xref .py .py-meth .docutils .literal
.notranslate}]{.pre} to make up for the removal of its
[<code>priority</code>{.docutils .literal .notranslate}]{.pre} parameter.</p>
</li>
<li>
<p>The [<code>spider</code>{.docutils .literal .notranslate}]{.pre} attribute has
been removed. Use [<code>crawler.spider</code>{.xref .py .py-attr .docutils
.literal .notranslate}]{.pre} instead.</p>
</li>
</ul>
<p>The following changes affect specifically the
[<code>DownloaderAwarePriorityQueue</code>{.xref .py .py-class .docutils .literal
.notranslate}]{.pre} class and may affect subclasses:</p>
<ul>
<li>A new [<code>pqueues</code>{.xref .py .py-attr .docutils .literal
.notranslate}]{.pre} attribute offers a mapping of downloader slot
names to the corresponding instances of
[<code>downstream_queue_cls</code>{.xref .py .py-attr .docutils .literal
.notranslate}]{.pre}.</li>
</ul>
<p>(<a href="https://github.com/scrapy/scrapy/issues/3884">issue 3884</a>{.reference
.external})
:::
:::</p>
<p>::: {#scrapy-1-8-3-2022-07-25 .section}
[]{#release-1-8-3}</p>
<h4 id="scrapy-183-2022-07-25headerlink"><a class="header" href="#scrapy-183-2022-07-25headerlink">Scrapy 1.8.3 (2022-07-25)<a href="#scrapy-1-8-3-2022-07-25" title="Permalink to this heading">¶</a>{.headerlink}</a></h4>
<p><strong>Security bug fix:</strong></p>
<ul>
<li>
<p>When <a href="index.html#scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware" title="scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware">[<code>HttpProxyMiddleware</code>{.xref .py .py-class .docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal} processes a request with <a href="index.html#std-reqmeta-proxy">[<code>proxy</code>{.xref .std
.std-reqmeta .docutils .literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal} metadata, and that <a href="index.html#std-reqmeta-proxy">[<code>proxy</code>{.xref
.std .std-reqmeta .docutils .literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal} metadata includes proxy credentials,
<a href="index.html#scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware" title="scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware">[<code>HttpProxyMiddleware</code>{.xref .py .py-class .docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal} sets the [<code>Proxy-Authorization</code>{.docutils .literal
.notranslate}]{.pre} header, but only if that header is not already
set.</p>
<p>There are third-party proxy-rotation downloader middlewares that set
different <a href="index.html#std-reqmeta-proxy">[<code>proxy</code>{.xref .std .std-reqmeta .docutils .literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal} metadata every time they process a
request.</p>
<p>Because of request retries and redirects, the same request can be
processed by downloader middlewares more than once, including both
<a href="index.html#scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware" title="scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware">[<code>HttpProxyMiddleware</code>{.xref .py .py-class .docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal} and any third-party proxy-rotation downloader middleware.</p>
<p>These third-party proxy-rotation downloader middlewares could change
the <a href="index.html#std-reqmeta-proxy">[<code>proxy</code>{.xref .std .std-reqmeta .docutils .literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal} metadata of a request to a new value,
but fail to remove the [<code>Proxy-Authorization</code>{.docutils .literal
.notranslate}]{.pre} header from the previous value of the
<a href="index.html#std-reqmeta-proxy">[<code>proxy</code>{.xref .std .std-reqmeta .docutils .literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal} metadata, causing the credentials of
one proxy to be sent to a different proxy.</p>
<p>To prevent the unintended leaking of proxy credentials, the behavior
of <a href="index.html#scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware" title="scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware">[<code>HttpProxyMiddleware</code>{.xref .py .py-class .docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal} is now as follows when processing a request:</p>
<ul>
<li>
<p>If the request being processed defines <a href="index.html#std-reqmeta-proxy">[<code>proxy</code>{.xref .std
.std-reqmeta .docutils .literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal} metadata that includes
credentials, the [<code>Proxy-Authorization</code>{.docutils .literal
.notranslate}]{.pre} header is always updated to feature those
credentials.</p>
</li>
<li>
<p>If the request being processed defines <a href="index.html#std-reqmeta-proxy">[<code>proxy</code>{.xref .std
.std-reqmeta .docutils .literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal} metadata without credentials, the
[<code>Proxy-Authorization</code>{.docutils .literal .notranslate}]{.pre}
header is removed <em>unless</em> it was originally defined for the
same proxy URL.</p>
<p>To remove proxy credentials while keeping the same proxy URL,
remove the [<code>Proxy-Authorization</code>{.docutils .literal
.notranslate}]{.pre} header.</p>
</li>
<li>
<p>If the request has no <a href="index.html#std-reqmeta-proxy">[<code>proxy</code>{.xref .std .std-reqmeta
.docutils .literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal} metadata, or that metadata is a
falsy value (e.g. [<code>None</code>{.docutils .literal
.notranslate}]{.pre}), the [<code>Proxy-Authorization</code>{.docutils
.literal .notranslate}]{.pre} header is removed.</p>
<p>It is no longer possible to set a proxy URL through the
<a href="index.html#std-reqmeta-proxy">[<code>proxy</code>{.xref .std .std-reqmeta .docutils .literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal} metadata but set the credentials
through the [<code>Proxy-Authorization</code>{.docutils .literal
.notranslate}]{.pre} header. Set proxy credentials through the
<a href="index.html#std-reqmeta-proxy">[<code>proxy</code>{.xref .std .std-reqmeta .docutils .literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal} metadata instead.
:::</p>
</li>
</ul>
</li>
</ul>
<p>::: {#scrapy-1-8-2-2022-03-01 .section}
[]{#release-1-8-2}</p>
<h4 id="scrapy-182-2022-03-01headerlink"><a class="header" href="#scrapy-182-2022-03-01headerlink">Scrapy 1.8.2 (2022-03-01)<a href="#scrapy-1-8-2-2022-03-01" title="Permalink to this heading">¶</a>{.headerlink}</a></h4>
<p><strong>Security bug fixes:</strong></p>
<ul>
<li>
<p>When a <a href="index.html#scrapy.http.Request" title="scrapy.http.Request">[<code>Request</code>{.xref .py .py-class .docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal} object with cookies defined gets a redirect response
causing a new <a href="index.html#scrapy.http.Request" title="scrapy.http.Request">[<code>Request</code>{.xref .py .py-class .docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal} object to be scheduled, the cookies defined in the
original <a href="index.html#scrapy.http.Request" title="scrapy.http.Request">[<code>Request</code>{.xref .py .py-class .docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal} object are no longer copied into the new
<a href="index.html#scrapy.http.Request" title="scrapy.http.Request">[<code>Request</code>{.xref .py .py-class .docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal} object.</p>
<p>If you manually set the [<code>Cookie</code>{.docutils .literal
.notranslate}]{.pre} header on a <a href="index.html#scrapy.http.Request" title="scrapy.http.Request">[<code>Request</code>{.xref .py .py-class
.docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal} object and the domain name of the redirect URL is not an
exact match for the domain of the URL of the original
<a href="index.html#scrapy.http.Request" title="scrapy.http.Request">[<code>Request</code>{.xref .py .py-class .docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal} object, your [<code>Cookie</code>{.docutils .literal
.notranslate}]{.pre} header is now dropped from the new
<a href="index.html#scrapy.http.Request" title="scrapy.http.Request">[<code>Request</code>{.xref .py .py-class .docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal} object.</p>
<p>The old behavior could be exploited by an attacker to gain access to
your cookies. Please, see the <a href="https://github.com/scrapy/scrapy/security/advisories/GHSA-cjvr-mfj7-j4j8">cjvr-mfj7-j4j8 security
advisory</a>{.reference
.external} for more information.</p>
<p>::: {.admonition .note}
Note</p>
<p>It is still possible to enable the sharing of cookies between
different domains with a shared domain suffix (e.g.
[<code>example.com</code>{.docutils .literal .notranslate}]{.pre} and any
subdomain) by defining the shared domain suffix (e.g.
[<code>example.com</code>{.docutils .literal .notranslate}]{.pre}) as the
cookie domain when defining your cookies. See the documentation of
the <a href="index.html#scrapy.http.Request" title="scrapy.http.Request">[<code>Request</code>{.xref .py .py-class .docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal} class for more information.
:::</p>
</li>
<li>
<p>When the domain of a cookie, either received in the
[<code>Set-Cookie</code>{.docutils .literal .notranslate}]{.pre} header of a
response or defined in a <a href="index.html#scrapy.http.Request" title="scrapy.http.Request">[<code>Request</code>{.xref .py .py-class .docutils
.literal
.notranslate}]{.pre}</a>{.reference
.internal} object, is set to a <a href="https://publicsuffix.org/">public
suffix</a>{.reference .external}, the cookie
is now ignored unless the cookie domain is the same as the request
domain.</p>
<p>The old behavior could be exploited by an attacker to inject cookies
into your requests to some other domains. Please, see the
<a href="https://github.com/scrapy/scrapy/security/advisories/GHSA-mfjm-vh54-3f96">mfjm-vh54-3f96 security
advisory</a>{.reference
.external} for more information.
:::</p>
</li>
</ul>
<p>::: {#scrapy-1-8-1-2021-10-05 .section}
[]{#release-1-8-1}</p>
<h4 id="scrapy-181-2021-10-05headerlink"><a class="header" href="#scrapy-181-2021-10-05headerlink">Scrapy 1.8.1 (2021-10-05)<a href="#scrapy-1-8-1-2021-10-05" title="Permalink to this heading">¶</a>{.headerlink}</a></h4>
<ul>
<li>
<p><strong>Security bug fix:</strong></p>
<p>If you use <a href="index.html#scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware" title="scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware">[<code>HttpAuthMiddleware</code>{.xref .py .py-class .docutils
.literal
.notranslate}]{.pre}</a>{.reference
.internal} (i.e. the [<code>http_user</code>{.docutils .literal
.notranslate}]{.pre} and [<code>http_pass</code>{.docutils .literal
.notranslate}]{.pre} spider attributes) for HTTP authentication, any
request exposes your credentials to the request target.</p>
<p>To prevent unintended exposure of authentication credentials to
unintended domains, you must now additionally set a new, additional
spider attribute, [<code>http_auth_domain</code>{.docutils .literal
.notranslate}]{.pre}, and point it to the specific domain to which
the authentication credentials must be sent.</p>
<p>If the [<code>http_auth_domain</code>{.docutils .literal .notranslate}]{.pre}
spider attribute is not set, the domain of the first request will be
considered the HTTP authentication target, and authentication
credentials will only be sent in requests targeting that domain.</p>
<p>If you need to send the same HTTP authentication credentials to
multiple domains, you can use
<a href="https://w3lib.readthedocs.io/en/latest/w3lib.html#w3lib.http.basic_auth_header" title="(in w3lib v2.1)">[<code>w3lib.http.basic_auth_header()</code>{.xref .py .py-func .docutils
.literal
.notranslate}]{.pre}</a>{.reference
.external} instead to set the value of the
[<code>Authorization</code>{.docutils .literal .notranslate}]{.pre} header of
your requests.</p>
<p>If you <em>really</em> want your spider to send the same HTTP
authentication credentials to any domain, set the
[<code>http_auth_domain</code>{.docutils .literal .notranslate}]{.pre} spider
attribute to [<code>None</code>{.docutils .literal .notranslate}]{.pre}.</p>
<p>Finally, if you are a user of
<a href="https://github.com/scrapy-plugins/scrapy-splash">scrapy-splash</a>{.reference
.external}, know that this version of Scrapy breaks compatibility
with scrapy-splash 0.7.2 and earlier. You will need to upgrade
scrapy-splash to a greater version for it to continue to work.
:::</p>
</li>
</ul>
<p>::: {#scrapy-1-8-0-2019-10-28 .section}
[]{#release-1-8-0}</p>
<h4 id="scrapy-180-2019-10-28headerlink"><a class="header" href="#scrapy-180-2019-10-28headerlink">Scrapy 1.8.0 (2019-10-28)<a href="#scrapy-1-8-0-2019-10-28" title="Permalink to this heading">¶</a>{.headerlink}</a></h4>
<p>Highlights:</p>
<ul>
<li>
<p>Dropped Python 3.4 support and updated minimum requirements; made
Python 3.8 support official</p>
</li>
<li>
<p>New <a href="index.html#scrapy.http.Request.from_curl" title="scrapy.http.Request.from_curl">[<code>Request.from_curl</code>{.xref .py .py-meth .docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal} class method</p>
</li>
<li>
<p>New <a href="index.html#std-setting-ROBOTSTXT_PARSER">[<code>ROBOTSTXT_PARSER</code>{.xref .std .std-setting .docutils .literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal} and <a href="index.html#std-setting-ROBOTSTXT_USER_AGENT">[<code>ROBOTSTXT_USER_AGENT</code>{.xref
.std .std-setting .docutils .literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal} settings</p>
</li>
<li>
<p>New <a href="index.html#std-setting-DOWNLOADER_CLIENT_TLS_CIPHERS">[<code>DOWNLOADER_CLIENT_TLS_CIPHERS</code>{.xref .std .std-setting
.docutils .literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal} and
<a href="index.html#std-setting-DOWNLOADER_CLIENT_TLS_VERBOSE_LOGGING">[<code>DOWNLOADER_CLIENT_TLS_VERBOSE_LOGGING</code>{.xref .std .std-setting
.docutils .literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal} settings</p>
</li>
</ul>
<p>::: {#id82 .section}</p>
<h5 id="backward-incompatible-changesheaderlink-7"><a class="header" href="#backward-incompatible-changesheaderlink-7">Backward-incompatible changes<a href="#id82" title="Permalink to this heading">¶</a>{.headerlink}</a></h5>
<ul>
<li>
<p>Python 3.4 is no longer supported, and some of the minimum
requirements of Scrapy have also changed:</p>
<ul>
<li>
<p><a href="https://cssselect.readthedocs.io/en/latest/index.html" title="(in cssselect v1.2.0)">[cssselect]{.xref .std
.std-doc}</a>{.reference
.external} 0.9.1</p>
</li>
<li>
<p><a href="https://cryptography.io/en/latest/">cryptography</a>{.reference
.external} 2.0</p>
</li>
<li>
<p><a href="https://lxml.de/">lxml</a>{.reference .external} 3.5.0</p>
</li>
<li>
<p><a href="https://www.pyopenssl.org/en/stable/">pyOpenSSL</a>{.reference
.external} 16.2.0</p>
</li>
<li>
<p><a href="https://github.com/scrapy/queuelib">queuelib</a>{.reference
.external} 1.4.2</p>
</li>
<li>
<p><a href="https://service-identity.readthedocs.io/en/stable/">service_identity</a>{.reference
.external} 16.0.0</p>
</li>
<li>
<p><a href="https://six.readthedocs.io/">six</a>{.reference .external} 1.10.0</p>
</li>
<li>
<p><a href="https://twistedmatrix.com/trac/">Twisted</a>{.reference .external}
17.9.0 (16.0.0 with Python 2)</p>
</li>
<li>
<p><a href="https://zopeinterface.readthedocs.io/en/latest/">zope.interface</a>{.reference
.external} 4.1.3</p>
</li>
</ul>
<p>(<a href="https://github.com/scrapy/scrapy/issues/3892">issue
3892</a>{.reference
.external})</p>
</li>
<li>
<p>[<code>JSONRequest</code>{.docutils .literal .notranslate}]{.pre} is now called
<a href="index.html#scrapy.http.JsonRequest" title="scrapy.http.JsonRequest">[<code>JsonRequest</code>{.xref .py .py-class .docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal} for consistency with similar classes (<a href="https://github.com/scrapy/scrapy/issues/3929">issue
3929</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/3982">issue
3982</a>{.reference
.external})</p>
</li>
<li>
<p>If you are using a custom context factory
(<a href="index.html#std-setting-DOWNLOADER_CLIENTCONTEXTFACTORY">[<code>DOWNLOADER_CLIENTCONTEXTFACTORY</code>{.xref .std .std-setting
.docutils .literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal}), its [<code>__init__</code>{.docutils .literal
.notranslate}]{.pre} method must accept two new parameters:
[<code>tls_verbose_logging</code>{.docutils .literal .notranslate}]{.pre} and
[<code>tls_ciphers</code>{.docutils .literal .notranslate}]{.pre} (<a href="https://github.com/scrapy/scrapy/issues/2111">issue
2111</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/3392">issue
3392</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/3442">issue
3442</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/3450">issue
3450</a>{.reference
.external})</p>
</li>
<li>
<p><a href="index.html#scrapy.loader.ItemLoader" title="scrapy.loader.ItemLoader">[<code>ItemLoader</code>{.xref .py .py-class .docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal} now turns the values of its input item into lists:</p>
<p>::: {.highlight-pycon .notranslate}
::: highlight
&gt;&gt;&gt; item = MyItem()
&gt;&gt;&gt; item[&quot;field&quot;] = &quot;value1&quot;
&gt;&gt;&gt; loader = ItemLoader(item=item)
&gt;&gt;&gt; item[&quot;field&quot;]
['value1']
:::
:::</p>
<p>This is needed to allow adding values to existing fields
([<code>loader.add_value('field',</code>{.docutils .literal
.notranslate}]{.pre}<code> </code>{.docutils .literal
.notranslate}[<code>'value2')</code>{.docutils .literal .notranslate}]{.pre}).</p>
<p>(<a href="https://github.com/scrapy/scrapy/issues/3804">issue
3804</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/3819">issue
3819</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/3897">issue
3897</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/3976">issue
3976</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/3998">issue
3998</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/4036">issue
4036</a>{.reference
.external})</p>
</li>
</ul>
<p>See also <a href="#id86">[Deprecation removals]{.std .std-ref}</a>{.hoverxref
.tooltip .reference .internal} below.
:::</p>
<p>::: {#id83 .section}</p>
<h5 id="new-featuresheaderlink-13"><a class="header" href="#new-featuresheaderlink-13">New features<a href="#id83" title="Permalink to this heading">¶</a>{.headerlink}</a></h5>
<ul>
<li>
<p>A new <a href="index.html#scrapy.http.Request.from_curl" title="scrapy.http.Request.from_curl">[<code>Request.from_curl</code>{.xref .py .py-meth .docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal} class method allows <a href="index.html#requests-from-curl">[creating a request from a cURL
command]{.std .std-ref}</a>{.hoverxref
.tooltip .reference .internal} (<a href="https://github.com/scrapy/scrapy/issues/2985">issue
2985</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/3862">issue
3862</a>{.reference
.external})</p>
</li>
<li>
<p>A new <a href="index.html#std-setting-ROBOTSTXT_PARSER">[<code>ROBOTSTXT_PARSER</code>{.xref .std .std-setting .docutils
.literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal} setting allows choosing which
<a href="https://www.robotstxt.org/">robots.txt</a>{.reference .external}
parser to use. It includes built-in support for
<a href="index.html#python-robotfileparser">[RobotFileParser]{.std
.std-ref}</a>{.hoverxref .tooltip
.reference .internal}, <a href="index.html#protego-parser">[Protego]{.std
.std-ref}</a>{.hoverxref .tooltip .reference
.internal} (default), <a href="index.html#reppy-parser">[Reppy]{.std
.std-ref}</a>{.hoverxref .tooltip .reference
.internal}, and <a href="index.html#rerp-parser">[Robotexclusionrulesparser]{.std
.std-ref}</a>{.hoverxref .tooltip .reference
.internal}, and allows you to <a href="index.html#support-for-new-robots-parser">[implement support for additional
parsers]{.std
.std-ref}</a>{.hoverxref
.tooltip .reference .internal} (<a href="https://github.com/scrapy/scrapy/issues/754">issue
754</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/2669">issue
2669</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/3796">issue
3796</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/3935">issue
3935</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/3969">issue
3969</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/4006">issue
4006</a>{.reference
.external})</p>
</li>
<li>
<p>A new <a href="index.html#std-setting-ROBOTSTXT_USER_AGENT">[<code>ROBOTSTXT_USER_AGENT</code>{.xref .std .std-setting .docutils
.literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal} setting allows defining a separate
user agent string to use for
<a href="https://www.robotstxt.org/">robots.txt</a>{.reference .external}
parsing (<a href="https://github.com/scrapy/scrapy/issues/3931">issue
3931</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/3966">issue
3966</a>{.reference
.external})</p>
</li>
<li>
<p><a href="index.html#scrapy.spiders.Rule" title="scrapy.spiders.Rule">[<code>Rule</code>{.xref .py .py-class .docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal} no longer requires a <a href="index.html#scrapy.linkextractors.lxmlhtml.LxmlLinkExtractor" title="scrapy.linkextractors.lxmlhtml.LxmlLinkExtractor">[<code>LinkExtractor</code>{.xref .py
.py-class .docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal} parameter (<a href="https://github.com/scrapy/scrapy/issues/781">issue
781</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/4016">issue
4016</a>{.reference
.external})</p>
</li>
<li>
<p>Use the new <a href="index.html#std-setting-DOWNLOADER_CLIENT_TLS_CIPHERS">[<code>DOWNLOADER_CLIENT_TLS_CIPHERS</code>{.xref .std
.std-setting .docutils .literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal} setting to customize the TLS/SSL
ciphers used by the default HTTP/1.1 downloader (<a href="https://github.com/scrapy/scrapy/issues/3392">issue
3392</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/3442">issue
3442</a>{.reference
.external})</p>
</li>
<li>
<p>Set the new <a href="index.html#std-setting-DOWNLOADER_CLIENT_TLS_VERBOSE_LOGGING">[<code>DOWNLOADER_CLIENT_TLS_VERBOSE_LOGGING</code>{.xref .std
.std-setting .docutils .literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal} setting to [<code>True</code>{.docutils .literal
.notranslate}]{.pre} to enable debug-level messages about TLS
connection parameters after establishing HTTPS connections (<a href="https://github.com/scrapy/scrapy/issues/2111">issue
2111</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/3450">issue
3450</a>{.reference
.external})</p>
</li>
<li>
<p>Callbacks that receive keyword arguments (see
<a href="index.html#scrapy.http.Request.cb_kwargs" title="scrapy.http.Request.cb_kwargs">[<code>Request.cb_kwargs</code>{.xref .py .py-attr .docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal}) can now be tested using the new <a href="index.html#scrapy.contracts.default.CallbackKeywordArgumentsContract" title="scrapy.contracts.default.CallbackKeywordArgumentsContract">[<code>@cb_kwargs</code>{.xref .py
.py-class .docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal} <a href="index.html#topics-contracts">[spider contract]{.std
.std-ref}</a>{.hoverxref .tooltip
.reference .internal} (<a href="https://github.com/scrapy/scrapy/issues/3985">issue
3985</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/3988">issue
3988</a>{.reference
.external})</p>
</li>
<li>
<p>When a <a href="index.html#scrapy.contracts.default.ScrapesContract" title="scrapy.contracts.default.ScrapesContract">[<code>@scrapes</code>{.xref .py .py-class .docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal} spider contract fails, all missing fields are now
reported (<a href="https://github.com/scrapy/scrapy/issues/766">issue
766</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/3939">issue
3939</a>{.reference
.external})</p>
</li>
<li>
<p><a href="index.html#custom-log-formats">[Custom log formats]{.std
.std-ref}</a>{.hoverxref .tooltip
.reference .internal} can now drop messages by having the
corresponding methods of the configured <a href="index.html#std-setting-LOG_FORMATTER">[<code>LOG_FORMATTER</code>{.xref .std
.std-setting .docutils .literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal} return [<code>None</code>{.docutils .literal
.notranslate}]{.pre} (<a href="https://github.com/scrapy/scrapy/issues/3984">issue
3984</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/3987">issue
3987</a>{.reference
.external})</p>
</li>
<li>
<p>A much improved completion definition is now available for
<a href="https://www.zsh.org/">Zsh</a>{.reference .external} (<a href="https://github.com/scrapy/scrapy/issues/4069">issue
4069</a>{.reference
.external})
:::</p>
</li>
</ul>
<p>::: {#id84 .section}</p>
<h5 id="bug-fixesheaderlink-13"><a class="header" href="#bug-fixesheaderlink-13">Bug fixes<a href="#id84" title="Permalink to this heading">¶</a>{.headerlink}</a></h5>
<ul>
<li>
<p><a href="index.html#scrapy.loader.ItemLoader.load_item" title="scrapy.loader.ItemLoader.load_item">[<code>ItemLoader.load_item()</code>{.xref .py .py-meth .docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal} no longer makes later calls to
<a href="index.html#scrapy.loader.ItemLoader.get_output_value" title="scrapy.loader.ItemLoader.get_output_value">[<code>ItemLoader.get_output_value()</code>{.xref .py .py-meth .docutils
.literal
.notranslate}]{.pre}</a>{.reference
.internal} or <a href="index.html#scrapy.loader.ItemLoader.load_item" title="scrapy.loader.ItemLoader.load_item">[<code>ItemLoader.load_item()</code>{.xref .py .py-meth
.docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal} return empty data (<a href="https://github.com/scrapy/scrapy/issues/3804">issue
3804</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/3819">issue
3819</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/3897">issue
3897</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/3976">issue
3976</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/3998">issue
3998</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/4036">issue
4036</a>{.reference
.external})</p>
</li>
<li>
<p>Fixed <a href="index.html#scrapy.statscollectors.DummyStatsCollector" title="scrapy.statscollectors.DummyStatsCollector">[<code>DummyStatsCollector</code>{.xref .py .py-class .docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal} raising a <a href="https://docs.python.org/3/library/exceptions.html#TypeError" title="(in Python v3.12)">[<code>TypeError</code>{.xref .py .py-exc .docutils
.literal
.notranslate}]{.pre}</a>{.reference
.external} exception (<a href="https://github.com/scrapy/scrapy/issues/4007">issue
4007</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/4052">issue
4052</a>{.reference
.external})</p>
</li>
<li>
<p><a href="index.html#scrapy.pipelines.files.FilesPipeline.file_path" title="scrapy.pipelines.files.FilesPipeline.file_path">[<code>FilesPipeline.file_path</code>{.xref .py .py-meth .docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal} and <a href="index.html#scrapy.pipelines.images.ImagesPipeline.file_path" title="scrapy.pipelines.images.ImagesPipeline.file_path">[<code>ImagesPipeline.file_path</code>{.xref .py .py-meth
.docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal} no longer choose file extensions that are not <a href="https://www.iana.org/assignments/media-types/media-types.xhtml">registered
with
IANA</a>{.reference
.external} (<a href="https://github.com/scrapy/scrapy/issues/1287">issue
1287</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/3953">issue
3953</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/3954">issue
3954</a>{.reference
.external})</p>
</li>
<li>
<p>When using <a href="https://github.com/boto/botocore">botocore</a>{.reference
.external} to persist files in S3, all botocore-supported headers
are properly mapped now (<a href="https://github.com/scrapy/scrapy/issues/3904">issue
3904</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/3905">issue
3905</a>{.reference
.external})</p>
</li>
<li>
<p>FTP passwords in [<code>FEED_URI</code>{.xref .std .std-setting .docutils
.literal .notranslate}]{.pre} containing percent-escaped characters
are now properly decoded (<a href="https://github.com/scrapy/scrapy/issues/3941">issue
3941</a>{.reference
.external})</p>
</li>
<li>
<p>A memory-handling and error-handling issue in
[<code>scrapy.utils.ssl.get_temp_key_info()</code>{.xref .py .py-func .docutils
.literal .notranslate}]{.pre} has been fixed (<a href="https://github.com/scrapy/scrapy/issues/3920">issue
3920</a>{.reference
.external})
:::</p>
</li>
</ul>
<p>::: {#id85 .section}</p>
<h5 id="documentationheaderlink-13"><a class="header" href="#documentationheaderlink-13">Documentation<a href="#id85" title="Permalink to this heading">¶</a>{.headerlink}</a></h5>
<ul>
<li>
<p>The documentation now covers how to define and configure a <a href="index.html#custom-log-formats">[custom
log format]{.std
.std-ref}</a>{.hoverxref .tooltip
.reference .internal} (<a href="https://github.com/scrapy/scrapy/issues/3616">issue
3616</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/3660">issue
3660</a>{.reference
.external})</p>
</li>
<li>
<p>API documentation added for <a href="index.html#scrapy.exporters.MarshalItemExporter" title="scrapy.exporters.MarshalItemExporter">[<code>MarshalItemExporter</code>{.xref .py
.py-class .docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal} and <a href="index.html#scrapy.exporters.PythonItemExporter" title="scrapy.exporters.PythonItemExporter">[<code>PythonItemExporter</code>{.xref .py .py-class .docutils
.literal
.notranslate}]{.pre}</a>{.reference
.internal} (<a href="https://github.com/scrapy/scrapy/issues/3973">issue
3973</a>{.reference
.external})</p>
</li>
<li>
<p>API documentation added for [<code>BaseItem</code>{.xref .py .py-class
.docutils .literal .notranslate}]{.pre} and <a href="index.html#scrapy.item.ItemMeta" title="scrapy.item.ItemMeta">[<code>ItemMeta</code>{.xref .py
.py-class .docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal} (<a href="https://github.com/scrapy/scrapy/issues/3999">issue
3999</a>{.reference
.external})</p>
</li>
<li>
<p>Minor documentation fixes (<a href="https://github.com/scrapy/scrapy/issues/2998">issue
2998</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/3398">issue
3398</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/3597">issue
3597</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/3894">issue
3894</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/3934">issue
3934</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/3978">issue
3978</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/3993">issue
3993</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/4022">issue
4022</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/4028">issue
4028</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/4033">issue
4033</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/4046">issue
4046</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/4050">issue
4050</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/4055">issue
4055</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/4056">issue
4056</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/4061">issue
4061</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/4072">issue
4072</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/4071">issue
4071</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/4079">issue
4079</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/4081">issue
4081</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/4089">issue
4089</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/4093">issue
4093</a>{.reference
.external})
:::</p>
</li>
</ul>
<p>::: {#id86 .section}
[]{#id87}</p>
<h5 id="deprecation-removalsheaderlink-9"><a class="header" href="#deprecation-removalsheaderlink-9">Deprecation removals<a href="#id86" title="Permalink to this heading">¶</a>{.headerlink}</a></h5>
<ul>
<li>[<code>scrapy.xlib</code>{.docutils .literal .notranslate}]{.pre} has been
removed (<a href="https://github.com/scrapy/scrapy/issues/4015">issue
4015</a>{.reference
.external})
:::</li>
</ul>
<p>::: {#id88 .section}
[]{#id89}</p>
<h5 id="deprecationsheaderlink-12"><a class="header" href="#deprecationsheaderlink-12">Deprecations<a href="#id88" title="Permalink to this heading">¶</a>{.headerlink}</a></h5>
<ul>
<li>
<p>The <a href="https://github.com/google/leveldb">LevelDB</a>{.reference
.external} storage backend
([<code>scrapy.extensions.httpcache.LeveldbCacheStorage</code>{.docutils
.literal .notranslate}]{.pre}) of <a href="index.html#scrapy.downloadermiddlewares.httpcache.HttpCacheMiddleware" title="scrapy.downloadermiddlewares.httpcache.HttpCacheMiddleware">[<code>HttpCacheMiddleware</code>{.xref .py
.py-class .docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal} is deprecated (<a href="https://github.com/scrapy/scrapy/issues/4085">issue
4085</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/4092">issue
4092</a>{.reference
.external})</p>
</li>
<li>
<p>Use of the undocumented
[<code>SCRAPY_PICKLED_SETTINGS_TO_OVERRIDE</code>{.docutils .literal
.notranslate}]{.pre} environment variable is deprecated (<a href="https://github.com/scrapy/scrapy/issues/3910">issue
3910</a>{.reference
.external})</p>
</li>
<li>
<p>[<code>scrapy.item.DictItem</code>{.docutils .literal .notranslate}]{.pre} is
deprecated, use [<code>Item</code>{.xref .py .py-class .docutils .literal
.notranslate}]{.pre} instead (<a href="https://github.com/scrapy/scrapy/issues/3999">issue
3999</a>{.reference
.external})
:::</p>
</li>
</ul>
<p>::: {#other-changes .section}</p>
<h5 id="other-changesheaderlink"><a class="header" href="#other-changesheaderlink">Other changes<a href="#other-changes" title="Permalink to this heading">¶</a>{.headerlink}</a></h5>
<ul>
<li>
<p>Minimum versions of optional Scrapy requirements that are covered by
continuous integration tests have been updated:</p>
<ul>
<li>
<p><a href="https://github.com/boto/botocore">botocore</a>{.reference
.external} 1.3.23</p>
</li>
<li>
<p><a href="https://python-pillow.org/">Pillow</a>{.reference .external} 3.4.2</p>
</li>
</ul>
<p>Lower versions of these optional requirements may work, but it is
not guaranteed (<a href="https://github.com/scrapy/scrapy/issues/3892">issue
3892</a>{.reference
.external})</p>
</li>
<li>
<p>GitHub templates for bug reports and feature requests (<a href="https://github.com/scrapy/scrapy/issues/3126">issue
3126</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/3471">issue
3471</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/3749">issue
3749</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/3754">issue
3754</a>{.reference
.external})</p>
</li>
<li>
<p>Continuous integration fixes (<a href="https://github.com/scrapy/scrapy/issues/3923">issue
3923</a>{.reference
.external})</p>
</li>
<li>
<p>Code cleanup (<a href="https://github.com/scrapy/scrapy/issues/3391">issue
3391</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/3907">issue
3907</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/3946">issue
3946</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/3950">issue
3950</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/4023">issue
4023</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/4031">issue
4031</a>{.reference
.external})
:::
:::</p>
</li>
</ul>
<p>::: {#scrapy-1-7-4-2019-10-21 .section}
[]{#release-1-7-4}</p>
<h4 id="scrapy-174-2019-10-21headerlink"><a class="header" href="#scrapy-174-2019-10-21headerlink">Scrapy 1.7.4 (2019-10-21)<a href="#scrapy-1-7-4-2019-10-21" title="Permalink to this heading">¶</a>{.headerlink}</a></h4>
<p>Revert the fix for <a href="https://github.com/scrapy/scrapy/issues/3804">issue
3804</a>{.reference
.external} (<a href="https://github.com/scrapy/scrapy/issues/3819">issue
3819</a>{.reference
.external}), which has a few undesired side effects (<a href="https://github.com/scrapy/scrapy/issues/3897">issue
3897</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/3976">issue
3976</a>{.reference
.external}).</p>
<p>As a result, when an item loader is initialized with an item,
<a href="index.html#scrapy.loader.ItemLoader.load_item" title="scrapy.loader.ItemLoader.load_item">[<code>ItemLoader.load_item()</code>{.xref .py .py-meth .docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal} once again makes later calls to
<a href="index.html#scrapy.loader.ItemLoader.get_output_value" title="scrapy.loader.ItemLoader.get_output_value">[<code>ItemLoader.get_output_value()</code>{.xref .py .py-meth .docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal} or <a href="index.html#scrapy.loader.ItemLoader.load_item" title="scrapy.loader.ItemLoader.load_item">[<code>ItemLoader.load_item()</code>{.xref .py .py-meth .docutils
.literal
.notranslate}]{.pre}</a>{.reference
.internal} return empty data.
:::</p>
<p>::: {#scrapy-1-7-3-2019-08-01 .section}
[]{#release-1-7-3}</p>
<h4 id="scrapy-173-2019-08-01headerlink"><a class="header" href="#scrapy-173-2019-08-01headerlink">Scrapy 1.7.3 (2019-08-01)<a href="#scrapy-1-7-3-2019-08-01" title="Permalink to this heading">¶</a>{.headerlink}</a></h4>
<p>Enforce lxml 4.3.5 or lower for Python 3.4 (<a href="https://github.com/scrapy/scrapy/issues/3912">issue
3912</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/3918">issue
3918</a>{.reference
.external}).
:::</p>
<p>::: {#scrapy-1-7-2-2019-07-23 .section}
[]{#release-1-7-2}</p>
<h4 id="scrapy-172-2019-07-23headerlink"><a class="header" href="#scrapy-172-2019-07-23headerlink">Scrapy 1.7.2 (2019-07-23)<a href="#scrapy-1-7-2-2019-07-23" title="Permalink to this heading">¶</a>{.headerlink}</a></h4>
<p>Fix Python 2 support (<a href="https://github.com/scrapy/scrapy/issues/3889">issue
3889</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/3893">issue
3893</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/3896">issue
3896</a>{.reference
.external}).
:::</p>
<p>::: {#scrapy-1-7-1-2019-07-18 .section}
[]{#release-1-7-1}</p>
<h4 id="scrapy-171-2019-07-18headerlink"><a class="header" href="#scrapy-171-2019-07-18headerlink">Scrapy 1.7.1 (2019-07-18)<a href="#scrapy-1-7-1-2019-07-18" title="Permalink to this heading">¶</a>{.headerlink}</a></h4>
<p>Re-packaging of Scrapy 1.7.0, which was missing some changes in PyPI.
:::</p>
<p>::: {#scrapy-1-7-0-2019-07-18 .section}
[]{#release-1-7-0}</p>
<h4 id="scrapy-170-2019-07-18headerlink"><a class="header" href="#scrapy-170-2019-07-18headerlink">Scrapy 1.7.0 (2019-07-18)<a href="#scrapy-1-7-0-2019-07-18" title="Permalink to this heading">¶</a>{.headerlink}</a></h4>
<p>::: {.admonition .note}
Note</p>
<p>Make sure you install Scrapy 1.7.1. The Scrapy 1.7.0 package in PyPI is
the result of an erroneous commit tagging and does not include all the
changes described below.
:::</p>
<p>Highlights:</p>
<ul>
<li>
<p>Improvements for crawls targeting multiple domains</p>
</li>
<li>
<p>A cleaner way to pass arguments to callbacks</p>
</li>
<li>
<p>A new class for JSON requests</p>
</li>
<li>
<p>Improvements for rule-based spiders</p>
</li>
<li>
<p>New features for feed exports</p>
</li>
</ul>
<p>::: {#id90 .section}</p>
<h5 id="backward-incompatible-changesheaderlink-8"><a class="header" href="#backward-incompatible-changesheaderlink-8">Backward-incompatible changes<a href="#id90" title="Permalink to this heading">¶</a>{.headerlink}</a></h5>
<ul>
<li>
<p>[<code>429</code>{.docutils .literal .notranslate}]{.pre} is now part of the
<a href="index.html#std-setting-RETRY_HTTP_CODES">[<code>RETRY_HTTP_CODES</code>{.xref .std .std-setting .docutils .literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal} setting by default</p>
<p>This change is <strong>backward incompatible</strong>. If you don't want to retry
[<code>429</code>{.docutils .literal .notranslate}]{.pre}, you must override
<a href="index.html#std-setting-RETRY_HTTP_CODES">[<code>RETRY_HTTP_CODES</code>{.xref .std .std-setting .docutils .literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal} accordingly.</p>
</li>
<li>
<p><a href="index.html#scrapy.crawler.Crawler" title="scrapy.crawler.Crawler">[<code>Crawler</code>{.xref .py .py-class .docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal}, <a href="index.html#scrapy.crawler.CrawlerRunner.crawl" title="scrapy.crawler.CrawlerRunner.crawl">[<code>CrawlerRunner.crawl</code>{.xref .py .py-class .docutils
.literal
.notranslate}]{.pre}</a>{.reference
.internal} and <a href="index.html#scrapy.crawler.CrawlerRunner.create_crawler" title="scrapy.crawler.CrawlerRunner.create_crawler">[<code>CrawlerRunner.create_crawler</code>{.xref .py .py-class
.docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal} no longer accept a <a href="index.html#scrapy.spiders.Spider" title="scrapy.spiders.Spider">[<code>Spider</code>{.xref .py .py-class
.docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal} subclass instance, they only accept a <a href="index.html#scrapy.spiders.Spider" title="scrapy.spiders.Spider">[<code>Spider</code>{.xref
.py .py-class .docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal} subclass now.</p>
<p><a href="index.html#scrapy.spiders.Spider" title="scrapy.spiders.Spider">[<code>Spider</code>{.xref .py .py-class .docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal} subclass instances were never meant to work, and they
were not working as one would expect: instead of using the passed
<a href="index.html#scrapy.spiders.Spider" title="scrapy.spiders.Spider">[<code>Spider</code>{.xref .py .py-class .docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal} subclass instance, their [<code>from_crawler</code>{.xref .py
.py-class .docutils .literal .notranslate}]{.pre} method was called
to generate a new instance.</p>
</li>
<li>
<p>Non-default values for the <a href="index.html#std-setting-SCHEDULER_PRIORITY_QUEUE">[<code>SCHEDULER_PRIORITY_QUEUE</code>{.xref .std
.std-setting .docutils .literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal} setting may stop working. Scheduler
priority queue classes now need to handle <a href="index.html#scrapy.http.Request" title="scrapy.http.Request">[<code>Request</code>{.xref .py
.py-class .docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal} objects instead of arbitrary Python data structures.</p>
</li>
<li>
<p>An additional [<code>crawler</code>{.docutils .literal .notranslate}]{.pre}
parameter has been added to the [<code>__init__</code>{.docutils .literal
.notranslate}]{.pre} method of the <a href="index.html#scrapy.core.scheduler.Scheduler" title="scrapy.core.scheduler.Scheduler">[<code>Scheduler</code>{.xref .py .py-class
.docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal} class. Custom scheduler subclasses which don't accept
arbitrary parameters in their [<code>__init__</code>{.docutils .literal
.notranslate}]{.pre} method might break because of this change.</p>
<p>For more information, see <a href="index.html#std-setting-SCHEDULER">[<code>SCHEDULER</code>{.xref .std .std-setting
.docutils .literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal}.</p>
</li>
</ul>
<p>See also <a href="#id94">[Deprecation removals]{.std .std-ref}</a>{.hoverxref
.tooltip .reference .internal} below.
:::</p>
<p>::: {#id91 .section}</p>
<h5 id="new-featuresheaderlink-14"><a class="header" href="#new-featuresheaderlink-14">New features<a href="#id91" title="Permalink to this heading">¶</a>{.headerlink}</a></h5>
<ul>
<li>
<p>A new scheduler priority queue,
[<code>scrapy.pqueues.DownloaderAwarePriorityQueue</code>{.docutils .literal
.notranslate}]{.pre}, may be <a href="index.html#broad-crawls-scheduler-priority-queue">[enabled]{.std
.std-ref}</a>{.hoverxref
.tooltip .reference .internal} for a significant scheduling
improvement on crawls targeting multiple web domains, at the cost of
no <a href="index.html#std-setting-CONCURRENT_REQUESTS_PER_IP">[<code>CONCURRENT_REQUESTS_PER_IP</code>{.xref .std .std-setting .docutils
.literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal} support (<a href="https://github.com/scrapy/scrapy/issues/3520">issue
3520</a>{.reference
.external})</p>
</li>
<li>
<p>A new <a href="index.html#scrapy.http.Request.cb_kwargs" title="scrapy.http.Request.cb_kwargs">[<code>Request.cb_kwargs</code>{.xref .py .py-attr .docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal} attribute provides a cleaner way to pass keyword
arguments to callback methods (<a href="https://github.com/scrapy/scrapy/issues/1138">issue
1138</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/3563">issue
3563</a>{.reference
.external})</p>
</li>
<li>
<p>A new <a href="index.html#scrapy.http.JsonRequest" title="scrapy.http.JsonRequest">[<code>JSONRequest</code>{.xref .py .py-class .docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal} class offers a more convenient way to build JSON requests
(<a href="https://github.com/scrapy/scrapy/issues/3504">issue
3504</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/3505">issue
3505</a>{.reference
.external})</p>
</li>
<li>
<p>A [<code>process_request</code>{.docutils .literal .notranslate}]{.pre}
callback passed to the <a href="index.html#scrapy.spiders.Rule" title="scrapy.spiders.Rule">[<code>Rule</code>{.xref .py .py-class .docutils
.literal
.notranslate}]{.pre}</a>{.reference
.internal} [<code>__init__</code>{.docutils .literal .notranslate}]{.pre}
method now receives the <a href="index.html#scrapy.http.Response" title="scrapy.http.Response">[<code>Response</code>{.xref .py .py-class .docutils
.literal
.notranslate}]{.pre}</a>{.reference
.internal} object that originated the request as its second argument
(<a href="https://github.com/scrapy/scrapy/issues/3682">issue
3682</a>{.reference
.external})</p>
</li>
<li>
<p>A new [<code>restrict_text</code>{.docutils .literal .notranslate}]{.pre}
parameter for the <a href="index.html#scrapy.linkextractors.lxmlhtml.LxmlLinkExtractor" title="scrapy.linkextractors.lxmlhtml.LxmlLinkExtractor">[<code>LinkExtractor</code>{.xref .py .py-attr .docutils
.literal
.notranslate}]{.pre}</a>{.reference
.internal} [<code>__init__</code>{.docutils .literal .notranslate}]{.pre}
method allows filtering links by linking text (<a href="https://github.com/scrapy/scrapy/issues/3622">issue
3622</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/3635">issue
3635</a>{.reference
.external})</p>
</li>
<li>
<p>A new <a href="index.html#std-setting-FEED_STORAGE_S3_ACL">[<code>FEED_STORAGE_S3_ACL</code>{.xref .std .std-setting .docutils
.literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal} setting allows defining a custom ACL
for feeds exported to Amazon S3 (<a href="https://github.com/scrapy/scrapy/issues/3607">issue
3607</a>{.reference
.external})</p>
</li>
<li>
<p>A new <a href="index.html#std-setting-FEED_STORAGE_FTP_ACTIVE">[<code>FEED_STORAGE_FTP_ACTIVE</code>{.xref .std .std-setting .docutils
.literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal} setting allows using FTP's active
connection mode for feeds exported to FTP servers (<a href="https://github.com/scrapy/scrapy/issues/3829">issue
3829</a>{.reference
.external})</p>
</li>
<li>
<p>A new <a href="index.html#std-setting-METAREFRESH_IGNORE_TAGS">[<code>METAREFRESH_IGNORE_TAGS</code>{.xref .std .std-setting .docutils
.literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal} setting allows overriding which HTML
tags are ignored when searching a response for HTML meta tags that
trigger a redirect (<a href="https://github.com/scrapy/scrapy/issues/1422">issue
1422</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/3768">issue
3768</a>{.reference
.external})</p>
</li>
<li>
<p>A new <a href="index.html#std-reqmeta-redirect_reasons">[<code>redirect_reasons</code>{.xref .std .std-reqmeta .docutils
.literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal} request meta key exposes the reason
(status code, meta refresh) behind every followed redirect (<a href="https://github.com/scrapy/scrapy/issues/3581">issue
3581</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/3687">issue
3687</a>{.reference
.external})</p>
</li>
<li>
<p>The [<code>SCRAPY_CHECK</code>{.docutils .literal .notranslate}]{.pre} variable
is now set to the [<code>true</code>{.docutils .literal .notranslate}]{.pre}
string during runs of the <a href="index.html#std-command-check">[<code>check</code>{.xref .std .std-command
.docutils .literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal} command, which allows <a href="index.html#detecting-contract-check-runs">[detecting
contract check runs from code]{.std
.std-ref}</a>{.hoverxref
.tooltip .reference .internal} (<a href="https://github.com/scrapy/scrapy/issues/3704">issue
3704</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/3739">issue
3739</a>{.reference
.external})</p>
</li>
<li>
<p>A new [<code>Item.deepcopy()</code>{.xref .py .py-meth .docutils .literal
.notranslate}]{.pre} method makes it easier to <a href="index.html#copying-items">[deep-copy
items]{.std .std-ref}</a>{.hoverxref .tooltip
.reference .internal} (<a href="https://github.com/scrapy/scrapy/issues/1493">issue
1493</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/3671">issue
3671</a>{.reference
.external})</p>
</li>
<li>
<p><a href="index.html#scrapy.extensions.corestats.CoreStats" title="scrapy.extensions.corestats.CoreStats">[<code>CoreStats</code>{.xref .py .py-class .docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal} also logs [<code>elapsed_time_seconds</code>{.docutils .literal
.notranslate}]{.pre} now (<a href="https://github.com/scrapy/scrapy/issues/3638">issue
3638</a>{.reference
.external})</p>
</li>
<li>
<p>Exceptions from <a href="index.html#scrapy.loader.ItemLoader" title="scrapy.loader.ItemLoader">[<code>ItemLoader</code>{.xref .py .py-class .docutils
.literal
.notranslate}]{.pre}</a>{.reference
.internal} <a href="index.html#topics-loaders-processors">[input and output processors]{.std
.std-ref}</a>{.hoverxref .tooltip
.reference .internal} are now more verbose (<a href="https://github.com/scrapy/scrapy/issues/3836">issue
3836</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/3840">issue
3840</a>{.reference
.external})</p>
</li>
<li>
<p><a href="index.html#scrapy.crawler.Crawler" title="scrapy.crawler.Crawler">[<code>Crawler</code>{.xref .py .py-class .docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal}, <a href="index.html#scrapy.crawler.CrawlerRunner.crawl" title="scrapy.crawler.CrawlerRunner.crawl">[<code>CrawlerRunner.crawl</code>{.xref .py .py-class .docutils
.literal
.notranslate}]{.pre}</a>{.reference
.internal} and <a href="index.html#scrapy.crawler.CrawlerRunner.create_crawler" title="scrapy.crawler.CrawlerRunner.create_crawler">[<code>CrawlerRunner.create_crawler</code>{.xref .py .py-class
.docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal} now fail gracefully if they receive a <a href="index.html#scrapy.spiders.Spider" title="scrapy.spiders.Spider">[<code>Spider</code>{.xref
.py .py-class .docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal} subclass instance instead of the subclass itself (<a href="https://github.com/scrapy/scrapy/issues/2283">issue
2283</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/3610">issue
3610</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/3872">issue
3872</a>{.reference
.external})
:::</p>
</li>
</ul>
<p>::: {#id92 .section}</p>
<h5 id="bug-fixesheaderlink-14"><a class="header" href="#bug-fixesheaderlink-14">Bug fixes<a href="#id92" title="Permalink to this heading">¶</a>{.headerlink}</a></h5>
<ul>
<li>
<p><a href="index.html#scrapy.spidermiddlewares.SpiderMiddleware.process_spider_exception" title="scrapy.spidermiddlewares.SpiderMiddleware.process_spider_exception">[<code>process_spider_exception()</code>{.xref .py .py-meth .docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal} is now also invoked for generators (<a href="https://github.com/scrapy/scrapy/issues/220">issue
220</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/2061">issue
2061</a>{.reference
.external})</p>
</li>
<li>
<p>System exceptions like
<a href="https://docs.python.org/3/library/exceptions.html#KeyboardInterrupt">KeyboardInterrupt</a>{.reference
.external} are no longer caught (<a href="https://github.com/scrapy/scrapy/issues/3726">issue
3726</a>{.reference
.external})</p>
</li>
<li>
<p><a href="index.html#scrapy.loader.ItemLoader.load_item" title="scrapy.loader.ItemLoader.load_item">[<code>ItemLoader.load_item()</code>{.xref .py .py-meth .docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal} no longer makes later calls to
<a href="index.html#scrapy.loader.ItemLoader.get_output_value" title="scrapy.loader.ItemLoader.get_output_value">[<code>ItemLoader.get_output_value()</code>{.xref .py .py-meth .docutils
.literal
.notranslate}]{.pre}</a>{.reference
.internal} or <a href="index.html#scrapy.loader.ItemLoader.load_item" title="scrapy.loader.ItemLoader.load_item">[<code>ItemLoader.load_item()</code>{.xref .py .py-meth
.docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal} return empty data (<a href="https://github.com/scrapy/scrapy/issues/3804">issue
3804</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/3819">issue
3819</a>{.reference
.external})</p>
</li>
<li>
<p>The images pipeline (<a href="index.html#scrapy.pipelines.images.ImagesPipeline" title="scrapy.pipelines.images.ImagesPipeline">[<code>ImagesPipeline</code>{.xref .py .py-class
.docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal}) no longer ignores these Amazon S3 settings:
<a href="index.html#std-setting-AWS_ENDPOINT_URL">[<code>AWS_ENDPOINT_URL</code>{.xref .std .std-setting .docutils .literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal}, <a href="index.html#std-setting-AWS_REGION_NAME">[<code>AWS_REGION_NAME</code>{.xref .std
.std-setting .docutils .literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal}, <a href="index.html#std-setting-AWS_USE_SSL">[<code>AWS_USE_SSL</code>{.xref .std
.std-setting .docutils .literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal}, <a href="index.html#std-setting-AWS_VERIFY">[<code>AWS_VERIFY</code>{.xref .std
.std-setting .docutils .literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal} (<a href="https://github.com/scrapy/scrapy/issues/3625">issue
3625</a>{.reference
.external})</p>
</li>
<li>
<p>Fixed a memory leak in
[<code>scrapy.pipelines.media.MediaPipeline</code>{.docutils .literal
.notranslate}]{.pre} affecting, for example, non-200 responses and
exceptions from custom middlewares (<a href="https://github.com/scrapy/scrapy/issues/3813">issue
3813</a>{.reference
.external})</p>
</li>
<li>
<p>Requests with private callbacks are now correctly unserialized from
disk (<a href="https://github.com/scrapy/scrapy/issues/3790">issue
3790</a>{.reference
.external})</p>
</li>
<li>
<p>[<code>FormRequest.from_response()</code>{.xref .py .py-meth .docutils .literal
.notranslate}]{.pre} now handles invalid methods like major web
browsers (<a href="https://github.com/scrapy/scrapy/issues/3777">issue
3777</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/3794">issue
3794</a>{.reference
.external})
:::</p>
</li>
</ul>
<p>::: {#id93 .section}</p>
<h5 id="documentationheaderlink-14"><a class="header" href="#documentationheaderlink-14">Documentation<a href="#id93" title="Permalink to this heading">¶</a>{.headerlink}</a></h5>
<ul>
<li>
<p>A new topic, <a href="index.html#topics-dynamic-content">[Selecting dynamically-loaded content]{.std
.std-ref}</a>{.hoverxref .tooltip
.reference .internal}, covers recommended approaches to read
dynamically-loaded data (<a href="https://github.com/scrapy/scrapy/issues/3703">issue
3703</a>{.reference
.external})</p>
</li>
<li>
<p><a href="index.html#topics-broad-crawls">[Broad Crawls]{.std
.std-ref}</a>{.hoverxref .tooltip
.reference .internal} now features information about memory usage
(<a href="https://github.com/scrapy/scrapy/issues/1264">issue
1264</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/3866">issue
3866</a>{.reference
.external})</p>
</li>
<li>
<p>The documentation of <a href="index.html#scrapy.spiders.Rule" title="scrapy.spiders.Rule">[<code>Rule</code>{.xref .py .py-class .docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal} now covers how to access the text of a link when using
<a href="index.html#scrapy.spiders.CrawlSpider" title="scrapy.spiders.CrawlSpider">[<code>CrawlSpider</code>{.xref .py .py-class .docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal} (<a href="https://github.com/scrapy/scrapy/issues/3711">issue
3711</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/3712">issue
3712</a>{.reference
.external})</p>
</li>
<li>
<p>A new section, <a href="index.html#httpcache-storage-custom">[Writing your own storage backend]{.std
.std-ref}</a>{.hoverxref .tooltip
.reference .internal}, covers writing a custom cache storage backend
for <a href="index.html#scrapy.downloadermiddlewares.httpcache.HttpCacheMiddleware" title="scrapy.downloadermiddlewares.httpcache.HttpCacheMiddleware">[<code>HttpCacheMiddleware</code>{.xref .py .py-class .docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal} (<a href="https://github.com/scrapy/scrapy/issues/3683">issue
3683</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/3692">issue
3692</a>{.reference
.external})</p>
</li>
<li>
<p>A new <a href="index.html#faq">[FAQ]{.std .std-ref}</a>{.hoverxref .tooltip
.reference .internal} entry, <a href="index.html#faq-split-item">[How to split an item into multiple
items in an item pipeline?]{.std
.std-ref}</a>{.hoverxref .tooltip .reference
.internal}, explains what to do when you want to split an item into
multiple items from an item pipeline (<a href="https://github.com/scrapy/scrapy/issues/2240">issue
2240</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/3672">issue
3672</a>{.reference
.external})</p>
</li>
<li>
<p>Updated the <a href="index.html#faq-bfo-dfo">[FAQ entry about crawl order]{.std
.std-ref}</a>{.hoverxref .tooltip .reference
.internal} to explain why the first few requests rarely follow the
desired order (<a href="https://github.com/scrapy/scrapy/issues/1739">issue
1739</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/3621">issue
3621</a>{.reference
.external})</p>
</li>
<li>
<p>The <a href="index.html#std-setting-LOGSTATS_INTERVAL">[<code>LOGSTATS_INTERVAL</code>{.xref .std .std-setting .docutils .literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal} setting (<a href="https://github.com/scrapy/scrapy/issues/3730">issue
3730</a>{.reference
.external}), the <a href="index.html#scrapy.pipelines.files.FilesPipeline.file_path" title="scrapy.pipelines.files.FilesPipeline.file_path">[<code>FilesPipeline.file_path</code>{.xref .py .py-meth
.docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal} and <a href="index.html#scrapy.pipelines.images.ImagesPipeline.file_path" title="scrapy.pipelines.images.ImagesPipeline.file_path">[<code>ImagesPipeline.file_path</code>{.xref .py .py-meth
.docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal} methods (<a href="https://github.com/scrapy/scrapy/issues/2253">issue
2253</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/3609">issue
3609</a>{.reference
.external}) and the <a href="index.html#scrapy.crawler.Crawler.stop" title="scrapy.crawler.Crawler.stop">[<code>Crawler.stop()</code>{.xref .py .py-meth .docutils
.literal
.notranslate}]{.pre}</a>{.reference
.internal} method (<a href="https://github.com/scrapy/scrapy/issues/3842">issue
3842</a>{.reference
.external}) are now documented</p>
</li>
<li>
<p>Some parts of the documentation that were confusing or misleading
are now clearer (<a href="https://github.com/scrapy/scrapy/issues/1347">issue
1347</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/1789">issue
1789</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/2289">issue
2289</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/3069">issue
3069</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/3615">issue
3615</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/3626">issue
3626</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/3668">issue
3668</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/3670">issue
3670</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/3673">issue
3673</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/3728">issue
3728</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/3762">issue
3762</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/3861">issue
3861</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/3882">issue
3882</a>{.reference
.external})</p>
</li>
<li>
<p>Minor documentation fixes (<a href="https://github.com/scrapy/scrapy/issues/3648">issue
3648</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/3649">issue
3649</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/3662">issue
3662</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/3674">issue
3674</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/3676">issue
3676</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/3694">issue
3694</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/3724">issue
3724</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/3764">issue
3764</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/3767">issue
3767</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/3791">issue
3791</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/3797">issue
3797</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/3806">issue
3806</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/3812">issue
3812</a>{.reference
.external})
:::</p>
</li>
</ul>
<p>::: {#id94 .section}
[]{#id95}</p>
<h5 id="deprecation-removalsheaderlink-10"><a class="header" href="#deprecation-removalsheaderlink-10">Deprecation removals<a href="#id94" title="Permalink to this heading">¶</a>{.headerlink}</a></h5>
<p>The following deprecated APIs have been removed (<a href="https://github.com/scrapy/scrapy/issues/3578">issue
3578</a>{.reference
.external}):</p>
<ul>
<li>
<p>[<code>scrapy.conf</code>{.docutils .literal .notranslate}]{.pre} (use
<a href="index.html#scrapy.crawler.Crawler.settings" title="scrapy.crawler.Crawler.settings">[<code>Crawler.settings</code>{.xref .py .py-attr .docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal})</p>
</li>
<li>
<p>From [<code>scrapy.core.downloader.handlers</code>{.docutils .literal
.notranslate}]{.pre}:</p>
<ul>
<li>[<code>http.HttpDownloadHandler</code>{.docutils .literal
.notranslate}]{.pre} (use
[<code>http10.HTTP10DownloadHandler</code>{.docutils .literal
.notranslate}]{.pre})</li>
</ul>
</li>
<li>
<p>[<code>scrapy.loader.ItemLoader._get_values</code>{.docutils .literal
.notranslate}]{.pre} (use [<code>_get_xpathvalues</code>{.docutils .literal
.notranslate}]{.pre})</p>
</li>
<li>
<p>[<code>scrapy.loader.XPathItemLoader</code>{.docutils .literal
.notranslate}]{.pre} (use <a href="index.html#scrapy.loader.ItemLoader" title="scrapy.loader.ItemLoader">[<code>ItemLoader</code>{.xref .py .py-class
.docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal})</p>
</li>
<li>
<p>[<code>scrapy.log</code>{.docutils .literal .notranslate}]{.pre} (see
<a href="index.html#topics-logging">[Logging]{.std .std-ref}</a>{.hoverxref
.tooltip .reference .internal})</p>
</li>
<li>
<p>From [<code>scrapy.pipelines</code>{.docutils .literal .notranslate}]{.pre}:</p>
<ul>
<li>
<p>[<code>files.FilesPipeline.file_key</code>{.docutils .literal
.notranslate}]{.pre} (use [<code>file_path</code>{.docutils .literal
.notranslate}]{.pre})</p>
</li>
<li>
<p>[<code>images.ImagesPipeline.file_key</code>{.docutils .literal
.notranslate}]{.pre} (use [<code>file_path</code>{.docutils .literal
.notranslate}]{.pre})</p>
</li>
<li>
<p>[<code>images.ImagesPipeline.image_key</code>{.docutils .literal
.notranslate}]{.pre} (use [<code>file_path</code>{.docutils .literal
.notranslate}]{.pre})</p>
</li>
<li>
<p>[<code>images.ImagesPipeline.thumb_key</code>{.docutils .literal
.notranslate}]{.pre} (use [<code>thumb_path</code>{.docutils .literal
.notranslate}]{.pre})</p>
</li>
</ul>
</li>
<li>
<p>From both [<code>scrapy.selector</code>{.docutils .literal .notranslate}]{.pre}
and [<code>scrapy.selector.lxmlsel</code>{.docutils .literal
.notranslate}]{.pre}:</p>
<ul>
<li>
<p>[<code>HtmlXPathSelector</code>{.docutils .literal .notranslate}]{.pre}
(use <a href="index.html#scrapy.selector.Selector" title="scrapy.selector.Selector">[<code>Selector</code>{.xref .py .py-class .docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal})</p>
</li>
<li>
<p>[<code>XmlXPathSelector</code>{.docutils .literal .notranslate}]{.pre} (use
<a href="index.html#scrapy.selector.Selector" title="scrapy.selector.Selector">[<code>Selector</code>{.xref .py .py-class .docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal})</p>
</li>
<li>
<p>[<code>XPathSelector</code>{.docutils .literal .notranslate}]{.pre} (use
<a href="index.html#scrapy.selector.Selector" title="scrapy.selector.Selector">[<code>Selector</code>{.xref .py .py-class .docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal})</p>
</li>
<li>
<p>[<code>XPathSelectorList</code>{.docutils .literal .notranslate}]{.pre}
(use <a href="index.html#scrapy.selector.Selector" title="scrapy.selector.Selector">[<code>Selector</code>{.xref .py .py-class .docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal})</p>
</li>
</ul>
</li>
<li>
<p>From [<code>scrapy.selector.csstranslator</code>{.docutils .literal
.notranslate}]{.pre}:</p>
<ul>
<li>
<p>[<code>ScrapyGenericTranslator</code>{.docutils .literal
.notranslate}]{.pre} (use
<a href="https://parsel.readthedocs.io/en/latest/parsel.html#parsel.csstranslator.GenericTranslator">parsel.csstranslator.GenericTranslator</a>{.reference
.external})</p>
</li>
<li>
<p>[<code>ScrapyHTMLTranslator</code>{.docutils .literal .notranslate}]{.pre}
(use
<a href="https://parsel.readthedocs.io/en/latest/parsel.html#parsel.csstranslator.HTMLTranslator">parsel.csstranslator.HTMLTranslator</a>{.reference
.external})</p>
</li>
<li>
<p>[<code>ScrapyXPathExpr</code>{.docutils .literal .notranslate}]{.pre} (use
<a href="https://parsel.readthedocs.io/en/latest/parsel.html#parsel.csstranslator.XPathExpr">parsel.csstranslator.XPathExpr</a>{.reference
.external})</p>
</li>
</ul>
</li>
<li>
<p>From <a href="index.html#scrapy.selector.Selector" title="scrapy.selector.Selector">[<code>Selector</code>{.xref .py .py-class .docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal}:</p>
<ul>
<li>
<p>[<code>_root</code>{.docutils .literal .notranslate}]{.pre} (both the
[<code>__init__</code>{.docutils .literal .notranslate}]{.pre} method
argument and the object property, use [<code>root</code>{.docutils .literal
.notranslate}]{.pre})</p>
</li>
<li>
<p>[<code>extract_unquoted</code>{.docutils .literal .notranslate}]{.pre} (use
[<code>getall</code>{.docutils .literal .notranslate}]{.pre})</p>
</li>
<li>
<p>[<code>select</code>{.docutils .literal .notranslate}]{.pre} (use
[<code>xpath</code>{.docutils .literal .notranslate}]{.pre})</p>
</li>
</ul>
</li>
<li>
<p>From <a href="index.html#scrapy.selector.SelectorList" title="scrapy.selector.SelectorList">[<code>SelectorList</code>{.xref .py .py-class .docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal}:</p>
<ul>
<li>
<p>[<code>extract_unquoted</code>{.docutils .literal .notranslate}]{.pre} (use
[<code>getall</code>{.docutils .literal .notranslate}]{.pre})</p>
</li>
<li>
<p>[<code>select</code>{.docutils .literal .notranslate}]{.pre} (use
[<code>xpath</code>{.docutils .literal .notranslate}]{.pre})</p>
</li>
<li>
<p>[<code>x</code>{.docutils .literal .notranslate}]{.pre} (use
[<code>xpath</code>{.docutils .literal .notranslate}]{.pre})</p>
</li>
</ul>
</li>
<li>
<p>[<code>scrapy.spiders.BaseSpider</code>{.docutils .literal .notranslate}]{.pre}
(use <a href="index.html#scrapy.spiders.Spider" title="scrapy.spiders.Spider">[<code>Spider</code>{.xref .py .py-class .docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal})</p>
</li>
<li>
<p>From <a href="index.html#scrapy.spiders.Spider" title="scrapy.spiders.Spider">[<code>Spider</code>{.xref .py .py-class .docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal} (and subclasses):</p>
<ul>
<li>
<p>[<code>DOWNLOAD_DELAY</code>{.docutils .literal .notranslate}]{.pre} (use
<a href="index.html#spider-download-delay-attribute">[download_delay]{.std
.std-ref}</a>{.hoverxref
.tooltip .reference .internal})</p>
</li>
<li>
<p>[<code>set_crawler</code>{.docutils .literal .notranslate}]{.pre} (use
[<code>from_crawler()</code>{.xref .py .py-meth .docutils .literal
.notranslate}]{.pre})</p>
</li>
</ul>
</li>
<li>
<p>[<code>scrapy.spiders.spiders</code>{.docutils .literal .notranslate}]{.pre}
(use <a href="index.html#scrapy.spiderloader.SpiderLoader" title="scrapy.spiderloader.SpiderLoader">[<code>SpiderLoader</code>{.xref .py .py-class .docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal})</p>
</li>
<li>
<p>[<code>scrapy.telnet</code>{.docutils .literal .notranslate}]{.pre} (use
<a href="index.html#module-scrapy.extensions.telnet" title="scrapy.extensions.telnet: Telnet console">[<code>scrapy.extensions.telnet</code>{.xref .py .py-mod .docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal})</p>
</li>
<li>
<p>From [<code>scrapy.utils.python</code>{.docutils .literal .notranslate}]{.pre}:</p>
<ul>
<li>
<p>[<code>str_to_unicode</code>{.docutils .literal .notranslate}]{.pre} (use
[<code>to_unicode</code>{.docutils .literal .notranslate}]{.pre})</p>
</li>
<li>
<p>[<code>unicode_to_str</code>{.docutils .literal .notranslate}]{.pre} (use
[<code>to_bytes</code>{.docutils .literal .notranslate}]{.pre})</p>
</li>
</ul>
</li>
<li>
<p>[<code>scrapy.utils.response.body_or_str</code>{.docutils .literal
.notranslate}]{.pre}</p>
</li>
</ul>
<p>The following deprecated settings have also been removed (<a href="https://github.com/scrapy/scrapy/issues/3578">issue
3578</a>{.reference
.external}):</p>
<ul>
<li>[<code>SPIDER_MANAGER_CLASS</code>{.docutils .literal .notranslate}]{.pre} (use
<a href="index.html#std-setting-SPIDER_LOADER_CLASS">[<code>SPIDER_LOADER_CLASS</code>{.xref .std .std-setting .docutils .literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal})
:::</li>
</ul>
<p>::: {#id96 .section}
[]{#id97}</p>
<h5 id="deprecationsheaderlink-13"><a class="header" href="#deprecationsheaderlink-13">Deprecations<a href="#id96" title="Permalink to this heading">¶</a>{.headerlink}</a></h5>
<ul>
<li>
<p>The [<code>queuelib.PriorityQueue</code>{.docutils .literal
.notranslate}]{.pre} value for the
<a href="index.html#std-setting-SCHEDULER_PRIORITY_QUEUE">[<code>SCHEDULER_PRIORITY_QUEUE</code>{.xref .std .std-setting .docutils
.literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal} setting is deprecated. Use
[<code>scrapy.pqueues.ScrapyPriorityQueue</code>{.docutils .literal
.notranslate}]{.pre} instead.</p>
</li>
<li>
<p>[<code>process_request</code>{.docutils .literal .notranslate}]{.pre} callbacks
passed to <a href="index.html#scrapy.spiders.Rule" title="scrapy.spiders.Rule">[<code>Rule</code>{.xref .py .py-class .docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal} that do not accept two arguments are deprecated.</p>
</li>
<li>
<p>The following modules are deprecated:</p>
<ul>
<li>
<p>[<code>scrapy.utils.http</code>{.docutils .literal .notranslate}]{.pre}
(use
<a href="https://w3lib.readthedocs.io/en/latest/w3lib.html#module-w3lib.http">w3lib.http</a>{.reference
.external})</p>
</li>
<li>
<p>[<code>scrapy.utils.markup</code>{.docutils .literal .notranslate}]{.pre}
(use
<a href="https://w3lib.readthedocs.io/en/latest/w3lib.html#module-w3lib.html">w3lib.html</a>{.reference
.external})</p>
</li>
<li>
<p>[<code>scrapy.utils.multipart</code>{.docutils .literal
.notranslate}]{.pre} (use
<a href="https://urllib3.readthedocs.io/en/latest/index.html">urllib3</a>{.reference
.external})</p>
</li>
</ul>
</li>
<li>
<p>The [<code>scrapy.utils.datatypes.MergeDict</code>{.docutils .literal
.notranslate}]{.pre} class is deprecated for Python 3 code bases.
Use <a href="https://docs.python.org/3/library/collections.html#collections.ChainMap" title="(in Python v3.12)">[<code>ChainMap</code>{.xref .py .py-class .docutils .literal
.notranslate}]{.pre}</a>{.reference
.external} instead. (<a href="https://github.com/scrapy/scrapy/issues/3878">issue
3878</a>{.reference
.external})</p>
</li>
<li>
<p>The [<code>scrapy.utils.gz.is_gzipped</code>{.docutils .literal
.notranslate}]{.pre} function is deprecated. Use
[<code>scrapy.utils.gz.gzip_magic_number</code>{.docutils .literal
.notranslate}]{.pre} instead.
:::</p>
</li>
</ul>
<p>::: {#id98 .section}</p>
<h5 id="other-changesheaderlink-1"><a class="header" href="#other-changesheaderlink-1">Other changes<a href="#id98" title="Permalink to this heading">¶</a>{.headerlink}</a></h5>
<ul>
<li>
<p>It is now possible to run all tests from the same
<a href="https://pypi.org/project/tox/">tox</a>{.reference .external}
environment in parallel; the documentation now covers <a href="index.html#running-tests">[this and
other ways to run tests]{.std
.std-ref}</a>{.hoverxref .tooltip .reference
.internal} (<a href="https://github.com/scrapy/scrapy/issues/3707">issue
3707</a>{.reference
.external})</p>
</li>
<li>
<p>It is now possible to generate an API documentation coverage report
(<a href="https://github.com/scrapy/scrapy/issues/3806">issue
3806</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/3810">issue
3810</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/3860">issue
3860</a>{.reference
.external})</p>
</li>
<li>
<p>The <a href="index.html#documentation-policies">[documentation policies]{.std
.std-ref}</a>{.hoverxref .tooltip
.reference .internal} now require
<a href="https://docs.python.org/3/glossary.html#term-docstring">docstrings</a>{.reference
.external} (<a href="https://github.com/scrapy/scrapy/issues/3701">issue
3701</a>{.reference
.external}) that follow <a href="https://www.python.org/dev/peps/pep-0257/">PEP
257</a>{.reference
.external} (<a href="https://github.com/scrapy/scrapy/issues/3748">issue
3748</a>{.reference
.external})</p>
</li>
<li>
<p>Internal fixes and cleanup (<a href="https://github.com/scrapy/scrapy/issues/3629">issue
3629</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/3643">issue
3643</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/3684">issue
3684</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/3698">issue
3698</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/3734">issue
3734</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/3735">issue
3735</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/3736">issue
3736</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/3737">issue
3737</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/3809">issue
3809</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/3821">issue
3821</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/3825">issue
3825</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/3827">issue
3827</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/3833">issue
3833</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/3857">issue
3857</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/3877">issue
3877</a>{.reference
.external})
:::
:::</p>
</li>
</ul>
<p>::: {#scrapy-1-6-0-2019-01-30 .section}
[]{#release-1-6-0}</p>
<h4 id="scrapy-160-2019-01-30headerlink"><a class="header" href="#scrapy-160-2019-01-30headerlink">Scrapy 1.6.0 (2019-01-30)<a href="#scrapy-1-6-0-2019-01-30" title="Permalink to this heading">¶</a>{.headerlink}</a></h4>
<p>Highlights:</p>
<ul>
<li>
<p>better Windows support;</p>
</li>
<li>
<p>Python 3.7 compatibility;</p>
</li>
<li>
<p>big documentation improvements, including a switch from
[<code>.extract_first()</code>{.docutils .literal .notranslate}]{.pre} +
[<code>.extract()</code>{.docutils .literal .notranslate}]{.pre} API to
[<code>.get()</code>{.docutils .literal .notranslate}]{.pre} +
[<code>.getall()</code>{.docutils .literal .notranslate}]{.pre} API;</p>
</li>
<li>
<p>feed exports, FilePipeline and MediaPipeline improvements;</p>
</li>
<li>
<p>better extensibility: <a href="index.html#std-signal-item_error">[<code>item_error</code>{.xref .std .std-signal
.docutils .literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal} and
<a href="index.html#std-signal-request_reached_downloader">[<code>request_reached_downloader</code>{.xref .std .std-signal .docutils
.literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal} signals; [<code>from_crawler</code>{.docutils
.literal .notranslate}]{.pre} support for feed exporters, feed
storages and dupefilters.</p>
</li>
<li>
<p>[<code>scrapy.contracts</code>{.docutils .literal .notranslate}]{.pre} fixes
and new features;</p>
</li>
<li>
<p>telnet console security improvements, first released as a backport
in <a href="#release-1-5-2">[Scrapy 1.5.2 (2019-01-22)]{.std
.std-ref}</a>{.hoverxref .tooltip .reference
.internal};</p>
</li>
<li>
<p>clean-up of the deprecated code;</p>
</li>
<li>
<p>various bug fixes, small new features and usability improvements
across the codebase.</p>
</li>
</ul>
<p>::: {#selector-api-changes .section}</p>
<h5 id="selector-api-changesheaderlink"><a class="header" href="#selector-api-changesheaderlink">Selector API changes<a href="#selector-api-changes" title="Permalink to this heading">¶</a>{.headerlink}</a></h5>
<p>While these are not changes in Scrapy itself, but rather in the
<a href="https://github.com/scrapy/parsel">parsel</a>{.reference .external} library
which Scrapy uses for xpath/css selectors, these changes are worth
mentioning here. Scrapy now depends on parsel &gt;= 1.5, and Scrapy
documentation is updated to follow recent [<code>parsel</code>{.docutils .literal
.notranslate}]{.pre} API conventions.</p>
<p>Most visible change is that [<code>.get()</code>{.docutils .literal
.notranslate}]{.pre} and [<code>.getall()</code>{.docutils .literal
.notranslate}]{.pre} selector methods are now preferred over
[<code>.extract_first()</code>{.docutils .literal .notranslate}]{.pre} and
[<code>.extract()</code>{.docutils .literal .notranslate}]{.pre}. We feel that
these new methods result in a more concise and readable code. See
<a href="index.html#old-extraction-api">[extract() and extract_first()]{.std
.std-ref}</a>{.hoverxref .tooltip .reference
.internal} for more details.</p>
<p>::: {.admonition .note}
Note</p>
<p>There are currently <strong>no plans</strong> to deprecate [<code>.extract()</code>{.docutils
.literal .notranslate}]{.pre} and [<code>.extract_first()</code>{.docutils .literal
.notranslate}]{.pre} methods.
:::</p>
<p>Another useful new feature is the introduction of
[<code>Selector.attrib</code>{.docutils .literal .notranslate}]{.pre} and
[<code>SelectorList.attrib</code>{.docutils .literal .notranslate}]{.pre}
properties, which make it easier to get attributes of HTML elements. See
<a href="index.html#selecting-attributes">[Selecting element attributes]{.std
.std-ref}</a>{.hoverxref .tooltip
.reference .internal}.</p>
<p>CSS selectors are cached in parsel &gt;= 1.5, which makes them faster when
the same CSS path is used many times. This is very common in case of
Scrapy spiders: callbacks are usually called several times, on different
pages.</p>
<p>If you're using custom [<code>Selector</code>{.docutils .literal
.notranslate}]{.pre} or [<code>SelectorList</code>{.docutils .literal
.notranslate}]{.pre} subclasses, a <strong>backward incompatible</strong> change in
parsel may affect your code. See <a href="https://parsel.readthedocs.io/en/latest/history.html">parsel
changelog</a>{.reference
.external} for a detailed description, as well as for the full list of
improvements.
:::</p>
<p>::: {#telnet-console .section}</p>
<h5 id="telnet-consoleheaderlink-1"><a class="header" href="#telnet-consoleheaderlink-1">Telnet console<a href="#telnet-console" title="Permalink to this heading">¶</a>{.headerlink}</a></h5>
<p><strong>Backward incompatible</strong>: Scrapy's telnet console now requires username
and password. See <a href="index.html#topics-telnetconsole">[Telnet Console]{.std
.std-ref}</a>{.hoverxref .tooltip
.reference .internal} for more details. This change fixes a <strong>security
issue</strong>; see <a href="#release-1-5-2">[Scrapy 1.5.2 (2019-01-22)]{.std
.std-ref}</a>{.hoverxref .tooltip .reference .internal}
release notes for details.
:::</p>
<p>::: {#new-extensibility-features .section}</p>
<h5 id="new-extensibility-featuresheaderlink"><a class="header" href="#new-extensibility-featuresheaderlink">New extensibility features<a href="#new-extensibility-features" title="Permalink to this heading">¶</a>{.headerlink}</a></h5>
<ul>
<li>
<p>[<code>from_crawler</code>{.docutils .literal .notranslate}]{.pre} support is
added to feed exporters and feed storages. This, among other things,
allows to access Scrapy settings from custom feed storages and
exporters (<a href="https://github.com/scrapy/scrapy/issues/1605">issue
1605</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/3348">issue
3348</a>{.reference
.external}).</p>
</li>
<li>
<p>[<code>from_crawler</code>{.docutils .literal .notranslate}]{.pre} support is
added to dupefilters (<a href="https://github.com/scrapy/scrapy/issues/2956">issue
2956</a>{.reference
.external}); this allows to access e.g. settings or a spider from a
dupefilter.</p>
</li>
<li>
<p><a href="index.html#std-signal-item_error">[<code>item_error</code>{.xref .std .std-signal .docutils .literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal} is fired when an error happens in a
pipeline (<a href="https://github.com/scrapy/scrapy/issues/3256">issue
3256</a>{.reference
.external});</p>
</li>
<li>
<p><a href="index.html#std-signal-request_reached_downloader">[<code>request_reached_downloader</code>{.xref .std .std-signal .docutils
.literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal} is fired when Downloader gets a new
Request; this signal can be useful e.g. for custom Schedulers
(<a href="https://github.com/scrapy/scrapy/issues/3393">issue
3393</a>{.reference
.external}).</p>
</li>
<li>
<p>new SitemapSpider <a href="index.html#scrapy.spiders.SitemapSpider.sitemap_filter" title="scrapy.spiders.SitemapSpider.sitemap_filter">[<code>sitemap_filter()</code>{.xref .py .py-meth .docutils
.literal
.notranslate}]{.pre}</a>{.reference
.internal} method which allows to select sitemap entries based on
their attributes in SitemapSpider subclasses (<a href="https://github.com/scrapy/scrapy/issues/3512">issue
3512</a>{.reference
.external}).</p>
</li>
<li>
<p>Lazy loading of Downloader Handlers is now optional; this enables
better initialization error handling in custom Downloader Handlers
(<a href="https://github.com/scrapy/scrapy/issues/3394">issue
3394</a>{.reference
.external}).
:::</p>
</li>
</ul>
<p>::: {#new-filepipeline-and-mediapipeline-features .section}</p>
<h5 id="new-filepipeline-and-mediapipeline-featuresheaderlink"><a class="header" href="#new-filepipeline-and-mediapipeline-featuresheaderlink">New FilePipeline and MediaPipeline features<a href="#new-filepipeline-and-mediapipeline-features" title="Permalink to this heading">¶</a>{.headerlink}</a></h5>
<ul>
<li>
<p>Expose more options for S3FilesStore: <a href="index.html#std-setting-AWS_ENDPOINT_URL">[<code>AWS_ENDPOINT_URL</code>{.xref
.std .std-setting .docutils .literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal}, <a href="index.html#std-setting-AWS_USE_SSL">[<code>AWS_USE_SSL</code>{.xref .std
.std-setting .docutils .literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal}, <a href="index.html#std-setting-AWS_VERIFY">[<code>AWS_VERIFY</code>{.xref .std
.std-setting .docutils .literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal}, <a href="index.html#std-setting-AWS_REGION_NAME">[<code>AWS_REGION_NAME</code>{.xref .std
.std-setting .docutils .literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal}. For example, this allows to use
alternative or self-hosted AWS-compatible providers (<a href="https://github.com/scrapy/scrapy/issues/2609">issue
2609</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/3548">issue
3548</a>{.reference
.external}).</p>
</li>
<li>
<p>ACL support for Google Cloud Storage: <a href="index.html#std-setting-FILES_STORE_GCS_ACL">[<code>FILES_STORE_GCS_ACL</code>{.xref
.std .std-setting .docutils .literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal} and <a href="index.html#std-setting-IMAGES_STORE_GCS_ACL">[<code>IMAGES_STORE_GCS_ACL</code>{.xref
.std .std-setting .docutils .literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal} (<a href="https://github.com/scrapy/scrapy/issues/3199">issue
3199</a>{.reference
.external}).
:::</p>
</li>
</ul>
<p>::: {#scrapy-contracts-improvements .section}</p>
<h5 id="scrapycontractsdocutils-literal-notranslatepre-improvementsheaderlink"><a class="header" href="#scrapycontractsdocutils-literal-notranslatepre-improvementsheaderlink">[<code>scrapy.contracts</code>{.docutils .literal .notranslate}]{.pre} improvements<a href="#scrapy-contracts-improvements" title="Permalink to this heading">¶</a>{.headerlink}</a></h5>
<ul>
<li>
<p>Exceptions in contracts code are handled better (<a href="https://github.com/scrapy/scrapy/issues/3377">issue
3377</a>{.reference
.external});</p>
</li>
<li>
<p>[<code>dont_filter=True</code>{.docutils .literal .notranslate}]{.pre} is used
for contract requests, which allows to test different callbacks with
the same URL (<a href="https://github.com/scrapy/scrapy/issues/3381">issue
3381</a>{.reference
.external});</p>
</li>
<li>
<p>[<code>request_cls</code>{.docutils .literal .notranslate}]{.pre} attribute in
Contract subclasses allow to use different Request classes in
contracts, for example FormRequest (<a href="https://github.com/scrapy/scrapy/issues/3383">issue
3383</a>{.reference
.external}).</p>
</li>
<li>
<p>Fixed errback handling in contracts, e.g. for cases where a contract
is executed for URL which returns non-200 response (<a href="https://github.com/scrapy/scrapy/issues/3371">issue
3371</a>{.reference
.external}).
:::</p>
</li>
</ul>
<p>::: {#usability-improvements .section}</p>
<h5 id="usability-improvementsheaderlink"><a class="header" href="#usability-improvementsheaderlink">Usability improvements<a href="#usability-improvements" title="Permalink to this heading">¶</a>{.headerlink}</a></h5>
<ul>
<li>
<p>more stats for RobotsTxtMiddleware (<a href="https://github.com/scrapy/scrapy/issues/3100">issue
3100</a>{.reference
.external})</p>
</li>
<li>
<p>INFO log level is used to show telnet host/port (<a href="https://github.com/scrapy/scrapy/issues/3115">issue
3115</a>{.reference
.external})</p>
</li>
<li>
<p>a message is added to IgnoreRequest in RobotsTxtMiddleware (<a href="https://github.com/scrapy/scrapy/issues/3113">issue
3113</a>{.reference
.external})</p>
</li>
<li>
<p>better validation of [<code>url</code>{.docutils .literal .notranslate}]{.pre}
argument in [<code>Response.follow</code>{.docutils .literal
.notranslate}]{.pre} (<a href="https://github.com/scrapy/scrapy/issues/3131">issue
3131</a>{.reference
.external})</p>
</li>
<li>
<p>non-zero exit code is returned from Scrapy commands when error
happens on spider initialization (<a href="https://github.com/scrapy/scrapy/issues/3226">issue
3226</a>{.reference
.external})</p>
</li>
<li>
<p>Link extraction improvements: &quot;ftp&quot; is added to scheme list (<a href="https://github.com/scrapy/scrapy/issues/3152">issue
3152</a>{.reference
.external}); &quot;flv&quot; is added to common video extensions (<a href="https://github.com/scrapy/scrapy/issues/3165">issue
3165</a>{.reference
.external})</p>
</li>
<li>
<p>better error message when an exporter is disabled (<a href="https://github.com/scrapy/scrapy/issues/3358">issue
3358</a>{.reference
.external});</p>
</li>
<li>
<p>[<code>scrapy</code>{.docutils .literal .notranslate}]{.pre}<code> </code>{.docutils
.literal .notranslate}[<code>shell</code>{.docutils .literal
.notranslate}]{.pre}<code> </code>{.docutils .literal
.notranslate}[<code>--help</code>{.docutils .literal .notranslate}]{.pre}
mentions syntax required for local files ([<code>./file.html</code>{.docutils
.literal .notranslate}]{.pre}) - <a href="https://github.com/scrapy/scrapy/issues/3496">issue
3496</a>{.reference
.external}.</p>
</li>
<li>
<p>Referer header value is added to RFPDupeFilter log messages (<a href="https://github.com/scrapy/scrapy/issues/3588">issue
3588</a>{.reference
.external})
:::</p>
</li>
</ul>
<p>::: {#id99 .section}</p>
<h5 id="bug-fixesheaderlink-15"><a class="header" href="#bug-fixesheaderlink-15">Bug fixes<a href="#id99" title="Permalink to this heading">¶</a>{.headerlink}</a></h5>
<ul>
<li>
<p>fixed issue with extra blank lines in .csv exports under Windows
(<a href="https://github.com/scrapy/scrapy/issues/3039">issue
3039</a>{.reference
.external});</p>
</li>
<li>
<p>proper handling of pickling errors in Python 3 when serializing
objects for disk queues (<a href="https://github.com/scrapy/scrapy/issues/3082">issue
3082</a>{.reference
.external})</p>
</li>
<li>
<p>flags are now preserved when copying Requests (<a href="https://github.com/scrapy/scrapy/issues/3342">issue
3342</a>{.reference
.external});</p>
</li>
<li>
<p>FormRequest.from_response clickdata shouldn't ignore elements with
[<code>input[type=image]</code>{.docutils .literal .notranslate}]{.pre} (<a href="https://github.com/scrapy/scrapy/issues/3153">issue
3153</a>{.reference
.external}).</p>
</li>
<li>
<p>FormRequest.from_response should preserve duplicate keys (<a href="https://github.com/scrapy/scrapy/issues/3247">issue
3247</a>{.reference
.external})
:::</p>
</li>
</ul>
<p>::: {#documentation-improvements .section}</p>
<h5 id="documentation-improvementsheaderlink"><a class="header" href="#documentation-improvementsheaderlink">Documentation improvements<a href="#documentation-improvements" title="Permalink to this heading">¶</a>{.headerlink}</a></h5>
<ul>
<li>
<p>Docs are re-written to suggest .get/.getall API instead of
.extract/.extract_first. Also, <a href="index.html#topics-selectors">[Selectors]{.std
.std-ref}</a>{.hoverxref .tooltip
.reference .internal} docs are updated and re-structured to match
latest parsel docs; they now contain more topics, such as
<a href="index.html#selecting-attributes">[Selecting element attributes]{.std
.std-ref}</a>{.hoverxref .tooltip
.reference .internal} or <a href="index.html#topics-selectors-css-extensions">[Extensions to CSS Selectors]{.std
.std-ref}</a>{.hoverxref
.tooltip .reference .internal} (<a href="https://github.com/scrapy/scrapy/issues/3390">issue
3390</a>{.reference
.external}).</p>
</li>
<li>
<p><a href="index.html#topics-developer-tools">[Using your browser's Developer Tools for scraping]{.std
.std-ref}</a>{.hoverxref .tooltip
.reference .internal} is a new tutorial which replaces old Firefox
and Firebug tutorials (<a href="https://github.com/scrapy/scrapy/issues/3400">issue
3400</a>{.reference
.external}).</p>
</li>
<li>
<p>SCRAPY_PROJECT environment variable is documented (<a href="https://github.com/scrapy/scrapy/issues/3518">issue
3518</a>{.reference
.external});</p>
</li>
<li>
<p>troubleshooting section is added to install instructions (<a href="https://github.com/scrapy/scrapy/issues/3517">issue
3517</a>{.reference
.external});</p>
</li>
<li>
<p>improved links to beginner resources in the tutorial (<a href="https://github.com/scrapy/scrapy/issues/3367">issue
3367</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/3468">issue
3468</a>{.reference
.external});</p>
</li>
<li>
<p>fixed <a href="index.html#std-setting-RETRY_HTTP_CODES">[<code>RETRY_HTTP_CODES</code>{.xref .std .std-setting .docutils
.literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal} default values in docs (<a href="https://github.com/scrapy/scrapy/issues/3335">issue
3335</a>{.reference
.external});</p>
</li>
<li>
<p>remove unused [<code>DEPTH_STATS</code>{.docutils .literal .notranslate}]{.pre}
option from docs (<a href="https://github.com/scrapy/scrapy/issues/3245">issue
3245</a>{.reference
.external});</p>
</li>
<li>
<p>other cleanups (<a href="https://github.com/scrapy/scrapy/issues/3347">issue
3347</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/3350">issue
3350</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/3445">issue
3445</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/3544">issue
3544</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/3605">issue
3605</a>{.reference
.external}).
:::</p>
</li>
</ul>
<p>::: {#id100 .section}</p>
<h5 id="deprecation-removalsheaderlink-11"><a class="header" href="#deprecation-removalsheaderlink-11">Deprecation removals<a href="#id100" title="Permalink to this heading">¶</a>{.headerlink}</a></h5>
<p>Compatibility shims for pre-1.0 Scrapy module names are removed (<a href="https://github.com/scrapy/scrapy/issues/3318">issue
3318</a>{.reference
.external}):</p>
<ul>
<li>
<p>[<code>scrapy.command</code>{.docutils .literal .notranslate}]{.pre}</p>
</li>
<li>
<p>[<code>scrapy.contrib</code>{.docutils .literal .notranslate}]{.pre} (with all
submodules)</p>
</li>
<li>
<p>[<code>scrapy.contrib_exp</code>{.docutils .literal .notranslate}]{.pre} (with
all submodules)</p>
</li>
<li>
<p>[<code>scrapy.dupefilter</code>{.docutils .literal .notranslate}]{.pre}</p>
</li>
<li>
<p>[<code>scrapy.linkextractor</code>{.docutils .literal .notranslate}]{.pre}</p>
</li>
<li>
<p>[<code>scrapy.project</code>{.docutils .literal .notranslate}]{.pre}</p>
</li>
<li>
<p>[<code>scrapy.spider</code>{.docutils .literal .notranslate}]{.pre}</p>
</li>
<li>
<p>[<code>scrapy.spidermanager</code>{.docutils .literal .notranslate}]{.pre}</p>
</li>
<li>
<p>[<code>scrapy.squeue</code>{.docutils .literal .notranslate}]{.pre}</p>
</li>
<li>
<p>[<code>scrapy.stats</code>{.docutils .literal .notranslate}]{.pre}</p>
</li>
<li>
<p>[<code>scrapy.statscol</code>{.docutils .literal .notranslate}]{.pre}</p>
</li>
<li>
<p>[<code>scrapy.utils.decorator</code>{.docutils .literal .notranslate}]{.pre}</p>
</li>
</ul>
<p>See <a href="#module-relocations">[Module Relocations]{.std
.std-ref}</a>{.hoverxref .tooltip .reference
.internal} for more information, or use suggestions from Scrapy 1.5.x
deprecation warnings to update your code.</p>
<p>Other deprecation removals:</p>
<ul>
<li>
<p>Deprecated scrapy.interfaces.ISpiderManager is removed; please use
scrapy.interfaces.ISpiderLoader.</p>
</li>
<li>
<p>Deprecated [<code>CrawlerSettings</code>{.docutils .literal
.notranslate}]{.pre} class is removed (<a href="https://github.com/scrapy/scrapy/issues/3327">issue
3327</a>{.reference
.external}).</p>
</li>
<li>
<p>Deprecated [<code>Settings.overrides</code>{.docutils .literal
.notranslate}]{.pre} and [<code>Settings.defaults</code>{.docutils .literal
.notranslate}]{.pre} attributes are removed (<a href="https://github.com/scrapy/scrapy/issues/3327">issue
3327</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/3359">issue
3359</a>{.reference
.external}).
:::</p>
</li>
</ul>
<p>::: {#other-improvements-cleanups .section}</p>
<h5 id="other-improvements-cleanupsheaderlink"><a class="header" href="#other-improvements-cleanupsheaderlink">Other improvements, cleanups<a href="#other-improvements-cleanups" title="Permalink to this heading">¶</a>{.headerlink}</a></h5>
<ul>
<li>
<p>All Scrapy tests now pass on Windows; Scrapy testing suite is
executed in a Windows environment on CI (<a href="https://github.com/scrapy/scrapy/issues/3315">issue
3315</a>{.reference
.external}).</p>
</li>
<li>
<p>Python 3.7 support (<a href="https://github.com/scrapy/scrapy/issues/3326">issue
3326</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/3150">issue
3150</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/3547">issue
3547</a>{.reference
.external}).</p>
</li>
<li>
<p>Testing and CI fixes (<a href="https://github.com/scrapy/scrapy/issues/3526">issue
3526</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/3538">issue
3538</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/3308">issue
3308</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/3311">issue
3311</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/3309">issue
3309</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/3305">issue
3305</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/3210">issue
3210</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/3299">issue
3299</a>{.reference
.external})</p>
</li>
<li>
<p>[<code>scrapy.http.cookies.CookieJar.clear</code>{.docutils .literal
.notranslate}]{.pre} accepts &quot;domain&quot;, &quot;path&quot; and &quot;name&quot; optional
arguments (<a href="https://github.com/scrapy/scrapy/issues/3231">issue
3231</a>{.reference
.external}).</p>
</li>
<li>
<p>additional files are included to sdist (<a href="https://github.com/scrapy/scrapy/issues/3495">issue
3495</a>{.reference
.external});</p>
</li>
<li>
<p>code style fixes (<a href="https://github.com/scrapy/scrapy/issues/3405">issue
3405</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/3304">issue
3304</a>{.reference
.external});</p>
</li>
<li>
<p>unneeded .strip() call is removed (<a href="https://github.com/scrapy/scrapy/issues/3519">issue
3519</a>{.reference
.external});</p>
</li>
<li>
<p>collections.deque is used to store MiddlewareManager methods instead
of a list (<a href="https://github.com/scrapy/scrapy/issues/3476">issue
3476</a>{.reference
.external})
:::
:::</p>
</li>
</ul>
<p>::: {#scrapy-1-5-2-2019-01-22 .section}
[]{#release-1-5-2}</p>
<h4 id="scrapy-152-2019-01-22headerlink"><a class="header" href="#scrapy-152-2019-01-22headerlink">Scrapy 1.5.2 (2019-01-22)<a href="#scrapy-1-5-2-2019-01-22" title="Permalink to this heading">¶</a>{.headerlink}</a></h4>
<ul>
<li>
<p><em>Security bugfix</em>: Telnet console extension can be easily exploited
by rogue websites POSTing content to
<a href="http://localhost:6023">http://localhost:6023</a>{.reference
.external}, we haven't found a way to exploit it from Scrapy, but it
is very easy to trick a browser to do so and elevates the risk for
local development environment.</p>
<p><em>The fix is backward incompatible</em>, it enables telnet user-password
authentication by default with a random generated password. If you
can't upgrade right away, please consider setting
<a href="index.html#std-setting-TELNETCONSOLE_PORT">[<code>TELNETCONSOLE_PORT</code>{.xref .std .std-setting .docutils .literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal} out of its default value.</p>
<p>See <a href="index.html#topics-telnetconsole">[telnet console]{.std
.std-ref}</a>{.hoverxref .tooltip
.reference .internal} documentation for more info</p>
</li>
<li>
<p>Backport CI build failure under GCE environment due to boto import
error.
:::</p>
</li>
</ul>
<p>::: {#scrapy-1-5-1-2018-07-12 .section}
[]{#release-1-5-1}</p>
<h4 id="scrapy-151-2018-07-12headerlink"><a class="header" href="#scrapy-151-2018-07-12headerlink">Scrapy 1.5.1 (2018-07-12)<a href="#scrapy-1-5-1-2018-07-12" title="Permalink to this heading">¶</a>{.headerlink}</a></h4>
<p>This is a maintenance release with important bug fixes, but no new
features:</p>
<ul>
<li>
<p>[<code>O(N^2)</code>{.docutils .literal .notranslate}]{.pre} gzip decompression
issue which affected Python 3 and PyPy is fixed (<a href="https://github.com/scrapy/scrapy/issues/3281">issue
3281</a>{.reference
.external});</p>
</li>
<li>
<p>skipping of TLS validation errors is improved (<a href="https://github.com/scrapy/scrapy/issues/3166">issue
3166</a>{.reference
.external});</p>
</li>
<li>
<p>Ctrl-C handling is fixed in Python 3.5+ (<a href="https://github.com/scrapy/scrapy/issues/3096">issue
3096</a>{.reference
.external});</p>
</li>
<li>
<p>testing fixes (<a href="https://github.com/scrapy/scrapy/issues/3092">issue
3092</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/3263">issue
3263</a>{.reference
.external});</p>
</li>
<li>
<p>documentation improvements (<a href="https://github.com/scrapy/scrapy/issues/3058">issue
3058</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/3059">issue
3059</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/3089">issue
3089</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/3123">issue
3123</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/3127">issue
3127</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/3189">issue
3189</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/3224">issue
3224</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/3280">issue
3280</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/3279">issue
3279</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/3201">issue
3201</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/3260">issue
3260</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/3284">issue
3284</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/3298">issue
3298</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/3294">issue
3294</a>{.reference
.external}).
:::</p>
</li>
</ul>
<p>::: {#scrapy-1-5-0-2017-12-29 .section}
[]{#release-1-5-0}</p>
<h4 id="scrapy-150-2017-12-29headerlink"><a class="header" href="#scrapy-150-2017-12-29headerlink">Scrapy 1.5.0 (2017-12-29)<a href="#scrapy-1-5-0-2017-12-29" title="Permalink to this heading">¶</a>{.headerlink}</a></h4>
<p>This release brings small new features and improvements across the
codebase. Some highlights:</p>
<ul>
<li>
<p>Google Cloud Storage is supported in FilesPipeline and
ImagesPipeline.</p>
</li>
<li>
<p>Crawling with proxy servers becomes more efficient, as connections
to proxies can be reused now.</p>
</li>
<li>
<p>Warnings, exception and logging messages are improved to make
debugging easier.</p>
</li>
<li>
<p>[<code>scrapy</code>{.docutils .literal .notranslate}]{.pre}<code> </code>{.docutils
.literal .notranslate}[<code>parse</code>{.docutils .literal
.notranslate}]{.pre} command now allows to set custom request meta
via [<code>--meta</code>{.docutils .literal .notranslate}]{.pre} argument.</p>
</li>
<li>
<p>Compatibility with Python 3.6, PyPy and PyPy3 is improved; PyPy and
PyPy3 are now supported officially, by running tests on CI.</p>
</li>
<li>
<p>Better default handling of HTTP 308, 522 and 524 status codes.</p>
</li>
<li>
<p>Documentation is improved, as usual.</p>
</li>
</ul>
<p>::: {#id101 .section}</p>
<h5 id="backward-incompatible-changesheaderlink-9"><a class="header" href="#backward-incompatible-changesheaderlink-9">Backward Incompatible Changes<a href="#id101" title="Permalink to this heading">¶</a>{.headerlink}</a></h5>
<ul>
<li>
<p>Scrapy 1.5 drops support for Python 3.3.</p>
</li>
<li>
<p>Default Scrapy User-Agent now uses https link to scrapy.org (<a href="https://github.com/scrapy/scrapy/issues/2983">issue
2983</a>{.reference
.external}). <strong>This is technically backward-incompatible</strong>; override
<a href="index.html#std-setting-USER_AGENT">[<code>USER_AGENT</code>{.xref .std .std-setting .docutils .literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal} if you relied on old value.</p>
</li>
<li>
<p>Logging of settings overridden by [<code>custom_settings</code>{.docutils
.literal .notranslate}]{.pre} is fixed; <strong>this is technically
backward-incompatible</strong> because the logger changes from
[<code>[scrapy.utils.log]</code>{.docutils .literal .notranslate}]{.pre} to
[<code>[scrapy.crawler]</code>{.docutils .literal .notranslate}]{.pre}. If
you're parsing Scrapy logs, please update your log parsers (<a href="https://github.com/scrapy/scrapy/issues/1343">issue
1343</a>{.reference
.external}).</p>
</li>
<li>
<p>LinkExtractor now ignores [<code>m4v</code>{.docutils .literal
.notranslate}]{.pre} extension by default, this is change in
behavior.</p>
</li>
<li>
<p>522 and 524 status codes are added to [<code>RETRY_HTTP_CODES</code>{.docutils
.literal .notranslate}]{.pre} (<a href="https://github.com/scrapy/scrapy/issues/2851">issue
2851</a>{.reference
.external})
:::</p>
</li>
</ul>
<p>::: {#id102 .section}</p>
<h5 id="new-featuresheaderlink-15"><a class="header" href="#new-featuresheaderlink-15">New features<a href="#id102" title="Permalink to this heading">¶</a>{.headerlink}</a></h5>
<ul>
<li>
<p>Support [<code>&lt;link&gt;</code>{.docutils .literal .notranslate}]{.pre} tags in
[<code>Response.follow</code>{.docutils .literal .notranslate}]{.pre} (<a href="https://github.com/scrapy/scrapy/issues/2785">issue
2785</a>{.reference
.external})</p>
</li>
<li>
<p>Support for [<code>ptpython</code>{.docutils .literal .notranslate}]{.pre} REPL
(<a href="https://github.com/scrapy/scrapy/issues/2654">issue
2654</a>{.reference
.external})</p>
</li>
<li>
<p>Google Cloud Storage support for FilesPipeline and ImagesPipeline
(<a href="https://github.com/scrapy/scrapy/issues/2923">issue
2923</a>{.reference
.external}).</p>
</li>
<li>
<p>New [<code>--meta</code>{.docutils .literal .notranslate}]{.pre} option of the
&quot;scrapy parse&quot; command allows to pass additional request.meta
(<a href="https://github.com/scrapy/scrapy/issues/2883">issue
2883</a>{.reference
.external})</p>
</li>
<li>
<p>Populate spider variable when using
[<code>shell.inspect_response</code>{.docutils .literal .notranslate}]{.pre}
(<a href="https://github.com/scrapy/scrapy/issues/2812">issue
2812</a>{.reference
.external})</p>
</li>
<li>
<p>Handle HTTP 308 Permanent Redirect (<a href="https://github.com/scrapy/scrapy/issues/2844">issue
2844</a>{.reference
.external})</p>
</li>
<li>
<p>Add 522 and 524 to [<code>RETRY_HTTP_CODES</code>{.docutils .literal
.notranslate}]{.pre} (<a href="https://github.com/scrapy/scrapy/issues/2851">issue
2851</a>{.reference
.external})</p>
</li>
<li>
<p>Log versions information at startup (<a href="https://github.com/scrapy/scrapy/issues/2857">issue
2857</a>{.reference
.external})</p>
</li>
<li>
<p>[<code>scrapy.mail.MailSender</code>{.docutils .literal .notranslate}]{.pre}
now works in Python 3 (it requires Twisted 17.9.0)</p>
</li>
<li>
<p>Connections to proxy servers are reused (<a href="https://github.com/scrapy/scrapy/issues/2743">issue
2743</a>{.reference
.external})</p>
</li>
<li>
<p>Add template for a downloader middleware (<a href="https://github.com/scrapy/scrapy/issues/2755">issue
2755</a>{.reference
.external})</p>
</li>
<li>
<p>Explicit message for NotImplementedError when parse callback not
defined (<a href="https://github.com/scrapy/scrapy/issues/2831">issue
2831</a>{.reference
.external})</p>
</li>
<li>
<p>CrawlerProcess got an option to disable installation of root log
handler (<a href="https://github.com/scrapy/scrapy/issues/2921">issue
2921</a>{.reference
.external})</p>
</li>
<li>
<p>LinkExtractor now ignores [<code>m4v</code>{.docutils .literal
.notranslate}]{.pre} extension by default</p>
</li>
<li>
<p>Better log messages for responses over <a href="index.html#std-setting-DOWNLOAD_WARNSIZE">[<code>DOWNLOAD_WARNSIZE</code>{.xref
.std .std-setting .docutils .literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal} and <a href="index.html#std-setting-DOWNLOAD_MAXSIZE">[<code>DOWNLOAD_MAXSIZE</code>{.xref .std
.std-setting .docutils .literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal} limits (<a href="https://github.com/scrapy/scrapy/issues/2927">issue
2927</a>{.reference
.external})</p>
</li>
<li>
<p>Show warning when a URL is put to
[<code>Spider.allowed_domains</code>{.docutils .literal .notranslate}]{.pre}
instead of a domain (<a href="https://github.com/scrapy/scrapy/issues/2250">issue
2250</a>{.reference
.external}).
:::</p>
</li>
</ul>
<p>::: {#id103 .section}</p>
<h5 id="bug-fixesheaderlink-16"><a class="header" href="#bug-fixesheaderlink-16">Bug fixes<a href="#id103" title="Permalink to this heading">¶</a>{.headerlink}</a></h5>
<ul>
<li>
<p>Fix logging of settings overridden by [<code>custom_settings</code>{.docutils
.literal .notranslate}]{.pre}; <strong>this is technically
backward-incompatible</strong> because the logger changes from
[<code>[scrapy.utils.log]</code>{.docutils .literal .notranslate}]{.pre} to
[<code>[scrapy.crawler]</code>{.docutils .literal .notranslate}]{.pre}, so
please update your log parsers if needed (<a href="https://github.com/scrapy/scrapy/issues/1343">issue
1343</a>{.reference
.external})</p>
</li>
<li>
<p>Default Scrapy User-Agent now uses https link to scrapy.org (<a href="https://github.com/scrapy/scrapy/issues/2983">issue
2983</a>{.reference
.external}). <strong>This is technically backward-incompatible</strong>; override
<a href="index.html#std-setting-USER_AGENT">[<code>USER_AGENT</code>{.xref .std .std-setting .docutils .literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal} if you relied on old value.</p>
</li>
<li>
<p>Fix PyPy and PyPy3 test failures, support them officially (<a href="https://github.com/scrapy/scrapy/issues/2793">issue
2793</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/2935">issue
2935</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/2990">issue
2990</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/3050">issue
3050</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/2213">issue
2213</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/3048">issue
3048</a>{.reference
.external})</p>
</li>
<li>
<p>Fix DNS resolver when [<code>DNSCACHE_ENABLED=False</code>{.docutils .literal
.notranslate}]{.pre} (<a href="https://github.com/scrapy/scrapy/issues/2811">issue
2811</a>{.reference
.external})</p>
</li>
<li>
<p>Add [<code>cryptography</code>{.docutils .literal .notranslate}]{.pre} for
Debian Jessie tox test env (<a href="https://github.com/scrapy/scrapy/issues/2848">issue
2848</a>{.reference
.external})</p>
</li>
<li>
<p>Add verification to check if Request callback is callable (<a href="https://github.com/scrapy/scrapy/issues/2766">issue
2766</a>{.reference
.external})</p>
</li>
<li>
<p>Port [<code>extras/qpsclient.py</code>{.docutils .literal .notranslate}]{.pre}
to Python 3 (<a href="https://github.com/scrapy/scrapy/issues/2849">issue
2849</a>{.reference
.external})</p>
</li>
<li>
<p>Use getfullargspec under the scenes for Python 3 to stop
DeprecationWarning (<a href="https://github.com/scrapy/scrapy/issues/2862">issue
2862</a>{.reference
.external})</p>
</li>
<li>
<p>Update deprecated test aliases (<a href="https://github.com/scrapy/scrapy/issues/2876">issue
2876</a>{.reference
.external})</p>
</li>
<li>
<p>Fix [<code>SitemapSpider</code>{.docutils .literal .notranslate}]{.pre} support
for alternate links (<a href="https://github.com/scrapy/scrapy/issues/2853">issue
2853</a>{.reference
.external})
:::</p>
</li>
</ul>
<p>::: {#docs .section}</p>
<h5 id="docsheaderlink"><a class="header" href="#docsheaderlink">Docs<a href="#docs" title="Permalink to this heading">¶</a>{.headerlink}</a></h5>
<ul>
<li>
<p>Added missing bullet point for the
[<code>AUTOTHROTTLE_TARGET_CONCURRENCY</code>{.docutils .literal
.notranslate}]{.pre} setting. (<a href="https://github.com/scrapy/scrapy/issues/2756">issue
2756</a>{.reference
.external})</p>
</li>
<li>
<p>Update Contributing docs, document new support channels (<a href="https://github.com/scrapy/scrapy/issues/2762">issue
2762</a>{.reference
.external}, issue:3038)</p>
</li>
<li>
<p>Include references to Scrapy subreddit in the docs</p>
</li>
<li>
<p>Fix broken links; use <a href="https://">https://</a>{.reference .external} for
external links (<a href="https://github.com/scrapy/scrapy/issues/2978">issue
2978</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/2982">issue
2982</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/2958">issue
2958</a>{.reference
.external})</p>
</li>
<li>
<p>Document CloseSpider extension better (<a href="https://github.com/scrapy/scrapy/issues/2759">issue
2759</a>{.reference
.external})</p>
</li>
<li>
<p>Use [<code>pymongo.collection.Collection.insert_one()</code>{.docutils .literal
.notranslate}]{.pre} in MongoDB example (<a href="https://github.com/scrapy/scrapy/issues/2781">issue
2781</a>{.reference
.external})</p>
</li>
<li>
<p>Spelling mistake and typos (<a href="https://github.com/scrapy/scrapy/issues/2828">issue
2828</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/2837">issue
2837</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/2884">issue
2884</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/2924">issue
2924</a>{.reference
.external})</p>
</li>
<li>
<p>Clarify [<code>CSVFeedSpider.headers</code>{.docutils .literal
.notranslate}]{.pre} documentation (<a href="https://github.com/scrapy/scrapy/issues/2826">issue
2826</a>{.reference
.external})</p>
</li>
<li>
<p>Document [<code>DontCloseSpider</code>{.docutils .literal .notranslate}]{.pre}
exception and clarify [<code>spider_idle</code>{.docutils .literal
.notranslate}]{.pre} (<a href="https://github.com/scrapy/scrapy/issues/2791">issue
2791</a>{.reference
.external})</p>
</li>
<li>
<p>Update &quot;Releases&quot; section in README (<a href="https://github.com/scrapy/scrapy/issues/2764">issue
2764</a>{.reference
.external})</p>
</li>
<li>
<p>Fix rst syntax in [<code>DOWNLOAD_FAIL_ON_DATALOSS</code>{.docutils .literal
.notranslate}]{.pre} docs (<a href="https://github.com/scrapy/scrapy/issues/2763">issue
2763</a>{.reference
.external})</p>
</li>
<li>
<p>Small fix in description of startproject arguments (<a href="https://github.com/scrapy/scrapy/issues/2866">issue
2866</a>{.reference
.external})</p>
</li>
<li>
<p>Clarify data types in Response.body docs (<a href="https://github.com/scrapy/scrapy/issues/2922">issue
2922</a>{.reference
.external})</p>
</li>
<li>
<p>Add a note about [<code>request.meta['depth']</code>{.docutils .literal
.notranslate}]{.pre} to DepthMiddleware docs (<a href="https://github.com/scrapy/scrapy/issues/2374">issue
2374</a>{.reference
.external})</p>
</li>
<li>
<p>Add a note about [<code>request.meta['dont_merge_cookies']</code>{.docutils
.literal .notranslate}]{.pre} to CookiesMiddleware docs (<a href="https://github.com/scrapy/scrapy/issues/2999">issue
2999</a>{.reference
.external})</p>
</li>
<li>
<p>Up-to-date example of project structure (<a href="https://github.com/scrapy/scrapy/issues/2964">issue
2964</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/2976">issue
2976</a>{.reference
.external})</p>
</li>
<li>
<p>A better example of ItemExporters usage (<a href="https://github.com/scrapy/scrapy/issues/2989">issue
2989</a>{.reference
.external})</p>
</li>
<li>
<p>Document [<code>from_crawler</code>{.docutils .literal .notranslate}]{.pre}
methods for spider and downloader middlewares (<a href="https://github.com/scrapy/scrapy/issues/3019">issue
3019</a>{.reference
.external})
:::
:::</p>
</li>
</ul>
<p>::: {#scrapy-1-4-0-2017-05-18 .section}
[]{#release-1-4-0}</p>
<h4 id="scrapy-140-2017-05-18headerlink"><a class="header" href="#scrapy-140-2017-05-18headerlink">Scrapy 1.4.0 (2017-05-18)<a href="#scrapy-1-4-0-2017-05-18" title="Permalink to this heading">¶</a>{.headerlink}</a></h4>
<p>Scrapy 1.4 does not bring that many breathtaking new features but quite
a few handy improvements nonetheless.</p>
<p>Scrapy now supports anonymous FTP sessions with customizable user and
password via the new <a href="index.html#std-setting-FTP_USER">[<code>FTP_USER</code>{.xref .std .std-setting .docutils
.literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal} and <a href="index.html#std-setting-FTP_PASSWORD">[<code>FTP_PASSWORD</code>{.xref .std
.std-setting .docutils .literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal} settings. And if you're using Twisted
version 17.1.0 or above, FTP is now available with Python 3.</p>
<p>There's a new <a href="index.html#scrapy.http.TextResponse.follow" title="scrapy.http.TextResponse.follow">[<code>response.follow</code>{.xref .py .py-meth .docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal} method for creating requests; <strong>it is now a recommended way
to create Requests in Scrapy spiders</strong>. This method makes it easier to
write correct spiders; [<code>response.follow</code>{.docutils .literal
.notranslate}]{.pre} has several advantages over creating
[<code>scrapy.Request</code>{.docutils .literal .notranslate}]{.pre} objects
directly:</p>
<ul>
<li>
<p>it handles relative URLs;</p>
</li>
<li>
<p>it works properly with non-ascii URLs on non-UTF8 pages;</p>
</li>
<li>
<p>in addition to absolute and relative URLs it supports Selectors; for
[<code>&lt;a&gt;</code>{.docutils .literal .notranslate}]{.pre} elements it can also
extract their href values.</p>
</li>
</ul>
<p>For example, instead of this:</p>
<p>::: {.highlight-default .notranslate}
::: highlight
for href in response.css('li.page a::attr(href)').extract():
url = response.urljoin(href)
yield scrapy.Request(url, self.parse, encoding=response.encoding)
:::
:::</p>
<p>One can now write this:</p>
<p>::: {.highlight-default .notranslate}
::: highlight
for a in response.css('li.page a'):
yield response.follow(a, self.parse)
:::
:::</p>
<p>Link extractors are also improved. They work similarly to what a regular
modern browser would do: leading and trailing whitespace are removed
from attributes (think [<code>href=&quot;</code>{.docutils .literal
.notranslate}]{.pre}<code>   </code>{.docutils .literal
.notranslate}[<code>http://example.com&quot;</code>{.docutils .literal
.notranslate}]{.pre}) when building [<code>Link</code>{.docutils .literal
.notranslate}]{.pre} objects. This whitespace-stripping also happens for
[<code>action</code>{.docutils .literal .notranslate}]{.pre} attributes with
[<code>FormRequest</code>{.docutils .literal .notranslate}]{.pre}.</p>
<p><strong>Please also note that link extractors do not canonicalize URLs by
default anymore.</strong> This was puzzling users every now and then, and it's
not what browsers do in fact, so we removed that extra transformation on
extracted links.</p>
<p>For those of you wanting more control on the [<code>Referer:</code>{.docutils
.literal .notranslate}]{.pre} header that Scrapy sends when following
links, you can set your own [<code>Referrer</code>{.docutils .literal
.notranslate}]{.pre}<code> </code>{.docutils .literal
.notranslate}[<code>Policy</code>{.docutils .literal .notranslate}]{.pre}. Prior to
Scrapy 1.4, the default [<code>RefererMiddleware</code>{.docutils .literal
.notranslate}]{.pre} would simply and blindly set it to the URL of the
response that generated the HTTP request (which could leak information
on your URL seeds). By default, Scrapy now behaves much like your
regular browser does. And this policy is fully customizable with W3C
standard values (or with something really custom of your own if you
wish). See <a href="index.html#std-setting-REFERRER_POLICY">[<code>REFERRER_POLICY</code>{.xref .std .std-setting .docutils
.literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal} for details.</p>
<p>To make Scrapy spiders easier to debug, Scrapy logs more stats by
default in 1.4: memory usage stats, detailed retry stats, detailed HTTP
error code stats. A similar change is that HTTP cache path is also
visible in logs now.</p>
<p>Last but not least, Scrapy now has the option to make JSON and XML items
more human-readable, with newlines between items and even custom
indenting offset, using the new <a href="index.html#std-setting-FEED_EXPORT_INDENT">[<code>FEED_EXPORT_INDENT</code>{.xref .std
.std-setting .docutils .literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal} setting.</p>
<p>Enjoy! (Or read on for the rest of changes in this release.)</p>
<p>::: {#deprecations-and-backward-incompatible-changes .section}</p>
<h5 id="deprecations-and-backward-incompatible-changesheaderlink"><a class="header" href="#deprecations-and-backward-incompatible-changesheaderlink">Deprecations and Backward Incompatible Changes<a href="#deprecations-and-backward-incompatible-changes" title="Permalink to this heading">¶</a>{.headerlink}</a></h5>
<ul>
<li>
<p>Default to [<code>canonicalize=False</code>{.docutils .literal
.notranslate}]{.pre} in
<a href="index.html#scrapy.linkextractors.lxmlhtml.LxmlLinkExtractor" title="scrapy.linkextractors.lxmlhtml.LxmlLinkExtractor">[<code>scrapy.linkextractors.LinkExtractor</code>{.xref .py .py-class
.docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal} (<a href="https://github.com/scrapy/scrapy/issues/2537">issue
2537</a>{.reference
.external}, fixes <a href="https://github.com/scrapy/scrapy/issues/1941">issue
1941</a>{.reference
.external} and <a href="https://github.com/scrapy/scrapy/issues/1982">issue
1982</a>{.reference
.external}): <strong>warning, this is technically backward-incompatible</strong></p>
</li>
<li>
<p>Enable memusage extension by default (<a href="https://github.com/scrapy/scrapy/issues/2539">issue
2539</a>{.reference
.external}, fixes <a href="https://github.com/scrapy/scrapy/issues/2187">issue
2187</a>{.reference
.external}); <strong>this is technically backward-incompatible</strong> so please
check if you have any non-default [<code>MEMUSAGE_***</code>{.docutils .literal
.notranslate}]{.pre} options set.</p>
</li>
<li>
<p>[<code>EDITOR</code>{.docutils .literal .notranslate}]{.pre} environment
variable now takes precedence over [<code>EDITOR</code>{.docutils .literal
.notranslate}]{.pre} option defined in settings.py (<a href="https://github.com/scrapy/scrapy/issues/1829">issue
1829</a>{.reference
.external}); Scrapy default settings no longer depend on environment
variables. <strong>This is technically a backward incompatible change</strong>.</p>
</li>
<li>
<p>[<code>Spider.make_requests_from_url</code>{.docutils .literal
.notranslate}]{.pre} is deprecated (<a href="https://github.com/scrapy/scrapy/issues/1728">issue
1728</a>{.reference
.external}, fixes <a href="https://github.com/scrapy/scrapy/issues/1495">issue
1495</a>{.reference
.external}).
:::</p>
</li>
</ul>
<p>::: {#id104 .section}</p>
<h5 id="new-featuresheaderlink-16"><a class="header" href="#new-featuresheaderlink-16">New Features<a href="#id104" title="Permalink to this heading">¶</a>{.headerlink}</a></h5>
<ul>
<li>
<p>Accept proxy credentials in <a href="index.html#std-reqmeta-proxy">[<code>proxy</code>{.xref .std .std-reqmeta
.docutils .literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal} request meta key (<a href="https://github.com/scrapy/scrapy/issues/2526">issue
2526</a>{.reference
.external})</p>
</li>
<li>
<p>Support
<a href="https://www.ietf.org/rfc/rfc7932.txt">brotli-compressed</a>{.reference
.external} content; requires optional
<a href="https://github.com/python-hyper/brotlipy/">brotlipy</a>{.reference
.external} (<a href="https://github.com/scrapy/scrapy/issues/2535">issue
2535</a>{.reference
.external})</p>
</li>
<li>
<p>New <a href="index.html#response-follow-example">[response.follow]{.std
.std-ref}</a>{.hoverxref .tooltip
.reference .internal} shortcut for creating requests (<a href="https://github.com/scrapy/scrapy/issues/1940">issue
1940</a>{.reference
.external})</p>
</li>
<li>
<p>Added [<code>flags</code>{.docutils .literal .notranslate}]{.pre} argument and
attribute to <a href="index.html#scrapy.http.Request" title="scrapy.http.Request">[<code>Request</code>{.xref .py .py-class .docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal} objects (<a href="https://github.com/scrapy/scrapy/issues/2047">issue
2047</a>{.reference
.external})</p>
</li>
<li>
<p>Support Anonymous FTP (<a href="https://github.com/scrapy/scrapy/issues/2342">issue
2342</a>{.reference
.external})</p>
</li>
<li>
<p>Added [<code>retry/count</code>{.docutils .literal .notranslate}]{.pre},
[<code>retry/max_reached</code>{.docutils .literal .notranslate}]{.pre} and
[<code>retry/reason_count/&lt;reason&gt;</code>{.docutils .literal
.notranslate}]{.pre} stats to <a href="index.html#scrapy.downloadermiddlewares.retry.RetryMiddleware" title="scrapy.downloadermiddlewares.retry.RetryMiddleware">[<code>RetryMiddleware</code>{.xref .py
.py-class .docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal} (<a href="https://github.com/scrapy/scrapy/issues/2543">issue
2543</a>{.reference
.external})</p>
</li>
<li>
<p>Added [<code>httperror/response_ignored_count</code>{.docutils .literal
.notranslate}]{.pre} and
[<code>httperror/response_ignored_status_count/&lt;status&gt;</code>{.docutils
.literal .notranslate}]{.pre} stats to <a href="index.html#scrapy.spidermiddlewares.httperror.HttpErrorMiddleware" title="scrapy.spidermiddlewares.httperror.HttpErrorMiddleware">[<code>HttpErrorMiddleware</code>{.xref
.py .py-class .docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal} (<a href="https://github.com/scrapy/scrapy/issues/2566">issue
2566</a>{.reference
.external})</p>
</li>
<li>
<p>Customizable <a href="index.html#std-setting-REFERRER_POLICY">[<code>Referrer</code>{.xref .std .std-setting .docutils .literal
.notranslate}]{.pre}<code> </code>{.xref .std .std-setting .docutils .literal
.notranslate}[<code>policy</code>{.xref .std .std-setting .docutils .literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal} in <a href="index.html#scrapy.spidermiddlewares.referer.RefererMiddleware" title="scrapy.spidermiddlewares.referer.RefererMiddleware">[<code>RefererMiddleware</code>{.xref .py
.py-class .docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal} (<a href="https://github.com/scrapy/scrapy/issues/2306">issue
2306</a>{.reference
.external})</p>
</li>
<li>
<p>New [<code>data:</code>{.docutils .literal .notranslate}]{.pre} URI download
handler (<a href="https://github.com/scrapy/scrapy/issues/2334">issue
2334</a>{.reference
.external}, fixes <a href="https://github.com/scrapy/scrapy/issues/2156">issue
2156</a>{.reference
.external})</p>
</li>
<li>
<p>Log cache directory when HTTP Cache is used (<a href="https://github.com/scrapy/scrapy/issues/2611">issue
2611</a>{.reference
.external}, fixes <a href="https://github.com/scrapy/scrapy/issues/2604">issue
2604</a>{.reference
.external})</p>
</li>
<li>
<p>Warn users when project contains duplicate spider names (fixes
<a href="https://github.com/scrapy/scrapy/issues/2181">issue
2181</a>{.reference
.external})</p>
</li>
<li>
<p>[<code>scrapy.utils.datatypes.CaselessDict</code>{.docutils .literal
.notranslate}]{.pre} now accepts [<code>Mapping</code>{.docutils .literal
.notranslate}]{.pre} instances and not only dicts (<a href="https://github.com/scrapy/scrapy/issues/2646">issue
2646</a>{.reference
.external})</p>
</li>
<li>
<p><a href="index.html#topics-media-pipeline">[Media downloads]{.std
.std-ref}</a>{.hoverxref .tooltip
.reference .internal}, with <a href="index.html#scrapy.pipelines.files.FilesPipeline" title="scrapy.pipelines.files.FilesPipeline">[<code>FilesPipeline</code>{.xref .py .py-class
.docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal} or <a href="index.html#scrapy.pipelines.images.ImagesPipeline" title="scrapy.pipelines.images.ImagesPipeline">[<code>ImagesPipeline</code>{.xref .py .py-class .docutils
.literal
.notranslate}]{.pre}</a>{.reference
.internal}, can now optionally handle HTTP redirects using the new
<a href="index.html#std-setting-MEDIA_ALLOW_REDIRECTS">[<code>MEDIA_ALLOW_REDIRECTS</code>{.xref .std .std-setting .docutils .literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal} setting (<a href="https://github.com/scrapy/scrapy/issues/2616">issue
2616</a>{.reference
.external}, fixes <a href="https://github.com/scrapy/scrapy/issues/2004">issue
2004</a>{.reference
.external})</p>
</li>
<li>
<p>Accept non-complete responses from websites using a new
<a href="index.html#std-setting-DOWNLOAD_FAIL_ON_DATALOSS">[<code>DOWNLOAD_FAIL_ON_DATALOSS</code>{.xref .std .std-setting .docutils
.literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal} setting (<a href="https://github.com/scrapy/scrapy/issues/2590">issue
2590</a>{.reference
.external}, fixes <a href="https://github.com/scrapy/scrapy/issues/2586">issue
2586</a>{.reference
.external})</p>
</li>
<li>
<p>Optional pretty-printing of JSON and XML items via
<a href="index.html#std-setting-FEED_EXPORT_INDENT">[<code>FEED_EXPORT_INDENT</code>{.xref .std .std-setting .docutils .literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal} setting (<a href="https://github.com/scrapy/scrapy/issues/2456">issue
2456</a>{.reference
.external}, fixes <a href="https://github.com/scrapy/scrapy/issues/1327">issue
1327</a>{.reference
.external})</p>
</li>
<li>
<p>Allow dropping fields in [<code>FormRequest.from_response</code>{.docutils
.literal .notranslate}]{.pre} formdata when [<code>None</code>{.docutils
.literal .notranslate}]{.pre} value is passed (<a href="https://github.com/scrapy/scrapy/issues/667">issue
667</a>{.reference
.external})</p>
</li>
<li>
<p>Per-request retry times with the new <a href="index.html#std-reqmeta-max_retry_times">[<code>max_retry_times</code>{.xref .std
.std-reqmeta .docutils .literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal} meta key (<a href="https://github.com/scrapy/scrapy/issues/2642">issue
2642</a>{.reference
.external})</p>
</li>
<li>
<p>[<code>python</code>{.docutils .literal .notranslate}]{.pre}<code> </code>{.docutils
.literal .notranslate}[<code>-m</code>{.docutils .literal
.notranslate}]{.pre}<code> </code>{.docutils .literal
.notranslate}[<code>scrapy</code>{.docutils .literal .notranslate}]{.pre} as a
more explicit alternative to [<code>scrapy</code>{.docutils .literal
.notranslate}]{.pre} command (<a href="https://github.com/scrapy/scrapy/issues/2740">issue
2740</a>{.reference
.external})
:::</p>
</li>
</ul>
<p>::: {#id105 .section}</p>
<h5 id="bug-fixesheaderlink-17"><a class="header" href="#bug-fixesheaderlink-17">Bug fixes<a href="#id105" title="Permalink to this heading">¶</a>{.headerlink}</a></h5>
<ul>
<li>
<p>LinkExtractor now strips leading and trailing whitespaces from
attributes (<a href="https://github.com/scrapy/scrapy/issues/2547">issue
2547</a>{.reference
.external}, fixes <a href="https://github.com/scrapy/scrapy/issues/1614">issue
1614</a>{.reference
.external})</p>
</li>
<li>
<p>Properly handle whitespaces in action attribute in
[<code>FormRequest</code>{.xref .py .py-class .docutils .literal
.notranslate}]{.pre} (<a href="https://github.com/scrapy/scrapy/issues/2548">issue
2548</a>{.reference
.external})</p>
</li>
<li>
<p>Buffer CONNECT response bytes from proxy until all HTTP headers are
received (<a href="https://github.com/scrapy/scrapy/issues/2495">issue
2495</a>{.reference
.external}, fixes <a href="https://github.com/scrapy/scrapy/issues/2491">issue
2491</a>{.reference
.external})</p>
</li>
<li>
<p>FTP downloader now works on Python 3, provided you use
Twisted&gt;=17.1 (<a href="https://github.com/scrapy/scrapy/issues/2599">issue
2599</a>{.reference
.external})</p>
</li>
<li>
<p>Use body to choose response type after decompressing content (<a href="https://github.com/scrapy/scrapy/issues/2393">issue
2393</a>{.reference
.external}, fixes <a href="https://github.com/scrapy/scrapy/issues/2145">issue
2145</a>{.reference
.external})</p>
</li>
<li>
<p>Always decompress [<code>Content-Encoding:</code>{.docutils .literal
.notranslate}]{.pre}<code> </code>{.docutils .literal
.notranslate}[<code>gzip</code>{.docutils .literal .notranslate}]{.pre} at
<a href="index.html#scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware" title="scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware">[<code>HttpCompressionMiddleware</code>{.xref .py .py-class .docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal} stage (<a href="https://github.com/scrapy/scrapy/issues/2391">issue
2391</a>{.reference
.external})</p>
</li>
<li>
<p>Respect custom log level in [<code>Spider.custom_settings</code>{.docutils
.literal .notranslate}]{.pre} (<a href="https://github.com/scrapy/scrapy/issues/2581">issue
2581</a>{.reference
.external}, fixes <a href="https://github.com/scrapy/scrapy/issues/1612">issue
1612</a>{.reference
.external})</p>
</li>
<li>
<p>'make htmlview' fix for macOS (<a href="https://github.com/scrapy/scrapy/issues/2661">issue
2661</a>{.reference
.external})</p>
</li>
<li>
<p>Remove &quot;commands&quot; from the command list (<a href="https://github.com/scrapy/scrapy/issues/2695">issue
2695</a>{.reference
.external})</p>
</li>
<li>
<p>Fix duplicate Content-Length header for POST requests with empty
body (<a href="https://github.com/scrapy/scrapy/issues/2677">issue
2677</a>{.reference
.external})</p>
</li>
<li>
<p>Properly cancel large downloads, i.e. above
<a href="index.html#std-setting-DOWNLOAD_MAXSIZE">[<code>DOWNLOAD_MAXSIZE</code>{.xref .std .std-setting .docutils .literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal} (<a href="https://github.com/scrapy/scrapy/issues/1616">issue
1616</a>{.reference
.external})</p>
</li>
<li>
<p>ImagesPipeline: fixed processing of transparent PNG images with
palette (<a href="https://github.com/scrapy/scrapy/issues/2675">issue
2675</a>{.reference
.external})
:::</p>
</li>
</ul>
<p>::: {#cleanups-refactoring .section}</p>
<h5 id="cleanups--refactoringheaderlink"><a class="header" href="#cleanups--refactoringheaderlink">Cleanups &amp; Refactoring<a href="#cleanups-refactoring" title="Permalink to this heading">¶</a>{.headerlink}</a></h5>
<ul>
<li>
<p>Tests: remove temp files and folders (<a href="https://github.com/scrapy/scrapy/issues/2570">issue
2570</a>{.reference
.external}), fixed ProjectUtilsTest on macOS (<a href="https://github.com/scrapy/scrapy/issues/2569">issue
2569</a>{.reference
.external}), use portable pypy for Linux on Travis CI (<a href="https://github.com/scrapy/scrapy/issues/2710">issue
2710</a>{.reference
.external})</p>
</li>
<li>
<p>Separate building request from [<code>_requests_to_follow</code>{.docutils
.literal .notranslate}]{.pre} in CrawlSpider (<a href="https://github.com/scrapy/scrapy/issues/2562">issue
2562</a>{.reference
.external})</p>
</li>
<li>
<p>Remove &quot;Python 3 progress&quot; badge (<a href="https://github.com/scrapy/scrapy/issues/2567">issue
2567</a>{.reference
.external})</p>
</li>
<li>
<p>Add a couple more lines to [<code>.gitignore</code>{.docutils .literal
.notranslate}]{.pre} (<a href="https://github.com/scrapy/scrapy/issues/2557">issue
2557</a>{.reference
.external})</p>
</li>
<li>
<p>Remove bumpversion prerelease configuration (<a href="https://github.com/scrapy/scrapy/issues/2159">issue
2159</a>{.reference
.external})</p>
</li>
<li>
<p>Add codecov.yml file (<a href="https://github.com/scrapy/scrapy/issues/2750">issue
2750</a>{.reference
.external})</p>
</li>
<li>
<p>Set context factory implementation based on Twisted version (<a href="https://github.com/scrapy/scrapy/issues/2577">issue
2577</a>{.reference
.external}, fixes <a href="https://github.com/scrapy/scrapy/issues/2560">issue
2560</a>{.reference
.external})</p>
</li>
<li>
<p>Add omitted [<code>self</code>{.docutils .literal .notranslate}]{.pre}
arguments in default project middleware template (<a href="https://github.com/scrapy/scrapy/issues/2595">issue
2595</a>{.reference
.external})</p>
</li>
<li>
<p>Remove redundant [<code>slot.add_request()</code>{.docutils .literal
.notranslate}]{.pre} call in ExecutionEngine (<a href="https://github.com/scrapy/scrapy/issues/2617">issue
2617</a>{.reference
.external})</p>
</li>
<li>
<p>Catch more specific [<code>os.error</code>{.docutils .literal
.notranslate}]{.pre} exception in
[<code>scrapy.pipelines.files.FSFilesStore</code>{.docutils .literal
.notranslate}]{.pre} (<a href="https://github.com/scrapy/scrapy/issues/2644">issue
2644</a>{.reference
.external})</p>
</li>
<li>
<p>Change &quot;localhost&quot; test server certificate (<a href="https://github.com/scrapy/scrapy/issues/2720">issue
2720</a>{.reference
.external})</p>
</li>
<li>
<p>Remove unused [<code>MEMUSAGE_REPORT</code>{.docutils .literal
.notranslate}]{.pre} setting (<a href="https://github.com/scrapy/scrapy/issues/2576">issue
2576</a>{.reference
.external})
:::</p>
</li>
</ul>
<p>::: {#id106 .section}</p>
<h5 id="documentationheaderlink-15"><a class="header" href="#documentationheaderlink-15">Documentation<a href="#id106" title="Permalink to this heading">¶</a>{.headerlink}</a></h5>
<ul>
<li>
<p>Binary mode is required for exporters (<a href="https://github.com/scrapy/scrapy/issues/2564">issue
2564</a>{.reference
.external}, fixes <a href="https://github.com/scrapy/scrapy/issues/2553">issue
2553</a>{.reference
.external})</p>
</li>
<li>
<p>Mention issue with [<code>FormRequest.from_response</code>{.xref .py .py-meth
.docutils .literal .notranslate}]{.pre} due to bug in lxml (<a href="https://github.com/scrapy/scrapy/issues/2572">issue
2572</a>{.reference
.external})</p>
</li>
<li>
<p>Use single quotes uniformly in templates (<a href="https://github.com/scrapy/scrapy/issues/2596">issue
2596</a>{.reference
.external})</p>
</li>
<li>
<p>Document <a href="index.html#std-reqmeta-ftp_user">[<code>ftp_user</code>{.xref .std .std-reqmeta .docutils .literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal} and <a href="index.html#std-reqmeta-ftp_password">[<code>ftp_password</code>{.xref .std
.std-reqmeta .docutils .literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal} meta keys (<a href="https://github.com/scrapy/scrapy/issues/2587">issue
2587</a>{.reference
.external})</p>
</li>
<li>
<p>Removed section on deprecated [<code>contrib/</code>{.docutils .literal
.notranslate}]{.pre} (<a href="https://github.com/scrapy/scrapy/issues/2636">issue
2636</a>{.reference
.external})</p>
</li>
<li>
<p>Recommend Anaconda when installing Scrapy on Windows (<a href="https://github.com/scrapy/scrapy/issues/2477">issue
2477</a>{.reference
.external}, fixes <a href="https://github.com/scrapy/scrapy/issues/2475">issue
2475</a>{.reference
.external})</p>
</li>
<li>
<p>FAQ: rewrite note on Python 3 support on Windows (<a href="https://github.com/scrapy/scrapy/issues/2690">issue
2690</a>{.reference
.external})</p>
</li>
<li>
<p>Rearrange selector sections (<a href="https://github.com/scrapy/scrapy/issues/2705">issue
2705</a>{.reference
.external})</p>
</li>
<li>
<p>Remove [<code>__nonzero__</code>{.docutils .literal .notranslate}]{.pre} from
<a href="index.html#scrapy.selector.SelectorList" title="scrapy.selector.SelectorList">[<code>SelectorList</code>{.xref .py .py-class .docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal} docs (<a href="https://github.com/scrapy/scrapy/issues/2683">issue
2683</a>{.reference
.external})</p>
</li>
<li>
<p>Mention how to disable request filtering in documentation of
<a href="index.html#std-setting-DUPEFILTER_CLASS">[<code>DUPEFILTER_CLASS</code>{.xref .std .std-setting .docutils .literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal} setting (<a href="https://github.com/scrapy/scrapy/issues/2714">issue
2714</a>{.reference
.external})</p>
</li>
<li>
<p>Add sphinx_rtd_theme to docs setup readme (<a href="https://github.com/scrapy/scrapy/issues/2668">issue
2668</a>{.reference
.external})</p>
</li>
<li>
<p>Open file in text mode in JSON item writer example (<a href="https://github.com/scrapy/scrapy/issues/2729">issue
2729</a>{.reference
.external})</p>
</li>
<li>
<p>Clarify [<code>allowed_domains</code>{.docutils .literal .notranslate}]{.pre}
example (<a href="https://github.com/scrapy/scrapy/issues/2670">issue
2670</a>{.reference
.external})
:::
:::</p>
</li>
</ul>
<p>::: {#scrapy-1-3-3-2017-03-10 .section}
[]{#release-1-3-3}</p>
<h4 id="scrapy-133-2017-03-10headerlink"><a class="header" href="#scrapy-133-2017-03-10headerlink">Scrapy 1.3.3 (2017-03-10)<a href="#scrapy-1-3-3-2017-03-10" title="Permalink to this heading">¶</a>{.headerlink}</a></h4>
<p>::: {#id107 .section}</p>
<h5 id="bug-fixesheaderlink-18"><a class="header" href="#bug-fixesheaderlink-18">Bug fixes<a href="#id107" title="Permalink to this heading">¶</a>{.headerlink}</a></h5>
<ul>
<li>Make [<code>SpiderLoader</code>{.docutils .literal .notranslate}]{.pre} raise
[<code>ImportError</code>{.docutils .literal .notranslate}]{.pre} again by
default for missing dependencies and wrong <a href="index.html#std-setting-SPIDER_MODULES">[<code>SPIDER_MODULES</code>{.xref
.std .std-setting .docutils .literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal}. These exceptions were silenced as
warnings since 1.3.0. A new setting is introduced to toggle between
warning or exception if needed ; see
<a href="index.html#std-setting-SPIDER_LOADER_WARN_ONLY">[<code>SPIDER_LOADER_WARN_ONLY</code>{.xref .std .std-setting .docutils
.literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal} for details.
:::
:::</li>
</ul>
<p>::: {#scrapy-1-3-2-2017-02-13 .section}
[]{#release-1-3-2}</p>
<h4 id="scrapy-132-2017-02-13headerlink"><a class="header" href="#scrapy-132-2017-02-13headerlink">Scrapy 1.3.2 (2017-02-13)<a href="#scrapy-1-3-2-2017-02-13" title="Permalink to this heading">¶</a>{.headerlink}</a></h4>
<p>::: {#id108 .section}</p>
<h5 id="bug-fixesheaderlink-19"><a class="header" href="#bug-fixesheaderlink-19">Bug fixes<a href="#id108" title="Permalink to this heading">¶</a>{.headerlink}</a></h5>
<ul>
<li>
<p>Preserve request class when converting to/from dicts (utils.reqser)
(<a href="https://github.com/scrapy/scrapy/issues/2510">issue
2510</a>{.reference
.external}).</p>
</li>
<li>
<p>Use consistent selectors for author field in tutorial (<a href="https://github.com/scrapy/scrapy/issues/2551">issue
2551</a>{.reference
.external}).</p>
</li>
<li>
<p>Fix TLS compatibility in Twisted 17+ (<a href="https://github.com/scrapy/scrapy/issues/2558">issue
2558</a>{.reference
.external})
:::
:::</p>
</li>
</ul>
<p>::: {#scrapy-1-3-1-2017-02-08 .section}
[]{#release-1-3-1}</p>
<h4 id="scrapy-131-2017-02-08headerlink"><a class="header" href="#scrapy-131-2017-02-08headerlink">Scrapy 1.3.1 (2017-02-08)<a href="#scrapy-1-3-1-2017-02-08" title="Permalink to this heading">¶</a>{.headerlink}</a></h4>
<p>::: {#id109 .section}</p>
<h5 id="new-featuresheaderlink-17"><a class="header" href="#new-featuresheaderlink-17">New features<a href="#id109" title="Permalink to this heading">¶</a>{.headerlink}</a></h5>
<ul>
<li>
<p>Support [<code>'True'</code>{.docutils .literal .notranslate}]{.pre} and
[<code>'False'</code>{.docutils .literal .notranslate}]{.pre} string values for
boolean settings (<a href="https://github.com/scrapy/scrapy/issues/2519">issue
2519</a>{.reference
.external}); you can now do something like [<code>scrapy</code>{.docutils
.literal .notranslate}]{.pre}<code> </code>{.docutils .literal
.notranslate}[<code>crawl</code>{.docutils .literal
.notranslate}]{.pre}<code> </code>{.docutils .literal
.notranslate}[<code>myspider</code>{.docutils .literal
.notranslate}]{.pre}<code> </code>{.docutils .literal
.notranslate}[<code>-s</code>{.docutils .literal
.notranslate}]{.pre}<code> </code>{.docutils .literal
.notranslate}[<code>REDIRECT_ENABLED=False</code>{.docutils .literal
.notranslate}]{.pre}.</p>
</li>
<li>
<p>Support kwargs with [<code>response.xpath()</code>{.docutils .literal
.notranslate}]{.pre} to use <a href="index.html#topics-selectors-xpath-variables">[XPath variables]{.std
.std-ref}</a>{.hoverxref
.tooltip .reference .internal} and ad-hoc namespaces declarations ;
this requires at least Parsel v1.1 (<a href="https://github.com/scrapy/scrapy/issues/2457">issue
2457</a>{.reference
.external}).</p>
</li>
<li>
<p>Add support for Python 3.6 (<a href="https://github.com/scrapy/scrapy/issues/2485">issue
2485</a>{.reference
.external}).</p>
</li>
<li>
<p>Run tests on PyPy (warning: some tests still fail, so PyPy is not
supported yet).
:::</p>
</li>
</ul>
<p>::: {#id110 .section}</p>
<h5 id="bug-fixesheaderlink-20"><a class="header" href="#bug-fixesheaderlink-20">Bug fixes<a href="#id110" title="Permalink to this heading">¶</a>{.headerlink}</a></h5>
<ul>
<li>
<p>Enforce [<code>DNS_TIMEOUT</code>{.docutils .literal .notranslate}]{.pre}
setting (<a href="https://github.com/scrapy/scrapy/issues/2496">issue
2496</a>{.reference
.external}).</p>
</li>
<li>
<p>Fix <a href="index.html#std-command-view">[<code>view</code>{.xref .std .std-command .docutils .literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal} command ; it was a regression in
v1.3.0 (<a href="https://github.com/scrapy/scrapy/issues/2503">issue
2503</a>{.reference
.external}).</p>
</li>
<li>
<p>Fix tests regarding [<code>*_EXPIRES</code>{.docutils .literal
.notranslate}]{.pre}<code> </code>{.docutils .literal
.notranslate}[<code>settings</code>{.docutils .literal .notranslate}]{.pre}
with Files/Images pipelines (<a href="https://github.com/scrapy/scrapy/issues/2460">issue
2460</a>{.reference
.external}).</p>
</li>
<li>
<p>Fix name of generated pipeline class when using basic project
template (<a href="https://github.com/scrapy/scrapy/issues/2466">issue
2466</a>{.reference
.external}).</p>
</li>
<li>
<p>Fix compatibility with Twisted 17+ (<a href="https://github.com/scrapy/scrapy/issues/2496">issue
2496</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/2528">issue
2528</a>{.reference
.external}).</p>
</li>
<li>
<p>Fix [<code>scrapy.Item</code>{.docutils .literal .notranslate}]{.pre}
inheritance on Python 3.6 (<a href="https://github.com/scrapy/scrapy/issues/2511">issue
2511</a>{.reference
.external}).</p>
</li>
<li>
<p>Enforce numeric values for components order in
[<code>SPIDER_MIDDLEWARES</code>{.docutils .literal .notranslate}]{.pre},
[<code>DOWNLOADER_MIDDLEWARES</code>{.docutils .literal .notranslate}]{.pre},
[<code>EXTENSIONS</code>{.docutils .literal .notranslate}]{.pre} and
[<code>SPIDER_CONTRACTS</code>{.docutils .literal .notranslate}]{.pre} (<a href="https://github.com/scrapy/scrapy/issues/2420">issue
2420</a>{.reference
.external}).
:::</p>
</li>
</ul>
<p>::: {#id111 .section}</p>
<h5 id="documentationheaderlink-16"><a class="header" href="#documentationheaderlink-16">Documentation<a href="#id111" title="Permalink to this heading">¶</a>{.headerlink}</a></h5>
<ul>
<li>
<p>Reword Code of Conduct section and upgrade to Contributor Covenant
v1.4 (<a href="https://github.com/scrapy/scrapy/issues/2469">issue
2469</a>{.reference
.external}).</p>
</li>
<li>
<p>Clarify that passing spider arguments converts them to spider
attributes (<a href="https://github.com/scrapy/scrapy/issues/2483">issue
2483</a>{.reference
.external}).</p>
</li>
<li>
<p>Document [<code>formid</code>{.docutils .literal .notranslate}]{.pre} argument
on [<code>FormRequest.from_response()</code>{.docutils .literal
.notranslate}]{.pre} (<a href="https://github.com/scrapy/scrapy/issues/2497">issue
2497</a>{.reference
.external}).</p>
</li>
<li>
<p>Add .rst extension to README files (<a href="https://github.com/scrapy/scrapy/issues/2507">issue
2507</a>{.reference
.external}).</p>
</li>
<li>
<p>Mention LevelDB cache storage backend (<a href="https://github.com/scrapy/scrapy/issues/2525">issue
2525</a>{.reference
.external}).</p>
</li>
<li>
<p>Use [<code>yield</code>{.docutils .literal .notranslate}]{.pre} in sample
callback code (<a href="https://github.com/scrapy/scrapy/issues/2533">issue
2533</a>{.reference
.external}).</p>
</li>
<li>
<p>Add note about HTML entities decoding with
[<code>.re()/.re_first()</code>{.docutils .literal .notranslate}]{.pre} (<a href="https://github.com/scrapy/scrapy/issues/1704">issue
1704</a>{.reference
.external}).</p>
</li>
<li>
<p>Typos (<a href="https://github.com/scrapy/scrapy/issues/2512">issue
2512</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/2534">issue
2534</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/2531">issue
2531</a>{.reference
.external}).
:::</p>
</li>
</ul>
<p>::: {#cleanups .section}</p>
<h5 id="cleanupsheaderlink"><a class="header" href="#cleanupsheaderlink">Cleanups<a href="#cleanups" title="Permalink to this heading">¶</a>{.headerlink}</a></h5>
<ul>
<li>
<p>Remove redundant check in [<code>MetaRefreshMiddleware</code>{.docutils
.literal .notranslate}]{.pre} (<a href="https://github.com/scrapy/scrapy/issues/2542">issue
2542</a>{.reference
.external}).</p>
</li>
<li>
<p>Faster checks in [<code>LinkExtractor</code>{.docutils .literal
.notranslate}]{.pre} for allow/deny patterns (<a href="https://github.com/scrapy/scrapy/issues/2538">issue
2538</a>{.reference
.external}).</p>
</li>
<li>
<p>Remove dead code supporting old Twisted versions (<a href="https://github.com/scrapy/scrapy/issues/2544">issue
2544</a>{.reference
.external}).
:::
:::</p>
</li>
</ul>
<p>::: {#scrapy-1-3-0-2016-12-21 .section}
[]{#release-1-3-0}</p>
<h4 id="scrapy-130-2016-12-21headerlink"><a class="header" href="#scrapy-130-2016-12-21headerlink">Scrapy 1.3.0 (2016-12-21)<a href="#scrapy-1-3-0-2016-12-21" title="Permalink to this heading">¶</a>{.headerlink}</a></h4>
<p>This release comes rather soon after 1.2.2 for one main reason: it was
found out that releases since 0.18 up to 1.2.2 (included) use some
backported code from Twisted ([<code>scrapy.xlib.tx.*</code>{.docutils .literal
.notranslate}]{.pre}), even if newer Twisted modules are available.
Scrapy now uses [<code>twisted.web.client</code>{.docutils .literal
.notranslate}]{.pre} and [<code>twisted.internet.endpoints</code>{.docutils
.literal .notranslate}]{.pre} directly. (See also cleanups below.)</p>
<p>As it is a major change, we wanted to get the bug fix out quickly while
not breaking any projects using the 1.2 series.</p>
<p>::: {#id112 .section}</p>
<h5 id="new-featuresheaderlink-18"><a class="header" href="#new-featuresheaderlink-18">New Features<a href="#id112" title="Permalink to this heading">¶</a>{.headerlink}</a></h5>
<ul>
<li>
<p>[<code>MailSender</code>{.docutils .literal .notranslate}]{.pre} now accepts
single strings as values for [<code>to</code>{.docutils .literal
.notranslate}]{.pre} and [<code>cc</code>{.docutils .literal
.notranslate}]{.pre} arguments (<a href="https://github.com/scrapy/scrapy/issues/2272">issue
2272</a>{.reference
.external})</p>
</li>
<li>
<p>[<code>scrapy</code>{.docutils .literal .notranslate}]{.pre}<code> </code>{.docutils
.literal .notranslate}[<code>fetch</code>{.docutils .literal
.notranslate}]{.pre}<code> </code>{.docutils .literal
.notranslate}[<code>url</code>{.docutils .literal .notranslate}]{.pre},
[<code>scrapy</code>{.docutils .literal .notranslate}]{.pre}<code> </code>{.docutils
.literal .notranslate}[<code>shell</code>{.docutils .literal
.notranslate}]{.pre}<code> </code>{.docutils .literal
.notranslate}[<code>url</code>{.docutils .literal .notranslate}]{.pre} and
[<code>fetch(url)</code>{.docutils .literal .notranslate}]{.pre} inside Scrapy
shell now follow HTTP redirections by default (<a href="https://github.com/scrapy/scrapy/issues/2290">issue
2290</a>{.reference
.external}); See <a href="index.html#std-command-fetch">[<code>fetch</code>{.xref .std .std-command .docutils
.literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal} and <a href="index.html#std-command-shell">[<code>shell</code>{.xref .std .std-command
.docutils .literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal} for details.</p>
</li>
<li>
<p>[<code>HttpErrorMiddleware</code>{.docutils .literal .notranslate}]{.pre} now
logs errors with [<code>INFO</code>{.docutils .literal .notranslate}]{.pre}
level instead of [<code>DEBUG</code>{.docutils .literal .notranslate}]{.pre};
this is technically <strong>backward incompatible</strong> so please check your
log parsers.</p>
</li>
<li>
<p>By default, logger names now use a long-form path, e.g.
[<code>[scrapy.extensions.logstats]</code>{.docutils .literal
.notranslate}]{.pre}, instead of the shorter &quot;top-level&quot; variant of
prior releases (e.g. [<code>[scrapy]</code>{.docutils .literal
.notranslate}]{.pre}); this is <strong>backward incompatible</strong> if you have
log parsers expecting the short logger name part. You can switch
back to short logger names using <a href="index.html#std-setting-LOG_SHORT_NAMES">[<code>LOG_SHORT_NAMES</code>{.xref .std
.std-setting .docutils .literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal} set to [<code>True</code>{.docutils .literal
.notranslate}]{.pre}.
:::</p>
</li>
</ul>
<p>::: {#dependencies-cleanups .section}</p>
<h5 id="dependencies--cleanupsheaderlink"><a class="header" href="#dependencies--cleanupsheaderlink">Dependencies &amp; Cleanups<a href="#dependencies-cleanups" title="Permalink to this heading">¶</a>{.headerlink}</a></h5>
<ul>
<li>
<p>Scrapy now requires Twisted &gt;= 13.1 which is the case for many
Linux distributions already.</p>
</li>
<li>
<p>As a consequence, we got rid of [<code>scrapy.xlib.tx.*</code>{.docutils
.literal .notranslate}]{.pre} modules, which copied some of Twisted
code for users stuck with an &quot;old&quot; Twisted version</p>
</li>
<li>
<p>[<code>ChunkedTransferMiddleware</code>{.docutils .literal .notranslate}]{.pre}
is deprecated and removed from the default downloader middlewares.
:::
:::</p>
</li>
</ul>
<p>::: {#scrapy-1-2-3-2017-03-03 .section}
[]{#release-1-2-3}</p>
<h4 id="scrapy-123-2017-03-03headerlink"><a class="header" href="#scrapy-123-2017-03-03headerlink">Scrapy 1.2.3 (2017-03-03)<a href="#scrapy-1-2-3-2017-03-03" title="Permalink to this heading">¶</a>{.headerlink}</a></h4>
<ul>
<li>Packaging fix: disallow unsupported Twisted versions in setup.py
:::</li>
</ul>
<p>::: {#scrapy-1-2-2-2016-12-06 .section}
[]{#release-1-2-2}</p>
<h4 id="scrapy-122-2016-12-06headerlink"><a class="header" href="#scrapy-122-2016-12-06headerlink">Scrapy 1.2.2 (2016-12-06)<a href="#scrapy-1-2-2-2016-12-06" title="Permalink to this heading">¶</a>{.headerlink}</a></h4>
<p>::: {#id113 .section}</p>
<h5 id="bug-fixesheaderlink-21"><a class="header" href="#bug-fixesheaderlink-21">Bug fixes<a href="#id113" title="Permalink to this heading">¶</a>{.headerlink}</a></h5>
<ul>
<li>
<p>Fix a cryptic traceback when a pipeline fails on
[<code>open_spider()</code>{.docutils .literal .notranslate}]{.pre} (<a href="https://github.com/scrapy/scrapy/issues/2011">issue
2011</a>{.reference
.external})</p>
</li>
<li>
<p>Fix embedded IPython shell variables (fixing <a href="https://github.com/scrapy/scrapy/issues/396">issue
396</a>{.reference
.external} that re-appeared in 1.2.0, fixed in <a href="https://github.com/scrapy/scrapy/issues/2418">issue
2418</a>{.reference
.external})</p>
</li>
<li>
<p>A couple of patches when dealing with robots.txt:</p>
<ul>
<li>
<p>handle (non-standard) relative sitemap URLs (<a href="https://github.com/scrapy/scrapy/issues/2390">issue
2390</a>{.reference
.external})</p>
</li>
<li>
<p>handle non-ASCII URLs and User-Agents in Python 2 (<a href="https://github.com/scrapy/scrapy/issues/2373">issue
2373</a>{.reference
.external})
:::</p>
</li>
</ul>
</li>
</ul>
<p>::: {#id114 .section}</p>
<h5 id="documentationheaderlink-17"><a class="header" href="#documentationheaderlink-17">Documentation<a href="#id114" title="Permalink to this heading">¶</a>{.headerlink}</a></h5>
<ul>
<li>
<p>Document [<code>&quot;download_latency&quot;</code>{.docutils .literal
.notranslate}]{.pre} key in [<code>Request</code>{.docutils .literal
.notranslate}]{.pre}'s [<code>meta</code>{.docutils .literal
.notranslate}]{.pre} dict (<a href="https://github.com/scrapy/scrapy/issues/2033">issue
2033</a>{.reference
.external})</p>
</li>
<li>
<p>Remove page on (deprecated &amp; unsupported) Ubuntu packages from ToC
(<a href="https://github.com/scrapy/scrapy/issues/2335">issue
2335</a>{.reference
.external})</p>
</li>
<li>
<p>A few fixed typos (<a href="https://github.com/scrapy/scrapy/issues/2346">issue
2346</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/2369">issue
2369</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/2369">issue
2369</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/2380">issue
2380</a>{.reference
.external}) and clarifications (<a href="https://github.com/scrapy/scrapy/issues/2354">issue
2354</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/2325">issue
2325</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/2414">issue
2414</a>{.reference
.external})
:::</p>
</li>
</ul>
<p>::: {#id115 .section}</p>
<h5 id="other-changesheaderlink-2"><a class="header" href="#other-changesheaderlink-2">Other changes<a href="#id115" title="Permalink to this heading">¶</a>{.headerlink}</a></h5>
<ul>
<li>
<p>Advertize
<a href="https://anaconda.org/conda-forge/scrapy">conda-forge</a>{.reference
.external} as Scrapy's official conda channel (<a href="https://github.com/scrapy/scrapy/issues/2387">issue
2387</a>{.reference
.external})</p>
</li>
<li>
<p>More helpful error messages when trying to use [<code>.css()</code>{.docutils
.literal .notranslate}]{.pre} or [<code>.xpath()</code>{.docutils .literal
.notranslate}]{.pre} on non-Text Responses (<a href="https://github.com/scrapy/scrapy/issues/2264">issue
2264</a>{.reference
.external})</p>
</li>
<li>
<p>[<code>startproject</code>{.docutils .literal .notranslate}]{.pre} command now
generates a sample [<code>middlewares.py</code>{.docutils .literal
.notranslate}]{.pre} file (<a href="https://github.com/scrapy/scrapy/issues/2335">issue
2335</a>{.reference
.external})</p>
</li>
<li>
<p>Add more dependencies' version info in [<code>scrapy</code>{.docutils .literal
.notranslate}]{.pre}<code> </code>{.docutils .literal
.notranslate}[<code>version</code>{.docutils .literal .notranslate}]{.pre}
verbose output (<a href="https://github.com/scrapy/scrapy/issues/2404">issue
2404</a>{.reference
.external})</p>
</li>
<li>
<p>Remove all [<code>*.pyc</code>{.docutils .literal .notranslate}]{.pre} files
from source distribution (<a href="https://github.com/scrapy/scrapy/issues/2386">issue
2386</a>{.reference
.external})
:::
:::</p>
</li>
</ul>
<p>::: {#scrapy-1-2-1-2016-10-21 .section}
[]{#release-1-2-1}</p>
<h4 id="scrapy-121-2016-10-21headerlink"><a class="header" href="#scrapy-121-2016-10-21headerlink">Scrapy 1.2.1 (2016-10-21)<a href="#scrapy-1-2-1-2016-10-21" title="Permalink to this heading">¶</a>{.headerlink}</a></h4>
<p>::: {#id116 .section}</p>
<h5 id="bug-fixesheaderlink-22"><a class="header" href="#bug-fixesheaderlink-22">Bug fixes<a href="#id116" title="Permalink to this heading">¶</a>{.headerlink}</a></h5>
<ul>
<li>
<p>Include OpenSSL's more permissive default ciphers when establishing
TLS/SSL connections (<a href="https://github.com/scrapy/scrapy/issues/2314">issue
2314</a>{.reference
.external}).</p>
</li>
<li>
<p>Fix &quot;Location&quot; HTTP header decoding on non-ASCII URL redirects
(<a href="https://github.com/scrapy/scrapy/issues/2321">issue
2321</a>{.reference
.external}).
:::</p>
</li>
</ul>
<p>::: {#id117 .section}</p>
<h5 id="documentationheaderlink-18"><a class="header" href="#documentationheaderlink-18">Documentation<a href="#id117" title="Permalink to this heading">¶</a>{.headerlink}</a></h5>
<ul>
<li>
<p>Fix JsonWriterPipeline example (<a href="https://github.com/scrapy/scrapy/issues/2302">issue
2302</a>{.reference
.external}).</p>
</li>
<li>
<p>Various notes: <a href="https://github.com/scrapy/scrapy/issues/2330">issue
2330</a>{.reference
.external} on spider names, <a href="https://github.com/scrapy/scrapy/issues/2329">issue
2329</a>{.reference
.external} on middleware methods processing order, <a href="https://github.com/scrapy/scrapy/issues/2327">issue
2327</a>{.reference
.external} on getting multi-valued HTTP headers as lists.
:::</p>
</li>
</ul>
<p>::: {#id118 .section}</p>
<h5 id="other-changesheaderlink-3"><a class="header" href="#other-changesheaderlink-3">Other changes<a href="#id118" title="Permalink to this heading">¶</a>{.headerlink}</a></h5>
<ul>
<li>Removed [<code>www.</code>{.docutils .literal .notranslate}]{.pre} from
[<code>start_urls</code>{.docutils .literal .notranslate}]{.pre} in built-in
spider templates (<a href="https://github.com/scrapy/scrapy/issues/2299">issue
2299</a>{.reference
.external}).
:::
:::</li>
</ul>
<p>::: {#scrapy-1-2-0-2016-10-03 .section}
[]{#release-1-2-0}</p>
<h4 id="scrapy-120-2016-10-03headerlink"><a class="header" href="#scrapy-120-2016-10-03headerlink">Scrapy 1.2.0 (2016-10-03)<a href="#scrapy-1-2-0-2016-10-03" title="Permalink to this heading">¶</a>{.headerlink}</a></h4>
<p>::: {#id119 .section}</p>
<h5 id="new-featuresheaderlink-19"><a class="header" href="#new-featuresheaderlink-19">New Features<a href="#id119" title="Permalink to this heading">¶</a>{.headerlink}</a></h5>
<ul>
<li>
<p>New <a href="index.html#std-setting-FEED_EXPORT_ENCODING">[<code>FEED_EXPORT_ENCODING</code>{.xref .std .std-setting .docutils
.literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal} setting to customize the encoding
used when writing items to a file. This can be used to turn off
[<code>\uXXXX</code>{.docutils .literal .notranslate}]{.pre} escapes in JSON
output. This is also useful for those wanting something else than
UTF-8 for XML or CSV output (<a href="https://github.com/scrapy/scrapy/issues/2034">issue
2034</a>{.reference
.external}).</p>
</li>
<li>
<p>[<code>startproject</code>{.docutils .literal .notranslate}]{.pre} command now
supports an optional destination directory to override the default
one based on the project name (<a href="https://github.com/scrapy/scrapy/issues/2005">issue
2005</a>{.reference
.external}).</p>
</li>
<li>
<p>New <a href="index.html#std-setting-SCHEDULER_DEBUG">[<code>SCHEDULER_DEBUG</code>{.xref .std .std-setting .docutils .literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal} setting to log requests serialization
failures (<a href="https://github.com/scrapy/scrapy/issues/1610">issue
1610</a>{.reference
.external}).</p>
</li>
<li>
<p>JSON encoder now supports serialization of [<code>set</code>{.docutils .literal
.notranslate}]{.pre} instances (<a href="https://github.com/scrapy/scrapy/issues/2058">issue
2058</a>{.reference
.external}).</p>
</li>
<li>
<p>Interpret [<code>application/json-amazonui-streaming</code>{.docutils .literal
.notranslate}]{.pre} as [<code>TextResponse</code>{.docutils .literal
.notranslate}]{.pre} (<a href="https://github.com/scrapy/scrapy/issues/1503">issue
1503</a>{.reference
.external}).</p>
</li>
<li>
<p>[<code>scrapy</code>{.docutils .literal .notranslate}]{.pre} is imported by
default when using shell tools (<a href="index.html#std-command-shell">[<code>shell</code>{.xref .std .std-command
.docutils .literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal}, <a href="index.html#topics-shell-inspect-response">[inspect_response]{.std
.std-ref}</a>{.hoverxref
.tooltip .reference .internal}) (<a href="https://github.com/scrapy/scrapy/issues/2248">issue
2248</a>{.reference
.external}).
:::</p>
</li>
</ul>
<p>::: {#id120 .section}</p>
<h5 id="bug-fixesheaderlink-23"><a class="header" href="#bug-fixesheaderlink-23">Bug fixes<a href="#id120" title="Permalink to this heading">¶</a>{.headerlink}</a></h5>
<ul>
<li>
<p>DefaultRequestHeaders middleware now runs before UserAgent
middleware (<a href="https://github.com/scrapy/scrapy/issues/2088">issue
2088</a>{.reference
.external}). <strong>Warning: this is technically backward incompatible</strong>,
though we consider this a bug fix.</p>
</li>
<li>
<p>HTTP cache extension and plugins that use the [<code>.scrapy</code>{.docutils
.literal .notranslate}]{.pre} data directory now work outside
projects (<a href="https://github.com/scrapy/scrapy/issues/1581">issue
1581</a>{.reference
.external}). <strong>Warning: this is technically backward incompatible</strong>,
though we consider this a bug fix.</p>
</li>
<li>
<p>[<code>Selector</code>{.docutils .literal .notranslate}]{.pre} does not allow
passing both [<code>response</code>{.docutils .literal .notranslate}]{.pre} and
[<code>text</code>{.docutils .literal .notranslate}]{.pre} anymore (<a href="https://github.com/scrapy/scrapy/issues/2153">issue
2153</a>{.reference
.external}).</p>
</li>
<li>
<p>Fixed logging of wrong callback name with [<code>scrapy</code>{.docutils
.literal .notranslate}]{.pre}<code> </code>{.docutils .literal
.notranslate}[<code>parse</code>{.docutils .literal .notranslate}]{.pre}
(<a href="https://github.com/scrapy/scrapy/issues/2169">issue
2169</a>{.reference
.external}).</p>
</li>
<li>
<p>Fix for an odd gzip decompression bug (<a href="https://github.com/scrapy/scrapy/issues/1606">issue
1606</a>{.reference
.external}).</p>
</li>
<li>
<p>Fix for selected callbacks when using [<code>CrawlSpider</code>{.docutils
.literal .notranslate}]{.pre} with <a href="index.html#std-command-parse">[<code>scrapy</code>{.xref .std
.std-command .docutils .literal .notranslate}]{.pre}<code> </code>{.xref .std
.std-command .docutils .literal .notranslate}[<code>parse</code>{.xref .std
.std-command .docutils .literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal} (<a href="https://github.com/scrapy/scrapy/issues/2225">issue
2225</a>{.reference
.external}).</p>
</li>
<li>
<p>Fix for invalid JSON and XML files when spider yields no items
(<a href="https://github.com/scrapy/scrapy/issues/872">issue 872</a>{.reference
.external}).</p>
</li>
<li>
<p>Implement [<code>flush()</code>{.docutils .literal .notranslate}]{.pre} for
[<code>StreamLogger</code>{.docutils .literal .notranslate}]{.pre} avoiding a
warning in logs (<a href="https://github.com/scrapy/scrapy/issues/2125">issue
2125</a>{.reference
.external}).
:::</p>
</li>
</ul>
<p>::: {#refactoring .section}</p>
<h5 id="refactoringheaderlink"><a class="header" href="#refactoringheaderlink">Refactoring<a href="#refactoring" title="Permalink to this heading">¶</a>{.headerlink}</a></h5>
<ul>
<li>[<code>canonicalize_url</code>{.docutils .literal .notranslate}]{.pre} has been
moved to
<a href="https://w3lib.readthedocs.io/en/latest/w3lib.html#w3lib.url.canonicalize_url">w3lib.url</a>{.reference
.external} (<a href="https://github.com/scrapy/scrapy/issues/2168">issue
2168</a>{.reference
.external}).
:::</li>
</ul>
<p>::: {#tests-requirements .section}</p>
<h5 id="tests--requirementsheaderlink"><a class="header" href="#tests--requirementsheaderlink">Tests &amp; Requirements<a href="#tests-requirements" title="Permalink to this heading">¶</a>{.headerlink}</a></h5>
<p>Scrapy's new requirements baseline is Debian 8 &quot;Jessie&quot;. It was
previously Ubuntu 12.04 Precise. What this means in practice is that we
run continuous integration tests with these (main) packages versions at
a minimum: Twisted 14.0, pyOpenSSL 0.14, lxml 3.4.</p>
<p>Scrapy may very well work with older versions of these packages (the
code base still has switches for older Twisted versions for example) but
it is not guaranteed (because it's not tested anymore).
:::</p>
<p>::: {#id121 .section}</p>
<h5 id="documentationheaderlink-19"><a class="header" href="#documentationheaderlink-19">Documentation<a href="#id121" title="Permalink to this heading">¶</a>{.headerlink}</a></h5>
<ul>
<li>
<p>Grammar fixes: <a href="https://github.com/scrapy/scrapy/issues/2128">issue
2128</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/1566">issue
1566</a>{.reference
.external}.</p>
</li>
<li>
<p>Download stats badge removed from README (<a href="https://github.com/scrapy/scrapy/issues/2160">issue
2160</a>{.reference
.external}).</p>
</li>
<li>
<p>New Scrapy <a href="index.html#topics-architecture">[architecture diagram]{.std
.std-ref}</a>{.hoverxref .tooltip
.reference .internal} (<a href="https://github.com/scrapy/scrapy/issues/2165">issue
2165</a>{.reference
.external}).</p>
</li>
<li>
<p>Updated [<code>Response</code>{.docutils .literal .notranslate}]{.pre}
parameters documentation (<a href="https://github.com/scrapy/scrapy/issues/2197">issue
2197</a>{.reference
.external}).</p>
</li>
<li>
<p>Reworded misleading <a href="index.html#std-setting-RANDOMIZE_DOWNLOAD_DELAY">[<code>RANDOMIZE_DOWNLOAD_DELAY</code>{.xref .std
.std-setting .docutils .literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal} description (<a href="https://github.com/scrapy/scrapy/issues/2190">issue
2190</a>{.reference
.external}).</p>
</li>
<li>
<p>Add StackOverflow as a support channel (<a href="https://github.com/scrapy/scrapy/issues/2257">issue
2257</a>{.reference
.external}).
:::
:::</p>
</li>
</ul>
<p>::: {#scrapy-1-1-4-2017-03-03 .section}
[]{#release-1-1-4}</p>
<h4 id="scrapy-114-2017-03-03headerlink"><a class="header" href="#scrapy-114-2017-03-03headerlink">Scrapy 1.1.4 (2017-03-03)<a href="#scrapy-1-1-4-2017-03-03" title="Permalink to this heading">¶</a>{.headerlink}</a></h4>
<ul>
<li>Packaging fix: disallow unsupported Twisted versions in setup.py
:::</li>
</ul>
<p>::: {#scrapy-1-1-3-2016-09-22 .section}
[]{#release-1-1-3}</p>
<h4 id="scrapy-113-2016-09-22headerlink"><a class="header" href="#scrapy-113-2016-09-22headerlink">Scrapy 1.1.3 (2016-09-22)<a href="#scrapy-1-1-3-2016-09-22" title="Permalink to this heading">¶</a>{.headerlink}</a></h4>
<p>::: {#id122 .section}</p>
<h5 id="bug-fixesheaderlink-24"><a class="header" href="#bug-fixesheaderlink-24">Bug fixes<a href="#id122" title="Permalink to this heading">¶</a>{.headerlink}</a></h5>
<ul>
<li>Class attributes for subclasses of [<code>ImagesPipeline</code>{.docutils
.literal .notranslate}]{.pre} and [<code>FilesPipeline</code>{.docutils
.literal .notranslate}]{.pre} work as they did before 1.1.1 (<a href="https://github.com/scrapy/scrapy/issues/2243">issue
2243</a>{.reference
.external}, fixes <a href="https://github.com/scrapy/scrapy/issues/2198">issue
2198</a>{.reference
.external})
:::</li>
</ul>
<p>::: {#id123 .section}</p>
<h5 id="documentationheaderlink-20"><a class="header" href="#documentationheaderlink-20">Documentation<a href="#id123" title="Permalink to this heading">¶</a>{.headerlink}</a></h5>
<ul>
<li><a href="index.html#intro-overview">[Overview]{.std .std-ref}</a>{.hoverxref
.tooltip .reference .internal} and <a href="index.html#intro-tutorial">[tutorial]{.std
.std-ref}</a>{.hoverxref .tooltip .reference
.internal} rewritten to use
<a href="http://toscrape.com">http://toscrape.com</a>{.reference .external}
websites (<a href="https://github.com/scrapy/scrapy/issues/2236">issue
2236</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/2249">issue
2249</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/2252">issue
2252</a>{.reference
.external}).
:::
:::</li>
</ul>
<p>::: {#scrapy-1-1-2-2016-08-18 .section}
[]{#release-1-1-2}</p>
<h4 id="scrapy-112-2016-08-18headerlink"><a class="header" href="#scrapy-112-2016-08-18headerlink">Scrapy 1.1.2 (2016-08-18)<a href="#scrapy-1-1-2-2016-08-18" title="Permalink to this heading">¶</a>{.headerlink}</a></h4>
<p>::: {#id124 .section}</p>
<h5 id="bug-fixesheaderlink-25"><a class="header" href="#bug-fixesheaderlink-25">Bug fixes<a href="#id124" title="Permalink to this heading">¶</a>{.headerlink}</a></h5>
<ul>
<li>
<p>Introduce a missing <a href="index.html#std-setting-IMAGES_STORE_S3_ACL">[<code>IMAGES_STORE_S3_ACL</code>{.xref .std .std-setting
.docutils .literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal} setting to override the default ACL
policy in [<code>ImagesPipeline</code>{.docutils .literal .notranslate}]{.pre}
when uploading images to S3 (note that default ACL policy is
&quot;private&quot; -- instead of &quot;public-read&quot; -- since Scrapy 1.1.0)</p>
</li>
<li>
<p><a href="index.html#std-setting-IMAGES_EXPIRES">[<code>IMAGES_EXPIRES</code>{.xref .std .std-setting .docutils .literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal} default value set back to 90 (the
regression was introduced in 1.1.1)
:::
:::</p>
</li>
</ul>
<p>::: {#scrapy-1-1-1-2016-07-13 .section}
[]{#release-1-1-1}</p>
<h4 id="scrapy-111-2016-07-13headerlink"><a class="header" href="#scrapy-111-2016-07-13headerlink">Scrapy 1.1.1 (2016-07-13)<a href="#scrapy-1-1-1-2016-07-13" title="Permalink to this heading">¶</a>{.headerlink}</a></h4>
<p>::: {#id125 .section}</p>
<h5 id="bug-fixesheaderlink-26"><a class="header" href="#bug-fixesheaderlink-26">Bug fixes<a href="#id125" title="Permalink to this heading">¶</a>{.headerlink}</a></h5>
<ul>
<li>
<p>Add &quot;Host&quot; header in CONNECT requests to HTTPS proxies (<a href="https://github.com/scrapy/scrapy/issues/2069">issue
2069</a>{.reference
.external})</p>
</li>
<li>
<p>Use response [<code>body</code>{.docutils .literal .notranslate}]{.pre} when
choosing response class (<a href="https://github.com/scrapy/scrapy/issues/2001">issue
2001</a>{.reference
.external}, fixes <a href="https://github.com/scrapy/scrapy/issues/2000">issue
2000</a>{.reference
.external})</p>
</li>
<li>
<p>Do not fail on canonicalizing URLs with wrong netlocs (<a href="https://github.com/scrapy/scrapy/issues/2038">issue
2038</a>{.reference
.external}, fixes <a href="https://github.com/scrapy/scrapy/issues/2010">issue
2010</a>{.reference
.external})</p>
</li>
<li>
<p>a few fixes for [<code>HttpCompressionMiddleware</code>{.docutils .literal
.notranslate}]{.pre} (and [<code>SitemapSpider</code>{.docutils .literal
.notranslate}]{.pre}):</p>
<ul>
<li>
<p>Do not decode HEAD responses (<a href="https://github.com/scrapy/scrapy/issues/2008">issue
2008</a>{.reference
.external}, fixes <a href="https://github.com/scrapy/scrapy/issues/1899">issue
1899</a>{.reference
.external})</p>
</li>
<li>
<p>Handle charset parameter in gzip Content-Type header (<a href="https://github.com/scrapy/scrapy/issues/2050">issue
2050</a>{.reference
.external}, fixes <a href="https://github.com/scrapy/scrapy/issues/2049">issue
2049</a>{.reference
.external})</p>
</li>
<li>
<p>Do not decompress gzip octet-stream responses (<a href="https://github.com/scrapy/scrapy/issues/2065">issue
2065</a>{.reference
.external}, fixes <a href="https://github.com/scrapy/scrapy/issues/2063">issue
2063</a>{.reference
.external})</p>
</li>
</ul>
</li>
<li>
<p>Catch (and ignore with a warning) exception when verifying
certificate against IP-address hosts (<a href="https://github.com/scrapy/scrapy/issues/2094">issue
2094</a>{.reference
.external}, fixes <a href="https://github.com/scrapy/scrapy/issues/2092">issue
2092</a>{.reference
.external})</p>
</li>
<li>
<p>Make [<code>FilesPipeline</code>{.docutils .literal .notranslate}]{.pre} and
[<code>ImagesPipeline</code>{.docutils .literal .notranslate}]{.pre} backward
compatible again regarding the use of legacy class attributes for
customization (<a href="https://github.com/scrapy/scrapy/issues/1989">issue
1989</a>{.reference
.external}, fixes <a href="https://github.com/scrapy/scrapy/issues/1985">issue
1985</a>{.reference
.external})
:::</p>
</li>
</ul>
<p>::: {#id126 .section}</p>
<h5 id="new-featuresheaderlink-20"><a class="header" href="#new-featuresheaderlink-20">New features<a href="#id126" title="Permalink to this heading">¶</a>{.headerlink}</a></h5>
<ul>
<li>
<p>Enable genspider command outside project folder (<a href="https://github.com/scrapy/scrapy/issues/2052">issue
2052</a>{.reference
.external})</p>
</li>
<li>
<p>Retry HTTPS CONNECT [<code>TunnelError</code>{.docutils .literal
.notranslate}]{.pre} by default (<a href="https://github.com/scrapy/scrapy/issues/1974">issue
1974</a>{.reference
.external})
:::</p>
</li>
</ul>
<p>::: {#id127 .section}</p>
<h5 id="documentationheaderlink-21"><a class="header" href="#documentationheaderlink-21">Documentation<a href="#id127" title="Permalink to this heading">¶</a>{.headerlink}</a></h5>
<ul>
<li>
<p>[<code>FEED_TEMPDIR</code>{.docutils .literal .notranslate}]{.pre} setting at
lexicographical position (<a href="https://github.com/scrapy/scrapy/commit/9b3c72c">commit
9b3c72c</a>{.reference
.external})</p>
</li>
<li>
<p>Use idiomatic [<code>.extract_first()</code>{.docutils .literal
.notranslate}]{.pre} in overview (<a href="https://github.com/scrapy/scrapy/issues/1994">issue
1994</a>{.reference
.external})</p>
</li>
<li>
<p>Update years in copyright notice (<a href="https://github.com/scrapy/scrapy/commit/c2c8036">commit
c2c8036</a>{.reference
.external})</p>
</li>
<li>
<p>Add information and example on errbacks (<a href="https://github.com/scrapy/scrapy/issues/1995">issue
1995</a>{.reference
.external})</p>
</li>
<li>
<p>Use &quot;url&quot; variable in downloader middleware example (<a href="https://github.com/scrapy/scrapy/issues/2015">issue
2015</a>{.reference
.external})</p>
</li>
<li>
<p>Grammar fixes (<a href="https://github.com/scrapy/scrapy/issues/2054">issue
2054</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/2120">issue
2120</a>{.reference
.external})</p>
</li>
<li>
<p>New FAQ entry on using BeautifulSoup in spider callbacks (<a href="https://github.com/scrapy/scrapy/issues/2048">issue
2048</a>{.reference
.external})</p>
</li>
<li>
<p>Add notes about Scrapy not working on Windows with Python 3 (<a href="https://github.com/scrapy/scrapy/issues/2060">issue
2060</a>{.reference
.external})</p>
</li>
<li>
<p>Encourage complete titles in pull requests (<a href="https://github.com/scrapy/scrapy/issues/2026">issue
2026</a>{.reference
.external})
:::</p>
</li>
</ul>
<p>::: {#tests .section}</p>
<h5 id="testsheaderlink"><a class="header" href="#testsheaderlink">Tests<a href="#tests" title="Permalink to this heading">¶</a>{.headerlink}</a></h5>
<ul>
<li>Upgrade py.test requirement on Travis CI and Pin pytest-cov to 2.2.1
(<a href="https://github.com/scrapy/scrapy/issues/2095">issue
2095</a>{.reference
.external})
:::
:::</li>
</ul>
<p>::: {#scrapy-1-1-0-2016-05-11 .section}
[]{#release-1-1-0}</p>
<h4 id="scrapy-110-2016-05-11headerlink"><a class="header" href="#scrapy-110-2016-05-11headerlink">Scrapy 1.1.0 (2016-05-11)<a href="#scrapy-1-1-0-2016-05-11" title="Permalink to this heading">¶</a>{.headerlink}</a></h4>
<p>This 1.1 release brings a lot of interesting features and bug fixes:</p>
<ul>
<li>
<p>Scrapy 1.1 has beta Python 3 support (requires Twisted &gt;= 15.5).
See <a href="#news-betapy3">[Beta Python 3 Support]{.std
.std-ref}</a>{.hoverxref .tooltip .reference .internal}
for more details and some limitations.</p>
</li>
<li>
<p>Hot new features:</p>
<ul>
<li>
<p>Item loaders now support nested loaders (<a href="https://github.com/scrapy/scrapy/issues/1467">issue
1467</a>{.reference
.external}).</p>
</li>
<li>
<p>[<code>FormRequest.from_response</code>{.docutils .literal
.notranslate}]{.pre} improvements (<a href="https://github.com/scrapy/scrapy/issues/1382">issue
1382</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/1137">issue
1137</a>{.reference
.external}).</p>
</li>
<li>
<p>Added setting <a href="index.html#std-setting-AUTOTHROTTLE_TARGET_CONCURRENCY">[<code>AUTOTHROTTLE_TARGET_CONCURRENCY</code>{.xref .std
.std-setting .docutils .literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal} and improved AutoThrottle docs
(<a href="https://github.com/scrapy/scrapy/issues/1324">issue
1324</a>{.reference
.external}).</p>
</li>
<li>
<p>Added [<code>response.text</code>{.docutils .literal .notranslate}]{.pre}
to get body as unicode (<a href="https://github.com/scrapy/scrapy/issues/1730">issue
1730</a>{.reference
.external}).</p>
</li>
<li>
<p>Anonymous S3 connections (<a href="https://github.com/scrapy/scrapy/issues/1358">issue
1358</a>{.reference
.external}).</p>
</li>
<li>
<p>Deferreds in downloader middlewares (<a href="https://github.com/scrapy/scrapy/issues/1473">issue
1473</a>{.reference
.external}). This enables better robots.txt handling (<a href="https://github.com/scrapy/scrapy/issues/1471">issue
1471</a>{.reference
.external}).</p>
</li>
<li>
<p>HTTP caching now follows RFC2616 more closely, added settings
<a href="index.html#std-setting-HTTPCACHE_ALWAYS_STORE">[<code>HTTPCACHE_ALWAYS_STORE</code>{.xref .std .std-setting .docutils
.literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal} and
<a href="index.html#std-setting-HTTPCACHE_IGNORE_RESPONSE_CACHE_CONTROLS">[<code>HTTPCACHE_IGNORE_RESPONSE_CACHE_CONTROLS</code>{.xref .std
.std-setting .docutils .literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal} (<a href="https://github.com/scrapy/scrapy/issues/1151">issue
1151</a>{.reference
.external}).</p>
</li>
<li>
<p>Selectors were extracted to the
<a href="https://github.com/scrapy/parsel">parsel</a>{.reference .external}
library (<a href="https://github.com/scrapy/scrapy/issues/1409">issue
1409</a>{.reference
.external}). This means you can use Scrapy Selectors without
Scrapy and also upgrade the selectors engine without needing to
upgrade Scrapy.</p>
</li>
<li>
<p>HTTPS downloader now does TLS protocol negotiation by default,
instead of forcing TLS 1.0. You can also set the SSL/TLS method
using the new <a href="index.html#std-setting-DOWNLOADER_CLIENT_TLS_METHOD">[<code>DOWNLOADER_CLIENT_TLS_METHOD</code>{.xref .std
.std-setting .docutils .literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal}.</p>
</li>
</ul>
</li>
<li>
<p>These bug fixes may require your attention:</p>
<ul>
<li>
<p>Don't retry bad requests (HTTP 400) by default (<a href="https://github.com/scrapy/scrapy/issues/1289">issue
1289</a>{.reference
.external}). If you need the old behavior, add [<code>400</code>{.docutils
.literal .notranslate}]{.pre} to <a href="index.html#std-setting-RETRY_HTTP_CODES">[<code>RETRY_HTTP_CODES</code>{.xref .std
.std-setting .docutils .literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal}.</p>
</li>
<li>
<p>Fix shell files argument handling (<a href="https://github.com/scrapy/scrapy/issues/1710">issue
1710</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/1550">issue
1550</a>{.reference
.external}). If you try [<code>scrapy</code>{.docutils .literal
.notranslate}]{.pre}<code> </code>{.docutils .literal
.notranslate}[<code>shell</code>{.docutils .literal
.notranslate}]{.pre}<code> </code>{.docutils .literal
.notranslate}[<code>index.html</code>{.docutils .literal
.notranslate}]{.pre} it will try to load the URL
<a href="http://index.html">http://index.html</a>{.reference .external},
use [<code>scrapy</code>{.docutils .literal
.notranslate}]{.pre}<code> </code>{.docutils .literal
.notranslate}[<code>shell</code>{.docutils .literal
.notranslate}]{.pre}<code> </code>{.docutils .literal
.notranslate}[<code>./index.html</code>{.docutils .literal
.notranslate}]{.pre} to load a local file.</p>
</li>
<li>
<p>Robots.txt compliance is now enabled by default for
newly-created projects (<a href="https://github.com/scrapy/scrapy/issues/1724">issue
1724</a>{.reference
.external}). Scrapy will also wait for robots.txt to be
downloaded before proceeding with the crawl (<a href="https://github.com/scrapy/scrapy/issues/1735">issue
1735</a>{.reference
.external}). If you want to disable this behavior, update
<a href="index.html#std-setting-ROBOTSTXT_OBEY">[<code>ROBOTSTXT_OBEY</code>{.xref .std .std-setting .docutils .literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal} in [<code>settings.py</code>{.docutils
.literal .notranslate}]{.pre} file after creating a new project.</p>
</li>
<li>
<p>Exporters now work on unicode, instead of bytes by default
(<a href="https://github.com/scrapy/scrapy/issues/1080">issue
1080</a>{.reference
.external}). If you use <a href="index.html#scrapy.exporters.PythonItemExporter" title="scrapy.exporters.PythonItemExporter">[<code>PythonItemExporter</code>{.xref .py
.py-class .docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal}, you may want to update your code to disable binary
mode which is now deprecated.</p>
</li>
<li>
<p>Accept XML node names containing dots as valid (<a href="https://github.com/scrapy/scrapy/issues/1533">issue
1533</a>{.reference
.external}).</p>
</li>
<li>
<p>When uploading files or images to S3 (with
[<code>FilesPipeline</code>{.docutils .literal .notranslate}]{.pre} or
[<code>ImagesPipeline</code>{.docutils .literal .notranslate}]{.pre}), the
default ACL policy is now &quot;private&quot; instead of &quot;public&quot;
<strong>Warning: backward incompatible!</strong>. You can use
<a href="index.html#std-setting-FILES_STORE_S3_ACL">[<code>FILES_STORE_S3_ACL</code>{.xref .std .std-setting .docutils
.literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal} to change it.</p>
</li>
<li>
<p>We've reimplemented [<code>canonicalize_url()</code>{.docutils .literal
.notranslate}]{.pre} for more correct output, especially for
URLs with non-ASCII characters (<a href="https://github.com/scrapy/scrapy/issues/1947">issue
1947</a>{.reference
.external}). This could change link extractors output compared
to previous Scrapy versions. This may also invalidate some cache
entries you could still have from pre-1.1 runs. <strong>Warning:
backward incompatible!</strong>.</p>
</li>
</ul>
</li>
</ul>
<p>Keep reading for more details on other improvements and bug fixes.</p>
<p>::: {#beta-python-3-support .section}
[]{#news-betapy3}</p>
<h5 id="beta-python-3-supportheaderlink"><a class="header" href="#beta-python-3-supportheaderlink">Beta Python 3 Support<a href="#beta-python-3-support" title="Permalink to this heading">¶</a>{.headerlink}</a></h5>
<p>We have been <a href="https://github.com/scrapy/scrapy/wiki/Python-3-Porting">hard at work to make Scrapy run on Python
3</a>{.reference
.external}. As a result, now you can run spiders on Python 3.3, 3.4 and
3.5 (Twisted &gt;= 15.5 required). Some features are still missing (and
some may never be ported).</p>
<p>Almost all builtin extensions/middlewares are expected to work. However,
we are aware of some limitations in Python 3:</p>
<ul>
<li>
<p>Scrapy does not work on Windows with Python 3</p>
</li>
<li>
<p>Sending emails is not supported</p>
</li>
<li>
<p>FTP download handler is not supported</p>
</li>
<li>
<p>Telnet console is not supported
:::</p>
</li>
</ul>
<p>::: {#additional-new-features-and-enhancements .section}</p>
<h5 id="additional-new-features-and-enhancementsheaderlink"><a class="header" href="#additional-new-features-and-enhancementsheaderlink">Additional New Features and Enhancements<a href="#additional-new-features-and-enhancements" title="Permalink to this heading">¶</a>{.headerlink}</a></h5>
<ul>
<li>
<p>Scrapy now has a <a href="https://github.com/scrapy/scrapy/blob/master/CODE_OF_CONDUCT.md">Code of
Conduct</a>{.reference
.external} (<a href="https://github.com/scrapy/scrapy/issues/1681">issue
1681</a>{.reference
.external}).</p>
</li>
<li>
<p>Command line tool now has completion for zsh (<a href="https://github.com/scrapy/scrapy/issues/934">issue
934</a>{.reference
.external}).</p>
</li>
<li>
<p>Improvements to [<code>scrapy</code>{.docutils .literal
.notranslate}]{.pre}<code> </code>{.docutils .literal
.notranslate}[<code>shell</code>{.docutils .literal .notranslate}]{.pre}:</p>
<ul>
<li>
<p>Support for bpython and configure preferred Python shell via
[<code>SCRAPY_PYTHON_SHELL</code>{.docutils .literal .notranslate}]{.pre}
(<a href="https://github.com/scrapy/scrapy/issues/1100">issue
1100</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/1444">issue
1444</a>{.reference
.external}).</p>
</li>
<li>
<p>Support URLs without scheme (<a href="https://github.com/scrapy/scrapy/issues/1498">issue
1498</a>{.reference
.external}) <strong>Warning: backward incompatible!</strong></p>
</li>
<li>
<p>Bring back support for relative file path (<a href="https://github.com/scrapy/scrapy/issues/1710">issue
1710</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/1550">issue
1550</a>{.reference
.external}).</p>
</li>
</ul>
</li>
<li>
<p>Added <a href="index.html#std-setting-MEMUSAGE_CHECK_INTERVAL_SECONDS">[<code>MEMUSAGE_CHECK_INTERVAL_SECONDS</code>{.xref .std .std-setting
.docutils .literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal} setting to change default check
interval (<a href="https://github.com/scrapy/scrapy/issues/1282">issue
1282</a>{.reference
.external}).</p>
</li>
<li>
<p>Download handlers are now lazy-loaded on first request using their
scheme (<a href="https://github.com/scrapy/scrapy/issues/1390">issue
1390</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/1421">issue
1421</a>{.reference
.external}).</p>
</li>
<li>
<p>HTTPS download handlers do not force TLS 1.0 anymore; instead,
OpenSSL's [<code>SSLv23_method()/TLS_method()</code>{.docutils .literal
.notranslate}]{.pre} is used allowing to try negotiating with the
remote hosts the highest TLS protocol version it can (<a href="https://github.com/scrapy/scrapy/issues/1794">issue
1794</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/1629">issue
1629</a>{.reference
.external}).</p>
</li>
<li>
<p>[<code>RedirectMiddleware</code>{.docutils .literal .notranslate}]{.pre} now
skips the status codes from [<code>handle_httpstatus_list</code>{.docutils
.literal .notranslate}]{.pre} on spider attribute or in
[<code>Request</code>{.docutils .literal .notranslate}]{.pre}'s
[<code>meta</code>{.docutils .literal .notranslate}]{.pre} key (<a href="https://github.com/scrapy/scrapy/issues/1334">issue
1334</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/1364">issue
1364</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/1447">issue
1447</a>{.reference
.external}).</p>
</li>
<li>
<p>Form submission:</p>
<ul>
<li>
<p>now works with [<code>&lt;button&gt;</code>{.docutils .literal
.notranslate}]{.pre} elements too (<a href="https://github.com/scrapy/scrapy/issues/1469">issue
1469</a>{.reference
.external}).</p>
</li>
<li>
<p>an empty string is now used for submit buttons without a value
(<a href="https://github.com/scrapy/scrapy/issues/1472">issue
1472</a>{.reference
.external})</p>
</li>
</ul>
</li>
<li>
<p>Dict-like settings now have per-key priorities (<a href="https://github.com/scrapy/scrapy/issues/1135">issue
1135</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/1149">issue
1149</a>{.reference
.external} and <a href="https://github.com/scrapy/scrapy/issues/1586">issue
1586</a>{.reference
.external}).</p>
</li>
<li>
<p>Sending non-ASCII emails (<a href="https://github.com/scrapy/scrapy/issues/1662">issue
1662</a>{.reference
.external})</p>
</li>
<li>
<p>[<code>CloseSpider</code>{.docutils .literal .notranslate}]{.pre} and
[<code>SpiderState</code>{.docutils .literal .notranslate}]{.pre} extensions
now get disabled if no relevant setting is set (<a href="https://github.com/scrapy/scrapy/issues/1723">issue
1723</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/1725">issue
1725</a>{.reference
.external}).</p>
</li>
<li>
<p>Added method [<code>ExecutionEngine.close</code>{.docutils .literal
.notranslate}]{.pre} (<a href="https://github.com/scrapy/scrapy/issues/1423">issue
1423</a>{.reference
.external}).</p>
</li>
<li>
<p>Added method [<code>CrawlerRunner.create_crawler</code>{.docutils .literal
.notranslate}]{.pre} (<a href="https://github.com/scrapy/scrapy/issues/1528">issue
1528</a>{.reference
.external}).</p>
</li>
<li>
<p>Scheduler priority queue can now be customized via
<a href="index.html#std-setting-SCHEDULER_PRIORITY_QUEUE">[<code>SCHEDULER_PRIORITY_QUEUE</code>{.xref .std .std-setting .docutils
.literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal} (<a href="https://github.com/scrapy/scrapy/issues/1822">issue
1822</a>{.reference
.external}).</p>
</li>
<li>
<p>[<code>.pps</code>{.docutils .literal .notranslate}]{.pre} links are now
ignored by default in link extractors (<a href="https://github.com/scrapy/scrapy/issues/1835">issue
1835</a>{.reference
.external}).</p>
</li>
<li>
<p>temporary data folder for FTP and S3 feed storages can be customized
using a new <a href="index.html#std-setting-FEED_TEMPDIR">[<code>FEED_TEMPDIR</code>{.xref .std .std-setting .docutils
.literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal} setting (<a href="https://github.com/scrapy/scrapy/issues/1847">issue
1847</a>{.reference
.external}).</p>
</li>
<li>
<p>[<code>FilesPipeline</code>{.docutils .literal .notranslate}]{.pre} and
[<code>ImagesPipeline</code>{.docutils .literal .notranslate}]{.pre} settings
are now instance attributes instead of class attributes, enabling
spider-specific behaviors (<a href="https://github.com/scrapy/scrapy/issues/1891">issue
1891</a>{.reference
.external}).</p>
</li>
<li>
<p>[<code>JsonItemExporter</code>{.docutils .literal .notranslate}]{.pre} now
formats opening and closing square brackets on their own line (first
and last lines of output file) (<a href="https://github.com/scrapy/scrapy/issues/1950">issue
1950</a>{.reference
.external}).</p>
</li>
<li>
<p>If available, [<code>botocore</code>{.docutils .literal .notranslate}]{.pre} is
used for [<code>S3FeedStorage</code>{.docutils .literal .notranslate}]{.pre},
[<code>S3DownloadHandler</code>{.docutils .literal .notranslate}]{.pre} and
[<code>S3FilesStore</code>{.docutils .literal .notranslate}]{.pre} (<a href="https://github.com/scrapy/scrapy/issues/1761">issue
1761</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/1883">issue
1883</a>{.reference
.external}).</p>
</li>
<li>
<p>Tons of documentation updates and related fixes (<a href="https://github.com/scrapy/scrapy/issues/1291">issue
1291</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/1302">issue
1302</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/1335">issue
1335</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/1683">issue
1683</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/1660">issue
1660</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/1642">issue
1642</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/1721">issue
1721</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/1727">issue
1727</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/1879">issue
1879</a>{.reference
.external}).</p>
</li>
<li>
<p>Other refactoring, optimizations and cleanup (<a href="https://github.com/scrapy/scrapy/issues/1476">issue
1476</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/1481">issue
1481</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/1477">issue
1477</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/1315">issue
1315</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/1290">issue
1290</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/1750">issue
1750</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/1881">issue
1881</a>{.reference
.external}).
:::</p>
</li>
</ul>
<p>::: {#deprecations-and-removals .section}</p>
<h5 id="deprecations-and-removalsheaderlink"><a class="header" href="#deprecations-and-removalsheaderlink">Deprecations and Removals<a href="#deprecations-and-removals" title="Permalink to this heading">¶</a>{.headerlink}</a></h5>
<ul>
<li>
<p>Added [<code>to_bytes</code>{.docutils .literal .notranslate}]{.pre} and
[<code>to_unicode</code>{.docutils .literal .notranslate}]{.pre}, deprecated
[<code>str_to_unicode</code>{.docutils .literal .notranslate}]{.pre} and
[<code>unicode_to_str</code>{.docutils .literal .notranslate}]{.pre} functions
(<a href="https://github.com/scrapy/scrapy/issues/778">issue 778</a>{.reference
.external}).</p>
</li>
<li>
<p>[<code>binary_is_text</code>{.docutils .literal .notranslate}]{.pre} is
introduced, to replace use of [<code>isbinarytext</code>{.docutils .literal
.notranslate}]{.pre} (but with inverse return value) (<a href="https://github.com/scrapy/scrapy/issues/1851">issue
1851</a>{.reference
.external})</p>
</li>
<li>
<p>The [<code>optional_features</code>{.docutils .literal .notranslate}]{.pre} set
has been removed (<a href="https://github.com/scrapy/scrapy/issues/1359">issue
1359</a>{.reference
.external}).</p>
</li>
<li>
<p>The [<code>--lsprof</code>{.docutils .literal .notranslate}]{.pre} command line
option has been removed (<a href="https://github.com/scrapy/scrapy/issues/1689">issue
1689</a>{.reference
.external}). <strong>Warning: backward incompatible</strong>, but doesn't break
user code.</p>
</li>
<li>
<p>The following datatypes were deprecated (<a href="https://github.com/scrapy/scrapy/issues/1720">issue
1720</a>{.reference
.external}):</p>
<ul>
<li>
<p>[<code>scrapy.utils.datatypes.MultiValueDictKeyError</code>{.docutils
.literal .notranslate}]{.pre}</p>
</li>
<li>
<p>[<code>scrapy.utils.datatypes.MultiValueDict</code>{.docutils .literal
.notranslate}]{.pre}</p>
</li>
<li>
<p>[<code>scrapy.utils.datatypes.SiteNode</code>{.docutils .literal
.notranslate}]{.pre}</p>
</li>
</ul>
</li>
<li>
<p>The previously bundled [<code>scrapy.xlib.pydispatch</code>{.docutils .literal
.notranslate}]{.pre} library was deprecated and replaced by
<a href="https://pypi.org/project/PyDispatcher/">pydispatcher</a>{.reference
.external}.
:::</p>
</li>
</ul>
<p>::: {#relocations .section}</p>
<h5 id="relocationsheaderlink"><a class="header" href="#relocationsheaderlink">Relocations<a href="#relocations" title="Permalink to this heading">¶</a>{.headerlink}</a></h5>
<ul>
<li>
<p>[<code>telnetconsole</code>{.docutils .literal .notranslate}]{.pre} was
relocated to [<code>extensions/</code>{.docutils .literal .notranslate}]{.pre}
(<a href="https://github.com/scrapy/scrapy/issues/1524">issue
1524</a>{.reference
.external}).</p>
<ul>
<li>Note: telnet is not enabled on Python 3
(<a href="https://github.com/scrapy/scrapy/pull/1524#issuecomment-146985595">https://github.com/scrapy/scrapy/pull/1524#issuecomment-146985595</a>{.reference
.external})
:::</li>
</ul>
</li>
</ul>
<p>::: {#bugfixes .section}</p>
<h5 id="bugfixesheaderlink"><a class="header" href="#bugfixesheaderlink">Bugfixes<a href="#bugfixes" title="Permalink to this heading">¶</a>{.headerlink}</a></h5>
<ul>
<li>
<p>Scrapy does not retry requests that got a [<code>HTTP</code>{.docutils .literal
.notranslate}]{.pre}<code> </code>{.docutils .literal
.notranslate}[<code>400</code>{.docutils .literal
.notranslate}]{.pre}<code> </code>{.docutils .literal
.notranslate}[<code>Bad</code>{.docutils .literal
.notranslate}]{.pre}<code> </code>{.docutils .literal
.notranslate}[<code>Request</code>{.docutils .literal .notranslate}]{.pre}
response anymore (<a href="https://github.com/scrapy/scrapy/issues/1289">issue
1289</a>{.reference
.external}). <strong>Warning: backward incompatible!</strong></p>
</li>
<li>
<p>Support empty password for http_proxy config (<a href="https://github.com/scrapy/scrapy/issues/1274">issue
1274</a>{.reference
.external}).</p>
</li>
<li>
<p>Interpret [<code>application/x-json</code>{.docutils .literal
.notranslate}]{.pre} as [<code>TextResponse</code>{.docutils .literal
.notranslate}]{.pre} (<a href="https://github.com/scrapy/scrapy/issues/1333">issue
1333</a>{.reference
.external}).</p>
</li>
<li>
<p>Support link rel attribute with multiple values (<a href="https://github.com/scrapy/scrapy/issues/1201">issue
1201</a>{.reference
.external}).</p>
</li>
<li>
<p>Fixed [<code>scrapy.http.FormRequest.from_response</code>{.docutils .literal
.notranslate}]{.pre} when there is a [<code>&lt;base&gt;</code>{.docutils .literal
.notranslate}]{.pre} tag (<a href="https://github.com/scrapy/scrapy/issues/1564">issue
1564</a>{.reference
.external}).</p>
</li>
<li>
<p>Fixed <a href="index.html#std-setting-TEMPLATES_DIR">[<code>TEMPLATES_DIR</code>{.xref .std .std-setting .docutils .literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal} handling (<a href="https://github.com/scrapy/scrapy/issues/1575">issue
1575</a>{.reference
.external}).</p>
</li>
<li>
<p>Various [<code>FormRequest</code>{.docutils .literal .notranslate}]{.pre} fixes
(<a href="https://github.com/scrapy/scrapy/issues/1595">issue
1595</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/1596">issue
1596</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/1597">issue
1597</a>{.reference
.external}).</p>
</li>
<li>
<p>Makes [<code>_monkeypatches</code>{.docutils .literal .notranslate}]{.pre} more
robust (<a href="https://github.com/scrapy/scrapy/issues/1634">issue
1634</a>{.reference
.external}).</p>
</li>
<li>
<p>Fixed bug on [<code>XMLItemExporter</code>{.docutils .literal
.notranslate}]{.pre} with non-string fields in items (<a href="https://github.com/scrapy/scrapy/issues/1738">issue
1738</a>{.reference
.external}).</p>
</li>
<li>
<p>Fixed startproject command in macOS (<a href="https://github.com/scrapy/scrapy/issues/1635">issue
1635</a>{.reference
.external}).</p>
</li>
<li>
<p>Fixed <a href="index.html#scrapy.exporters.PythonItemExporter" title="scrapy.exporters.PythonItemExporter">[<code>PythonItemExporter</code>{.xref .py .py-class .docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal} and CSVExporter for non-string item types (<a href="https://github.com/scrapy/scrapy/issues/1737">issue
1737</a>{.reference
.external}).</p>
</li>
<li>
<p>Various logging related fixes (<a href="https://github.com/scrapy/scrapy/issues/1294">issue
1294</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/1419">issue
1419</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/1263">issue
1263</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/1624">issue
1624</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/1654">issue
1654</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/1722">issue
1722</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/1726">issue
1726</a>{.reference
.external} and <a href="https://github.com/scrapy/scrapy/issues/1303">issue
1303</a>{.reference
.external}).</p>
</li>
<li>
<p>Fixed bug in [<code>utils.template.render_templatefile()</code>{.docutils
.literal .notranslate}]{.pre} (<a href="https://github.com/scrapy/scrapy/issues/1212">issue
1212</a>{.reference
.external}).</p>
</li>
<li>
<p>sitemaps extraction from [<code>robots.txt</code>{.docutils .literal
.notranslate}]{.pre} is now case-insensitive (<a href="https://github.com/scrapy/scrapy/issues/1902">issue
1902</a>{.reference
.external}).</p>
</li>
<li>
<p>HTTPS+CONNECT tunnels could get mixed up when using multiple proxies
to same remote host (<a href="https://github.com/scrapy/scrapy/issues/1912">issue
1912</a>{.reference
.external}).
:::
:::</p>
</li>
</ul>
<p>::: {#scrapy-1-0-7-2017-03-03 .section}
[]{#release-1-0-7}</p>
<h4 id="scrapy-107-2017-03-03headerlink"><a class="header" href="#scrapy-107-2017-03-03headerlink">Scrapy 1.0.7 (2017-03-03)<a href="#scrapy-1-0-7-2017-03-03" title="Permalink to this heading">¶</a>{.headerlink}</a></h4>
<ul>
<li>Packaging fix: disallow unsupported Twisted versions in setup.py
:::</li>
</ul>
<p>::: {#scrapy-1-0-6-2016-05-04 .section}
[]{#release-1-0-6}</p>
<h4 id="scrapy-106-2016-05-04headerlink"><a class="header" href="#scrapy-106-2016-05-04headerlink">Scrapy 1.0.6 (2016-05-04)<a href="#scrapy-1-0-6-2016-05-04" title="Permalink to this heading">¶</a>{.headerlink}</a></h4>
<ul>
<li>
<p>FIX: RetryMiddleware is now robust to non-standard HTTP status codes
(<a href="https://github.com/scrapy/scrapy/issues/1857">issue
1857</a>{.reference
.external})</p>
</li>
<li>
<p>FIX: Filestorage HTTP cache was checking wrong modified time (<a href="https://github.com/scrapy/scrapy/issues/1875">issue
1875</a>{.reference
.external})</p>
</li>
<li>
<p>DOC: Support for Sphinx 1.4+ (<a href="https://github.com/scrapy/scrapy/issues/1893">issue
1893</a>{.reference
.external})</p>
</li>
<li>
<p>DOC: Consistency in selectors examples (<a href="https://github.com/scrapy/scrapy/issues/1869">issue
1869</a>{.reference
.external})
:::</p>
</li>
</ul>
<p>::: {#scrapy-1-0-5-2016-02-04 .section}
[]{#release-1-0-5}</p>
<h4 id="scrapy-105-2016-02-04headerlink"><a class="header" href="#scrapy-105-2016-02-04headerlink">Scrapy 1.0.5 (2016-02-04)<a href="#scrapy-1-0-5-2016-02-04" title="Permalink to this heading">¶</a>{.headerlink}</a></h4>
<ul>
<li>
<p>FIX: [Backport] Ignore bogus links in LinkExtractors (fixes <a href="https://github.com/scrapy/scrapy/issues/907">issue
907</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/commit/108195e">commit
108195e</a>{.reference
.external})</p>
</li>
<li>
<p>TST: Changed buildbot makefile to use 'pytest' (<a href="https://github.com/scrapy/scrapy/commit/1f3d90a">commit
1f3d90a</a>{.reference
.external})</p>
</li>
<li>
<p>DOC: Fixed typos in tutorial and media-pipeline (<a href="https://github.com/scrapy/scrapy/commit/808a9ea">commit
808a9ea</a>{.reference
.external} and <a href="https://github.com/scrapy/scrapy/commit/803bd87">commit
803bd87</a>{.reference
.external})</p>
</li>
<li>
<p>DOC: Add AjaxCrawlMiddleware to DOWNLOADER_MIDDLEWARES_BASE in
settings docs (<a href="https://github.com/scrapy/scrapy/commit/aa94121">commit
aa94121</a>{.reference
.external})
:::</p>
</li>
</ul>
<p>::: {#scrapy-1-0-4-2015-12-30 .section}
[]{#release-1-0-4}</p>
<h4 id="scrapy-104-2015-12-30headerlink"><a class="header" href="#scrapy-104-2015-12-30headerlink">Scrapy 1.0.4 (2015-12-30)<a href="#scrapy-1-0-4-2015-12-30" title="Permalink to this heading">¶</a>{.headerlink}</a></h4>
<ul>
<li>
<p>Ignoring xlib/tx folder, depending on Twisted version. (<a href="https://github.com/scrapy/scrapy/commit/7dfa979">commit
7dfa979</a>{.reference
.external})</p>
</li>
<li>
<p>Run on new travis-ci infra (<a href="https://github.com/scrapy/scrapy/commit/6e42f0b">commit
6e42f0b</a>{.reference
.external})</p>
</li>
<li>
<p>Spelling fixes (<a href="https://github.com/scrapy/scrapy/commit/823a1cc">commit
823a1cc</a>{.reference
.external})</p>
</li>
<li>
<p>escape nodename in xmliter regex (<a href="https://github.com/scrapy/scrapy/commit/da3c155">commit
da3c155</a>{.reference
.external})</p>
</li>
<li>
<p>test xml nodename with dots (<a href="https://github.com/scrapy/scrapy/commit/4418fc3">commit
4418fc3</a>{.reference
.external})</p>
</li>
<li>
<p>TST don't use broken Pillow version in tests (<a href="https://github.com/scrapy/scrapy/commit/a55078c">commit
a55078c</a>{.reference
.external})</p>
</li>
<li>
<p>disable log on version command. closes #1426 (<a href="https://github.com/scrapy/scrapy/commit/86fc330">commit
86fc330</a>{.reference
.external})</p>
</li>
<li>
<p>disable log on startproject command (<a href="https://github.com/scrapy/scrapy/commit/db4c9fe">commit
db4c9fe</a>{.reference
.external})</p>
</li>
<li>
<p>Add PyPI download stats badge (<a href="https://github.com/scrapy/scrapy/commit/df2b944">commit
df2b944</a>{.reference
.external})</p>
</li>
<li>
<p>don't run tests twice on Travis if a PR is made from a scrapy/scrapy
branch (<a href="https://github.com/scrapy/scrapy/commit/a83ab41">commit
a83ab41</a>{.reference
.external})</p>
</li>
<li>
<p>Add Python 3 porting status badge to the README (<a href="https://github.com/scrapy/scrapy/commit/73ac80d">commit
73ac80d</a>{.reference
.external})</p>
</li>
<li>
<p>fixed RFPDupeFilter persistence (<a href="https://github.com/scrapy/scrapy/commit/97d080e">commit
97d080e</a>{.reference
.external})</p>
</li>
<li>
<p>TST a test to show that dupefilter persistence is not working
(<a href="https://github.com/scrapy/scrapy/commit/97f2fb3">commit
97f2fb3</a>{.reference
.external})</p>
</li>
<li>
<p>explicit close file on <a href="file://">file://</a>{.reference .external}
scheme handler (<a href="https://github.com/scrapy/scrapy/commit/d9b4850">commit
d9b4850</a>{.reference
.external})</p>
</li>
<li>
<p>Disable dupefilter in shell (<a href="https://github.com/scrapy/scrapy/commit/c0d0734">commit
c0d0734</a>{.reference
.external})</p>
</li>
<li>
<p>DOC: Add captions to toctrees which appear in sidebar (<a href="https://github.com/scrapy/scrapy/commit/aa239ad">commit
aa239ad</a>{.reference
.external})</p>
</li>
<li>
<p>DOC Removed pywin32 from install instructions as it's already
declared as dependency. (<a href="https://github.com/scrapy/scrapy/commit/10eb400">commit
10eb400</a>{.reference
.external})</p>
</li>
<li>
<p>Added installation notes about using Conda for Windows and other
OSes. (<a href="https://github.com/scrapy/scrapy/commit/1c3600a">commit
1c3600a</a>{.reference
.external})</p>
</li>
<li>
<p>Fixed minor grammar issues. (<a href="https://github.com/scrapy/scrapy/commit/7f4ddd5">commit
7f4ddd5</a>{.reference
.external})</p>
</li>
<li>
<p>fixed a typo in the documentation. (<a href="https://github.com/scrapy/scrapy/commit/b71f677">commit
b71f677</a>{.reference
.external})</p>
</li>
<li>
<p>Version 1 now exists (<a href="https://github.com/scrapy/scrapy/commit/5456c0e">commit
5456c0e</a>{.reference
.external})</p>
</li>
<li>
<p>fix another invalid xpath error (<a href="https://github.com/scrapy/scrapy/commit/0a1366e">commit
0a1366e</a>{.reference
.external})</p>
</li>
<li>
<p>fix ValueError: Invalid XPath: //div/[id=&quot;not-exists&quot;]/text() on
selectors.rst (<a href="https://github.com/scrapy/scrapy/commit/ca8d60f">commit
ca8d60f</a>{.reference
.external})</p>
</li>
<li>
<p>Typos corrections (<a href="https://github.com/scrapy/scrapy/commit/7067117">commit
7067117</a>{.reference
.external})</p>
</li>
<li>
<p>fix typos in downloader-middleware.rst and exceptions.rst, middlware
-&gt; middleware (<a href="https://github.com/scrapy/scrapy/commit/32f115c">commit
32f115c</a>{.reference
.external})</p>
</li>
<li>
<p>Add note to Ubuntu install section about Debian compatibility
(<a href="https://github.com/scrapy/scrapy/commit/23fda69">commit
23fda69</a>{.reference
.external})</p>
</li>
<li>
<p>Replace alternative macOS install workaround with virtualenv
(<a href="https://github.com/scrapy/scrapy/commit/98b63ee">commit
98b63ee</a>{.reference
.external})</p>
</li>
<li>
<p>Reference Homebrew's homepage for installation instructions (<a href="https://github.com/scrapy/scrapy/commit/1925db1">commit
1925db1</a>{.reference
.external})</p>
</li>
<li>
<p>Add oldest supported tox version to contributing docs (<a href="https://github.com/scrapy/scrapy/commit/5d10d6d">commit
5d10d6d</a>{.reference
.external})</p>
</li>
<li>
<p>Note in install docs about pip being already included in
python&gt;=2.7.9 (<a href="https://github.com/scrapy/scrapy/commit/85c980e">commit
85c980e</a>{.reference
.external})</p>
</li>
<li>
<p>Add non-python dependencies to Ubuntu install section in the docs
(<a href="https://github.com/scrapy/scrapy/commit/fbd010d">commit
fbd010d</a>{.reference
.external})</p>
</li>
<li>
<p>Add macOS installation section to docs (<a href="https://github.com/scrapy/scrapy/commit/d8f4cba">commit
d8f4cba</a>{.reference
.external})</p>
</li>
<li>
<p>DOC(ENH): specify path to rtd theme explicitly (<a href="https://github.com/scrapy/scrapy/commit/de73b1a">commit
de73b1a</a>{.reference
.external})</p>
</li>
<li>
<p>minor: scrapy.Spider docs grammar (<a href="https://github.com/scrapy/scrapy/commit/1ddcc7b">commit
1ddcc7b</a>{.reference
.external})</p>
</li>
<li>
<p>Make common practices sample code match the comments (<a href="https://github.com/scrapy/scrapy/commit/1b85bcf">commit
1b85bcf</a>{.reference
.external})</p>
</li>
<li>
<p>nextcall repetitive calls (heartbeats). (<a href="https://github.com/scrapy/scrapy/commit/55f7104">commit
55f7104</a>{.reference
.external})</p>
</li>
<li>
<p>Backport fix compatibility with Twisted 15.4.0 (<a href="https://github.com/scrapy/scrapy/commit/b262411">commit
b262411</a>{.reference
.external})</p>
</li>
<li>
<p>pin pytest to 2.7.3 (<a href="https://github.com/scrapy/scrapy/commit/a6535c2">commit
a6535c2</a>{.reference
.external})</p>
</li>
<li>
<p>Merge pull request #1512 from mgedmin/patch-1 (<a href="https://github.com/scrapy/scrapy/commit/8876111">commit
8876111</a>{.reference
.external})</p>
</li>
<li>
<p>Merge pull request #1513 from mgedmin/patch-2 (<a href="https://github.com/scrapy/scrapy/commit/5d4daf8">commit
5d4daf8</a>{.reference
.external})</p>
</li>
<li>
<p>Typo (<a href="https://github.com/scrapy/scrapy/commit/f8d0682">commit
f8d0682</a>{.reference
.external})</p>
</li>
<li>
<p>Fix list formatting (<a href="https://github.com/scrapy/scrapy/commit/5f83a93">commit
5f83a93</a>{.reference
.external})</p>
</li>
<li>
<p>fix Scrapy squeue tests after recent changes to queuelib (<a href="https://github.com/scrapy/scrapy/commit/3365c01">commit
3365c01</a>{.reference
.external})</p>
</li>
<li>
<p>Merge pull request #1475 from rweindl/patch-1 (<a href="https://github.com/scrapy/scrapy/commit/2d688cd">commit
2d688cd</a>{.reference
.external})</p>
</li>
<li>
<p>Update tutorial.rst (<a href="https://github.com/scrapy/scrapy/commit/fbc1f25">commit
fbc1f25</a>{.reference
.external})</p>
</li>
<li>
<p>Merge pull request #1449 from rhoekman/patch-1 (<a href="https://github.com/scrapy/scrapy/commit/7d6538c">commit
7d6538c</a>{.reference
.external})</p>
</li>
<li>
<p>Small grammatical change (<a href="https://github.com/scrapy/scrapy/commit/8752294">commit
8752294</a>{.reference
.external})</p>
</li>
<li>
<p>Add openssl version to version command (<a href="https://github.com/scrapy/scrapy/commit/13c45ac">commit
13c45ac</a>{.reference
.external})
:::</p>
</li>
</ul>
<p>::: {#scrapy-1-0-3-2015-08-11 .section}
[]{#release-1-0-3}</p>
<h4 id="scrapy-103-2015-08-11headerlink"><a class="header" href="#scrapy-103-2015-08-11headerlink">Scrapy 1.0.3 (2015-08-11)<a href="#scrapy-1-0-3-2015-08-11" title="Permalink to this heading">¶</a>{.headerlink}</a></h4>
<ul>
<li>
<p>add service_identity to Scrapy install_requires (<a href="https://github.com/scrapy/scrapy/commit/cbc2501">commit
cbc2501</a>{.reference
.external})</p>
</li>
<li>
<p>Workaround for travis#296 (<a href="https://github.com/scrapy/scrapy/commit/66af9cd">commit
66af9cd</a>{.reference
.external})
:::</p>
</li>
</ul>
<p>::: {#scrapy-1-0-2-2015-08-06 .section}
[]{#release-1-0-2}</p>
<h4 id="scrapy-102-2015-08-06headerlink"><a class="header" href="#scrapy-102-2015-08-06headerlink">Scrapy 1.0.2 (2015-08-06)<a href="#scrapy-1-0-2-2015-08-06" title="Permalink to this heading">¶</a>{.headerlink}</a></h4>
<ul>
<li>
<p>Twisted 15.3.0 does not raises PicklingError serializing lambda
functions (<a href="https://github.com/scrapy/scrapy/commit/b04dd7d">commit
b04dd7d</a>{.reference
.external})</p>
</li>
<li>
<p>Minor method name fix (<a href="https://github.com/scrapy/scrapy/commit/6f85c7f">commit
6f85c7f</a>{.reference
.external})</p>
</li>
<li>
<p>minor: scrapy.Spider grammar and clarity (<a href="https://github.com/scrapy/scrapy/commit/9c9d2e0">commit
9c9d2e0</a>{.reference
.external})</p>
</li>
<li>
<p>Put a blurb about support channels in CONTRIBUTING (<a href="https://github.com/scrapy/scrapy/commit/c63882b">commit
c63882b</a>{.reference
.external})</p>
</li>
<li>
<p>Fixed typos (<a href="https://github.com/scrapy/scrapy/commit/a9ae7b0">commit
a9ae7b0</a>{.reference
.external})</p>
</li>
<li>
<p>Fix doc reference. (<a href="https://github.com/scrapy/scrapy/commit/7c8a4fe">commit
7c8a4fe</a>{.reference
.external})
:::</p>
</li>
</ul>
<p>::: {#scrapy-1-0-1-2015-07-01 .section}
[]{#release-1-0-1}</p>
<h4 id="scrapy-101-2015-07-01headerlink"><a class="header" href="#scrapy-101-2015-07-01headerlink">Scrapy 1.0.1 (2015-07-01)<a href="#scrapy-1-0-1-2015-07-01" title="Permalink to this heading">¶</a>{.headerlink}</a></h4>
<ul>
<li>
<p>Unquote request path before passing to FTPClient, it already escape
paths (<a href="https://github.com/scrapy/scrapy/commit/cc00ad2">commit
cc00ad2</a>{.reference
.external})</p>
</li>
<li>
<p>include tests/ to source distribution in MANIFEST.in (<a href="https://github.com/scrapy/scrapy/commit/eca227e">commit
eca227e</a>{.reference
.external})</p>
</li>
<li>
<p>DOC Fix SelectJmes documentation (<a href="https://github.com/scrapy/scrapy/commit/b8567bc">commit
b8567bc</a>{.reference
.external})</p>
</li>
<li>
<p>DOC Bring Ubuntu and Archlinux outside of Windows subsection
(<a href="https://github.com/scrapy/scrapy/commit/392233f">commit
392233f</a>{.reference
.external})</p>
</li>
<li>
<p>DOC remove version suffix from Ubuntu package (<a href="https://github.com/scrapy/scrapy/commit/5303c66">commit
5303c66</a>{.reference
.external})</p>
</li>
<li>
<p>DOC Update release date for 1.0 (<a href="https://github.com/scrapy/scrapy/commit/c89fa29">commit
c89fa29</a>{.reference
.external})
:::</p>
</li>
</ul>
<p>::: {#scrapy-1-0-0-2015-06-19 .section}
[]{#release-1-0-0}</p>
<h4 id="scrapy-100-2015-06-19headerlink"><a class="header" href="#scrapy-100-2015-06-19headerlink">Scrapy 1.0.0 (2015-06-19)<a href="#scrapy-1-0-0-2015-06-19" title="Permalink to this heading">¶</a>{.headerlink}</a></h4>
<p>You will find a lot of new features and bugfixes in this major release.
Make sure to check our updated <a href="index.html#intro-overview">[overview]{.std
.std-ref}</a>{.hoverxref .tooltip .reference
.internal} to get a glance of some of the changes, along with our
brushed <a href="index.html#intro-tutorial">[tutorial]{.std
.std-ref}</a>{.hoverxref .tooltip .reference
.internal}.</p>
<p>::: {#support-for-returning-dictionaries-in-spiders .section}</p>
<h5 id="support-for-returning-dictionaries-in-spidersheaderlink"><a class="header" href="#support-for-returning-dictionaries-in-spidersheaderlink">Support for returning dictionaries in spiders<a href="#support-for-returning-dictionaries-in-spiders" title="Permalink to this heading">¶</a>{.headerlink}</a></h5>
<p>Declaring and returning Scrapy Items is no longer necessary to collect
the scraped data from your spider, you can now return explicit
dictionaries instead.</p>
<p><em>Classic version</em></p>
<p>::: {.highlight-default .notranslate}
::: highlight
class MyItem(scrapy.Item):
url = scrapy.Field()</p>
<pre><code>class MySpider(scrapy.Spider):
    def parse(self, response):
        return MyItem(url=response.url)
</code></pre>
<p>:::
:::</p>
<p><em>New version</em></p>
<p>::: {.highlight-default .notranslate}
::: highlight
class MySpider(scrapy.Spider):
def parse(self, response):
return {'url': response.url}
:::
:::
:::</p>
<p>::: {#per-spider-settings-gsoc-2014 .section}</p>
<h5 id="per-spider-settings-gsoc-2014headerlink"><a class="header" href="#per-spider-settings-gsoc-2014headerlink">Per-spider settings (GSoC 2014)<a href="#per-spider-settings-gsoc-2014" title="Permalink to this heading">¶</a>{.headerlink}</a></h5>
<p>Last Google Summer of Code project accomplished an important redesign of
the mechanism used for populating settings, introducing explicit
priorities to override any given setting. As an extension of that goal,
we included a new level of priority for settings that act exclusively
for a single spider, allowing them to redefine project settings.</p>
<p>Start using it by defining a [<code>custom_settings</code>{.xref .py .py-attr
.docutils .literal .notranslate}]{.pre} class variable in your spider:</p>
<p>::: {.highlight-default .notranslate}
::: highlight
class MySpider(scrapy.Spider):
custom_settings = {
&quot;DOWNLOAD_DELAY&quot;: 5.0,
&quot;RETRY_ENABLED&quot;: False,
}
:::
:::</p>
<p>Read more about settings population: <a href="index.html#topics-settings">[Settings]{.std
.std-ref}</a>{.hoverxref .tooltip .reference
.internal}
:::</p>
<p>::: {#python-logging .section}</p>
<h5 id="python-loggingheaderlink"><a class="header" href="#python-loggingheaderlink">Python Logging<a href="#python-logging" title="Permalink to this heading">¶</a>{.headerlink}</a></h5>
<p>Scrapy 1.0 has moved away from Twisted logging to support Python built
in's as default logging system. We're maintaining backward compatibility
for most of the old custom interface to call logging functions, but
you'll get warnings to switch to the Python logging API entirely.</p>
<p><em>Old version</em></p>
<p>::: {.highlight-default .notranslate}
::: highlight
from scrapy import log
log.msg('MESSAGE', log.INFO)
:::
:::</p>
<p><em>New version</em></p>
<p>::: {.highlight-default .notranslate}
::: highlight
import logging
logging.info('MESSAGE')
:::
:::</p>
<p>Logging with spiders remains the same, but on top of the [<code>log()</code>{.xref
.py .py-meth .docutils .literal .notranslate}]{.pre} method you'll have
access to a custom [<code>logger</code>{.xref .py .py-attr .docutils .literal
.notranslate}]{.pre} created for the spider to issue log events:</p>
<p>::: {.highlight-default .notranslate}
::: highlight
class MySpider(scrapy.Spider):
def parse(self, response):
self.logger.info('Response received')
:::
:::</p>
<p>Read more in the logging documentation: <a href="index.html#topics-logging">[Logging]{.std
.std-ref}</a>{.hoverxref .tooltip .reference
.internal}
:::</p>
<p>::: {#crawler-api-refactoring-gsoc-2014 .section}</p>
<h5 id="crawler-api-refactoring-gsoc-2014headerlink"><a class="header" href="#crawler-api-refactoring-gsoc-2014headerlink">Crawler API refactoring (GSoC 2014)<a href="#crawler-api-refactoring-gsoc-2014" title="Permalink to this heading">¶</a>{.headerlink}</a></h5>
<p>Another milestone for last Google Summer of Code was a refactoring of
the internal API, seeking a simpler and easier usage. Check new core
interface in: <a href="index.html#topics-api">[Core API]{.std
.std-ref}</a>{.hoverxref .tooltip .reference
.internal}</p>
<p>A common situation where you will face these changes is while running
Scrapy from scripts. Here's a quick example of how to run a Spider
manually with the new API:</p>
<p>::: {.highlight-default .notranslate}
::: highlight
from scrapy.crawler import CrawlerProcess</p>
<pre><code>process = CrawlerProcess({
    'USER_AGENT': 'Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 5.1)'
})
process.crawl(MySpider)
process.start()
</code></pre>
<p>:::
:::</p>
<p>Bear in mind this feature is still under development and its API may
change until it reaches a stable status.</p>
<p>See more examples for scripts running Scrapy: <a href="index.html#topics-practices">[Common Practices]{.std
.std-ref}</a>{.hoverxref .tooltip .reference
.internal}
:::</p>
<p>::: {#module-relocations .section}
[]{#id128}</p>
<h5 id="module-relocationsheaderlink"><a class="header" href="#module-relocationsheaderlink">Module Relocations<a href="#module-relocations" title="Permalink to this heading">¶</a>{.headerlink}</a></h5>
<p>There's been a large rearrangement of modules trying to improve the
general structure of Scrapy. Main changes were separating various
subpackages into new projects and dissolving both
[<code>scrapy.contrib</code>{.docutils .literal .notranslate}]{.pre} and
[<code>scrapy.contrib_exp</code>{.docutils .literal .notranslate}]{.pre} into top
level packages. Backward compatibility was kept among internal
relocations, while importing deprecated modules expect warnings
indicating their new place.</p>
<p>::: {#full-list-of-relocations .section}</p>
<h6 id="full-list-of-relocationsheaderlink"><a class="header" href="#full-list-of-relocationsheaderlink">Full list of relocations<a href="#full-list-of-relocations" title="Permalink to this heading">¶</a>{.headerlink}</a></h6>
<p>Outsourced packages</p>
<p>::: {.admonition .note}
Note</p>
<p>These extensions went through some minor changes, e.g. some setting
names were changed. Please check the documentation in each new
repository to get familiar with the new usage.
:::</p>
<p>+-----------------------------------+-----------------------------------+
| Old location                      | New location                      |
+===================================+===================================+
| scrapy.commands.deploy            | [sc                               |
|                                   | rapyd-client](https://github.com/ |
|                                   | scrapy/scrapyd-client){.reference |
|                                   | .external} (See other             |
|                                   | alternatives here: [[Deploying    |
|                                   | Spiders]{.std                     |
|                                   | .std-ref}](ind                    |
|                                   | ex.html#topics-deploy){.hoverxref |
|                                   | .tooltip .reference .internal})   |
+-----------------------------------+-----------------------------------+
| scrapy.contrib.djangoitem         | [scrapy-djangoite                 |
|                                   | m](https://github.com/scrapy-plug |
|                                   | ins/scrapy-djangoitem){.reference |
|                                   | .external}                        |
+-----------------------------------+-----------------------------------+
| scrapy.webservice                 | [scrapy-jso                       |
|                                   | nrpc](https://github.com/scrapy-p |
|                                   | lugins/scrapy-jsonrpc){.reference |
|                                   | .external}                        |
+-----------------------------------+-----------------------------------+</p>
<p>[<code>scrapy.contrib_exp</code>{.docutils .literal .notranslate}]{.pre} and
[<code>scrapy.contrib</code>{.docutils .literal .notranslate}]{.pre} dissolutions</p>
<p>+-----------------------------------+-----------------------------------+
| Old location                      | New location                      |
+===================================+===================================+
| scrapy.contrib_exp.d              | scrapy.do                         |
| ownloadermiddleware.decompression | wnloadermiddlewares.decompression |
+-----------------------------------+-----------------------------------+
| scrapy.contrib_exp.iterators      | scrapy.utils.iterators            |
+-----------------------------------+-----------------------------------+
| sc                                | scrapy.downloadermiddlewares      |
| rapy.contrib.downloadermiddleware |                                   |
+-----------------------------------+-----------------------------------+
| scrapy.contrib.exporter           | scrapy.exporters                  |
+-----------------------------------+-----------------------------------+
| scrapy.contrib.linkextractors     | scrapy.linkextractors             |
+-----------------------------------+-----------------------------------+
| scrapy.contrib.loader             | scrapy.loader                     |
+-----------------------------------+-----------------------------------+
| scrapy.contrib.loader.processor   | scrapy.loader.processors          |
+-----------------------------------+-----------------------------------+
| scrapy.contrib.pipeline           | scrapy.pipelines                  |
+-----------------------------------+-----------------------------------+
| scrapy.contrib.spidermiddleware   | scrapy.spidermiddlewares          |
+-----------------------------------+-----------------------------------+
| scrapy.contrib.spiders            | scrapy.spiders                    |
+-----------------------------------+-----------------------------------+
| -   scrapy.contrib.closespider    | scrapy.extensions.*              |
|                                   |                                   |
| -   scrapy.contrib.corestats      |                                   |
|                                   |                                   |
| -   scrapy.contrib.debug          |                                   |
|                                   |                                   |
| -   scrapy.contrib.feedexport     |                                   |
|                                   |                                   |
| -   scrapy.contrib.httpcache      |                                   |
|                                   |                                   |
| -   scrapy.contrib.logstats       |                                   |
|                                   |                                   |
| -   scrapy.contrib.memdebug       |                                   |
|                                   |                                   |
| -   scrapy.contrib.memusage       |                                   |
|                                   |                                   |
| -   scrapy.contrib.spiderstate    |                                   |
|                                   |                                   |
| -   scrapy.contrib.statsmailer    |                                   |
|                                   |                                   |
| -   scrapy.contrib.throttle       |                                   |
+-----------------------------------+-----------------------------------+</p>
<p>Plural renames and Modules unification</p>
<p>+-----------------------------------+-----------------------------------+
| Old location                      | New location                      |
+===================================+===================================+
| scrapy.command                    | scrapy.commands                   |
+-----------------------------------+-----------------------------------+
| scrapy.dupefilter                 | scrapy.dupefilters                |
+-----------------------------------+-----------------------------------+
| scrapy.linkextractor              | scrapy.linkextractors             |
+-----------------------------------+-----------------------------------+
| scrapy.spider                     | scrapy.spiders                    |
+-----------------------------------+-----------------------------------+
| scrapy.squeue                     | scrapy.squeues                    |
+-----------------------------------+-----------------------------------+
| scrapy.statscol                   | scrapy.statscollectors            |
+-----------------------------------+-----------------------------------+
| scrapy.utils.decorator            | scrapy.utils.decorators           |
+-----------------------------------+-----------------------------------+</p>
<p>Class renames</p>
<p>+-----------------------------------+-----------------------------------+
| Old location                      | New location                      |
+===================================+===================================+
| s                                 | scrapy.spiderloader.SpiderLoader  |
| crapy.spidermanager.SpiderManager |                                   |
+-----------------------------------+-----------------------------------+</p>
<p>Settings renames</p>
<p>+-----------------------------------+-----------------------------------+
| Old location                      | New location                      |
+===================================+===================================+
| SPIDER_MANAGER_CLASS              | SPIDER_LOADER_CLASS               |
+-----------------------------------+-----------------------------------+
:::
:::</p>
<p>::: {#changelog .section}</p>
<h5 id="changelogheaderlink"><a class="header" href="#changelogheaderlink">Changelog<a href="#changelog" title="Permalink to this heading">¶</a>{.headerlink}</a></h5>
<p>New Features and Enhancements</p>
<ul>
<li>
<p>Python logging (<a href="https://github.com/scrapy/scrapy/issues/1060">issue
1060</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/1235">issue
1235</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/1236">issue
1236</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/1240">issue
1240</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/1259">issue
1259</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/1278">issue
1278</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/1286">issue
1286</a>{.reference
.external})</p>
</li>
<li>
<p>FEED_EXPORT_FIELDS option (<a href="https://github.com/scrapy/scrapy/issues/1159">issue
1159</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/1224">issue
1224</a>{.reference
.external})</p>
</li>
<li>
<p>Dns cache size and timeout options (<a href="https://github.com/scrapy/scrapy/issues/1132">issue
1132</a>{.reference
.external})</p>
</li>
<li>
<p>support namespace prefix in xmliter_lxml (<a href="https://github.com/scrapy/scrapy/issues/963">issue
963</a>{.reference
.external})</p>
</li>
<li>
<p>Reactor threadpool max size setting (<a href="https://github.com/scrapy/scrapy/issues/1123">issue
1123</a>{.reference
.external})</p>
</li>
<li>
<p>Allow spiders to return dicts. (<a href="https://github.com/scrapy/scrapy/issues/1081">issue
1081</a>{.reference
.external})</p>
</li>
<li>
<p>Add Response.urljoin() helper (<a href="https://github.com/scrapy/scrapy/issues/1086">issue
1086</a>{.reference
.external})</p>
</li>
<li>
<p>look in ~/.config/scrapy.cfg for user config (<a href="https://github.com/scrapy/scrapy/issues/1098">issue
1098</a>{.reference
.external})</p>
</li>
<li>
<p>handle TLS SNI (<a href="https://github.com/scrapy/scrapy/issues/1101">issue
1101</a>{.reference
.external})</p>
</li>
<li>
<p>Selectorlist extract first (<a href="https://github.com/scrapy/scrapy/issues/624">issue
624</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/1145">issue
1145</a>{.reference
.external})</p>
</li>
<li>
<p>Added JmesSelect (<a href="https://github.com/scrapy/scrapy/issues/1016">issue
1016</a>{.reference
.external})</p>
</li>
<li>
<p>add gzip compression to filesystem http cache backend (<a href="https://github.com/scrapy/scrapy/issues/1020">issue
1020</a>{.reference
.external})</p>
</li>
<li>
<p>CSS support in link extractors (<a href="https://github.com/scrapy/scrapy/issues/983">issue
983</a>{.reference
.external})</p>
</li>
<li>
<p>httpcache dont_cache meta #19 #689 (<a href="https://github.com/scrapy/scrapy/issues/821">issue
821</a>{.reference
.external})</p>
</li>
<li>
<p>add signal to be sent when request is dropped by the scheduler
(<a href="https://github.com/scrapy/scrapy/issues/961">issue 961</a>{.reference
.external})</p>
</li>
<li>
<p>avoid download large response (<a href="https://github.com/scrapy/scrapy/issues/946">issue
946</a>{.reference
.external})</p>
</li>
<li>
<p>Allow to specify the quotechar in CSVFeedSpider (<a href="https://github.com/scrapy/scrapy/issues/882">issue
882</a>{.reference
.external})</p>
</li>
<li>
<p>Add referer to &quot;Spider error processing&quot; log message (<a href="https://github.com/scrapy/scrapy/issues/795">issue
795</a>{.reference
.external})</p>
</li>
<li>
<p>process robots.txt once (<a href="https://github.com/scrapy/scrapy/issues/896">issue
896</a>{.reference
.external})</p>
</li>
<li>
<p>GSoC Per-spider settings (<a href="https://github.com/scrapy/scrapy/issues/854">issue
854</a>{.reference
.external})</p>
</li>
<li>
<p>Add project name validation (<a href="https://github.com/scrapy/scrapy/issues/817">issue
817</a>{.reference
.external})</p>
</li>
<li>
<p>GSoC API cleanup (<a href="https://github.com/scrapy/scrapy/issues/816">issue
816</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/1128">issue
1128</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/1147">issue
1147</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/1148">issue
1148</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/1156">issue
1156</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/1185">issue
1185</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/1187">issue
1187</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/1258">issue
1258</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/1268">issue
1268</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/1276">issue
1276</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/1285">issue
1285</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/1284">issue
1284</a>{.reference
.external})</p>
</li>
<li>
<p>Be more responsive with IO operations (<a href="https://github.com/scrapy/scrapy/issues/1074">issue
1074</a>{.reference
.external} and <a href="https://github.com/scrapy/scrapy/issues/1075">issue
1075</a>{.reference
.external})</p>
</li>
<li>
<p>Do leveldb compaction for httpcache on closing (<a href="https://github.com/scrapy/scrapy/issues/1297">issue
1297</a>{.reference
.external})</p>
</li>
</ul>
<p>Deprecations and Removals</p>
<ul>
<li>
<p>Deprecate htmlparser link extractor (<a href="https://github.com/scrapy/scrapy/issues/1205">issue
1205</a>{.reference
.external})</p>
</li>
<li>
<p>remove deprecated code from FeedExporter (<a href="https://github.com/scrapy/scrapy/issues/1155">issue
1155</a>{.reference
.external})</p>
</li>
<li>
<p>a leftover for.15 compatibility (<a href="https://github.com/scrapy/scrapy/issues/925">issue
925</a>{.reference
.external})</p>
</li>
<li>
<p>drop support for CONCURRENT_REQUESTS_PER_SPIDER (<a href="https://github.com/scrapy/scrapy/issues/895">issue
895</a>{.reference
.external})</p>
</li>
<li>
<p>Drop old engine code (<a href="https://github.com/scrapy/scrapy/issues/911">issue
911</a>{.reference
.external})</p>
</li>
<li>
<p>Deprecate SgmlLinkExtractor (<a href="https://github.com/scrapy/scrapy/issues/777">issue
777</a>{.reference
.external})</p>
</li>
</ul>
<p>Relocations</p>
<ul>
<li>
<p>Move exporters/__init__.py to exporters.py (<a href="https://github.com/scrapy/scrapy/issues/1242">issue
1242</a>{.reference
.external})</p>
</li>
<li>
<p>Move base classes to their packages (<a href="https://github.com/scrapy/scrapy/issues/1218">issue
1218</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/1233">issue
1233</a>{.reference
.external})</p>
</li>
<li>
<p>Module relocation (<a href="https://github.com/scrapy/scrapy/issues/1181">issue
1181</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/1210">issue
1210</a>{.reference
.external})</p>
</li>
<li>
<p>rename SpiderManager to SpiderLoader (<a href="https://github.com/scrapy/scrapy/issues/1166">issue
1166</a>{.reference
.external})</p>
</li>
<li>
<p>Remove djangoitem (<a href="https://github.com/scrapy/scrapy/issues/1177">issue
1177</a>{.reference
.external})</p>
</li>
<li>
<p>remove scrapy deploy command (<a href="https://github.com/scrapy/scrapy/issues/1102">issue
1102</a>{.reference
.external})</p>
</li>
<li>
<p>dissolve contrib_exp (<a href="https://github.com/scrapy/scrapy/issues/1134">issue
1134</a>{.reference
.external})</p>
</li>
<li>
<p>Deleted bin folder from root, fixes #913 (<a href="https://github.com/scrapy/scrapy/issues/914">issue
914</a>{.reference
.external})</p>
</li>
<li>
<p>Remove jsonrpc based webservice (<a href="https://github.com/scrapy/scrapy/issues/859">issue
859</a>{.reference
.external})</p>
</li>
<li>
<p>Move Test cases under project root dir (<a href="https://github.com/scrapy/scrapy/issues/827">issue
827</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/841">issue
841</a>{.reference
.external})</p>
</li>
<li>
<p>Fix backward incompatibility for relocated paths in settings (<a href="https://github.com/scrapy/scrapy/issues/1267">issue
1267</a>{.reference
.external})</p>
</li>
</ul>
<p>Documentation</p>
<ul>
<li>
<p>CrawlerProcess documentation (<a href="https://github.com/scrapy/scrapy/issues/1190">issue
1190</a>{.reference
.external})</p>
</li>
<li>
<p>Favoring web scraping over screen scraping in the descriptions
(<a href="https://github.com/scrapy/scrapy/issues/1188">issue
1188</a>{.reference
.external})</p>
</li>
<li>
<p>Some improvements for Scrapy tutorial (<a href="https://github.com/scrapy/scrapy/issues/1180">issue
1180</a>{.reference
.external})</p>
</li>
<li>
<p>Documenting Files Pipeline together with Images Pipeline (<a href="https://github.com/scrapy/scrapy/issues/1150">issue
1150</a>{.reference
.external})</p>
</li>
<li>
<p>deployment docs tweaks (<a href="https://github.com/scrapy/scrapy/issues/1164">issue
1164</a>{.reference
.external})</p>
</li>
<li>
<p>Added deployment section covering scrapyd-deploy and shub (<a href="https://github.com/scrapy/scrapy/issues/1124">issue
1124</a>{.reference
.external})</p>
</li>
<li>
<p>Adding more settings to project template (<a href="https://github.com/scrapy/scrapy/issues/1073">issue
1073</a>{.reference
.external})</p>
</li>
<li>
<p>some improvements to overview page (<a href="https://github.com/scrapy/scrapy/issues/1106">issue
1106</a>{.reference
.external})</p>
</li>
<li>
<p>Updated link in docs/topics/architecture.rst (<a href="https://github.com/scrapy/scrapy/issues/647">issue
647</a>{.reference
.external})</p>
</li>
<li>
<p>DOC reorder topics (<a href="https://github.com/scrapy/scrapy/issues/1022">issue
1022</a>{.reference
.external})</p>
</li>
<li>
<p>updating list of Request.meta special keys (<a href="https://github.com/scrapy/scrapy/issues/1071">issue
1071</a>{.reference
.external})</p>
</li>
<li>
<p>DOC document download_timeout (<a href="https://github.com/scrapy/scrapy/issues/898">issue
898</a>{.reference
.external})</p>
</li>
<li>
<p>DOC simplify extension docs (<a href="https://github.com/scrapy/scrapy/issues/893">issue
893</a>{.reference
.external})</p>
</li>
<li>
<p>Leaks docs (<a href="https://github.com/scrapy/scrapy/issues/894">issue
894</a>{.reference
.external})</p>
</li>
<li>
<p>DOC document from_crawler method for item pipelines (<a href="https://github.com/scrapy/scrapy/issues/904">issue
904</a>{.reference
.external})</p>
</li>
<li>
<p>Spider_error doesn't support deferreds (<a href="https://github.com/scrapy/scrapy/issues/1292">issue
1292</a>{.reference
.external})</p>
</li>
<li>
<p>Corrections &amp; Sphinx related fixes (<a href="https://github.com/scrapy/scrapy/issues/1220">issue
1220</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/1219">issue
1219</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/1196">issue
1196</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/1172">issue
1172</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/1171">issue
1171</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/1169">issue
1169</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/1160">issue
1160</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/1154">issue
1154</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/1127">issue
1127</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/1112">issue
1112</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/1105">issue
1105</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/1041">issue
1041</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/1082">issue
1082</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/1033">issue
1033</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/944">issue
944</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/866">issue
866</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/864">issue
864</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/796">issue
796</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/1260">issue
1260</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/1271">issue
1271</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/1293">issue
1293</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/1298">issue
1298</a>{.reference
.external})</p>
</li>
</ul>
<p>Bugfixes</p>
<ul>
<li>
<p>Item multi inheritance fix (<a href="https://github.com/scrapy/scrapy/issues/353">issue
353</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/1228">issue
1228</a>{.reference
.external})</p>
</li>
<li>
<p>ItemLoader.load_item: iterate over copy of fields (<a href="https://github.com/scrapy/scrapy/issues/722">issue
722</a>{.reference
.external})</p>
</li>
<li>
<p>Fix Unhandled error in Deferred (RobotsTxtMiddleware) (<a href="https://github.com/scrapy/scrapy/issues/1131">issue
1131</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/1197">issue
1197</a>{.reference
.external})</p>
</li>
<li>
<p>Force to read DOWNLOAD_TIMEOUT as int (<a href="https://github.com/scrapy/scrapy/issues/954">issue
954</a>{.reference
.external})</p>
</li>
<li>
<p>scrapy.utils.misc.load_object should print full traceback (<a href="https://github.com/scrapy/scrapy/issues/902">issue
902</a>{.reference
.external})</p>
</li>
<li>
<p>Fix bug for &quot;.local&quot; host name (<a href="https://github.com/scrapy/scrapy/issues/878">issue
878</a>{.reference
.external})</p>
</li>
<li>
<p>Fix for Enabled extensions, middlewares, pipelines info not printed
anymore (<a href="https://github.com/scrapy/scrapy/issues/879">issue
879</a>{.reference
.external})</p>
</li>
<li>
<p>fix dont_merge_cookies bad behaviour when set to false on meta
(<a href="https://github.com/scrapy/scrapy/issues/846">issue 846</a>{.reference
.external})</p>
</li>
</ul>
<p>Python 3 In Progress Support</p>
<ul>
<li>
<p>disable scrapy.telnet if twisted.conch is not available (<a href="https://github.com/scrapy/scrapy/issues/1161">issue
1161</a>{.reference
.external})</p>
</li>
<li>
<p>fix Python 3 syntax errors in ajaxcrawl.py (<a href="https://github.com/scrapy/scrapy/issues/1162">issue
1162</a>{.reference
.external})</p>
</li>
<li>
<p>more python3 compatibility changes for urllib (<a href="https://github.com/scrapy/scrapy/issues/1121">issue
1121</a>{.reference
.external})</p>
</li>
<li>
<p>assertItemsEqual was renamed to assertCountEqual in Python 3.
(<a href="https://github.com/scrapy/scrapy/issues/1070">issue
1070</a>{.reference
.external})</p>
</li>
<li>
<p>Import unittest.mock if available. (<a href="https://github.com/scrapy/scrapy/issues/1066">issue
1066</a>{.reference
.external})</p>
</li>
<li>
<p>updated deprecated cgi.parse_qsl to use six's parse_qsl (<a href="https://github.com/scrapy/scrapy/issues/909">issue
909</a>{.reference
.external})</p>
</li>
<li>
<p>Prevent Python 3 port regressions (<a href="https://github.com/scrapy/scrapy/issues/830">issue
830</a>{.reference
.external})</p>
</li>
<li>
<p>PY3: use MutableMapping for python 3 (<a href="https://github.com/scrapy/scrapy/issues/810">issue
810</a>{.reference
.external})</p>
</li>
<li>
<p>PY3: use six.BytesIO and six.moves.cStringIO (<a href="https://github.com/scrapy/scrapy/issues/803">issue
803</a>{.reference
.external})</p>
</li>
<li>
<p>PY3: fix xmlrpclib and email imports (<a href="https://github.com/scrapy/scrapy/issues/801">issue
801</a>{.reference
.external})</p>
</li>
<li>
<p>PY3: use six for robotparser and urlparse (<a href="https://github.com/scrapy/scrapy/issues/800">issue
800</a>{.reference
.external})</p>
</li>
<li>
<p>PY3: use six.iterkeys, six.iteritems, and tempfile (<a href="https://github.com/scrapy/scrapy/issues/799">issue
799</a>{.reference
.external})</p>
</li>
<li>
<p>PY3: fix has_key and use six.moves.configparser (<a href="https://github.com/scrapy/scrapy/issues/798">issue
798</a>{.reference
.external})</p>
</li>
<li>
<p>PY3: use six.moves.cPickle (<a href="https://github.com/scrapy/scrapy/issues/797">issue
797</a>{.reference
.external})</p>
</li>
<li>
<p>PY3 make it possible to run some tests in Python3 (<a href="https://github.com/scrapy/scrapy/issues/776">issue
776</a>{.reference
.external})</p>
</li>
</ul>
<p>Tests</p>
<ul>
<li>
<p>remove unnecessary lines from py3-ignores (<a href="https://github.com/scrapy/scrapy/issues/1243">issue
1243</a>{.reference
.external})</p>
</li>
<li>
<p>Fix remaining warnings from pytest while collecting tests (<a href="https://github.com/scrapy/scrapy/issues/1206">issue
1206</a>{.reference
.external})</p>
</li>
<li>
<p>Add docs build to travis (<a href="https://github.com/scrapy/scrapy/issues/1234">issue
1234</a>{.reference
.external})</p>
</li>
<li>
<p>TST don't collect tests from deprecated modules. (<a href="https://github.com/scrapy/scrapy/issues/1165">issue
1165</a>{.reference
.external})</p>
</li>
<li>
<p>install service_identity package in tests to prevent warnings
(<a href="https://github.com/scrapy/scrapy/issues/1168">issue
1168</a>{.reference
.external})</p>
</li>
<li>
<p>Fix deprecated settings API in tests (<a href="https://github.com/scrapy/scrapy/issues/1152">issue
1152</a>{.reference
.external})</p>
</li>
<li>
<p>Add test for webclient with POST method and no body given (<a href="https://github.com/scrapy/scrapy/issues/1089">issue
1089</a>{.reference
.external})</p>
</li>
<li>
<p>py3-ignores.txt supports comments (<a href="https://github.com/scrapy/scrapy/issues/1044">issue
1044</a>{.reference
.external})</p>
</li>
<li>
<p>modernize some of the asserts (<a href="https://github.com/scrapy/scrapy/issues/835">issue
835</a>{.reference
.external})</p>
</li>
<li>
<p>selector.__repr__ test (<a href="https://github.com/scrapy/scrapy/issues/779">issue
779</a>{.reference
.external})</p>
</li>
</ul>
<p>Code refactoring</p>
<ul>
<li>
<p>CSVFeedSpider cleanup: use iterate_spider_output (<a href="https://github.com/scrapy/scrapy/issues/1079">issue
1079</a>{.reference
.external})</p>
</li>
<li>
<p>remove unnecessary check from scrapy.utils.spider.iter_spider_output
(<a href="https://github.com/scrapy/scrapy/issues/1078">issue
1078</a>{.reference
.external})</p>
</li>
<li>
<p>Pydispatch pep8 (<a href="https://github.com/scrapy/scrapy/issues/992">issue
992</a>{.reference
.external})</p>
</li>
<li>
<p>Removed unused 'load=False' parameter from walk_modules() (<a href="https://github.com/scrapy/scrapy/issues/871">issue
871</a>{.reference
.external})</p>
</li>
<li>
<p>For consistency, use [<code>job_dir</code>{.docutils .literal
.notranslate}]{.pre} helper in [<code>SpiderState</code>{.docutils .literal
.notranslate}]{.pre} extension. (<a href="https://github.com/scrapy/scrapy/issues/805">issue
805</a>{.reference
.external})</p>
</li>
<li>
<p>rename &quot;sflo&quot; local variables to less cryptic &quot;log_observer&quot; (<a href="https://github.com/scrapy/scrapy/issues/775">issue
775</a>{.reference
.external})
:::
:::</p>
</li>
</ul>
<p>::: {#scrapy-0-24-6-2015-04-20 .section}</p>
<h4 id="scrapy-0246-2015-04-20headerlink"><a class="header" href="#scrapy-0246-2015-04-20headerlink">Scrapy 0.24.6 (2015-04-20)<a href="#scrapy-0-24-6-2015-04-20" title="Permalink to this heading">¶</a>{.headerlink}</a></h4>
<ul>
<li>
<p>encode invalid xpath with unicode_escape under PY2 (<a href="https://github.com/scrapy/scrapy/commit/07cb3e5">commit
07cb3e5</a>{.reference
.external})</p>
</li>
<li>
<p>fix IPython shell scope issue and load IPython user config (<a href="https://github.com/scrapy/scrapy/commit/2c8e573">commit
2c8e573</a>{.reference
.external})</p>
</li>
<li>
<p>Fix small typo in the docs (<a href="https://github.com/scrapy/scrapy/commit/d694019">commit
d694019</a>{.reference
.external})</p>
</li>
<li>
<p>Fix small typo (<a href="https://github.com/scrapy/scrapy/commit/f92fa83">commit
f92fa83</a>{.reference
.external})</p>
</li>
<li>
<p>Converted sel.xpath() calls to response.xpath() in Extracting the
data (<a href="https://github.com/scrapy/scrapy/commit/c2c6d15">commit
c2c6d15</a>{.reference
.external})
:::</p>
</li>
</ul>
<p>::: {#scrapy-0-24-5-2015-02-25 .section}</p>
<h4 id="scrapy-0245-2015-02-25headerlink"><a class="header" href="#scrapy-0245-2015-02-25headerlink">Scrapy 0.24.5 (2015-02-25)<a href="#scrapy-0-24-5-2015-02-25" title="Permalink to this heading">¶</a>{.headerlink}</a></h4>
<ul>
<li>
<p>Support new _getEndpoint Agent signatures on Twisted 15.0.0
(<a href="https://github.com/scrapy/scrapy/commit/540b9bc">commit
540b9bc</a>{.reference
.external})</p>
</li>
<li>
<p>DOC a couple more references are fixed (<a href="https://github.com/scrapy/scrapy/commit/b4c454b">commit
b4c454b</a>{.reference
.external})</p>
</li>
<li>
<p>DOC fix a reference (<a href="https://github.com/scrapy/scrapy/commit/e3c1260">commit
e3c1260</a>{.reference
.external})</p>
</li>
<li>
<p>t.i.b.ThreadedResolver is now a new-style class (<a href="https://github.com/scrapy/scrapy/commit/9e13f42">commit
9e13f42</a>{.reference
.external})</p>
</li>
<li>
<p>S3DownloadHandler: fix auth for requests with quoted paths/query
params (<a href="https://github.com/scrapy/scrapy/commit/cdb9a0b">commit
cdb9a0b</a>{.reference
.external})</p>
</li>
<li>
<p>fixed the variable types in mailsender documentation (<a href="https://github.com/scrapy/scrapy/commit/bb3a848">commit
bb3a848</a>{.reference
.external})</p>
</li>
<li>
<p>Reset items_scraped instead of item_count (<a href="https://github.com/scrapy/scrapy/commit/edb07a4">commit
edb07a4</a>{.reference
.external})</p>
</li>
<li>
<p>Tentative attention message about what document to read for
contributions (<a href="https://github.com/scrapy/scrapy/commit/7ee6f7a">commit
7ee6f7a</a>{.reference
.external})</p>
</li>
<li>
<p>mitmproxy 0.10.1 needs netlib 0.10.1 too (<a href="https://github.com/scrapy/scrapy/commit/874fcdd">commit
874fcdd</a>{.reference
.external})</p>
</li>
<li>
<p>pin mitmproxy 0.10.1 as &gt;0.11 does not work with tests (<a href="https://github.com/scrapy/scrapy/commit/c6b21f0">commit
c6b21f0</a>{.reference
.external})</p>
</li>
<li>
<p>Test the parse command locally instead of against an external url
(<a href="https://github.com/scrapy/scrapy/commit/c3a6628">commit
c3a6628</a>{.reference
.external})</p>
</li>
<li>
<p>Patches Twisted issue while closing the connection pool on
HTTPDownloadHandler (<a href="https://github.com/scrapy/scrapy/commit/d0bf957">commit
d0bf957</a>{.reference
.external})</p>
</li>
<li>
<p>Updates documentation on dynamic item classes. (<a href="https://github.com/scrapy/scrapy/commit/eeb589a">commit
eeb589a</a>{.reference
.external})</p>
</li>
<li>
<p>Merge pull request #943 from Lazar-T/patch-3 (<a href="https://github.com/scrapy/scrapy/commit/5fdab02">commit
5fdab02</a>{.reference
.external})</p>
</li>
<li>
<p>typo (<a href="https://github.com/scrapy/scrapy/commit/b0ae199">commit
b0ae199</a>{.reference
.external})</p>
</li>
<li>
<p>pywin32 is required by Twisted. closes #937 (<a href="https://github.com/scrapy/scrapy/commit/5cb0cfb">commit
5cb0cfb</a>{.reference
.external})</p>
</li>
<li>
<p>Update install.rst (<a href="https://github.com/scrapy/scrapy/commit/781286b">commit
781286b</a>{.reference
.external})</p>
</li>
<li>
<p>Merge pull request #928 from Lazar-T/patch-1 (<a href="https://github.com/scrapy/scrapy/commit/b415d04">commit
b415d04</a>{.reference
.external})</p>
</li>
<li>
<p>comma instead of fullstop (<a href="https://github.com/scrapy/scrapy/commit/627b9ba">commit
627b9ba</a>{.reference
.external})</p>
</li>
<li>
<p>Merge pull request #885 from jsma/patch-1 (<a href="https://github.com/scrapy/scrapy/commit/de909ad">commit
de909ad</a>{.reference
.external})</p>
</li>
<li>
<p>Update request-response.rst (<a href="https://github.com/scrapy/scrapy/commit/3f3263d">commit
3f3263d</a>{.reference
.external})</p>
</li>
<li>
<p>SgmlLinkExtractor - fix for parsing &lt;area&gt; tag with Unicode
present (<a href="https://github.com/scrapy/scrapy/commit/49b40f0">commit
49b40f0</a>{.reference
.external})
:::</p>
</li>
</ul>
<p>::: {#scrapy-0-24-4-2014-08-09 .section}</p>
<h4 id="scrapy-0244-2014-08-09headerlink"><a class="header" href="#scrapy-0244-2014-08-09headerlink">Scrapy 0.24.4 (2014-08-09)<a href="#scrapy-0-24-4-2014-08-09" title="Permalink to this heading">¶</a>{.headerlink}</a></h4>
<ul>
<li>
<p>pem file is used by mockserver and required by scrapy bench (<a href="https://github.com/scrapy/scrapy/commit/5eddc68">commit
5eddc68</a>{.reference
.external})</p>
</li>
<li>
<p>scrapy bench needs scrapy.tests* (<a href="https://github.com/scrapy/scrapy/commit/d6cb999">commit
d6cb999</a>{.reference
.external})
:::</p>
</li>
</ul>
<p>::: {#scrapy-0-24-3-2014-08-09 .section}</p>
<h4 id="scrapy-0243-2014-08-09headerlink"><a class="header" href="#scrapy-0243-2014-08-09headerlink">Scrapy 0.24.3 (2014-08-09)<a href="#scrapy-0-24-3-2014-08-09" title="Permalink to this heading">¶</a>{.headerlink}</a></h4>
<ul>
<li>
<p>no need to waste travis-ci time on py3 for 0.24 (<a href="https://github.com/scrapy/scrapy/commit/8e080c1">commit
8e080c1</a>{.reference
.external})</p>
</li>
<li>
<p>Update installation docs (<a href="https://github.com/scrapy/scrapy/commit/1d0c096">commit
1d0c096</a>{.reference
.external})</p>
</li>
<li>
<p>There is a trove classifier for Scrapy framework! (<a href="https://github.com/scrapy/scrapy/commit/4c701d7">commit
4c701d7</a>{.reference
.external})</p>
</li>
<li>
<p>update other places where w3lib version is mentioned (<a href="https://github.com/scrapy/scrapy/commit/d109c13">commit
d109c13</a>{.reference
.external})</p>
</li>
<li>
<p>Update w3lib requirement to 1.8.0 (<a href="https://github.com/scrapy/scrapy/commit/39d2ce5">commit
39d2ce5</a>{.reference
.external})</p>
</li>
<li>
<p>Use w3lib.html.replace_entities() (remove_entities() is deprecated)
(<a href="https://github.com/scrapy/scrapy/commit/180d3ad">commit
180d3ad</a>{.reference
.external})</p>
</li>
<li>
<p>set zip_safe=False (<a href="https://github.com/scrapy/scrapy/commit/a51ee8b">commit
a51ee8b</a>{.reference
.external})</p>
</li>
<li>
<p>do not ship tests package (<a href="https://github.com/scrapy/scrapy/commit/ee3b371">commit
ee3b371</a>{.reference
.external})</p>
</li>
<li>
<p>scrapy.bat is not needed anymore (<a href="https://github.com/scrapy/scrapy/commit/c3861cf">commit
c3861cf</a>{.reference
.external})</p>
</li>
<li>
<p>Modernize setup.py (<a href="https://github.com/scrapy/scrapy/commit/362e322">commit
362e322</a>{.reference
.external})</p>
</li>
<li>
<p>headers can not handle non-string values (<a href="https://github.com/scrapy/scrapy/commit/94a5c65">commit
94a5c65</a>{.reference
.external})</p>
</li>
<li>
<p>fix ftp test cases (<a href="https://github.com/scrapy/scrapy/commit/a274a7f">commit
a274a7f</a>{.reference
.external})</p>
</li>
<li>
<p>The sum up of travis-ci builds are taking like 50min to complete
(<a href="https://github.com/scrapy/scrapy/commit/ae1e2cc">commit
ae1e2cc</a>{.reference
.external})</p>
</li>
<li>
<p>Update shell.rst typo (<a href="https://github.com/scrapy/scrapy/commit/e49c96a">commit
e49c96a</a>{.reference
.external})</p>
</li>
<li>
<p>removes weird indentation in the shell results (<a href="https://github.com/scrapy/scrapy/commit/1ca489d">commit
1ca489d</a>{.reference
.external})</p>
</li>
<li>
<p>improved explanations, clarified blog post as source, added link for
XPath string functions in the spec (<a href="https://github.com/scrapy/scrapy/commit/65c8f05">commit
65c8f05</a>{.reference
.external})</p>
</li>
<li>
<p>renamed UserTimeoutError and ServerTimeouterror #583 (<a href="https://github.com/scrapy/scrapy/commit/037f6ab">commit
037f6ab</a>{.reference
.external})</p>
</li>
<li>
<p>adding some xpath tips to selectors docs (<a href="https://github.com/scrapy/scrapy/commit/2d103e0">commit
2d103e0</a>{.reference
.external})</p>
</li>
<li>
<p>fix tests to account for
<a href="https://github.com/scrapy/w3lib/pull/23">https://github.com/scrapy/w3lib/pull/23</a>{.reference
.external} (<a href="https://github.com/scrapy/scrapy/commit/f8d366a">commit
f8d366a</a>{.reference
.external})</p>
</li>
<li>
<p>get_func_args maximum recursion fix #728 (<a href="https://github.com/scrapy/scrapy/commit/81344ea">commit
81344ea</a>{.reference
.external})</p>
</li>
<li>
<p>Updated input/output processor example according to #560. (<a href="https://github.com/scrapy/scrapy/commit/f7c4ea8">commit
f7c4ea8</a>{.reference
.external})</p>
</li>
<li>
<p>Fixed Python syntax in tutorial. (<a href="https://github.com/scrapy/scrapy/commit/db59ed9">commit
db59ed9</a>{.reference
.external})</p>
</li>
<li>
<p>Add test case for tunneling proxy (<a href="https://github.com/scrapy/scrapy/commit/f090260">commit
f090260</a>{.reference
.external})</p>
</li>
<li>
<p>Bugfix for leaking Proxy-Authorization header to remote host when
using tunneling (<a href="https://github.com/scrapy/scrapy/commit/d8793af">commit
d8793af</a>{.reference
.external})</p>
</li>
<li>
<p>Extract links from XHTML documents with MIME-Type &quot;application/xml&quot;
(<a href="https://github.com/scrapy/scrapy/commit/ed1f376">commit
ed1f376</a>{.reference
.external})</p>
</li>
<li>
<p>Merge pull request #793 from roysc/patch-1 (<a href="https://github.com/scrapy/scrapy/commit/91a1106">commit
91a1106</a>{.reference
.external})</p>
</li>
<li>
<p>Fix typo in commands.rst (<a href="https://github.com/scrapy/scrapy/commit/743e1e2">commit
743e1e2</a>{.reference
.external})</p>
</li>
<li>
<p>better testcase for settings.overrides.setdefault (<a href="https://github.com/scrapy/scrapy/commit/e22daaf">commit
e22daaf</a>{.reference
.external})</p>
</li>
<li>
<p>Using CRLF as line marker according to http 1.1 definition (<a href="https://github.com/scrapy/scrapy/commit/5ec430b">commit
5ec430b</a>{.reference
.external})
:::</p>
</li>
</ul>
<p>::: {#scrapy-0-24-2-2014-07-08 .section}</p>
<h4 id="scrapy-0242-2014-07-08headerlink"><a class="header" href="#scrapy-0242-2014-07-08headerlink">Scrapy 0.24.2 (2014-07-08)<a href="#scrapy-0-24-2-2014-07-08" title="Permalink to this heading">¶</a>{.headerlink}</a></h4>
<ul>
<li>
<p>Use a mutable mapping to proxy deprecated settings.overrides and
settings.defaults attribute (<a href="https://github.com/scrapy/scrapy/commit/e5e8133">commit
e5e8133</a>{.reference
.external})</p>
</li>
<li>
<p>there is not support for python3 yet (<a href="https://github.com/scrapy/scrapy/commit/3cd6146">commit
3cd6146</a>{.reference
.external})</p>
</li>
<li>
<p>Update python compatible version set to Debian packages (<a href="https://github.com/scrapy/scrapy/commit/fa5d76b">commit
fa5d76b</a>{.reference
.external})</p>
</li>
<li>
<p>DOC fix formatting in release notes (<a href="https://github.com/scrapy/scrapy/commit/c6a9e20">commit
c6a9e20</a>{.reference
.external})
:::</p>
</li>
</ul>
<p>::: {#scrapy-0-24-1-2014-06-27 .section}</p>
<h4 id="scrapy-0241-2014-06-27headerlink"><a class="header" href="#scrapy-0241-2014-06-27headerlink">Scrapy 0.24.1 (2014-06-27)<a href="#scrapy-0-24-1-2014-06-27" title="Permalink to this heading">¶</a>{.headerlink}</a></h4>
<ul>
<li>Fix deprecated CrawlerSettings and increase backward compatibility
with .defaults attribute (<a href="https://github.com/scrapy/scrapy/commit/8e3f20a">commit
8e3f20a</a>{.reference
.external})
:::</li>
</ul>
<p>::: {#scrapy-0-24-0-2014-06-26 .section}</p>
<h4 id="scrapy-0240-2014-06-26headerlink"><a class="header" href="#scrapy-0240-2014-06-26headerlink">Scrapy 0.24.0 (2014-06-26)<a href="#scrapy-0-24-0-2014-06-26" title="Permalink to this heading">¶</a>{.headerlink}</a></h4>
<p>::: {#enhancements .section}</p>
<h5 id="enhancementsheaderlink"><a class="header" href="#enhancementsheaderlink">Enhancements<a href="#enhancements" title="Permalink to this heading">¶</a>{.headerlink}</a></h5>
<ul>
<li>
<p>Improve Scrapy top-level namespace (<a href="https://github.com/scrapy/scrapy/issues/494">issue
494</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/684">issue
684</a>{.reference
.external})</p>
</li>
<li>
<p>Add selector shortcuts to responses (<a href="https://github.com/scrapy/scrapy/issues/554">issue
554</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/690">issue
690</a>{.reference
.external})</p>
</li>
<li>
<p>Add new lxml based LinkExtractor to replace unmaintained
SgmlLinkExtractor (<a href="https://github.com/scrapy/scrapy/issues/559">issue
559</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/761">issue
761</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/763">issue
763</a>{.reference
.external})</p>
</li>
<li>
<p>Cleanup settings API - part of per-spider settings <strong>GSoC project</strong>
(<a href="https://github.com/scrapy/scrapy/issues/737">issue 737</a>{.reference
.external})</p>
</li>
<li>
<p>Add UTF8 encoding header to templates (<a href="https://github.com/scrapy/scrapy/issues/688">issue
688</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/762">issue
762</a>{.reference
.external})</p>
</li>
<li>
<p>Telnet console now binds to 127.0.0.1 by default (<a href="https://github.com/scrapy/scrapy/issues/699">issue
699</a>{.reference
.external})</p>
</li>
<li>
<p>Update Debian/Ubuntu install instructions (<a href="https://github.com/scrapy/scrapy/issues/509">issue
509</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/549">issue
549</a>{.reference
.external})</p>
</li>
<li>
<p>Disable smart strings in lxml XPath evaluations (<a href="https://github.com/scrapy/scrapy/issues/535">issue
535</a>{.reference
.external})</p>
</li>
<li>
<p>Restore filesystem based cache as default for http cache middleware
(<a href="https://github.com/scrapy/scrapy/issues/541">issue 541</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/500">issue
500</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/571">issue
571</a>{.reference
.external})</p>
</li>
<li>
<p>Expose current crawler in Scrapy shell (<a href="https://github.com/scrapy/scrapy/issues/557">issue
557</a>{.reference
.external})</p>
</li>
<li>
<p>Improve testsuite comparing CSV and XML exporters (<a href="https://github.com/scrapy/scrapy/issues/570">issue
570</a>{.reference
.external})</p>
</li>
<li>
<p>New [<code>offsite/filtered</code>{.docutils .literal .notranslate}]{.pre} and
[<code>offsite/domains</code>{.docutils .literal .notranslate}]{.pre} stats
(<a href="https://github.com/scrapy/scrapy/issues/566">issue 566</a>{.reference
.external})</p>
</li>
<li>
<p>Support process_links as generator in CrawlSpider (<a href="https://github.com/scrapy/scrapy/issues/555">issue
555</a>{.reference
.external})</p>
</li>
<li>
<p>Verbose logging and new stats counters for DupeFilter (<a href="https://github.com/scrapy/scrapy/issues/553">issue
553</a>{.reference
.external})</p>
</li>
<li>
<p>Add a mimetype parameter to [<code>MailSender.send()</code>{.docutils .literal
.notranslate}]{.pre} (<a href="https://github.com/scrapy/scrapy/issues/602">issue
602</a>{.reference
.external})</p>
</li>
<li>
<p>Generalize file pipeline log messages (<a href="https://github.com/scrapy/scrapy/issues/622">issue
622</a>{.reference
.external})</p>
</li>
<li>
<p>Replace unencodeable codepoints with html entities in
SGMLLinkExtractor (<a href="https://github.com/scrapy/scrapy/issues/565">issue
565</a>{.reference
.external})</p>
</li>
<li>
<p>Converted SEP documents to rst format (<a href="https://github.com/scrapy/scrapy/issues/629">issue
629</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/630">issue
630</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/638">issue
638</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/632">issue
632</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/636">issue
636</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/640">issue
640</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/635">issue
635</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/634">issue
634</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/639">issue
639</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/637">issue
637</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/631">issue
631</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/633">issue
633</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/641">issue
641</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/642">issue
642</a>{.reference
.external})</p>
</li>
<li>
<p>Tests and docs for clickdata's nr index in FormRequest (<a href="https://github.com/scrapy/scrapy/issues/646">issue
646</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/645">issue
645</a>{.reference
.external})</p>
</li>
<li>
<p>Allow to disable a downloader handler just like any other component
(<a href="https://github.com/scrapy/scrapy/issues/650">issue 650</a>{.reference
.external})</p>
</li>
<li>
<p>Log when a request is discarded after too many redirections (<a href="https://github.com/scrapy/scrapy/issues/654">issue
654</a>{.reference
.external})</p>
</li>
<li>
<p>Log error responses if they are not handled by spider callbacks
(<a href="https://github.com/scrapy/scrapy/issues/612">issue 612</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/656">issue
656</a>{.reference
.external})</p>
</li>
<li>
<p>Add content-type check to http compression mw (<a href="https://github.com/scrapy/scrapy/issues/193">issue
193</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/660">issue
660</a>{.reference
.external})</p>
</li>
<li>
<p>Run pypy tests using latest pypi from ppa (<a href="https://github.com/scrapy/scrapy/issues/674">issue
674</a>{.reference
.external})</p>
</li>
<li>
<p>Run test suite using pytest instead of trial (<a href="https://github.com/scrapy/scrapy/issues/679">issue
679</a>{.reference
.external})</p>
</li>
<li>
<p>Build docs and check for dead links in tox environment (<a href="https://github.com/scrapy/scrapy/issues/687">issue
687</a>{.reference
.external})</p>
</li>
<li>
<p>Make scrapy.version_info a tuple of integers (<a href="https://github.com/scrapy/scrapy/issues/681">issue
681</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/692">issue
692</a>{.reference
.external})</p>
</li>
<li>
<p>Infer exporter's output format from filename extensions (<a href="https://github.com/scrapy/scrapy/issues/546">issue
546</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/659">issue
659</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/760">issue
760</a>{.reference
.external})</p>
</li>
<li>
<p>Support case-insensitive domains in
[<code>url_is_from_any_domain()</code>{.docutils .literal .notranslate}]{.pre}
(<a href="https://github.com/scrapy/scrapy/issues/693">issue 693</a>{.reference
.external})</p>
</li>
<li>
<p>Remove pep8 warnings in project and spider templates (<a href="https://github.com/scrapy/scrapy/issues/698">issue
698</a>{.reference
.external})</p>
</li>
<li>
<p>Tests and docs for [<code>request_fingerprint</code>{.docutils .literal
.notranslate}]{.pre} function (<a href="https://github.com/scrapy/scrapy/issues/597">issue
597</a>{.reference
.external})</p>
</li>
<li>
<p>Update SEP-19 for GSoC project [<code>per-spider</code>{.docutils .literal
.notranslate}]{.pre}<code> </code>{.docutils .literal
.notranslate}[<code>settings</code>{.docutils .literal .notranslate}]{.pre}
(<a href="https://github.com/scrapy/scrapy/issues/705">issue 705</a>{.reference
.external})</p>
</li>
<li>
<p>Set exit code to non-zero when contracts fails (<a href="https://github.com/scrapy/scrapy/issues/727">issue
727</a>{.reference
.external})</p>
</li>
<li>
<p>Add a setting to control what class is instantiated as Downloader
component (<a href="https://github.com/scrapy/scrapy/issues/738">issue
738</a>{.reference
.external})</p>
</li>
<li>
<p>Pass response in [<code>item_dropped</code>{.docutils .literal
.notranslate}]{.pre} signal (<a href="https://github.com/scrapy/scrapy/issues/724">issue
724</a>{.reference
.external})</p>
</li>
<li>
<p>Improve [<code>scrapy</code>{.docutils .literal
.notranslate}]{.pre}<code> </code>{.docutils .literal
.notranslate}[<code>check</code>{.docutils .literal .notranslate}]{.pre}
contracts command (<a href="https://github.com/scrapy/scrapy/issues/733">issue
733</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/752">issue
752</a>{.reference
.external})</p>
</li>
<li>
<p>Document [<code>spider.closed()</code>{.docutils .literal .notranslate}]{.pre}
shortcut (<a href="https://github.com/scrapy/scrapy/issues/719">issue
719</a>{.reference
.external})</p>
</li>
<li>
<p>Document [<code>request_scheduled</code>{.docutils .literal
.notranslate}]{.pre} signal (<a href="https://github.com/scrapy/scrapy/issues/746">issue
746</a>{.reference
.external})</p>
</li>
<li>
<p>Add a note about reporting security issues (<a href="https://github.com/scrapy/scrapy/issues/697">issue
697</a>{.reference
.external})</p>
</li>
<li>
<p>Add LevelDB http cache storage backend (<a href="https://github.com/scrapy/scrapy/issues/626">issue
626</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/500">issue
500</a>{.reference
.external})</p>
</li>
<li>
<p>Sort spider list output of [<code>scrapy</code>{.docutils .literal
.notranslate}]{.pre}<code> </code>{.docutils .literal
.notranslate}[<code>list</code>{.docutils .literal .notranslate}]{.pre} command
(<a href="https://github.com/scrapy/scrapy/issues/742">issue 742</a>{.reference
.external})</p>
</li>
<li>
<p>Multiple documentation enhancements and fixes (<a href="https://github.com/scrapy/scrapy/issues/575">issue
575</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/587">issue
587</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/590">issue
590</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/596">issue
596</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/610">issue
610</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/617">issue
617</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/618">issue
618</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/627">issue
627</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/613">issue
613</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/643">issue
643</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/654">issue
654</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/675">issue
675</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/663">issue
663</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/711">issue
711</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/714">issue
714</a>{.reference
.external})
:::</p>
</li>
</ul>
<p>::: {#id129 .section}</p>
<h5 id="bugfixesheaderlink-1"><a class="header" href="#bugfixesheaderlink-1">Bugfixes<a href="#id129" title="Permalink to this heading">¶</a>{.headerlink}</a></h5>
<ul>
<li>
<p>Encode unicode URL value when creating Links in RegexLinkExtractor
(<a href="https://github.com/scrapy/scrapy/issues/561">issue 561</a>{.reference
.external})</p>
</li>
<li>
<p>Ignore None values in ItemLoader processors (<a href="https://github.com/scrapy/scrapy/issues/556">issue
556</a>{.reference
.external})</p>
</li>
<li>
<p>Fix link text when there is an inner tag in SGMLLinkExtractor and
HtmlParserLinkExtractor (<a href="https://github.com/scrapy/scrapy/issues/485">issue
485</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/574">issue
574</a>{.reference
.external})</p>
</li>
<li>
<p>Fix wrong checks on subclassing of deprecated classes (<a href="https://github.com/scrapy/scrapy/issues/581">issue
581</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/584">issue
584</a>{.reference
.external})</p>
</li>
<li>
<p>Handle errors caused by inspect.stack() failures (<a href="https://github.com/scrapy/scrapy/issues/582">issue
582</a>{.reference
.external})</p>
</li>
<li>
<p>Fix a reference to unexistent engine attribute (<a href="https://github.com/scrapy/scrapy/issues/593">issue
593</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/594">issue
594</a>{.reference
.external})</p>
</li>
<li>
<p>Fix dynamic itemclass example usage of type() (<a href="https://github.com/scrapy/scrapy/issues/603">issue
603</a>{.reference
.external})</p>
</li>
<li>
<p>Use lucasdemarchi/codespell to fix typos (<a href="https://github.com/scrapy/scrapy/issues/628">issue
628</a>{.reference
.external})</p>
</li>
<li>
<p>Fix default value of attrs argument in SgmlLinkExtractor to be tuple
(<a href="https://github.com/scrapy/scrapy/issues/661">issue 661</a>{.reference
.external})</p>
</li>
<li>
<p>Fix XXE flaw in sitemap reader (<a href="https://github.com/scrapy/scrapy/issues/676">issue
676</a>{.reference
.external})</p>
</li>
<li>
<p>Fix engine to support filtered start requests (<a href="https://github.com/scrapy/scrapy/issues/707">issue
707</a>{.reference
.external})</p>
</li>
<li>
<p>Fix offsite middleware case on urls with no hostnames (<a href="https://github.com/scrapy/scrapy/issues/745">issue
745</a>{.reference
.external})</p>
</li>
<li>
<p>Testsuite doesn't require PIL anymore (<a href="https://github.com/scrapy/scrapy/issues/585">issue
585</a>{.reference
.external})
:::
:::</p>
</li>
</ul>
<p>::: {#scrapy-0-22-2-released-2014-02-14 .section}</p>
<h4 id="scrapy-0222-released-2014-02-14headerlink"><a class="header" href="#scrapy-0222-released-2014-02-14headerlink">Scrapy 0.22.2 (released 2014-02-14)<a href="#scrapy-0-22-2-released-2014-02-14" title="Permalink to this heading">¶</a>{.headerlink}</a></h4>
<ul>
<li>
<p>fix a reference to unexistent engine.slots. closes #593 (<a href="https://github.com/scrapy/scrapy/commit/13c099a">commit
13c099a</a>{.reference
.external})</p>
</li>
<li>
<p>downloaderMW doc typo (spiderMW doc copy remnant) (<a href="https://github.com/scrapy/scrapy/commit/8ae11bf">commit
8ae11bf</a>{.reference
.external})</p>
</li>
<li>
<p>Correct typos (<a href="https://github.com/scrapy/scrapy/commit/1346037">commit
1346037</a>{.reference
.external})
:::</p>
</li>
</ul>
<p>::: {#scrapy-0-22-1-released-2014-02-08 .section}</p>
<h4 id="scrapy-0221-released-2014-02-08headerlink"><a class="header" href="#scrapy-0221-released-2014-02-08headerlink">Scrapy 0.22.1 (released 2014-02-08)<a href="#scrapy-0-22-1-released-2014-02-08" title="Permalink to this heading">¶</a>{.headerlink}</a></h4>
<ul>
<li>
<p>localhost666 can resolve under certain circumstances (<a href="https://github.com/scrapy/scrapy/commit/2ec2279">commit
2ec2279</a>{.reference
.external})</p>
</li>
<li>
<p>test inspect.stack failure (<a href="https://github.com/scrapy/scrapy/commit/cc3eda3">commit
cc3eda3</a>{.reference
.external})</p>
</li>
<li>
<p>Handle cases when inspect.stack() fails (<a href="https://github.com/scrapy/scrapy/commit/8cb44f9">commit
8cb44f9</a>{.reference
.external})</p>
</li>
<li>
<p>Fix wrong checks on subclassing of deprecated classes. closes #581
(<a href="https://github.com/scrapy/scrapy/commit/46d98d6">commit
46d98d6</a>{.reference
.external})</p>
</li>
<li>
<p>Docs: 4-space indent for final spider example (<a href="https://github.com/scrapy/scrapy/commit/13846de">commit
13846de</a>{.reference
.external})</p>
</li>
<li>
<p>Fix HtmlParserLinkExtractor and tests after #485 merge (<a href="https://github.com/scrapy/scrapy/commit/368a946">commit
368a946</a>{.reference
.external})</p>
</li>
<li>
<p>BaseSgmlLinkExtractor: Fixed the missing space when the link has an
inner tag (<a href="https://github.com/scrapy/scrapy/commit/b566388">commit
b566388</a>{.reference
.external})</p>
</li>
<li>
<p>BaseSgmlLinkExtractor: Added unit test of a link with an inner tag
(<a href="https://github.com/scrapy/scrapy/commit/c1cb418">commit
c1cb418</a>{.reference
.external})</p>
</li>
<li>
<p>BaseSgmlLinkExtractor: Fixed unknown_endtag() so that it only set
current_link=None when the end tag match the opening tag (<a href="https://github.com/scrapy/scrapy/commit/7e4d627">commit
7e4d627</a>{.reference
.external})</p>
</li>
<li>
<p>Fix tests for Travis-CI build (<a href="https://github.com/scrapy/scrapy/commit/76c7e20">commit
76c7e20</a>{.reference
.external})</p>
</li>
<li>
<p>replace unencodeable codepoints with html entities. fixes #562 and
#285 (<a href="https://github.com/scrapy/scrapy/commit/5f87b17">commit
5f87b17</a>{.reference
.external})</p>
</li>
<li>
<p>RegexLinkExtractor: encode URL unicode value when creating Links
(<a href="https://github.com/scrapy/scrapy/commit/d0ee545">commit
d0ee545</a>{.reference
.external})</p>
</li>
<li>
<p>Updated the tutorial crawl output with latest output. (<a href="https://github.com/scrapy/scrapy/commit/8da65de">commit
8da65de</a>{.reference
.external})</p>
</li>
<li>
<p>Updated shell docs with the crawler reference and fixed the actual
shell output. (<a href="https://github.com/scrapy/scrapy/commit/875b9ab">commit
875b9ab</a>{.reference
.external})</p>
</li>
<li>
<p>PEP8 minor edits. (<a href="https://github.com/scrapy/scrapy/commit/f89efaf">commit
f89efaf</a>{.reference
.external})</p>
</li>
<li>
<p>Expose current crawler in the Scrapy shell. (<a href="https://github.com/scrapy/scrapy/commit/5349cec">commit
5349cec</a>{.reference
.external})</p>
</li>
<li>
<p>Unused re import and PEP8 minor edits. (<a href="https://github.com/scrapy/scrapy/commit/387f414">commit
387f414</a>{.reference
.external})</p>
</li>
<li>
<p>Ignore None's values when using the ItemLoader. (<a href="https://github.com/scrapy/scrapy/commit/0632546">commit
0632546</a>{.reference
.external})</p>
</li>
<li>
<p>DOC Fixed HTTPCACHE_STORAGE typo in the default value which is now
Filesystem instead Dbm. (<a href="https://github.com/scrapy/scrapy/commit/cde9a8c">commit
cde9a8c</a>{.reference
.external})</p>
</li>
<li>
<p>show Ubuntu setup instructions as literal code (<a href="https://github.com/scrapy/scrapy/commit/fb5c9c5">commit
fb5c9c5</a>{.reference
.external})</p>
</li>
<li>
<p>Update Ubuntu installation instructions (<a href="https://github.com/scrapy/scrapy/commit/70fb105">commit
70fb105</a>{.reference
.external})</p>
</li>
<li>
<p>Merge pull request #550 from stray-leone/patch-1 (<a href="https://github.com/scrapy/scrapy/commit/6f70b6a">commit
6f70b6a</a>{.reference
.external})</p>
</li>
<li>
<p>modify the version of Scrapy Ubuntu package (<a href="https://github.com/scrapy/scrapy/commit/725900d">commit
725900d</a>{.reference
.external})</p>
</li>
<li>
<p>fix 0.22.0 release date (<a href="https://github.com/scrapy/scrapy/commit/af0219a">commit
af0219a</a>{.reference
.external})</p>
</li>
<li>
<p>fix typos in news.rst and remove (not released yet) header (<a href="https://github.com/scrapy/scrapy/commit/b7f58f4">commit
b7f58f4</a>{.reference
.external})
:::</p>
</li>
</ul>
<p>::: {#scrapy-0-22-0-released-2014-01-17 .section}</p>
<h4 id="scrapy-0220-released-2014-01-17headerlink"><a class="header" href="#scrapy-0220-released-2014-01-17headerlink">Scrapy 0.22.0 (released 2014-01-17)<a href="#scrapy-0-22-0-released-2014-01-17" title="Permalink to this heading">¶</a>{.headerlink}</a></h4>
<p>::: {#id130 .section}</p>
<h5 id="enhancementsheaderlink-1"><a class="header" href="#enhancementsheaderlink-1">Enhancements<a href="#id130" title="Permalink to this heading">¶</a>{.headerlink}</a></h5>
<ul>
<li>
<p>[<strong>Backward incompatible</strong>] Switched HTTPCacheMiddleware backend
to filesystem (<a href="https://github.com/scrapy/scrapy/issues/541">issue
541</a>{.reference
.external}) To restore old backend set
[<code>HTTPCACHE_STORAGE</code>{.docutils .literal .notranslate}]{.pre} to
[<code>scrapy.contrib.httpcache.DbmCacheStorage</code>{.docutils .literal
.notranslate}]{.pre}</p>
</li>
<li>
<p>Proxy https:// urls using CONNECT method (<a href="https://github.com/scrapy/scrapy/issues/392">issue
392</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/397">issue
397</a>{.reference
.external})</p>
</li>
<li>
<p>Add a middleware to crawl ajax crawlable pages as defined by google
(<a href="https://github.com/scrapy/scrapy/issues/343">issue 343</a>{.reference
.external})</p>
</li>
<li>
<p>Rename scrapy.spider.BaseSpider to scrapy.spider.Spider (<a href="https://github.com/scrapy/scrapy/issues/510">issue
510</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/519">issue
519</a>{.reference
.external})</p>
</li>
<li>
<p>Selectors register EXSLT namespaces by default (<a href="https://github.com/scrapy/scrapy/issues/472">issue
472</a>{.reference
.external})</p>
</li>
<li>
<p>Unify item loaders similar to selectors renaming (<a href="https://github.com/scrapy/scrapy/issues/461">issue
461</a>{.reference
.external})</p>
</li>
<li>
<p>Make [<code>RFPDupeFilter</code>{.docutils .literal .notranslate}]{.pre} class
easily subclassable (<a href="https://github.com/scrapy/scrapy/issues/533">issue
533</a>{.reference
.external})</p>
</li>
<li>
<p>Improve test coverage and forthcoming Python 3 support (<a href="https://github.com/scrapy/scrapy/issues/525">issue
525</a>{.reference
.external})</p>
</li>
<li>
<p>Promote startup info on settings and middleware to INFO level
(<a href="https://github.com/scrapy/scrapy/issues/520">issue 520</a>{.reference
.external})</p>
</li>
<li>
<p>Support partials in [<code>get_func_args</code>{.docutils .literal
.notranslate}]{.pre} util (<a href="https://github.com/scrapy/scrapy/issues/506">issue
506</a>{.reference
.external}, issue:504)</p>
</li>
<li>
<p>Allow running individual tests via tox (<a href="https://github.com/scrapy/scrapy/issues/503">issue
503</a>{.reference
.external})</p>
</li>
<li>
<p>Update extensions ignored by link extractors (<a href="https://github.com/scrapy/scrapy/issues/498">issue
498</a>{.reference
.external})</p>
</li>
<li>
<p>Add middleware methods to get files/images/thumbs paths (<a href="https://github.com/scrapy/scrapy/issues/490">issue
490</a>{.reference
.external})</p>
</li>
<li>
<p>Improve offsite middleware tests (<a href="https://github.com/scrapy/scrapy/issues/478">issue
478</a>{.reference
.external})</p>
</li>
<li>
<p>Add a way to skip default Referer header set by RefererMiddleware
(<a href="https://github.com/scrapy/scrapy/issues/475">issue 475</a>{.reference
.external})</p>
</li>
<li>
<p>Do not send [<code>x-gzip</code>{.docutils .literal .notranslate}]{.pre} in
default [<code>Accept-Encoding</code>{.docutils .literal .notranslate}]{.pre}
header (<a href="https://github.com/scrapy/scrapy/issues/469">issue
469</a>{.reference
.external})</p>
</li>
<li>
<p>Support defining http error handling using settings (<a href="https://github.com/scrapy/scrapy/issues/466">issue
466</a>{.reference
.external})</p>
</li>
<li>
<p>Use modern python idioms wherever you find legacies (<a href="https://github.com/scrapy/scrapy/issues/497">issue
497</a>{.reference
.external})</p>
</li>
<li>
<p>Improve and correct documentation (<a href="https://github.com/scrapy/scrapy/issues/527">issue
527</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/524">issue
524</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/521">issue
521</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/517">issue
517</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/512">issue
512</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/505">issue
505</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/502">issue
502</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/489">issue
489</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/465">issue
465</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/460">issue
460</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/425">issue
425</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/536">issue
536</a>{.reference
.external})
:::</p>
</li>
</ul>
<p>::: {#fixes .section}</p>
<h5 id="fixesheaderlink"><a class="header" href="#fixesheaderlink">Fixes<a href="#fixes" title="Permalink to this heading">¶</a>{.headerlink}</a></h5>
<ul>
<li>
<p>Update Selector class imports in CrawlSpider template (<a href="https://github.com/scrapy/scrapy/issues/484">issue
484</a>{.reference
.external})</p>
</li>
<li>
<p>Fix unexistent reference to [<code>engine.slots</code>{.docutils .literal
.notranslate}]{.pre} (<a href="https://github.com/scrapy/scrapy/issues/464">issue
464</a>{.reference
.external})</p>
</li>
<li>
<p>Do not try to call [<code>body_as_unicode()</code>{.docutils .literal
.notranslate}]{.pre} on a non-TextResponse instance (<a href="https://github.com/scrapy/scrapy/issues/462">issue
462</a>{.reference
.external})</p>
</li>
<li>
<p>Warn when subclassing XPathItemLoader, previously it only warned on
instantiation. (<a href="https://github.com/scrapy/scrapy/issues/523">issue
523</a>{.reference
.external})</p>
</li>
<li>
<p>Warn when subclassing XPathSelector, previously it only warned on
instantiation. (<a href="https://github.com/scrapy/scrapy/issues/537">issue
537</a>{.reference
.external})</p>
</li>
<li>
<p>Multiple fixes to memory stats (<a href="https://github.com/scrapy/scrapy/issues/531">issue
531</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/530">issue
530</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/529">issue
529</a>{.reference
.external})</p>
</li>
<li>
<p>Fix overriding url in [<code>FormRequest.from_response()</code>{.docutils
.literal .notranslate}]{.pre} (<a href="https://github.com/scrapy/scrapy/issues/507">issue
507</a>{.reference
.external})</p>
</li>
<li>
<p>Fix tests runner under pip 1.5 (<a href="https://github.com/scrapy/scrapy/issues/513">issue
513</a>{.reference
.external})</p>
</li>
<li>
<p>Fix logging error when spider name is unicode (<a href="https://github.com/scrapy/scrapy/issues/479">issue
479</a>{.reference
.external})
:::
:::</p>
</li>
</ul>
<p>::: {#scrapy-0-20-2-released-2013-12-09 .section}</p>
<h4 id="scrapy-0202-released-2013-12-09headerlink"><a class="header" href="#scrapy-0202-released-2013-12-09headerlink">Scrapy 0.20.2 (released 2013-12-09)<a href="#scrapy-0-20-2-released-2013-12-09" title="Permalink to this heading">¶</a>{.headerlink}</a></h4>
<ul>
<li>
<p>Update CrawlSpider Template with Selector changes (<a href="https://github.com/scrapy/scrapy/commit/6d1457d">commit
6d1457d</a>{.reference
.external})</p>
</li>
<li>
<p>fix method name in tutorial. closes GH-480 (<a href="https://github.com/scrapy/scrapy/commit/b4fc359">commit
b4fc359</a>{.reference
.external}
:::</p>
</li>
</ul>
<p>::: {#scrapy-0-20-1-released-2013-11-28 .section}</p>
<h4 id="scrapy-0201-released-2013-11-28headerlink"><a class="header" href="#scrapy-0201-released-2013-11-28headerlink">Scrapy 0.20.1 (released 2013-11-28)<a href="#scrapy-0-20-1-released-2013-11-28" title="Permalink to this heading">¶</a>{.headerlink}</a></h4>
<ul>
<li>
<p>include_package_data is required to build wheels from published
sources (<a href="https://github.com/scrapy/scrapy/commit/5ba1ad5">commit
5ba1ad5</a>{.reference
.external})</p>
</li>
<li>
<p>process_parallel was leaking the failures on its internal deferreds.
closes #458 (<a href="https://github.com/scrapy/scrapy/commit/419a780">commit
419a780</a>{.reference
.external})
:::</p>
</li>
</ul>
<p>::: {#scrapy-0-20-0-released-2013-11-08 .section}</p>
<h4 id="scrapy-0200-released-2013-11-08headerlink"><a class="header" href="#scrapy-0200-released-2013-11-08headerlink">Scrapy 0.20.0 (released 2013-11-08)<a href="#scrapy-0-20-0-released-2013-11-08" title="Permalink to this heading">¶</a>{.headerlink}</a></h4>
<p>::: {#id131 .section}</p>
<h5 id="enhancementsheaderlink-2"><a class="header" href="#enhancementsheaderlink-2">Enhancements<a href="#id131" title="Permalink to this heading">¶</a>{.headerlink}</a></h5>
<ul>
<li>
<p>New Selector's API including CSS selectors (<a href="https://github.com/scrapy/scrapy/issues/395">issue
395</a>{.reference
.external} and <a href="https://github.com/scrapy/scrapy/issues/426">issue
426</a>{.reference
.external}),</p>
</li>
<li>
<p>Request/Response url/body attributes are now immutable (modifying
them had been deprecated for a long time)</p>
</li>
<li>
<p><a href="index.html#std-setting-ITEM_PIPELINES">[<code>ITEM_PIPELINES</code>{.xref .std .std-setting .docutils .literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal} is now defined as a dict (instead of
a list)</p>
</li>
<li>
<p>Sitemap spider can fetch alternate URLs (<a href="https://github.com/scrapy/scrapy/issues/360">issue
360</a>{.reference
.external})</p>
</li>
<li>
<p>[<code>Selector.remove_namespaces()</code>{.docutils .literal
.notranslate}]{.pre} now remove namespaces from element's
attributes. (<a href="https://github.com/scrapy/scrapy/issues/416">issue
416</a>{.reference
.external})</p>
</li>
<li>
<p>Paved the road for Python 3.3+ (<a href="https://github.com/scrapy/scrapy/issues/435">issue
435</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/436">issue
436</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/431">issue
431</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/452">issue
452</a>{.reference
.external})</p>
</li>
<li>
<p>New item exporter using native python types with nesting support
(<a href="https://github.com/scrapy/scrapy/issues/366">issue 366</a>{.reference
.external})</p>
</li>
<li>
<p>Tune HTTP1.1 pool size so it matches concurrency defined by settings
(<a href="https://github.com/scrapy/scrapy/commit/b43b5f575">commit
b43b5f575</a>{.reference
.external})</p>
</li>
<li>
<p>scrapy.mail.MailSender now can connect over TLS or upgrade using
STARTTLS (<a href="https://github.com/scrapy/scrapy/issues/327">issue
327</a>{.reference
.external})</p>
</li>
<li>
<p>New FilesPipeline with functionality factored out from
ImagesPipeline (<a href="https://github.com/scrapy/scrapy/issues/370">issue
370</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/409">issue
409</a>{.reference
.external})</p>
</li>
<li>
<p>Recommend Pillow instead of PIL for image handling (<a href="https://github.com/scrapy/scrapy/issues/317">issue
317</a>{.reference
.external})</p>
</li>
<li>
<p>Added Debian packages for Ubuntu Quantal and Raring (<a href="https://github.com/scrapy/scrapy/commit/86230c0">commit
86230c0</a>{.reference
.external})</p>
</li>
<li>
<p>Mock server (used for tests) can listen for HTTPS requests (<a href="https://github.com/scrapy/scrapy/issues/410">issue
410</a>{.reference
.external})</p>
</li>
<li>
<p>Remove multi spider support from multiple core components (<a href="https://github.com/scrapy/scrapy/issues/422">issue
422</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/421">issue
421</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/420">issue
420</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/419">issue
419</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/423">issue
423</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/418">issue
418</a>{.reference
.external})</p>
</li>
<li>
<p>Travis-CI now tests Scrapy changes against development versions of
[<code>w3lib</code>{.docutils .literal .notranslate}]{.pre} and
[<code>queuelib</code>{.docutils .literal .notranslate}]{.pre} python packages.</p>
</li>
<li>
<p>Add pypy 2.1 to continuous integration tests (<a href="https://github.com/scrapy/scrapy/commit/ecfa7431">commit
ecfa7431</a>{.reference
.external})</p>
</li>
<li>
<p>Pylinted, pep8 and removed old-style exceptions from source (<a href="https://github.com/scrapy/scrapy/issues/430">issue
430</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/432">issue
432</a>{.reference
.external})</p>
</li>
<li>
<p>Use importlib for parametric imports (<a href="https://github.com/scrapy/scrapy/issues/445">issue
445</a>{.reference
.external})</p>
</li>
<li>
<p>Handle a regression introduced in Python 2.7.5 that affects
XmlItemExporter (<a href="https://github.com/scrapy/scrapy/issues/372">issue
372</a>{.reference
.external})</p>
</li>
<li>
<p>Bugfix crawling shutdown on SIGINT (<a href="https://github.com/scrapy/scrapy/issues/450">issue
450</a>{.reference
.external})</p>
</li>
<li>
<p>Do not submit [<code>reset</code>{.docutils .literal .notranslate}]{.pre} type
inputs in FormRequest.from_response (<a href="https://github.com/scrapy/scrapy/commit/b326b87">commit
b326b87</a>{.reference
.external})</p>
</li>
<li>
<p>Do not silence download errors when request errback raises an
exception (<a href="https://github.com/scrapy/scrapy/commit/684cfc0">commit
684cfc0</a>{.reference
.external})
:::</p>
</li>
</ul>
<p>::: {#id132 .section}</p>
<h5 id="bugfixesheaderlink-2"><a class="header" href="#bugfixesheaderlink-2">Bugfixes<a href="#id132" title="Permalink to this heading">¶</a>{.headerlink}</a></h5>
<ul>
<li>
<p>Fix tests under Django 1.6 (<a href="https://github.com/scrapy/scrapy/commit/b6bed44c">commit
b6bed44c</a>{.reference
.external})</p>
</li>
<li>
<p>Lot of bugfixes to retry middleware under disconnections using HTTP
1.1 download handler</p>
</li>
<li>
<p>Fix inconsistencies among Twisted releases (<a href="https://github.com/scrapy/scrapy/issues/406">issue
406</a>{.reference
.external})</p>
</li>
<li>
<p>Fix Scrapy shell bugs (<a href="https://github.com/scrapy/scrapy/issues/418">issue
418</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/407">issue
407</a>{.reference
.external})</p>
</li>
<li>
<p>Fix invalid variable name in setup.py (<a href="https://github.com/scrapy/scrapy/issues/429">issue
429</a>{.reference
.external})</p>
</li>
<li>
<p>Fix tutorial references (<a href="https://github.com/scrapy/scrapy/issues/387">issue
387</a>{.reference
.external})</p>
</li>
<li>
<p>Improve request-response docs (<a href="https://github.com/scrapy/scrapy/issues/391">issue
391</a>{.reference
.external})</p>
</li>
<li>
<p>Improve best practices docs (<a href="https://github.com/scrapy/scrapy/issues/399">issue
399</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/400">issue
400</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/401">issue
401</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/402">issue
402</a>{.reference
.external})</p>
</li>
<li>
<p>Improve django integration docs (<a href="https://github.com/scrapy/scrapy/issues/404">issue
404</a>{.reference
.external})</p>
</li>
<li>
<p>Document [<code>bindaddress</code>{.docutils .literal .notranslate}]{.pre}
request meta (<a href="https://github.com/scrapy/scrapy/commit/37c24e01d7">commit
37c24e01d7</a>{.reference
.external})</p>
</li>
<li>
<p>Improve [<code>Request</code>{.docutils .literal .notranslate}]{.pre} class
documentation (<a href="https://github.com/scrapy/scrapy/issues/226">issue
226</a>{.reference
.external})
:::</p>
</li>
</ul>
<p>::: {#other .section}</p>
<h5 id="otherheaderlink"><a class="header" href="#otherheaderlink">Other<a href="#other" title="Permalink to this heading">¶</a>{.headerlink}</a></h5>
<ul>
<li>
<p>Dropped Python 2.6 support (<a href="https://github.com/scrapy/scrapy/issues/448">issue
448</a>{.reference
.external})</p>
</li>
<li>
<p>Add <a href="https://cssselect.readthedocs.io/en/latest/index.html" title="(in cssselect v1.2.0)">[cssselect]{.xref .std
.std-doc}</a>{.reference
.external} python package as install dependency</p>
</li>
<li>
<p>Drop libxml2 and multi selector's backend support,
<a href="https://lxml.de/">lxml</a>{.reference .external} is required from now
on.</p>
</li>
<li>
<p>Minimum Twisted version increased to 10.0.0, dropped Twisted 8.0
support.</p>
</li>
<li>
<p>Running test suite now requires [<code>mock</code>{.docutils .literal
.notranslate}]{.pre} python library (<a href="https://github.com/scrapy/scrapy/issues/390">issue
390</a>{.reference
.external})
:::</p>
</li>
</ul>
<p>::: {#thanks .section}</p>
<h5 id="thanksheaderlink"><a class="header" href="#thanksheaderlink">Thanks<a href="#thanks" title="Permalink to this heading">¶</a>{.headerlink}</a></h5>
<p>Thanks to everyone who contribute to this release!</p>
<p>List of contributors sorted by number of commits:</p>
<p>::: {.highlight-default .notranslate}
::: highlight
69 Daniel Graña &lt;dangra@...&gt;
37 Pablo Hoffman &lt;pablo@...&gt;
13 Mikhail Korobov &lt;kmike84@...&gt;
9 Alex Cepoi &lt;alex.cepoi@...&gt;
9 alexanderlukanin13 &lt;alexander.lukanin.13@...&gt;
8 Rolando Espinoza La fuente &lt;darkrho@...&gt;
8 Lukasz Biedrycki &lt;lukasz.biedrycki@...&gt;
6 Nicolas Ramirez &lt;nramirez.uy@...&gt;
3 Paul Tremberth &lt;paul.tremberth@...&gt;
2 Martin Olveyra &lt;molveyra@...&gt;
2 Stefan &lt;misc@...&gt;
2 Rolando Espinoza &lt;darkrho@...&gt;
2 Loren Davie &lt;loren@...&gt;
2 irgmedeiros &lt;irgmedeiros@...&gt;
1 Stefan Koch &lt;taikano@...&gt;
1 Stefan &lt;cct@...&gt;
1 scraperdragon &lt;dragon@...&gt;
1 Kumara Tharmalingam &lt;ktharmal@...&gt;
1 Francesco Piccinno &lt;stack.box@...&gt;
1 Marcos Campal &lt;duendex@...&gt;
1 Dragon Dave &lt;dragon@...&gt;
1 Capi Etheriel &lt;barraponto@...&gt;
1 cacovsky &lt;amarquesferraz@...&gt;
1 Berend Iwema &lt;berend@...&gt;
:::
:::
:::
:::</p>
<p>::: {#scrapy-0-18-4-released-2013-10-10 .section}</p>
<h4 id="scrapy-0184-released-2013-10-10headerlink"><a class="header" href="#scrapy-0184-released-2013-10-10headerlink">Scrapy 0.18.4 (released 2013-10-10)<a href="#scrapy-0-18-4-released-2013-10-10" title="Permalink to this heading">¶</a>{.headerlink}</a></h4>
<ul>
<li>
<p>IPython refuses to update the namespace. fix #396 (<a href="https://github.com/scrapy/scrapy/commit/3d32c4f">commit
3d32c4f</a>{.reference
.external})</p>
</li>
<li>
<p>Fix AlreadyCalledError replacing a request in shell command. closes
#407 (<a href="https://github.com/scrapy/scrapy/commit/b1d8919">commit
b1d8919</a>{.reference
.external})</p>
</li>
<li>
<p>Fix start_requests laziness and early hangs (<a href="https://github.com/scrapy/scrapy/commit/89faf52">commit
89faf52</a>{.reference
.external})
:::</p>
</li>
</ul>
<p>::: {#scrapy-0-18-3-released-2013-10-03 .section}</p>
<h4 id="scrapy-0183-released-2013-10-03headerlink"><a class="header" href="#scrapy-0183-released-2013-10-03headerlink">Scrapy 0.18.3 (released 2013-10-03)<a href="#scrapy-0-18-3-released-2013-10-03" title="Permalink to this heading">¶</a>{.headerlink}</a></h4>
<ul>
<li>
<p>fix regression on lazy evaluation of start requests (<a href="https://github.com/scrapy/scrapy/commit/12693a5">commit
12693a5</a>{.reference
.external})</p>
</li>
<li>
<p>forms: do not submit reset inputs (<a href="https://github.com/scrapy/scrapy/commit/e429f63">commit
e429f63</a>{.reference
.external})</p>
</li>
<li>
<p>increase unittest timeouts to decrease travis false positive
failures (<a href="https://github.com/scrapy/scrapy/commit/912202e">commit
912202e</a>{.reference
.external})</p>
</li>
<li>
<p>backport master fixes to json exporter (<a href="https://github.com/scrapy/scrapy/commit/cfc2d46">commit
cfc2d46</a>{.reference
.external})</p>
</li>
<li>
<p>Fix permission and set umask before generating sdist tarball
(<a href="https://github.com/scrapy/scrapy/commit/06149e0">commit
06149e0</a>{.reference
.external})
:::</p>
</li>
</ul>
<p>::: {#scrapy-0-18-2-released-2013-09-03 .section}</p>
<h4 id="scrapy-0182-released-2013-09-03headerlink"><a class="header" href="#scrapy-0182-released-2013-09-03headerlink">Scrapy 0.18.2 (released 2013-09-03)<a href="#scrapy-0-18-2-released-2013-09-03" title="Permalink to this heading">¶</a>{.headerlink}</a></h4>
<ul>
<li>Backport [<code>scrapy</code>{.docutils .literal
.notranslate}]{.pre}<code> </code>{.docutils .literal
.notranslate}[<code>check</code>{.docutils .literal .notranslate}]{.pre}
command fixes and backward compatible multi crawler process(<a href="https://github.com/scrapy/scrapy/issues/339">issue
339</a>{.reference
.external})
:::</li>
</ul>
<p>::: {#scrapy-0-18-1-released-2013-08-27 .section}</p>
<h4 id="scrapy-0181-released-2013-08-27headerlink"><a class="header" href="#scrapy-0181-released-2013-08-27headerlink">Scrapy 0.18.1 (released 2013-08-27)<a href="#scrapy-0-18-1-released-2013-08-27" title="Permalink to this heading">¶</a>{.headerlink}</a></h4>
<ul>
<li>
<p>remove extra import added by cherry picked changes (<a href="https://github.com/scrapy/scrapy/commit/d20304e">commit
d20304e</a>{.reference
.external})</p>
</li>
<li>
<p>fix crawling tests under twisted pre 11.0.0 (<a href="https://github.com/scrapy/scrapy/commit/1994f38">commit
1994f38</a>{.reference
.external})</p>
</li>
<li>
<p>py26 can not format zero length fields {} (<a href="https://github.com/scrapy/scrapy/commit/abf756f">commit
abf756f</a>{.reference
.external})</p>
</li>
<li>
<p>test PotentiaDataLoss errors on unbound responses (<a href="https://github.com/scrapy/scrapy/commit/b15470d">commit
b15470d</a>{.reference
.external})</p>
</li>
<li>
<p>Treat responses without content-length or Transfer-Encoding as good
responses (<a href="https://github.com/scrapy/scrapy/commit/c4bf324">commit
c4bf324</a>{.reference
.external})</p>
</li>
<li>
<p>do no include ResponseFailed if http11 handler is not enabled
(<a href="https://github.com/scrapy/scrapy/commit/6cbe684">commit
6cbe684</a>{.reference
.external})</p>
</li>
<li>
<p>New HTTP client wraps connection lost in ResponseFailed exception.
fix #373 (<a href="https://github.com/scrapy/scrapy/commit/1a20bba">commit
1a20bba</a>{.reference
.external})</p>
</li>
<li>
<p>limit travis-ci build matrix (<a href="https://github.com/scrapy/scrapy/commit/3b01bb8">commit
3b01bb8</a>{.reference
.external})</p>
</li>
<li>
<p>Merge pull request #375 from peterarenot/patch-1 (<a href="https://github.com/scrapy/scrapy/commit/fa766d7">commit
fa766d7</a>{.reference
.external})</p>
</li>
<li>
<p>Fixed so it refers to the correct folder (<a href="https://github.com/scrapy/scrapy/commit/3283809">commit
3283809</a>{.reference
.external})</p>
</li>
<li>
<p>added Quantal &amp; Raring to support Ubuntu releases (<a href="https://github.com/scrapy/scrapy/commit/1411923">commit
1411923</a>{.reference
.external})</p>
</li>
<li>
<p>fix retry middleware which didn't retry certain connection errors
after the upgrade to http1 client, closes GH-373 (<a href="https://github.com/scrapy/scrapy/commit/bb35ed0">commit
bb35ed0</a>{.reference
.external})</p>
</li>
<li>
<p>fix XmlItemExporter in Python 2.7.4 and 2.7.5 (<a href="https://github.com/scrapy/scrapy/commit/de3e451">commit
de3e451</a>{.reference
.external})</p>
</li>
<li>
<p>minor updates to 0.18 release notes (<a href="https://github.com/scrapy/scrapy/commit/c45e5f1">commit
c45e5f1</a>{.reference
.external})</p>
</li>
<li>
<p>fix contributors list format (<a href="https://github.com/scrapy/scrapy/commit/0b60031">commit
0b60031</a>{.reference
.external})
:::</p>
</li>
</ul>
<p>::: {#scrapy-0-18-0-released-2013-08-09 .section}</p>
<h4 id="scrapy-0180-released-2013-08-09headerlink"><a class="header" href="#scrapy-0180-released-2013-08-09headerlink">Scrapy 0.18.0 (released 2013-08-09)<a href="#scrapy-0-18-0-released-2013-08-09" title="Permalink to this heading">¶</a>{.headerlink}</a></h4>
<ul>
<li>
<p>Lot of improvements to testsuite run using Tox, including a way to
test on pypi</p>
</li>
<li>
<p>Handle GET parameters for AJAX crawlable urls (<a href="https://github.com/scrapy/scrapy/commit/3fe2a32">commit
3fe2a32</a>{.reference
.external})</p>
</li>
<li>
<p>Use lxml recover option to parse sitemaps (<a href="https://github.com/scrapy/scrapy/issues/347">issue
347</a>{.reference
.external})</p>
</li>
<li>
<p>Bugfix cookie merging by hostname and not by netloc (<a href="https://github.com/scrapy/scrapy/issues/352">issue
352</a>{.reference
.external})</p>
</li>
<li>
<p>Support disabling [<code>HttpCompressionMiddleware</code>{.docutils .literal
.notranslate}]{.pre} using a flag setting (<a href="https://github.com/scrapy/scrapy/issues/359">issue
359</a>{.reference
.external})</p>
</li>
<li>
<p>Support xml namespaces using [<code>iternodes</code>{.docutils .literal
.notranslate}]{.pre} parser in [<code>XMLFeedSpider</code>{.docutils .literal
.notranslate}]{.pre} (<a href="https://github.com/scrapy/scrapy/issues/12">issue
12</a>{.reference
.external})</p>
</li>
<li>
<p>Support [<code>dont_cache</code>{.docutils .literal .notranslate}]{.pre}
request meta flag (<a href="https://github.com/scrapy/scrapy/issues/19">issue
19</a>{.reference
.external})</p>
</li>
<li>
<p>Bugfix [<code>scrapy.utils.gz.gunzip</code>{.docutils .literal
.notranslate}]{.pre} broken by changes in python 2.7.4 (<a href="https://github.com/scrapy/scrapy/commit/4dc76e">commit
4dc76e</a>{.reference
.external})</p>
</li>
<li>
<p>Bugfix url encoding on [<code>SgmlLinkExtractor</code>{.docutils .literal
.notranslate}]{.pre} (<a href="https://github.com/scrapy/scrapy/issues/24">issue
24</a>{.reference
.external})</p>
</li>
<li>
<p>Bugfix [<code>TakeFirst</code>{.docutils .literal .notranslate}]{.pre}
processor shouldn't discard zero (0) value (<a href="https://github.com/scrapy/scrapy/issues/59">issue
59</a>{.reference
.external})</p>
</li>
<li>
<p>Support nested items in xml exporter (<a href="https://github.com/scrapy/scrapy/issues/66">issue
66</a>{.reference
.external})</p>
</li>
<li>
<p>Improve cookies handling performance (<a href="https://github.com/scrapy/scrapy/issues/77">issue
77</a>{.reference
.external})</p>
</li>
<li>
<p>Log dupe filtered requests once (<a href="https://github.com/scrapy/scrapy/issues/105">issue
105</a>{.reference
.external})</p>
</li>
<li>
<p>Split redirection middleware into status and meta based middlewares
(<a href="https://github.com/scrapy/scrapy/issues/78">issue 78</a>{.reference
.external})</p>
</li>
<li>
<p>Use HTTP1.1 as default downloader handler (<a href="https://github.com/scrapy/scrapy/issues/109">issue
109</a>{.reference
.external} and <a href="https://github.com/scrapy/scrapy/issues/318">issue
318</a>{.reference
.external})</p>
</li>
<li>
<p>Support xpath form selection on
[<code>FormRequest.from_response</code>{.docutils .literal .notranslate}]{.pre}
(<a href="https://github.com/scrapy/scrapy/issues/185">issue 185</a>{.reference
.external})</p>
</li>
<li>
<p>Bugfix unicode decoding error on [<code>SgmlLinkExtractor</code>{.docutils
.literal .notranslate}]{.pre} (<a href="https://github.com/scrapy/scrapy/issues/199">issue
199</a>{.reference
.external})</p>
</li>
<li>
<p>Bugfix signal dispatching on pypi interpreter (<a href="https://github.com/scrapy/scrapy/issues/205">issue
205</a>{.reference
.external})</p>
</li>
<li>
<p>Improve request delay and concurrency handling (<a href="https://github.com/scrapy/scrapy/issues/206">issue
206</a>{.reference
.external})</p>
</li>
<li>
<p>Add RFC2616 cache policy to [<code>HttpCacheMiddleware</code>{.docutils
.literal .notranslate}]{.pre} (<a href="https://github.com/scrapy/scrapy/issues/212">issue
212</a>{.reference
.external})</p>
</li>
<li>
<p>Allow customization of messages logged by engine (<a href="https://github.com/scrapy/scrapy/issues/214">issue
214</a>{.reference
.external})</p>
</li>
<li>
<p>Multiples improvements to [<code>DjangoItem</code>{.docutils .literal
.notranslate}]{.pre} (<a href="https://github.com/scrapy/scrapy/issues/217">issue
217</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/218">issue
218</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/221">issue
221</a>{.reference
.external})</p>
</li>
<li>
<p>Extend Scrapy commands using setuptools entry points (<a href="https://github.com/scrapy/scrapy/issues/260">issue
260</a>{.reference
.external})</p>
</li>
<li>
<p>Allow spider [<code>allowed_domains</code>{.docutils .literal
.notranslate}]{.pre} value to be set/tuple (<a href="https://github.com/scrapy/scrapy/issues/261">issue
261</a>{.reference
.external})</p>
</li>
<li>
<p>Support [<code>settings.getdict</code>{.docutils .literal .notranslate}]{.pre}
(<a href="https://github.com/scrapy/scrapy/issues/269">issue 269</a>{.reference
.external})</p>
</li>
<li>
<p>Simplify internal [<code>scrapy.core.scraper</code>{.docutils .literal
.notranslate}]{.pre} slot handling (<a href="https://github.com/scrapy/scrapy/issues/271">issue
271</a>{.reference
.external})</p>
</li>
<li>
<p>Added [<code>Item.copy</code>{.docutils .literal .notranslate}]{.pre} (<a href="https://github.com/scrapy/scrapy/issues/290">issue
290</a>{.reference
.external})</p>
</li>
<li>
<p>Collect idle downloader slots (<a href="https://github.com/scrapy/scrapy/issues/297">issue
297</a>{.reference
.external})</p>
</li>
<li>
<p>Add [<code>ftp://</code>{.docutils .literal .notranslate}]{.pre} scheme
downloader handler (<a href="https://github.com/scrapy/scrapy/issues/329">issue
329</a>{.reference
.external})</p>
</li>
<li>
<p>Added downloader benchmark webserver and spider tools
<a href="index.html#benchmarking">[Benchmarking]{.std .std-ref}</a>{.hoverxref
.tooltip .reference .internal}</p>
</li>
<li>
<p>Moved persistent (on disk) queues to a separate project
(<a href="https://github.com/scrapy/queuelib">queuelib</a>{.reference
.external}) which Scrapy now depends on</p>
</li>
<li>
<p>Add Scrapy commands using external libraries (<a href="https://github.com/scrapy/scrapy/issues/260">issue
260</a>{.reference
.external})</p>
</li>
<li>
<p>Added [<code>--pdb</code>{.docutils .literal .notranslate}]{.pre} option to
[<code>scrapy</code>{.docutils .literal .notranslate}]{.pre} command line tool</p>
</li>
<li>
<p>Added <a href="index.html#scrapy.selector.Selector.remove_namespaces" title="scrapy.selector.Selector.remove_namespaces">[<code>XPathSelector.remove_namespaces</code>{.xref .py .py-meth
.docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal} which allows to remove all namespaces from XML documents
for convenience (to work with namespace-less XPaths). Documented in
<a href="index.html#topics-selectors">[Selectors]{.std .std-ref}</a>{.hoverxref
.tooltip .reference .internal}.</p>
</li>
<li>
<p>Several improvements to spider contracts</p>
</li>
<li>
<p>New default middleware named MetaRefreshMiddleware that handles
meta-refresh html tag redirections,</p>
</li>
<li>
<p>MetaRefreshMiddleware and RedirectMiddleware have different
priorities to address #62</p>
</li>
<li>
<p>added from_crawler method to spiders</p>
</li>
<li>
<p>added system tests with mock server</p>
</li>
<li>
<p>more improvements to macOS compatibility (thanks Alex Cepoi)</p>
</li>
<li>
<p>several more cleanups to singletons and multi-spider support (thanks
Nicolas Ramirez)</p>
</li>
<li>
<p>support custom download slots</p>
</li>
<li>
<p>added --spider option to &quot;shell&quot; command.</p>
</li>
<li>
<p>log overridden settings when Scrapy starts</p>
</li>
</ul>
<p>Thanks to everyone who contribute to this release. Here is a list of
contributors sorted by number of commits:</p>
<p>::: {.highlight-default .notranslate}
::: highlight
130 Pablo Hoffman &lt;pablo@...&gt;
97 Daniel Graña &lt;dangra@...&gt;
20 Nicolás Ramírez &lt;nramirez.uy@...&gt;
13 Mikhail Korobov &lt;kmike84@...&gt;
12 Pedro Faustino &lt;pedrobandim@...&gt;
11 Steven Almeroth &lt;sroth77@...&gt;
5 Rolando Espinoza La fuente &lt;darkrho@...&gt;
4 Michal Danilak &lt;mimino.coder@...&gt;
4 Alex Cepoi &lt;alex.cepoi@...&gt;
4 Alexandr N Zamaraev (aka tonal) &lt;tonal@...&gt;
3 paul &lt;paul.tremberth@...&gt;
3 Martin Olveyra &lt;molveyra@...&gt;
3 Jordi Llonch &lt;llonchj@...&gt;
3 arijitchakraborty &lt;myself.arijit@...&gt;
2 Shane Evans &lt;shane.evans@...&gt;
2 joehillen &lt;joehillen@...&gt;
2 Hart &lt;HartSimha@...&gt;
2 Dan &lt;ellisd23@...&gt;
1 Zuhao Wan &lt;wanzuhao@...&gt;
1 whodatninja &lt;blake@...&gt;
1 vkrest &lt;v.krestiannykov@...&gt;
1 tpeng &lt;pengtaoo@...&gt;
1 Tom Mortimer-Jones &lt;tom@...&gt;
1 Rocio Aramberri &lt;roschegel@...&gt;
1 Pedro &lt;pedro@...&gt;
1 notsobad &lt;wangxiaohugg@...&gt;
1 Natan L &lt;kuyanatan.nlao@...&gt;
1 Mark Grey &lt;mark.grey@...&gt;
1 Luan &lt;luanpab@...&gt;
1 Libor Nenadál &lt;libor.nenadal@...&gt;
1 Juan M Uys &lt;opyate@...&gt;
1 Jonas Brunsgaard &lt;jonas.brunsgaard@...&gt;
1 Ilya Baryshev &lt;baryshev@...&gt;
1 Hasnain Lakhani &lt;m.hasnain.lakhani@...&gt;
1 Emanuel Schorsch &lt;emschorsch@...&gt;
1 Chris Tilden &lt;chris.tilden@...&gt;
1 Capi Etheriel &lt;barraponto@...&gt;
1 cacovsky &lt;amarquesferraz@...&gt;
1 Berend Iwema &lt;berend@...&gt;
:::
:::
:::</p>
<p>::: {#scrapy-0-16-5-released-2013-05-30 .section}</p>
<h4 id="scrapy-0165-released-2013-05-30headerlink"><a class="header" href="#scrapy-0165-released-2013-05-30headerlink">Scrapy 0.16.5 (released 2013-05-30)<a href="#scrapy-0-16-5-released-2013-05-30" title="Permalink to this heading">¶</a>{.headerlink}</a></h4>
<ul>
<li>
<p>obey request method when Scrapy deploy is redirected to a new
endpoint (<a href="https://github.com/scrapy/scrapy/commit/8c4fcee">commit
8c4fcee</a>{.reference
.external})</p>
</li>
<li>
<p>fix inaccurate downloader middleware documentation. refs #280
(<a href="https://github.com/scrapy/scrapy/commit/40667cb">commit
40667cb</a>{.reference
.external})</p>
</li>
<li>
<p>doc: remove links to diveintopython.org, which is no longer
available. closes #246 (<a href="https://github.com/scrapy/scrapy/commit/bd58bfa">commit
bd58bfa</a>{.reference
.external})</p>
</li>
<li>
<p>Find form nodes in invalid html5 documents (<a href="https://github.com/scrapy/scrapy/commit/e3d6945">commit
e3d6945</a>{.reference
.external})</p>
</li>
<li>
<p>Fix typo labeling attrs type bool instead of list (<a href="https://github.com/scrapy/scrapy/commit/a274276">commit
a274276</a>{.reference
.external})
:::</p>
</li>
</ul>
<p>::: {#scrapy-0-16-4-released-2013-01-23 .section}</p>
<h4 id="scrapy-0164-released-2013-01-23headerlink"><a class="header" href="#scrapy-0164-released-2013-01-23headerlink">Scrapy 0.16.4 (released 2013-01-23)<a href="#scrapy-0-16-4-released-2013-01-23" title="Permalink to this heading">¶</a>{.headerlink}</a></h4>
<ul>
<li>
<p>fixes spelling errors in documentation (<a href="https://github.com/scrapy/scrapy/commit/6d2b3aa">commit
6d2b3aa</a>{.reference
.external})</p>
</li>
<li>
<p>add doc about disabling an extension. refs #132 (<a href="https://github.com/scrapy/scrapy/commit/c90de33">commit
c90de33</a>{.reference
.external})</p>
</li>
<li>
<p>Fixed error message formatting. log.err() doesn't support cool
formatting and when error occurred, the message was: &quot;ERROR: Error
processing %(item)s&quot; (<a href="https://github.com/scrapy/scrapy/commit/c16150c">commit
c16150c</a>{.reference
.external})</p>
</li>
<li>
<p>lint and improve images pipeline error logging (<a href="https://github.com/scrapy/scrapy/commit/56b45fc">commit
56b45fc</a>{.reference
.external})</p>
</li>
<li>
<p>fixed doc typos (<a href="https://github.com/scrapy/scrapy/commit/243be84">commit
243be84</a>{.reference
.external})</p>
</li>
<li>
<p>add documentation topics: Broad Crawls &amp; Common Practices (<a href="https://github.com/scrapy/scrapy/commit/1fbb715">commit
1fbb715</a>{.reference
.external})</p>
</li>
<li>
<p>fix bug in Scrapy parse command when spider is not specified
explicitly. closes #209 (<a href="https://github.com/scrapy/scrapy/commit/c72e682">commit
c72e682</a>{.reference
.external})</p>
</li>
<li>
<p>Update docs/topics/commands.rst (<a href="https://github.com/scrapy/scrapy/commit/28eac7a">commit
28eac7a</a>{.reference
.external})
:::</p>
</li>
</ul>
<p>::: {#scrapy-0-16-3-released-2012-12-07 .section}</p>
<h4 id="scrapy-0163-released-2012-12-07headerlink"><a class="header" href="#scrapy-0163-released-2012-12-07headerlink">Scrapy 0.16.3 (released 2012-12-07)<a href="#scrapy-0-16-3-released-2012-12-07" title="Permalink to this heading">¶</a>{.headerlink}</a></h4>
<ul>
<li>
<p>Remove concurrency limitation when using download delays and still
ensure inter-request delays are enforced (<a href="https://github.com/scrapy/scrapy/commit/487b9b5">commit
487b9b5</a>{.reference
.external})</p>
</li>
<li>
<p>add error details when image pipeline fails (<a href="https://github.com/scrapy/scrapy/commit/8232569">commit
8232569</a>{.reference
.external})</p>
</li>
<li>
<p>improve macOS compatibility (<a href="https://github.com/scrapy/scrapy/commit/8dcf8aa">commit
8dcf8aa</a>{.reference
.external})</p>
</li>
<li>
<p>setup.py: use README.rst to populate long_description (<a href="https://github.com/scrapy/scrapy/commit/7b5310d">commit
7b5310d</a>{.reference
.external})</p>
</li>
<li>
<p>doc: removed obsolete references to ClientForm (<a href="https://github.com/scrapy/scrapy/commit/80f9bb6">commit
80f9bb6</a>{.reference
.external})</p>
</li>
<li>
<p>correct docs for default storage backend (<a href="https://github.com/scrapy/scrapy/commit/2aa491b">commit
2aa491b</a>{.reference
.external})</p>
</li>
<li>
<p>doc: removed broken proxyhub link from FAQ (<a href="https://github.com/scrapy/scrapy/commit/bdf61c4">commit
bdf61c4</a>{.reference
.external})</p>
</li>
<li>
<p>Fixed docs typo in SpiderOpenCloseLogging example (<a href="https://github.com/scrapy/scrapy/commit/7184094">commit
7184094</a>{.reference
.external})
:::</p>
</li>
</ul>
<p>::: {#scrapy-0-16-2-released-2012-11-09 .section}</p>
<h4 id="scrapy-0162-released-2012-11-09headerlink"><a class="header" href="#scrapy-0162-released-2012-11-09headerlink">Scrapy 0.16.2 (released 2012-11-09)<a href="#scrapy-0-16-2-released-2012-11-09" title="Permalink to this heading">¶</a>{.headerlink}</a></h4>
<ul>
<li>
<p>Scrapy contracts: python2.6 compat (<a href="https://github.com/scrapy/scrapy/commit/a4a9199">commit
a4a9199</a>{.reference
.external})</p>
</li>
<li>
<p>Scrapy contracts verbose option (<a href="https://github.com/scrapy/scrapy/commit/ec41673">commit
ec41673</a>{.reference
.external})</p>
</li>
<li>
<p>proper unittest-like output for Scrapy contracts (<a href="https://github.com/scrapy/scrapy/commit/86635e4">commit
86635e4</a>{.reference
.external})</p>
</li>
<li>
<p>added open_in_browser to debugging doc (<a href="https://github.com/scrapy/scrapy/commit/c9b690d">commit
c9b690d</a>{.reference
.external})</p>
</li>
<li>
<p>removed reference to global Scrapy stats from settings doc (<a href="https://github.com/scrapy/scrapy/commit/dd55067">commit
dd55067</a>{.reference
.external})</p>
</li>
<li>
<p>Fix SpiderState bug in Windows platforms (<a href="https://github.com/scrapy/scrapy/commit/58998f4">commit
58998f4</a>{.reference
.external})
:::</p>
</li>
</ul>
<p>::: {#scrapy-0-16-1-released-2012-10-26 .section}</p>
<h4 id="scrapy-0161-released-2012-10-26headerlink"><a class="header" href="#scrapy-0161-released-2012-10-26headerlink">Scrapy 0.16.1 (released 2012-10-26)<a href="#scrapy-0-16-1-released-2012-10-26" title="Permalink to this heading">¶</a>{.headerlink}</a></h4>
<ul>
<li>
<p>fixed LogStats extension, which got broken after a wrong merge
before the 0.16 release (<a href="https://github.com/scrapy/scrapy/commit/8c780fd">commit
8c780fd</a>{.reference
.external})</p>
</li>
<li>
<p>better backward compatibility for scrapy.conf.settings (<a href="https://github.com/scrapy/scrapy/commit/3403089">commit
3403089</a>{.reference
.external})</p>
</li>
<li>
<p>extended documentation on how to access crawler stats from
extensions (<a href="https://github.com/scrapy/scrapy/commit/c4da0b5">commit
c4da0b5</a>{.reference
.external})</p>
</li>
<li>
<p>removed .hgtags (no longer needed now that Scrapy uses git) (<a href="https://github.com/scrapy/scrapy/commit/d52c188">commit
d52c188</a>{.reference
.external})</p>
</li>
<li>
<p>fix dashes under rst headers (<a href="https://github.com/scrapy/scrapy/commit/fa4f7f9">commit
fa4f7f9</a>{.reference
.external})</p>
</li>
<li>
<p>set release date for 0.16.0 in news (<a href="https://github.com/scrapy/scrapy/commit/e292246">commit
e292246</a>{.reference
.external})
:::</p>
</li>
</ul>
<p>::: {#scrapy-0-16-0-released-2012-10-18 .section}</p>
<h4 id="scrapy-0160-released-2012-10-18headerlink"><a class="header" href="#scrapy-0160-released-2012-10-18headerlink">Scrapy 0.16.0 (released 2012-10-18)<a href="#scrapy-0-16-0-released-2012-10-18" title="Permalink to this heading">¶</a>{.headerlink}</a></h4>
<p>Scrapy changes:</p>
<ul>
<li>
<p>added <a href="index.html#topics-contracts">[Spiders Contracts]{.std
.std-ref}</a>{.hoverxref .tooltip
.reference .internal}, a mechanism for testing spiders in a
formal/reproducible way</p>
</li>
<li>
<p>added options [<code>-o</code>{.docutils .literal .notranslate}]{.pre} and
[<code>-t</code>{.docutils .literal .notranslate}]{.pre} to the
<a href="index.html#std-command-runspider">[<code>runspider</code>{.xref .std .std-command .docutils .literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal} command</p>
</li>
<li>
<p>documented <a href="index.html#document-topics/autothrottle">[AutoThrottle
extension]{.doc}</a>{.reference
.internal} and added to extensions installed by default. You still
need to enable it with <a href="index.html#std-setting-AUTOTHROTTLE_ENABLED">[<code>AUTOTHROTTLE_ENABLED</code>{.xref .std
.std-setting .docutils .literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal}</p>
</li>
<li>
<p>major Stats Collection refactoring: removed separation of
global/per-spider stats, removed stats-related signals
([<code>stats_spider_opened</code>{.docutils .literal .notranslate}]{.pre},
etc). Stats are much simpler now, backward compatibility is kept on
the Stats Collector API and signals.</p>
</li>
<li>
<p>added <a href="index.html#scrapy.spidermiddlewares.SpiderMiddleware.process_start_requests" title="scrapy.spidermiddlewares.SpiderMiddleware.process_start_requests">[<code>process_start_requests()</code>{.xref .py .py-meth .docutils
.literal
.notranslate}]{.pre}</a>{.reference
.internal} method to spider middlewares</p>
</li>
<li>
<p>dropped Signals singleton. Signals should now be accessed through
the Crawler.signals attribute. See the signals documentation for
more info.</p>
</li>
<li>
<p>dropped Stats Collector singleton. Stats can now be accessed through
the Crawler.stats attribute. See the stats collection documentation
for more info.</p>
</li>
<li>
<p>documented <a href="index.html#topics-api">[Core API]{.std
.std-ref}</a>{.hoverxref .tooltip .reference
.internal}</p>
</li>
<li>
<p>[<code>lxml</code>{.docutils .literal .notranslate}]{.pre} is now the default
selectors backend instead of [<code>libxml2</code>{.docutils .literal
.notranslate}]{.pre}</p>
</li>
<li>
<p>ported FormRequest.from_response() to use
<a href="https://lxml.de/">lxml</a>{.reference .external} instead of
<a href="http://wwwsearch.sourceforge.net/old/ClientForm/">ClientForm</a>{.reference
.external}</p>
</li>
<li>
<p>removed modules: [<code>scrapy.xlib.BeautifulSoup</code>{.docutils .literal
.notranslate}]{.pre} and [<code>scrapy.xlib.ClientForm</code>{.docutils
.literal .notranslate}]{.pre}</p>
</li>
<li>
<p>SitemapSpider: added support for sitemap urls ending in .xml and
.xml.gz, even if they advertise a wrong content type (<a href="https://github.com/scrapy/scrapy/commit/10ed28b">commit
10ed28b</a>{.reference
.external})</p>
</li>
<li>
<p>StackTraceDump extension: also dump trackref live references
(<a href="https://github.com/scrapy/scrapy/commit/fe2ce93">commit
fe2ce93</a>{.reference
.external})</p>
</li>
<li>
<p>nested items now fully supported in JSON and JSONLines exporters</p>
</li>
<li>
<p>added <a href="index.html#std-reqmeta-cookiejar">[<code>cookiejar</code>{.xref .std .std-reqmeta .docutils .literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal} Request meta key to support multiple
cookie sessions per spider</p>
</li>
<li>
<p>decoupled encoding detection code to
<a href="https://github.com/scrapy/w3lib/blob/master/w3lib/encoding.py">w3lib.encoding</a>{.reference
.external}, and ported Scrapy code to use that module</p>
</li>
<li>
<p>dropped support for Python 2.5. See
<a href="https://blog.scrapinghub.com/2012/02/27/scrapy-0-15-dropping-support-for-python-2-5/">https://blog.scrapinghub.com/2012/02/27/scrapy-0-15-dropping-support-for-python-2-5/</a>{.reference
.external}</p>
</li>
<li>
<p>dropped support for Twisted 2.5</p>
</li>
<li>
<p>added <a href="index.html#std-setting-REFERER_ENABLED">[<code>REFERER_ENABLED</code>{.xref .std .std-setting .docutils .literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal} setting, to control referer
middleware</p>
</li>
<li>
<p>changed default user agent to: [<code>Scrapy/VERSION</code>{.docutils .literal
.notranslate}]{.pre}<code> </code>{.docutils .literal
.notranslate}[<code>(+http://scrapy.org)</code>{.docutils .literal
.notranslate}]{.pre}</p>
</li>
<li>
<p>removed (undocumented) [<code>HTMLImageLinkExtractor</code>{.docutils .literal
.notranslate}]{.pre} class from
[<code>scrapy.contrib.linkextractors.image</code>{.docutils .literal
.notranslate}]{.pre}</p>
</li>
<li>
<p>removed per-spider settings (to be replaced by instantiating
multiple crawler objects)</p>
</li>
<li>
<p>[<code>USER_AGENT</code>{.docutils .literal .notranslate}]{.pre} spider
attribute will no longer work, use [<code>user_agent</code>{.docutils .literal
.notranslate}]{.pre} attribute instead</p>
</li>
<li>
<p>[<code>DOWNLOAD_TIMEOUT</code>{.docutils .literal .notranslate}]{.pre} spider
attribute will no longer work, use [<code>download_timeout</code>{.docutils
.literal .notranslate}]{.pre} attribute instead</p>
</li>
<li>
<p>removed [<code>ENCODING_ALIASES</code>{.docutils .literal .notranslate}]{.pre}
setting, as encoding auto-detection has been moved to the
<a href="https://github.com/scrapy/w3lib">w3lib</a>{.reference .external}
library</p>
</li>
<li>
<p>promoted <a href="index.html#topics-djangoitem">[DjangoItem]{.std
.std-ref}</a>{.hoverxref .tooltip
.reference .internal} to main contrib</p>
</li>
<li>
<p>LogFormatter method now return dicts(instead of strings) to support
lazy formatting (<a href="https://github.com/scrapy/scrapy/issues/164">issue
164</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/commit/dcef7b0">commit
dcef7b0</a>{.reference
.external})</p>
</li>
<li>
<p>downloader handlers (<a href="index.html#std-setting-DOWNLOAD_HANDLERS">[<code>DOWNLOAD_HANDLERS</code>{.xref .std .std-setting
.docutils .literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal} setting) now receive settings as the
first argument of the [<code>__init__</code>{.docutils .literal
.notranslate}]{.pre} method</p>
</li>
<li>
<p>replaced memory usage accounting with (more portable)
<a href="https://docs.python.org/2/library/resource.html">resource</a>{.reference
.external} module, removed [<code>scrapy.utils.memory</code>{.docutils .literal
.notranslate}]{.pre} module</p>
</li>
<li>
<p>removed signal: [<code>scrapy.mail.mail_sent</code>{.docutils .literal
.notranslate}]{.pre}</p>
</li>
<li>
<p>removed [<code>TRACK_REFS</code>{.docutils .literal .notranslate}]{.pre}
setting, now <a href="index.html#topics-leaks-trackrefs">[trackrefs]{.std
.std-ref}</a>{.hoverxref .tooltip
.reference .internal} is always enabled</p>
</li>
<li>
<p>DBM is now the default storage backend for HTTP cache middleware</p>
</li>
<li>
<p>number of log messages (per level) are now tracked through Scrapy
stats (stat name: [<code>log_count/LEVEL</code>{.docutils .literal
.notranslate}]{.pre})</p>
</li>
<li>
<p>number received responses are now tracked through Scrapy stats (stat
name: [<code>response_received_count</code>{.docutils .literal
.notranslate}]{.pre})</p>
</li>
<li>
<p>removed [<code>scrapy.log.started</code>{.docutils .literal
.notranslate}]{.pre} attribute
:::</p>
</li>
</ul>
<p>::: {#scrapy-0-14-4 .section}</p>
<h4 id="scrapy-0144headerlink"><a class="header" href="#scrapy-0144headerlink">Scrapy 0.14.4<a href="#scrapy-0-14-4" title="Permalink to this heading">¶</a>{.headerlink}</a></h4>
<ul>
<li>
<p>added precise to supported Ubuntu distros (<a href="https://github.com/scrapy/scrapy/commit/b7e46df">commit
b7e46df</a>{.reference
.external})</p>
</li>
<li>
<p>fixed bug in json-rpc webservice reported in
<a href="https://groups.google.com/forum/#!topic/scrapy-users/qgVBmFybNAQ/discussion">https://groups.google.com/forum/#!topic/scrapy-users/qgVBmFybNAQ/discussion</a>{.reference
.external}. also removed no longer supported 'run' command from
extras/scrapy-ws.py (<a href="https://github.com/scrapy/scrapy/commit/340fbdb">commit
340fbdb</a>{.reference
.external})</p>
</li>
<li>
<p>meta tag attributes for content-type http equiv can be in any order.
#123 (<a href="https://github.com/scrapy/scrapy/commit/0cb68af">commit
0cb68af</a>{.reference
.external})</p>
</li>
<li>
<p>replace &quot;import Image&quot; by more standard &quot;from PIL import Image&quot;.
closes #88 (<a href="https://github.com/scrapy/scrapy/commit/4d17048">commit
4d17048</a>{.reference
.external})</p>
</li>
<li>
<p>return trial status as bin/runtests.sh exit value. #118 (<a href="https://github.com/scrapy/scrapy/commit/b7b2e7f">commit
b7b2e7f</a>{.reference
.external})
:::</p>
</li>
</ul>
<p>::: {#scrapy-0-14-3 .section}</p>
<h4 id="scrapy-0143headerlink"><a class="header" href="#scrapy-0143headerlink">Scrapy 0.14.3<a href="#scrapy-0-14-3" title="Permalink to this heading">¶</a>{.headerlink}</a></h4>
<ul>
<li>
<p>forgot to include pydispatch license. #118 (<a href="https://github.com/scrapy/scrapy/commit/fd85f9c">commit
fd85f9c</a>{.reference
.external})</p>
</li>
<li>
<p>include egg files used by testsuite in source distribution. #118
(<a href="https://github.com/scrapy/scrapy/commit/c897793">commit
c897793</a>{.reference
.external})</p>
</li>
<li>
<p>update docstring in project template to avoid confusion with
genspider command, which may be considered as an advanced feature.
refs #107 (<a href="https://github.com/scrapy/scrapy/commit/2548dcc">commit
2548dcc</a>{.reference
.external})</p>
</li>
<li>
<p>added note to docs/topics/firebug.rst about google directory being
shut down (<a href="https://github.com/scrapy/scrapy/commit/668e352">commit
668e352</a>{.reference
.external})</p>
</li>
<li>
<p>don't discard slot when empty, just save in another dict in order to
recycle if needed again. (<a href="https://github.com/scrapy/scrapy/commit/8e9f607">commit
8e9f607</a>{.reference
.external})</p>
</li>
<li>
<p>do not fail handling unicode xpaths in libxml2 backed selectors
(<a href="https://github.com/scrapy/scrapy/commit/b830e95">commit
b830e95</a>{.reference
.external})</p>
</li>
<li>
<p>fixed minor mistake in Request objects documentation (<a href="https://github.com/scrapy/scrapy/commit/bf3c9ee">commit
bf3c9ee</a>{.reference
.external})</p>
</li>
<li>
<p>fixed minor defect in link extractors documentation (<a href="https://github.com/scrapy/scrapy/commit/ba14f38">commit
ba14f38</a>{.reference
.external})</p>
</li>
<li>
<p>removed some obsolete remaining code related to sqlite support in
Scrapy (<a href="https://github.com/scrapy/scrapy/commit/0665175">commit
0665175</a>{.reference
.external})
:::</p>
</li>
</ul>
<p>::: {#scrapy-0-14-2 .section}</p>
<h4 id="scrapy-0142headerlink"><a class="header" href="#scrapy-0142headerlink">Scrapy 0.14.2<a href="#scrapy-0-14-2" title="Permalink to this heading">¶</a>{.headerlink}</a></h4>
<ul>
<li>
<p>move buffer pointing to start of file before computing checksum.
refs #92 (<a href="https://github.com/scrapy/scrapy/commit/6a5bef2">commit
6a5bef2</a>{.reference
.external})</p>
</li>
<li>
<p>Compute image checksum before persisting images. closes #92 (<a href="https://github.com/scrapy/scrapy/commit/9817df1">commit
9817df1</a>{.reference
.external})</p>
</li>
<li>
<p>remove leaking references in cached failures (<a href="https://github.com/scrapy/scrapy/commit/673a120">commit
673a120</a>{.reference
.external})</p>
</li>
<li>
<p>fixed bug in MemoryUsage extension: get_engine_status() takes
exactly 1 argument (0 given) (<a href="https://github.com/scrapy/scrapy/commit/11133e9">commit
11133e9</a>{.reference
.external})</p>
</li>
<li>
<p>fixed struct.error on http compression middleware. closes #87
(<a href="https://github.com/scrapy/scrapy/commit/1423140">commit
1423140</a>{.reference
.external})</p>
</li>
<li>
<p>ajax crawling wasn't expanding for unicode urls (<a href="https://github.com/scrapy/scrapy/commit/0de3fb4">commit
0de3fb4</a>{.reference
.external})</p>
</li>
<li>
<p>Catch start_requests iterator errors. refs #83 (<a href="https://github.com/scrapy/scrapy/commit/454a21d">commit
454a21d</a>{.reference
.external})</p>
</li>
<li>
<p>Speed-up libxml2 XPathSelector (<a href="https://github.com/scrapy/scrapy/commit/2fbd662">commit
2fbd662</a>{.reference
.external})</p>
</li>
<li>
<p>updated versioning doc according to recent changes (<a href="https://github.com/scrapy/scrapy/commit/0a070f5">commit
0a070f5</a>{.reference
.external})</p>
</li>
<li>
<p>scrapyd: fixed documentation link (<a href="https://github.com/scrapy/scrapy/commit/2b4e4c3">commit
2b4e4c3</a>{.reference
.external})</p>
</li>
<li>
<p>extras/makedeb.py: no longer obtaining version from git (<a href="https://github.com/scrapy/scrapy/commit/caffe0e">commit
caffe0e</a>{.reference
.external})
:::</p>
</li>
</ul>
<p>::: {#scrapy-0-14-1 .section}</p>
<h4 id="scrapy-0141headerlink"><a class="header" href="#scrapy-0141headerlink">Scrapy 0.14.1<a href="#scrapy-0-14-1" title="Permalink to this heading">¶</a>{.headerlink}</a></h4>
<ul>
<li>
<p>extras/makedeb.py: no longer obtaining version from git (<a href="https://github.com/scrapy/scrapy/commit/caffe0e">commit
caffe0e</a>{.reference
.external})</p>
</li>
<li>
<p>bumped version to 0.14.1 (<a href="https://github.com/scrapy/scrapy/commit/6cb9e1c">commit
6cb9e1c</a>{.reference
.external})</p>
</li>
<li>
<p>fixed reference to tutorial directory (<a href="https://github.com/scrapy/scrapy/commit/4b86bd6">commit
4b86bd6</a>{.reference
.external})</p>
</li>
<li>
<p>doc: removed duplicated callback argument from Request.replace()
(<a href="https://github.com/scrapy/scrapy/commit/1aeccdd">commit
1aeccdd</a>{.reference
.external})</p>
</li>
<li>
<p>fixed formatting of scrapyd doc (<a href="https://github.com/scrapy/scrapy/commit/8bf19e6">commit
8bf19e6</a>{.reference
.external})</p>
</li>
<li>
<p>Dump stacks for all running threads and fix engine status dumped by
StackTraceDump extension (<a href="https://github.com/scrapy/scrapy/commit/14a8e6e">commit
14a8e6e</a>{.reference
.external})</p>
</li>
<li>
<p>added comment about why we disable ssl on boto images upload
(<a href="https://github.com/scrapy/scrapy/commit/5223575">commit
5223575</a>{.reference
.external})</p>
</li>
<li>
<p>SSL handshaking hangs when doing too many parallel connections to S3
(<a href="https://github.com/scrapy/scrapy/commit/63d583d">commit
63d583d</a>{.reference
.external})</p>
</li>
<li>
<p>change tutorial to follow changes on dmoz site (<a href="https://github.com/scrapy/scrapy/commit/bcb3198">commit
bcb3198</a>{.reference
.external})</p>
</li>
<li>
<p>Avoid _disconnectedDeferred AttributeError exception in
Twisted&gt;=11.1.0 (<a href="https://github.com/scrapy/scrapy/commit/98f3f87">commit
98f3f87</a>{.reference
.external})</p>
</li>
<li>
<p>allow spider to set autothrottle max concurrency (<a href="https://github.com/scrapy/scrapy/commit/175a4b5">commit
175a4b5</a>{.reference
.external})
:::</p>
</li>
</ul>
<p>::: {#scrapy-0-14 .section}</p>
<h4 id="scrapy-014headerlink"><a class="header" href="#scrapy-014headerlink">Scrapy 0.14<a href="#scrapy-0-14" title="Permalink to this heading">¶</a>{.headerlink}</a></h4>
<p>::: {#new-features-and-settings .section}</p>
<h5 id="new-features-and-settingsheaderlink"><a class="header" href="#new-features-and-settingsheaderlink">New features and settings<a href="#new-features-and-settings" title="Permalink to this heading">¶</a>{.headerlink}</a></h5>
<ul>
<li>
<p>Support for <a href="https://developers.google.com/search/docs/ajax-crawling/docs/getting-started?csw=1">AJAX crawlable
urls</a>{.reference
.external}</p>
</li>
<li>
<p>New persistent scheduler that stores requests on disk, allowing to
suspend and resume crawls
(<a href="http://hg.scrapy.org/scrapy/changeset/2737">r2737</a>{.reference
.external})</p>
</li>
<li>
<p>added [<code>-o</code>{.docutils .literal .notranslate}]{.pre} option to
[<code>scrapy</code>{.docutils .literal .notranslate}]{.pre}<code> </code>{.docutils
.literal .notranslate}[<code>crawl</code>{.docutils .literal
.notranslate}]{.pre}, a shortcut for dumping scraped items into a
file (or standard output using [<code>-</code>{.docutils .literal
.notranslate}]{.pre})</p>
</li>
<li>
<p>Added support for passing custom settings to Scrapyd
[<code>schedule.json</code>{.docutils .literal .notranslate}]{.pre} api
(<a href="http://hg.scrapy.org/scrapy/changeset/2779">r2779</a>{.reference
.external},
<a href="http://hg.scrapy.org/scrapy/changeset/2783">r2783</a>{.reference
.external})</p>
</li>
<li>
<p>New [<code>ChunkedTransferMiddleware</code>{.docutils .literal
.notranslate}]{.pre} (enabled by default) to support <a href="https://en.wikipedia.org/wiki/Chunked_transfer_encoding">chunked
transfer
encoding</a>{.reference
.external}
(<a href="http://hg.scrapy.org/scrapy/changeset/2769">r2769</a>{.reference
.external})</p>
</li>
<li>
<p>Add boto 2.0 support for S3 downloader handler
(<a href="http://hg.scrapy.org/scrapy/changeset/2763">r2763</a>{.reference
.external})</p>
</li>
<li>
<p>Added
<a href="https://docs.python.org/2/library/marshal.html">marshal</a>{.reference
.external} to formats supported by feed exports
(<a href="http://hg.scrapy.org/scrapy/changeset/2744">r2744</a>{.reference
.external})</p>
</li>
<li>
<p>In request errbacks, offending requests are now received in
[<code>failure.request</code>{.docutils .literal .notranslate}]{.pre} attribute
(<a href="http://hg.scrapy.org/scrapy/changeset/2738">r2738</a>{.reference
.external})</p>
</li>
<li></li>
</ul>
<pre><code>Big downloader refactoring to support per domain/ip concurrency limits ([r2732](http://hg.scrapy.org/scrapy/changeset/2732){.reference .external})

:   -   

        [`CONCURRENT_REQUESTS_PER_SPIDER`{.docutils .literal .notranslate}]{.pre} setting has been deprecated and replaced by:

        :   -   [[`CONCURRENT_REQUESTS`{.xref .std .std-setting
                .docutils .literal
                .notranslate}]{.pre}](index.html#std-setting-CONCURRENT_REQUESTS){.hoverxref
                .tooltip .reference .internal},
                [[`CONCURRENT_REQUESTS_PER_DOMAIN`{.xref .std
                .std-setting .docutils .literal
                .notranslate}]{.pre}](index.html#std-setting-CONCURRENT_REQUESTS_PER_DOMAIN){.hoverxref
                .tooltip .reference .internal},
                [[`CONCURRENT_REQUESTS_PER_IP`{.xref .std
                .std-setting .docutils .literal
                .notranslate}]{.pre}](index.html#std-setting-CONCURRENT_REQUESTS_PER_IP){.hoverxref
                .tooltip .reference .internal}

    -   check the documentation for more details
</code></pre>
<ul>
<li>
<p>Added builtin caching DNS resolver
(<a href="http://hg.scrapy.org/scrapy/changeset/2728">r2728</a>{.reference
.external})</p>
</li>
<li>
<p>Moved Amazon AWS-related components/extensions (SQS spider queue,
SimpleDB stats collector) to a separate project:
[scaws](<a href="https://github.com/scrapinghub/scaws">https://github.com/scrapinghub/scaws</a>{.reference
.external})
(<a href="http://hg.scrapy.org/scrapy/changeset/2706">r2706</a>{.reference
.external},
<a href="http://hg.scrapy.org/scrapy/changeset/2714">r2714</a>{.reference
.external})</p>
</li>
<li>
<p>Moved spider queues to scrapyd: [<code>scrapy.spiderqueue</code>{.docutils
.literal .notranslate}]{.pre} -&gt; [<code>scrapyd.spiderqueue</code>{.docutils
.literal .notranslate}]{.pre}
(<a href="http://hg.scrapy.org/scrapy/changeset/2708">r2708</a>{.reference
.external})</p>
</li>
<li>
<p>Moved sqlite utils to scrapyd: [<code>scrapy.utils.sqlite</code>{.docutils
.literal .notranslate}]{.pre} -&gt; [<code>scrapyd.sqlite</code>{.docutils
.literal .notranslate}]{.pre}
(<a href="http://hg.scrapy.org/scrapy/changeset/2781">r2781</a>{.reference
.external})</p>
</li>
<li>
<p>Real support for returning iterators on
[<code>start_requests()</code>{.docutils .literal .notranslate}]{.pre} method.
The iterator is now consumed during the crawl when the spider is
getting idle
(<a href="http://hg.scrapy.org/scrapy/changeset/2704">r2704</a>{.reference
.external})</p>
</li>
<li>
<p>Added <a href="index.html#std-setting-REDIRECT_ENABLED">[<code>REDIRECT_ENABLED</code>{.xref .std .std-setting .docutils
.literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal} setting to quickly enable/disable the
redirect middleware
(<a href="http://hg.scrapy.org/scrapy/changeset/2697">r2697</a>{.reference
.external})</p>
</li>
<li>
<p>Added <a href="index.html#std-setting-RETRY_ENABLED">[<code>RETRY_ENABLED</code>{.xref .std .std-setting .docutils .literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal} setting to quickly enable/disable the
retry middleware
(<a href="http://hg.scrapy.org/scrapy/changeset/2694">r2694</a>{.reference
.external})</p>
</li>
<li>
<p>Added [<code>CloseSpider</code>{.docutils .literal .notranslate}]{.pre}
exception to manually close spiders
(<a href="http://hg.scrapy.org/scrapy/changeset/2691">r2691</a>{.reference
.external})</p>
</li>
<li>
<p>Improved encoding detection by adding support for HTML5 meta charset
declaration
(<a href="http://hg.scrapy.org/scrapy/changeset/2690">r2690</a>{.reference
.external})</p>
</li>
<li>
<p>Refactored close spider behavior to wait for all downloads to finish
and be processed by spiders, before closing the spider
(<a href="http://hg.scrapy.org/scrapy/changeset/2688">r2688</a>{.reference
.external})</p>
</li>
<li>
<p>Added [<code>SitemapSpider</code>{.docutils .literal .notranslate}]{.pre} (see
documentation in Spiders page)
(<a href="http://hg.scrapy.org/scrapy/changeset/2658">r2658</a>{.reference
.external})</p>
</li>
<li>
<p>Added [<code>LogStats</code>{.docutils .literal .notranslate}]{.pre} extension
for periodically logging basic stats (like crawled pages and scraped
items)
(<a href="http://hg.scrapy.org/scrapy/changeset/2657">r2657</a>{.reference
.external})</p>
</li>
<li>
<p>Make handling of gzipped responses more robust (#319,
<a href="http://hg.scrapy.org/scrapy/changeset/2643">r2643</a>{.reference
.external}). Now Scrapy will try and decompress as much as possible
from a gzipped response, instead of failing with an
[<code>IOError</code>{.docutils .literal .notranslate}]{.pre}.</p>
</li>
<li>
<p>Simplified !MemoryDebugger extension to use stats for dumping memory
debugging info
(<a href="http://hg.scrapy.org/scrapy/changeset/2639">r2639</a>{.reference
.external})</p>
</li>
<li>
<p>Added new command to edit spiders: [<code>scrapy</code>{.docutils .literal
.notranslate}]{.pre}<code> </code>{.docutils .literal
.notranslate}[<code>edit</code>{.docutils .literal .notranslate}]{.pre}
(<a href="http://hg.scrapy.org/scrapy/changeset/2636">r2636</a>{.reference
.external}) and [<code>-e</code>{.docutils .literal .notranslate}]{.pre} flag
to [<code>genspider</code>{.docutils .literal .notranslate}]{.pre} command that
uses it
(<a href="http://hg.scrapy.org/scrapy/changeset/2653">r2653</a>{.reference
.external})</p>
</li>
<li>
<p>Changed default representation of items to pretty-printed dicts.
(<a href="http://hg.scrapy.org/scrapy/changeset/2631">r2631</a>{.reference
.external}). This improves default logging by making log more
readable in the default case, for both Scraped and Dropped lines.</p>
</li>
<li>
<p>Added <a href="index.html#std-signal-spider_error">[<code>spider_error</code>{.xref .std .std-signal .docutils .literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal} signal
(<a href="http://hg.scrapy.org/scrapy/changeset/2628">r2628</a>{.reference
.external})</p>
</li>
<li>
<p>Added <a href="index.html#std-setting-COOKIES_ENABLED">[<code>COOKIES_ENABLED</code>{.xref .std .std-setting .docutils .literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal} setting
(<a href="http://hg.scrapy.org/scrapy/changeset/2625">r2625</a>{.reference
.external})</p>
</li>
<li>
<p>Stats are now dumped to Scrapy log (default value of
<a href="index.html#std-setting-STATS_DUMP">[<code>STATS_DUMP</code>{.xref .std .std-setting .docutils .literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal} setting has been changed to
[<code>True</code>{.docutils .literal .notranslate}]{.pre}). This is to make
Scrapy users more aware of Scrapy stats and the data that is
collected there.</p>
</li>
<li>
<p>Added support for dynamically adjusting download delay and maximum
concurrent requests
(<a href="http://hg.scrapy.org/scrapy/changeset/2599">r2599</a>{.reference
.external})</p>
</li>
<li>
<p>Added new DBM HTTP cache storage backend
(<a href="http://hg.scrapy.org/scrapy/changeset/2576">r2576</a>{.reference
.external})</p>
</li>
<li>
<p>Added [<code>listjobs.json</code>{.docutils .literal .notranslate}]{.pre} API
to Scrapyd
(<a href="http://hg.scrapy.org/scrapy/changeset/2571">r2571</a>{.reference
.external})</p>
</li>
<li>
<p>[<code>CsvItemExporter</code>{.docutils .literal .notranslate}]{.pre}: added
[<code>join_multivalued</code>{.docutils .literal .notranslate}]{.pre}
parameter
(<a href="http://hg.scrapy.org/scrapy/changeset/2578">r2578</a>{.reference
.external})</p>
</li>
<li>
<p>Added namespace support to [<code>xmliter_lxml</code>{.docutils .literal
.notranslate}]{.pre}
(<a href="http://hg.scrapy.org/scrapy/changeset/2552">r2552</a>{.reference
.external})</p>
</li>
<li>
<p>Improved cookies middleware by making [<code>COOKIES_DEBUG</code>{.docutils
.literal .notranslate}]{.pre} nicer and documenting it
(<a href="http://hg.scrapy.org/scrapy/changeset/2579">r2579</a>{.reference
.external})</p>
</li>
<li>
<p>Several improvements to Scrapyd and Link extractors
:::</p>
</li>
</ul>
<p>::: {#code-rearranged-and-removed .section}</p>
<h5 id="code-rearranged-and-removedheaderlink"><a class="header" href="#code-rearranged-and-removedheaderlink">Code rearranged and removed<a href="#code-rearranged-and-removed" title="Permalink to this heading">¶</a>{.headerlink}</a></h5>
<ul>
<li></li>
</ul>
<pre><code>Merged item passed and item scraped concepts, as they have often proved confusing in the past. This means: ([r2630](http://hg.scrapy.org/scrapy/changeset/2630){.reference .external})

:   -   original item_scraped signal was removed

    -   original item_passed signal was renamed to item_scraped

    -   old log lines [`Scraped`{.docutils .literal
        .notranslate}]{.pre}` `{.docutils .literal
        .notranslate}[`Item...`{.docutils .literal
        .notranslate}]{.pre} were removed

    -   old log lines [`Passed`{.docutils .literal
        .notranslate}]{.pre}` `{.docutils .literal
        .notranslate}[`Item...`{.docutils .literal
        .notranslate}]{.pre} were renamed to [`Scraped`{.docutils
        .literal .notranslate}]{.pre}` `{.docutils .literal
        .notranslate}[`Item...`{.docutils .literal
        .notranslate}]{.pre} lines and downgraded to
        [`DEBUG`{.docutils .literal .notranslate}]{.pre} level
</code></pre>
<ul>
<li></li>
</ul>
<pre><code>Reduced Scrapy codebase by striping part of Scrapy code into two new libraries:

:   -   [w3lib](https://github.com/scrapy/w3lib){.reference
        .external} (several functions from
        [`scrapy.utils.{http,markup,multipart,response,url}`{.docutils
        .literal .notranslate}]{.pre}, done in
        [r2584](http://hg.scrapy.org/scrapy/changeset/2584){.reference
        .external})

    -   [scrapely](https://github.com/scrapy/scrapely){.reference
        .external} (was [`scrapy.contrib.ibl`{.docutils .literal
        .notranslate}]{.pre}, done in
        [r2586](http://hg.scrapy.org/scrapy/changeset/2586){.reference
        .external})
</code></pre>
<ul>
<li>
<p>Removed unused function:
[<code>scrapy.utils.request.request_info()</code>{.docutils .literal
.notranslate}]{.pre}
(<a href="http://hg.scrapy.org/scrapy/changeset/2577">r2577</a>{.reference
.external})</p>
</li>
<li>
<p>Removed googledir project from [<code>examples/googledir</code>{.docutils
.literal .notranslate}]{.pre}. There's now a new example project
called [<code>dirbot</code>{.docutils .literal .notranslate}]{.pre} available
on GitHub:
<a href="https://github.com/scrapy/dirbot">https://github.com/scrapy/dirbot</a>{.reference
.external}</p>
</li>
<li>
<p>Removed support for default field values in Scrapy items
(<a href="http://hg.scrapy.org/scrapy/changeset/2616">r2616</a>{.reference
.external})</p>
</li>
<li>
<p>Removed experimental crawlspider v2
(<a href="http://hg.scrapy.org/scrapy/changeset/2632">r2632</a>{.reference
.external})</p>
</li>
<li>
<p>Removed scheduler middleware to simplify architecture. Duplicates
filter is now done in the scheduler itself, using the same dupe
filtering class as before ([<code>DUPEFILTER_CLASS</code>{.docutils .literal
.notranslate}]{.pre} setting)
(<a href="http://hg.scrapy.org/scrapy/changeset/2640">r2640</a>{.reference
.external})</p>
</li>
<li>
<p>Removed support for passing urls to [<code>scrapy</code>{.docutils .literal
.notranslate}]{.pre}<code> </code>{.docutils .literal
.notranslate}[<code>crawl</code>{.docutils .literal .notranslate}]{.pre}
command (use [<code>scrapy</code>{.docutils .literal
.notranslate}]{.pre}<code> </code>{.docutils .literal
.notranslate}[<code>parse</code>{.docutils .literal .notranslate}]{.pre}
instead)
(<a href="http://hg.scrapy.org/scrapy/changeset/2704">r2704</a>{.reference
.external})</p>
</li>
<li>
<p>Removed deprecated Execution Queue
(<a href="http://hg.scrapy.org/scrapy/changeset/2704">r2704</a>{.reference
.external})</p>
</li>
<li>
<p>Removed (undocumented) spider context extension (from
scrapy.contrib.spidercontext)
(<a href="http://hg.scrapy.org/scrapy/changeset/2780">r2780</a>{.reference
.external})</p>
</li>
<li>
<p>removed [<code>CONCURRENT_SPIDERS</code>{.docutils .literal
.notranslate}]{.pre} setting (use scrapyd maxproc instead)
(<a href="http://hg.scrapy.org/scrapy/changeset/2789">r2789</a>{.reference
.external})</p>
</li>
<li>
<p>Renamed attributes of core components: downloader.sites -&gt;
downloader.slots, scraper.sites -&gt; scraper.slots
(<a href="http://hg.scrapy.org/scrapy/changeset/2717">r2717</a>{.reference
.external},
<a href="http://hg.scrapy.org/scrapy/changeset/2718">r2718</a>{.reference
.external})</p>
</li>
<li>
<p>Renamed setting [<code>CLOSESPIDER_ITEMPASSED</code>{.docutils .literal
.notranslate}]{.pre} to <a href="index.html#std-setting-CLOSESPIDER_ITEMCOUNT">[<code>CLOSESPIDER_ITEMCOUNT</code>{.xref .std
.std-setting .docutils .literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal}
(<a href="http://hg.scrapy.org/scrapy/changeset/2655">r2655</a>{.reference
.external}). Backward compatibility kept.
:::
:::</p>
</li>
</ul>
<p>::: {#scrapy-0-12 .section}</p>
<h4 id="scrapy-012headerlink"><a class="header" href="#scrapy-012headerlink">Scrapy 0.12<a href="#scrapy-0-12" title="Permalink to this heading">¶</a>{.headerlink}</a></h4>
<p>The numbers like #NNN reference tickets in the old issue tracker (Trac)
which is no longer available.</p>
<p>::: {#new-features-and-improvements .section}</p>
<h5 id="new-features-and-improvementsheaderlink"><a class="header" href="#new-features-and-improvementsheaderlink">New features and improvements<a href="#new-features-and-improvements" title="Permalink to this heading">¶</a>{.headerlink}</a></h5>
<ul>
<li>
<p>Passed item is now sent in the [<code>item</code>{.docutils .literal
.notranslate}]{.pre} argument of the <a href="index.html#std-signal-item_scraped">[<code>item_passed</code>{.xref .std
.std-signal .docutils .literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal} (#273)</p>
</li>
<li>
<p>Added verbose option to [<code>scrapy</code>{.docutils .literal
.notranslate}]{.pre}<code> </code>{.docutils .literal
.notranslate}[<code>version</code>{.docutils .literal .notranslate}]{.pre}
command, useful for bug reports (#298)</p>
</li>
<li>
<p>HTTP cache now stored by default in the project data dir (#279)</p>
</li>
<li>
<p>Added project data storage directory (#276, #277)</p>
</li>
<li>
<p>Documented file structure of Scrapy projects (see command-line tool
doc)</p>
</li>
<li>
<p>New lxml backend for XPath selectors (#147)</p>
</li>
<li>
<p>Per-spider settings (#245)</p>
</li>
<li>
<p>Support exit codes to signal errors in Scrapy commands (#248)</p>
</li>
<li>
<p>Added [<code>-c</code>{.docutils .literal .notranslate}]{.pre} argument to
[<code>scrapy</code>{.docutils .literal .notranslate}]{.pre}<code> </code>{.docutils
.literal .notranslate}[<code>shell</code>{.docutils .literal
.notranslate}]{.pre} command</p>
</li>
<li>
<p>Made [<code>libxml2</code>{.docutils .literal .notranslate}]{.pre} optional
(#260)</p>
</li>
<li>
<p>New [<code>deploy</code>{.docutils .literal .notranslate}]{.pre} command (#261)</p>
</li>
<li>
<p>Added <a href="index.html#std-setting-CLOSESPIDER_PAGECOUNT">[<code>CLOSESPIDER_PAGECOUNT</code>{.xref .std .std-setting .docutils
.literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal} setting (#253)</p>
</li>
<li>
<p>Added <a href="index.html#std-setting-CLOSESPIDER_ERRORCOUNT">[<code>CLOSESPIDER_ERRORCOUNT</code>{.xref .std .std-setting .docutils
.literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal} setting (#254)
:::</p>
</li>
</ul>
<p>::: {#scrapyd-changes .section}</p>
<h5 id="scrapyd-changesheaderlink"><a class="header" href="#scrapyd-changesheaderlink">Scrapyd changes<a href="#scrapyd-changes" title="Permalink to this heading">¶</a>{.headerlink}</a></h5>
<ul>
<li>
<p>Scrapyd now uses one process per spider</p>
</li>
<li>
<p>It stores one log file per spider run, and rotate them keeping the
latest 5 logs per spider (by default)</p>
</li>
<li>
<p>A minimal web ui was added, available at
<a href="http://localhost:6800">http://localhost:6800</a>{.reference .external}
by default</p>
</li>
<li>
<p>There is now a [<code>scrapy</code>{.docutils .literal
.notranslate}]{.pre}<code> </code>{.docutils .literal
.notranslate}[<code>server</code>{.docutils .literal .notranslate}]{.pre}
command to start a Scrapyd server of the current project
:::</p>
</li>
</ul>
<p>::: {#changes-to-settings .section}</p>
<h5 id="changes-to-settingsheaderlink"><a class="header" href="#changes-to-settingsheaderlink">Changes to settings<a href="#changes-to-settings" title="Permalink to this heading">¶</a>{.headerlink}</a></h5>
<ul>
<li>
<p>added [<code>HTTPCACHE_ENABLED</code>{.docutils .literal .notranslate}]{.pre}
setting (False by default) to enable HTTP cache middleware</p>
</li>
<li>
<p>changed [<code>HTTPCACHE_EXPIRATION_SECS</code>{.docutils .literal
.notranslate}]{.pre} semantics: now zero means &quot;never expire&quot;.
:::</p>
</li>
</ul>
<p>::: {#deprecated-obsoleted-functionality .section}</p>
<h5 id="deprecatedobsoleted-functionalityheaderlink"><a class="header" href="#deprecatedobsoleted-functionalityheaderlink">Deprecated/obsoleted functionality<a href="#deprecated-obsoleted-functionality" title="Permalink to this heading">¶</a>{.headerlink}</a></h5>
<ul>
<li>
<p>Deprecated [<code>runserver</code>{.docutils .literal .notranslate}]{.pre}
command in favor of [<code>server</code>{.docutils .literal
.notranslate}]{.pre} command which starts a Scrapyd server. See
also: Scrapyd changes</p>
</li>
<li>
<p>Deprecated [<code>queue</code>{.docutils .literal .notranslate}]{.pre} command
in favor of using Scrapyd [<code>schedule.json</code>{.docutils .literal
.notranslate}]{.pre} API. See also: Scrapyd changes</p>
</li>
<li>
<p>Removed the !LxmlItemLoader (experimental contrib which never
graduated to main contrib)
:::
:::</p>
</li>
</ul>
<p>::: {#scrapy-0-10 .section}</p>
<h4 id="scrapy-010headerlink"><a class="header" href="#scrapy-010headerlink">Scrapy 0.10<a href="#scrapy-0-10" title="Permalink to this heading">¶</a>{.headerlink}</a></h4>
<p>The numbers like #NNN reference tickets in the old issue tracker (Trac)
which is no longer available.</p>
<p>::: {#id133 .section}</p>
<h5 id="new-features-and-improvementsheaderlink-1"><a class="header" href="#new-features-and-improvementsheaderlink-1">New features and improvements<a href="#id133" title="Permalink to this heading">¶</a>{.headerlink}</a></h5>
<ul>
<li>
<p>New Scrapy service called [<code>scrapyd</code>{.docutils .literal
.notranslate}]{.pre} for deploying Scrapy crawlers in production
(#218) (documentation available)</p>
</li>
<li>
<p>Simplified Images pipeline usage which doesn't require subclassing
your own images pipeline now (#217)</p>
</li>
<li>
<p>Scrapy shell now shows the Scrapy log by default (#206)</p>
</li>
<li>
<p>Refactored execution queue in a common base code and pluggable
backends called &quot;spider queues&quot; (#220)</p>
</li>
<li>
<p>New persistent spider queue (based on SQLite) (#198), available by
default, which allows to start Scrapy in server mode and then
schedule spiders to run.</p>
</li>
<li>
<p>Added documentation for Scrapy command-line tool and all its
available sub-commands. (documentation available)</p>
</li>
<li>
<p>Feed exporters with pluggable backends (#197) (documentation
available)</p>
</li>
<li>
<p>Deferred signals (#193)</p>
</li>
<li>
<p>Added two new methods to item pipeline open_spider(), close_spider()
with deferred support (#195)</p>
</li>
<li>
<p>Support for overriding default request headers per spider (#181)</p>
</li>
<li>
<p>Replaced default Spider Manager with one with similar functionality
but not depending on Twisted Plugins (#186)</p>
</li>
<li>
<p>Split Debian package into two packages - the library and the service
(#187)</p>
</li>
<li>
<p>Scrapy log refactoring (#188)</p>
</li>
<li>
<p>New extension for keeping persistent spider contexts among different
runs (#203)</p>
</li>
<li>
<p>Added [<code>dont_redirect</code>{.docutils .literal .notranslate}]{.pre}
request.meta key for avoiding redirects (#233)</p>
</li>
<li>
<p>Added [<code>dont_retry</code>{.docutils .literal .notranslate}]{.pre}
request.meta key for avoiding retries (#234)
:::</p>
</li>
</ul>
<p>::: {#command-line-tool-changes .section}</p>
<h5 id="command-line-tool-changesheaderlink"><a class="header" href="#command-line-tool-changesheaderlink">Command-line tool changes<a href="#command-line-tool-changes" title="Permalink to this heading">¶</a>{.headerlink}</a></h5>
<ul>
<li>
<p>New [<code>scrapy</code>{.docutils .literal .notranslate}]{.pre} command which
replaces the old [<code>scrapy-ctl.py</code>{.docutils .literal
.notranslate}]{.pre} (#199) - there is only one global
[<code>scrapy</code>{.docutils .literal .notranslate}]{.pre} command now,
instead of one [<code>scrapy-ctl.py</code>{.docutils .literal
.notranslate}]{.pre} per project - Added [<code>scrapy.bat</code>{.docutils
.literal .notranslate}]{.pre} script for running more conveniently
from Windows</p>
</li>
<li>
<p>Added bash completion to command-line tool (#210)</p>
</li>
<li>
<p>Renamed command [<code>start</code>{.docutils .literal .notranslate}]{.pre} to
[<code>runserver</code>{.docutils .literal .notranslate}]{.pre} (#209)
:::</p>
</li>
</ul>
<p>::: {#api-changes .section}</p>
<h5 id="api-changesheaderlink"><a class="header" href="#api-changesheaderlink">API changes<a href="#api-changes" title="Permalink to this heading">¶</a>{.headerlink}</a></h5>
<ul>
<li>
<p>[<code>url</code>{.docutils .literal .notranslate}]{.pre} and [<code>body</code>{.docutils
.literal .notranslate}]{.pre} attributes of Request objects are now
read-only (#230)</p>
</li>
<li>
<p>[<code>Request.copy()</code>{.docutils .literal .notranslate}]{.pre} and
[<code>Request.replace()</code>{.docutils .literal .notranslate}]{.pre} now
also copies their [<code>callback</code>{.docutils .literal
.notranslate}]{.pre} and [<code>errback</code>{.docutils .literal
.notranslate}]{.pre} attributes (#231)</p>
</li>
<li>
<p>Removed [<code>UrlFilterMiddleware</code>{.docutils .literal
.notranslate}]{.pre} from [<code>scrapy.contrib</code>{.docutils .literal
.notranslate}]{.pre} (already disabled by default)</p>
</li>
<li>
<p>Offsite middleware doesn't filter out any request coming from a
spider that doesn't have a allowed_domains attribute (#225)</p>
</li>
<li>
<p>Removed Spider Manager [<code>load()</code>{.docutils .literal
.notranslate}]{.pre} method. Now spiders are loaded in the
[<code>__init__</code>{.docutils .literal .notranslate}]{.pre} method itself.</p>
</li>
<li></li>
</ul>
<pre><code>Changes to Scrapy Manager (now called &quot;Crawler&quot;):

:   -   [`scrapy.core.manager.ScrapyManager`{.docutils .literal
        .notranslate}]{.pre} class renamed to
        [`scrapy.crawler.Crawler`{.docutils .literal
        .notranslate}]{.pre}

    -   [`scrapy.core.manager.scrapymanager`{.docutils .literal
        .notranslate}]{.pre} singleton moved to
        [`scrapy.project.crawler`{.docutils .literal
        .notranslate}]{.pre}
</code></pre>
<ul>
<li>
<p>Moved module: [<code>scrapy.contrib.spidermanager</code>{.docutils .literal
.notranslate}]{.pre} to [<code>scrapy.spidermanager</code>{.docutils .literal
.notranslate}]{.pre}</p>
</li>
<li>
<p>Spider Manager singleton moved from
[<code>scrapy.spider.spiders</code>{.docutils .literal .notranslate}]{.pre} to
the [<code>spiders`</code>{.docutils .literal
.notranslate}]{.pre}<code> </code>{.docutils .literal
.notranslate}[<code>attribute</code>{.docutils .literal
.notranslate}]{.pre}<code> </code>{.docutils .literal
.notranslate}[<code>of</code>{.docutils .literal
.notranslate}]{.pre}<code> </code>{.docutils .literal
.notranslate}[<code>``scrapy.project.crawler</code>{.docutils .literal
.notranslate}]{.pre} singleton.</p>
</li>
<li></li>
</ul>
<pre><code>moved Stats Collector classes: (#204)

:   -   [`scrapy.stats.collector.StatsCollector`{.docutils .literal
        .notranslate}]{.pre} to
        [`scrapy.statscol.StatsCollector`{.docutils .literal
        .notranslate}]{.pre}

    -   [`scrapy.stats.collector.SimpledbStatsCollector`{.docutils
        .literal .notranslate}]{.pre} to
        [`scrapy.contrib.statscol.SimpledbStatsCollector`{.docutils
        .literal .notranslate}]{.pre}
</code></pre>
<ul>
<li>
<p>default per-command settings are now specified in the
[<code>default_settings</code>{.docutils .literal .notranslate}]{.pre}
attribute of command object class (#201)</p>
</li>
<li></li>
</ul>
<pre><code>changed arguments of Item pipeline [`process_item()`{.docutils .literal .notranslate}]{.pre} method from [`(spider,`{.docutils .literal .notranslate}]{.pre}` `{.docutils .literal .notranslate}[`item)`{.docutils .literal .notranslate}]{.pre} to [`(item,`{.docutils .literal .notranslate}]{.pre}` `{.docutils .literal .notranslate}[`spider)`{.docutils .literal .notranslate}]{.pre}

:   -   backward compatibility kept (with deprecation warning)
</code></pre>
<ul>
<li></li>
</ul>
<pre><code>moved [`scrapy.core.signals`{.docutils .literal .notranslate}]{.pre} module to [`scrapy.signals`{.docutils .literal .notranslate}]{.pre}

:   -   backward compatibility kept (with deprecation warning)
</code></pre>
<ul>
<li></li>
</ul>
<pre><code>moved [`scrapy.core.exceptions`{.docutils .literal .notranslate}]{.pre} module to [`scrapy.exceptions`{.docutils .literal .notranslate}]{.pre}

:   -   backward compatibility kept (with deprecation warning)
</code></pre>
<ul>
<li>
<p>added [<code>handles_request()</code>{.docutils .literal .notranslate}]{.pre}
class method to [<code>BaseSpider</code>{.docutils .literal
.notranslate}]{.pre}</p>
</li>
<li>
<p>dropped [<code>scrapy.log.exc()</code>{.docutils .literal .notranslate}]{.pre}
function (use [<code>scrapy.log.err()</code>{.docutils .literal
.notranslate}]{.pre} instead)</p>
</li>
<li>
<p>dropped [<code>component</code>{.docutils .literal .notranslate}]{.pre}
argument of [<code>scrapy.log.msg()</code>{.docutils .literal
.notranslate}]{.pre} function</p>
</li>
<li>
<p>dropped [<code>scrapy.log.log_level</code>{.docutils .literal
.notranslate}]{.pre} attribute</p>
</li>
<li>
<p>Added [<code>from_settings()</code>{.docutils .literal .notranslate}]{.pre}
class methods to Spider Manager, and Item Pipeline Manager
:::</p>
</li>
</ul>
<p>::: {#id134 .section}</p>
<h5 id="changes-to-settingsheaderlink-1"><a class="header" href="#changes-to-settingsheaderlink-1">Changes to settings<a href="#id134" title="Permalink to this heading">¶</a>{.headerlink}</a></h5>
<ul>
<li>
<p>Added [<code>HTTPCACHE_IGNORE_SCHEMES</code>{.docutils .literal
.notranslate}]{.pre} setting to ignore certain schemes on
!HttpCacheMiddleware (#225)</p>
</li>
<li>
<p>Added [<code>SPIDER_QUEUE_CLASS</code>{.docutils .literal .notranslate}]{.pre}
setting which defines the spider queue to use (#220)</p>
</li>
<li>
<p>Added [<code>KEEP_ALIVE</code>{.docutils .literal .notranslate}]{.pre} setting
(#220)</p>
</li>
<li>
<p>Removed [<code>SERVICE_QUEUE</code>{.docutils .literal .notranslate}]{.pre}
setting (#220)</p>
</li>
<li>
<p>Removed [<code>COMMANDS_SETTINGS_MODULE</code>{.docutils .literal
.notranslate}]{.pre} setting (#201)</p>
</li>
<li>
<p>Renamed [<code>REQUEST_HANDLERS</code>{.docutils .literal .notranslate}]{.pre}
to [<code>DOWNLOAD_HANDLERS</code>{.docutils .literal .notranslate}]{.pre} and
make download handlers classes (instead of functions)
:::
:::</p>
</li>
</ul>
<p>::: {#scrapy-0-9 .section}</p>
<h4 id="scrapy-09headerlink"><a class="header" href="#scrapy-09headerlink">Scrapy 0.9<a href="#scrapy-0-9" title="Permalink to this heading">¶</a>{.headerlink}</a></h4>
<p>The numbers like #NNN reference tickets in the old issue tracker (Trac)
which is no longer available.</p>
<p>::: {#id135 .section}</p>
<h5 id="new-features-and-improvementsheaderlink-2"><a class="header" href="#new-features-and-improvementsheaderlink-2">New features and improvements<a href="#id135" title="Permalink to this heading">¶</a>{.headerlink}</a></h5>
<ul>
<li>
<p>Added SMTP-AUTH support to scrapy.mail</p>
</li>
<li>
<p>New settings added: [<code>MAIL_USER</code>{.docutils .literal
.notranslate}]{.pre}, [<code>MAIL_PASS</code>{.docutils .literal
.notranslate}]{.pre}
(<a href="http://hg.scrapy.org/scrapy/changeset/2065">r2065</a>{.reference
.external} | #149)</p>
</li>
<li>
<p>Added new scrapy-ctl view command - To view URL in the browser, as
seen by Scrapy
(<a href="http://hg.scrapy.org/scrapy/changeset/2039">r2039</a>{.reference
.external})</p>
</li>
<li>
<p>Added web service for controlling Scrapy process (this also
deprecates the web console.
(<a href="http://hg.scrapy.org/scrapy/changeset/2053">r2053</a>{.reference
.external} | #167)</p>
</li>
<li>
<p>Support for running Scrapy as a service, for production systems
(<a href="http://hg.scrapy.org/scrapy/changeset/1988">r1988</a>{.reference
.external},
<a href="http://hg.scrapy.org/scrapy/changeset/2054">r2054</a>{.reference
.external},
<a href="http://hg.scrapy.org/scrapy/changeset/2055">r2055</a>{.reference
.external},
<a href="http://hg.scrapy.org/scrapy/changeset/2056">r2056</a>{.reference
.external},
<a href="http://hg.scrapy.org/scrapy/changeset/2057">r2057</a>{.reference
.external} | #168)</p>
</li>
<li>
<p>Added wrapper induction library (documentation only available in
source code for now).
(<a href="http://hg.scrapy.org/scrapy/changeset/2011">r2011</a>{.reference
.external})</p>
</li>
<li>
<p>Simplified and improved response encoding support
(<a href="http://hg.scrapy.org/scrapy/changeset/1961">r1961</a>{.reference
.external},
<a href="http://hg.scrapy.org/scrapy/changeset/1969">r1969</a>{.reference
.external})</p>
</li>
<li>
<p>Added [<code>LOG_ENCODING</code>{.docutils .literal .notranslate}]{.pre}
setting
(<a href="http://hg.scrapy.org/scrapy/changeset/1956">r1956</a>{.reference
.external}, documentation available)</p>
</li>
<li>
<p>Added [<code>RANDOMIZE_DOWNLOAD_DELAY</code>{.docutils .literal
.notranslate}]{.pre} setting (enabled by default)
(<a href="http://hg.scrapy.org/scrapy/changeset/1923">r1923</a>{.reference
.external}, doc available)</p>
</li>
<li>
<p>[<code>MailSender</code>{.docutils .literal .notranslate}]{.pre} is no longer
IO-blocking
(<a href="http://hg.scrapy.org/scrapy/changeset/1955">r1955</a>{.reference
.external} | #146)</p>
</li>
<li>
<p>Linkextractors and new Crawlspider now handle relative base tag urls
(<a href="http://hg.scrapy.org/scrapy/changeset/1960">r1960</a>{.reference
.external} | #148)</p>
</li>
<li>
<p>Several improvements to Item Loaders and processors
(<a href="http://hg.scrapy.org/scrapy/changeset/2022">r2022</a>{.reference
.external},
<a href="http://hg.scrapy.org/scrapy/changeset/2023">r2023</a>{.reference
.external},
<a href="http://hg.scrapy.org/scrapy/changeset/2024">r2024</a>{.reference
.external},
<a href="http://hg.scrapy.org/scrapy/changeset/2025">r2025</a>{.reference
.external},
<a href="http://hg.scrapy.org/scrapy/changeset/2026">r2026</a>{.reference
.external},
<a href="http://hg.scrapy.org/scrapy/changeset/2027">r2027</a>{.reference
.external},
<a href="http://hg.scrapy.org/scrapy/changeset/2028">r2028</a>{.reference
.external},
<a href="http://hg.scrapy.org/scrapy/changeset/2029">r2029</a>{.reference
.external},
<a href="http://hg.scrapy.org/scrapy/changeset/2030">r2030</a>{.reference
.external})</p>
</li>
<li>
<p>Added support for adding variables to telnet console
(<a href="http://hg.scrapy.org/scrapy/changeset/2047">r2047</a>{.reference
.external} | #165)</p>
</li>
<li>
<p>Support for requests without callbacks
(<a href="http://hg.scrapy.org/scrapy/changeset/2050">r2050</a>{.reference
.external} | #166)
:::</p>
</li>
</ul>
<p>::: {#id136 .section}</p>
<h5 id="api-changesheaderlink-1"><a class="header" href="#api-changesheaderlink-1">API changes<a href="#id136" title="Permalink to this heading">¶</a>{.headerlink}</a></h5>
<ul>
<li>
<p>Change [<code>Spider.domain_name</code>{.docutils .literal .notranslate}]{.pre}
to [<code>Spider.name</code>{.docutils .literal .notranslate}]{.pre} (SEP-012,
<a href="http://hg.scrapy.org/scrapy/changeset/1975">r1975</a>{.reference
.external})</p>
</li>
<li>
<p>[<code>Response.encoding</code>{.docutils .literal .notranslate}]{.pre} is now
the detected encoding
(<a href="http://hg.scrapy.org/scrapy/changeset/1961">r1961</a>{.reference
.external})</p>
</li>
<li>
<p>[<code>HttpErrorMiddleware</code>{.docutils .literal .notranslate}]{.pre} now
returns None or raises an exception
(<a href="http://hg.scrapy.org/scrapy/changeset/2006">r2006</a>{.reference
.external} | #157)</p>
</li>
<li>
<p>[<code>scrapy.command</code>{.docutils .literal .notranslate}]{.pre} modules
relocation
(<a href="http://hg.scrapy.org/scrapy/changeset/2035">r2035</a>{.reference
.external},
<a href="http://hg.scrapy.org/scrapy/changeset/2036">r2036</a>{.reference
.external},
<a href="http://hg.scrapy.org/scrapy/changeset/2037">r2037</a>{.reference
.external})</p>
</li>
<li>
<p>Added [<code>ExecutionQueue</code>{.docutils .literal .notranslate}]{.pre} for
feeding spiders to scrape
(<a href="http://hg.scrapy.org/scrapy/changeset/2034">r2034</a>{.reference
.external})</p>
</li>
<li>
<p>Removed [<code>ExecutionEngine</code>{.docutils .literal .notranslate}]{.pre}
singleton
(<a href="http://hg.scrapy.org/scrapy/changeset/2039">r2039</a>{.reference
.external})</p>
</li>
<li>
<p>Ported [<code>S3ImagesStore</code>{.docutils .literal .notranslate}]{.pre}
(images pipeline) to use boto and threads
(<a href="http://hg.scrapy.org/scrapy/changeset/2033">r2033</a>{.reference
.external})</p>
</li>
<li>
<p>Moved module: [<code>scrapy.management.telnet</code>{.docutils .literal
.notranslate}]{.pre} to [<code>scrapy.telnet</code>{.docutils .literal
.notranslate}]{.pre}
(<a href="http://hg.scrapy.org/scrapy/changeset/2047">r2047</a>{.reference
.external})
:::</p>
</li>
</ul>
<p>::: {#changes-to-default-settings .section}</p>
<h5 id="changes-to-default-settingsheaderlink"><a class="header" href="#changes-to-default-settingsheaderlink">Changes to default settings<a href="#changes-to-default-settings" title="Permalink to this heading">¶</a>{.headerlink}</a></h5>
<ul>
<li>Changed default [<code>SCHEDULER_ORDER</code>{.docutils .literal
.notranslate}]{.pre} to [<code>DFO</code>{.docutils .literal
.notranslate}]{.pre}
(<a href="http://hg.scrapy.org/scrapy/changeset/1939">r1939</a>{.reference
.external})
:::
:::</li>
</ul>
<p>::: {#scrapy-0-8 .section}</p>
<h4 id="scrapy-08headerlink"><a class="header" href="#scrapy-08headerlink">Scrapy 0.8<a href="#scrapy-0-8" title="Permalink to this heading">¶</a>{.headerlink}</a></h4>
<p>The numbers like #NNN reference tickets in the old issue tracker (Trac)
which is no longer available.</p>
<p>::: {#id137 .section}</p>
<h5 id="new-featuresheaderlink-21"><a class="header" href="#new-featuresheaderlink-21">New features<a href="#id137" title="Permalink to this heading">¶</a>{.headerlink}</a></h5>
<ul>
<li>
<p>Added DEFAULT_RESPONSE_ENCODING setting
(<a href="http://hg.scrapy.org/scrapy/changeset/1809">r1809</a>{.reference
.external})</p>
</li>
<li>
<p>Added [<code>dont_click</code>{.docutils .literal .notranslate}]{.pre} argument
to [<code>FormRequest.from_response()</code>{.docutils .literal
.notranslate}]{.pre} method
(<a href="http://hg.scrapy.org/scrapy/changeset/1813">r1813</a>{.reference
.external},
<a href="http://hg.scrapy.org/scrapy/changeset/1816">r1816</a>{.reference
.external})</p>
</li>
<li>
<p>Added [<code>clickdata</code>{.docutils .literal .notranslate}]{.pre} argument
to [<code>FormRequest.from_response()</code>{.docutils .literal
.notranslate}]{.pre} method
(<a href="http://hg.scrapy.org/scrapy/changeset/1802">r1802</a>{.reference
.external},
<a href="http://hg.scrapy.org/scrapy/changeset/1803">r1803</a>{.reference
.external})</p>
</li>
<li>
<p>Added support for HTTP proxies ([<code>HttpProxyMiddleware</code>{.docutils
.literal .notranslate}]{.pre})
(<a href="http://hg.scrapy.org/scrapy/changeset/1781">r1781</a>{.reference
.external},
<a href="http://hg.scrapy.org/scrapy/changeset/1785">r1785</a>{.reference
.external})</p>
</li>
<li>
<p>Offsite spider middleware now logs messages when filtering out
requests
(<a href="http://hg.scrapy.org/scrapy/changeset/1841">r1841</a>{.reference
.external})
:::</p>
</li>
</ul>
<p>::: {#id138 .section}</p>
<h5 id="backward-incompatible-changesheaderlink-10"><a class="header" href="#backward-incompatible-changesheaderlink-10">Backward-incompatible changes<a href="#id138" title="Permalink to this heading">¶</a>{.headerlink}</a></h5>
<ul>
<li>
<p>Changed [<code>scrapy.utils.response.get_meta_refresh()</code>{.docutils
.literal .notranslate}]{.pre} signature
(<a href="http://hg.scrapy.org/scrapy/changeset/1804">r1804</a>{.reference
.external})</p>
</li>
<li>
<p>Removed deprecated [<code>scrapy.item.ScrapedItem</code>{.docutils .literal
.notranslate}]{.pre} class - use [<code>scrapy.item.Item</code>{.docutils
.literal .notranslate}]{.pre}<code> </code>{.docutils .literal
.notranslate}[<code>instead</code>{.docutils .literal .notranslate}]{.pre}
(<a href="http://hg.scrapy.org/scrapy/changeset/1838">r1838</a>{.reference
.external})</p>
</li>
<li>
<p>Removed deprecated [<code>scrapy.xpath</code>{.docutils .literal
.notranslate}]{.pre} module - use [<code>scrapy.selector</code>{.docutils
.literal .notranslate}]{.pre} instead.
(<a href="http://hg.scrapy.org/scrapy/changeset/1836">r1836</a>{.reference
.external})</p>
</li>
<li>
<p>Removed deprecated [<code>core.signals.domain_open</code>{.docutils .literal
.notranslate}]{.pre} signal - use
[<code>core.signals.domain_opened</code>{.docutils .literal
.notranslate}]{.pre} instead
(<a href="http://hg.scrapy.org/scrapy/changeset/1822">r1822</a>{.reference
.external})</p>
</li>
<li></li>
</ul>
<pre><code>[`log.msg()`{.docutils .literal .notranslate}]{.pre} now receives a [`spider`{.docutils .literal .notranslate}]{.pre} argument ([r1822](http://hg.scrapy.org/scrapy/changeset/1822){.reference .external})

:   -   Old domain argument has been deprecated and will be removed
        in 0.9. For spiders, you should always use the
        [`spider`{.docutils .literal .notranslate}]{.pre} argument
        and pass spider references. If you really want to pass a
        string, use the [`component`{.docutils .literal
        .notranslate}]{.pre} argument instead.
</code></pre>
<ul>
<li>
<p>Changed core signals [<code>domain_opened</code>{.docutils .literal
.notranslate}]{.pre}, [<code>domain_closed</code>{.docutils .literal
.notranslate}]{.pre}, [<code>domain_idle</code>{.docutils .literal
.notranslate}]{.pre}</p>
</li>
<li></li>
</ul>
<pre><code>Changed Item pipeline to use spiders instead of domains

:   -   The [`domain`{.docutils .literal .notranslate}]{.pre}
        argument of [`process_item()`{.docutils .literal
        .notranslate}]{.pre} item pipeline method was changed to
        [`spider`{.docutils .literal .notranslate}]{.pre}, the new
        signature is: [`process_item(spider,`{.docutils .literal
        .notranslate}]{.pre}` `{.docutils .literal
        .notranslate}[`item)`{.docutils .literal
        .notranslate}]{.pre}
        ([r1827](http://hg.scrapy.org/scrapy/changeset/1827){.reference
        .external} \| #105)

    -   To quickly port your code (to work with Scrapy 0.8) just use
        [`spider.domain_name`{.docutils .literal
        .notranslate}]{.pre} where you previously used
        [`domain`{.docutils .literal .notranslate}]{.pre}.
</code></pre>
<ul>
<li></li>
</ul>
<pre><code>Changed Stats API to use spiders instead of domains ([r1849](http://hg.scrapy.org/scrapy/changeset/1849){.reference .external} \| #113)

:   -   [`StatsCollector`{.docutils .literal .notranslate}]{.pre}
        was changed to receive spider references (instead of
        domains) in its methods ([`set_value`{.docutils .literal
        .notranslate}]{.pre}, [`inc_value`{.docutils .literal
        .notranslate}]{.pre}, etc).

    -   added [`StatsCollector.iter_spider_stats()`{.docutils
        .literal .notranslate}]{.pre} method

    -   removed [`StatsCollector.list_domains()`{.docutils .literal
        .notranslate}]{.pre} method

    -   Also, Stats signals were renamed and now pass around spider
        references (instead of domains). Here's a summary of the
        changes:

    -   To quickly port your code (to work with Scrapy 0.8) just use
        [`spider.domain_name`{.docutils .literal
        .notranslate}]{.pre} where you previously used
        [`domain`{.docutils .literal .notranslate}]{.pre}.
        [`spider_stats`{.docutils .literal .notranslate}]{.pre}
        contains exactly the same data as [`domain_stats`{.docutils
        .literal .notranslate}]{.pre}.
</code></pre>
<ul>
<li></li>
</ul>
<pre><code>[`CloseDomain`{.docutils .literal .notranslate}]{.pre} extension moved to [`scrapy.contrib.closespider.CloseSpider`{.docutils .literal .notranslate}]{.pre} ([r1833](http://hg.scrapy.org/scrapy/changeset/1833){.reference .external})

:   -   

        Its settings were also renamed:

        :   -   [`CLOSEDOMAIN_TIMEOUT`{.docutils .literal
                .notranslate}]{.pre} to
                [`CLOSESPIDER_TIMEOUT`{.docutils .literal
                .notranslate}]{.pre}

            -   [`CLOSEDOMAIN_ITEMCOUNT`{.docutils .literal
                .notranslate}]{.pre} to
                [`CLOSESPIDER_ITEMCOUNT`{.docutils .literal
                .notranslate}]{.pre}
</code></pre>
<ul>
<li>
<p>Removed deprecated [<code>SCRAPYSETTINGS_MODULE</code>{.docutils .literal
.notranslate}]{.pre} environment variable - use
[<code>SCRAPY_SETTINGS_MODULE</code>{.docutils .literal .notranslate}]{.pre}
instead
(<a href="http://hg.scrapy.org/scrapy/changeset/1840">r1840</a>{.reference
.external})</p>
</li>
<li>
<p>Renamed setting: [<code>REQUESTS_PER_DOMAIN</code>{.docutils .literal
.notranslate}]{.pre} to [<code>CONCURRENT_REQUESTS_PER_SPIDER</code>{.docutils
.literal .notranslate}]{.pre}
(<a href="http://hg.scrapy.org/scrapy/changeset/1830">r1830</a>{.reference
.external},
<a href="http://hg.scrapy.org/scrapy/changeset/1844">r1844</a>{.reference
.external})</p>
</li>
<li>
<p>Renamed setting: [<code>CONCURRENT_DOMAINS</code>{.docutils .literal
.notranslate}]{.pre} to [<code>CONCURRENT_SPIDERS</code>{.docutils .literal
.notranslate}]{.pre}
(<a href="http://hg.scrapy.org/scrapy/changeset/1830">r1830</a>{.reference
.external})</p>
</li>
<li>
<p>Refactored HTTP Cache middleware</p>
</li>
<li>
<p>HTTP Cache middleware has been heavily refactored, retaining the
same functionality except for the domain sectorization which was
removed.
(<a href="http://hg.scrapy.org/scrapy/changeset/1843">r1843</a>{.reference
.external} )</p>
</li>
<li>
<p>Renamed exception: [<code>DontCloseDomain</code>{.docutils .literal
.notranslate}]{.pre} to [<code>DontCloseSpider</code>{.docutils .literal
.notranslate}]{.pre}
(<a href="http://hg.scrapy.org/scrapy/changeset/1859">r1859</a>{.reference
.external} | #120)</p>
</li>
<li>
<p>Renamed extension: [<code>DelayedCloseDomain</code>{.docutils .literal
.notranslate}]{.pre} to [<code>SpiderCloseDelay</code>{.docutils .literal
.notranslate}]{.pre}
(<a href="http://hg.scrapy.org/scrapy/changeset/1861">r1861</a>{.reference
.external} | #121)</p>
</li>
<li>
<p>Removed obsolete
[<code>scrapy.utils.markup.remove_escape_chars</code>{.docutils .literal
.notranslate}]{.pre} function - use
[<code>scrapy.utils.markup.replace_escape_chars</code>{.docutils .literal
.notranslate}]{.pre} instead
(<a href="http://hg.scrapy.org/scrapy/changeset/1865">r1865</a>{.reference
.external})
:::
:::</p>
</li>
</ul>
<p>::: {#scrapy-0-7 .section}</p>
<h4 id="scrapy-07headerlink"><a class="header" href="#scrapy-07headerlink">Scrapy 0.7<a href="#scrapy-0-7" title="Permalink to this heading">¶</a>{.headerlink}</a></h4>
<p>First release of Scrapy.
:::
:::</p>
<p>[]{#document-contributing}</p>
<p>::: {#contributing-to-scrapy .section}
[]{#topics-contributing}</p>
<h3 id="contributing-to-scrapyheaderlink"><a class="header" href="#contributing-to-scrapyheaderlink">Contributing to Scrapy<a href="#contributing-to-scrapy" title="Permalink to this heading">¶</a>{.headerlink}</a></h3>
<p>::: {.admonition .important}
Important</p>
<p>Double check that you are reading the most recent version of this
document at
<a href="https://docs.scrapy.org/en/master/contributing.html">https://docs.scrapy.org/en/master/contributing.html</a>{.reference
.external}
:::</p>
<p>There are many ways to contribute to Scrapy. Here are some of them:</p>
<ul>
<li>
<p>Report bugs and request features in the <a href="https://github.com/scrapy/scrapy/issues">issue
tracker</a>{.reference
.external}, trying to follow the guidelines detailed in <a href="#reporting-bugs">Reporting
bugs</a>{.reference .internal} below.</p>
</li>
<li>
<p>Submit patches for new functionalities and/or bug fixes. Please read
<a href="#writing-patches">[Writing patches]{.std .std-ref}</a>{.hoverxref
.tooltip .reference .internal} and <a href="#id2">Submitting
patches</a>{.reference .internal} below for details on how to
write and submit a patch.</p>
</li>
<li>
<p>Blog about Scrapy. Tell the world how you're using Scrapy. This will
help newcomers with more examples and will help the Scrapy project
to increase its visibility.</p>
</li>
<li>
<p>Join the <a href="https://reddit.com/r/scrapy">Scrapy subreddit</a>{.reference
.external} and share your ideas on how to improve Scrapy. We're
always open to suggestions.</p>
</li>
<li>
<p>Answer Scrapy questions at <a href="https://stackoverflow.com/questions/tagged/scrapy">Stack
Overflow</a>{.reference
.external}.</p>
</li>
</ul>
<p>::: {#reporting-bugs .section}</p>
<h4 id="reporting-bugsheaderlink"><a class="header" href="#reporting-bugsheaderlink">Reporting bugs<a href="#reporting-bugs" title="Permalink to this heading">¶</a>{.headerlink}</a></h4>
<p>::: {.admonition .note}
Note</p>
<p>Please report security issues <strong>only</strong> to
<a href="mailto:scrapy-security%40googlegroups.com">scrapy-security@googlegroups.com</a>{.reference
.external}. This is a private list only open to trusted Scrapy
developers, and its archives are not public.
:::</p>
<p>Well-written bug reports are very helpful, so keep in mind the following
guidelines when you're going to report a new bug.</p>
<ul>
<li>
<p>check the <a href="index.html#faq">[FAQ]{.std .std-ref}</a>{.hoverxref .tooltip
.reference .internal} first to see if your issue is addressed in a
well-known question</p>
</li>
<li>
<p>if you have a general question about Scrapy usage, please ask it at
<a href="https://stackoverflow.com/questions/tagged/scrapy">Stack
Overflow</a>{.reference
.external} (use &quot;scrapy&quot; tag).</p>
</li>
<li>
<p>check the <a href="https://github.com/scrapy/scrapy/issues">open
issues</a>{.reference
.external} to see if the issue has already been reported. If it has,
don't dismiss the report, but check the ticket history and comments.
If you have additional useful information, please leave a comment,
or consider <a href="#writing-patches">[sending a pull request]{.std
.std-ref}</a>{.hoverxref .tooltip .reference
.internal} with a fix.</p>
</li>
<li>
<p>search the
<a href="https://groups.google.com/forum/#!forum/scrapy-users">scrapy-users</a>{.reference
.external} list and <a href="https://reddit.com/r/scrapy">Scrapy
subreddit</a>{.reference .external} to see
if it has been discussed there, or if you're not sure if what you're
seeing is a bug. You can also ask in the [<code>#scrapy</code>{.docutils
.literal .notranslate}]{.pre} IRC channel.</p>
</li>
<li>
<p>write <strong>complete, reproducible, specific bug reports</strong>. The smaller
the test case, the better. Remember that other developers won't have
your project to reproduce the bug, so please include all relevant
files required to reproduce it. See for example StackOverflow's
guide on creating a <a href="https://stackoverflow.com/help/mcve">Minimal, Complete, and Verifiable
example</a>{.reference .external}
exhibiting the issue.</p>
</li>
<li>
<p>the most awesome way to provide a complete reproducible example is
to send a pull request which adds a failing test case to the Scrapy
testing suite (see <a href="#submitting-patches">[Submitting patches]{.std
.std-ref}</a>{.hoverxref .tooltip .reference
.internal}). This is helpful even if you don't have an intention to
fix the issue yourselves.</p>
</li>
<li>
<p>include the output of [<code>scrapy</code>{.docutils .literal
.notranslate}]{.pre}<code> </code>{.docutils .literal
.notranslate}[<code>version</code>{.docutils .literal
.notranslate}]{.pre}<code> </code>{.docutils .literal
.notranslate}[<code>-v</code>{.docutils .literal .notranslate}]{.pre} so
developers working on your bug know exactly which version and
platform it occurred on, which is often very helpful for reproducing
it, or knowing if it was already fixed.
:::</p>
</li>
</ul>
<p>::: {#writing-patches .section}
[]{#id1}</p>
<h4 id="writing-patchesheaderlink"><a class="header" href="#writing-patchesheaderlink">Writing patches<a href="#writing-patches" title="Permalink to this heading">¶</a>{.headerlink}</a></h4>
<p>Scrapy has a list of <a href="https://github.com/scrapy/scrapy/issues?q=is%3Aissue+is%3Aopen+label%3A%22good+first+issue%22">good first
issues</a>{.reference
.external} and <a href="https://github.com/scrapy/scrapy/issues?q=is%3Aissue+is%3Aopen+label%3A%22help+wanted%22">help wanted
issues</a>{.reference
.external} that you can work on. These issues are a great way to get
started with contributing to Scrapy. If you're new to the codebase, you
may want to focus on documentation or testing-related issues, as they
are always useful and can help you get more familiar with the project.
You can also check Scrapy's <a href="https://app.codecov.io/gh/scrapy/scrapy">test
coverage</a>{.reference .external}
to see which areas may benefit from more tests.</p>
<p>The better a patch is written, the higher the chances that it'll get
accepted and the sooner it will be merged.</p>
<p>Well-written patches should:</p>
<ul>
<li>
<p>contain the minimum amount of code required for the specific change.
Small patches are easier to review and merge. So, if you're doing
more than one change (or bug fix), please consider submitting one
patch per change. Do not collapse multiple changes into a single
patch. For big changes consider using a patch queue.</p>
</li>
<li>
<p>pass all unit-tests. See <a href="#id6">Running tests</a>{.reference .internal}
below.</p>
</li>
<li>
<p>include one (or more) test cases that check the bug fixed or the new
functionality added. See <a href="#writing-tests">Writing tests</a>{.reference
.internal} below.</p>
</li>
<li>
<p>if you're adding or changing a public (documented) API, please
include the documentation changes in the same patch. See
<a href="#id5">Documentation policies</a>{.reference .internal} below.</p>
</li>
<li>
<p>if you're adding a private API, please add a regular expression to
the [<code>coverage_ignore_pyobjects</code>{.docutils .literal
.notranslate}]{.pre} variable of [<code>docs/conf.py</code>{.docutils .literal
.notranslate}]{.pre} to exclude the new private API from
documentation coverage checks.</p>
<p>To see if your private API is skipped properly, generate a
documentation coverage report as follows:</p>
<p>::: {.highlight-default .notranslate}
::: highlight
tox -e docs-coverage
:::
:::</p>
</li>
<li>
<p>if you are removing deprecated code, first make sure that at least 1
year (12 months) has passed since the release that introduced the
deprecation. See <a href="index.html#deprecation-policy">[Deprecation policy]{.std
.std-ref}</a>{.hoverxref .tooltip
.reference .internal}.
:::</p>
</li>
</ul>
<p>::: {#submitting-patches .section}
[]{#id2}</p>
<h4 id="submitting-patchesheaderlink"><a class="header" href="#submitting-patchesheaderlink">Submitting patches<a href="#submitting-patches" title="Permalink to this heading">¶</a>{.headerlink}</a></h4>
<p>The best way to submit a patch is to issue a <a href="https://help.github.com/en/github/collaborating-with-issues-and-pull-requests/creating-a-pull-request">pull
request</a>{.reference
.external} on GitHub, optionally creating a new issue first.</p>
<p>Remember to explain what was fixed or the new functionality (what it is,
why it's needed, etc). The more info you include, the easier will be for
core developers to understand and accept your patch.</p>
<p>You can also discuss the new functionality (or bug fix) before creating
the patch, but it's always good to have a patch ready to illustrate your
arguments and show that you have put some additional thought into the
subject. A good starting point is to send a pull request on GitHub. It
can be simple enough to illustrate your idea, and leave
documentation/tests for later, after the idea has been validated and
proven useful. Alternatively, you can start a conversation in the
<a href="https://reddit.com/r/scrapy">Scrapy subreddit</a>{.reference .external} to
discuss your idea first.</p>
<p>Sometimes there is an existing pull request for the problem you'd like
to solve, which is stalled for some reason. Often the pull request is in
a right direction, but changes are requested by Scrapy maintainers, and
the original pull request author hasn't had time to address them. In
this case consider picking up this pull request: open a new pull request
with all commits from the original pull request, as well as additional
changes to address the raised issues. Doing so helps a lot; it is not
considered rude as long as the original author is acknowledged by
keeping his/her commits.</p>
<p>You can pull an existing pull request to a local branch by running
[<code>git</code>{.docutils .literal .notranslate}]{.pre}<code> </code>{.docutils .literal
.notranslate}[<code>fetch</code>{.docutils .literal
.notranslate}]{.pre}<code> </code>{.docutils .literal
.notranslate}[<code>upstream</code>{.docutils .literal
.notranslate}]{.pre}<code> </code>{.docutils .literal
.notranslate}[<code>pull/$PR_NUMBER/head:$BRANCH_NAME_TO_CREATE</code>{.docutils
.literal .notranslate}]{.pre} (replace 'upstream' with a remote name for
scrapy repository, [<code>$PR_NUMBER</code>{.docutils .literal .notranslate}]{.pre}
with an ID of the pull request, and [<code>$BRANCH_NAME_TO_CREATE</code>{.docutils
.literal .notranslate}]{.pre} with a name of the branch you want to
create locally). See also:
<a href="https://help.github.com/en/github/collaborating-with-issues-and-pull-requests/checking-out-pull-requests-locally#modifying-an-inactive-pull-request-locally">https://help.github.com/en/github/collaborating-with-issues-and-pull-requests/checking-out-pull-requests-locally#modifying-an-inactive-pull-request-locally</a>{.reference
.external}.</p>
<p>When writing GitHub pull requests, try to keep titles short but
descriptive. E.g. For bug #411: &quot;Scrapy hangs if an exception raises in
start_requests&quot; prefer &quot;Fix hanging when exception occurs in
start_requests (#411)&quot; instead of &quot;Fix for #411&quot;. Complete titles make
it easy to skim through the issue tracker.</p>
<p>Finally, try to keep aesthetic changes ([]{#index-0 .target}<a href="https://peps.python.org/pep-0008/"><strong>PEP
8</strong></a>{.pep .reference .external}
compliance, unused imports removal, etc) in separate commits from
functional changes. This will make pull requests easier to review and
more likely to get merged.
:::</p>
<p>::: {#coding-style .section}
[]{#id3}</p>
<h4 id="coding-styleheaderlink"><a class="header" href="#coding-styleheaderlink">Coding style<a href="#coding-style" title="Permalink to this heading">¶</a>{.headerlink}</a></h4>
<p>Please follow these coding conventions when writing code for inclusion
in Scrapy:</p>
<ul>
<li>
<p>We use <a href="https://black.readthedocs.io/en/stable/">black</a>{.reference
.external} for code formatting. There is a hook in the pre-commit
config that will automatically format your code before every commit.
You can also run black manually with [<code>tox</code>{.docutils .literal
.notranslate}]{.pre}<code> </code>{.docutils .literal
.notranslate}[<code>-e</code>{.docutils .literal
.notranslate}]{.pre}<code> </code>{.docutils .literal
.notranslate}[<code>black</code>{.docutils .literal .notranslate}]{.pre}.</p>
</li>
<li>
<p>Don't put your name in the code you contribute; git provides enough
metadata to identify author of the code. See
<a href="https://help.github.com/en/github/using-git/setting-your-username-in-git">https://help.github.com/en/github/using-git/setting-your-username-in-git</a>{.reference
.external} for setup instructions.
:::</p>
</li>
</ul>
<p>::: {#pre-commit .section}
[]{#scrapy-pre-commit}</p>
<h4 id="pre-commitheaderlink"><a class="header" href="#pre-commitheaderlink">Pre-commit<a href="#pre-commit" title="Permalink to this heading">¶</a>{.headerlink}</a></h4>
<p>We use <a href="https://pre-commit.com/">pre-commit</a>{.reference .external} to
automatically address simple code issues before every commit.</p>
<p>After your create a local clone of your fork of the Scrapy repository:</p>
<ol>
<li>
<p><a href="https://pre-commit.com/#installation">Install
pre-commit</a>{.reference
.external}.</p>
</li>
<li>
<p>On the root of your local clone of the Scrapy repository, run the
following command:</p>
<p>::: {.highlight-bash .notranslate}
::: highlight
pre-commit install
:::
:::</p>
</li>
</ol>
<p>Now pre-commit will check your changes every time you create a Git
commit. Upon finding issues, pre-commit aborts your commit, and either
fixes those issues automatically, or only reports them to you. If it
fixes those issues automatically, creating your commit again should
succeed. Otherwise, you may need to address the corresponding issues
manually first.
:::</p>
<p>::: {#documentation-policies .section}
[]{#id5}</p>
<h4 id="documentation-policiesheaderlink"><a class="header" href="#documentation-policiesheaderlink">Documentation policies<a href="#documentation-policies" title="Permalink to this heading">¶</a>{.headerlink}</a></h4>
<p>For reference documentation of API members (classes, methods, etc.) use
docstrings and make sure that the Sphinx documentation uses the
<a href="https://www.sphinx-doc.org/en/master/usage/extensions/autodoc.html#module-sphinx.ext.autodoc" title="(in Sphinx v7.3.0)">[<code>autodoc</code>{.xref .py .py-mod .docutils .literal
.notranslate}]{.pre}</a>{.reference
.external} extension to pull the docstrings. API reference documentation
should follow docstring conventions (<a href="https://www.python.org/dev/peps/pep-0257/">PEP
257</a>{.reference .external})
and be IDE-friendly: short, to the point, and it may provide short
examples.</p>
<p>Other types of documentation, such as tutorials or topics, should be
covered in files within the [<code>docs/</code>{.docutils .literal
.notranslate}]{.pre} directory. This includes documentation that is
specific to an API member, but goes beyond API reference documentation.</p>
<p>In any case, if something is covered in a docstring, use the
<a href="https://www.sphinx-doc.org/en/master/usage/extensions/autodoc.html#module-sphinx.ext.autodoc" title="(in Sphinx v7.3.0)">[<code>autodoc</code>{.xref .py .py-mod .docutils .literal
.notranslate}]{.pre}</a>{.reference
.external} extension to pull the docstring into the documentation
instead of duplicating the docstring in files within the
[<code>docs/</code>{.docutils .literal .notranslate}]{.pre} directory.</p>
<p>Documentation updates that cover new or modified features must use
Sphinx's <a href="https://www.sphinx-doc.org/en/master/usage/restructuredtext/directives.html#directive-versionadded" title="(in Sphinx v7.3.0)">[<code>versionadded</code>{.xref .rst .rst-dir .docutils .literal
.notranslate}]{.pre}</a>{.reference
.external} and <a href="https://www.sphinx-doc.org/en/master/usage/restructuredtext/directives.html#directive-versionchanged" title="(in Sphinx v7.3.0)">[<code>versionchanged</code>{.xref .rst .rst-dir .docutils .literal
.notranslate}]{.pre}</a>{.reference
.external} directives. Use [<code>VERSION</code>{.docutils .literal
.notranslate}]{.pre} as version, we will replace it with the actual
version right before the corresponding release. When we release a new
major or minor version of Scrapy, we remove these directives if they are
older than 3 years.</p>
<p>Documentation about deprecated features must be removed as those
features are deprecated, so that new readers do not run into it. New
deprecations and deprecation removals are documented in the <a href="index.html#news">[release
notes]{.std .std-ref}</a>{.hoverxref .tooltip .reference
.internal}.
:::</p>
<p>::: {#tests .section}</p>
<h4 id="testsheaderlink-1"><a class="header" href="#testsheaderlink-1">Tests<a href="#tests" title="Permalink to this heading">¶</a>{.headerlink}</a></h4>
<p>Tests are implemented using the <a href="https://docs.twisted.org/en/stable/development/test-standard.html" title="(in Twisted v23.10)">[Twisted unit-testing framework]{.xref
.std
.std-doc}</a>{.reference
.external}. Running tests requires <a href="https://tox.wiki/en/latest/index.html" title="(in Python v4.11)">[tox]{.xref .std
.std-doc}</a>{.reference
.external}.</p>
<p>::: {#running-tests .section}
[]{#id6}</p>
<h5 id="running-testsheaderlink"><a class="header" href="#running-testsheaderlink">Running tests<a href="#running-tests" title="Permalink to this heading">¶</a>{.headerlink}</a></h5>
<p>To run all tests:</p>
<p>::: {.highlight-default .notranslate}
::: highlight
tox
:::
:::</p>
<p>To run a specific test (say [<code>tests/test_loader.py</code>{.docutils .literal
.notranslate}]{.pre}) use:</p>
<blockquote>
<div>
<p>[<code>tox</code>{.docutils .literal .notranslate}]{.pre}<code> </code>{.docutils .literal
.notranslate}[<code>--</code>{.docutils .literal
.notranslate}]{.pre}<code> </code>{.docutils .literal
.notranslate}[<code>tests/test_loader.py</code>{.docutils .literal
.notranslate}]{.pre}</p>
</div>
</blockquote>
<p>To run the tests on a specific <a href="https://tox.wiki/en/latest/index.html" title="(in Python v4.11)">[tox]{.xref .std
.std-doc}</a>{.reference
.external} environment, use [<code>-e</code>{.docutils .literal
.notranslate}]{.pre}<code> </code>{.docutils .literal
.notranslate}[<code>&lt;name&gt;</code>{.docutils .literal .notranslate}]{.pre} with an
environment name from [<code>tox.ini</code>{.docutils .literal
.notranslate}]{.pre}. For example, to run the tests with Python 3.10
use:</p>
<p>::: {.highlight-default .notranslate}
::: highlight
tox -e py310
:::
:::</p>
<p>You can also specify a comma-separated list of environments, and use
<a href="https://tox.wiki/en/latest/user_guide.html#parallel-mode" title="(in Python v4.11)">[tox's parallel mode]{.xref .std
.std-ref}</a>{.reference
.external} to run the tests on multiple environments in parallel:</p>
<p>::: {.highlight-default .notranslate}
::: highlight
tox -e py39,py310 -p auto
:::
:::</p>
<p>To pass command-line options to <a href="https://docs.pytest.org/en/latest/index.html" title="(in pytest v0.1.dev83+g81c06b3)">[pytest]{.xref .std
.std-doc}</a>{.reference
.external}, add them after [<code>--</code>{.docutils .literal .notranslate}]{.pre}
in your call to <a href="https://tox.wiki/en/latest/index.html" title="(in Python v4.11)">[tox]{.xref .std
.std-doc}</a>{.reference
.external}. Using [<code>--</code>{.docutils .literal .notranslate}]{.pre}
overrides the default positional arguments defined in
[<code>tox.ini</code>{.docutils .literal .notranslate}]{.pre}, so you must include
those default positional arguments ([<code>scrapy</code>{.docutils .literal
.notranslate}]{.pre}<code> </code>{.docutils .literal
.notranslate}[<code>tests</code>{.docutils .literal .notranslate}]{.pre}) after
[<code>--</code>{.docutils .literal .notranslate}]{.pre} as well:</p>
<p>::: {.highlight-default .notranslate}
::: highlight
tox -- scrapy tests -x  # stop after first failure
:::
:::</p>
<p>You can also use the
<a href="https://github.com/pytest-dev/pytest-xdist">pytest-xdist</a>{.reference
.external} plugin. For example, to run all tests on the Python 3.10
<a href="https://tox.wiki/en/latest/index.html" title="(in Python v4.11)">[tox]{.xref .std
.std-doc}</a>{.reference
.external} environment using all your CPU cores:</p>
<p>::: {.highlight-default .notranslate}
::: highlight
tox -e py310 -- scrapy tests -n auto
:::
:::</p>
<p>To see coverage report install <a href="https://coverage.readthedocs.io/en/latest/index.html" title="(in Coverage.py v7.3.2)">[coverage]{.xref .std
.std-doc}</a>{.reference
.external} ([<code>pip</code>{.docutils .literal .notranslate}]{.pre}<code> </code>{.docutils
.literal .notranslate}[<code>install</code>{.docutils .literal
.notranslate}]{.pre}<code> </code>{.docutils .literal
.notranslate}[<code>coverage</code>{.docutils .literal .notranslate}]{.pre}) and
run:</p>
<blockquote>
<div>
<p>[<code>coverage</code>{.docutils .literal .notranslate}]{.pre}<code> </code>{.docutils
.literal .notranslate}[<code>report</code>{.docutils .literal
.notranslate}]{.pre}</p>
</div>
</blockquote>
<p>see output of [<code>coverage</code>{.docutils .literal
.notranslate}]{.pre}<code> </code>{.docutils .literal
.notranslate}[<code>--help</code>{.docutils .literal .notranslate}]{.pre} for more
options like html or xml report.
:::</p>
<p>::: {#writing-tests .section}</p>
<h5 id="writing-testsheaderlink"><a class="header" href="#writing-testsheaderlink">Writing tests<a href="#writing-tests" title="Permalink to this heading">¶</a>{.headerlink}</a></h5>
<p>All functionality (including new features and bug fixes) must include a
test case to check that it works as expected, so please include tests
for your patches if you want them to get accepted sooner.</p>
<p>Scrapy uses unit-tests, which are located in the
<a href="https://github.com/scrapy/scrapy/tree/master/tests">tests/</a>{.reference
.external} directory. Their module name typically resembles the full
path of the module they're testing. For example, the item loaders code
is in:</p>
<p>::: {.highlight-default .notranslate}
::: highlight
scrapy.loader
:::
:::</p>
<p>And their unit-tests are in:</p>
<p>::: {.highlight-default .notranslate}
::: highlight
tests/test_loader.py
:::
:::
:::
:::
:::</p>
<p>[]{#document-versioning}</p>
<p>::: {#versioning-and-api-stability .section}
[]{#versioning}</p>
<h3 id="versioning-and-api-stabilityheaderlink"><a class="header" href="#versioning-and-api-stabilityheaderlink">Versioning and API stability<a href="#versioning-and-api-stability" title="Permalink to this heading">¶</a>{.headerlink}</a></h3>
<p>::: {#id1 .section}</p>
<h4 id="versioningheaderlink"><a class="header" href="#versioningheaderlink">Versioning<a href="#id1" title="Permalink to this heading">¶</a>{.headerlink}</a></h4>
<p>There are 3 numbers in a Scrapy version: <em>A.B.C</em></p>
<ul>
<li>
<p><em>A</em> is the major version. This will rarely change and will signify
very large changes.</p>
</li>
<li>
<p><em>B</em> is the release number. This will include many changes including
features and things that possibly break backward compatibility,
although we strive to keep these cases at a minimum.</p>
</li>
<li>
<p><em>C</em> is the bugfix release number.</p>
</li>
</ul>
<p>Backward-incompatibilities are explicitly mentioned in the <a href="index.html#news">[release
notes]{.std .std-ref}</a>{.hoverxref .tooltip .reference
.internal}, and may require special attention before upgrading.</p>
<p>Development releases do not follow 3-numbers version and are generally
released as [<code>dev</code>{.docutils .literal .notranslate}]{.pre} suffixed
versions, e.g. [<code>1.3dev</code>{.docutils .literal .notranslate}]{.pre}.</p>
<p>::: {.admonition .note}
Note</p>
<p>With Scrapy 0.* series, Scrapy used <a href="https://en.wikipedia.org/wiki/Software_versioning#Odd-numbered_versions_for_development_releases">odd-numbered versions for
development
releases</a>{.reference
.external}. This is not the case anymore from Scrapy 1.0 onwards.</p>
<p>Starting with Scrapy 1.0, all releases should be considered
production-ready.
:::</p>
<p>For example:</p>
<ul>
<li><em>1.1.1</em> is the first bugfix release of the <em>1.1</em> series (safe to use
in production)
:::</li>
</ul>
<p>::: {#api-stability .section}</p>
<h4 id="api-stabilityheaderlink"><a class="header" href="#api-stabilityheaderlink">API stability<a href="#api-stability" title="Permalink to this heading">¶</a>{.headerlink}</a></h4>
<p>API stability was one of the major goals for the <em>1.0</em> release.</p>
<p>Methods or functions that start with a single dash ([<code>_</code>{.docutils
.literal .notranslate}]{.pre}) are private and should never be relied as
stable.</p>
<p>Also, keep in mind that stable doesn't mean complete: stable APIs could
grow new methods or functionality but the existing methods should keep
working the same way.
:::</p>
<p>::: {#deprecation-policy .section}
[]{#id2}</p>
<h4 id="deprecation-policyheaderlink"><a class="header" href="#deprecation-policyheaderlink">Deprecation policy<a href="#deprecation-policy" title="Permalink to this heading">¶</a>{.headerlink}</a></h4>
<p>We aim to maintain support for deprecated Scrapy features for at least 1
year.</p>
<p>For example, if a feature is deprecated in a Scrapy version released on
June 15th 2020, that feature should continue to work in versions
released on June 14th 2021 or before that.</p>
<p>Any new Scrapy release after a year <em>may</em> remove support for that
deprecated feature.</p>
<p>All deprecated features removed in a Scrapy release are explicitly
mentioned in the <a href="index.html#news">[release notes]{.std
.std-ref}</a>{.hoverxref .tooltip .reference .internal}.
:::
:::
:::</p>
<p><a href="index.html#document-news">[Release notes]{.doc}</a>{.reference .internal}</p>
<p>:   See what has changed in recent Scrapy versions.</p>
<p><a href="index.html#document-contributing">[Contributing to Scrapy]{.doc}</a>{.reference .internal}</p>
<p>:   Learn how to contribute to the Scrapy project.</p>
<p><a href="index.html#document-versioning">[Versioning and API stability]{.doc}</a>{.reference .internal}</p>
<p>:   Understand Scrapy versioning and API stability.
:::
:::
:::
:::</p>
<hr />
<p>::: {role=&quot;contentinfo&quot;}
© Copyright 2008--2023, Scrapy developers. [Revision <code>70ba3a08</code>.
]{.commit} [Last updated on Nov 30, 2023. ]{.lastupdated}
:::</p>
<p>Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
<a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a> provided by
<a href="https://readthedocs.org">Read the Docs</a>.
:::
:::
:::
:::</p>
<p>::: {.rst-versions toggle=&quot;rst-versions&quot; role=&quot;note&quot; aria-label=&quot;Versions&quot;}
[ [ Read the Docs]{.fa .fa-book} v: master []{.fa .fa-caret-down}
]{.rst-current-version toggle=&quot;rst-current-version&quot;}</p>
<p>::: rst-other-versions</p>
<p>Versions
:   <a href="/en/master/">master</a>
:   <a href="/en/latest/">latest</a>
:   <a href="/en/stable/">stable</a>
:   <a href="/en/2.11/">2.11</a>
:   <a href="/en/2.10/">2.10</a>
:   <a href="/en/2.9/">2.9</a>
:   <a href="/en/2.8/">2.8</a>
:   <a href="/en/2.7/">2.7</a>
:   <a href="/en/2.6/">2.6</a>
:   <a href="/en/2.5/">2.5</a>
:   <a href="/en/2.4/">2.4</a>
:   <a href="/en/2.3/">2.3</a>
:   <a href="/en/2.2/">2.2</a>
:   <a href="/en/2.1/">2.1</a>
:   <a href="/en/2.0/">2.0</a>
:   <a href="/en/1.8/">1.8</a>
:   <a href="/en/1.7/">1.7</a>
:   <a href="/en/1.6/">1.6</a>
:   <a href="/en/1.5/">1.5</a>
:   <a href="/en/1.4/">1.4</a>
:   <a href="/en/1.3/">1.3</a>
:   <a href="/en/1.2/">1.2</a>
:   <a href="/en/1.1/">1.1</a>
:   <a href="/en/1.0/">1.0</a>
:   <a href="/en/0.24/">0.24</a>
:   <a href="/en/0.22/">0.22</a>
:   <a href="/en/0.20/">0.20</a>
:   <a href="/en/0.18/">0.18</a>
:   <a href="/en/0.16/">0.16</a>
:   <a href="/en/0.14/">0.14</a>
:   <a href="/en/0.12/">0.12</a>
:   <a href="/en/0.10.3/">0.10.3</a>
:   <a href="/en/0.9/">0.9</a>
:   <a href="/en/xpath-tutorial/">xpath-tutorial</a></p>
<pre><code class="language-{=html}">&lt;!-- --&gt;
</code></pre>
<p>Downloads
:   <a href="//docs.scrapy.org/_/downloads/en/master/pdf/">pdf</a>
:   <a href="//docs.scrapy.org/_/downloads/en/master/htmlzip/">html</a>
:   <a href="//docs.scrapy.org/_/downloads/en/master/epub/">epub</a></p>
<pre><code class="language-{=html}">&lt;!-- --&gt;
</code></pre>
<p>On Read the Docs
:   <a href="//readthedocs.org/projects/scrapy/?fromdocs=scrapy">Project Home</a>
:   <a href="//readthedocs.org/builds/scrapy/?fromdocs=scrapy">Builds</a>
:::
:::</p>

                    </main>

                    <nav class="nav-wrapper" aria-label="Page navigation">
                        <!-- Mobile navigation buttons -->

                            <a rel="next" href="../Scrapy/Scrapy1.html" class="mobile-nav-chapters next" title="Next chapter" aria-label="Next chapter" aria-keyshortcuts="Right">
                                <i class="fa fa-angle-right"></i>
                            </a>

                        <div style="clear: both"></div>
                    </nav>
                </div>
            </div>

            <nav class="nav-wide-wrapper" aria-label="Page navigation">

                    <a rel="next" href="../Scrapy/Scrapy1.html" class="nav-chapters next" title="Next chapter" aria-label="Next chapter" aria-keyshortcuts="Right">
                        <i class="fa fa-angle-right"></i>
                    </a>
            </nav>

        </div>




        <script>
            window.playground_copyable = true;
        </script>



        <script src="../clipboard.min.js"></script>
        <script src="../highlight.js"></script>
        <script src="../book.js"></script>

        <!-- Custom JS scripts -->


    </div>
    </body>
</html>
