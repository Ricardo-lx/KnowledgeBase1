<!DOCTYPE HTML>
<html lang="en" class="sidebar-visible no-js light">
    <head>
        <!-- Book generated using mdBook -->
        <meta charset="UTF-8">
        <title>Scrapy10</title>


        <!-- Custom HTML head -->
        
        <meta name="description" content="">
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <meta name="theme-color" content="#ffffff" />

        <link rel="icon" href="../favicon.svg">
        <link rel="shortcut icon" href="../favicon.png">
        <link rel="stylesheet" href="../css/variables.css">
        <link rel="stylesheet" href="../css/general.css">
        <link rel="stylesheet" href="../css/chrome.css">
        <link rel="stylesheet" href="../css/print.css" media="print">

        <!-- Fonts -->
        <link rel="stylesheet" href="../FontAwesome/css/font-awesome.css">
        <link rel="stylesheet" href="../fonts/fonts.css">

        <!-- Highlight.js Stylesheets -->
        <link rel="stylesheet" href="../highlight.css">
        <link rel="stylesheet" href="../tomorrow-night.css">
        <link rel="stylesheet" href="../ayu-highlight.css">

        <!-- Custom theme stylesheets -->

    </head>
    <body>
    <div id="body-container">
        <!-- Provide site root to javascript -->
        <script>
            var path_to_root = "../";
            var default_theme = window.matchMedia("(prefers-color-scheme: dark)").matches ? "navy" : "light";
        </script>

        <!-- Work around some values being stored in localStorage wrapped in quotes -->
        <script>
            try {
                var theme = localStorage.getItem('mdbook-theme');
                var sidebar = localStorage.getItem('mdbook-sidebar');

                if (theme.startsWith('"') && theme.endsWith('"')) {
                    localStorage.setItem('mdbook-theme', theme.slice(1, theme.length - 1));
                }

                if (sidebar.startsWith('"') && sidebar.endsWith('"')) {
                    localStorage.setItem('mdbook-sidebar', sidebar.slice(1, sidebar.length - 1));
                }
            } catch (e) { }
        </script>

        <!-- Set the theme before any content is loaded, prevents flash -->
        <script>
            var theme;
            try { theme = localStorage.getItem('mdbook-theme'); } catch(e) { }
            if (theme === null || theme === undefined) { theme = default_theme; }
            var html = document.querySelector('html');
            html.classList.remove('no-js')
            html.classList.remove('light')
            html.classList.add(theme);
            html.classList.add('js');
        </script>

        <!-- Hide / unhide sidebar before it is displayed -->
        <script>
            var html = document.querySelector('html');
            var sidebar = null;
            if (document.body.clientWidth >= 1080) {
                try { sidebar = localStorage.getItem('mdbook-sidebar'); } catch(e) { }
                sidebar = sidebar || 'visible';
            } else {
                sidebar = 'hidden';
            }
            html.classList.remove('sidebar-visible');
            html.classList.add("sidebar-" + sidebar);
        </script>

        <nav id="sidebar" class="sidebar" aria-label="Table of contents">
            <div class="sidebar-scrollbox">
                <ol class="chapter"><li class="chapter-item expanded "><a href="../Scrapy/Scrapy.html"><strong aria-hidden="true">1.</strong> Scrapy(-v master)</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="../Scrapy/Scrapy1.html"><strong aria-hidden="true">1.1.</strong> Scrapy1</a></li><li class="chapter-item expanded "><a href="../Scrapy/Scrapy2.html"><strong aria-hidden="true">1.2.</strong> Scrapy2</a></li><li class="chapter-item expanded "><a href="../Scrapy/Scrapy3.html"><strong aria-hidden="true">1.3.</strong> Scrapy3</a></li><li class="chapter-item expanded "><a href="../Scrapy/Scrapy4.html"><strong aria-hidden="true">1.4.</strong> Scrapy4</a></li><li class="chapter-item expanded "><a href="../Scrapy/Scrapy5.html"><strong aria-hidden="true">1.5.</strong> Scrapy5</a></li><li class="chapter-item expanded "><a href="../Scrapy/Scrapy6.html"><strong aria-hidden="true">1.6.</strong> Scrapy6</a></li><li class="chapter-item expanded "><a href="../Scrapy/Scrapy7.html"><strong aria-hidden="true">1.7.</strong> Scrapy7</a></li><li class="chapter-item expanded "><a href="../Scrapy/Scrapy8.html"><strong aria-hidden="true">1.8.</strong> Scrapy8</a></li><li class="chapter-item expanded "><a href="../Scrapy/Scrapy9.html"><strong aria-hidden="true">1.9.</strong> Scrapy9</a></li><li class="chapter-item expanded "><a href="../Scrapy/Scrapy10.html" class="active"><strong aria-hidden="true">1.10.</strong> Scrapy10</a></li></ol></li></ol>
            </div>
            <div id="sidebar-resize-handle" class="sidebar-resize-handle"></div>
        </nav>

        <div id="page-wrapper" class="page-wrapper">

            <div class="page">
                                <div id="menu-bar-hover-placeholder"></div>
                <div id="menu-bar" class="menu-bar sticky bordered">
                    <div class="left-buttons">
                        <button id="sidebar-toggle" class="icon-button" type="button" title="Toggle Table of Contents" aria-label="Toggle Table of Contents" aria-controls="sidebar">
                            <i class="fa fa-bars"></i>
                        </button>
                        <button id="theme-toggle" class="icon-button" type="button" title="Change theme" aria-label="Change theme" aria-haspopup="true" aria-expanded="false" aria-controls="theme-list">
                            <i class="fa fa-paint-brush"></i>
                        </button>
                        <ul id="theme-list" class="theme-popup" aria-label="Themes" role="menu">
                            <li role="none"><button role="menuitem" class="theme" id="light">Light</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="rust">Rust</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="coal">Coal</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="navy">Navy</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="ayu">Ayu</button></li>
                        </ul>
                        <button id="search-toggle" class="icon-button" type="button" title="Search. (Shortkey: s)" aria-label="Toggle Searchbar" aria-expanded="false" aria-keyshortcuts="S" aria-controls="searchbar">
                            <i class="fa fa-search"></i>
                        </button>
                    </div>

                    <h1 class="menu-title"></h1>

                    <div class="right-buttons">
                        <a href="../print.html" title="Print this book" aria-label="Print this book">
                            <i id="print-button" class="fa fa-print"></i>
                        </a>

                    </div>
                </div>

                <div id="search-wrapper" class="hidden">
                    <form id="searchbar-outer" class="searchbar-outer">
                        <input type="search" id="searchbar" name="searchbar" placeholder="Search this book ..." aria-controls="searchresults-outer" aria-describedby="searchresults-header">
                    </form>
                    <div id="searchresults-outer" class="searchresults-outer hidden">
                        <div id="searchresults-header" class="searchresults-header"></div>
                        <ul id="searchresults">
                        </ul>
                    </div>
                </div>

                <!-- Apply ARIA attributes after the sidebar and the sidebar toggle button are added to the DOM -->
                <script>
                    document.getElementById('sidebar-toggle').setAttribute('aria-expanded', sidebar === 'visible');
                    document.getElementById('sidebar').setAttribute('aria-hidden', sidebar !== 'visible');
                    Array.from(document.querySelectorAll('#sidebar a')).forEach(function(link) {
                        link.setAttribute('tabIndex', sidebar === 'visible' ? 0 : -1);
                    });
                </script>

                <div id="content" class="content">
                    <main>
                        <pre><code>cc00ad2](https://github.com/scrapy/scrapy/commit/cc00ad2){.reference
.external})
</code></pre>
<ul>
<li>
<p>include tests/ to source distribution in MANIFEST.in (<a href="https://github.com/scrapy/scrapy/commit/eca227e">commit
eca227e</a>{.reference
.external})</p>
</li>
<li>
<p>DOC Fix SelectJmes documentation (<a href="https://github.com/scrapy/scrapy/commit/b8567bc">commit
b8567bc</a>{.reference
.external})</p>
</li>
<li>
<p>DOC Bring Ubuntu and Archlinux outside of Windows subsection
(<a href="https://github.com/scrapy/scrapy/commit/392233f">commit
392233f</a>{.reference
.external})</p>
</li>
<li>
<p>DOC remove version suffix from Ubuntu package (<a href="https://github.com/scrapy/scrapy/commit/5303c66">commit
5303c66</a>{.reference
.external})</p>
</li>
<li>
<p>DOC Update release date for 1.0 (<a href="https://github.com/scrapy/scrapy/commit/c89fa29">commit
c89fa29</a>{.reference
.external})
:::</p>
</li>
</ul>
<p>::: {#scrapy-1-0-0-2015-06-19 .section}
[]{#release-1-0-0}</p>
<h4 id="scrapy-100-2015-06-19headerlink"><a class="header" href="#scrapy-100-2015-06-19headerlink">Scrapy 1.0.0 (2015-06-19)<a href="#scrapy-1-0-0-2015-06-19" title="Permalink to this heading">Â¶</a>{.headerlink}</a></h4>
<p>You will find a lot of new features and bugfixes in this major release.
Make sure to check our updated <a href="index.html#intro-overview">[overview]{.std
.std-ref}</a>{.hoverxref .tooltip .reference
.internal} to get a glance of some of the changes, along with our
brushed <a href="index.html#intro-tutorial">[tutorial]{.std
.std-ref}</a>{.hoverxref .tooltip .reference
.internal}.</p>
<p>::: {#support-for-returning-dictionaries-in-spiders .section}</p>
<h5 id="support-for-returning-dictionaries-in-spidersheaderlink"><a class="header" href="#support-for-returning-dictionaries-in-spidersheaderlink">Support for returning dictionaries in spiders<a href="#support-for-returning-dictionaries-in-spiders" title="Permalink to this heading">Â¶</a>{.headerlink}</a></h5>
<p>Declaring and returning Scrapy Items is no longer necessary to collect
the scraped data from your spider, you can now return explicit
dictionaries instead.</p>
<p><em>Classic version</em></p>
<p>::: {.highlight-default .notranslate}
::: highlight
class MyItem(scrapy.Item):
url = scrapy.Field()</p>
<pre><code>class MySpider(scrapy.Spider):
    def parse(self, response):
        return MyItem(url=response.url)
</code></pre>
<p>:::
:::</p>
<p><em>New version</em></p>
<p>::: {.highlight-default .notranslate}
::: highlight
class MySpider(scrapy.Spider):
def parse(self, response):
return {'url': response.url}
:::
:::
:::</p>
<p>::: {#per-spider-settings-gsoc-2014 .section}</p>
<h5 id="per-spider-settings-gsoc-2014headerlink"><a class="header" href="#per-spider-settings-gsoc-2014headerlink">Per-spider settings (GSoC 2014)<a href="#per-spider-settings-gsoc-2014" title="Permalink to this heading">Â¶</a>{.headerlink}</a></h5>
<p>Last Google Summer of Code project accomplished an important redesign of
the mechanism used for populating settings, introducing explicit
priorities to override any given setting. As an extension of that goal,
we included a new level of priority for settings that act exclusively
for a single spider, allowing them to redefine project settings.</p>
<p>Start using it by defining a [<code>custom_settings</code>{.xref .py .py-attr
.docutils .literal .notranslate}]{.pre} class variable in your spider:</p>
<p>::: {.highlight-default .notranslate}
::: highlight
class MySpider(scrapy.Spider):
custom_settings = {
&quot;DOWNLOAD_DELAY&quot;: 5.0,
&quot;RETRY_ENABLED&quot;: False,
}
:::
:::</p>
<p>Read more about settings population: <a href="index.html#topics-settings">[Settings]{.std
.std-ref}</a>{.hoverxref .tooltip .reference
.internal}
:::</p>
<p>::: {#python-logging .section}</p>
<h5 id="python-loggingheaderlink"><a class="header" href="#python-loggingheaderlink">Python Logging<a href="#python-logging" title="Permalink to this heading">Â¶</a>{.headerlink}</a></h5>
<p>Scrapy 1.0 has moved away from Twisted logging to support Python built
in's as default logging system. We're maintaining backward compatibility
for most of the old custom interface to call logging functions, but
you'll get warnings to switch to the Python logging API entirely.</p>
<p><em>Old version</em></p>
<p>::: {.highlight-default .notranslate}
::: highlight
from scrapy import log
log.msg('MESSAGE', log.INFO)
:::
:::</p>
<p><em>New version</em></p>
<p>::: {.highlight-default .notranslate}
::: highlight
import logging
logging.info('MESSAGE')
:::
:::</p>
<p>Logging with spiders remains the same, but on top of the [<code>log()</code>{.xref
.py .py-meth .docutils .literal .notranslate}]{.pre} method you'll have
access to a custom [<code>logger</code>{.xref .py .py-attr .docutils .literal
.notranslate}]{.pre} created for the spider to issue log events:</p>
<p>::: {.highlight-default .notranslate}
::: highlight
class MySpider(scrapy.Spider):
def parse(self, response):
self.logger.info('Response received')
:::
:::</p>
<p>Read more in the logging documentation: <a href="index.html#topics-logging">[Logging]{.std
.std-ref}</a>{.hoverxref .tooltip .reference
.internal}
:::</p>
<p>::: {#crawler-api-refactoring-gsoc-2014 .section}</p>
<h5 id="crawler-api-refactoring-gsoc-2014headerlink"><a class="header" href="#crawler-api-refactoring-gsoc-2014headerlink">Crawler API refactoring (GSoC 2014)<a href="#crawler-api-refactoring-gsoc-2014" title="Permalink to this heading">Â¶</a>{.headerlink}</a></h5>
<p>Another milestone for last Google Summer of Code was a refactoring of
the internal API, seeking a simpler and easier usage. Check new core
interface in: <a href="index.html#topics-api">[Core API]{.std
.std-ref}</a>{.hoverxref .tooltip .reference
.internal}</p>
<p>A common situation where you will face these changes is while running
Scrapy from scripts. Here's a quick example of how to run a Spider
manually with the new API:</p>
<p>::: {.highlight-default .notranslate}
::: highlight
from scrapy.crawler import CrawlerProcess</p>
<pre><code>process = CrawlerProcess({
    'USER_AGENT': 'Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 5.1)'
})
process.crawl(MySpider)
process.start()
</code></pre>
<p>:::
:::</p>
<p>Bear in mind this feature is still under development and its API may
change until it reaches a stable status.</p>
<p>See more examples for scripts running Scrapy: <a href="index.html#topics-practices">[Common Practices]{.std
.std-ref}</a>{.hoverxref .tooltip .reference
.internal}
:::</p>
<p>::: {#module-relocations .section}
[]{#id128}</p>
<h5 id="module-relocationsheaderlink"><a class="header" href="#module-relocationsheaderlink">Module Relocations<a href="#module-relocations" title="Permalink to this heading">Â¶</a>{.headerlink}</a></h5>
<p>There's been a large rearrangement of modules trying to improve the
general structure of Scrapy. Main changes were separating various
subpackages into new projects and dissolving both
[<code>scrapy.contrib</code>{.docutils .literal .notranslate}]{.pre} and
[<code>scrapy.contrib_exp</code>{.docutils .literal .notranslate}]{.pre} into top
level packages. Backward compatibility was kept among internal
relocations, while importing deprecated modules expect warnings
indicating their new place.</p>
<p>::: {#full-list-of-relocations .section}</p>
<h6 id="full-list-of-relocationsheaderlink"><a class="header" href="#full-list-of-relocationsheaderlink">Full list of relocations<a href="#full-list-of-relocations" title="Permalink to this heading">Â¶</a>{.headerlink}</a></h6>
<p>Outsourced packages</p>
<p>::: {.admonition .note}
Note</p>
<p>These extensions went through some minor changes, e.g. some setting
names were changed. Please check the documentation in each new
repository to get familiar with the new usage.
:::</p>
<p>+-----------------------------------+-----------------------------------+
| Old location                      | New location                      |
+===================================+===================================+
| scrapy.commands.deploy            | [sc                               |
|                                   | rapyd-client](https://github.com/ |
|                                   | scrapy/scrapyd-client){.reference |
|                                   | .external} (See other             |
|                                   | alternatives here: [[Deploying    |
|                                   | Spiders]{.std                     |
|                                   | .std-ref}](ind                    |
|                                   | ex.html#topics-deploy){.hoverxref |
|                                   | .tooltip .reference .internal})   |
+-----------------------------------+-----------------------------------+
| scrapy.contrib.djangoitem         | [scrapy-djangoite                 |
|                                   | m](https://github.com/scrapy-plug |
|                                   | ins/scrapy-djangoitem){.reference |
|                                   | .external}                        |
+-----------------------------------+-----------------------------------+
| scrapy.webservice                 | [scrapy-jso                       |
|                                   | nrpc](https://github.com/scrapy-p |
|                                   | lugins/scrapy-jsonrpc){.reference |
|                                   | .external}                        |
+-----------------------------------+-----------------------------------+</p>
<p>[<code>scrapy.contrib_exp</code>{.docutils .literal .notranslate}]{.pre} and
[<code>scrapy.contrib</code>{.docutils .literal .notranslate}]{.pre} dissolutions</p>
<p>+-----------------------------------+-----------------------------------+
| Old location                      | New location                      |
+===================================+===================================+
| scrapy.contrib_exp.d              | scrapy.do                         |
| ownloadermiddleware.decompression | wnloadermiddlewares.decompression |
+-----------------------------------+-----------------------------------+
| scrapy.contrib_exp.iterators      | scrapy.utils.iterators            |
+-----------------------------------+-----------------------------------+
| sc                                | scrapy.downloadermiddlewares      |
| rapy.contrib.downloadermiddleware |                                   |
+-----------------------------------+-----------------------------------+
| scrapy.contrib.exporter           | scrapy.exporters                  |
+-----------------------------------+-----------------------------------+
| scrapy.contrib.linkextractors     | scrapy.linkextractors             |
+-----------------------------------+-----------------------------------+
| scrapy.contrib.loader             | scrapy.loader                     |
+-----------------------------------+-----------------------------------+
| scrapy.contrib.loader.processor   | scrapy.loader.processors          |
+-----------------------------------+-----------------------------------+
| scrapy.contrib.pipeline           | scrapy.pipelines                  |
+-----------------------------------+-----------------------------------+
| scrapy.contrib.spidermiddleware   | scrapy.spidermiddlewares          |
+-----------------------------------+-----------------------------------+
| scrapy.contrib.spiders            | scrapy.spiders                    |
+-----------------------------------+-----------------------------------+
| -   scrapy.contrib.closespider    | scrapy.extensions.*              |
|                                   |                                   |
| -   scrapy.contrib.corestats      |                                   |
|                                   |                                   |
| -   scrapy.contrib.debug          |                                   |
|                                   |                                   |
| -   scrapy.contrib.feedexport     |                                   |
|                                   |                                   |
| -   scrapy.contrib.httpcache      |                                   |
|                                   |                                   |
| -   scrapy.contrib.logstats       |                                   |
|                                   |                                   |
| -   scrapy.contrib.memdebug       |                                   |
|                                   |                                   |
| -   scrapy.contrib.memusage       |                                   |
|                                   |                                   |
| -   scrapy.contrib.spiderstate    |                                   |
|                                   |                                   |
| -   scrapy.contrib.statsmailer    |                                   |
|                                   |                                   |
| -   scrapy.contrib.throttle       |                                   |
+-----------------------------------+-----------------------------------+</p>
<p>Plural renames and Modules unification</p>
<p>+-----------------------------------+-----------------------------------+
| Old location                      | New location                      |
+===================================+===================================+
| scrapy.command                    | scrapy.commands                   |
+-----------------------------------+-----------------------------------+
| scrapy.dupefilter                 | scrapy.dupefilters                |
+-----------------------------------+-----------------------------------+
| scrapy.linkextractor              | scrapy.linkextractors             |
+-----------------------------------+-----------------------------------+
| scrapy.spider                     | scrapy.spiders                    |
+-----------------------------------+-----------------------------------+
| scrapy.squeue                     | scrapy.squeues                    |
+-----------------------------------+-----------------------------------+
| scrapy.statscol                   | scrapy.statscollectors            |
+-----------------------------------+-----------------------------------+
| scrapy.utils.decorator            | scrapy.utils.decorators           |
+-----------------------------------+-----------------------------------+</p>
<p>Class renames</p>
<p>+-----------------------------------+-----------------------------------+
| Old location                      | New location                      |
+===================================+===================================+
| s                                 | scrapy.spiderloader.SpiderLoader  |
| crapy.spidermanager.SpiderManager |                                   |
+-----------------------------------+-----------------------------------+</p>
<p>Settings renames</p>
<p>+-----------------------------------+-----------------------------------+
| Old location                      | New location                      |
+===================================+===================================+
| SPIDER_MANAGER_CLASS              | SPIDER_LOADER_CLASS               |
+-----------------------------------+-----------------------------------+
:::
:::</p>
<p>::: {#changelog .section}</p>
<h5 id="changelogheaderlink"><a class="header" href="#changelogheaderlink">Changelog<a href="#changelog" title="Permalink to this heading">Â¶</a>{.headerlink}</a></h5>
<p>New Features and Enhancements</p>
<ul>
<li>
<p>Python logging (<a href="https://github.com/scrapy/scrapy/issues/1060">issue
1060</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/1235">issue
1235</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/1236">issue
1236</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/1240">issue
1240</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/1259">issue
1259</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/1278">issue
1278</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/1286">issue
1286</a>{.reference
.external})</p>
</li>
<li>
<p>FEED_EXPORT_FIELDS option (<a href="https://github.com/scrapy/scrapy/issues/1159">issue
1159</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/1224">issue
1224</a>{.reference
.external})</p>
</li>
<li>
<p>Dns cache size and timeout options (<a href="https://github.com/scrapy/scrapy/issues/1132">issue
1132</a>{.reference
.external})</p>
</li>
<li>
<p>support namespace prefix in xmliter_lxml (<a href="https://github.com/scrapy/scrapy/issues/963">issue
963</a>{.reference
.external})</p>
</li>
<li>
<p>Reactor threadpool max size setting (<a href="https://github.com/scrapy/scrapy/issues/1123">issue
1123</a>{.reference
.external})</p>
</li>
<li>
<p>Allow spiders to return dicts. (<a href="https://github.com/scrapy/scrapy/issues/1081">issue
1081</a>{.reference
.external})</p>
</li>
<li>
<p>Add Response.urljoin() helper (<a href="https://github.com/scrapy/scrapy/issues/1086">issue
1086</a>{.reference
.external})</p>
</li>
<li>
<p>look in ~/.config/scrapy.cfg for user config (<a href="https://github.com/scrapy/scrapy/issues/1098">issue
1098</a>{.reference
.external})</p>
</li>
<li>
<p>handle TLS SNI (<a href="https://github.com/scrapy/scrapy/issues/1101">issue
1101</a>{.reference
.external})</p>
</li>
<li>
<p>Selectorlist extract first (<a href="https://github.com/scrapy/scrapy/issues/624">issue
624</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/1145">issue
1145</a>{.reference
.external})</p>
</li>
<li>
<p>Added JmesSelect (<a href="https://github.com/scrapy/scrapy/issues/1016">issue
1016</a>{.reference
.external})</p>
</li>
<li>
<p>add gzip compression to filesystem http cache backend (<a href="https://github.com/scrapy/scrapy/issues/1020">issue
1020</a>{.reference
.external})</p>
</li>
<li>
<p>CSS support in link extractors (<a href="https://github.com/scrapy/scrapy/issues/983">issue
983</a>{.reference
.external})</p>
</li>
<li>
<p>httpcache dont_cache meta #19 #689 (<a href="https://github.com/scrapy/scrapy/issues/821">issue
821</a>{.reference
.external})</p>
</li>
<li>
<p>add signal to be sent when request is dropped by the scheduler
(<a href="https://github.com/scrapy/scrapy/issues/961">issue 961</a>{.reference
.external})</p>
</li>
<li>
<p>avoid download large response (<a href="https://github.com/scrapy/scrapy/issues/946">issue
946</a>{.reference
.external})</p>
</li>
<li>
<p>Allow to specify the quotechar in CSVFeedSpider (<a href="https://github.com/scrapy/scrapy/issues/882">issue
882</a>{.reference
.external})</p>
</li>
<li>
<p>Add referer to &quot;Spider error processing&quot; log message (<a href="https://github.com/scrapy/scrapy/issues/795">issue
795</a>{.reference
.external})</p>
</li>
<li>
<p>process robots.txt once (<a href="https://github.com/scrapy/scrapy/issues/896">issue
896</a>{.reference
.external})</p>
</li>
<li>
<p>GSoC Per-spider settings (<a href="https://github.com/scrapy/scrapy/issues/854">issue
854</a>{.reference
.external})</p>
</li>
<li>
<p>Add project name validation (<a href="https://github.com/scrapy/scrapy/issues/817">issue
817</a>{.reference
.external})</p>
</li>
<li>
<p>GSoC API cleanup (<a href="https://github.com/scrapy/scrapy/issues/816">issue
816</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/1128">issue
1128</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/1147">issue
1147</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/1148">issue
1148</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/1156">issue
1156</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/1185">issue
1185</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/1187">issue
1187</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/1258">issue
1258</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/1268">issue
1268</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/1276">issue
1276</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/1285">issue
1285</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/1284">issue
1284</a>{.reference
.external})</p>
</li>
<li>
<p>Be more responsive with IO operations (<a href="https://github.com/scrapy/scrapy/issues/1074">issue
1074</a>{.reference
.external} and <a href="https://github.com/scrapy/scrapy/issues/1075">issue
1075</a>{.reference
.external})</p>
</li>
<li>
<p>Do leveldb compaction for httpcache on closing (<a href="https://github.com/scrapy/scrapy/issues/1297">issue
1297</a>{.reference
.external})</p>
</li>
</ul>
<p>Deprecations and Removals</p>
<ul>
<li>
<p>Deprecate htmlparser link extractor (<a href="https://github.com/scrapy/scrapy/issues/1205">issue
1205</a>{.reference
.external})</p>
</li>
<li>
<p>remove deprecated code from FeedExporter (<a href="https://github.com/scrapy/scrapy/issues/1155">issue
1155</a>{.reference
.external})</p>
</li>
<li>
<p>a leftover for.15 compatibility (<a href="https://github.com/scrapy/scrapy/issues/925">issue
925</a>{.reference
.external})</p>
</li>
<li>
<p>drop support for CONCURRENT_REQUESTS_PER_SPIDER (<a href="https://github.com/scrapy/scrapy/issues/895">issue
895</a>{.reference
.external})</p>
</li>
<li>
<p>Drop old engine code (<a href="https://github.com/scrapy/scrapy/issues/911">issue
911</a>{.reference
.external})</p>
</li>
<li>
<p>Deprecate SgmlLinkExtractor (<a href="https://github.com/scrapy/scrapy/issues/777">issue
777</a>{.reference
.external})</p>
</li>
</ul>
<p>Relocations</p>
<ul>
<li>
<p>Move exporters/__init__.py to exporters.py (<a href="https://github.com/scrapy/scrapy/issues/1242">issue
1242</a>{.reference
.external})</p>
</li>
<li>
<p>Move base classes to their packages (<a href="https://github.com/scrapy/scrapy/issues/1218">issue
1218</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/1233">issue
1233</a>{.reference
.external})</p>
</li>
<li>
<p>Module relocation (<a href="https://github.com/scrapy/scrapy/issues/1181">issue
1181</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/1210">issue
1210</a>{.reference
.external})</p>
</li>
<li>
<p>rename SpiderManager to SpiderLoader (<a href="https://github.com/scrapy/scrapy/issues/1166">issue
1166</a>{.reference
.external})</p>
</li>
<li>
<p>Remove djangoitem (<a href="https://github.com/scrapy/scrapy/issues/1177">issue
1177</a>{.reference
.external})</p>
</li>
<li>
<p>remove scrapy deploy command (<a href="https://github.com/scrapy/scrapy/issues/1102">issue
1102</a>{.reference
.external})</p>
</li>
<li>
<p>dissolve contrib_exp (<a href="https://github.com/scrapy/scrapy/issues/1134">issue
1134</a>{.reference
.external})</p>
</li>
<li>
<p>Deleted bin folder from root, fixes #913 (<a href="https://github.com/scrapy/scrapy/issues/914">issue
914</a>{.reference
.external})</p>
</li>
<li>
<p>Remove jsonrpc based webservice (<a href="https://github.com/scrapy/scrapy/issues/859">issue
859</a>{.reference
.external})</p>
</li>
<li>
<p>Move Test cases under project root dir (<a href="https://github.com/scrapy/scrapy/issues/827">issue
827</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/841">issue
841</a>{.reference
.external})</p>
</li>
<li>
<p>Fix backward incompatibility for relocated paths in settings (<a href="https://github.com/scrapy/scrapy/issues/1267">issue
1267</a>{.reference
.external})</p>
</li>
</ul>
<p>Documentation</p>
<ul>
<li>
<p>CrawlerProcess documentation (<a href="https://github.com/scrapy/scrapy/issues/1190">issue
1190</a>{.reference
.external})</p>
</li>
<li>
<p>Favoring web scraping over screen scraping in the descriptions
(<a href="https://github.com/scrapy/scrapy/issues/1188">issue
1188</a>{.reference
.external})</p>
</li>
<li>
<p>Some improvements for Scrapy tutorial (<a href="https://github.com/scrapy/scrapy/issues/1180">issue
1180</a>{.reference
.external})</p>
</li>
<li>
<p>Documenting Files Pipeline together with Images Pipeline (<a href="https://github.com/scrapy/scrapy/issues/1150">issue
1150</a>{.reference
.external})</p>
</li>
<li>
<p>deployment docs tweaks (<a href="https://github.com/scrapy/scrapy/issues/1164">issue
1164</a>{.reference
.external})</p>
</li>
<li>
<p>Added deployment section covering scrapyd-deploy and shub (<a href="https://github.com/scrapy/scrapy/issues/1124">issue
1124</a>{.reference
.external})</p>
</li>
<li>
<p>Adding more settings to project template (<a href="https://github.com/scrapy/scrapy/issues/1073">issue
1073</a>{.reference
.external})</p>
</li>
<li>
<p>some improvements to overview page (<a href="https://github.com/scrapy/scrapy/issues/1106">issue
1106</a>{.reference
.external})</p>
</li>
<li>
<p>Updated link in docs/topics/architecture.rst (<a href="https://github.com/scrapy/scrapy/issues/647">issue
647</a>{.reference
.external})</p>
</li>
<li>
<p>DOC reorder topics (<a href="https://github.com/scrapy/scrapy/issues/1022">issue
1022</a>{.reference
.external})</p>
</li>
<li>
<p>updating list of Request.meta special keys (<a href="https://github.com/scrapy/scrapy/issues/1071">issue
1071</a>{.reference
.external})</p>
</li>
<li>
<p>DOC document download_timeout (<a href="https://github.com/scrapy/scrapy/issues/898">issue
898</a>{.reference
.external})</p>
</li>
<li>
<p>DOC simplify extension docs (<a href="https://github.com/scrapy/scrapy/issues/893">issue
893</a>{.reference
.external})</p>
</li>
<li>
<p>Leaks docs (<a href="https://github.com/scrapy/scrapy/issues/894">issue
894</a>{.reference
.external})</p>
</li>
<li>
<p>DOC document from_crawler method for item pipelines (<a href="https://github.com/scrapy/scrapy/issues/904">issue
904</a>{.reference
.external})</p>
</li>
<li>
<p>Spider_error doesn't support deferreds (<a href="https://github.com/scrapy/scrapy/issues/1292">issue
1292</a>{.reference
.external})</p>
</li>
<li>
<p>Corrections &amp; Sphinx related fixes (<a href="https://github.com/scrapy/scrapy/issues/1220">issue
1220</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/1219">issue
1219</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/1196">issue
1196</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/1172">issue
1172</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/1171">issue
1171</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/1169">issue
1169</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/1160">issue
1160</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/1154">issue
1154</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/1127">issue
1127</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/1112">issue
1112</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/1105">issue
1105</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/1041">issue
1041</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/1082">issue
1082</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/1033">issue
1033</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/944">issue
944</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/866">issue
866</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/864">issue
864</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/796">issue
796</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/1260">issue
1260</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/1271">issue
1271</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/1293">issue
1293</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/1298">issue
1298</a>{.reference
.external})</p>
</li>
</ul>
<p>Bugfixes</p>
<ul>
<li>
<p>Item multi inheritance fix (<a href="https://github.com/scrapy/scrapy/issues/353">issue
353</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/1228">issue
1228</a>{.reference
.external})</p>
</li>
<li>
<p>ItemLoader.load_item: iterate over copy of fields (<a href="https://github.com/scrapy/scrapy/issues/722">issue
722</a>{.reference
.external})</p>
</li>
<li>
<p>Fix Unhandled error in Deferred (RobotsTxtMiddleware) (<a href="https://github.com/scrapy/scrapy/issues/1131">issue
1131</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/1197">issue
1197</a>{.reference
.external})</p>
</li>
<li>
<p>Force to read DOWNLOAD_TIMEOUT as int (<a href="https://github.com/scrapy/scrapy/issues/954">issue
954</a>{.reference
.external})</p>
</li>
<li>
<p>scrapy.utils.misc.load_object should print full traceback (<a href="https://github.com/scrapy/scrapy/issues/902">issue
902</a>{.reference
.external})</p>
</li>
<li>
<p>Fix bug for &quot;.local&quot; host name (<a href="https://github.com/scrapy/scrapy/issues/878">issue
878</a>{.reference
.external})</p>
</li>
<li>
<p>Fix for Enabled extensions, middlewares, pipelines info not printed
anymore (<a href="https://github.com/scrapy/scrapy/issues/879">issue
879</a>{.reference
.external})</p>
</li>
<li>
<p>fix dont_merge_cookies bad behaviour when set to false on meta
(<a href="https://github.com/scrapy/scrapy/issues/846">issue 846</a>{.reference
.external})</p>
</li>
</ul>
<p>Python 3 In Progress Support</p>
<ul>
<li>
<p>disable scrapy.telnet if twisted.conch is not available (<a href="https://github.com/scrapy/scrapy/issues/1161">issue
1161</a>{.reference
.external})</p>
</li>
<li>
<p>fix Python 3 syntax errors in ajaxcrawl.py (<a href="https://github.com/scrapy/scrapy/issues/1162">issue
1162</a>{.reference
.external})</p>
</li>
<li>
<p>more python3 compatibility changes for urllib (<a href="https://github.com/scrapy/scrapy/issues/1121">issue
1121</a>{.reference
.external})</p>
</li>
<li>
<p>assertItemsEqual was renamed to assertCountEqual in Python 3.
(<a href="https://github.com/scrapy/scrapy/issues/1070">issue
1070</a>{.reference
.external})</p>
</li>
<li>
<p>Import unittest.mock if available. (<a href="https://github.com/scrapy/scrapy/issues/1066">issue
1066</a>{.reference
.external})</p>
</li>
<li>
<p>updated deprecated cgi.parse_qsl to use six's parse_qsl (<a href="https://github.com/scrapy/scrapy/issues/909">issue
909</a>{.reference
.external})</p>
</li>
<li>
<p>Prevent Python 3 port regressions (<a href="https://github.com/scrapy/scrapy/issues/830">issue
830</a>{.reference
.external})</p>
</li>
<li>
<p>PY3: use MutableMapping for python 3 (<a href="https://github.com/scrapy/scrapy/issues/810">issue
810</a>{.reference
.external})</p>
</li>
<li>
<p>PY3: use six.BytesIO and six.moves.cStringIO (<a href="https://github.com/scrapy/scrapy/issues/803">issue
803</a>{.reference
.external})</p>
</li>
<li>
<p>PY3: fix xmlrpclib and email imports (<a href="https://github.com/scrapy/scrapy/issues/801">issue
801</a>{.reference
.external})</p>
</li>
<li>
<p>PY3: use six for robotparser and urlparse (<a href="https://github.com/scrapy/scrapy/issues/800">issue
800</a>{.reference
.external})</p>
</li>
<li>
<p>PY3: use six.iterkeys, six.iteritems, and tempfile (<a href="https://github.com/scrapy/scrapy/issues/799">issue
799</a>{.reference
.external})</p>
</li>
<li>
<p>PY3: fix has_key and use six.moves.configparser (<a href="https://github.com/scrapy/scrapy/issues/798">issue
798</a>{.reference
.external})</p>
</li>
<li>
<p>PY3: use six.moves.cPickle (<a href="https://github.com/scrapy/scrapy/issues/797">issue
797</a>{.reference
.external})</p>
</li>
<li>
<p>PY3 make it possible to run some tests in Python3 (<a href="https://github.com/scrapy/scrapy/issues/776">issue
776</a>{.reference
.external})</p>
</li>
</ul>
<p>Tests</p>
<ul>
<li>
<p>remove unnecessary lines from py3-ignores (<a href="https://github.com/scrapy/scrapy/issues/1243">issue
1243</a>{.reference
.external})</p>
</li>
<li>
<p>Fix remaining warnings from pytest while collecting tests (<a href="https://github.com/scrapy/scrapy/issues/1206">issue
1206</a>{.reference
.external})</p>
</li>
<li>
<p>Add docs build to travis (<a href="https://github.com/scrapy/scrapy/issues/1234">issue
1234</a>{.reference
.external})</p>
</li>
<li>
<p>TST don't collect tests from deprecated modules. (<a href="https://github.com/scrapy/scrapy/issues/1165">issue
1165</a>{.reference
.external})</p>
</li>
<li>
<p>install service_identity package in tests to prevent warnings
(<a href="https://github.com/scrapy/scrapy/issues/1168">issue
1168</a>{.reference
.external})</p>
</li>
<li>
<p>Fix deprecated settings API in tests (<a href="https://github.com/scrapy/scrapy/issues/1152">issue
1152</a>{.reference
.external})</p>
</li>
<li>
<p>Add test for webclient with POST method and no body given (<a href="https://github.com/scrapy/scrapy/issues/1089">issue
1089</a>{.reference
.external})</p>
</li>
<li>
<p>py3-ignores.txt supports comments (<a href="https://github.com/scrapy/scrapy/issues/1044">issue
1044</a>{.reference
.external})</p>
</li>
<li>
<p>modernize some of the asserts (<a href="https://github.com/scrapy/scrapy/issues/835">issue
835</a>{.reference
.external})</p>
</li>
<li>
<p>selector.__repr__ test (<a href="https://github.com/scrapy/scrapy/issues/779">issue
779</a>{.reference
.external})</p>
</li>
</ul>
<p>Code refactoring</p>
<ul>
<li>
<p>CSVFeedSpider cleanup: use iterate_spider_output (<a href="https://github.com/scrapy/scrapy/issues/1079">issue
1079</a>{.reference
.external})</p>
</li>
<li>
<p>remove unnecessary check from scrapy.utils.spider.iter_spider_output
(<a href="https://github.com/scrapy/scrapy/issues/1078">issue
1078</a>{.reference
.external})</p>
</li>
<li>
<p>Pydispatch pep8 (<a href="https://github.com/scrapy/scrapy/issues/992">issue
992</a>{.reference
.external})</p>
</li>
<li>
<p>Removed unused 'load=False' parameter from walk_modules() (<a href="https://github.com/scrapy/scrapy/issues/871">issue
871</a>{.reference
.external})</p>
</li>
<li>
<p>For consistency, use [<code>job_dir</code>{.docutils .literal
.notranslate}]{.pre} helper in [<code>SpiderState</code>{.docutils .literal
.notranslate}]{.pre} extension. (<a href="https://github.com/scrapy/scrapy/issues/805">issue
805</a>{.reference
.external})</p>
</li>
<li>
<p>rename &quot;sflo&quot; local variables to less cryptic &quot;log_observer&quot; (<a href="https://github.com/scrapy/scrapy/issues/775">issue
775</a>{.reference
.external})
:::
:::</p>
</li>
</ul>
<p>::: {#scrapy-0-24-6-2015-04-20 .section}</p>
<h4 id="scrapy-0246-2015-04-20headerlink"><a class="header" href="#scrapy-0246-2015-04-20headerlink">Scrapy 0.24.6 (2015-04-20)<a href="#scrapy-0-24-6-2015-04-20" title="Permalink to this heading">Â¶</a>{.headerlink}</a></h4>
<ul>
<li>
<p>encode invalid xpath with unicode_escape under PY2 (<a href="https://github.com/scrapy/scrapy/commit/07cb3e5">commit
07cb3e5</a>{.reference
.external})</p>
</li>
<li>
<p>fix IPython shell scope issue and load IPython user config (<a href="https://github.com/scrapy/scrapy/commit/2c8e573">commit
2c8e573</a>{.reference
.external})</p>
</li>
<li>
<p>Fix small typo in the docs (<a href="https://github.com/scrapy/scrapy/commit/d694019">commit
d694019</a>{.reference
.external})</p>
</li>
<li>
<p>Fix small typo (<a href="https://github.com/scrapy/scrapy/commit/f92fa83">commit
f92fa83</a>{.reference
.external})</p>
</li>
<li>
<p>Converted sel.xpath() calls to response.xpath() in Extracting the
data (<a href="https://github.com/scrapy/scrapy/commit/c2c6d15">commit
c2c6d15</a>{.reference
.external})
:::</p>
</li>
</ul>
<p>::: {#scrapy-0-24-5-2015-02-25 .section}</p>
<h4 id="scrapy-0245-2015-02-25headerlink"><a class="header" href="#scrapy-0245-2015-02-25headerlink">Scrapy 0.24.5 (2015-02-25)<a href="#scrapy-0-24-5-2015-02-25" title="Permalink to this heading">Â¶</a>{.headerlink}</a></h4>
<ul>
<li>
<p>Support new _getEndpoint Agent signatures on Twisted 15.0.0
(<a href="https://github.com/scrapy/scrapy/commit/540b9bc">commit
540b9bc</a>{.reference
.external})</p>
</li>
<li>
<p>DOC a couple more references are fixed (<a href="https://github.com/scrapy/scrapy/commit/b4c454b">commit
b4c454b</a>{.reference
.external})</p>
</li>
<li>
<p>DOC fix a reference (<a href="https://github.com/scrapy/scrapy/commit/e3c1260">commit
e3c1260</a>{.reference
.external})</p>
</li>
<li>
<p>t.i.b.ThreadedResolver is now a new-style class (<a href="https://github.com/scrapy/scrapy/commit/9e13f42">commit
9e13f42</a>{.reference
.external})</p>
</li>
<li>
<p>S3DownloadHandler: fix auth for requests with quoted paths/query
params (<a href="https://github.com/scrapy/scrapy/commit/cdb9a0b">commit
cdb9a0b</a>{.reference
.external})</p>
</li>
<li>
<p>fixed the variable types in mailsender documentation (<a href="https://github.com/scrapy/scrapy/commit/bb3a848">commit
bb3a848</a>{.reference
.external})</p>
</li>
<li>
<p>Reset items_scraped instead of item_count (<a href="https://github.com/scrapy/scrapy/commit/edb07a4">commit
edb07a4</a>{.reference
.external})</p>
</li>
<li>
<p>Tentative attention message about what document to read for
contributions (<a href="https://github.com/scrapy/scrapy/commit/7ee6f7a">commit
7ee6f7a</a>{.reference
.external})</p>
</li>
<li>
<p>mitmproxy 0.10.1 needs netlib 0.10.1 too (<a href="https://github.com/scrapy/scrapy/commit/874fcdd">commit
874fcdd</a>{.reference
.external})</p>
</li>
<li>
<p>pin mitmproxy 0.10.1 as &gt;0.11 does not work with tests (<a href="https://github.com/scrapy/scrapy/commit/c6b21f0">commit
c6b21f0</a>{.reference
.external})</p>
</li>
<li>
<p>Test the parse command locally instead of against an external url
(<a href="https://github.com/scrapy/scrapy/commit/c3a6628">commit
c3a6628</a>{.reference
.external})</p>
</li>
<li>
<p>Patches Twisted issue while closing the connection pool on
HTTPDownloadHandler (<a href="https://github.com/scrapy/scrapy/commit/d0bf957">commit
d0bf957</a>{.reference
.external})</p>
</li>
<li>
<p>Updates documentation on dynamic item classes. (<a href="https://github.com/scrapy/scrapy/commit/eeb589a">commit
eeb589a</a>{.reference
.external})</p>
</li>
<li>
<p>Merge pull request #943 from Lazar-T/patch-3 (<a href="https://github.com/scrapy/scrapy/commit/5fdab02">commit
5fdab02</a>{.reference
.external})</p>
</li>
<li>
<p>typo (<a href="https://github.com/scrapy/scrapy/commit/b0ae199">commit
b0ae199</a>{.reference
.external})</p>
</li>
<li>
<p>pywin32 is required by Twisted. closes #937 (<a href="https://github.com/scrapy/scrapy/commit/5cb0cfb">commit
5cb0cfb</a>{.reference
.external})</p>
</li>
<li>
<p>Update install.rst (<a href="https://github.com/scrapy/scrapy/commit/781286b">commit
781286b</a>{.reference
.external})</p>
</li>
<li>
<p>Merge pull request #928 from Lazar-T/patch-1 (<a href="https://github.com/scrapy/scrapy/commit/b415d04">commit
b415d04</a>{.reference
.external})</p>
</li>
<li>
<p>comma instead of fullstop (<a href="https://github.com/scrapy/scrapy/commit/627b9ba">commit
627b9ba</a>{.reference
.external})</p>
</li>
<li>
<p>Merge pull request #885 from jsma/patch-1 (<a href="https://github.com/scrapy/scrapy/commit/de909ad">commit
de909ad</a>{.reference
.external})</p>
</li>
<li>
<p>Update request-response.rst (<a href="https://github.com/scrapy/scrapy/commit/3f3263d">commit
3f3263d</a>{.reference
.external})</p>
</li>
<li>
<p>SgmlLinkExtractor - fix for parsing &lt;area&gt; tag with Unicode
present (<a href="https://github.com/scrapy/scrapy/commit/49b40f0">commit
49b40f0</a>{.reference
.external})
:::</p>
</li>
</ul>
<p>::: {#scrapy-0-24-4-2014-08-09 .section}</p>
<h4 id="scrapy-0244-2014-08-09headerlink"><a class="header" href="#scrapy-0244-2014-08-09headerlink">Scrapy 0.24.4 (2014-08-09)<a href="#scrapy-0-24-4-2014-08-09" title="Permalink to this heading">Â¶</a>{.headerlink}</a></h4>
<ul>
<li>
<p>pem file is used by mockserver and required by scrapy bench (<a href="https://github.com/scrapy/scrapy/commit/5eddc68">commit
5eddc68</a>{.reference
.external})</p>
</li>
<li>
<p>scrapy bench needs scrapy.tests* (<a href="https://github.com/scrapy/scrapy/commit/d6cb999">commit
d6cb999</a>{.reference
.external})
:::</p>
</li>
</ul>
<p>::: {#scrapy-0-24-3-2014-08-09 .section}</p>
<h4 id="scrapy-0243-2014-08-09headerlink"><a class="header" href="#scrapy-0243-2014-08-09headerlink">Scrapy 0.24.3 (2014-08-09)<a href="#scrapy-0-24-3-2014-08-09" title="Permalink to this heading">Â¶</a>{.headerlink}</a></h4>
<ul>
<li>
<p>no need to waste travis-ci time on py3 for 0.24 (<a href="https://github.com/scrapy/scrapy/commit/8e080c1">commit
8e080c1</a>{.reference
.external})</p>
</li>
<li>
<p>Update installation docs (<a href="https://github.com/scrapy/scrapy/commit/1d0c096">commit
1d0c096</a>{.reference
.external})</p>
</li>
<li>
<p>There is a trove classifier for Scrapy framework! (<a href="https://github.com/scrapy/scrapy/commit/4c701d7">commit
4c701d7</a>{.reference
.external})</p>
</li>
<li>
<p>update other places where w3lib version is mentioned (<a href="https://github.com/scrapy/scrapy/commit/d109c13">commit
d109c13</a>{.reference
.external})</p>
</li>
<li>
<p>Update w3lib requirement to 1.8.0 (<a href="https://github.com/scrapy/scrapy/commit/39d2ce5">commit
39d2ce5</a>{.reference
.external})</p>
</li>
<li>
<p>Use w3lib.html.replace_entities() (remove_entities() is deprecated)
(<a href="https://github.com/scrapy/scrapy/commit/180d3ad">commit
180d3ad</a>{.reference
.external})</p>
</li>
<li>
<p>set zip_safe=False (<a href="https://github.com/scrapy/scrapy/commit/a51ee8b">commit
a51ee8b</a>{.reference
.external})</p>
</li>
<li>
<p>do not ship tests package (<a href="https://github.com/scrapy/scrapy/commit/ee3b371">commit
ee3b371</a>{.reference
.external})</p>
</li>
<li>
<p>scrapy.bat is not needed anymore (<a href="https://github.com/scrapy/scrapy/commit/c3861cf">commit
c3861cf</a>{.reference
.external})</p>
</li>
<li>
<p>Modernize setup.py (<a href="https://github.com/scrapy/scrapy/commit/362e322">commit
362e322</a>{.reference
.external})</p>
</li>
<li>
<p>headers can not handle non-string values (<a href="https://github.com/scrapy/scrapy/commit/94a5c65">commit
94a5c65</a>{.reference
.external})</p>
</li>
<li>
<p>fix ftp test cases (<a href="https://github.com/scrapy/scrapy/commit/a274a7f">commit
a274a7f</a>{.reference
.external})</p>
</li>
<li>
<p>The sum up of travis-ci builds are taking like 50min to complete
(<a href="https://github.com/scrapy/scrapy/commit/ae1e2cc">commit
ae1e2cc</a>{.reference
.external})</p>
</li>
<li>
<p>Update shell.rst typo (<a href="https://github.com/scrapy/scrapy/commit/e49c96a">commit
e49c96a</a>{.reference
.external})</p>
</li>
<li>
<p>removes weird indentation in the shell results (<a href="https://github.com/scrapy/scrapy/commit/1ca489d">commit
1ca489d</a>{.reference
.external})</p>
</li>
<li>
<p>improved explanations, clarified blog post as source, added link for
XPath string functions in the spec (<a href="https://github.com/scrapy/scrapy/commit/65c8f05">commit
65c8f05</a>{.reference
.external})</p>
</li>
<li>
<p>renamed UserTimeoutError and ServerTimeouterror #583 (<a href="https://github.com/scrapy/scrapy/commit/037f6ab">commit
037f6ab</a>{.reference
.external})</p>
</li>
<li>
<p>adding some xpath tips to selectors docs (<a href="https://github.com/scrapy/scrapy/commit/2d103e0">commit
2d103e0</a>{.reference
.external})</p>
</li>
<li>
<p>fix tests to account for
<a href="https://github.com/scrapy/w3lib/pull/23">https://github.com/scrapy/w3lib/pull/23</a>{.reference
.external} (<a href="https://github.com/scrapy/scrapy/commit/f8d366a">commit
f8d366a</a>{.reference
.external})</p>
</li>
<li>
<p>get_func_args maximum recursion fix #728 (<a href="https://github.com/scrapy/scrapy/commit/81344ea">commit
81344ea</a>{.reference
.external})</p>
</li>
<li>
<p>Updated input/output processor example according to #560. (<a href="https://github.com/scrapy/scrapy/commit/f7c4ea8">commit
f7c4ea8</a>{.reference
.external})</p>
</li>
<li>
<p>Fixed Python syntax in tutorial. (<a href="https://github.com/scrapy/scrapy/commit/db59ed9">commit
db59ed9</a>{.reference
.external})</p>
</li>
<li>
<p>Add test case for tunneling proxy (<a href="https://github.com/scrapy/scrapy/commit/f090260">commit
f090260</a>{.reference
.external})</p>
</li>
<li>
<p>Bugfix for leaking Proxy-Authorization header to remote host when
using tunneling (<a href="https://github.com/scrapy/scrapy/commit/d8793af">commit
d8793af</a>{.reference
.external})</p>
</li>
<li>
<p>Extract links from XHTML documents with MIME-Type &quot;application/xml&quot;
(<a href="https://github.com/scrapy/scrapy/commit/ed1f376">commit
ed1f376</a>{.reference
.external})</p>
</li>
<li>
<p>Merge pull request #793 from roysc/patch-1 (<a href="https://github.com/scrapy/scrapy/commit/91a1106">commit
91a1106</a>{.reference
.external})</p>
</li>
<li>
<p>Fix typo in commands.rst (<a href="https://github.com/scrapy/scrapy/commit/743e1e2">commit
743e1e2</a>{.reference
.external})</p>
</li>
<li>
<p>better testcase for settings.overrides.setdefault (<a href="https://github.com/scrapy/scrapy/commit/e22daaf">commit
e22daaf</a>{.reference
.external})</p>
</li>
<li>
<p>Using CRLF as line marker according to http 1.1 definition (<a href="https://github.com/scrapy/scrapy/commit/5ec430b">commit
5ec430b</a>{.reference
.external})
:::</p>
</li>
</ul>
<p>::: {#scrapy-0-24-2-2014-07-08 .section}</p>
<h4 id="scrapy-0242-2014-07-08headerlink"><a class="header" href="#scrapy-0242-2014-07-08headerlink">Scrapy 0.24.2 (2014-07-08)<a href="#scrapy-0-24-2-2014-07-08" title="Permalink to this heading">Â¶</a>{.headerlink}</a></h4>
<ul>
<li>
<p>Use a mutable mapping to proxy deprecated settings.overrides and
settings.defaults attribute (<a href="https://github.com/scrapy/scrapy/commit/e5e8133">commit
e5e8133</a>{.reference
.external})</p>
</li>
<li>
<p>there is not support for python3 yet (<a href="https://github.com/scrapy/scrapy/commit/3cd6146">commit
3cd6146</a>{.reference
.external})</p>
</li>
<li>
<p>Update python compatible version set to Debian packages (<a href="https://github.com/scrapy/scrapy/commit/fa5d76b">commit
fa5d76b</a>{.reference
.external})</p>
</li>
<li>
<p>DOC fix formatting in release notes (<a href="https://github.com/scrapy/scrapy/commit/c6a9e20">commit
c6a9e20</a>{.reference
.external})
:::</p>
</li>
</ul>
<p>::: {#scrapy-0-24-1-2014-06-27 .section}</p>
<h4 id="scrapy-0241-2014-06-27headerlink"><a class="header" href="#scrapy-0241-2014-06-27headerlink">Scrapy 0.24.1 (2014-06-27)<a href="#scrapy-0-24-1-2014-06-27" title="Permalink to this heading">Â¶</a>{.headerlink}</a></h4>
<ul>
<li>Fix deprecated CrawlerSettings and increase backward compatibility
with .defaults attribute (<a href="https://github.com/scrapy/scrapy/commit/8e3f20a">commit
8e3f20a</a>{.reference
.external})
:::</li>
</ul>
<p>::: {#scrapy-0-24-0-2014-06-26 .section}</p>
<h4 id="scrapy-0240-2014-06-26headerlink"><a class="header" href="#scrapy-0240-2014-06-26headerlink">Scrapy 0.24.0 (2014-06-26)<a href="#scrapy-0-24-0-2014-06-26" title="Permalink to this heading">Â¶</a>{.headerlink}</a></h4>
<p>::: {#enhancements .section}</p>
<h5 id="enhancementsheaderlink"><a class="header" href="#enhancementsheaderlink">Enhancements<a href="#enhancements" title="Permalink to this heading">Â¶</a>{.headerlink}</a></h5>
<ul>
<li>
<p>Improve Scrapy top-level namespace (<a href="https://github.com/scrapy/scrapy/issues/494">issue
494</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/684">issue
684</a>{.reference
.external})</p>
</li>
<li>
<p>Add selector shortcuts to responses (<a href="https://github.com/scrapy/scrapy/issues/554">issue
554</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/690">issue
690</a>{.reference
.external})</p>
</li>
<li>
<p>Add new lxml based LinkExtractor to replace unmaintained
SgmlLinkExtractor (<a href="https://github.com/scrapy/scrapy/issues/559">issue
559</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/761">issue
761</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/763">issue
763</a>{.reference
.external})</p>
</li>
<li>
<p>Cleanup settings API - part of per-spider settings <strong>GSoC project</strong>
(<a href="https://github.com/scrapy/scrapy/issues/737">issue 737</a>{.reference
.external})</p>
</li>
<li>
<p>Add UTF8 encoding header to templates (<a href="https://github.com/scrapy/scrapy/issues/688">issue
688</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/762">issue
762</a>{.reference
.external})</p>
</li>
<li>
<p>Telnet console now binds to 127.0.0.1 by default (<a href="https://github.com/scrapy/scrapy/issues/699">issue
699</a>{.reference
.external})</p>
</li>
<li>
<p>Update Debian/Ubuntu install instructions (<a href="https://github.com/scrapy/scrapy/issues/509">issue
509</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/549">issue
549</a>{.reference
.external})</p>
</li>
<li>
<p>Disable smart strings in lxml XPath evaluations (<a href="https://github.com/scrapy/scrapy/issues/535">issue
535</a>{.reference
.external})</p>
</li>
<li>
<p>Restore filesystem based cache as default for http cache middleware
(<a href="https://github.com/scrapy/scrapy/issues/541">issue 541</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/500">issue
500</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/571">issue
571</a>{.reference
.external})</p>
</li>
<li>
<p>Expose current crawler in Scrapy shell (<a href="https://github.com/scrapy/scrapy/issues/557">issue
557</a>{.reference
.external})</p>
</li>
<li>
<p>Improve testsuite comparing CSV and XML exporters (<a href="https://github.com/scrapy/scrapy/issues/570">issue
570</a>{.reference
.external})</p>
</li>
<li>
<p>New [<code>offsite/filtered</code>{.docutils .literal .notranslate}]{.pre} and
[<code>offsite/domains</code>{.docutils .literal .notranslate}]{.pre} stats
(<a href="https://github.com/scrapy/scrapy/issues/566">issue 566</a>{.reference
.external})</p>
</li>
<li>
<p>Support process_links as generator in CrawlSpider (<a href="https://github.com/scrapy/scrapy/issues/555">issue
555</a>{.reference
.external})</p>
</li>
<li>
<p>Verbose logging and new stats counters for DupeFilter (<a href="https://github.com/scrapy/scrapy/issues/553">issue
553</a>{.reference
.external})</p>
</li>
<li>
<p>Add a mimetype parameter to [<code>MailSender.send()</code>{.docutils .literal
.notranslate}]{.pre} (<a href="https://github.com/scrapy/scrapy/issues/602">issue
602</a>{.reference
.external})</p>
</li>
<li>
<p>Generalize file pipeline log messages (<a href="https://github.com/scrapy/scrapy/issues/622">issue
622</a>{.reference
.external})</p>
</li>
<li>
<p>Replace unencodeable codepoints with html entities in
SGMLLinkExtractor (<a href="https://github.com/scrapy/scrapy/issues/565">issue
565</a>{.reference
.external})</p>
</li>
<li>
<p>Converted SEP documents to rst format (<a href="https://github.com/scrapy/scrapy/issues/629">issue
629</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/630">issue
630</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/638">issue
638</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/632">issue
632</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/636">issue
636</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/640">issue
640</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/635">issue
635</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/634">issue
634</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/639">issue
639</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/637">issue
637</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/631">issue
631</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/633">issue
633</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/641">issue
641</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/642">issue
642</a>{.reference
.external})</p>
</li>
<li>
<p>Tests and docs for clickdata's nr index in FormRequest (<a href="https://github.com/scrapy/scrapy/issues/646">issue
646</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/645">issue
645</a>{.reference
.external})</p>
</li>
<li>
<p>Allow to disable a downloader handler just like any other component
(<a href="https://github.com/scrapy/scrapy/issues/650">issue 650</a>{.reference
.external})</p>
</li>
<li>
<p>Log when a request is discarded after too many redirections (<a href="https://github.com/scrapy/scrapy/issues/654">issue
654</a>{.reference
.external})</p>
</li>
<li>
<p>Log error responses if they are not handled by spider callbacks
(<a href="https://github.com/scrapy/scrapy/issues/612">issue 612</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/656">issue
656</a>{.reference
.external})</p>
</li>
<li>
<p>Add content-type check to http compression mw (<a href="https://github.com/scrapy/scrapy/issues/193">issue
193</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/660">issue
660</a>{.reference
.external})</p>
</li>
<li>
<p>Run pypy tests using latest pypi from ppa (<a href="https://github.com/scrapy/scrapy/issues/674">issue
674</a>{.reference
.external})</p>
</li>
<li>
<p>Run test suite using pytest instead of trial (<a href="https://github.com/scrapy/scrapy/issues/679">issue
679</a>{.reference
.external})</p>
</li>
<li>
<p>Build docs and check for dead links in tox environment (<a href="https://github.com/scrapy/scrapy/issues/687">issue
687</a>{.reference
.external})</p>
</li>
<li>
<p>Make scrapy.version_info a tuple of integers (<a href="https://github.com/scrapy/scrapy/issues/681">issue
681</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/692">issue
692</a>{.reference
.external})</p>
</li>
<li>
<p>Infer exporter's output format from filename extensions (<a href="https://github.com/scrapy/scrapy/issues/546">issue
546</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/659">issue
659</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/760">issue
760</a>{.reference
.external})</p>
</li>
<li>
<p>Support case-insensitive domains in
[<code>url_is_from_any_domain()</code>{.docutils .literal .notranslate}]{.pre}
(<a href="https://github.com/scrapy/scrapy/issues/693">issue 693</a>{.reference
.external})</p>
</li>
<li>
<p>Remove pep8 warnings in project and spider templates (<a href="https://github.com/scrapy/scrapy/issues/698">issue
698</a>{.reference
.external})</p>
</li>
<li>
<p>Tests and docs for [<code>request_fingerprint</code>{.docutils .literal
.notranslate}]{.pre} function (<a href="https://github.com/scrapy/scrapy/issues/597">issue
597</a>{.reference
.external})</p>
</li>
<li>
<p>Update SEP-19 for GSoC project [<code>per-spider</code>{.docutils .literal
.notranslate}]{.pre}<code> </code>{.docutils .literal
.notranslate}[<code>settings</code>{.docutils .literal .notranslate}]{.pre}
(<a href="https://github.com/scrapy/scrapy/issues/705">issue 705</a>{.reference
.external})</p>
</li>
<li>
<p>Set exit code to non-zero when contracts fails (<a href="https://github.com/scrapy/scrapy/issues/727">issue
727</a>{.reference
.external})</p>
</li>
<li>
<p>Add a setting to control what class is instantiated as Downloader
component (<a href="https://github.com/scrapy/scrapy/issues/738">issue
738</a>{.reference
.external})</p>
</li>
<li>
<p>Pass response in [<code>item_dropped</code>{.docutils .literal
.notranslate}]{.pre} signal (<a href="https://github.com/scrapy/scrapy/issues/724">issue
724</a>{.reference
.external})</p>
</li>
<li>
<p>Improve [<code>scrapy</code>{.docutils .literal
.notranslate}]{.pre}<code> </code>{.docutils .literal
.notranslate}[<code>check</code>{.docutils .literal .notranslate}]{.pre}
contracts command (<a href="https://github.com/scrapy/scrapy/issues/733">issue
733</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/752">issue
752</a>{.reference
.external})</p>
</li>
<li>
<p>Document [<code>spider.closed()</code>{.docutils .literal .notranslate}]{.pre}
shortcut (<a href="https://github.com/scrapy/scrapy/issues/719">issue
719</a>{.reference
.external})</p>
</li>
<li>
<p>Document [<code>request_scheduled</code>{.docutils .literal
.notranslate}]{.pre} signal (<a href="https://github.com/scrapy/scrapy/issues/746">issue
746</a>{.reference
.external})</p>
</li>
<li>
<p>Add a note about reporting security issues (<a href="https://github.com/scrapy/scrapy/issues/697">issue
697</a>{.reference
.external})</p>
</li>
<li>
<p>Add LevelDB http cache storage backend (<a href="https://github.com/scrapy/scrapy/issues/626">issue
626</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/500">issue
500</a>{.reference
.external})</p>
</li>
<li>
<p>Sort spider list output of [<code>scrapy</code>{.docutils .literal
.notranslate}]{.pre}<code> </code>{.docutils .literal
.notranslate}[<code>list</code>{.docutils .literal .notranslate}]{.pre} command
(<a href="https://github.com/scrapy/scrapy/issues/742">issue 742</a>{.reference
.external})</p>
</li>
<li>
<p>Multiple documentation enhancements and fixes (<a href="https://github.com/scrapy/scrapy/issues/575">issue
575</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/587">issue
587</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/590">issue
590</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/596">issue
596</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/610">issue
610</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/617">issue
617</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/618">issue
618</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/627">issue
627</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/613">issue
613</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/643">issue
643</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/654">issue
654</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/675">issue
675</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/663">issue
663</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/711">issue
711</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/714">issue
714</a>{.reference
.external})
:::</p>
</li>
</ul>
<p>::: {#id129 .section}</p>
<h5 id="bugfixesheaderlink"><a class="header" href="#bugfixesheaderlink">Bugfixes<a href="#id129" title="Permalink to this heading">Â¶</a>{.headerlink}</a></h5>
<ul>
<li>
<p>Encode unicode URL value when creating Links in RegexLinkExtractor
(<a href="https://github.com/scrapy/scrapy/issues/561">issue 561</a>{.reference
.external})</p>
</li>
<li>
<p>Ignore None values in ItemLoader processors (<a href="https://github.com/scrapy/scrapy/issues/556">issue
556</a>{.reference
.external})</p>
</li>
<li>
<p>Fix link text when there is an inner tag in SGMLLinkExtractor and
HtmlParserLinkExtractor (<a href="https://github.com/scrapy/scrapy/issues/485">issue
485</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/574">issue
574</a>{.reference
.external})</p>
</li>
<li>
<p>Fix wrong checks on subclassing of deprecated classes (<a href="https://github.com/scrapy/scrapy/issues/581">issue
581</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/584">issue
584</a>{.reference
.external})</p>
</li>
<li>
<p>Handle errors caused by inspect.stack() failures (<a href="https://github.com/scrapy/scrapy/issues/582">issue
582</a>{.reference
.external})</p>
</li>
<li>
<p>Fix a reference to unexistent engine attribute (<a href="https://github.com/scrapy/scrapy/issues/593">issue
593</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/594">issue
594</a>{.reference
.external})</p>
</li>
<li>
<p>Fix dynamic itemclass example usage of type() (<a href="https://github.com/scrapy/scrapy/issues/603">issue
603</a>{.reference
.external})</p>
</li>
<li>
<p>Use lucasdemarchi/codespell to fix typos (<a href="https://github.com/scrapy/scrapy/issues/628">issue
628</a>{.reference
.external})</p>
</li>
<li>
<p>Fix default value of attrs argument in SgmlLinkExtractor to be tuple
(<a href="https://github.com/scrapy/scrapy/issues/661">issue 661</a>{.reference
.external})</p>
</li>
<li>
<p>Fix XXE flaw in sitemap reader (<a href="https://github.com/scrapy/scrapy/issues/676">issue
676</a>{.reference
.external})</p>
</li>
<li>
<p>Fix engine to support filtered start requests (<a href="https://github.com/scrapy/scrapy/issues/707">issue
707</a>{.reference
.external})</p>
</li>
<li>
<p>Fix offsite middleware case on urls with no hostnames (<a href="https://github.com/scrapy/scrapy/issues/745">issue
745</a>{.reference
.external})</p>
</li>
<li>
<p>Testsuite doesn't require PIL anymore (<a href="https://github.com/scrapy/scrapy/issues/585">issue
585</a>{.reference
.external})
:::
:::</p>
</li>
</ul>
<p>::: {#scrapy-0-22-2-released-2014-02-14 .section}</p>
<h4 id="scrapy-0222-released-2014-02-14headerlink"><a class="header" href="#scrapy-0222-released-2014-02-14headerlink">Scrapy 0.22.2 (released 2014-02-14)<a href="#scrapy-0-22-2-released-2014-02-14" title="Permalink to this heading">Â¶</a>{.headerlink}</a></h4>
<ul>
<li>
<p>fix a reference to unexistent engine.slots. closes #593 (<a href="https://github.com/scrapy/scrapy/commit/13c099a">commit
13c099a</a>{.reference
.external})</p>
</li>
<li>
<p>downloaderMW doc typo (spiderMW doc copy remnant) (<a href="https://github.com/scrapy/scrapy/commit/8ae11bf">commit
8ae11bf</a>{.reference
.external})</p>
</li>
<li>
<p>Correct typos (<a href="https://github.com/scrapy/scrapy/commit/1346037">commit
1346037</a>{.reference
.external})
:::</p>
</li>
</ul>
<p>::: {#scrapy-0-22-1-released-2014-02-08 .section}</p>
<h4 id="scrapy-0221-released-2014-02-08headerlink"><a class="header" href="#scrapy-0221-released-2014-02-08headerlink">Scrapy 0.22.1 (released 2014-02-08)<a href="#scrapy-0-22-1-released-2014-02-08" title="Permalink to this heading">Â¶</a>{.headerlink}</a></h4>
<ul>
<li>
<p>localhost666 can resolve under certain circumstances (<a href="https://github.com/scrapy/scrapy/commit/2ec2279">commit
2ec2279</a>{.reference
.external})</p>
</li>
<li>
<p>test inspect.stack failure (<a href="https://github.com/scrapy/scrapy/commit/cc3eda3">commit
cc3eda3</a>{.reference
.external})</p>
</li>
<li>
<p>Handle cases when inspect.stack() fails (<a href="https://github.com/scrapy/scrapy/commit/8cb44f9">commit
8cb44f9</a>{.reference
.external})</p>
</li>
<li>
<p>Fix wrong checks on subclassing of deprecated classes. closes #581
(<a href="https://github.com/scrapy/scrapy/commit/46d98d6">commit
46d98d6</a>{.reference
.external})</p>
</li>
<li>
<p>Docs: 4-space indent for final spider example (<a href="https://github.com/scrapy/scrapy/commit/13846de">commit
13846de</a>{.reference
.external})</p>
</li>
<li>
<p>Fix HtmlParserLinkExtractor and tests after #485 merge (<a href="https://github.com/scrapy/scrapy/commit/368a946">commit
368a946</a>{.reference
.external})</p>
</li>
<li>
<p>BaseSgmlLinkExtractor: Fixed the missing space when the link has an
inner tag (<a href="https://github.com/scrapy/scrapy/commit/b566388">commit
b566388</a>{.reference
.external})</p>
</li>
<li>
<p>BaseSgmlLinkExtractor: Added unit test of a link with an inner tag
(<a href="https://github.com/scrapy/scrapy/commit/c1cb418">commit
c1cb418</a>{.reference
.external})</p>
</li>
<li>
<p>BaseSgmlLinkExtractor: Fixed unknown_endtag() so that it only set
current_link=None when the end tag match the opening tag (<a href="https://github.com/scrapy/scrapy/commit/7e4d627">commit
7e4d627</a>{.reference
.external})</p>
</li>
<li>
<p>Fix tests for Travis-CI build (<a href="https://github.com/scrapy/scrapy/commit/76c7e20">commit
76c7e20</a>{.reference
.external})</p>
</li>
<li>
<p>replace unencodeable codepoints with html entities. fixes #562 and
#285 (<a href="https://github.com/scrapy/scrapy/commit/5f87b17">commit
5f87b17</a>{.reference
.external})</p>
</li>
<li>
<p>RegexLinkExtractor: encode URL unicode value when creating Links
(<a href="https://github.com/scrapy/scrapy/commit/d0ee545">commit
d0ee545</a>{.reference
.external})</p>
</li>
<li>
<p>Updated the tutorial crawl output with latest output. (<a href="https://github.com/scrapy/scrapy/commit/8da65de">commit
8da65de</a>{.reference
.external})</p>
</li>
<li>
<p>Updated shell docs with the crawler reference and fixed the actual
shell output. (<a href="https://github.com/scrapy/scrapy/commit/875b9ab">commit
875b9ab</a>{.reference
.external})</p>
</li>
<li>
<p>PEP8 minor edits. (<a href="https://github.com/scrapy/scrapy/commit/f89efaf">commit
f89efaf</a>{.reference
.external})</p>
</li>
<li>
<p>Expose current crawler in the Scrapy shell. (<a href="https://github.com/scrapy/scrapy/commit/5349cec">commit
5349cec</a>{.reference
.external})</p>
</li>
<li>
<p>Unused re import and PEP8 minor edits. (<a href="https://github.com/scrapy/scrapy/commit/387f414">commit
387f414</a>{.reference
.external})</p>
</li>
<li>
<p>Ignore None's values when using the ItemLoader. (<a href="https://github.com/scrapy/scrapy/commit/0632546">commit
0632546</a>{.reference
.external})</p>
</li>
<li>
<p>DOC Fixed HTTPCACHE_STORAGE typo in the default value which is now
Filesystem instead Dbm. (<a href="https://github.com/scrapy/scrapy/commit/cde9a8c">commit
cde9a8c</a>{.reference
.external})</p>
</li>
<li>
<p>show Ubuntu setup instructions as literal code (<a href="https://github.com/scrapy/scrapy/commit/fb5c9c5">commit
fb5c9c5</a>{.reference
.external})</p>
</li>
<li>
<p>Update Ubuntu installation instructions (<a href="https://github.com/scrapy/scrapy/commit/70fb105">commit
70fb105</a>{.reference
.external})</p>
</li>
<li>
<p>Merge pull request #550 from stray-leone/patch-1 (<a href="https://github.com/scrapy/scrapy/commit/6f70b6a">commit
6f70b6a</a>{.reference
.external})</p>
</li>
<li>
<p>modify the version of Scrapy Ubuntu package (<a href="https://github.com/scrapy/scrapy/commit/725900d">commit
725900d</a>{.reference
.external})</p>
</li>
<li>
<p>fix 0.22.0 release date (<a href="https://github.com/scrapy/scrapy/commit/af0219a">commit
af0219a</a>{.reference
.external})</p>
</li>
<li>
<p>fix typos in news.rst and remove (not released yet) header (<a href="https://github.com/scrapy/scrapy/commit/b7f58f4">commit
b7f58f4</a>{.reference
.external})
:::</p>
</li>
</ul>
<p>::: {#scrapy-0-22-0-released-2014-01-17 .section}</p>
<h4 id="scrapy-0220-released-2014-01-17headerlink"><a class="header" href="#scrapy-0220-released-2014-01-17headerlink">Scrapy 0.22.0 (released 2014-01-17)<a href="#scrapy-0-22-0-released-2014-01-17" title="Permalink to this heading">Â¶</a>{.headerlink}</a></h4>
<p>::: {#id130 .section}</p>
<h5 id="enhancementsheaderlink-1"><a class="header" href="#enhancementsheaderlink-1">Enhancements<a href="#id130" title="Permalink to this heading">Â¶</a>{.headerlink}</a></h5>
<ul>
<li>
<p>[<strong>Backward incompatible</strong>] Switched HTTPCacheMiddleware backend
to filesystem (<a href="https://github.com/scrapy/scrapy/issues/541">issue
541</a>{.reference
.external}) To restore old backend set
[<code>HTTPCACHE_STORAGE</code>{.docutils .literal .notranslate}]{.pre} to
[<code>scrapy.contrib.httpcache.DbmCacheStorage</code>{.docutils .literal
.notranslate}]{.pre}</p>
</li>
<li>
<p>Proxy https:// urls using CONNECT method (<a href="https://github.com/scrapy/scrapy/issues/392">issue
392</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/397">issue
397</a>{.reference
.external})</p>
</li>
<li>
<p>Add a middleware to crawl ajax crawlable pages as defined by google
(<a href="https://github.com/scrapy/scrapy/issues/343">issue 343</a>{.reference
.external})</p>
</li>
<li>
<p>Rename scrapy.spider.BaseSpider to scrapy.spider.Spider (<a href="https://github.com/scrapy/scrapy/issues/510">issue
510</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/519">issue
519</a>{.reference
.external})</p>
</li>
<li>
<p>Selectors register EXSLT namespaces by default (<a href="https://github.com/scrapy/scrapy/issues/472">issue
472</a>{.reference
.external})</p>
</li>
<li>
<p>Unify item loaders similar to selectors renaming (<a href="https://github.com/scrapy/scrapy/issues/461">issue
461</a>{.reference
.external})</p>
</li>
<li>
<p>Make [<code>RFPDupeFilter</code>{.docutils .literal .notranslate}]{.pre} class
easily subclassable (<a href="https://github.com/scrapy/scrapy/issues/533">issue
533</a>{.reference
.external})</p>
</li>
<li>
<p>Improve test coverage and forthcoming Python 3 support (<a href="https://github.com/scrapy/scrapy/issues/525">issue
525</a>{.reference
.external})</p>
</li>
<li>
<p>Promote startup info on settings and middleware to INFO level
(<a href="https://github.com/scrapy/scrapy/issues/520">issue 520</a>{.reference
.external})</p>
</li>
<li>
<p>Support partials in [<code>get_func_args</code>{.docutils .literal
.notranslate}]{.pre} util (<a href="https://github.com/scrapy/scrapy/issues/506">issue
506</a>{.reference
.external}, issue:504)</p>
</li>
<li>
<p>Allow running individual tests via tox (<a href="https://github.com/scrapy/scrapy/issues/503">issue
503</a>{.reference
.external})</p>
</li>
<li>
<p>Update extensions ignored by link extractors (<a href="https://github.com/scrapy/scrapy/issues/498">issue
498</a>{.reference
.external})</p>
</li>
<li>
<p>Add middleware methods to get files/images/thumbs paths (<a href="https://github.com/scrapy/scrapy/issues/490">issue
490</a>{.reference
.external})</p>
</li>
<li>
<p>Improve offsite middleware tests (<a href="https://github.com/scrapy/scrapy/issues/478">issue
478</a>{.reference
.external})</p>
</li>
<li>
<p>Add a way to skip default Referer header set by RefererMiddleware
(<a href="https://github.com/scrapy/scrapy/issues/475">issue 475</a>{.reference
.external})</p>
</li>
<li>
<p>Do not send [<code>x-gzip</code>{.docutils .literal .notranslate}]{.pre} in
default [<code>Accept-Encoding</code>{.docutils .literal .notranslate}]{.pre}
header (<a href="https://github.com/scrapy/scrapy/issues/469">issue
469</a>{.reference
.external})</p>
</li>
<li>
<p>Support defining http error handling using settings (<a href="https://github.com/scrapy/scrapy/issues/466">issue
466</a>{.reference
.external})</p>
</li>
<li>
<p>Use modern python idioms wherever you find legacies (<a href="https://github.com/scrapy/scrapy/issues/497">issue
497</a>{.reference
.external})</p>
</li>
<li>
<p>Improve and correct documentation (<a href="https://github.com/scrapy/scrapy/issues/527">issue
527</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/524">issue
524</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/521">issue
521</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/517">issue
517</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/512">issue
512</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/505">issue
505</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/502">issue
502</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/489">issue
489</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/465">issue
465</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/460">issue
460</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/425">issue
425</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/536">issue
536</a>{.reference
.external})
:::</p>
</li>
</ul>
<p>::: {#fixes .section}</p>
<h5 id="fixesheaderlink"><a class="header" href="#fixesheaderlink">Fixes<a href="#fixes" title="Permalink to this heading">Â¶</a>{.headerlink}</a></h5>
<ul>
<li>
<p>Update Selector class imports in CrawlSpider template (<a href="https://github.com/scrapy/scrapy/issues/484">issue
484</a>{.reference
.external})</p>
</li>
<li>
<p>Fix unexistent reference to [<code>engine.slots</code>{.docutils .literal
.notranslate}]{.pre} (<a href="https://github.com/scrapy/scrapy/issues/464">issue
464</a>{.reference
.external})</p>
</li>
<li>
<p>Do not try to call [<code>body_as_unicode()</code>{.docutils .literal
.notranslate}]{.pre} on a non-TextResponse instance (<a href="https://github.com/scrapy/scrapy/issues/462">issue
462</a>{.reference
.external})</p>
</li>
<li>
<p>Warn when subclassing XPathItemLoader, previously it only warned on
instantiation. (<a href="https://github.com/scrapy/scrapy/issues/523">issue
523</a>{.reference
.external})</p>
</li>
<li>
<p>Warn when subclassing XPathSelector, previously it only warned on
instantiation. (<a href="https://github.com/scrapy/scrapy/issues/537">issue
537</a>{.reference
.external})</p>
</li>
<li>
<p>Multiple fixes to memory stats (<a href="https://github.com/scrapy/scrapy/issues/531">issue
531</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/530">issue
530</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/529">issue
529</a>{.reference
.external})</p>
</li>
<li>
<p>Fix overriding url in [<code>FormRequest.from_response()</code>{.docutils
.literal .notranslate}]{.pre} (<a href="https://github.com/scrapy/scrapy/issues/507">issue
507</a>{.reference
.external})</p>
</li>
<li>
<p>Fix tests runner under pip 1.5 (<a href="https://github.com/scrapy/scrapy/issues/513">issue
513</a>{.reference
.external})</p>
</li>
<li>
<p>Fix logging error when spider name is unicode (<a href="https://github.com/scrapy/scrapy/issues/479">issue
479</a>{.reference
.external})
:::
:::</p>
</li>
</ul>
<p>::: {#scrapy-0-20-2-released-2013-12-09 .section}</p>
<h4 id="scrapy-0202-released-2013-12-09headerlink"><a class="header" href="#scrapy-0202-released-2013-12-09headerlink">Scrapy 0.20.2 (released 2013-12-09)<a href="#scrapy-0-20-2-released-2013-12-09" title="Permalink to this heading">Â¶</a>{.headerlink}</a></h4>
<ul>
<li>
<p>Update CrawlSpider Template with Selector changes (<a href="https://github.com/scrapy/scrapy/commit/6d1457d">commit
6d1457d</a>{.reference
.external})</p>
</li>
<li>
<p>fix method name in tutorial. closes GH-480 (<a href="https://github.com/scrapy/scrapy/commit/b4fc359">commit
b4fc359</a>{.reference
.external}
:::</p>
</li>
</ul>
<p>::: {#scrapy-0-20-1-released-2013-11-28 .section}</p>
<h4 id="scrapy-0201-released-2013-11-28headerlink"><a class="header" href="#scrapy-0201-released-2013-11-28headerlink">Scrapy 0.20.1 (released 2013-11-28)<a href="#scrapy-0-20-1-released-2013-11-28" title="Permalink to this heading">Â¶</a>{.headerlink}</a></h4>
<ul>
<li>
<p>include_package_data is required to build wheels from published
sources (<a href="https://github.com/scrapy/scrapy/commit/5ba1ad5">commit
5ba1ad5</a>{.reference
.external})</p>
</li>
<li>
<p>process_parallel was leaking the failures on its internal deferreds.
closes #458 (<a href="https://github.com/scrapy/scrapy/commit/419a780">commit
419a780</a>{.reference
.external})
:::</p>
</li>
</ul>
<p>::: {#scrapy-0-20-0-released-2013-11-08 .section}</p>
<h4 id="scrapy-0200-released-2013-11-08headerlink"><a class="header" href="#scrapy-0200-released-2013-11-08headerlink">Scrapy 0.20.0 (released 2013-11-08)<a href="#scrapy-0-20-0-released-2013-11-08" title="Permalink to this heading">Â¶</a>{.headerlink}</a></h4>
<p>::: {#id131 .section}</p>
<h5 id="enhancementsheaderlink-2"><a class="header" href="#enhancementsheaderlink-2">Enhancements<a href="#id131" title="Permalink to this heading">Â¶</a>{.headerlink}</a></h5>
<ul>
<li>
<p>New Selector's API including CSS selectors (<a href="https://github.com/scrapy/scrapy/issues/395">issue
395</a>{.reference
.external} and <a href="https://github.com/scrapy/scrapy/issues/426">issue
426</a>{.reference
.external}),</p>
</li>
<li>
<p>Request/Response url/body attributes are now immutable (modifying
them had been deprecated for a long time)</p>
</li>
<li>
<p><a href="index.html#std-setting-ITEM_PIPELINES">[<code>ITEM_PIPELINES</code>{.xref .std .std-setting .docutils .literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal} is now defined as a dict (instead of
a list)</p>
</li>
<li>
<p>Sitemap spider can fetch alternate URLs (<a href="https://github.com/scrapy/scrapy/issues/360">issue
360</a>{.reference
.external})</p>
</li>
<li>
<p>[<code>Selector.remove_namespaces()</code>{.docutils .literal
.notranslate}]{.pre} now remove namespaces from element's
attributes. (<a href="https://github.com/scrapy/scrapy/issues/416">issue
416</a>{.reference
.external})</p>
</li>
<li>
<p>Paved the road for Python 3.3+ (<a href="https://github.com/scrapy/scrapy/issues/435">issue
435</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/436">issue
436</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/431">issue
431</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/452">issue
452</a>{.reference
.external})</p>
</li>
<li>
<p>New item exporter using native python types with nesting support
(<a href="https://github.com/scrapy/scrapy/issues/366">issue 366</a>{.reference
.external})</p>
</li>
<li>
<p>Tune HTTP1.1 pool size so it matches concurrency defined by settings
(<a href="https://github.com/scrapy/scrapy/commit/b43b5f575">commit
b43b5f575</a>{.reference
.external})</p>
</li>
<li>
<p>scrapy.mail.MailSender now can connect over TLS or upgrade using
STARTTLS (<a href="https://github.com/scrapy/scrapy/issues/327">issue
327</a>{.reference
.external})</p>
</li>
<li>
<p>New FilesPipeline with functionality factored out from
ImagesPipeline (<a href="https://github.com/scrapy/scrapy/issues/370">issue
370</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/409">issue
409</a>{.reference
.external})</p>
</li>
<li>
<p>Recommend Pillow instead of PIL for image handling (<a href="https://github.com/scrapy/scrapy/issues/317">issue
317</a>{.reference
.external})</p>
</li>
<li>
<p>Added Debian packages for Ubuntu Quantal and Raring (<a href="https://github.com/scrapy/scrapy/commit/86230c0">commit
86230c0</a>{.reference
.external})</p>
</li>
<li>
<p>Mock server (used for tests) can listen for HTTPS requests (<a href="https://github.com/scrapy/scrapy/issues/410">issue
410</a>{.reference
.external})</p>
</li>
<li>
<p>Remove multi spider support from multiple core components (<a href="https://github.com/scrapy/scrapy/issues/422">issue
422</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/421">issue
421</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/420">issue
420</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/419">issue
419</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/423">issue
423</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/418">issue
418</a>{.reference
.external})</p>
</li>
<li>
<p>Travis-CI now tests Scrapy changes against development versions of
[<code>w3lib</code>{.docutils .literal .notranslate}]{.pre} and
[<code>queuelib</code>{.docutils .literal .notranslate}]{.pre} python packages.</p>
</li>
<li>
<p>Add pypy 2.1 to continuous integration tests (<a href="https://github.com/scrapy/scrapy/commit/ecfa7431">commit
ecfa7431</a>{.reference
.external})</p>
</li>
<li>
<p>Pylinted, pep8 and removed old-style exceptions from source (<a href="https://github.com/scrapy/scrapy/issues/430">issue
430</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/432">issue
432</a>{.reference
.external})</p>
</li>
<li>
<p>Use importlib for parametric imports (<a href="https://github.com/scrapy/scrapy/issues/445">issue
445</a>{.reference
.external})</p>
</li>
<li>
<p>Handle a regression introduced in Python 2.7.5 that affects
XmlItemExporter (<a href="https://github.com/scrapy/scrapy/issues/372">issue
372</a>{.reference
.external})</p>
</li>
<li>
<p>Bugfix crawling shutdown on SIGINT (<a href="https://github.com/scrapy/scrapy/issues/450">issue
450</a>{.reference
.external})</p>
</li>
<li>
<p>Do not submit [<code>reset</code>{.docutils .literal .notranslate}]{.pre} type
inputs in FormRequest.from_response (<a href="https://github.com/scrapy/scrapy/commit/b326b87">commit
b326b87</a>{.reference
.external})</p>
</li>
<li>
<p>Do not silence download errors when request errback raises an
exception (<a href="https://github.com/scrapy/scrapy/commit/684cfc0">commit
684cfc0</a>{.reference
.external})
:::</p>
</li>
</ul>
<p>::: {#id132 .section}</p>
<h5 id="bugfixesheaderlink-1"><a class="header" href="#bugfixesheaderlink-1">Bugfixes<a href="#id132" title="Permalink to this heading">Â¶</a>{.headerlink}</a></h5>
<ul>
<li>
<p>Fix tests under Django 1.6 (<a href="https://github.com/scrapy/scrapy/commit/b6bed44c">commit
b6bed44c</a>{.reference
.external})</p>
</li>
<li>
<p>Lot of bugfixes to retry middleware under disconnections using HTTP
1.1 download handler</p>
</li>
<li>
<p>Fix inconsistencies among Twisted releases (<a href="https://github.com/scrapy/scrapy/issues/406">issue
406</a>{.reference
.external})</p>
</li>
<li>
<p>Fix Scrapy shell bugs (<a href="https://github.com/scrapy/scrapy/issues/418">issue
418</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/407">issue
407</a>{.reference
.external})</p>
</li>
<li>
<p>Fix invalid variable name in setup.py (<a href="https://github.com/scrapy/scrapy/issues/429">issue
429</a>{.reference
.external})</p>
</li>
<li>
<p>Fix tutorial references (<a href="https://github.com/scrapy/scrapy/issues/387">issue
387</a>{.reference
.external})</p>
</li>
<li>
<p>Improve request-response docs (<a href="https://github.com/scrapy/scrapy/issues/391">issue
391</a>{.reference
.external})</p>
</li>
<li>
<p>Improve best practices docs (<a href="https://github.com/scrapy/scrapy/issues/399">issue
399</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/400">issue
400</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/401">issue
401</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/402">issue
402</a>{.reference
.external})</p>
</li>
<li>
<p>Improve django integration docs (<a href="https://github.com/scrapy/scrapy/issues/404">issue
404</a>{.reference
.external})</p>
</li>
<li>
<p>Document [<code>bindaddress</code>{.docutils .literal .notranslate}]{.pre}
request meta (<a href="https://github.com/scrapy/scrapy/commit/37c24e01d7">commit
37c24e01d7</a>{.reference
.external})</p>
</li>
<li>
<p>Improve [<code>Request</code>{.docutils .literal .notranslate}]{.pre} class
documentation (<a href="https://github.com/scrapy/scrapy/issues/226">issue
226</a>{.reference
.external})
:::</p>
</li>
</ul>
<p>::: {#other .section}</p>
<h5 id="otherheaderlink"><a class="header" href="#otherheaderlink">Other<a href="#other" title="Permalink to this heading">Â¶</a>{.headerlink}</a></h5>
<ul>
<li>
<p>Dropped Python 2.6 support (<a href="https://github.com/scrapy/scrapy/issues/448">issue
448</a>{.reference
.external})</p>
</li>
<li>
<p>Add <a href="https://cssselect.readthedocs.io/en/latest/index.html" title="(in cssselect v1.2.0)">[cssselect]{.xref .std
.std-doc}</a>{.reference
.external} python package as install dependency</p>
</li>
<li>
<p>Drop libxml2 and multi selector's backend support,
<a href="https://lxml.de/">lxml</a>{.reference .external} is required from now
on.</p>
</li>
<li>
<p>Minimum Twisted version increased to 10.0.0, dropped Twisted 8.0
support.</p>
</li>
<li>
<p>Running test suite now requires [<code>mock</code>{.docutils .literal
.notranslate}]{.pre} python library (<a href="https://github.com/scrapy/scrapy/issues/390">issue
390</a>{.reference
.external})
:::</p>
</li>
</ul>
<p>::: {#thanks .section}</p>
<h5 id="thanksheaderlink"><a class="header" href="#thanksheaderlink">Thanks<a href="#thanks" title="Permalink to this heading">Â¶</a>{.headerlink}</a></h5>
<p>Thanks to everyone who contribute to this release!</p>
<p>List of contributors sorted by number of commits:</p>
<p>::: {.highlight-default .notranslate}
::: highlight
69 Daniel GraÃ±a &lt;dangra@...&gt;
37 Pablo Hoffman &lt;pablo@...&gt;
13 Mikhail Korobov &lt;kmike84@...&gt;
9 Alex Cepoi &lt;alex.cepoi@...&gt;
9 alexanderlukanin13 &lt;alexander.lukanin.13@...&gt;
8 Rolando Espinoza La fuente &lt;darkrho@...&gt;
8 Lukasz Biedrycki &lt;lukasz.biedrycki@...&gt;
6 Nicolas Ramirez &lt;nramirez.uy@...&gt;
3 Paul Tremberth &lt;paul.tremberth@...&gt;
2 Martin Olveyra &lt;molveyra@...&gt;
2 Stefan &lt;misc@...&gt;
2 Rolando Espinoza &lt;darkrho@...&gt;
2 Loren Davie &lt;loren@...&gt;
2 irgmedeiros &lt;irgmedeiros@...&gt;
1 Stefan Koch &lt;taikano@...&gt;
1 Stefan &lt;cct@...&gt;
1 scraperdragon &lt;dragon@...&gt;
1 Kumara Tharmalingam &lt;ktharmal@...&gt;
1 Francesco Piccinno &lt;stack.box@...&gt;
1 Marcos Campal &lt;duendex@...&gt;
1 Dragon Dave &lt;dragon@...&gt;
1 Capi Etheriel &lt;barraponto@...&gt;
1 cacovsky &lt;amarquesferraz@...&gt;
1 Berend Iwema &lt;berend@...&gt;
:::
:::
:::
:::</p>
<p>::: {#scrapy-0-18-4-released-2013-10-10 .section}</p>
<h4 id="scrapy-0184-released-2013-10-10headerlink"><a class="header" href="#scrapy-0184-released-2013-10-10headerlink">Scrapy 0.18.4 (released 2013-10-10)<a href="#scrapy-0-18-4-released-2013-10-10" title="Permalink to this heading">Â¶</a>{.headerlink}</a></h4>
<ul>
<li>
<p>IPython refuses to update the namespace. fix #396 (<a href="https://github.com/scrapy/scrapy/commit/3d32c4f">commit
3d32c4f</a>{.reference
.external})</p>
</li>
<li>
<p>Fix AlreadyCalledError replacing a request in shell command. closes
#407 (<a href="https://github.com/scrapy/scrapy/commit/b1d8919">commit
b1d8919</a>{.reference
.external})</p>
</li>
<li>
<p>Fix start_requests laziness and early hangs (<a href="https://github.com/scrapy/scrapy/commit/89faf52">commit
89faf52</a>{.reference
.external})
:::</p>
</li>
</ul>
<p>::: {#scrapy-0-18-3-released-2013-10-03 .section}</p>
<h4 id="scrapy-0183-released-2013-10-03headerlink"><a class="header" href="#scrapy-0183-released-2013-10-03headerlink">Scrapy 0.18.3 (released 2013-10-03)<a href="#scrapy-0-18-3-released-2013-10-03" title="Permalink to this heading">Â¶</a>{.headerlink}</a></h4>
<ul>
<li>
<p>fix regression on lazy evaluation of start requests (<a href="https://github.com/scrapy/scrapy/commit/12693a5">commit
12693a5</a>{.reference
.external})</p>
</li>
<li>
<p>forms: do not submit reset inputs (<a href="https://github.com/scrapy/scrapy/commit/e429f63">commit
e429f63</a>{.reference
.external})</p>
</li>
<li>
<p>increase unittest timeouts to decrease travis false positive
failures (<a href="https://github.com/scrapy/scrapy/commit/912202e">commit
912202e</a>{.reference
.external})</p>
</li>
<li>
<p>backport master fixes to json exporter (<a href="https://github.com/scrapy/scrapy/commit/cfc2d46">commit
cfc2d46</a>{.reference
.external})</p>
</li>
<li>
<p>Fix permission and set umask before generating sdist tarball
(<a href="https://github.com/scrapy/scrapy/commit/06149e0">commit
06149e0</a>{.reference
.external})
:::</p>
</li>
</ul>
<p>::: {#scrapy-0-18-2-released-2013-09-03 .section}</p>
<h4 id="scrapy-0182-released-2013-09-03headerlink"><a class="header" href="#scrapy-0182-released-2013-09-03headerlink">Scrapy 0.18.2 (released 2013-09-03)<a href="#scrapy-0-18-2-released-2013-09-03" title="Permalink to this heading">Â¶</a>{.headerlink}</a></h4>
<ul>
<li>Backport [<code>scrapy</code>{.docutils .literal
.notranslate}]{.pre}<code> </code>{.docutils .literal
.notranslate}[<code>check</code>{.docutils .literal .notranslate}]{.pre}
command fixes and backward compatible multi crawler process(<a href="https://github.com/scrapy/scrapy/issues/339">issue
339</a>{.reference
.external})
:::</li>
</ul>
<p>::: {#scrapy-0-18-1-released-2013-08-27 .section}</p>
<h4 id="scrapy-0181-released-2013-08-27headerlink"><a class="header" href="#scrapy-0181-released-2013-08-27headerlink">Scrapy 0.18.1 (released 2013-08-27)<a href="#scrapy-0-18-1-released-2013-08-27" title="Permalink to this heading">Â¶</a>{.headerlink}</a></h4>
<ul>
<li>
<p>remove extra import added by cherry picked changes (<a href="https://github.com/scrapy/scrapy/commit/d20304e">commit
d20304e</a>{.reference
.external})</p>
</li>
<li>
<p>fix crawling tests under twisted pre 11.0.0 (<a href="https://github.com/scrapy/scrapy/commit/1994f38">commit
1994f38</a>{.reference
.external})</p>
</li>
<li>
<p>py26 can not format zero length fields {} (<a href="https://github.com/scrapy/scrapy/commit/abf756f">commit
abf756f</a>{.reference
.external})</p>
</li>
<li>
<p>test PotentiaDataLoss errors on unbound responses (<a href="https://github.com/scrapy/scrapy/commit/b15470d">commit
b15470d</a>{.reference
.external})</p>
</li>
<li>
<p>Treat responses without content-length or Transfer-Encoding as good
responses (<a href="https://github.com/scrapy/scrapy/commit/c4bf324">commit
c4bf324</a>{.reference
.external})</p>
</li>
<li>
<p>do no include ResponseFailed if http11 handler is not enabled
(<a href="https://github.com/scrapy/scrapy/commit/6cbe684">commit
6cbe684</a>{.reference
.external})</p>
</li>
<li>
<p>New HTTP client wraps connection lost in ResponseFailed exception.
fix #373 (<a href="https://github.com/scrapy/scrapy/commit/1a20bba">commit
1a20bba</a>{.reference
.external})</p>
</li>
<li>
<p>limit travis-ci build matrix (<a href="https://github.com/scrapy/scrapy/commit/3b01bb8">commit
3b01bb8</a>{.reference
.external})</p>
</li>
<li>
<p>Merge pull request #375 from peterarenot/patch-1 (<a href="https://github.com/scrapy/scrapy/commit/fa766d7">commit
fa766d7</a>{.reference
.external})</p>
</li>
<li>
<p>Fixed so it refers to the correct folder (<a href="https://github.com/scrapy/scrapy/commit/3283809">commit
3283809</a>{.reference
.external})</p>
</li>
<li>
<p>added Quantal &amp; Raring to support Ubuntu releases (<a href="https://github.com/scrapy/scrapy/commit/1411923">commit
1411923</a>{.reference
.external})</p>
</li>
<li>
<p>fix retry middleware which didn't retry certain connection errors
after the upgrade to http1 client, closes GH-373 (<a href="https://github.com/scrapy/scrapy/commit/bb35ed0">commit
bb35ed0</a>{.reference
.external})</p>
</li>
<li>
<p>fix XmlItemExporter in Python 2.7.4 and 2.7.5 (<a href="https://github.com/scrapy/scrapy/commit/de3e451">commit
de3e451</a>{.reference
.external})</p>
</li>
<li>
<p>minor updates to 0.18 release notes (<a href="https://github.com/scrapy/scrapy/commit/c45e5f1">commit
c45e5f1</a>{.reference
.external})</p>
</li>
<li>
<p>fix contributors list format (<a href="https://github.com/scrapy/scrapy/commit/0b60031">commit
0b60031</a>{.reference
.external})
:::</p>
</li>
</ul>
<p>::: {#scrapy-0-18-0-released-2013-08-09 .section}</p>
<h4 id="scrapy-0180-released-2013-08-09headerlink"><a class="header" href="#scrapy-0180-released-2013-08-09headerlink">Scrapy 0.18.0 (released 2013-08-09)<a href="#scrapy-0-18-0-released-2013-08-09" title="Permalink to this heading">Â¶</a>{.headerlink}</a></h4>
<ul>
<li>
<p>Lot of improvements to testsuite run using Tox, including a way to
test on pypi</p>
</li>
<li>
<p>Handle GET parameters for AJAX crawlable urls (<a href="https://github.com/scrapy/scrapy/commit/3fe2a32">commit
3fe2a32</a>{.reference
.external})</p>
</li>
<li>
<p>Use lxml recover option to parse sitemaps (<a href="https://github.com/scrapy/scrapy/issues/347">issue
347</a>{.reference
.external})</p>
</li>
<li>
<p>Bugfix cookie merging by hostname and not by netloc (<a href="https://github.com/scrapy/scrapy/issues/352">issue
352</a>{.reference
.external})</p>
</li>
<li>
<p>Support disabling [<code>HttpCompressionMiddleware</code>{.docutils .literal
.notranslate}]{.pre} using a flag setting (<a href="https://github.com/scrapy/scrapy/issues/359">issue
359</a>{.reference
.external})</p>
</li>
<li>
<p>Support xml namespaces using [<code>iternodes</code>{.docutils .literal
.notranslate}]{.pre} parser in [<code>XMLFeedSpider</code>{.docutils .literal
.notranslate}]{.pre} (<a href="https://github.com/scrapy/scrapy/issues/12">issue
12</a>{.reference
.external})</p>
</li>
<li>
<p>Support [<code>dont_cache</code>{.docutils .literal .notranslate}]{.pre}
request meta flag (<a href="https://github.com/scrapy/scrapy/issues/19">issue
19</a>{.reference
.external})</p>
</li>
<li>
<p>Bugfix [<code>scrapy.utils.gz.gunzip</code>{.docutils .literal
.notranslate}]{.pre} broken by changes in python 2.7.4 (<a href="https://github.com/scrapy/scrapy/commit/4dc76e">commit
4dc76e</a>{.reference
.external})</p>
</li>
<li>
<p>Bugfix url encoding on [<code>SgmlLinkExtractor</code>{.docutils .literal
.notranslate}]{.pre} (<a href="https://github.com/scrapy/scrapy/issues/24">issue
24</a>{.reference
.external})</p>
</li>
<li>
<p>Bugfix [<code>TakeFirst</code>{.docutils .literal .notranslate}]{.pre}
processor shouldn't discard zero (0) value (<a href="https://github.com/scrapy/scrapy/issues/59">issue
59</a>{.reference
.external})</p>
</li>
<li>
<p>Support nested items in xml exporter (<a href="https://github.com/scrapy/scrapy/issues/66">issue
66</a>{.reference
.external})</p>
</li>
<li>
<p>Improve cookies handling performance (<a href="https://github.com/scrapy/scrapy/issues/77">issue
77</a>{.reference
.external})</p>
</li>
<li>
<p>Log dupe filtered requests once (<a href="https://github.com/scrapy/scrapy/issues/105">issue
105</a>{.reference
.external})</p>
</li>
<li>
<p>Split redirection middleware into status and meta based middlewares
(<a href="https://github.com/scrapy/scrapy/issues/78">issue 78</a>{.reference
.external})</p>
</li>
<li>
<p>Use HTTP1.1 as default downloader handler (<a href="https://github.com/scrapy/scrapy/issues/109">issue
109</a>{.reference
.external} and <a href="https://github.com/scrapy/scrapy/issues/318">issue
318</a>{.reference
.external})</p>
</li>
<li>
<p>Support xpath form selection on
[<code>FormRequest.from_response</code>{.docutils .literal .notranslate}]{.pre}
(<a href="https://github.com/scrapy/scrapy/issues/185">issue 185</a>{.reference
.external})</p>
</li>
<li>
<p>Bugfix unicode decoding error on [<code>SgmlLinkExtractor</code>{.docutils
.literal .notranslate}]{.pre} (<a href="https://github.com/scrapy/scrapy/issues/199">issue
199</a>{.reference
.external})</p>
</li>
<li>
<p>Bugfix signal dispatching on pypi interpreter (<a href="https://github.com/scrapy/scrapy/issues/205">issue
205</a>{.reference
.external})</p>
</li>
<li>
<p>Improve request delay and concurrency handling (<a href="https://github.com/scrapy/scrapy/issues/206">issue
206</a>{.reference
.external})</p>
</li>
<li>
<p>Add RFC2616 cache policy to [<code>HttpCacheMiddleware</code>{.docutils
.literal .notranslate}]{.pre} (<a href="https://github.com/scrapy/scrapy/issues/212">issue
212</a>{.reference
.external})</p>
</li>
<li>
<p>Allow customization of messages logged by engine (<a href="https://github.com/scrapy/scrapy/issues/214">issue
214</a>{.reference
.external})</p>
</li>
<li>
<p>Multiples improvements to [<code>DjangoItem</code>{.docutils .literal
.notranslate}]{.pre} (<a href="https://github.com/scrapy/scrapy/issues/217">issue
217</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/218">issue
218</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/221">issue
221</a>{.reference
.external})</p>
</li>
<li>
<p>Extend Scrapy commands using setuptools entry points (<a href="https://github.com/scrapy/scrapy/issues/260">issue
260</a>{.reference
.external})</p>
</li>
<li>
<p>Allow spider [<code>allowed_domains</code>{.docutils .literal
.notranslate}]{.pre} value to be set/tuple (<a href="https://github.com/scrapy/scrapy/issues/261">issue
261</a>{.reference
.external})</p>
</li>
<li>
<p>Support [<code>settings.getdict</code>{.docutils .literal .notranslate}]{.pre}
(<a href="https://github.com/scrapy/scrapy/issues/269">issue 269</a>{.reference
.external})</p>
</li>
<li>
<p>Simplify internal [<code>scrapy.core.scraper</code>{.docutils .literal
.notranslate}]{.pre} slot handling (<a href="https://github.com/scrapy/scrapy/issues/271">issue
271</a>{.reference
.external})</p>
</li>
<li>
<p>Added [<code>Item.copy</code>{.docutils .literal .notranslate}]{.pre} (<a href="https://github.com/scrapy/scrapy/issues/290">issue
290</a>{.reference
.external})</p>
</li>
<li>
<p>Collect idle downloader slots (<a href="https://github.com/scrapy/scrapy/issues/297">issue
297</a>{.reference
.external})</p>
</li>
<li>
<p>Add [<code>ftp://</code>{.docutils .literal .notranslate}]{.pre} scheme
downloader handler (<a href="https://github.com/scrapy/scrapy/issues/329">issue
329</a>{.reference
.external})</p>
</li>
<li>
<p>Added downloader benchmark webserver and spider tools
<a href="index.html#benchmarking">[Benchmarking]{.std .std-ref}</a>{.hoverxref
.tooltip .reference .internal}</p>
</li>
<li>
<p>Moved persistent (on disk) queues to a separate project
(<a href="https://github.com/scrapy/queuelib">queuelib</a>{.reference
.external}) which Scrapy now depends on</p>
</li>
<li>
<p>Add Scrapy commands using external libraries (<a href="https://github.com/scrapy/scrapy/issues/260">issue
260</a>{.reference
.external})</p>
</li>
<li>
<p>Added [<code>--pdb</code>{.docutils .literal .notranslate}]{.pre} option to
[<code>scrapy</code>{.docutils .literal .notranslate}]{.pre} command line tool</p>
</li>
<li>
<p>Added <a href="index.html#scrapy.selector.Selector.remove_namespaces" title="scrapy.selector.Selector.remove_namespaces">[<code>XPathSelector.remove_namespaces</code>{.xref .py .py-meth
.docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal} which allows to remove all namespaces from XML documents
for convenience (to work with namespace-less XPaths). Documented in
<a href="index.html#topics-selectors">[Selectors]{.std .std-ref}</a>{.hoverxref
.tooltip .reference .internal}.</p>
</li>
<li>
<p>Several improvements to spider contracts</p>
</li>
<li>
<p>New default middleware named MetaRefreshMiddleware that handles
meta-refresh html tag redirections,</p>
</li>
<li>
<p>MetaRefreshMiddleware and RedirectMiddleware have different
priorities to address #62</p>
</li>
<li>
<p>added from_crawler method to spiders</p>
</li>
<li>
<p>added system tests with mock server</p>
</li>
<li>
<p>more improvements to macOS compatibility (thanks Alex Cepoi)</p>
</li>
<li>
<p>several more cleanups to singletons and multi-spider support (thanks
Nicolas Ramirez)</p>
</li>
<li>
<p>support custom download slots</p>
</li>
<li>
<p>added --spider option to &quot;shell&quot; command.</p>
</li>
<li>
<p>log overridden settings when Scrapy starts</p>
</li>
</ul>
<p>Thanks to everyone who contribute to this release. Here is a list of
contributors sorted by number of commits:</p>
<p>::: {.highlight-default .notranslate}
::: highlight
130 Pablo Hoffman &lt;pablo@...&gt;
97 Daniel GraÃ±a &lt;dangra@...&gt;
20 NicolÃ¡s RamÃ­rez &lt;nramirez.uy@...&gt;
13 Mikhail Korobov &lt;kmike84@...&gt;
12 Pedro Faustino &lt;pedrobandim@...&gt;
11 Steven Almeroth &lt;sroth77@...&gt;
5 Rolando Espinoza La fuente &lt;darkrho@...&gt;
4 Michal Danilak &lt;mimino.coder@...&gt;
4 Alex Cepoi &lt;alex.cepoi@...&gt;
4 Alexandr N Zamaraev (aka tonal) &lt;tonal@...&gt;
3 paul &lt;paul.tremberth@...&gt;
3 Martin Olveyra &lt;molveyra@...&gt;
3 Jordi Llonch &lt;llonchj@...&gt;
3 arijitchakraborty &lt;myself.arijit@...&gt;
2 Shane Evans &lt;shane.evans@...&gt;
2 joehillen &lt;joehillen@...&gt;
2 Hart &lt;HartSimha@...&gt;
2 Dan &lt;ellisd23@...&gt;
1 Zuhao Wan &lt;wanzuhao@...&gt;
1 whodatninja &lt;blake@...&gt;
1 vkrest &lt;v.krestiannykov@...&gt;
1 tpeng &lt;pengtaoo@...&gt;
1 Tom Mortimer-Jones &lt;tom@...&gt;
1 Rocio Aramberri &lt;roschegel@...&gt;
1 Pedro &lt;pedro@...&gt;
1 notsobad &lt;wangxiaohugg@...&gt;
1 Natan L &lt;kuyanatan.nlao@...&gt;
1 Mark Grey &lt;mark.grey@...&gt;
1 Luan &lt;luanpab@...&gt;
1 Libor NenadÃ¡l &lt;libor.nenadal@...&gt;
1 Juan M Uys &lt;opyate@...&gt;
1 Jonas Brunsgaard &lt;jonas.brunsgaard@...&gt;
1 Ilya Baryshev &lt;baryshev@...&gt;
1 Hasnain Lakhani &lt;m.hasnain.lakhani@...&gt;
1 Emanuel Schorsch &lt;emschorsch@...&gt;
1 Chris Tilden &lt;chris.tilden@...&gt;
1 Capi Etheriel &lt;barraponto@...&gt;
1 cacovsky &lt;amarquesferraz@...&gt;
1 Berend Iwema &lt;berend@...&gt;
:::
:::
:::</p>
<p>::: {#scrapy-0-16-5-released-2013-05-30 .section}</p>
<h4 id="scrapy-0165-released-2013-05-30headerlink"><a class="header" href="#scrapy-0165-released-2013-05-30headerlink">Scrapy 0.16.5 (released 2013-05-30)<a href="#scrapy-0-16-5-released-2013-05-30" title="Permalink to this heading">Â¶</a>{.headerlink}</a></h4>
<ul>
<li>
<p>obey request method when Scrapy deploy is redirected to a new
endpoint (<a href="https://github.com/scrapy/scrapy/commit/8c4fcee">commit
8c4fcee</a>{.reference
.external})</p>
</li>
<li>
<p>fix inaccurate downloader middleware documentation. refs #280
(<a href="https://github.com/scrapy/scrapy/commit/40667cb">commit
40667cb</a>{.reference
.external})</p>
</li>
<li>
<p>doc: remove links to diveintopython.org, which is no longer
available. closes #246 (<a href="https://github.com/scrapy/scrapy/commit/bd58bfa">commit
bd58bfa</a>{.reference
.external})</p>
</li>
<li>
<p>Find form nodes in invalid html5 documents (<a href="https://github.com/scrapy/scrapy/commit/e3d6945">commit
e3d6945</a>{.reference
.external})</p>
</li>
<li>
<p>Fix typo labeling attrs type bool instead of list (<a href="https://github.com/scrapy/scrapy/commit/a274276">commit
a274276</a>{.reference
.external})
:::</p>
</li>
</ul>
<p>::: {#scrapy-0-16-4-released-2013-01-23 .section}</p>
<h4 id="scrapy-0164-released-2013-01-23headerlink"><a class="header" href="#scrapy-0164-released-2013-01-23headerlink">Scrapy 0.16.4 (released 2013-01-23)<a href="#scrapy-0-16-4-released-2013-01-23" title="Permalink to this heading">Â¶</a>{.headerlink}</a></h4>
<ul>
<li>
<p>fixes spelling errors in documentation (<a href="https://github.com/scrapy/scrapy/commit/6d2b3aa">commit
6d2b3aa</a>{.reference
.external})</p>
</li>
<li>
<p>add doc about disabling an extension. refs #132 (<a href="https://github.com/scrapy/scrapy/commit/c90de33">commit
c90de33</a>{.reference
.external})</p>
</li>
<li>
<p>Fixed error message formatting. log.err() doesn't support cool
formatting and when error occurred, the message was: &quot;ERROR: Error
processing %(item)s&quot; (<a href="https://github.com/scrapy/scrapy/commit/c16150c">commit
c16150c</a>{.reference
.external})</p>
</li>
<li>
<p>lint and improve images pipeline error logging (<a href="https://github.com/scrapy/scrapy/commit/56b45fc">commit
56b45fc</a>{.reference
.external})</p>
</li>
<li>
<p>fixed doc typos (<a href="https://github.com/scrapy/scrapy/commit/243be84">commit
243be84</a>{.reference
.external})</p>
</li>
<li>
<p>add documentation topics: Broad Crawls &amp; Common Practices (<a href="https://github.com/scrapy/scrapy/commit/1fbb715">commit
1fbb715</a>{.reference
.external})</p>
</li>
<li>
<p>fix bug in Scrapy parse command when spider is not specified
explicitly. closes #209 (<a href="https://github.com/scrapy/scrapy/commit/c72e682">commit
c72e682</a>{.reference
.external})</p>
</li>
<li>
<p>Update docs/topics/commands.rst (<a href="https://github.com/scrapy/scrapy/commit/28eac7a">commit
28eac7a</a>{.reference
.external})
:::</p>
</li>
</ul>
<p>::: {#scrapy-0-16-3-released-2012-12-07 .section}</p>
<h4 id="scrapy-0163-released-2012-12-07headerlink"><a class="header" href="#scrapy-0163-released-2012-12-07headerlink">Scrapy 0.16.3 (released 2012-12-07)<a href="#scrapy-0-16-3-released-2012-12-07" title="Permalink to this heading">Â¶</a>{.headerlink}</a></h4>
<ul>
<li>
<p>Remove concurrency limitation when using download delays and still
ensure inter-request delays are enforced (<a href="https://github.com/scrapy/scrapy/commit/487b9b5">commit
487b9b5</a>{.reference
.external})</p>
</li>
<li>
<p>add error details when image pipeline fails (<a href="https://github.com/scrapy/scrapy/commit/8232569">commit
8232569</a>{.reference
.external})</p>
</li>
<li>
<p>improve macOS compatibility (<a href="https://github.com/scrapy/scrapy/commit/8dcf8aa">commit
8dcf8aa</a>{.reference
.external})</p>
</li>
<li>
<p>setup.py: use README.rst to populate long_description (<a href="https://github.com/scrapy/scrapy/commit/7b5310d">commit
7b5310d</a>{.reference
.external})</p>
</li>
<li>
<p>doc: removed obsolete references to ClientForm (<a href="https://github.com/scrapy/scrapy/commit/80f9bb6">commit
80f9bb6</a>{.reference
.external})</p>
</li>
<li>
<p>correct docs for default storage backend (<a href="https://github.com/scrapy/scrapy/commit/2aa491b">commit
2aa491b</a>{.reference
.external})</p>
</li>
<li>
<p>doc: removed broken proxyhub link from FAQ (<a href="https://github.com/scrapy/scrapy/commit/bdf61c4">commit
bdf61c4</a>{.reference
.external})</p>
</li>
<li>
<p>Fixed docs typo in SpiderOpenCloseLogging example (<a href="https://github.com/scrapy/scrapy/commit/7184094">commit
7184094</a>{.reference
.external})
:::</p>
</li>
</ul>
<p>::: {#scrapy-0-16-2-released-2012-11-09 .section}</p>
<h4 id="scrapy-0162-released-2012-11-09headerlink"><a class="header" href="#scrapy-0162-released-2012-11-09headerlink">Scrapy 0.16.2 (released 2012-11-09)<a href="#scrapy-0-16-2-released-2012-11-09" title="Permalink to this heading">Â¶</a>{.headerlink}</a></h4>
<ul>
<li>
<p>Scrapy contracts: python2.6 compat (<a href="https://github.com/scrapy/scrapy/commit/a4a9199">commit
a4a9199</a>{.reference
.external})</p>
</li>
<li>
<p>Scrapy contracts verbose option (<a href="https://github.com/scrapy/scrapy/commit/ec41673">commit
ec41673</a>{.reference
.external})</p>
</li>
<li>
<p>proper unittest-like output for Scrapy contracts (<a href="https://github.com/scrapy/scrapy/commit/86635e4">commit
86635e4</a>{.reference
.external})</p>
</li>
<li>
<p>added open_in_browser to debugging doc (<a href="https://github.com/scrapy/scrapy/commit/c9b690d">commit
c9b690d</a>{.reference
.external})</p>
</li>
<li>
<p>removed reference to global Scrapy stats from settings doc (<a href="https://github.com/scrapy/scrapy/commit/dd55067">commit
dd55067</a>{.reference
.external})</p>
</li>
<li>
<p>Fix SpiderState bug in Windows platforms (<a href="https://github.com/scrapy/scrapy/commit/58998f4">commit
58998f4</a>{.reference
.external})
:::</p>
</li>
</ul>
<p>::: {#scrapy-0-16-1-released-2012-10-26 .section}</p>
<h4 id="scrapy-0161-released-2012-10-26headerlink"><a class="header" href="#scrapy-0161-released-2012-10-26headerlink">Scrapy 0.16.1 (released 2012-10-26)<a href="#scrapy-0-16-1-released-2012-10-26" title="Permalink to this heading">Â¶</a>{.headerlink}</a></h4>
<ul>
<li>
<p>fixed LogStats extension, which got broken after a wrong merge
before the 0.16 release (<a href="https://github.com/scrapy/scrapy/commit/8c780fd">commit
8c780fd</a>{.reference
.external})</p>
</li>
<li>
<p>better backward compatibility for scrapy.conf.settings (<a href="https://github.com/scrapy/scrapy/commit/3403089">commit
3403089</a>{.reference
.external})</p>
</li>
<li>
<p>extended documentation on how to access crawler stats from
extensions (<a href="https://github.com/scrapy/scrapy/commit/c4da0b5">commit
c4da0b5</a>{.reference
.external})</p>
</li>
<li>
<p>removed .hgtags (no longer needed now that Scrapy uses git) (<a href="https://github.com/scrapy/scrapy/commit/d52c188">commit
d52c188</a>{.reference
.external})</p>
</li>
<li>
<p>fix dashes under rst headers (<a href="https://github.com/scrapy/scrapy/commit/fa4f7f9">commit
fa4f7f9</a>{.reference
.external})</p>
</li>
<li>
<p>set release date for 0.16.0 in news (<a href="https://github.com/scrapy/scrapy/commit/e292246">commit
e292246</a>{.reference
.external})
:::</p>
</li>
</ul>
<p>::: {#scrapy-0-16-0-released-2012-10-18 .section}</p>
<h4 id="scrapy-0160-released-2012-10-18headerlink"><a class="header" href="#scrapy-0160-released-2012-10-18headerlink">Scrapy 0.16.0 (released 2012-10-18)<a href="#scrapy-0-16-0-released-2012-10-18" title="Permalink to this heading">Â¶</a>{.headerlink}</a></h4>
<p>Scrapy changes:</p>
<ul>
<li>
<p>added <a href="index.html#topics-contracts">[Spiders Contracts]{.std
.std-ref}</a>{.hoverxref .tooltip
.reference .internal}, a mechanism for testing spiders in a
formal/reproducible way</p>
</li>
<li>
<p>added options [<code>-o</code>{.docutils .literal .notranslate}]{.pre} and
[<code>-t</code>{.docutils .literal .notranslate}]{.pre} to the
<a href="index.html#std-command-runspider">[<code>runspider</code>{.xref .std .std-command .docutils .literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal} command</p>
</li>
<li>
<p>documented <a href="index.html#document-topics/autothrottle">[AutoThrottle
extension]{.doc}</a>{.reference
.internal} and added to extensions installed by default. You still
need to enable it with <a href="index.html#std-setting-AUTOTHROTTLE_ENABLED">[<code>AUTOTHROTTLE_ENABLED</code>{.xref .std
.std-setting .docutils .literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal}</p>
</li>
<li>
<p>major Stats Collection refactoring: removed separation of
global/per-spider stats, removed stats-related signals
([<code>stats_spider_opened</code>{.docutils .literal .notranslate}]{.pre},
etc). Stats are much simpler now, backward compatibility is kept on
the Stats Collector API and signals.</p>
</li>
<li>
<p>added <a href="index.html#scrapy.spidermiddlewares.SpiderMiddleware.process_start_requests" title="scrapy.spidermiddlewares.SpiderMiddleware.process_start_requests">[<code>process_start_requests()</code>{.xref .py .py-meth .docutils
.literal
.notranslate}]{.pre}</a>{.reference
.internal} method to spider middlewares</p>
</li>
<li>
<p>dropped Signals singleton. Signals should now be accessed through
the Crawler.signals attribute. See the signals documentation for
more info.</p>
</li>
<li>
<p>dropped Stats Collector singleton. Stats can now be accessed through
the Crawler.stats attribute. See the stats collection documentation
for more info.</p>
</li>
<li>
<p>documented <a href="index.html#topics-api">[Core API]{.std
.std-ref}</a>{.hoverxref .tooltip .reference
.internal}</p>
</li>
<li>
<p>[<code>lxml</code>{.docutils .literal .notranslate}]{.pre} is now the default
selectors backend instead of [<code>libxml2</code>{.docutils .literal
.notranslate}]{.pre}</p>
</li>
<li>
<p>ported FormRequest.from_response() to use
<a href="https://lxml.de/">lxml</a>{.reference .external} instead of
<a href="http://wwwsearch.sourceforge.net/old/ClientForm/">ClientForm</a>{.reference
.external}</p>
</li>
<li>
<p>removed modules: [<code>scrapy.xlib.BeautifulSoup</code>{.docutils .literal
.notranslate}]{.pre} and [<code>scrapy.xlib.ClientForm</code>{.docutils
.literal .notranslate}]{.pre}</p>
</li>
<li>
<p>SitemapSpider: added support for sitemap urls ending in .xml and
.xml.gz, even if they advertise a wrong content type (<a href="https://github.com/scrapy/scrapy/commit/10ed28b">commit
10ed28b</a>{.reference
.external})</p>
</li>
<li>
<p>StackTraceDump extension: also dump trackref live references
(<a href="https://github.com/scrapy/scrapy/commit/fe2ce93">commit
fe2ce93</a>{.reference
.external})</p>
</li>
<li>
<p>nested items now fully supported in JSON and JSONLines exporters</p>
</li>
<li>
<p>added <a href="index.html#std-reqmeta-cookiejar">[<code>cookiejar</code>{.xref .std .std-reqmeta .docutils .literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal} Request meta key to support multiple
cookie sessions per spider</p>
</li>
<li>
<p>decoupled encoding detection code to
<a href="https://github.com/scrapy/w3lib/blob/master/w3lib/encoding.py">w3lib.encoding</a>{.reference
.external}, and ported Scrapy code to use that module</p>
</li>
<li>
<p>dropped support for Python 2.5. See
<a href="https://blog.scrapinghub.com/2012/02/27/scrapy-0-15-dropping-support-for-python-2-5/">https://blog.scrapinghub.com/2012/02/27/scrapy-0-15-dropping-support-for-python-2-5/</a>{.reference
.external}</p>
</li>
<li>
<p>dropped support for Twisted 2.5</p>
</li>
<li>
<p>added <a href="index.html#std-setting-REFERER_ENABLED">[<code>REFERER_ENABLED</code>{.xref .std .std-setting .docutils .literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal} setting, to control referer
middleware</p>
</li>
<li>
<p>changed default user agent to: [<code>Scrapy/VERSION</code>{.docutils .literal
.notranslate}]{.pre}<code> </code>{.docutils .literal
.notranslate}[<code>(+http://scrapy.org)</code>{.docutils .literal
.notranslate}]{.pre}</p>
</li>
<li>
<p>removed (undocumented) [<code>HTMLImageLinkExtractor</code>{.docutils .literal
.notranslate}]{.pre} class from
[<code>scrapy.contrib.linkextractors.image</code>{.docutils .literal
.notranslate}]{.pre}</p>
</li>
<li>
<p>removed per-spider settings (to be replaced by instantiating
multiple crawler objects)</p>
</li>
<li>
<p>[<code>USER_AGENT</code>{.docutils .literal .notranslate}]{.pre} spider
attribute will no longer work, use [<code>user_agent</code>{.docutils .literal
.notranslate}]{.pre} attribute instead</p>
</li>
<li>
<p>[<code>DOWNLOAD_TIMEOUT</code>{.docutils .literal .notranslate}]{.pre} spider
attribute will no longer work, use [<code>download_timeout</code>{.docutils
.literal .notranslate}]{.pre} attribute instead</p>
</li>
<li>
<p>removed [<code>ENCODING_ALIASES</code>{.docutils .literal .notranslate}]{.pre}
setting, as encoding auto-detection has been moved to the
<a href="https://github.com/scrapy/w3lib">w3lib</a>{.reference .external}
library</p>
</li>
<li>
<p>promoted <a href="index.html#topics-djangoitem">[DjangoItem]{.std
.std-ref}</a>{.hoverxref .tooltip
.reference .internal} to main contrib</p>
</li>
<li>
<p>LogFormatter method now return dicts(instead of strings) to support
lazy formatting (<a href="https://github.com/scrapy/scrapy/issues/164">issue
164</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/commit/dcef7b0">commit
dcef7b0</a>{.reference
.external})</p>
</li>
<li>
<p>downloader handlers (<a href="index.html#std-setting-DOWNLOAD_HANDLERS">[<code>DOWNLOAD_HANDLERS</code>{.xref .std .std-setting
.docutils .literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal} setting) now receive settings as the
first argument of the [<code>__init__</code>{.docutils .literal
.notranslate}]{.pre} method</p>
</li>
<li>
<p>replaced memory usage accounting with (more portable)
<a href="https://docs.python.org/2/library/resource.html">resource</a>{.reference
.external} module, removed [<code>scrapy.utils.memory</code>{.docutils .literal
.notranslate}]{.pre} module</p>
</li>
<li>
<p>removed signal: [<code>scrapy.mail.mail_sent</code>{.docutils .literal
.notranslate}]{.pre}</p>
</li>
<li>
<p>removed [<code>TRACK_REFS</code>{.docutils .literal .notranslate}]{.pre}
setting, now <a href="index.html#topics-leaks-trackrefs">[trackrefs]{.std
.std-ref}</a>{.hoverxref .tooltip
.reference .internal} is always enabled</p>
</li>
<li>
<p>DBM is now the default storage backend for HTTP cache middleware</p>
</li>
<li>
<p>number of log messages (per level) are now tracked through Scrapy
stats (stat name: [<code>log_count/LEVEL</code>{.docutils .literal
.notranslate}]{.pre})</p>
</li>
<li>
<p>number received responses are now tracked through Scrapy stats (stat
name: [<code>response_received_count</code>{.docutils .literal
.notranslate}]{.pre})</p>
</li>
<li>
<p>removed [<code>scrapy.log.started</code>{.docutils .literal
.notranslate}]{.pre} attribute
:::</p>
</li>
</ul>
<p>::: {#scrapy-0-14-4 .section}</p>
<h4 id="scrapy-0144headerlink"><a class="header" href="#scrapy-0144headerlink">Scrapy 0.14.4<a href="#scrapy-0-14-4" title="Permalink to this heading">Â¶</a>{.headerlink}</a></h4>
<ul>
<li>
<p>added precise to supported Ubuntu distros (<a href="https://github.com/scrapy/scrapy/commit/b7e46df">commit
b7e46df</a>{.reference
.external})</p>
</li>
<li>
<p>fixed bug in json-rpc webservice reported in
<a href="https://groups.google.com/forum/#!topic/scrapy-users/qgVBmFybNAQ/discussion">https://groups.google.com/forum/#!topic/scrapy-users/qgVBmFybNAQ/discussion</a>{.reference
.external}. also removed no longer supported 'run' command from
extras/scrapy-ws.py (<a href="https://github.com/scrapy/scrapy/commit/340fbdb">commit
340fbdb</a>{.reference
.external})</p>
</li>
<li>
<p>meta tag attributes for content-type http equiv can be in any order.
#123 (<a href="https://github.com/scrapy/scrapy/commit/0cb68af">commit
0cb68af</a>{.reference
.external})</p>
</li>
<li>
<p>replace &quot;import Image&quot; by more standard &quot;from PIL import Image&quot;.
closes #88 (<a href="https://github.com/scrapy/scrapy/commit/4d17048">commit
4d17048</a>{.reference
.external})</p>
</li>
<li>
<p>return trial status as bin/runtests.sh exit value. #118 (<a href="https://github.com/scrapy/scrapy/commit/b7b2e7f">commit
b7b2e7f</a>{.reference
.external})
:::</p>
</li>
</ul>
<p>::: {#scrapy-0-14-3 .section}</p>
<h4 id="scrapy-0143headerlink"><a class="header" href="#scrapy-0143headerlink">Scrapy 0.14.3<a href="#scrapy-0-14-3" title="Permalink to this heading">Â¶</a>{.headerlink}</a></h4>
<ul>
<li>
<p>forgot to include pydispatch license. #118 (<a href="https://github.com/scrapy/scrapy/commit/fd85f9c">commit
fd85f9c</a>{.reference
.external})</p>
</li>
<li>
<p>include egg files used by testsuite in source distribution. #118
(<a href="https://github.com/scrapy/scrapy/commit/c897793">commit
c897793</a>{.reference
.external})</p>
</li>
<li>
<p>update docstring in project template to avoid confusion with
genspider command, which may be considered as an advanced feature.
refs #107 (<a href="https://github.com/scrapy/scrapy/commit/2548dcc">commit
2548dcc</a>{.reference
.external})</p>
</li>
<li>
<p>added note to docs/topics/firebug.rst about google directory being
shut down (<a href="https://github.com/scrapy/scrapy/commit/668e352">commit
668e352</a>{.reference
.external})</p>
</li>
<li>
<p>don't discard slot when empty, just save in another dict in order to
recycle if needed again. (<a href="https://github.com/scrapy/scrapy/commit/8e9f607">commit
8e9f607</a>{.reference
.external})</p>
</li>
<li>
<p>do not fail handling unicode xpaths in libxml2 backed selectors
(<a href="https://github.com/scrapy/scrapy/commit/b830e95">commit
b830e95</a>{.reference
.external})</p>
</li>
<li>
<p>fixed minor mistake in Request objects documentation (<a href="https://github.com/scrapy/scrapy/commit/bf3c9ee">commit
bf3c9ee</a>{.reference
.external})</p>
</li>
<li>
<p>fixed minor defect in link extractors documentation (<a href="https://github.com/scrapy/scrapy/commit/ba14f38">commit
ba14f38</a>{.reference
.external})</p>
</li>
<li>
<p>removed some obsolete remaining code related to sqlite support in
Scrapy (<a href="https://github.com/scrapy/scrapy/commit/0665175">commit
0665175</a>{.reference
.external})
:::</p>
</li>
</ul>
<p>::: {#scrapy-0-14-2 .section}</p>
<h4 id="scrapy-0142headerlink"><a class="header" href="#scrapy-0142headerlink">Scrapy 0.14.2<a href="#scrapy-0-14-2" title="Permalink to this heading">Â¶</a>{.headerlink}</a></h4>
<ul>
<li>
<p>move buffer pointing to start of file before computing checksum.
refs #92 (<a href="https://github.com/scrapy/scrapy/commit/6a5bef2">commit
6a5bef2</a>{.reference
.external})</p>
</li>
<li>
<p>Compute image checksum before persisting images. closes #92 (<a href="https://github.com/scrapy/scrapy/commit/9817df1">commit
9817df1</a>{.reference
.external})</p>
</li>
<li>
<p>remove leaking references in cached failures (<a href="https://github.com/scrapy/scrapy/commit/673a120">commit
673a120</a>{.reference
.external})</p>
</li>
<li>
<p>fixed bug in MemoryUsage extension: get_engine_status() takes
exactly 1 argument (0 given) (<a href="https://github.com/scrapy/scrapy/commit/11133e9">commit
11133e9</a>{.reference
.external})</p>
</li>
<li>
<p>fixed struct.error on http compression middleware. closes #87
(<a href="https://github.com/scrapy/scrapy/commit/1423140">commit
1423140</a>{.reference
.external})</p>
</li>
<li>
<p>ajax crawling wasn't expanding for unicode urls (<a href="https://github.com/scrapy/scrapy/commit/0de3fb4">commit
0de3fb4</a>{.reference
.external})</p>
</li>
<li>
<p>Catch start_requests iterator errors. refs #83 (<a href="https://github.com/scrapy/scrapy/commit/454a21d">commit
454a21d</a>{.reference
.external})</p>
</li>
<li>
<p>Speed-up libxml2 XPathSelector (<a href="https://github.com/scrapy/scrapy/commit/2fbd662">commit
2fbd662</a>{.reference
.external})</p>
</li>
<li>
<p>updated versioning doc according to recent changes (<a href="https://github.com/scrapy/scrapy/commit/0a070f5">commit
0a070f5</a>{.reference
.external})</p>
</li>
<li>
<p>scrapyd: fixed documentation link (<a href="https://github.com/scrapy/scrapy/commit/2b4e4c3">commit
2b4e4c3</a>{.reference
.external})</p>
</li>
<li>
<p>extras/makedeb.py: no longer obtaining version from git (<a href="https://github.com/scrapy/scrapy/commit/caffe0e">commit
caffe0e</a>{.reference
.external})
:::</p>
</li>
</ul>
<p>::: {#scrapy-0-14-1 .section}</p>
<h4 id="scrapy-0141headerlink"><a class="header" href="#scrapy-0141headerlink">Scrapy 0.14.1<a href="#scrapy-0-14-1" title="Permalink to this heading">Â¶</a>{.headerlink}</a></h4>
<ul>
<li>
<p>extras/makedeb.py: no longer obtaining version from git (<a href="https://github.com/scrapy/scrapy/commit/caffe0e">commit
caffe0e</a>{.reference
.external})</p>
</li>
<li>
<p>bumped version to 0.14.1 (<a href="https://github.com/scrapy/scrapy/commit/6cb9e1c">commit
6cb9e1c</a>{.reference
.external})</p>
</li>
<li>
<p>fixed reference to tutorial directory (<a href="https://github.com/scrapy/scrapy/commit/4b86bd6">commit
4b86bd6</a>{.reference
.external})</p>
</li>
<li>
<p>doc: removed duplicated callback argument from Request.replace()
(<a href="https://github.com/scrapy/scrapy/commit/1aeccdd">commit
1aeccdd</a>{.reference
.external})</p>
</li>
<li>
<p>fixed formatting of scrapyd doc (<a href="https://github.com/scrapy/scrapy/commit/8bf19e6">commit
8bf19e6</a>{.reference
.external})</p>
</li>
<li>
<p>Dump stacks for all running threads and fix engine status dumped by
StackTraceDump extension (<a href="https://github.com/scrapy/scrapy/commit/14a8e6e">commit
14a8e6e</a>{.reference
.external})</p>
</li>
<li>
<p>added comment about why we disable ssl on boto images upload
(<a href="https://github.com/scrapy/scrapy/commit/5223575">commit
5223575</a>{.reference
.external})</p>
</li>
<li>
<p>SSL handshaking hangs when doing too many parallel connections to S3
(<a href="https://github.com/scrapy/scrapy/commit/63d583d">commit
63d583d</a>{.reference
.external})</p>
</li>
<li>
<p>change tutorial to follow changes on dmoz site (<a href="https://github.com/scrapy/scrapy/commit/bcb3198">commit
bcb3198</a>{.reference
.external})</p>
</li>
<li>
<p>Avoid _disconnectedDeferred AttributeError exception in
Twisted&gt;=11.1.0 (<a href="https://github.com/scrapy/scrapy/commit/98f3f87">commit
98f3f87</a>{.reference
.external})</p>
</li>
<li>
<p>allow spider to set autothrottle max concurrency (<a href="https://github.com/scrapy/scrapy/commit/175a4b5">commit
175a4b5</a>{.reference
.external})
:::</p>
</li>
</ul>
<p>::: {#scrapy-0-14 .section}</p>
<h4 id="scrapy-014headerlink"><a class="header" href="#scrapy-014headerlink">Scrapy 0.14<a href="#scrapy-0-14" title="Permalink to this heading">Â¶</a>{.headerlink}</a></h4>
<p>::: {#new-features-and-settings .section}</p>
<h5 id="new-features-and-settingsheaderlink"><a class="header" href="#new-features-and-settingsheaderlink">New features and settings<a href="#new-features-and-settings" title="Permalink to this heading">Â¶</a>{.headerlink}</a></h5>
<ul>
<li>
<p>Support for <a href="https://developers.google.com/search/docs/ajax-crawling/docs/getting-started?csw=1">AJAX crawlable
urls</a>{.reference
.external}</p>
</li>
<li>
<p>New persistent scheduler that stores requests on disk, allowing to
suspend and resume crawls
(<a href="http://hg.scrapy.org/scrapy/changeset/2737">r2737</a>{.reference
.external})</p>
</li>
<li>
<p>added [<code>-o</code>{.docutils .literal .notranslate}]{.pre} option to
[<code>scrapy</code>{.docutils .literal .notranslate}]{.pre}<code> </code>{.docutils
.literal .notranslate}[<code>crawl</code>{.docutils .literal
.notranslate}]{.pre}, a shortcut for dumping scraped items into a
file (or standard output using [<code>-</code>{.docutils .literal
.notranslate}]{.pre})</p>
</li>
<li>
<p>Added support for passing custom settings to Scrapyd
[<code>schedule.json</code>{.docutils .literal .notranslate}]{.pre} api
(<a href="http://hg.scrapy.org/scrapy/changeset/2779">r2779</a>{.reference
.external},
<a href="http://hg.scrapy.org/scrapy/changeset/2783">r2783</a>{.reference
.external})</p>
</li>
<li>
<p>New [<code>ChunkedTransferMiddleware</code>{.docutils .literal
.notranslate}]{.pre} (enabled by default) to support <a href="https://en.wikipedia.org/wiki/Chunked_transfer_encoding">chunked
transfer
encoding</a>{.reference
.external}
(<a href="http://hg.scrapy.org/scrapy/changeset/2769">r2769</a>{.reference
.external})</p>
</li>
<li>
<p>Add boto 2.0 support for S3 downloader handler
(<a href="http://hg.scrapy.org/scrapy/changeset/2763">r2763</a>{.reference
.external})</p>
</li>
<li>
<p>Added
<a href="https://docs.python.org/2/library/marshal.html">marshal</a>{.reference
.external} to formats supported by feed exports
(<a href="http://hg.scrapy.org/scrapy/changeset/2744">r2744</a>{.reference
.external})</p>
</li>
<li>
<p>In request errbacks, offending requests are now received in
[<code>failure.request</code>{.docutils .literal .notranslate}]{.pre} attribute
(<a href="http://hg.scrapy.org/scrapy/changeset/2738">r2738</a>{.reference
.external})</p>
</li>
<li></li>
</ul>
<pre><code>Big downloader refactoring to support per domain/ip concurrency limits ([r2732](http://hg.scrapy.org/scrapy/changeset/2732){.reference .external})

:   -   

        [`CONCURRENT_REQUESTS_PER_SPIDER`{.docutils .literal .notranslate}]{.pre} setting has been deprecated and replaced by:

        :   -   [[`CONCURRENT_REQUESTS`{.xref .std .std-setting
                .docutils .literal
                .notranslate}]{.pre}](index.html#std-setting-CONCURRENT_REQUESTS){.hoverxref
                .tooltip .reference .internal},
                [[`CONCURRENT_REQUESTS_PER_DOMAIN`{.xref .std
                .std-setting .docutils .literal
                .notranslate}]{.pre}](index.html#std-setting-CONCURRENT_REQUESTS_PER_DOMAIN){.hoverxref
                .tooltip .reference .internal},
                [[`CONCURRENT_REQUESTS_PER_IP`{.xref .std
                .std-setting .docutils .literal
                .notranslate}]{.pre}](index.html#std-setting-CONCURRENT_REQUESTS_PER_IP){.hoverxref
                .tooltip .reference .internal}

    -   check the documentation for more details
</code></pre>
<ul>
<li>
<p>Added builtin caching DNS resolver
(<a href="http://hg.scrapy.org/scrapy/changeset/2728">r2728</a>{.reference
.external})</p>
</li>
<li>
<p>Moved Amazon AWS-related components/extensions (SQS spider queue,
SimpleDB stats collector) to a separate project:
[scaws](<a href="https://github.com/scrapinghub/scaws">https://github.com/scrapinghub/scaws</a>{.reference
.external})
(<a href="http://hg.scrapy.org/scrapy/changeset/2706">r2706</a>{.reference
.external},
<a href="http://hg.scrapy.org/scrapy/changeset/2714">r2714</a>{.reference
.external})</p>
</li>
<li>
<p>Moved spider queues to scrapyd: [<code>scrapy.spiderqueue</code>{.docutils
.literal .notranslate}]{.pre} -&gt; [<code>scrapyd.spiderqueue</code>{.docutils
.literal .notranslate}]{.pre}
(<a href="http://hg.scrapy.org/scrapy/changeset/2708">r2708</a>{.reference
.external})</p>
</li>
<li>
<p>Moved sqlite utils to scrapyd: [<code>scrapy.utils.sqlite</code>{.docutils
.literal .notranslate}]{.pre} -&gt; [<code>scrapyd.sqlite</code>{.docutils
.literal .notranslate}]{.pre}
(<a href="http://hg.scrapy.org/scrapy/changeset/2781">r2781</a>{.reference
.external})</p>
</li>
<li>
<p>Real support for returning iterators on
[<code>start_requests()</code>{.docutils .literal .notranslate}]{.pre} method.
The iterator is now consumed during the crawl when the spider is
getting idle
(<a href="http://hg.scrapy.org/scrapy/changeset/2704">r2704</a>{.reference
.external})</p>
</li>
<li>
<p>Added <a href="index.html#std-setting-REDIRECT_ENABLED">[<code>REDIRECT_ENABLED</code>{.xref .std .std-setting .docutils
.literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal} setting to quickly enable/disable the
redirect middleware
(<a href="http://hg.scrapy.org/scrapy/changeset/2697">r2697</a>{.reference
.external})</p>
</li>
<li>
<p>Added <a href="index.html#std-setting-RETRY_ENABLED">[<code>RETRY_ENABLED</code>{.xref .std .std-setting .docutils .literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal} setting to quickly enable/disable the
retry middleware
(<a href="http://hg.scrapy.org/scrapy/changeset/2694">r2694</a>{.reference
.external})</p>
</li>
<li>
<p>Added [<code>CloseSpider</code>{.docutils .literal .notranslate}]{.pre}
exception to manually close spiders
(<a href="http://hg.scrapy.org/scrapy/changeset/2691">r2691</a>{.reference
.external})</p>
</li>
<li>
<p>Improved encoding detection by adding support for HTML5 meta charset
declaration
(<a href="http://hg.scrapy.org/scrapy/changeset/2690">r2690</a>{.reference
.external})</p>
</li>
<li>
<p>Refactored close spider behavior to wait for all downloads to finish
and be processed by spiders, before closing the spider
(<a href="http://hg.scrapy.org/scrapy/changeset/2688">r2688</a>{.reference
.external})</p>
</li>
<li>
<p>Added [<code>SitemapSpider</code>{.docutils .literal .notranslate}]{.pre} (see
documentation in Spiders page)
(<a href="http://hg.scrapy.org/scrapy/changeset/2658">r2658</a>{.reference
.external})</p>
</li>
<li>
<p>Added [<code>LogStats</code>{.docutils .literal .notranslate}]{.pre} extension
for periodically logging basic stats (like crawled pages and scraped
items)
(<a href="http://hg.scrapy.org/scrapy/changeset/2657">r2657</a>{.reference
.external})</p>
</li>
<li>
<p>Make handling of gzipped responses more robust (#319,
<a href="http://hg.scrapy.org/scrapy/changeset/2643">r2643</a>{.reference
.external}). Now Scrapy will try and decompress as much as possible
from a gzipped response, instead of failing with an
[<code>IOError</code>{.docutils .literal .notranslate}]{.pre}.</p>
</li>
<li>
<p>Simplified !MemoryDebugger extension to use stats for dumping memory
debugging info
(<a href="http://hg.scrapy.org/scrapy/changeset/2639">r2639</a>{.reference
.external})</p>
</li>
<li>
<p>Added new command to edit spiders: [<code>scrapy</code>{.docutils .literal
.notranslate}]{.pre}<code> </code>{.docutils .literal
.notranslate}[<code>edit</code>{.docutils .literal .notranslate}]{.pre}
(<a href="http://hg.scrapy.org/scrapy/changeset/2636">r2636</a>{.reference
.external}) and [<code>-e</code>{.docutils .literal .notranslate}]{.pre} flag
to [<code>genspider</code>{.docutils .literal .notranslate}]{.pre} command that
uses it
(<a href="http://hg.scrapy.org/scrapy/changeset/2653">r2653</a>{.reference
.external})</p>
</li>
<li>
<p>Changed default representation of items to pretty-printed dicts.
(<a href="http://hg.scrapy.org/scrapy/changeset/2631">r2631</a>{.reference
.external}). This improves default logging by making log more
readable in the default case, for both Scraped and Dropped lines.</p>
</li>
<li>
<p>Added <a href="index.html#std-signal-spider_error">[<code>spider_error</code>{.xref .std .std-signal .docutils .literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal} signal
(<a href="http://hg.scrapy.org/scrapy/changeset/2628">r2628</a>{.reference
.external})</p>
</li>
<li>
<p>Added <a href="index.html#std-setting-COOKIES_ENABLED">[<code>COOKIES_ENABLED</code>{.xref .std .std-setting .docutils .literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal} setting
(<a href="http://hg.scrapy.org/scrapy/changeset/2625">r2625</a>{.reference
.external})</p>
</li>
<li>
<p>Stats are now dumped to Scrapy log (default value of
<a href="index.html#std-setting-STATS_DUMP">[<code>STATS_DUMP</code>{.xref .std .std-setting .docutils .literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal} setting has been changed to
[<code>True</code>{.docutils .literal .notranslate}]{.pre}). This is to make
Scrapy users more aware of Scrapy stats and the data that is
collected there.</p>
</li>
<li>
<p>Added support for dynamically adjusting download delay and maximum
concurrent requests
(<a href="http://hg.scrapy.org/scrapy/changeset/2599">r2599</a>{.reference
.external})</p>
</li>
<li>
<p>Added new DBM HTTP cache storage backend
(<a href="http://hg.scrapy.org/scrapy/changeset/2576">r2576</a>{.reference
.external})</p>
</li>
<li>
<p>Added [<code>listjobs.json</code>{.docutils .literal .notranslate}]{.pre} API
to Scrapyd
(<a href="http://hg.scrapy.org/scrapy/changeset/2571">r2571</a>{.reference
.external})</p>
</li>
<li>
<p>[<code>CsvItemExporter</code>{.docutils .literal .notranslate}]{.pre}: added
[<code>join_multivalued</code>{.docutils .literal .notranslate}]{.pre}
parameter
(<a href="http://hg.scrapy.org/scrapy/changeset/2578">r2578</a>{.reference
.external})</p>
</li>
<li>
<p>Added namespace support to [<code>xmliter_lxml</code>{.docutils .literal
.notranslate}]{.pre}
(<a href="http://hg.scrapy.org/scrapy/changeset/2552">r2552</a>{.reference
.external})</p>
</li>
<li>
<p>Improved cookies middleware by making [<code>COOKIES_DEBUG</code>{.docutils
.literal .notranslate}]{.pre} nicer and documenting it
(<a href="http://hg.scrapy.org/scrapy/changeset/2579">r2579</a>{.reference
.external})</p>
</li>
<li>
<p>Several improvements to Scrapyd and Link extractors
:::</p>
</li>
</ul>
<p>::: {#code-rearranged-and-removed .section}</p>
<h5 id="code-rearranged-and-removedheaderlink"><a class="header" href="#code-rearranged-and-removedheaderlink">Code rearranged and removed<a href="#code-rearranged-and-removed" title="Permalink to this heading">Â¶</a>{.headerlink}</a></h5>
<ul>
<li></li>
</ul>
<pre><code>Merged item passed and item scraped concepts, as they have often proved confusing in the past. This means: ([r2630](http://hg.scrapy.org/scrapy/changeset/2630){.reference .external})

:   -   original item_scraped signal was removed

    -   original item_passed signal was renamed to item_scraped

    -   old log lines [`Scraped`{.docutils .literal
        .notranslate}]{.pre}` `{.docutils .literal
        .notranslate}[`Item...`{.docutils .literal
        .notranslate}]{.pre} were removed

    -   old log lines [`Passed`{.docutils .literal
        .notranslate}]{.pre}` `{.docutils .literal
        .notranslate}[`Item...`{.docutils .literal
        .notranslate}]{.pre} were renamed to [`Scraped`{.docutils
        .literal .notranslate}]{.pre}` `{.docutils .literal
        .notranslate}[`Item...`{.docutils .literal
        .notranslate}]{.pre} lines and downgraded to
        [`DEBUG`{.docutils .literal .notranslate}]{.pre} level
</code></pre>
<ul>
<li></li>
</ul>
<pre><code>Reduced Scrapy codebase by striping part of Scrapy code into two new libraries:

:   -   [w3lib](https://github.com/scrapy/w3lib){.reference
        .external} (several functions from
        [`scrapy.utils.{http,markup,multipart,response,url}`{.docutils
        .literal .notranslate}]{.pre}, done in
        [r2584](http://hg.scrapy.org/scrapy/changeset/2584){.reference
        .external})

    -   [scrapely](https://github.com/scrapy/scrapely){.reference
        .external} (was [`scrapy.contrib.ibl`{.docutils .literal
        .notranslate}]{.pre}, done in
        [r2586](http://hg.scrapy.org/scrapy/changeset/2586){.reference
        .external})
</code></pre>
<ul>
<li>
<p>Removed unused function:
[<code>scrapy.utils.request.request_info()</code>{.docutils .literal
.notranslate}]{.pre}
(<a href="http://hg.scrapy.org/scrapy/changeset/2577">r2577</a>{.reference
.external})</p>
</li>
<li>
<p>Removed googledir project from [<code>examples/googledir</code>{.docutils
.literal .notranslate}]{.pre}. There's now a new example project
called [<code>dirbot</code>{.docutils .literal .notranslate}]{.pre} available
on GitHub:
<a href="https://github.com/scrapy/dirbot">https://github.com/scrapy/dirbot</a>{.reference
.external}</p>
</li>
<li>
<p>Removed support for default field values in Scrapy items
(<a href="http://hg.scrapy.org/scrapy/changeset/2616">r2616</a>{.reference
.external})</p>
</li>
<li>
<p>Removed experimental crawlspider v2
(<a href="http://hg.scrapy.org/scrapy/changeset/2632">r2632</a>{.reference
.external})</p>
</li>
<li>
<p>Removed scheduler middleware to simplify architecture. Duplicates
filter is now done in the scheduler itself, using the same dupe
filtering class as before ([<code>DUPEFILTER_CLASS</code>{.docutils .literal
.notranslate}]{.pre} setting)
(<a href="http://hg.scrapy.org/scrapy/changeset/2640">r2640</a>{.reference
.external})</p>
</li>
<li>
<p>Removed support for passing urls to [<code>scrapy</code>{.docutils .literal
.notranslate}]{.pre}<code> </code>{.docutils .literal
.notranslate}[<code>crawl</code>{.docutils .literal .notranslate}]{.pre}
command (use [<code>scrapy</code>{.docutils .literal
.notranslate}]{.pre}<code> </code>{.docutils .literal
.notranslate}[<code>parse</code>{.docutils .literal .notranslate}]{.pre}
instead)
(<a href="http://hg.scrapy.org/scrapy/changeset/2704">r2704</a>{.reference
.external})</p>
</li>
<li>
<p>Removed deprecated Execution Queue
(<a href="http://hg.scrapy.org/scrapy/changeset/2704">r2704</a>{.reference
.external})</p>
</li>
<li>
<p>Removed (undocumented) spider context extension (from
scrapy.contrib.spidercontext)
(<a href="http://hg.scrapy.org/scrapy/changeset/2780">r2780</a>{.reference
.external})</p>
</li>
<li>
<p>removed [<code>CONCURRENT_SPIDERS</code>{.docutils .literal
.notranslate}]{.pre} setting (use scrapyd maxproc instead)
(<a href="http://hg.scrapy.org/scrapy/changeset/2789">r2789</a>{.reference
.external})</p>
</li>
<li>
<p>Renamed attributes of core components: downloader.sites -&gt;
downloader.slots, scraper.sites -&gt; scraper.slots
(<a href="http://hg.scrapy.org/scrapy/changeset/2717">r2717</a>{.reference
.external},
<a href="http://hg.scrapy.org/scrapy/changeset/2718">r2718</a>{.reference
.external})</p>
</li>
<li>
<p>Renamed setting [<code>CLOSESPIDER_ITEMPASSED</code>{.docutils .literal
.notranslate}]{.pre} to <a href="index.html#std-setting-CLOSESPIDER_ITEMCOUNT">[<code>CLOSESPIDER_ITEMCOUNT</code>{.xref .std
.std-setting .docutils .literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal}
(<a href="http://hg.scrapy.org/scrapy/changeset/2655">r2655</a>{.reference
.external}). Backward compatibility kept.
:::
:::</p>
</li>
</ul>
<p>::: {#scrapy-0-12 .section}</p>
<h4 id="scrapy-012headerlink"><a class="header" href="#scrapy-012headerlink">Scrapy 0.12<a href="#scrapy-0-12" title="Permalink to this heading">Â¶</a>{.headerlink}</a></h4>
<p>The numbers like #NNN reference tickets in the old issue tracker (Trac)
which is no longer available.</p>
<p>::: {#new-features-and-improvements .section}</p>
<h5 id="new-features-and-improvementsheaderlink"><a class="header" href="#new-features-and-improvementsheaderlink">New features and improvements<a href="#new-features-and-improvements" title="Permalink to this heading">Â¶</a>{.headerlink}</a></h5>
<ul>
<li>
<p>Passed item is now sent in the [<code>item</code>{.docutils .literal
.notranslate}]{.pre} argument of the <a href="index.html#std-signal-item_scraped">[<code>item_passed</code>{.xref .std
.std-signal .docutils .literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal} (#273)</p>
</li>
<li>
<p>Added verbose option to [<code>scrapy</code>{.docutils .literal
.notranslate}]{.pre}<code> </code>{.docutils .literal
.notranslate}[<code>version</code>{.docutils .literal .notranslate}]{.pre}
command, useful for bug reports (#298)</p>
</li>
<li>
<p>HTTP cache now stored by default in the project data dir (#279)</p>
</li>
<li>
<p>Added project data storage directory (#276, #277)</p>
</li>
<li>
<p>Documented file structure of Scrapy projects (see command-line tool
doc)</p>
</li>
<li>
<p>New lxml backend for XPath selectors (#147)</p>
</li>
<li>
<p>Per-spider settings (#245)</p>
</li>
<li>
<p>Support exit codes to signal errors in Scrapy commands (#248)</p>
</li>
<li>
<p>Added [<code>-c</code>{.docutils .literal .notranslate}]{.pre} argument to
[<code>scrapy</code>{.docutils .literal .notranslate}]{.pre}<code> </code>{.docutils
.literal .notranslate}[<code>shell</code>{.docutils .literal
.notranslate}]{.pre} command</p>
</li>
<li>
<p>Made [<code>libxml2</code>{.docutils .literal .notranslate}]{.pre} optional
(#260)</p>
</li>
<li>
<p>New [<code>deploy</code>{.docutils .literal .notranslate}]{.pre} command (#261)</p>
</li>
<li>
<p>Added <a href="index.html#std-setting-CLOSESPIDER_PAGECOUNT">[<code>CLOSESPIDER_PAGECOUNT</code>{.xref .std .std-setting .docutils
.literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal} setting (#253)</p>
</li>
<li>
<p>Added <a href="index.html#std-setting-CLOSESPIDER_ERRORCOUNT">[<code>CLOSESPIDER_ERRORCOUNT</code>{.xref .std .std-setting .docutils
.literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal} setting (#254)
:::</p>
</li>
</ul>
<p>::: {#scrapyd-changes .section}</p>
<h5 id="scrapyd-changesheaderlink"><a class="header" href="#scrapyd-changesheaderlink">Scrapyd changes<a href="#scrapyd-changes" title="Permalink to this heading">Â¶</a>{.headerlink}</a></h5>
<ul>
<li>
<p>Scrapyd now uses one process per spider</p>
</li>
<li>
<p>It stores one log file per spider run, and rotate them keeping the
latest 5 logs per spider (by default)</p>
</li>
<li>
<p>A minimal web ui was added, available at
<a href="http://localhost:6800">http://localhost:6800</a>{.reference .external}
by default</p>
</li>
<li>
<p>There is now a [<code>scrapy</code>{.docutils .literal
.notranslate}]{.pre}<code> </code>{.docutils .literal
.notranslate}[<code>server</code>{.docutils .literal .notranslate}]{.pre}
command to start a Scrapyd server of the current project
:::</p>
</li>
</ul>
<p>::: {#changes-to-settings .section}</p>
<h5 id="changes-to-settingsheaderlink"><a class="header" href="#changes-to-settingsheaderlink">Changes to settings<a href="#changes-to-settings" title="Permalink to this heading">Â¶</a>{.headerlink}</a></h5>
<ul>
<li>
<p>added [<code>HTTPCACHE_ENABLED</code>{.docutils .literal .notranslate}]{.pre}
setting (False by default) to enable HTTP cache middleware</p>
</li>
<li>
<p>changed [<code>HTTPCACHE_EXPIRATION_SECS</code>{.docutils .literal
.notranslate}]{.pre} semantics: now zero means &quot;never expire&quot;.
:::</p>
</li>
</ul>
<p>::: {#deprecated-obsoleted-functionality .section}</p>
<h5 id="deprecatedobsoleted-functionalityheaderlink"><a class="header" href="#deprecatedobsoleted-functionalityheaderlink">Deprecated/obsoleted functionality<a href="#deprecated-obsoleted-functionality" title="Permalink to this heading">Â¶</a>{.headerlink}</a></h5>
<ul>
<li>
<p>Deprecated [<code>runserver</code>{.docutils .literal .notranslate}]{.pre}
command in favor of [<code>server</code>{.docutils .literal
.notranslate}]{.pre} command which starts a Scrapyd server. See
also: Scrapyd changes</p>
</li>
<li>
<p>Deprecated [<code>queue</code>{.docutils .literal .notranslate}]{.pre} command
in favor of using Scrapyd [<code>schedule.json</code>{.docutils .literal
.notranslate}]{.pre} API. See also: Scrapyd changes</p>
</li>
<li>
<p>Removed the !LxmlItemLoader (experimental contrib which never
graduated to main contrib)
:::
:::</p>
</li>
</ul>
<p>::: {#scrapy-0-10 .section}</p>
<h4 id="scrapy-010headerlink"><a class="header" href="#scrapy-010headerlink">Scrapy 0.10<a href="#scrapy-0-10" title="Permalink to this heading">Â¶</a>{.headerlink}</a></h4>
<p>The numbers like #NNN reference tickets in the old issue tracker (Trac)
which is no longer available.</p>
<p>::: {#id133 .section}</p>
<h5 id="new-features-and-improvementsheaderlink-1"><a class="header" href="#new-features-and-improvementsheaderlink-1">New features and improvements<a href="#id133" title="Permalink to this heading">Â¶</a>{.headerlink}</a></h5>
<ul>
<li>
<p>New Scrapy service called [<code>scrapyd</code>{.docutils .literal
.notranslate}]{.pre} for deploying Scrapy crawlers in production
(#218) (documentation available)</p>
</li>
<li>
<p>Simplified Images pipeline usage which doesn't require subclassing
your own images pipeline now (#217)</p>
</li>
<li>
<p>Scrapy shell now shows the Scrapy log by default (#206)</p>
</li>
<li>
<p>Refactored execution queue in a common base code and pluggable
backends called &quot;spider queues&quot; (#220)</p>
</li>
<li>
<p>New persistent spider queue (based on SQLite) (#198), available by
default, which allows to start Scrapy in server mode and then
schedule spiders to run.</p>
</li>
<li>
<p>Added documentation for Scrapy command-line tool and all its
available sub-commands. (documentation available)</p>
</li>
<li>
<p>Feed exporters with pluggable backends (#197) (documentation
available)</p>
</li>
<li>
<p>Deferred signals (#193)</p>
</li>
<li>
<p>Added two new methods to item pipeline open_spider(), close_spider()
with deferred support (#195)</p>
</li>
<li>
<p>Support for overriding default request headers per spider (#181)</p>
</li>
<li>
<p>Replaced default Spider Manager with one with similar functionality
but not depending on Twisted Plugins (#186)</p>
</li>
<li>
<p>Split Debian package into two packages - the library and the service
(#187)</p>
</li>
<li>
<p>Scrapy log refactoring (#188)</p>
</li>
<li>
<p>New extension for keeping persistent spider contexts among different
runs (#203)</p>
</li>
<li>
<p>Added [<code>dont_redirect</code>{.docutils .literal .notranslate}]{.pre}
request.meta key for avoiding redirects (#233)</p>
</li>
<li>
<p>Added [<code>dont_retry</code>{.docutils .literal .notranslate}]{.pre}
request.meta key for avoiding retries (#234)
:::</p>
</li>
</ul>
<p>::: {#command-line-tool-changes .section}</p>
<h5 id="command-line-tool-changesheaderlink"><a class="header" href="#command-line-tool-changesheaderlink">Command-line tool changes<a href="#command-line-tool-changes" title="Permalink to this heading">Â¶</a>{.headerlink}</a></h5>
<ul>
<li>
<p>New [<code>scrapy</code>{.docutils .literal .notranslate}]{.pre} command which
replaces the old [<code>scrapy-ctl.py</code>{.docutils .literal
.notranslate}]{.pre} (#199) - there is only one global
[<code>scrapy</code>{.docutils .literal .notranslate}]{.pre} command now,
instead of one [<code>scrapy-ctl.py</code>{.docutils .literal
.notranslate}]{.pre} per project - Added [<code>scrapy.bat</code>{.docutils
.literal .notranslate}]{.pre} script for running more conveniently
from Windows</p>
</li>
<li>
<p>Added bash completion to command-line tool (#210)</p>
</li>
<li>
<p>Renamed command [<code>start</code>{.docutils .literal .notranslate}]{.pre} to
[<code>runserver</code>{.docutils .literal .notranslate}]{.pre} (#209)
:::</p>
</li>
</ul>
<p>::: {#api-changes .section}</p>
<h5 id="api-changesheaderlink"><a class="header" href="#api-changesheaderlink">API changes<a href="#api-changes" title="Permalink to this heading">Â¶</a>{.headerlink}</a></h5>
<ul>
<li>
<p>[<code>url</code>{.docutils .literal .notranslate}]{.pre} and [<code>body</code>{.docutils
.literal .notranslate}]{.pre} attributes of Request objects are now
read-only (#230)</p>
</li>
<li>
<p>[<code>Request.copy()</code>{.docutils .literal .notranslate}]{.pre} and
[<code>Request.replace()</code>{.docutils .literal .notranslate}]{.pre} now
also copies their [<code>callback</code>{.docutils .literal
.notranslate}]{.pre} and [<code>errback</code>{.docutils .literal
.notranslate}]{.pre} attributes (#231)</p>
</li>
<li>
<p>Removed [<code>UrlFilterMiddleware</code>{.docutils .literal
.notranslate}]{.pre} from [<code>scrapy.contrib</code>{.docutils .literal
.notranslate}]{.pre} (already disabled by default)</p>
</li>
<li>
<p>Offsite middleware doesn't filter out any request coming from a
spider that doesn't have a allowed_domains attribute (#225)</p>
</li>
<li>
<p>Removed Spider Manager [<code>load()</code>{.docutils .literal
.notranslate}]{.pre} method. Now spiders are loaded in the
[<code>__init__</code>{.docutils .literal .notranslate}]{.pre} method itself.</p>
</li>
<li></li>
</ul>
<pre><code>Changes to Scrapy Manager (now called &quot;Crawler&quot;):

:   -   [`scrapy.core.manager.ScrapyManager`{.docutils .literal
        .notranslate}]{.pre} class renamed to
        [`scrapy.crawler.Crawler`{.docutils .literal
        .notranslate}]{.pre}

    -   [`scrapy.core.manager.scrapymanager`{.docutils .literal
        .notranslate}]{.pre} singleton moved to
        [`scrapy.project.crawler`{.docutils .literal
        .notranslate}]{.pre}
</code></pre>
<ul>
<li>
<p>Moved module: [<code>scrapy.contrib.spidermanager</code>{.docutils .literal
.notranslate}]{.pre} to [<code>scrapy.spidermanager</code>{.docutils .literal
.notranslate}]{.pre}</p>
</li>
<li>
<p>Spider Manager singleton moved from
[<code>scrapy.spider.spiders</code>{.docutils .literal .notranslate}]{.pre} to
the [<code>spiders`</code>{.docutils .literal
.notranslate}]{.pre}<code> </code>{.docutils .literal
.notranslate}[<code>attribute</code>{.docutils .literal
.notranslate}]{.pre}<code> </code>{.docutils .literal
.notranslate}[<code>of</code>{.docutils .literal
.notranslate}]{.pre}<code> </code>{.docutils .literal
.notranslate}[<code>``scrapy.project.crawler</code>{.docutils .literal
.notranslate}]{.pre} singleton.</p>
</li>
<li></li>
</ul>
<pre><code>moved Stats Collector classes: (#204)

:   -   [`scrapy.stats.collector.StatsCollector`{.docutils .literal
        .notranslate}]{.pre} to
        [`scrapy.statscol.StatsCollector`{.docutils .literal
        .notranslate}]{.pre}

    -   [`scrapy.stats.collector.SimpledbStatsCollector`{.docutils
        .literal .notranslate}]{.pre} to
        [`scrapy.contrib.statscol.SimpledbStatsCollector`{.docutils
        .literal .notranslate}]{.pre}
</code></pre>
<ul>
<li>
<p>default per-command settings are now specified in the
[<code>default_settings</code>{.docutils .literal .notranslate}]{.pre}
attribute of command object class (#201)</p>
</li>
<li></li>
</ul>
<pre><code>changed arguments of Item pipeline [`process_item()`{.docutils .literal .notranslate}]{.pre} method from [`(spider,`{.docutils .literal .notranslate}]{.pre}` `{.docutils .literal .notranslate}[`item)`{.docutils .literal .notranslate}]{.pre} to [`(item,`{.docutils .literal .notranslate}]{.pre}` `{.docutils .literal .notranslate}[`spider)`{.docutils .literal .notranslate}]{.pre}

:   -   backward compatibility kept (with deprecation warning)
</code></pre>
<ul>
<li></li>
</ul>
<pre><code>moved [`scrapy.core.signals`{.docutils .literal .notranslate}]{.pre} module to [`scrapy.signals`{.docutils .literal .notranslate}]{.pre}

:   -   backward compatibility kept (with deprecation warning)
</code></pre>
<ul>
<li></li>
</ul>
<pre><code>moved [`scrapy.core.exceptions`{.docutils .literal .notranslate}]{.pre} module to [`scrapy.exceptions`{.docutils .literal .notranslate}]{.pre}

:   -   backward compatibility kept (with deprecation warning)
</code></pre>
<ul>
<li>
<p>added [<code>handles_request()</code>{.docutils .literal .notranslate}]{.pre}
class method to [<code>BaseSpider</code>{.docutils .literal
.notranslate}]{.pre}</p>
</li>
<li>
<p>dropped [<code>scrapy.log.exc()</code>{.docutils .literal .notranslate}]{.pre}
function (use [<code>scrapy.log.err()</code>{.docutils .literal
.notranslate}]{.pre} instead)</p>
</li>
<li>
<p>dropped [<code>component</code>{.docutils .literal .notranslate}]{.pre}
argument of [<code>scrapy.log.msg()</code>{.docutils .literal
.notranslate}]{.pre} function</p>
</li>
<li>
<p>dropped [<code>scrapy.log.log_level</code>{.docutils .literal
.notranslate}]{.pre} attribute</p>
</li>
<li>
<p>Added [<code>from_settings()</code>{.docutils .literal .notranslate}]{.pre}
class methods to Spider Manager, and Item Pipeline Manager
:::</p>
</li>
</ul>
<p>::: {#id134 .section}</p>
<h5 id="changes-to-settingsheaderlink-1"><a class="header" href="#changes-to-settingsheaderlink-1">Changes to settings<a href="#id134" title="Permalink to this heading">Â¶</a>{.headerlink}</a></h5>
<ul>
<li>
<p>Added [<code>HTTPCACHE_IGNORE_SCHEMES</code>{.docutils .literal
.notranslate}]{.pre} setting to ignore certain schemes on
!HttpCacheMiddleware (#225)</p>
</li>
<li>
<p>Added [<code>SPIDER_QUEUE_CLASS</code>{.docutils .literal .notranslate}]{.pre}
setting which defines the spider queue to use (#220)</p>
</li>
<li>
<p>Added [<code>KEEP_ALIVE</code>{.docutils .literal .notranslate}]{.pre} setting
(#220)</p>
</li>
<li>
<p>Removed [<code>SERVICE_QUEUE</code>{.docutils .literal .notranslate}]{.pre}
setting (#220)</p>
</li>
<li>
<p>Removed [<code>COMMANDS_SETTINGS_MODULE</code>{.docutils .literal
.notranslate}]{.pre} setting (#201)</p>
</li>
<li>
<p>Renamed [<code>REQUEST_HANDLERS</code>{.docutils .literal .notranslate}]{.pre}
to [<code>DOWNLOAD_HANDLERS</code>{.docutils .literal .notranslate}]{.pre} and
make download handlers classes (instead of functions)
:::
:::</p>
</li>
</ul>
<p>::: {#scrapy-0-9 .section}</p>
<h4 id="scrapy-09headerlink"><a class="header" href="#scrapy-09headerlink">Scrapy 0.9<a href="#scrapy-0-9" title="Permalink to this heading">Â¶</a>{.headerlink}</a></h4>
<p>The numbers like #NNN reference tickets in the old issue tracker (Trac)
which is no longer available.</p>
<p>::: {#id135 .section}</p>
<h5 id="new-features-and-improvementsheaderlink-2"><a class="header" href="#new-features-and-improvementsheaderlink-2">New features and improvements<a href="#id135" title="Permalink to this heading">Â¶</a>{.headerlink}</a></h5>
<ul>
<li>
<p>Added SMTP-AUTH support to scrapy.mail</p>
</li>
<li>
<p>New settings added: [<code>MAIL_USER</code>{.docutils .literal
.notranslate}]{.pre}, [<code>MAIL_PASS</code>{.docutils .literal
.notranslate}]{.pre}
(<a href="http://hg.scrapy.org/scrapy/changeset/2065">r2065</a>{.reference
.external} | #149)</p>
</li>
<li>
<p>Added new scrapy-ctl view command - To view URL in the browser, as
seen by Scrapy
(<a href="http://hg.scrapy.org/scrapy/changeset/2039">r2039</a>{.reference
.external})</p>
</li>
<li>
<p>Added web service for controlling Scrapy process (this also
deprecates the web console.
(<a href="http://hg.scrapy.org/scrapy/changeset/2053">r2053</a>{.reference
.external} | #167)</p>
</li>
<li>
<p>Support for running Scrapy as a service, for production systems
(<a href="http://hg.scrapy.org/scrapy/changeset/1988">r1988</a>{.reference
.external},
<a href="http://hg.scrapy.org/scrapy/changeset/2054">r2054</a>{.reference
.external},
<a href="http://hg.scrapy.org/scrapy/changeset/2055">r2055</a>{.reference
.external},
<a href="http://hg.scrapy.org/scrapy/changeset/2056">r2056</a>{.reference
.external},
<a href="http://hg.scrapy.org/scrapy/changeset/2057">r2057</a>{.reference
.external} | #168)</p>
</li>
<li>
<p>Added wrapper induction library (documentation only available in
source code for now).
(<a href="http://hg.scrapy.org/scrapy/changeset/2011">r2011</a>{.reference
.external})</p>
</li>
<li>
<p>Simplified and improved response encoding support
(<a href="http://hg.scrapy.org/scrapy/changeset/1961">r1961</a>{.reference
.external},
<a href="http://hg.scrapy.org/scrapy/changeset/1969">r1969</a>{.reference
.external})</p>
</li>
<li>
<p>Added [<code>LOG_ENCODING</code>{.docutils .literal .notranslate}]{.pre}
setting
(<a href="http://hg.scrapy.org/scrapy/changeset/1956">r1956</a>{.reference
.external}, documentation available)</p>
</li>
<li>
<p>Added [<code>RANDOMIZE_DOWNLOAD_DELAY</code>{.docutils .literal
.notranslate}]{.pre} setting (enabled by default)
(<a href="http://hg.scrapy.org/scrapy/changeset/1923">r1923</a>{.reference
.external}, doc available)</p>
</li>
<li>
<p>[<code>MailSender</code>{.docutils .literal .notranslate}]{.pre} is no longer
IO-blocking
(<a href="http://hg.scrapy.org/scrapy/changeset/1955">r1955</a>{.reference
.external} | #146)</p>
</li>
<li>
<p>Linkextractors and new Crawlspider now handle relative base tag urls
(<a href="http://hg.scrapy.org/scrapy/changeset/1960">r1960</a>{.reference
.external} | #148)</p>
</li>
<li>
<p>Several improvements to Item Loaders and processors
(<a href="http://hg.scrapy.org/scrapy/changeset/2022">r2022</a>{.reference
.external},
<a href="http://hg.scrapy.org/scrapy/changeset/2023">r2023</a>{.reference
.external},
<a href="http://hg.scrapy.org/scrapy/changeset/2024">r2024</a>{.reference
.external},
<a href="http://hg.scrapy.org/scrapy/changeset/2025">r2025</a>{.reference
.external},
<a href="http://hg.scrapy.org/scrapy/changeset/2026">r2026</a>{.reference
.external},
<a href="http://hg.scrapy.org/scrapy/changeset/2027">r2027</a>{.reference
.external},
<a href="http://hg.scrapy.org/scrapy/changeset/2028">r2028</a>{.reference
.external},
<a href="http://hg.scrapy.org/scrapy/changeset/2029">r2029</a>{.reference
.external},
<a href="http://hg.scrapy.org/scrapy/changeset/2030">r2030</a>{.reference
.external})</p>
</li>
<li>
<p>Added support for adding variables to telnet console
(<a href="http://hg.scrapy.org/scrapy/changeset/2047">r2047</a>{.reference
.external} | #165)</p>
</li>
<li>
<p>Support for requests without callbacks
(<a href="http://hg.scrapy.org/scrapy/changeset/2050">r2050</a>{.reference
.external} | #166)
:::</p>
</li>
</ul>
<p>::: {#id136 .section}</p>
<h5 id="api-changesheaderlink-1"><a class="header" href="#api-changesheaderlink-1">API changes<a href="#id136" title="Permalink to this heading">Â¶</a>{.headerlink}</a></h5>
<ul>
<li>
<p>Change [<code>Spider.domain_name</code>{.docutils .literal .notranslate}]{.pre}
to [<code>Spider.name</code>{.docutils .literal .notranslate}]{.pre} (SEP-012,
<a href="http://hg.scrapy.org/scrapy/changeset/1975">r1975</a>{.reference
.external})</p>
</li>
<li>
<p>[<code>Response.encoding</code>{.docutils .literal .notranslate}]{.pre} is now
the detected encoding
(<a href="http://hg.scrapy.org/scrapy/changeset/1961">r1961</a>{.reference
.external})</p>
</li>
<li>
<p>[<code>HttpErrorMiddleware</code>{.docutils .literal .notranslate}]{.pre} now
returns None or raises an exception
(<a href="http://hg.scrapy.org/scrapy/changeset/2006">r2006</a>{.reference
.external} | #157)</p>
</li>
<li>
<p>[<code>scrapy.command</code>{.docutils .literal .notranslate}]{.pre} modules
relocation
(<a href="http://hg.scrapy.org/scrapy/changeset/2035">r2035</a>{.reference
.external},
<a href="http://hg.scrapy.org/scrapy/changeset/2036">r2036</a>{.reference
.external},
<a href="http://hg.scrapy.org/scrapy/changeset/2037">r2037</a>{.reference
.external})</p>
</li>
<li>
<p>Added [<code>ExecutionQueue</code>{.docutils .literal .notranslate}]{.pre} for
feeding spiders to scrape
(<a href="http://hg.scrapy.org/scrapy/changeset/2034">r2034</a>{.reference
.external})</p>
</li>
<li>
<p>Removed [<code>ExecutionEngine</code>{.docutils .literal .notranslate}]{.pre}
singleton
(<a href="http://hg.scrapy.org/scrapy/changeset/2039">r2039</a>{.reference
.external})</p>
</li>
<li>
<p>Ported [<code>S3ImagesStore</code>{.docutils .literal .notranslate}]{.pre}
(images pipeline) to use boto and threads
(<a href="http://hg.scrapy.org/scrapy/changeset/2033">r2033</a>{.reference
.external})</p>
</li>
<li>
<p>Moved module: [<code>scrapy.management.telnet</code>{.docutils .literal
.notranslate}]{.pre} to [<code>scrapy.telnet</code>{.docutils .literal
.notranslate}]{.pre}
(<a href="http://hg.scrapy.org/scrapy/changeset/2047">r2047</a>{.reference
.external})
:::</p>
</li>
</ul>
<p>::: {#changes-to-default-settings .section}</p>
<h5 id="changes-to-default-settingsheaderlink"><a class="header" href="#changes-to-default-settingsheaderlink">Changes to default settings<a href="#changes-to-default-settings" title="Permalink to this heading">Â¶</a>{.headerlink}</a></h5>
<ul>
<li>Changed default [<code>SCHEDULER_ORDER</code>{.docutils .literal
.notranslate}]{.pre} to [<code>DFO</code>{.docutils .literal
.notranslate}]{.pre}
(<a href="http://hg.scrapy.org/scrapy/changeset/1939">r1939</a>{.reference
.external})
:::
:::</li>
</ul>
<p>::: {#scrapy-0-8 .section}</p>
<h4 id="scrapy-08headerlink"><a class="header" href="#scrapy-08headerlink">Scrapy 0.8<a href="#scrapy-0-8" title="Permalink to this heading">Â¶</a>{.headerlink}</a></h4>
<p>The numbers like #NNN reference tickets in the old issue tracker (Trac)
which is no longer available.</p>
<p>::: {#id137 .section}</p>
<h5 id="new-featuresheaderlink"><a class="header" href="#new-featuresheaderlink">New features<a href="#id137" title="Permalink to this heading">Â¶</a>{.headerlink}</a></h5>
<ul>
<li>
<p>Added DEFAULT_RESPONSE_ENCODING setting
(<a href="http://hg.scrapy.org/scrapy/changeset/1809">r1809</a>{.reference
.external})</p>
</li>
<li>
<p>Added [<code>dont_click</code>{.docutils .literal .notranslate}]{.pre} argument
to [<code>FormRequest.from_response()</code>{.docutils .literal
.notranslate}]{.pre} method
(<a href="http://hg.scrapy.org/scrapy/changeset/1813">r1813</a>{.reference
.external},
<a href="http://hg.scrapy.org/scrapy/changeset/1816">r1816</a>{.reference
.external})</p>
</li>
<li>
<p>Added [<code>clickdata</code>{.docutils .literal .notranslate}]{.pre} argument
to [<code>FormRequest.from_response()</code>{.docutils .literal
.notranslate}]{.pre} method
(<a href="http://hg.scrapy.org/scrapy/changeset/1802">r1802</a>{.reference
.external},
<a href="http://hg.scrapy.org/scrapy/changeset/1803">r1803</a>{.reference
.external})</p>
</li>
<li>
<p>Added support for HTTP proxies ([<code>HttpProxyMiddleware</code>{.docutils
.literal .notranslate}]{.pre})
(<a href="http://hg.scrapy.org/scrapy/changeset/1781">r1781</a>{.reference
.external},
<a href="http://hg.scrapy.org/scrapy/changeset/1785">r1785</a>{.reference
.external})</p>
</li>
<li>
<p>Offsite spider middleware now logs messages when filtering out
requests
(<a href="http://hg.scrapy.org/scrapy/changeset/1841">r1841</a>{.reference
.external})
:::</p>
</li>
</ul>
<p>::: {#id138 .section}</p>
<h5 id="backward-incompatible-changesheaderlink"><a class="header" href="#backward-incompatible-changesheaderlink">Backward-incompatible changes<a href="#id138" title="Permalink to this heading">Â¶</a>{.headerlink}</a></h5>
<ul>
<li>
<p>Changed [<code>scrapy.utils.response.get_meta_refresh()</code>{.docutils
.literal .notranslate}]{.pre} signature
(<a href="http://hg.scrapy.org/scrapy/changeset/1804">r1804</a>{.reference
.external})</p>
</li>
<li>
<p>Removed deprecated [<code>scrapy.item.ScrapedItem</code>{.docutils .literal
.notranslate}]{.pre} class - use [<code>scrapy.item.Item</code>{.docutils
.literal .notranslate}]{.pre}<code> </code>{.docutils .literal
.notranslate}[<code>instead</code>{.docutils .literal .notranslate}]{.pre}
(<a href="http://hg.scrapy.org/scrapy/changeset/1838">r1838</a>{.reference
.external})</p>
</li>
<li>
<p>Removed deprecated [<code>scrapy.xpath</code>{.docutils .literal
.notranslate}]{.pre} module - use [<code>scrapy.selector</code>{.docutils
.literal .notranslate}]{.pre} instead.
(<a href="http://hg.scrapy.org/scrapy/changeset/1836">r1836</a>{.reference
.external})</p>
</li>
<li>
<p>Removed deprecated [<code>core.signals.domain_open</code>{.docutils .literal
.notranslate}]{.pre} signal - use
[<code>core.signals.domain_opened</code>{.docutils .literal
.notranslate}]{.pre} instead
(<a href="http://hg.scrapy.org/scrapy/changeset/1822">r1822</a>{.reference
.external})</p>
</li>
<li></li>
</ul>
<pre><code>[`log.msg()`{.docutils .literal .notranslate}]{.pre} now receives a [`spider`{.docutils .literal .notranslate}]{.pre} argument ([r1822](http://hg.scrapy.org/scrapy/changeset/1822){.reference .external})

:   -   Old domain argument has been deprecated and will be removed
        in 0.9. For spiders, you should always use the
        [`spider`{.docutils .literal .notranslate}]{.pre} argument
        and pass spider references. If you really want to pass a
        string, use the [`component`{.docutils .literal
        .notranslate}]{.pre} argument instead.
</code></pre>
<ul>
<li>
<p>Changed core signals [<code>domain_opened</code>{.docutils .literal
.notranslate}]{.pre}, [<code>domain_closed</code>{.docutils .literal
.notranslate}]{.pre}, [<code>domain_idle</code>{.docutils .literal
.notranslate}]{.pre}</p>
</li>
<li></li>
</ul>
<pre><code>Changed Item pipeline to use spiders instead of domains

:   -   The [`domain`{.docutils .literal .notranslate}]{.pre}
        argument of [`process_item()`{.docutils .literal
        .notranslate}]{.pre} item pipeline method was changed to
        [`spider`{.docutils .literal .notranslate}]{.pre}, the new
        signature is: [`process_item(spider,`{.docutils .literal
        .notranslate}]{.pre}` `{.docutils .literal
        .notranslate}[`item)`{.docutils .literal
        .notranslate}]{.pre}
        ([r1827](http://hg.scrapy.org/scrapy/changeset/1827){.reference
        .external} \| #105)

    -   To quickly port your code (to work with Scrapy 0.8) just use
        [`spider.domain_name`{.docutils .literal
        .notranslate}]{.pre} where you previously used
        [`domain`{.docutils .literal .notranslate}]{.pre}.
</code></pre>
<ul>
<li></li>
</ul>
<pre><code>Changed Stats API to use spiders instead of domains ([r1849](http://hg.scrapy.org/scrapy/changeset/1849){.reference .external} \| #113)

:   -   [`StatsCollector`{.docutils .literal .notranslate}]{.pre}
        was changed to receive spider references (instead of
        domains) in its methods ([`set_value`{.docutils .literal
        .notranslate}]{.pre}, [`inc_value`{.docutils .literal
        .notranslate}]{.pre}, etc).

    -   added [`StatsCollector.iter_spider_stats()`{.docutils
        .literal .notranslate}]{.pre} method

    -   removed [`StatsCollector.list_domains()`{.docutils .literal
        .notranslate}]{.pre} method

    -   Also, Stats signals were renamed and now pass around spider
        references (instead of domains). Here's a summary of the
        changes:

    -   To quickly port your code (to work with Scrapy 0.8) just use
        [`spider.domain_name`{.docutils .literal
        .notranslate}]{.pre} where you previously used
        [`domain`{.docutils .literal .notranslate}]{.pre}.
        [`spider_stats`{.docutils .literal .notranslate}]{.pre}
        contains exactly the same data as [`domain_stats`{.docutils
        .literal .notranslate}]{.pre}.
</code></pre>
<ul>
<li></li>
</ul>
<pre><code>[`CloseDomain`{.docutils .literal .notranslate}]{.pre} extension moved to [`scrapy.contrib.closespider.CloseSpider`{.docutils .literal .notranslate}]{.pre} ([r1833](http://hg.scrapy.org/scrapy/changeset/1833){.reference .external})

:   -   

        Its settings were also renamed:

        :   -   [`CLOSEDOMAIN_TIMEOUT`{.docutils .literal
                .notranslate}]{.pre} to
                [`CLOSESPIDER_TIMEOUT`{.docutils .literal
                .notranslate}]{.pre}

            -   [`CLOSEDOMAIN_ITEMCOUNT`{.docutils .literal
                .notranslate}]{.pre} to
                [`CLOSESPIDER_ITEMCOUNT`{.docutils .literal
                .notranslate}]{.pre}
</code></pre>
<ul>
<li>
<p>Removed deprecated [<code>SCRAPYSETTINGS_MODULE</code>{.docutils .literal
.notranslate}]{.pre} environment variable - use
[<code>SCRAPY_SETTINGS_MODULE</code>{.docutils .literal .notranslate}]{.pre}
instead
(<a href="http://hg.scrapy.org/scrapy/changeset/1840">r1840</a>{.reference
.external})</p>
</li>
<li>
<p>Renamed setting: [<code>REQUESTS_PER_DOMAIN</code>{.docutils .literal
.notranslate}]{.pre} to [<code>CONCURRENT_REQUESTS_PER_SPIDER</code>{.docutils
.literal .notranslate}]{.pre}
(<a href="http://hg.scrapy.org/scrapy/changeset/1830">r1830</a>{.reference
.external},
<a href="http://hg.scrapy.org/scrapy/changeset/1844">r1844</a>{.reference
.external})</p>
</li>
<li>
<p>Renamed setting: [<code>CONCURRENT_DOMAINS</code>{.docutils .literal
.notranslate}]{.pre} to [<code>CONCURRENT_SPIDERS</code>{.docutils .literal
.notranslate}]{.pre}
(<a href="http://hg.scrapy.org/scrapy/changeset/1830">r1830</a>{.reference
.external})</p>
</li>
<li>
<p>Refactored HTTP Cache middleware</p>
</li>
<li>
<p>HTTP Cache middleware has been heavily refactored, retaining the
same functionality except for the domain sectorization which was
removed.
(<a href="http://hg.scrapy.org/scrapy/changeset/1843">r1843</a>{.reference
.external} )</p>
</li>
<li>
<p>Renamed exception: [<code>DontCloseDomain</code>{.docutils .literal
.notranslate}]{.pre} to [<code>DontCloseSpider</code>{.docutils .literal
.notranslate}]{.pre}
(<a href="http://hg.scrapy.org/scrapy/changeset/1859">r1859</a>{.reference
.external} | #120)</p>
</li>
<li>
<p>Renamed extension: [<code>DelayedCloseDomain</code>{.docutils .literal
.notranslate}]{.pre} to [<code>SpiderCloseDelay</code>{.docutils .literal
.notranslate}]{.pre}
(<a href="http://hg.scrapy.org/scrapy/changeset/1861">r1861</a>{.reference
.external} | #121)</p>
</li>
<li>
<p>Removed obsolete
[<code>scrapy.utils.markup.remove_escape_chars</code>{.docutils .literal
.notranslate}]{.pre} function - use
[<code>scrapy.utils.markup.replace_escape_chars</code>{.docutils .literal
.notranslate}]{.pre} instead
(<a href="http://hg.scrapy.org/scrapy/changeset/1865">r1865</a>{.reference
.external})
:::
:::</p>
</li>
</ul>
<p>::: {#scrapy-0-7 .section}</p>
<h4 id="scrapy-07headerlink"><a class="header" href="#scrapy-07headerlink">Scrapy 0.7<a href="#scrapy-0-7" title="Permalink to this heading">Â¶</a>{.headerlink}</a></h4>
<p>First release of Scrapy.
:::
:::</p>
<p>[]{#document-contributing}</p>
<p>::: {#contributing-to-scrapy .section}
[]{#topics-contributing}</p>
<h3 id="contributing-to-scrapyheaderlink"><a class="header" href="#contributing-to-scrapyheaderlink">Contributing to Scrapy<a href="#contributing-to-scrapy" title="Permalink to this heading">Â¶</a>{.headerlink}</a></h3>
<p>::: {.admonition .important}
Important</p>
<p>Double check that you are reading the most recent version of this
document at
<a href="https://docs.scrapy.org/en/master/contributing.html">https://docs.scrapy.org/en/master/contributing.html</a>{.reference
.external}
:::</p>
<p>There are many ways to contribute to Scrapy. Here are some of them:</p>
<ul>
<li>
<p>Report bugs and request features in the <a href="https://github.com/scrapy/scrapy/issues">issue
tracker</a>{.reference
.external}, trying to follow the guidelines detailed in <a href="#reporting-bugs">Reporting
bugs</a>{.reference .internal} below.</p>
</li>
<li>
<p>Submit patches for new functionalities and/or bug fixes. Please read
<a href="#writing-patches">[Writing patches]{.std .std-ref}</a>{.hoverxref
.tooltip .reference .internal} and <a href="#id2">Submitting
patches</a>{.reference .internal} below for details on how to
write and submit a patch.</p>
</li>
<li>
<p>Blog about Scrapy. Tell the world how you're using Scrapy. This will
help newcomers with more examples and will help the Scrapy project
to increase its visibility.</p>
</li>
<li>
<p>Join the <a href="https://reddit.com/r/scrapy">Scrapy subreddit</a>{.reference
.external} and share your ideas on how to improve Scrapy. We're
always open to suggestions.</p>
</li>
<li>
<p>Answer Scrapy questions at <a href="https://stackoverflow.com/questions/tagged/scrapy">Stack
Overflow</a>{.reference
.external}.</p>
</li>
</ul>
<p>::: {#reporting-bugs .section}</p>
<h4 id="reporting-bugsheaderlink"><a class="header" href="#reporting-bugsheaderlink">Reporting bugs<a href="#reporting-bugs" title="Permalink to this heading">Â¶</a>{.headerlink}</a></h4>
<p>::: {.admonition .note}
Note</p>
<p>Please report security issues <strong>only</strong> to
<a href="mailto:scrapy-security%40googlegroups.com">scrapy-security@googlegroups.com</a>{.reference
.external}. This is a private list only open to trusted Scrapy
developers, and its archives are not public.
:::</p>
<p>Well-written bug reports are very helpful, so keep in mind the following
guidelines when you're going to report a new bug.</p>
<ul>
<li>
<p>check the <a href="index.html#faq">[FAQ]{.std .std-ref}</a>{.hoverxref .tooltip
.reference .internal} first to see if your issue is addressed in a
well-known question</p>
</li>
<li>
<p>if you have a general question about Scrapy usage, please ask it at
<a href="https://stackoverflow.com/questions/tagged/scrapy">Stack
Overflow</a>{.reference
.external} (use &quot;scrapy&quot; tag).</p>
</li>
<li>
<p>check the <a href="https://github.com/scrapy/scrapy/issues">open
issues</a>{.reference
.external} to see if the issue has already been reported. If it has,
don't dismiss the report, but check the ticket history and comments.
If you have additional useful information, please leave a comment,
or consider <a href="#writing-patches">[sending a pull request]{.std
.std-ref}</a>{.hoverxref .tooltip .reference
.internal} with a fix.</p>
</li>
<li>
<p>search the
<a href="https://groups.google.com/forum/#!forum/scrapy-users">scrapy-users</a>{.reference
.external} list and <a href="https://reddit.com/r/scrapy">Scrapy
subreddit</a>{.reference .external} to see
if it has been discussed there, or if you're not sure if what you're
seeing is a bug. You can also ask in the [<code>#scrapy</code>{.docutils
.literal .notranslate}]{.pre} IRC channel.</p>
</li>
<li>
<p>write <strong>complete, reproducible, specific bug reports</strong>. The smaller
the test case, the better. Remember that other developers won't have
your project to reproduce the bug, so please include all relevant
files required to reproduce it. See for example StackOverflow's
guide on creating a <a href="https://stackoverflow.com/help/mcve">Minimal, Complete, and Verifiable
example</a>{.reference .external}
exhibiting the issue.</p>
</li>
<li>
<p>the most awesome way to provide a complete reproducible example is
to send a pull request which adds a failing test case to the Scrapy
testing suite (see <a href="#submitting-patches">[Submitting patches]{.std
.std-ref}</a>{.hoverxref .tooltip .reference
.internal}). This is helpful even if you don't have an intention to
fix the issue yourselves.</p>
</li>
<li>
<p>include the output of [<code>scrapy</code>{.docutils .literal
.notranslate}]{.pre}<code> </code>{.docutils .literal
.notranslate}[<code>version</code>{.docutils .literal
.notranslate}]{.pre}<code> </code>{.docutils .literal
.notranslate}[<code>-v</code>{.docutils .literal .notranslate}]{.pre} so
developers working on your bug know exactly which version and
platform it occurred on, which is often very helpful for reproducing
it, or knowing if it was already fixed.
:::</p>
</li>
</ul>
<p>::: {#writing-patches .section}
[]{#id1}</p>
<h4 id="writing-patchesheaderlink"><a class="header" href="#writing-patchesheaderlink">Writing patches<a href="#writing-patches" title="Permalink to this heading">Â¶</a>{.headerlink}</a></h4>
<p>Scrapy has a list of <a href="https://github.com/scrapy/scrapy/issues?q=is%3Aissue+is%3Aopen+label%3A%22good+first+issue%22">good first
issues</a>{.reference
.external} and <a href="https://github.com/scrapy/scrapy/issues?q=is%3Aissue+is%3Aopen+label%3A%22help+wanted%22">help wanted
issues</a>{.reference
.external} that you can work on. These issues are a great way to get
started with contributing to Scrapy. If you're new to the codebase, you
may want to focus on documentation or testing-related issues, as they
are always useful and can help you get more familiar with the project.
You can also check Scrapy's <a href="https://app.codecov.io/gh/scrapy/scrapy">test
coverage</a>{.reference .external}
to see which areas may benefit from more tests.</p>
<p>The better a patch is written, the higher the chances that it'll get
accepted and the sooner it will be merged.</p>
<p>Well-written patches should:</p>
<ul>
<li>
<p>contain the minimum amount of code required for the specific change.
Small patches are easier to review and merge. So, if you're doing
more than one change (or bug fix), please consider submitting one
patch per change. Do not collapse multiple changes into a single
patch. For big changes consider using a patch queue.</p>
</li>
<li>
<p>pass all unit-tests. See <a href="#id6">Running tests</a>{.reference .internal}
below.</p>
</li>
<li>
<p>include one (or more) test cases that check the bug fixed or the new
functionality added. See <a href="#writing-tests">Writing tests</a>{.reference
.internal} below.</p>
</li>
<li>
<p>if you're adding or changing a public (documented) API, please
include the documentation changes in the same patch. See
<a href="#id5">Documentation policies</a>{.reference .internal} below.</p>
</li>
<li>
<p>if you're adding a private API, please add a regular expression to
the [<code>coverage_ignore_pyobjects</code>{.docutils .literal
.notranslate}]{.pre} variable of [<code>docs/conf.py</code>{.docutils .literal
.notranslate}]{.pre} to exclude the new private API from
documentation coverage checks.</p>
<p>To see if your private API is skipped properly, generate a
documentation coverage report as follows:</p>
<p>::: {.highlight-default .notranslate}
::: highlight
tox -e docs-coverage
:::
:::</p>
</li>
<li>
<p>if you are removing deprecated code, first make sure that at least 1
year (12 months) has passed since the release that introduced the
deprecation. See <a href="index.html#deprecation-policy">[Deprecation policy]{.std
.std-ref}</a>{.hoverxref .tooltip
.reference .internal}.
:::</p>
</li>
</ul>
<p>::: {#submitting-patches .section}
[]{#id2}</p>
<h4 id="submitting-patchesheaderlink"><a class="header" href="#submitting-patchesheaderlink">Submitting patches<a href="#submitting-patches" title="Permalink to this heading">Â¶</a>{.headerlink}</a></h4>
<p>The best way to submit a patch is to issue a <a href="https://help.github.com/en/github/collaborating-with-issues-and-pull-requests/creating-a-pull-request">pull
request</a>{.reference
.external} on GitHub, optionally creating a new issue first.</p>
<p>Remember to explain what was fixed or the new functionality (what it is,
why it's needed, etc). The more info you include, the easier will be for
core developers to understand and accept your patch.</p>
<p>You can also discuss the new functionality (or bug fix) before creating
the patch, but it's always good to have a patch ready to illustrate your
arguments and show that you have put some additional thought into the
subject. A good starting point is to send a pull request on GitHub. It
can be simple enough to illustrate your idea, and leave
documentation/tests for later, after the idea has been validated and
proven useful. Alternatively, you can start a conversation in the
<a href="https://reddit.com/r/scrapy">Scrapy subreddit</a>{.reference .external} to
discuss your idea first.</p>
<p>Sometimes there is an existing pull request for the problem you'd like
to solve, which is stalled for some reason. Often the pull request is in
a right direction, but changes are requested by Scrapy maintainers, and
the original pull request author hasn't had time to address them. In
this case consider picking up this pull request: open a new pull request
with all commits from the original pull request, as well as additional
changes to address the raised issues. Doing so helps a lot; it is not
considered rude as long as the original author is acknowledged by
keeping his/her commits.</p>
<p>You can pull an existing pull request to a local branch by running
[<code>git</code>{.docutils .literal .notranslate}]{.pre}<code> </code>{.docutils .literal
.notranslate}[<code>fetch</code>{.docutils .literal
.notranslate}]{.pre}<code> </code>{.docutils .literal
.notranslate}[<code>upstream</code>{.docutils .literal
.notranslate}]{.pre}<code> </code>{.docutils .literal
.notranslate}[<code>pull/$PR_NUMBER/head:$BRANCH_NAME_TO_CREATE</code>{.docutils
.literal .notranslate}]{.pre} (replace 'upstream' with a remote name for
scrapy repository, [<code>$PR_NUMBER</code>{.docutils .literal .notranslate}]{.pre}
with an ID of the pull request, and [<code>$BRANCH_NAME_TO_CREATE</code>{.docutils
.literal .notranslate}]{.pre} with a name of the branch you want to
create locally). See also:
<a href="https://help.github.com/en/github/collaborating-with-issues-and-pull-requests/checking-out-pull-requests-locally#modifying-an-inactive-pull-request-locally">https://help.github.com/en/github/collaborating-with-issues-and-pull-requests/checking-out-pull-requests-locally#modifying-an-inactive-pull-request-locally</a>{.reference
.external}.</p>
<p>When writing GitHub pull requests, try to keep titles short but
descriptive. E.g. For bug #411: &quot;Scrapy hangs if an exception raises in
start_requests&quot; prefer &quot;Fix hanging when exception occurs in
start_requests (#411)&quot; instead of &quot;Fix for #411&quot;. Complete titles make
it easy to skim through the issue tracker.</p>
<p>Finally, try to keep aesthetic changes ([]{#index-0 .target}<a href="https://peps.python.org/pep-0008/"><strong>PEP
8</strong></a>{.pep .reference .external}
compliance, unused imports removal, etc) in separate commits from
functional changes. This will make pull requests easier to review and
more likely to get merged.
:::</p>
<p>::: {#coding-style .section}
[]{#id3}</p>
<h4 id="coding-styleheaderlink"><a class="header" href="#coding-styleheaderlink">Coding style<a href="#coding-style" title="Permalink to this heading">Â¶</a>{.headerlink}</a></h4>
<p>Please follow these coding conventions when writing code for inclusion
in Scrapy:</p>
<ul>
<li>
<p>We use <a href="https://black.readthedocs.io/en/stable/">black</a>{.reference
.external} for code formatting. There is a hook in the pre-commit
config that will automatically format your code before every commit.
You can also run black manually with [<code>tox</code>{.docutils .literal
.notranslate}]{.pre}<code> </code>{.docutils .literal
.notranslate}[<code>-e</code>{.docutils .literal
.notranslate}]{.pre}<code> </code>{.docutils .literal
.notranslate}[<code>black</code>{.docutils .literal .notranslate}]{.pre}.</p>
</li>
<li>
<p>Don't put your name in the code you contribute; git provides enough
metadata to identify author of the code. See
<a href="https://help.github.com/en/github/using-git/setting-your-username-in-git">https://help.github.com/en/github/using-git/setting-your-username-in-git</a>{.reference
.external} for setup instructions.
:::</p>
</li>
</ul>
<p>::: {#pre-commit .section}
[]{#scrapy-pre-commit}</p>
<h4 id="pre-commitheaderlink"><a class="header" href="#pre-commitheaderlink">Pre-commit<a href="#pre-commit" title="Permalink to this heading">Â¶</a>{.headerlink}</a></h4>
<p>We use <a href="https://pre-commit.com/">pre-commit</a>{.reference .external} to
automatically address simple code issues before every commit.</p>
<p>After your create a local clone of your fork of the Scrapy repository:</p>
<ol>
<li>
<p><a href="https://pre-commit.com/#installation">Install
pre-commit</a>{.reference
.external}.</p>
</li>
<li>
<p>On the root of your local clone of the Scrapy repository, run the
following command:</p>
<p>::: {.highlight-bash .notranslate}
::: highlight
pre-commit install
:::
:::</p>
</li>
</ol>
<p>Now pre-commit will check your changes every time you create a Git
commit. Upon finding issues, pre-commit aborts your commit, and either
fixes those issues automatically, or only reports them to you. If it
fixes those issues automatically, creating your commit again should
succeed. Otherwise, you may need to address the corresponding issues
manually first.
:::</p>
<p>::: {#documentation-policies .section}
[]{#id5}</p>
<h4 id="documentation-policiesheaderlink"><a class="header" href="#documentation-policiesheaderlink">Documentation policies<a href="#documentation-policies" title="Permalink to this heading">Â¶</a>{.headerlink}</a></h4>
<p>For reference documentation of API members (classes, methods, etc.) use
docstrings and make sure that the Sphinx documentation uses the
<a href="https://www.sphinx-doc.org/en/master/usage/extensions/autodoc.html#module-sphinx.ext.autodoc" title="(in Sphinx v7.3.0)">[<code>autodoc</code>{.xref .py .py-mod .docutils .literal
.notranslate}]{.pre}</a>{.reference
.external} extension to pull the docstrings. API reference documentation
should follow docstring conventions (<a href="https://www.python.org/dev/peps/pep-0257/">PEP
257</a>{.reference .external})
and be IDE-friendly: short, to the point, and it may provide short
examples.</p>
<p>Other types of documentation, such as tutorials or topics, should be
covered in files within the [<code>docs/</code>{.docutils .literal
.notranslate}]{.pre} directory. This includes documentation that is
specific to an API member, but goes beyond API reference documentation.</p>
<p>In any case, if something is covered in a docstring, use the
<a href="https://www.sphinx-doc.org/en/master/usage/extensions/autodoc.html#module-sphinx.ext.autodoc" title="(in Sphinx v7.3.0)">[<code>autodoc</code>{.xref .py .py-mod .docutils .literal
.notranslate}]{.pre}</a>{.reference
.external} extension to pull the docstring into the documentation
instead of duplicating the docstring in files within the
[<code>docs/</code>{.docutils .literal .notranslate}]{.pre} directory.</p>
<p>Documentation updates that cover new or modified features must use
Sphinx's <a href="https://www.sphinx-doc.org/en/master/usage/restructuredtext/directives.html#directive-versionadded" title="(in Sphinx v7.3.0)">[<code>versionadded</code>{.xref .rst .rst-dir .docutils .literal
.notranslate}]{.pre}</a>{.reference
.external} and <a href="https://www.sphinx-doc.org/en/master/usage/restructuredtext/directives.html#directive-versionchanged" title="(in Sphinx v7.3.0)">[<code>versionchanged</code>{.xref .rst .rst-dir .docutils .literal
.notranslate}]{.pre}</a>{.reference
.external} directives. Use [<code>VERSION</code>{.docutils .literal
.notranslate}]{.pre} as version, we will replace it with the actual
version right before the corresponding release. When we release a new
major or minor version of Scrapy, we remove these directives if they are
older than 3 years.</p>
<p>Documentation about deprecated features must be removed as those
features are deprecated, so that new readers do not run into it. New
deprecations and deprecation removals are documented in the <a href="index.html#news">[release
notes]{.std .std-ref}</a>{.hoverxref .tooltip .reference
.internal}.
:::</p>
<p>::: {#tests .section}</p>
<h4 id="testsheaderlink"><a class="header" href="#testsheaderlink">Tests<a href="#tests" title="Permalink to this heading">Â¶</a>{.headerlink}</a></h4>
<p>Tests are implemented using the <a href="https://docs.twisted.org/en/stable/development/test-standard.html" title="(in Twisted v23.10)">[Twisted unit-testing framework]{.xref
.std
.std-doc}</a>{.reference
.external}. Running tests requires <a href="https://tox.wiki/en/latest/index.html" title="(in Python v4.11)">[tox]{.xref .std
.std-doc}</a>{.reference
.external}.</p>
<p>::: {#running-tests .section}
[]{#id6}</p>
<h5 id="running-testsheaderlink"><a class="header" href="#running-testsheaderlink">Running tests<a href="#running-tests" title="Permalink to this heading">Â¶</a>{.headerlink}</a></h5>
<p>To run all tests:</p>
<p>::: {.highlight-default .notranslate}
::: highlight
tox
:::
:::</p>
<p>To run a specific test (say [<code>tests/test_loader.py</code>{.docutils .literal
.notranslate}]{.pre}) use:</p>
<blockquote>
<div>
<p>[<code>tox</code>{.docutils .literal .notranslate}]{.pre}<code> </code>{.docutils .literal
.notranslate}[<code>--</code>{.docutils .literal
.notranslate}]{.pre}<code> </code>{.docutils .literal
.notranslate}[<code>tests/test_loader.py</code>{.docutils .literal
.notranslate}]{.pre}</p>
</div>
</blockquote>
<p>To run the tests on a specific <a href="https://tox.wiki/en/latest/index.html" title="(in Python v4.11)">[tox]{.xref .std
.std-doc}</a>{.reference
.external} environment, use [<code>-e</code>{.docutils .literal
.notranslate}]{.pre}<code> </code>{.docutils .literal
.notranslate}[<code>&lt;name&gt;</code>{.docutils .literal .notranslate}]{.pre} with an
environment name from [<code>tox.ini</code>{.docutils .literal
.notranslate}]{.pre}. For example, to run the tests with Python 3.10
use:</p>
<p>::: {.highlight-default .notranslate}
::: highlight
tox -e py310
:::
:::</p>
<p>You can also specify a comma-separated list of environments, and use
<a href="https://tox.wiki/en/latest/user_guide.html#parallel-mode" title="(in Python v4.11)">[tox's parallel mode]{.xref .std
.std-ref}</a>{.reference
.external} to run the tests on multiple environments in parallel:</p>
<p>::: {.highlight-default .notranslate}
::: highlight
tox -e py39,py310 -p auto
:::
:::</p>
<p>To pass command-line options to <a href="https://docs.pytest.org/en/latest/index.html" title="(in pytest v0.1.dev83+g81c06b3)">[pytest]{.xref .std
.std-doc}</a>{.reference
.external}, add them after [<code>--</code>{.docutils .literal .notranslate}]{.pre}
in your call to <a href="https://tox.wiki/en/latest/index.html" title="(in Python v4.11)">[tox]{.xref .std
.std-doc}</a>{.reference
.external}. Using [<code>--</code>{.docutils .literal .notranslate}]{.pre}
overrides the default positional arguments defined in
[<code>tox.ini</code>{.docutils .literal .notranslate}]{.pre}, so you must include
those default positional arguments ([<code>scrapy</code>{.docutils .literal
.notranslate}]{.pre}<code> </code>{.docutils .literal
.notranslate}[<code>tests</code>{.docutils .literal .notranslate}]{.pre}) after
[<code>--</code>{.docutils .literal .notranslate}]{.pre} as well:</p>
<p>::: {.highlight-default .notranslate}
::: highlight
tox -- scrapy tests -x  # stop after first failure
:::
:::</p>
<p>You can also use the
<a href="https://github.com/pytest-dev/pytest-xdist">pytest-xdist</a>{.reference
.external} plugin. For example, to run all tests on the Python 3.10
<a href="https://tox.wiki/en/latest/index.html" title="(in Python v4.11)">[tox]{.xref .std
.std-doc}</a>{.reference
.external} environment using all your CPU cores:</p>
<p>::: {.highlight-default .notranslate}
::: highlight
tox -e py310 -- scrapy tests -n auto
:::
:::</p>
<p>To see coverage report install <a href="https://coverage.readthedocs.io/en/latest/index.html" title="(in Coverage.py v7.3.2)">[coverage]{.xref .std
.std-doc}</a>{.reference
.external} ([<code>pip</code>{.docutils .literal .notranslate}]{.pre}<code> </code>{.docutils
.literal .notranslate}[<code>install</code>{.docutils .literal
.notranslate}]{.pre}<code> </code>{.docutils .literal
.notranslate}[<code>coverage</code>{.docutils .literal .notranslate}]{.pre}) and
run:</p>
<blockquote>
<div>
<p>[<code>coverage</code>{.docutils .literal .notranslate}]{.pre}<code> </code>{.docutils
.literal .notranslate}[<code>report</code>{.docutils .literal
.notranslate}]{.pre}</p>
</div>
</blockquote>
<p>see output of [<code>coverage</code>{.docutils .literal
.notranslate}]{.pre}<code> </code>{.docutils .literal
.notranslate}[<code>--help</code>{.docutils .literal .notranslate}]{.pre} for more
options like html or xml report.
:::</p>
<p>::: {#writing-tests .section}</p>
<h5 id="writing-testsheaderlink"><a class="header" href="#writing-testsheaderlink">Writing tests<a href="#writing-tests" title="Permalink to this heading">Â¶</a>{.headerlink}</a></h5>
<p>All functionality (including new features and bug fixes) must include a
test case to check that it works as expected, so please include tests
for your patches if you want them to get accepted sooner.</p>
<p>Scrapy uses unit-tests, which are located in the
<a href="https://github.com/scrapy/scrapy/tree/master/tests">tests/</a>{.reference
.external} directory. Their module name typically resembles the full
path of the module they're testing. For example, the item loaders code
is in:</p>
<p>::: {.highlight-default .notranslate}
::: highlight
scrapy.loader
:::
:::</p>
<p>And their unit-tests are in:</p>
<p>::: {.highlight-default .notranslate}
::: highlight
tests/test_loader.py
:::
:::
:::
:::
:::</p>
<p>[]{#document-versioning}</p>
<p>::: {#versioning-and-api-stability .section}
[]{#versioning}</p>
<h3 id="versioning-and-api-stabilityheaderlink"><a class="header" href="#versioning-and-api-stabilityheaderlink">Versioning and API stability<a href="#versioning-and-api-stability" title="Permalink to this heading">Â¶</a>{.headerlink}</a></h3>
<p>::: {#id1 .section}</p>
<h4 id="versioningheaderlink"><a class="header" href="#versioningheaderlink">Versioning<a href="#id1" title="Permalink to this heading">Â¶</a>{.headerlink}</a></h4>
<p>There are 3 numbers in a Scrapy version: <em>A.B.C</em></p>
<ul>
<li>
<p><em>A</em> is the major version. This will rarely change and will signify
very large changes.</p>
</li>
<li>
<p><em>B</em> is the release number. This will include many changes including
features and things that possibly break backward compatibility,
although we strive to keep these cases at a minimum.</p>
</li>
<li>
<p><em>C</em> is the bugfix release number.</p>
</li>
</ul>
<p>Backward-incompatibilities are explicitly mentioned in the <a href="index.html#news">[release
notes]{.std .std-ref}</a>{.hoverxref .tooltip .reference
.internal}, and may require special attention before upgrading.</p>
<p>Development releases do not follow 3-numbers version and are generally
released as [<code>dev</code>{.docutils .literal .notranslate}]{.pre} suffixed
versions, e.g. [<code>1.3dev</code>{.docutils .literal .notranslate}]{.pre}.</p>
<p>::: {.admonition .note}
Note</p>
<p>With Scrapy 0.* series, Scrapy used <a href="https://en.wikipedia.org/wiki/Software_versioning#Odd-numbered_versions_for_development_releases">odd-numbered versions for
development
releases</a>{.reference
.external}. This is not the case anymore from Scrapy 1.0 onwards.</p>
<p>Starting with Scrapy 1.0, all releases should be considered
production-ready.
:::</p>
<p>For example:</p>
<ul>
<li><em>1.1.1</em> is the first bugfix release of the <em>1.1</em> series (safe to use
in production)
:::</li>
</ul>
<p>::: {#api-stability .section}</p>
<h4 id="api-stabilityheaderlink"><a class="header" href="#api-stabilityheaderlink">API stability<a href="#api-stability" title="Permalink to this heading">Â¶</a>{.headerlink}</a></h4>
<p>API stability was one of the major goals for the <em>1.0</em> release.</p>
<p>Methods or functions that start with a single dash ([<code>_</code>{.docutils
.literal .notranslate}]{.pre}) are private and should never be relied as
stable.</p>
<p>Also, keep in mind that stable doesn't mean complete: stable APIs could
grow new methods or functionality but the existing methods should keep
working the same way.
:::</p>
<p>::: {#deprecation-policy .section}
[]{#id2}</p>
<h4 id="deprecation-policyheaderlink"><a class="header" href="#deprecation-policyheaderlink">Deprecation policy<a href="#deprecation-policy" title="Permalink to this heading">Â¶</a>{.headerlink}</a></h4>
<p>We aim to maintain support for deprecated Scrapy features for at least 1
year.</p>
<p>For example, if a feature is deprecated in a Scrapy version released on
June 15th 2020, that feature should continue to work in versions
released on June 14th 2021 or before that.</p>
<p>Any new Scrapy release after a year <em>may</em> remove support for that
deprecated feature.</p>
<p>All deprecated features removed in a Scrapy release are explicitly
mentioned in the <a href="index.html#news">[release notes]{.std
.std-ref}</a>{.hoverxref .tooltip .reference .internal}.
:::
:::
:::</p>
<p><a href="index.html#document-news">[Release notes]{.doc}</a>{.reference .internal}</p>
<p>:   See what has changed in recent Scrapy versions.</p>
<p><a href="index.html#document-contributing">[Contributing to Scrapy]{.doc}</a>{.reference .internal}</p>
<p>:   Learn how to contribute to the Scrapy project.</p>
<p><a href="index.html#document-versioning">[Versioning and API stability]{.doc}</a>{.reference .internal}</p>
<p>:   Understand Scrapy versioning and API stability.
:::
:::
:::
:::</p>
<hr />
<p>::: {role=&quot;contentinfo&quot;}
Â© Copyright 2008--2023, Scrapy developers. [Revision <code>70ba3a08</code>.
]{.commit} [Last updated on Nov 30, 2023. ]{.lastupdated}
:::</p>
<p>Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
<a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a> provided by
<a href="https://readthedocs.org">Read the Docs</a>.
:::
:::
:::
:::</p>
<p>::: {.rst-versions toggle=&quot;rst-versions&quot; role=&quot;note&quot; aria-label=&quot;Versions&quot;}
[ [ Read the Docs]{.fa .fa-book} v: master []{.fa .fa-caret-down}
]{.rst-current-version toggle=&quot;rst-current-version&quot;}</p>
<p>::: rst-other-versions</p>
<p>Versions
:   <a href="/en/master/">master</a>
:   <a href="/en/latest/">latest</a>
:   <a href="/en/stable/">stable</a>
:   <a href="/en/2.11/">2.11</a>
:   <a href="/en/2.10/">2.10</a>
:   <a href="/en/2.9/">2.9</a>
:   <a href="/en/2.8/">2.8</a>
:   <a href="/en/2.7/">2.7</a>
:   <a href="/en/2.6/">2.6</a>
:   <a href="/en/2.5/">2.5</a>
:   <a href="/en/2.4/">2.4</a>
:   <a href="/en/2.3/">2.3</a>
:   <a href="/en/2.2/">2.2</a>
:   <a href="/en/2.1/">2.1</a>
:   <a href="/en/2.0/">2.0</a>
:   <a href="/en/1.8/">1.8</a>
:   <a href="/en/1.7/">1.7</a>
:   <a href="/en/1.6/">1.6</a>
:   <a href="/en/1.5/">1.5</a>
:   <a href="/en/1.4/">1.4</a>
:   <a href="/en/1.3/">1.3</a>
:   <a href="/en/1.2/">1.2</a>
:   <a href="/en/1.1/">1.1</a>
:   <a href="/en/1.0/">1.0</a>
:   <a href="/en/0.24/">0.24</a>
:   <a href="/en/0.22/">0.22</a>
:   <a href="/en/0.20/">0.20</a>
:   <a href="/en/0.18/">0.18</a>
:   <a href="/en/0.16/">0.16</a>
:   <a href="/en/0.14/">0.14</a>
:   <a href="/en/0.12/">0.12</a>
:   <a href="/en/0.10.3/">0.10.3</a>
:   <a href="/en/0.9/">0.9</a>
:   <a href="/en/xpath-tutorial/">xpath-tutorial</a></p>
<pre><code class="language-{=html}">&lt;!-- --&gt;
</code></pre>
<p>Downloads
:   <a href="//docs.scrapy.org/_/downloads/en/master/pdf/">pdf</a>
:   <a href="//docs.scrapy.org/_/downloads/en/master/htmlzip/">html</a>
:   <a href="//docs.scrapy.org/_/downloads/en/master/epub/">epub</a></p>
<pre><code class="language-{=html}">&lt;!-- --&gt;
</code></pre>
<p>On Read the Docs
:   <a href="//readthedocs.org/projects/scrapy/?fromdocs=scrapy">Project Home</a>
:   <a href="//readthedocs.org/builds/scrapy/?fromdocs=scrapy">Builds</a>
:::
:::</p>

                    </main>

                    <nav class="nav-wrapper" aria-label="Page navigation">
                        <!-- Mobile navigation buttons -->
                            <a rel="prev" href="../Scrapy/Scrapy9.html" class="mobile-nav-chapters previous" title="Previous chapter" aria-label="Previous chapter" aria-keyshortcuts="Left">
                                <i class="fa fa-angle-left"></i>
                            </a>


                        <div style="clear: both"></div>
                    </nav>
                </div>
            </div>

            <nav class="nav-wide-wrapper" aria-label="Page navigation">
                    <a rel="prev" href="../Scrapy/Scrapy9.html" class="nav-chapters previous" title="Previous chapter" aria-label="Previous chapter" aria-keyshortcuts="Left">
                        <i class="fa fa-angle-left"></i>
                    </a>

            </nav>

        </div>




        <script>
            window.playground_copyable = true;
        </script>


        <script src="../elasticlunr.min.js"></script>
        <script src="../mark.min.js"></script>
        <script src="../searcher.js"></script>

        <script src="../clipboard.min.js"></script>
        <script src="../highlight.js"></script>
        <script src="../book.js"></script>

        <!-- Custom JS scripts -->


    </div>
    </body>
</html>
