<!DOCTYPE HTML>
<html lang="en" class="sidebar-visible no-js light">
    <head>
        <!-- Book generated using mdBook -->
        <meta charset="UTF-8">
        <title>Scrapy7 - KnowledgeBase</title>


        <!-- Custom HTML head -->
        
        <meta name="description" content="">
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <meta name="theme-color" content="#ffffff" />

        <link rel="icon" href="../favicon.svg">
        <link rel="shortcut icon" href="../favicon.png">
        <link rel="stylesheet" href="../css/variables.css">
        <link rel="stylesheet" href="../css/general.css">
        <link rel="stylesheet" href="../css/chrome.css">
        <link rel="stylesheet" href="../css/print.css" media="print">

        <!-- Fonts -->
        <link rel="stylesheet" href="../FontAwesome/css/font-awesome.css">
        <link rel="stylesheet" href="../fonts/fonts.css">

        <!-- Highlight.js Stylesheets -->
        <link rel="stylesheet" href="../highlight.css">
        <link rel="stylesheet" href="../tomorrow-night.css">
        <link rel="stylesheet" href="../ayu-highlight.css">

        <!-- Custom theme stylesheets -->

    </head>
    <body>
    <div id="body-container">
        <!-- Provide site root to javascript -->
        <script>
            var path_to_root = "../";
            var default_theme = window.matchMedia("(prefers-color-scheme: dark)").matches ? "navy" : "light";
        </script>

        <!-- Work around some values being stored in localStorage wrapped in quotes -->
        <script>
            try {
                var theme = localStorage.getItem('mdbook-theme');
                var sidebar = localStorage.getItem('mdbook-sidebar');

                if (theme.startsWith('"') && theme.endsWith('"')) {
                    localStorage.setItem('mdbook-theme', theme.slice(1, theme.length - 1));
                }

                if (sidebar.startsWith('"') && sidebar.endsWith('"')) {
                    localStorage.setItem('mdbook-sidebar', sidebar.slice(1, sidebar.length - 1));
                }
            } catch (e) { }
        </script>

        <!-- Set the theme before any content is loaded, prevents flash -->
        <script>
            var theme;
            try { theme = localStorage.getItem('mdbook-theme'); } catch(e) { }
            if (theme === null || theme === undefined) { theme = default_theme; }
            var html = document.querySelector('html');
            html.classList.remove('no-js')
            html.classList.remove('light')
            html.classList.add(theme);
            html.classList.add('js');
        </script>

        <!-- Hide / unhide sidebar before it is displayed -->
        <script>
            var html = document.querySelector('html');
            var sidebar = null;
            if (document.body.clientWidth >= 1080) {
                try { sidebar = localStorage.getItem('mdbook-sidebar'); } catch(e) { }
                sidebar = sidebar || 'visible';
            } else {
                sidebar = 'hidden';
            }
            html.classList.remove('sidebar-visible');
            html.classList.add("sidebar-" + sidebar);
        </script>

        <nav id="sidebar" class="sidebar" aria-label="Table of contents">
            <div class="sidebar-scrollbox">
                <ol class="chapter"><li class="chapter-item expanded "><a href="../Scrapy/Scrapy.html"><strong aria-hidden="true">1.</strong> Scrapy</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="../Scrapy/Scrapy1.html"><strong aria-hidden="true">1.1.</strong> Scrapy1</a></li><li class="chapter-item expanded "><a href="../Scrapy/Scrapy2.html"><strong aria-hidden="true">1.2.</strong> Scrapy2</a></li><li class="chapter-item expanded "><a href="../Scrapy/Scrapy3.html"><strong aria-hidden="true">1.3.</strong> Scrapy3</a></li><li class="chapter-item expanded "><a href="../Scrapy/Scrapy4.html"><strong aria-hidden="true">1.4.</strong> Scrapy4</a></li><li class="chapter-item expanded "><a href="../Scrapy/Scrapy5.html"><strong aria-hidden="true">1.5.</strong> Scrapy5</a></li><li class="chapter-item expanded "><a href="../Scrapy/Scrapy6.html"><strong aria-hidden="true">1.6.</strong> Scrapy6</a></li><li class="chapter-item expanded "><a href="../Scrapy/Scrapy7.html" class="active"><strong aria-hidden="true">1.7.</strong> Scrapy7</a></li><li class="chapter-item expanded "><a href="../Scrapy/Scrapy8.html"><strong aria-hidden="true">1.8.</strong> Scrapy8</a></li><li class="chapter-item expanded "><a href="../Scrapy/Scrapy9.html"><strong aria-hidden="true">1.9.</strong> Scrapy9</a></li><li class="chapter-item expanded "><a href="../Scrapy/Scrapy10.html"><strong aria-hidden="true">1.10.</strong> Scrapy10</a></li></ol></li><li class="chapter-item expanded "><a href="../ThinkPython/ThinkPython.html"><strong aria-hidden="true">2.</strong> ThinkPython</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="../ThinkPython/part_1.html"><strong aria-hidden="true">2.1.</strong> ThinkPython1</a></li><li class="chapter-item expanded "><a href="../ThinkPython/part_2.html"><strong aria-hidden="true">2.2.</strong> ThinkPython2</a></li><li class="chapter-item expanded "><a href="../ThinkPython/part_3.html"><strong aria-hidden="true">2.3.</strong> ThinkPython3</a></li><li class="chapter-item expanded "><a href="../ThinkPython/part_4.html"><strong aria-hidden="true">2.4.</strong> ThinkPython4</a></li><li class="chapter-item expanded "><a href="../ThinkPython/part_5.html"><strong aria-hidden="true">2.5.</strong> ThinkPython5</a></li><li class="chapter-item expanded "><a href="../ThinkPython/part_6.html"><strong aria-hidden="true">2.6.</strong> ThinkPython6</a></li><li class="chapter-item expanded "><a href="../ThinkPython/part_7.html"><strong aria-hidden="true">2.7.</strong> ThinkPython7</a></li><li class="chapter-item expanded "><a href="../ThinkPython/part_8.html"><strong aria-hidden="true">2.8.</strong> ThinkPython8</a></li><li class="chapter-item expanded "><a href="../ThinkPython/part_9.html"><strong aria-hidden="true">2.9.</strong> ThinkPython9</a></li><li class="chapter-item expanded "><a href="../ThinkPython/part_10.html"><strong aria-hidden="true">2.10.</strong> ThinkPython10</a></li></ol></li><li class="chapter-item expanded "><a href="../C-sharp-docs/C-sharp-docs.html"><strong aria-hidden="true">3.</strong> C-sharp-docs</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="../C-sharp-docs/part1.html"><strong aria-hidden="true">3.1.</strong> Csharp1</a></li><li class="chapter-item expanded "><a href="../C-sharp-docs/part2.html"><strong aria-hidden="true">3.2.</strong> Csharp2</a></li><li class="chapter-item expanded "><a href="../C-sharp-docs/part3.html"><strong aria-hidden="true">3.3.</strong> Csharp3</a></li><li class="chapter-item expanded "><a href="../C-sharp-docs/part4.html"><strong aria-hidden="true">3.4.</strong> Csharp4</a></li><li class="chapter-item expanded "><a href="../C-sharp-docs/part5.html"><strong aria-hidden="true">3.5.</strong> Csharp5</a></li><li class="chapter-item expanded "><a href="../C-sharp-docs/part6.html"><strong aria-hidden="true">3.6.</strong> Csharp6</a></li><li class="chapter-item expanded "><a href="../C-sharp-docs/part7.html"><strong aria-hidden="true">3.7.</strong> Csharp7</a></li><li class="chapter-item expanded "><a href="../C-sharp-docs/part8.html"><strong aria-hidden="true">3.8.</strong> Csharp8</a></li><li class="chapter-item expanded "><a href="../C-sharp-docs/part9.html"><strong aria-hidden="true">3.9.</strong> Csharp9</a></li><li class="chapter-item expanded "><a href="../C-sharp-docs/part10.html"><strong aria-hidden="true">3.10.</strong> Csharp10</a></li></ol></li><li class="chapter-item expanded "><a href="../php-docs/php-docs.html"><strong aria-hidden="true">4.</strong> php-docs</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="../php-docs/part1.html"><strong aria-hidden="true">4.1.</strong> php1</a></li><li class="chapter-item expanded "><a href="../php-docs/part2.html"><strong aria-hidden="true">4.2.</strong> php2</a></li><li class="chapter-item expanded "><a href="../php-docs/part3.html"><strong aria-hidden="true">4.3.</strong> php3</a></li><li class="chapter-item expanded "><a href="../php-docs/part4.html"><strong aria-hidden="true">4.4.</strong> php4</a></li><li class="chapter-item expanded "><a href="../php-docs/part5.html"><strong aria-hidden="true">4.5.</strong> php5</a></li><li class="chapter-item expanded "><a href="../php-docs/part6.html"><strong aria-hidden="true">4.6.</strong> php6</a></li><li class="chapter-item expanded "><a href="../php-docs/part7.html"><strong aria-hidden="true">4.7.</strong> php7</a></li><li class="chapter-item expanded "><a href="../php-docs/part8.html"><strong aria-hidden="true">4.8.</strong> php8</a></li><li class="chapter-item expanded "><a href="../php-docs/part9.html"><strong aria-hidden="true">4.9.</strong> php9</a></li><li class="chapter-item expanded "><a href="../php-docs/part10.html"><strong aria-hidden="true">4.10.</strong> php10</a></li></ol></li></ol>
            </div>
            <div id="sidebar-resize-handle" class="sidebar-resize-handle"></div>
        </nav>

        <div id="page-wrapper" class="page-wrapper">

            <div class="page">
                                <div id="menu-bar-hover-placeholder"></div>
                <div id="menu-bar" class="menu-bar sticky bordered">
                    <div class="left-buttons">
                        <button id="sidebar-toggle" class="icon-button" type="button" title="Toggle Table of Contents" aria-label="Toggle Table of Contents" aria-controls="sidebar">
                            <i class="fa fa-bars"></i>
                        </button>
                        <button id="theme-toggle" class="icon-button" type="button" title="Change theme" aria-label="Change theme" aria-haspopup="true" aria-expanded="false" aria-controls="theme-list">
                            <i class="fa fa-paint-brush"></i>
                        </button>
                        <ul id="theme-list" class="theme-popup" aria-label="Themes" role="menu">
                            <li role="none"><button role="menuitem" class="theme" id="light">Light</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="rust">Rust</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="coal">Coal</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="navy">Navy</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="ayu">Ayu</button></li>
                        </ul>
                    </div>

                    <h1 class="menu-title">KnowledgeBase</h1>

                    <div class="right-buttons">
                        <a href="../print.html" title="Print this book" aria-label="Print this book">
                            <i id="print-button" class="fa fa-print"></i>
                        </a>

                    </div>
                </div>


                <!-- Apply ARIA attributes after the sidebar and the sidebar toggle button are added to the DOM -->
                <script>
                    document.getElementById('sidebar-toggle').setAttribute('aria-expanded', sidebar === 'visible');
                    document.getElementById('sidebar').setAttribute('aria-hidden', sidebar !== 'visible');
                    Array.from(document.querySelectorAll('#sidebar a')).forEach(function(link) {
                        link.setAttribute('tabIndex', sidebar === 'visible' ? 0 : -1);
                    });
                </script>

                <div id="content" class="content">
                    <main>
                        <pre><code>    -   **response** ([[`Response`{.xref .py .py-class .docutils
        .literal
        .notranslate}]{.pre}](index.html#scrapy.http.Response &quot;scrapy.http.Response&quot;){.reference
        .internal} object) -- the response from where the item was
        scraped
</code></pre>
<p>:::</p>
<p>::: {#item-dropped .section}</p>
<h6 id="item_droppedheaderlink"><a class="header" href="#item_droppedheaderlink">item_dropped<a href="#item-dropped" title="Permalink to this heading">¶</a>{.headerlink}</a></h6>
<p>[]{#std-signal-item_dropped .target}</p>
<p>[[scrapy.signals.]{.pre}]{.sig-prename .descclassname}[[item_dropped]{.pre}]{.sig-name .descname}[(]{.sig-paren}<em>[[item]{.pre}]{.n}</em>, <em>[[response]{.pre}]{.n}</em>, <em>[[exception]{.pre}]{.n}</em>, <em>[[spider]{.pre}]{.n}</em>[)]{.sig-paren}<a href="#scrapy.signals.item_dropped" title="Permalink to this definition">¶</a>{.headerlink}</p>
<p>:   Sent after an item has been dropped from the <a href="index.html#topics-item-pipeline">[Item Pipeline]{.std
.std-ref}</a>{.hoverxref .tooltip
.reference .internal} when some stage raised a <a href="index.html#scrapy.exceptions.DropItem" title="scrapy.exceptions.DropItem">[<code>DropItem</code>{.xref
.py .py-exc .docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal} exception.</p>
<pre><code>This signal supports returning deferreds from its handlers.

Parameters

:   -   **item** ([[item object]{.std
        .std-ref}](index.html#item-types){.hoverxref .tooltip
        .reference .internal}) -- the item dropped from the [[Item
        Pipeline]{.std
        .std-ref}](index.html#topics-item-pipeline){.hoverxref
        .tooltip .reference .internal}

    -   **spider** ([[`Spider`{.xref .py .py-class .docutils
        .literal
        .notranslate}]{.pre}](index.html#scrapy.Spider &quot;scrapy.Spider&quot;){.reference
        .internal} object) -- the spider which scraped the item

    -   **response** ([[`Response`{.xref .py .py-class .docutils
        .literal
        .notranslate}]{.pre}](index.html#scrapy.http.Response &quot;scrapy.http.Response&quot;){.reference
        .internal} object) -- the response from where the item was
        dropped

    -   **exception** ([[`DropItem`{.xref .py .py-exc .docutils
        .literal
        .notranslate}]{.pre}](index.html#scrapy.exceptions.DropItem &quot;scrapy.exceptions.DropItem&quot;){.reference
        .internal} exception) -- the exception (which must be a
        [[`DropItem`{.xref .py .py-exc .docutils .literal
        .notranslate}]{.pre}](index.html#scrapy.exceptions.DropItem &quot;scrapy.exceptions.DropItem&quot;){.reference
        .internal} subclass) which caused the item to be dropped
</code></pre>
<p>:::</p>
<p>::: {#item-error .section}</p>
<h6 id="item_errorheaderlink"><a class="header" href="#item_errorheaderlink">item_error<a href="#item-error" title="Permalink to this heading">¶</a>{.headerlink}</a></h6>
<p>[]{#std-signal-item_error .target}</p>
<p>[[scrapy.signals.]{.pre}]{.sig-prename .descclassname}[[item_error]{.pre}]{.sig-name .descname}[(]{.sig-paren}<em>[[item]{.pre}]{.n}</em>, <em>[[response]{.pre}]{.n}</em>, <em>[[spider]{.pre}]{.n}</em>, <em>[[failure]{.pre}]{.n}</em>[)]{.sig-paren}<a href="#scrapy.signals.item_error" title="Permalink to this definition">¶</a>{.headerlink}</p>
<p>:   Sent when a <a href="index.html#topics-item-pipeline">[Item Pipeline]{.std
.std-ref}</a>{.hoverxref .tooltip
.reference .internal} generates an error (i.e. raises an exception),
except <a href="index.html#scrapy.exceptions.DropItem" title="scrapy.exceptions.DropItem">[<code>DropItem</code>{.xref .py .py-exc .docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal} exception.</p>
<pre><code>This signal supports returning deferreds from its handlers.

Parameters

:   -   **item** ([[item object]{.std
        .std-ref}](index.html#item-types){.hoverxref .tooltip
        .reference .internal}) -- the item that caused the error in
        the [[Item Pipeline]{.std
        .std-ref}](index.html#topics-item-pipeline){.hoverxref
        .tooltip .reference .internal}

    -   **response** ([[`Response`{.xref .py .py-class .docutils
        .literal
        .notranslate}]{.pre}](index.html#scrapy.http.Response &quot;scrapy.http.Response&quot;){.reference
        .internal} object) -- the response being processed when the
        exception was raised

    -   **spider** ([[`Spider`{.xref .py .py-class .docutils
        .literal
        .notranslate}]{.pre}](index.html#scrapy.Spider &quot;scrapy.Spider&quot;){.reference
        .internal} object) -- the spider which raised the exception

    -   **failure**
        ([*twisted.python.failure.Failure*](https://docs.twisted.org/en/stable/api/twisted.python.failure.Failure.html &quot;(in Twisted)&quot;){.reference
        .external}) -- the exception raised
</code></pre>
<p>:::
:::</p>
<p>::: {#spider-signals .section}</p>
<h5 id="spider-signalsheaderlink"><a class="header" href="#spider-signalsheaderlink">Spider signals<a href="#spider-signals" title="Permalink to this heading">¶</a>{.headerlink}</a></h5>
<p>::: {#spider-closed .section}</p>
<h6 id="spider_closedheaderlink"><a class="header" href="#spider_closedheaderlink">spider_closed<a href="#spider-closed" title="Permalink to this heading">¶</a>{.headerlink}</a></h6>
<p>[]{#std-signal-spider_closed .target}</p>
<p>[[scrapy.signals.]{.pre}]{.sig-prename .descclassname}[[spider_closed]{.pre}]{.sig-name .descname}[(]{.sig-paren}<em>[[spider]{.pre}]{.n}</em>, <em>[[reason]{.pre}]{.n}</em>[)]{.sig-paren}<a href="#scrapy.signals.spider_closed" title="Permalink to this definition">¶</a>{.headerlink}</p>
<p>:   Sent after a spider has been closed. This can be used to release
per-spider resources reserved on <a href="#std-signal-spider_opened">[<code>spider_opened</code>{.xref .std
.std-signal .docutils .literal
.notranslate}]{.pre}</a>{.hoverxref .tooltip
.reference .internal}.</p>
<pre><code>This signal supports returning deferreds from its handlers.

Parameters

:   -   **spider** ([[`Spider`{.xref .py .py-class .docutils
        .literal
        .notranslate}]{.pre}](index.html#scrapy.Spider &quot;scrapy.Spider&quot;){.reference
        .internal} object) -- the spider which has been closed

    -   **reason**
        ([*str*](https://docs.python.org/3/library/stdtypes.html#str &quot;(in Python v3.12)&quot;){.reference
        .external}) -- a string which describes the reason why the
        spider was closed. If it was closed because the spider has
        completed scraping, the reason is [`'finished'`{.docutils
        .literal .notranslate}]{.pre}. Otherwise, if the spider was
        manually closed by calling the [`close_spider`{.docutils
        .literal .notranslate}]{.pre} engine method, then the reason
        is the one passed in the [`reason`{.docutils .literal
        .notranslate}]{.pre} argument of that method (which defaults
        to [`'cancelled'`{.docutils .literal .notranslate}]{.pre}).
        If the engine was shutdown (for example, by hitting Ctrl-C
        to stop it) the reason will be [`'shutdown'`{.docutils
        .literal .notranslate}]{.pre}.
</code></pre>
<p>:::</p>
<p>::: {#spider-opened .section}</p>
<h6 id="spider_openedheaderlink"><a class="header" href="#spider_openedheaderlink">spider_opened<a href="#spider-opened" title="Permalink to this heading">¶</a>{.headerlink}</a></h6>
<p>[]{#std-signal-spider_opened .target}</p>
<p>[[scrapy.signals.]{.pre}]{.sig-prename .descclassname}[[spider_opened]{.pre}]{.sig-name .descname}[(]{.sig-paren}<em>[[spider]{.pre}]{.n}</em>[)]{.sig-paren}<a href="#scrapy.signals.spider_opened" title="Permalink to this definition">¶</a>{.headerlink}</p>
<p>:   Sent after a spider has been opened for crawling. This is typically
used to reserve per-spider resources, but can be used for any task
that needs to be performed when a spider is opened.</p>
<pre><code>This signal supports returning deferreds from its handlers.

Parameters

:   **spider** ([[`Spider`{.xref .py .py-class .docutils .literal
    .notranslate}]{.pre}](index.html#scrapy.Spider &quot;scrapy.Spider&quot;){.reference
    .internal} object) -- the spider which has been opened
</code></pre>
<p>:::</p>
<p>::: {#spider-idle .section}</p>
<h6 id="spider_idleheaderlink"><a class="header" href="#spider_idleheaderlink">spider_idle<a href="#spider-idle" title="Permalink to this heading">¶</a>{.headerlink}</a></h6>
<p>[]{#std-signal-spider_idle .target}</p>
<p>[[scrapy.signals.]{.pre}]{.sig-prename .descclassname}[[spider_idle]{.pre}]{.sig-name .descname}[(]{.sig-paren}<em>[[spider]{.pre}]{.n}</em>[)]{.sig-paren}<a href="#scrapy.signals.spider_idle" title="Permalink to this definition">¶</a>{.headerlink}</p>
<p>:   Sent when a spider has gone idle, which means the spider has no
further:</p>
<pre><code>&gt; &lt;div&gt;
&gt;
&gt; -   requests waiting to be downloaded
&gt;
&gt; -   requests scheduled
&gt;
&gt; -   items being processed in the item pipeline
&gt;
&gt; &lt;/div&gt;

If the idle state persists after all handlers of this signal have
finished, the engine starts closing the spider. After the spider has
finished closing, the [[`spider_closed`{.xref .std .std-signal
.docutils .literal
.notranslate}]{.pre}](#std-signal-spider_closed){.hoverxref .tooltip
.reference .internal} signal is sent.

You may raise a [[`DontCloseSpider`{.xref .py .py-exc .docutils
.literal
.notranslate}]{.pre}](index.html#scrapy.exceptions.DontCloseSpider &quot;scrapy.exceptions.DontCloseSpider&quot;){.reference
.internal} exception to prevent the spider from being closed.

Alternatively, you may raise a [[`CloseSpider`{.xref .py .py-exc
.docutils .literal
.notranslate}]{.pre}](index.html#scrapy.exceptions.CloseSpider &quot;scrapy.exceptions.CloseSpider&quot;){.reference
.internal} exception to provide a custom spider closing reason. An
idle handler is the perfect place to put some code that assesses the
final spider results and update the final closing reason accordingly
(e.g. setting it to 'too_few_results' instead of 'finished').

This signal does not support returning deferreds from its handlers.

Parameters

:   **spider** ([[`Spider`{.xref .py .py-class .docutils .literal
    .notranslate}]{.pre}](index.html#scrapy.Spider &quot;scrapy.Spider&quot;){.reference
    .internal} object) -- the spider which has gone idle
</code></pre>
<p>::: {.admonition .note}
Note</p>
<p>Scheduling some requests in your <a href="#std-signal-spider_idle">[<code>spider_idle</code>{.xref .std .std-signal
.docutils .literal
.notranslate}]{.pre}</a>{.hoverxref .tooltip
.reference .internal} handler does <strong>not</strong> guarantee that it can prevent
the spider from being closed, although it sometimes can. That's because
the spider may still remain idle if all the scheduled requests are
rejected by the scheduler (e.g. filtered due to duplication).
:::
:::</p>
<p>::: {#spider-error .section}</p>
<h6 id="spider_errorheaderlink"><a class="header" href="#spider_errorheaderlink">spider_error<a href="#spider-error" title="Permalink to this heading">¶</a>{.headerlink}</a></h6>
<p>[]{#std-signal-spider_error .target}</p>
<p>[[scrapy.signals.]{.pre}]{.sig-prename .descclassname}[[spider_error]{.pre}]{.sig-name .descname}[(]{.sig-paren}<em>[[failure]{.pre}]{.n}</em>, <em>[[response]{.pre}]{.n}</em>, <em>[[spider]{.pre}]{.n}</em>[)]{.sig-paren}<a href="#scrapy.signals.spider_error" title="Permalink to this definition">¶</a>{.headerlink}</p>
<p>:   Sent when a spider callback generates an error (i.e. raises an
exception).</p>
<pre><code>This signal does not support returning deferreds from its handlers.

Parameters

:   -   **failure**
        ([*twisted.python.failure.Failure*](https://docs.twisted.org/en/stable/api/twisted.python.failure.Failure.html &quot;(in Twisted)&quot;){.reference
        .external}) -- the exception raised

    -   **response** ([[`Response`{.xref .py .py-class .docutils
        .literal
        .notranslate}]{.pre}](index.html#scrapy.http.Response &quot;scrapy.http.Response&quot;){.reference
        .internal} object) -- the response being processed when the
        exception was raised

    -   **spider** ([[`Spider`{.xref .py .py-class .docutils
        .literal
        .notranslate}]{.pre}](index.html#scrapy.Spider &quot;scrapy.Spider&quot;){.reference
        .internal} object) -- the spider which raised the exception
</code></pre>
<p>:::</p>
<p>::: {#feed-slot-closed .section}</p>
<h6 id="feed_slot_closedheaderlink"><a class="header" href="#feed_slot_closedheaderlink">feed_slot_closed<a href="#feed-slot-closed" title="Permalink to this heading">¶</a>{.headerlink}</a></h6>
<p>[]{#std-signal-feed_slot_closed .target}</p>
<p>[[scrapy.signals.]{.pre}]{.sig-prename .descclassname}[[feed_slot_closed]{.pre}]{.sig-name .descname}[(]{.sig-paren}<em>[[slot]{.pre}]{.n}</em>[)]{.sig-paren}<a href="#scrapy.signals.feed_slot_closed" title="Permalink to this definition">¶</a>{.headerlink}</p>
<p>:   Sent when a <a href="index.html#topics-feed-exports">[feed exports]{.std
.std-ref}</a>{.hoverxref .tooltip
.reference .internal} slot is closed.</p>
<pre><code>This signal supports returning deferreds from its handlers.

Parameters

:   **slot** (*scrapy.extensions.feedexport.FeedSlot*) -- the slot
    closed
</code></pre>
<p>:::</p>
<p>::: {#feed-exporter-closed .section}</p>
<h6 id="feed_exporter_closedheaderlink"><a class="header" href="#feed_exporter_closedheaderlink">feed_exporter_closed<a href="#feed-exporter-closed" title="Permalink to this heading">¶</a>{.headerlink}</a></h6>
<p>[]{#std-signal-feed_exporter_closed .target}</p>
<p>[[scrapy.signals.]{.pre}]{.sig-prename .descclassname}[[feed_exporter_closed]{.pre}]{.sig-name .descname}[(]{.sig-paren}[)]{.sig-paren}<a href="#scrapy.signals.feed_exporter_closed" title="Permalink to this definition">¶</a>{.headerlink}</p>
<p>:   Sent when the <a href="index.html#topics-feed-exports">[feed exports]{.std
.std-ref}</a>{.hoverxref .tooltip
.reference .internal} extension is closed, during the handling of
the <a href="#std-signal-spider_closed">[<code>spider_closed</code>{.xref .std .std-signal .docutils .literal
.notranslate}]{.pre}</a>{.hoverxref .tooltip
.reference .internal} signal by the extension, after all feed
exporting has been handled.</p>
<pre><code>This signal supports returning deferreds from its handlers.
</code></pre>
<p>:::
:::</p>
<p>::: {#request-signals .section}</p>
<h5 id="request-signalsheaderlink"><a class="header" href="#request-signalsheaderlink">Request signals<a href="#request-signals" title="Permalink to this heading">¶</a>{.headerlink}</a></h5>
<p>::: {#request-scheduled .section}</p>
<h6 id="request_scheduledheaderlink"><a class="header" href="#request_scheduledheaderlink">request_scheduled<a href="#request-scheduled" title="Permalink to this heading">¶</a>{.headerlink}</a></h6>
<p>[]{#std-signal-request_scheduled .target}</p>
<p>[[scrapy.signals.]{.pre}]{.sig-prename .descclassname}[[request_scheduled]{.pre}]{.sig-name .descname}[(]{.sig-paren}<em>[[request]{.pre}]{.n}</em>, <em>[[spider]{.pre}]{.n}</em>[)]{.sig-paren}<a href="#scrapy.signals.request_scheduled" title="Permalink to this definition">¶</a>{.headerlink}</p>
<p>:   Sent when the engine schedules a [<code>Request</code>{.xref .py .py-class
.docutils .literal .notranslate}]{.pre}, to be downloaded later.</p>
<pre><code>This signal does not support returning deferreds from its handlers.

Parameters

:   -   **request** ([`Request`{.xref .py .py-class .docutils
        .literal .notranslate}]{.pre} object) -- the request that
        reached the scheduler

    -   **spider** ([[`Spider`{.xref .py .py-class .docutils
        .literal
        .notranslate}]{.pre}](index.html#scrapy.Spider &quot;scrapy.Spider&quot;){.reference
        .internal} object) -- the spider that yielded the request
</code></pre>
<p>:::</p>
<p>::: {#request-dropped .section}</p>
<h6 id="request_droppedheaderlink"><a class="header" href="#request_droppedheaderlink">request_dropped<a href="#request-dropped" title="Permalink to this heading">¶</a>{.headerlink}</a></h6>
<p>[]{#std-signal-request_dropped .target}</p>
<p>[[scrapy.signals.]{.pre}]{.sig-prename .descclassname}[[request_dropped]{.pre}]{.sig-name .descname}[(]{.sig-paren}<em>[[request]{.pre}]{.n}</em>, <em>[[spider]{.pre}]{.n}</em>[)]{.sig-paren}<a href="#scrapy.signals.request_dropped" title="Permalink to this definition">¶</a>{.headerlink}</p>
<p>:   Sent when a [<code>Request</code>{.xref .py .py-class .docutils .literal
.notranslate}]{.pre}, scheduled by the engine to be downloaded
later, is rejected by the scheduler.</p>
<pre><code>This signal does not support returning deferreds from its handlers.

Parameters

:   -   **request** ([`Request`{.xref .py .py-class .docutils
        .literal .notranslate}]{.pre} object) -- the request that
        reached the scheduler

    -   **spider** ([[`Spider`{.xref .py .py-class .docutils
        .literal
        .notranslate}]{.pre}](index.html#scrapy.Spider &quot;scrapy.Spider&quot;){.reference
        .internal} object) -- the spider that yielded the request
</code></pre>
<p>:::</p>
<p>::: {#request-reached-downloader .section}</p>
<h6 id="request_reached_downloaderheaderlink"><a class="header" href="#request_reached_downloaderheaderlink">request_reached_downloader<a href="#request-reached-downloader" title="Permalink to this heading">¶</a>{.headerlink}</a></h6>
<p>[]{#std-signal-request_reached_downloader .target}</p>
<p>[[scrapy.signals.]{.pre}]{.sig-prename .descclassname}[[request_reached_downloader]{.pre}]{.sig-name .descname}[(]{.sig-paren}<em>[[request]{.pre}]{.n}</em>, <em>[[spider]{.pre}]{.n}</em>[)]{.sig-paren}<a href="#scrapy.signals.request_reached_downloader" title="Permalink to this definition">¶</a>{.headerlink}</p>
<p>:   Sent when a [<code>Request</code>{.xref .py .py-class .docutils .literal
.notranslate}]{.pre} reached downloader.</p>
<pre><code>This signal does not support returning deferreds from its handlers.

Parameters

:   -   **request** ([`Request`{.xref .py .py-class .docutils
        .literal .notranslate}]{.pre} object) -- the request that
        reached downloader

    -   **spider** ([[`Spider`{.xref .py .py-class .docutils
        .literal
        .notranslate}]{.pre}](index.html#scrapy.Spider &quot;scrapy.Spider&quot;){.reference
        .internal} object) -- the spider that yielded the request
</code></pre>
<p>:::</p>
<p>::: {#request-left-downloader .section}</p>
<h6 id="request_left_downloaderheaderlink"><a class="header" href="#request_left_downloaderheaderlink">request_left_downloader<a href="#request-left-downloader" title="Permalink to this heading">¶</a>{.headerlink}</a></h6>
<p>[]{#std-signal-request_left_downloader .target}</p>
<p>[[scrapy.signals.]{.pre}]{.sig-prename .descclassname}[[request_left_downloader]{.pre}]{.sig-name .descname}[(]{.sig-paren}<em>[[request]{.pre}]{.n}</em>, <em>[[spider]{.pre}]{.n}</em>[)]{.sig-paren}<a href="#scrapy.signals.request_left_downloader" title="Permalink to this definition">¶</a>{.headerlink}</p>
<p>:   ::: versionadded
[New in version 2.0.]{.versionmodified .added}
:::</p>
<pre><code>Sent when a [`Request`{.xref .py .py-class .docutils .literal
.notranslate}]{.pre} leaves the downloader, even in case of failure.

This signal does not support returning deferreds from its handlers.

Parameters

:   -   **request** ([`Request`{.xref .py .py-class .docutils
        .literal .notranslate}]{.pre} object) -- the request that
        reached the downloader

    -   **spider** ([[`Spider`{.xref .py .py-class .docutils
        .literal
        .notranslate}]{.pre}](index.html#scrapy.Spider &quot;scrapy.Spider&quot;){.reference
        .internal} object) -- the spider that yielded the request
</code></pre>
<p>:::</p>
<p>::: {#bytes-received .section}</p>
<h6 id="bytes_receivedheaderlink"><a class="header" href="#bytes_receivedheaderlink">bytes_received<a href="#bytes-received" title="Permalink to this heading">¶</a>{.headerlink}</a></h6>
<p>::: versionadded
[New in version 2.2.]{.versionmodified .added}
:::</p>
<p>[]{#std-signal-bytes_received .target}</p>
<p>[[scrapy.signals.]{.pre}]{.sig-prename .descclassname}[[bytes_received]{.pre}]{.sig-name .descname}[(]{.sig-paren}<em>[[data]{.pre}]{.n}</em>, <em>[[request]{.pre}]{.n}</em>, <em>[[spider]{.pre}]{.n}</em>[)]{.sig-paren}<a href="#scrapy.signals.bytes_received" title="Permalink to this definition">¶</a>{.headerlink}</p>
<p>:   Sent by the HTTP 1.1 and S3 download handlers when a group of bytes
is received for a specific request. This signal might be fired
multiple times for the same request, with partial data each time.
For instance, a possible scenario for a 25 kb response would be two
signals fired with 10 kb of data, and a final one with 5 kb of data.</p>
<pre><code>Handlers for this signal can stop the download of a response while
it is in progress by raising the [[`StopDownload`{.xref .py .py-exc
.docutils .literal
.notranslate}]{.pre}](index.html#scrapy.exceptions.StopDownload &quot;scrapy.exceptions.StopDownload&quot;){.reference
.internal} exception. Please refer to the [[Stopping the download of
a Response]{.std
.std-ref}](index.html#topics-stop-response-download){.hoverxref
.tooltip .reference .internal} topic for additional information and
examples.

This signal does not support returning deferreds from its handlers.

Parameters

:   -   **data** ([[`bytes`{.xref .py .py-class .docutils .literal
        .notranslate}]{.pre}](https://docs.python.org/3/library/stdtypes.html#bytes &quot;(in Python v3.12)&quot;){.reference
        .external} object) -- the data received by the download
        handler

    -   **request** ([`Request`{.xref .py .py-class .docutils
        .literal .notranslate}]{.pre} object) -- the request that
        generated the download

    -   **spider** ([[`Spider`{.xref .py .py-class .docutils
        .literal
        .notranslate}]{.pre}](index.html#scrapy.Spider &quot;scrapy.Spider&quot;){.reference
        .internal} object) -- the spider associated with the
        response
</code></pre>
<p>:::</p>
<p>::: {#headers-received .section}</p>
<h6 id="headers_receivedheaderlink"><a class="header" href="#headers_receivedheaderlink">headers_received<a href="#headers-received" title="Permalink to this heading">¶</a>{.headerlink}</a></h6>
<p>::: versionadded
[New in version 2.5.]{.versionmodified .added}
:::</p>
<p>[]{#std-signal-headers_received .target}</p>
<p>[[scrapy.signals.]{.pre}]{.sig-prename .descclassname}[[headers_received]{.pre}]{.sig-name .descname}[(]{.sig-paren}<em>[[headers]{.pre}]{.n}</em>, <em>[[body_length]{.pre}]{.n}</em>, <em>[[request]{.pre}]{.n}</em>, <em>[[spider]{.pre}]{.n}</em>[)]{.sig-paren}<a href="#scrapy.signals.headers_received" title="Permalink to this definition">¶</a>{.headerlink}</p>
<p>:   Sent by the HTTP 1.1 and S3 download handlers when the response
headers are available for a given request, before downloading any
additional content.</p>
<pre><code>Handlers for this signal can stop the download of a response while
it is in progress by raising the [[`StopDownload`{.xref .py .py-exc
.docutils .literal
.notranslate}]{.pre}](index.html#scrapy.exceptions.StopDownload &quot;scrapy.exceptions.StopDownload&quot;){.reference
.internal} exception. Please refer to the [[Stopping the download of
a Response]{.std
.std-ref}](index.html#topics-stop-response-download){.hoverxref
.tooltip .reference .internal} topic for additional information and
examples.

This signal does not support returning deferreds from its handlers.

Parameters

:   -   **headers** ([`scrapy.http.headers.Headers`{.xref .py
        .py-class .docutils .literal .notranslate}]{.pre} object) --
        the headers received by the download handler

    -   **body_length** (int) -- expected size of the response body,
        in bytes

    -   **request** ([`Request`{.xref .py .py-class .docutils
        .literal .notranslate}]{.pre} object) -- the request that
        generated the download

    -   **spider** ([[`Spider`{.xref .py .py-class .docutils
        .literal
        .notranslate}]{.pre}](index.html#scrapy.Spider &quot;scrapy.Spider&quot;){.reference
        .internal} object) -- the spider associated with the
        response
</code></pre>
<p>:::
:::</p>
<p>::: {#response-signals .section}</p>
<h5 id="response-signalsheaderlink"><a class="header" href="#response-signalsheaderlink">Response signals<a href="#response-signals" title="Permalink to this heading">¶</a>{.headerlink}</a></h5>
<p>::: {#response-received .section}</p>
<h6 id="response_receivedheaderlink"><a class="header" href="#response_receivedheaderlink">response_received<a href="#response-received" title="Permalink to this heading">¶</a>{.headerlink}</a></h6>
<p>[]{#std-signal-response_received .target}</p>
<p>[[scrapy.signals.]{.pre}]{.sig-prename .descclassname}[[response_received]{.pre}]{.sig-name .descname}[(]{.sig-paren}<em>[[response]{.pre}]{.n}</em>, <em>[[request]{.pre}]{.n}</em>, <em>[[spider]{.pre}]{.n}</em>[)]{.sig-paren}<a href="#scrapy.signals.response_received" title="Permalink to this definition">¶</a>{.headerlink}</p>
<p>:   Sent when the engine receives a new <a href="index.html#scrapy.http.Response" title="scrapy.http.Response">[<code>Response</code>{.xref .py .py-class
.docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal} from the downloader.</p>
<pre><code>This signal does not support returning deferreds from its handlers.

Parameters

:   -   **response** ([[`Response`{.xref .py .py-class .docutils
        .literal
        .notranslate}]{.pre}](index.html#scrapy.http.Response &quot;scrapy.http.Response&quot;){.reference
        .internal} object) -- the response received

    -   **request** ([`Request`{.xref .py .py-class .docutils
        .literal .notranslate}]{.pre} object) -- the request that
        generated the response

    -   **spider** ([[`Spider`{.xref .py .py-class .docutils
        .literal
        .notranslate}]{.pre}](index.html#scrapy.Spider &quot;scrapy.Spider&quot;){.reference
        .internal} object) -- the spider for which the response is
        intended
</code></pre>
<p>::: {.admonition .note}
Note</p>
<p>The [<code>request</code>{.docutils .literal .notranslate}]{.pre} argument might
not contain the original request that reached the downloader, if a
<a href="index.html#topics-downloader-middleware">[Downloader Middleware]{.std
.std-ref}</a>{.hoverxref .tooltip
.reference .internal} modifies the <a href="index.html#scrapy.http.Response" title="scrapy.http.Response">[<code>Response</code>{.xref .py .py-class
.docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal} object and sets a specific [<code>request</code>{.docutils .literal
.notranslate}]{.pre} attribute.
:::
:::</p>
<p>::: {#response-downloaded .section}</p>
<h6 id="response_downloadedheaderlink"><a class="header" href="#response_downloadedheaderlink">response_downloaded<a href="#response-downloaded" title="Permalink to this heading">¶</a>{.headerlink}</a></h6>
<p>[]{#std-signal-response_downloaded .target}</p>
<p>[[scrapy.signals.]{.pre}]{.sig-prename .descclassname}[[response_downloaded]{.pre}]{.sig-name .descname}[(]{.sig-paren}<em>[[response]{.pre}]{.n}</em>, <em>[[request]{.pre}]{.n}</em>, <em>[[spider]{.pre}]{.n}</em>[)]{.sig-paren}<a href="#scrapy.signals.response_downloaded" title="Permalink to this definition">¶</a>{.headerlink}</p>
<p>:   Sent by the downloader right after a [<code>HTTPResponse</code>{.docutils
.literal .notranslate}]{.pre} is downloaded.</p>
<pre><code>This signal does not support returning deferreds from its handlers.

Parameters

:   -   **response** ([[`Response`{.xref .py .py-class .docutils
        .literal
        .notranslate}]{.pre}](index.html#scrapy.http.Response &quot;scrapy.http.Response&quot;){.reference
        .internal} object) -- the response downloaded

    -   **request** ([`Request`{.xref .py .py-class .docutils
        .literal .notranslate}]{.pre} object) -- the request that
        generated the response

    -   **spider** ([[`Spider`{.xref .py .py-class .docutils
        .literal
        .notranslate}]{.pre}](index.html#scrapy.Spider &quot;scrapy.Spider&quot;){.reference
        .internal} object) -- the spider for which the response is
        intended
</code></pre>
<p>:::
:::
:::
:::</p>
<p>[]{#document-topics/scheduler}</p>
<p>::: {#module-scrapy.core.scheduler .section}
[]{#scheduler}[]{#topics-scheduler}</p>
<h3 id="schedulerheaderlink"><a class="header" href="#schedulerheaderlink">Scheduler<a href="#module-scrapy.core.scheduler" title="Permalink to this heading">¶</a>{.headerlink}</a></h3>
<p>The scheduler component receives requests from the <a href="index.html#component-engine">[engine]{.std
.std-ref}</a>{.hoverxref .tooltip .reference
.internal} and stores them into persistent and/or non-persistent data
structures. It also gets those requests and feeds them back to the
engine when it asks for a next request to be downloaded.</p>
<p>::: {#overriding-the-default-scheduler .section}</p>
<h4 id="overriding-the-default-schedulerheaderlink"><a class="header" href="#overriding-the-default-schedulerheaderlink">Overriding the default scheduler<a href="#overriding-the-default-scheduler" title="Permalink to this heading">¶</a>{.headerlink}</a></h4>
<p>You can use your own custom scheduler class by supplying its full Python
path in the <a href="index.html#std-setting-SCHEDULER">[<code>SCHEDULER</code>{.xref .std .std-setting .docutils .literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal} setting.
:::</p>
<p>::: {#minimal-scheduler-interface .section}</p>
<h4 id="minimal-scheduler-interfaceheaderlink"><a class="header" href="#minimal-scheduler-interfaceheaderlink">Minimal scheduler interface<a href="#minimal-scheduler-interface" title="Permalink to this heading">¶</a>{.headerlink}</a></h4>
<p><em>[class]{.pre}[ ]{.w}</em>[[scrapy.core.scheduler.]{.pre}]{.sig-prename .descclassname}[[BaseScheduler]{.pre}]{.sig-name .descname}<a href="_modules/scrapy/core/scheduler.html#BaseScheduler">[[[source]]{.pre}]{.viewcode-link}</a>{.reference .internal}<a href="#scrapy.core.scheduler.BaseScheduler" title="Permalink to this definition">¶</a>{.headerlink}</p>
<p>:   The scheduler component is responsible for storing requests received
from the engine, and feeding them back upon request (also to the
engine).</p>
<pre><code>The original sources of said requests are:

-   Spider: [`start_requests`{.docutils .literal
    .notranslate}]{.pre} method, requests created for URLs in the
    [`start_urls`{.docutils .literal .notranslate}]{.pre} attribute,
    request callbacks

-   Spider middleware: [`process_spider_output`{.docutils .literal
    .notranslate}]{.pre} and [`process_spider_exception`{.docutils
    .literal .notranslate}]{.pre} methods

-   Downloader middleware: [`process_request`{.docutils .literal
    .notranslate}]{.pre}, [`process_response`{.docutils .literal
    .notranslate}]{.pre} and [`process_exception`{.docutils .literal
    .notranslate}]{.pre} methods

The order in which the scheduler returns its stored requests (via
the [`next_request`{.docutils .literal .notranslate}]{.pre} method)
plays a great part in determining the order in which those requests
are downloaded.

The methods defined in this class constitute the minimal interface
that the Scrapy engine will interact with.

[[close]{.pre}]{.sig-name .descname}[(]{.sig-paren}*[[reason]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[[[str]{.pre}](https://docs.python.org/3/library/stdtypes.html#str &quot;(in Python v3.12)&quot;){.reference .external}]{.n}*[)]{.sig-paren} [[→]{.sig-return-icon} [[[Optional]{.pre}](https://docs.python.org/3/library/typing.html#typing.Optional &quot;(in Python v3.12)&quot;){.reference .external}[[\[]{.pre}]{.p}[[Deferred]{.pre}](https://docs.twisted.org/en/stable/api/twisted.internet.defer.Deferred.html &quot;(in Twisted)&quot;){.reference .external}[[\]]{.pre}]{.p}]{.sig-return-typehint}]{.sig-return}[[[\[source\]]{.pre}]{.viewcode-link}](_modules/scrapy/core/scheduler.html#BaseScheduler.close){.reference .internal}[¶](#scrapy.core.scheduler.BaseScheduler.close &quot;Permalink to this definition&quot;){.headerlink}

:   Called when the spider is closed by the engine. It receives the
    reason why the crawl finished as argument and it's useful to
    execute cleaning code.

    Parameters

    :   **reason** ([[`str`{.xref .py .py-class .docutils .literal
        .notranslate}]{.pre}](https://docs.python.org/3/library/stdtypes.html#str &quot;(in Python v3.12)&quot;){.reference
        .external}) -- a string which describes the reason why the
        spider was closed

*[abstract]{.pre}[ ]{.w}*[[enqueue_request]{.pre}]{.sig-name .descname}[(]{.sig-paren}*[[request]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[[[Request]{.pre}](index.html#scrapy.http.Request &quot;scrapy.http.request.Request&quot;){.reference .internal}]{.n}*[)]{.sig-paren} [[→]{.sig-return-icon} [[[bool]{.pre}](https://docs.python.org/3/library/functions.html#bool &quot;(in Python v3.12)&quot;){.reference .external}]{.sig-return-typehint}]{.sig-return}[[[\[source\]]{.pre}]{.viewcode-link}](_modules/scrapy/core/scheduler.html#BaseScheduler.enqueue_request){.reference .internal}[¶](#scrapy.core.scheduler.BaseScheduler.enqueue_request &quot;Permalink to this definition&quot;){.headerlink}

:   Process a request received by the engine.

    Return [`True`{.docutils .literal .notranslate}]{.pre} if the
    request is stored correctly, [`False`{.docutils .literal
    .notranslate}]{.pre} otherwise.

    If [`False`{.docutils .literal .notranslate}]{.pre}, the engine
    will fire a [`request_dropped`{.docutils .literal
    .notranslate}]{.pre} signal, and will not make further attempts
    to schedule the request at a later time. For reference, the
    default Scrapy scheduler returns [`False`{.docutils .literal
    .notranslate}]{.pre} when the request is rejected by the
    dupefilter.

*[classmethod]{.pre}[ ]{.w}*[[from_crawler]{.pre}]{.sig-name .descname}[(]{.sig-paren}*[[crawler]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[[[Crawler]{.pre}](index.html#scrapy.crawler.Crawler &quot;scrapy.crawler.Crawler&quot;){.reference .internal}]{.n}*[)]{.sig-paren} [[→]{.sig-return-icon} [[Self]{.pre}]{.sig-return-typehint}]{.sig-return}[[[\[source\]]{.pre}]{.viewcode-link}](_modules/scrapy/core/scheduler.html#BaseScheduler.from_crawler){.reference .internal}[¶](#scrapy.core.scheduler.BaseScheduler.from_crawler &quot;Permalink to this definition&quot;){.headerlink}

:   Factory method which receives the current [[`Crawler`{.xref .py
    .py-class .docutils .literal
    .notranslate}]{.pre}](index.html#scrapy.crawler.Crawler &quot;scrapy.crawler.Crawler&quot;){.reference
    .internal} object as argument.

*[abstract]{.pre}[ ]{.w}*[[has_pending_requests]{.pre}]{.sig-name .descname}[(]{.sig-paren}[)]{.sig-paren} [[→]{.sig-return-icon} [[[bool]{.pre}](https://docs.python.org/3/library/functions.html#bool &quot;(in Python v3.12)&quot;){.reference .external}]{.sig-return-typehint}]{.sig-return}[[[\[source\]]{.pre}]{.viewcode-link}](_modules/scrapy/core/scheduler.html#BaseScheduler.has_pending_requests){.reference .internal}[¶](#scrapy.core.scheduler.BaseScheduler.has_pending_requests &quot;Permalink to this definition&quot;){.headerlink}

:   [`True`{.docutils .literal .notranslate}]{.pre} if the scheduler
    has enqueued requests, [`False`{.docutils .literal
    .notranslate}]{.pre} otherwise

*[abstract]{.pre}[ ]{.w}*[[next_request]{.pre}]{.sig-name .descname}[(]{.sig-paren}[)]{.sig-paren} [[→]{.sig-return-icon} [[[Optional]{.pre}](https://docs.python.org/3/library/typing.html#typing.Optional &quot;(in Python v3.12)&quot;){.reference .external}[[\[]{.pre}]{.p}[[Request]{.pre}](index.html#scrapy.http.Request &quot;scrapy.http.request.Request&quot;){.reference .internal}[[\]]{.pre}]{.p}]{.sig-return-typehint}]{.sig-return}[[[\[source\]]{.pre}]{.viewcode-link}](_modules/scrapy/core/scheduler.html#BaseScheduler.next_request){.reference .internal}[¶](#scrapy.core.scheduler.BaseScheduler.next_request &quot;Permalink to this definition&quot;){.headerlink}

:   Return the next [[`Request`{.xref .py .py-class .docutils
    .literal
    .notranslate}]{.pre}](index.html#scrapy.http.Request &quot;scrapy.http.Request&quot;){.reference
    .internal} to be processed, or [`None`{.docutils .literal
    .notranslate}]{.pre} to indicate that there are no requests to
    be considered ready at the moment.

    Returning [`None`{.docutils .literal .notranslate}]{.pre}
    implies that no request from the scheduler will be sent to the
    downloader in the current reactor cycle. The engine will
    continue calling [`next_request`{.docutils .literal
    .notranslate}]{.pre} until [`has_pending_requests`{.docutils
    .literal .notranslate}]{.pre} is [`False`{.docutils .literal
    .notranslate}]{.pre}.

[[open]{.pre}]{.sig-name .descname}[(]{.sig-paren}*[[spider]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[[[Spider]{.pre}](index.html#scrapy.spiders.Spider &quot;scrapy.spiders.Spider&quot;){.reference .internal}]{.n}*[)]{.sig-paren} [[→]{.sig-return-icon} [[[Optional]{.pre}](https://docs.python.org/3/library/typing.html#typing.Optional &quot;(in Python v3.12)&quot;){.reference .external}[[\[]{.pre}]{.p}[[Deferred]{.pre}](https://docs.twisted.org/en/stable/api/twisted.internet.defer.Deferred.html &quot;(in Twisted)&quot;){.reference .external}[[\]]{.pre}]{.p}]{.sig-return-typehint}]{.sig-return}[[[\[source\]]{.pre}]{.viewcode-link}](_modules/scrapy/core/scheduler.html#BaseScheduler.open){.reference .internal}[¶](#scrapy.core.scheduler.BaseScheduler.open &quot;Permalink to this definition&quot;){.headerlink}

:   Called when the spider is opened by the engine. It receives the
    spider instance as argument and it's useful to execute
    initialization code.

    Parameters

    :   **spider** ([[`Spider`{.xref .py .py-class .docutils
        .literal
        .notranslate}]{.pre}](index.html#scrapy.spiders.Spider &quot;scrapy.spiders.Spider&quot;){.reference
        .internal}) -- the spider object for the current crawl
</code></pre>
<p>:::</p>
<p>::: {#default-scrapy-scheduler .section}</p>
<h4 id="default-scrapy-schedulerheaderlink"><a class="header" href="#default-scrapy-schedulerheaderlink">Default Scrapy scheduler<a href="#default-scrapy-scheduler" title="Permalink to this heading">¶</a>{.headerlink}</a></h4>
<p><em>[class]{.pre}[ ]{.w}</em>[[scrapy.core.scheduler.]{.pre}]{.sig-prename .descclassname}[[Scheduler]{.pre}]{.sig-name .descname}[(]{.sig-paren}<em>[[dupefilter]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[[BaseDupeFilter]{.pre}]{.n}</em>, <em>[[jobdir]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[<a href="https://docs.python.org/3/library/typing.html#typing.Optional" title="(in Python v3.12)">[Optional]{.pre}</a>{.reference .external}[[[]{.pre}]{.p}<a href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.12)">[str]{.pre}</a>{.reference .external}[[]]{.pre}]{.p}]{.n}[ ]{.w}[[=]{.pre}]{.o}[ ]{.w}[[None]{.pre}]{.default_value}</em>, <em>[[dqclass]{.pre}]{.n}[[=]{.pre}]{.o}[[None]{.pre}]{.default_value}</em>, <em>[[mqclass]{.pre}]{.n}[[=]{.pre}]{.o}[[None]{.pre}]{.default_value}</em>, <em>[[logunser]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[<a href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.12)">[bool]{.pre}</a>{.reference .external}]{.n}[ ]{.w}[[=]{.pre}]{.o}[ ]{.w}[[False]{.pre}]{.default_value}</em>, <em>[[stats]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[<a href="https://docs.python.org/3/library/typing.html#typing.Optional" title="(in Python v3.12)">[Optional]{.pre}</a>{.reference .external}[[[]{.pre}]{.p}<a href="index.html#scrapy.statscollectors.StatsCollector" title="scrapy.statscollectors.StatsCollector">[StatsCollector]{.pre}</a>{.reference .internal}[[]]{.pre}]{.p}]{.n}[ ]{.w}[[=]{.pre}]{.o}[ ]{.w}[[None]{.pre}]{.default_value}</em>, <em>[[pqclass]{.pre}]{.n}[[=]{.pre}]{.o}[[None]{.pre}]{.default_value}</em>, <em>[[crawler]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[<a href="https://docs.python.org/3/library/typing.html#typing.Optional" title="(in Python v3.12)">[Optional]{.pre}</a>{.reference .external}[[[]{.pre}]{.p}<a href="index.html#scrapy.crawler.Crawler" title="scrapy.crawler.Crawler">[Crawler]{.pre}</a>{.reference .internal}[[]]{.pre}]{.p}]{.n}[ ]{.w}[[=]{.pre}]{.o}[ ]{.w}[[None]{.pre}]{.default_value}</em>[)]{.sig-paren}<a href="_modules/scrapy/core/scheduler.html#Scheduler">[[[source]]{.pre}]{.viewcode-link}</a>{.reference .internal}<a href="#scrapy.core.scheduler.Scheduler" title="Permalink to this definition">¶</a>{.headerlink}</p>
<p>:   Default Scrapy scheduler. This implementation also handles
duplication filtering via the <a href="index.html#std-setting-DUPEFILTER_CLASS">[<code>dupefilter</code>{.xref .std .std-setting
.docutils .literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal}.</p>
<pre><code>This scheduler stores requests into several priority queues (defined
by the [[`SCHEDULER_PRIORITY_QUEUE`{.xref .std .std-setting
.docutils .literal
.notranslate}]{.pre}](index.html#std-setting-SCHEDULER_PRIORITY_QUEUE){.hoverxref
.tooltip .reference .internal} setting). In turn, said priority
queues are backed by either memory or disk based queues
(respectively defined by the [[`SCHEDULER_MEMORY_QUEUE`{.xref .std
.std-setting .docutils .literal
.notranslate}]{.pre}](index.html#std-setting-SCHEDULER_MEMORY_QUEUE){.hoverxref
.tooltip .reference .internal} and [[`SCHEDULER_DISK_QUEUE`{.xref
.std .std-setting .docutils .literal
.notranslate}]{.pre}](index.html#std-setting-SCHEDULER_DISK_QUEUE){.hoverxref
.tooltip .reference .internal} settings).

Request prioritization is almost entirely delegated to the priority
queue. The only prioritization performed by this scheduler is using
the disk-based queue if present (i.e. if the [[`JOBDIR`{.xref .std
.std-setting .docutils .literal
.notranslate}]{.pre}](index.html#std-setting-JOBDIR){.hoverxref
.tooltip .reference .internal} setting is defined) and falling back
to the memory-based queue if a serialization error occurs. If the
disk queue is not present, the memory one is used directly.

Parameters

:   -   **dupefilter** ([`scrapy.dupefilters.BaseDupeFilter`{.xref
        .py .py-class .docutils .literal .notranslate}]{.pre}
        instance or similar: any class that implements the
        BaseDupeFilter interface) -- An object responsible for
        checking and filtering duplicate requests. The value for the
        [[`DUPEFILTER_CLASS`{.xref .std .std-setting .docutils
        .literal
        .notranslate}]{.pre}](index.html#std-setting-DUPEFILTER_CLASS){.hoverxref
        .tooltip .reference .internal} setting is used by default.

    -   **jobdir** ([[`str`{.xref .py .py-class .docutils .literal
        .notranslate}]{.pre}](https://docs.python.org/3/library/stdtypes.html#str &quot;(in Python v3.12)&quot;){.reference
        .external} or [`None`{.docutils .literal
        .notranslate}]{.pre}) -- The path of a directory to be used
        for persisting the crawl's state. The value for the
        [[`JOBDIR`{.xref .std .std-setting .docutils .literal
        .notranslate}]{.pre}](index.html#std-setting-JOBDIR){.hoverxref
        .tooltip .reference .internal} setting is used by default.
        See [[Jobs: pausing and resuming crawls]{.std
        .std-ref}](index.html#topics-jobs){.hoverxref .tooltip
        .reference .internal}.

    -   **dqclass** (*class*) -- A class to be used as persistent
        request queue. The value for the
        [[`SCHEDULER_DISK_QUEUE`{.xref .std .std-setting .docutils
        .literal
        .notranslate}]{.pre}](index.html#std-setting-SCHEDULER_DISK_QUEUE){.hoverxref
        .tooltip .reference .internal} setting is used by default.

    -   **mqclass** (*class*) -- A class to be used as
        non-persistent request queue. The value for the
        [[`SCHEDULER_MEMORY_QUEUE`{.xref .std .std-setting .docutils
        .literal
        .notranslate}]{.pre}](index.html#std-setting-SCHEDULER_MEMORY_QUEUE){.hoverxref
        .tooltip .reference .internal} setting is used by default.

    -   **logunser**
        ([*bool*](https://docs.python.org/3/library/functions.html#bool &quot;(in Python v3.12)&quot;){.reference
        .external}) -- A boolean that indicates whether or not
        unserializable requests should be logged. The value for the
        [[`SCHEDULER_DEBUG`{.xref .std .std-setting .docutils
        .literal
        .notranslate}]{.pre}](index.html#std-setting-SCHEDULER_DEBUG){.hoverxref
        .tooltip .reference .internal} setting is used by default.

    -   **stats** ([[`scrapy.statscollectors.StatsCollector`{.xref
        .py .py-class .docutils .literal
        .notranslate}]{.pre}](index.html#scrapy.statscollectors.StatsCollector &quot;scrapy.statscollectors.StatsCollector&quot;){.reference
        .internal} instance or similar: any class that implements
        the StatsCollector interface) -- A stats collector object to
        record stats about the request scheduling process. The value
        for the [[`STATS_CLASS`{.xref .std .std-setting .docutils
        .literal
        .notranslate}]{.pre}](index.html#std-setting-STATS_CLASS){.hoverxref
        .tooltip .reference .internal} setting is used by default.

    -   **pqclass** (*class*) -- A class to be used as priority
        queue for requests. The value for the
        [[`SCHEDULER_PRIORITY_QUEUE`{.xref .std .std-setting
        .docutils .literal
        .notranslate}]{.pre}](index.html#std-setting-SCHEDULER_PRIORITY_QUEUE){.hoverxref
        .tooltip .reference .internal} setting is used by default.

    -   **crawler** ([[`scrapy.crawler.Crawler`{.xref .py .py-class
        .docutils .literal
        .notranslate}]{.pre}](index.html#scrapy.crawler.Crawler &quot;scrapy.crawler.Crawler&quot;){.reference
        .internal}) -- The crawler object corresponding to the
        current crawl.

[[\_\_len\_\_]{.pre}]{.sig-name .descname}[(]{.sig-paren}[)]{.sig-paren} [[→]{.sig-return-icon} [[[int]{.pre}](https://docs.python.org/3/library/functions.html#int &quot;(in Python v3.12)&quot;){.reference .external}]{.sig-return-typehint}]{.sig-return}[[[\[source\]]{.pre}]{.viewcode-link}](_modules/scrapy/core/scheduler.html#Scheduler.__len__){.reference .internal}[¶](#scrapy.core.scheduler.Scheduler.__len__ &quot;Permalink to this definition&quot;){.headerlink}

:   Return the total amount of enqueued requests

[[close]{.pre}]{.sig-name .descname}[(]{.sig-paren}*[[reason]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[[[str]{.pre}](https://docs.python.org/3/library/stdtypes.html#str &quot;(in Python v3.12)&quot;){.reference .external}]{.n}*[)]{.sig-paren} [[→]{.sig-return-icon} [[[Optional]{.pre}](https://docs.python.org/3/library/typing.html#typing.Optional &quot;(in Python v3.12)&quot;){.reference .external}[[\[]{.pre}]{.p}[[Deferred]{.pre}](https://docs.twisted.org/en/stable/api/twisted.internet.defer.Deferred.html &quot;(in Twisted)&quot;){.reference .external}[[\]]{.pre}]{.p}]{.sig-return-typehint}]{.sig-return}[[[\[source\]]{.pre}]{.viewcode-link}](_modules/scrapy/core/scheduler.html#Scheduler.close){.reference .internal}[¶](#scrapy.core.scheduler.Scheduler.close &quot;Permalink to this definition&quot;){.headerlink}

:   1.  dump pending requests to disk if there is a disk queue

    2.  return the result of the dupefilter's [`close`{.docutils
        .literal .notranslate}]{.pre} method

[[enqueue_request]{.pre}]{.sig-name .descname}[(]{.sig-paren}*[[request]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[[[Request]{.pre}](index.html#scrapy.http.Request &quot;scrapy.http.request.Request&quot;){.reference .internal}]{.n}*[)]{.sig-paren} [[→]{.sig-return-icon} [[[bool]{.pre}](https://docs.python.org/3/library/functions.html#bool &quot;(in Python v3.12)&quot;){.reference .external}]{.sig-return-typehint}]{.sig-return}[[[\[source\]]{.pre}]{.viewcode-link}](_modules/scrapy/core/scheduler.html#Scheduler.enqueue_request){.reference .internal}[¶](#scrapy.core.scheduler.Scheduler.enqueue_request &quot;Permalink to this definition&quot;){.headerlink}

:   Unless the received request is filtered out by the Dupefilter,
    attempt to push it into the disk queue, falling back to pushing
    it into the memory queue.

    Increment the appropriate stats, such as:
    [`scheduler/enqueued`{.docutils .literal .notranslate}]{.pre},
    [`scheduler/enqueued/disk`{.docutils .literal
    .notranslate}]{.pre}, [`scheduler/enqueued/memory`{.docutils
    .literal .notranslate}]{.pre}.

    Return [`True`{.docutils .literal .notranslate}]{.pre} if the
    request was stored successfully, [`False`{.docutils .literal
    .notranslate}]{.pre} otherwise.

*[classmethod]{.pre}[ ]{.w}*[[from_crawler]{.pre}]{.sig-name .descname}[(]{.sig-paren}*[[crawler]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[[[Crawler]{.pre}](index.html#scrapy.crawler.Crawler &quot;scrapy.crawler.Crawler&quot;){.reference .internal}]{.n}*[)]{.sig-paren} [[→]{.sig-return-icon} [[SchedulerTV]{.pre}]{.sig-return-typehint}]{.sig-return}[[[\[source\]]{.pre}]{.viewcode-link}](_modules/scrapy/core/scheduler.html#Scheduler.from_crawler){.reference .internal}[¶](#scrapy.core.scheduler.Scheduler.from_crawler &quot;Permalink to this definition&quot;){.headerlink}

:   Factory method, initializes the scheduler with arguments taken
    from the crawl settings

[[has_pending_requests]{.pre}]{.sig-name .descname}[(]{.sig-paren}[)]{.sig-paren} [[→]{.sig-return-icon} [[[bool]{.pre}](https://docs.python.org/3/library/functions.html#bool &quot;(in Python v3.12)&quot;){.reference .external}]{.sig-return-typehint}]{.sig-return}[[[\[source\]]{.pre}]{.viewcode-link}](_modules/scrapy/core/scheduler.html#Scheduler.has_pending_requests){.reference .internal}[¶](#scrapy.core.scheduler.Scheduler.has_pending_requests &quot;Permalink to this definition&quot;){.headerlink}

:   [`True`{.docutils .literal .notranslate}]{.pre} if the scheduler
    has enqueued requests, [`False`{.docutils .literal
    .notranslate}]{.pre} otherwise

[[next_request]{.pre}]{.sig-name .descname}[(]{.sig-paren}[)]{.sig-paren} [[→]{.sig-return-icon} [[[Optional]{.pre}](https://docs.python.org/3/library/typing.html#typing.Optional &quot;(in Python v3.12)&quot;){.reference .external}[[\[]{.pre}]{.p}[[Request]{.pre}](index.html#scrapy.http.Request &quot;scrapy.http.request.Request&quot;){.reference .internal}[[\]]{.pre}]{.p}]{.sig-return-typehint}]{.sig-return}[[[\[source\]]{.pre}]{.viewcode-link}](_modules/scrapy/core/scheduler.html#Scheduler.next_request){.reference .internal}[¶](#scrapy.core.scheduler.Scheduler.next_request &quot;Permalink to this definition&quot;){.headerlink}

:   Return a [[`Request`{.xref .py .py-class .docutils .literal
    .notranslate}]{.pre}](index.html#scrapy.http.Request &quot;scrapy.http.Request&quot;){.reference
    .internal} object from the memory queue, falling back to the
    disk queue if the memory queue is empty. Return
    [`None`{.docutils .literal .notranslate}]{.pre} if there are no
    more enqueued requests.

    Increment the appropriate stats, such as:
    [`scheduler/dequeued`{.docutils .literal .notranslate}]{.pre},
    [`scheduler/dequeued/disk`{.docutils .literal
    .notranslate}]{.pre}, [`scheduler/dequeued/memory`{.docutils
    .literal .notranslate}]{.pre}.

[[open]{.pre}]{.sig-name .descname}[(]{.sig-paren}*[[spider]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[[[Spider]{.pre}](index.html#scrapy.spiders.Spider &quot;scrapy.spiders.Spider&quot;){.reference .internal}]{.n}*[)]{.sig-paren} [[→]{.sig-return-icon} [[[Optional]{.pre}](https://docs.python.org/3/library/typing.html#typing.Optional &quot;(in Python v3.12)&quot;){.reference .external}[[\[]{.pre}]{.p}[[Deferred]{.pre}](https://docs.twisted.org/en/stable/api/twisted.internet.defer.Deferred.html &quot;(in Twisted)&quot;){.reference .external}[[\]]{.pre}]{.p}]{.sig-return-typehint}]{.sig-return}[[[\[source\]]{.pre}]{.viewcode-link}](_modules/scrapy/core/scheduler.html#Scheduler.open){.reference .internal}[¶](#scrapy.core.scheduler.Scheduler.open &quot;Permalink to this definition&quot;){.headerlink}

:   1.  initialize the memory queue

    2.  initialize the disk queue if the [`jobdir`{.docutils
        .literal .notranslate}]{.pre} attribute is a valid directory

    3.  return the result of the dupefilter's [`open`{.docutils
        .literal .notranslate}]{.pre} method
</code></pre>
<p>:::
:::</p>
<p>[]{#document-topics/exporters}</p>
<p>::: {#module-scrapy.exporters .section}
[]{#item-exporters}[]{#topics-exporters}</p>
<h3 id="item-exportersheaderlink"><a class="header" href="#item-exportersheaderlink">Item Exporters<a href="#module-scrapy.exporters" title="Permalink to this heading">¶</a>{.headerlink}</a></h3>
<p>Once you have scraped your items, you often want to persist or export
those items, to use the data in some other application. That is, after
all, the whole purpose of the scraping process.</p>
<p>For this purpose Scrapy provides a collection of Item Exporters for
different output formats, such as XML, CSV or JSON.</p>
<p>::: {#using-item-exporters .section}</p>
<h4 id="using-item-exportersheaderlink"><a class="header" href="#using-item-exportersheaderlink">Using Item Exporters<a href="#using-item-exporters" title="Permalink to this heading">¶</a>{.headerlink}</a></h4>
<p>If you are in a hurry, and just want to use an Item Exporter to output
scraped data see the <a href="index.html#topics-feed-exports">[Feed exports]{.std
.std-ref}</a>{.hoverxref .tooltip
.reference .internal}. Otherwise, if you want to know how Item Exporters
work or need more custom functionality (not covered by the default
exports), continue reading below.</p>
<p>In order to use an Item Exporter, you must instantiate it with its
required args. Each Item Exporter requires different arguments, so check
each exporter documentation to be sure, in <a href="#topics-exporters-reference">[Built-in Item Exporters
reference]{.std .std-ref}</a>{.hoverxref
.tooltip .reference .internal}. After you have instantiated your
exporter, you have to:</p>
<p>1. call the method <a href="#scrapy.exporters.BaseItemExporter.start_exporting" title="scrapy.exporters.BaseItemExporter.start_exporting">[<code>start_exporting()</code>{.xref .py .py-meth .docutils
.literal
.notranslate}]{.pre}</a>{.reference
.internal} in order to signal the beginning of the exporting process</p>
<p>2. call the <a href="#scrapy.exporters.BaseItemExporter.export_item" title="scrapy.exporters.BaseItemExporter.export_item">[<code>export_item()</code>{.xref .py .py-meth .docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal} method for each item you want to export</p>
<p>3. and finally call the <a href="#scrapy.exporters.BaseItemExporter.finish_exporting" title="scrapy.exporters.BaseItemExporter.finish_exporting">[<code>finish_exporting()</code>{.xref .py .py-meth
.docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal} to signal the end of the exporting process</p>
<p>Here you can see an <a href="index.html#document-topics/item-pipeline">[Item
Pipeline]{.doc}</a>{.reference
.internal} which uses multiple Item Exporters to group scraped items to
different files according to the value of one of their fields:</p>
<p>::: {.highlight-python .notranslate}
::: highlight
from itemadapter import ItemAdapter
from scrapy.exporters import XmlItemExporter</p>
<pre><code>class PerYearXmlExportPipeline:
    &quot;&quot;&quot;Distribute items across multiple XML files according to their 'year' field&quot;&quot;&quot;

    def open_spider(self, spider):
        self.year_to_exporter = {}

    def close_spider(self, spider):
        for exporter, xml_file in self.year_to_exporter.values():
            exporter.finish_exporting()
            xml_file.close()

    def _exporter_for_item(self, item):
        adapter = ItemAdapter(item)
        year = adapter[&quot;year&quot;]
        if year not in self.year_to_exporter:
            xml_file = open(f&quot;{year}.xml&quot;, &quot;wb&quot;)
            exporter = XmlItemExporter(xml_file)
            exporter.start_exporting()
            self.year_to_exporter[year] = (exporter, xml_file)
        return self.year_to_exporter[year][0]

    def process_item(self, item, spider):
        exporter = self._exporter_for_item(item)
        exporter.export_item(item)
        return item
</code></pre>
<p>:::
:::
:::</p>
<p>::: {#serialization-of-item-fields .section}
[]{#topics-exporters-field-serialization}</p>
<h4 id="serialization-of-item-fieldsheaderlink"><a class="header" href="#serialization-of-item-fieldsheaderlink">Serialization of item fields<a href="#serialization-of-item-fields" title="Permalink to this heading">¶</a>{.headerlink}</a></h4>
<p>By default, the field values are passed unmodified to the underlying
serialization library, and the decision of how to serialize them is
delegated to each particular serialization library.</p>
<p>However, you can customize how each field value is serialized <em>before it
is passed to the serialization library</em>.</p>
<p>There are two ways to customize how a field will be serialized, which
are described next.</p>
<p>::: {#declaring-a-serializer-in-the-field .section}
[]{#topics-exporters-serializers}</p>
<h5 id="1-declaring-a-serializer-in-the-fieldheaderlink"><a class="header" href="#1-declaring-a-serializer-in-the-fieldheaderlink">1. Declaring a serializer in the field<a href="#declaring-a-serializer-in-the-field" title="Permalink to this heading">¶</a>{.headerlink}</a></h5>
<p>If you use [<code>Item</code>{.xref .py .py-class .docutils .literal
.notranslate}]{.pre} you can declare a serializer in the <a href="index.html#topics-items-fields">[field
metadata]{.std .std-ref}</a>{.hoverxref
.tooltip .reference .internal}. The serializer must be a callable which
receives a value and returns its serialized form.</p>
<p>Example:</p>
<p>::: {.highlight-python .notranslate}
::: highlight
import scrapy</p>
<pre><code>def serialize_price(value):
    return f&quot;$ {str(value)}&quot;


class Product(scrapy.Item):
    name = scrapy.Field()
    price = scrapy.Field(serializer=serialize_price)
</code></pre>
<p>:::
:::
:::</p>
<p>::: {#overriding-the-serialize-field-method .section}</p>
<h5 id="2-overriding-the-serialize_field-methodheaderlink"><a class="header" href="#2-overriding-the-serialize_field-methodheaderlink">2. Overriding the serialize_field() method<a href="#overriding-the-serialize-field-method" title="Permalink to this heading">¶</a>{.headerlink}</a></h5>
<p>You can also override the <a href="#scrapy.exporters.BaseItemExporter.serialize_field" title="scrapy.exporters.BaseItemExporter.serialize_field">[<code>serialize_field()</code>{.xref .py .py-meth
.docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal} method to customize how your field value will be exported.</p>
<p>Make sure you call the base class <a href="#scrapy.exporters.BaseItemExporter.serialize_field" title="scrapy.exporters.BaseItemExporter.serialize_field">[<code>serialize_field()</code>{.xref .py
.py-meth .docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal} method after your custom code.</p>
<p>Example:</p>
<p>::: {.highlight-python .notranslate}
::: highlight
from scrapy.exporters import XmlItemExporter</p>
<pre><code>class ProductXmlExporter(XmlItemExporter):
    def serialize_field(self, field, name, value):
        if name == &quot;price&quot;:
            return f&quot;$ {str(value)}&quot;
        return super().serialize_field(field, name, value)
</code></pre>
<p>:::
:::
:::
:::</p>
<p>::: {#built-in-item-exporters-reference .section}
[]{#topics-exporters-reference}</p>
<h4 id="built-in-item-exporters-referenceheaderlink"><a class="header" href="#built-in-item-exporters-referenceheaderlink">Built-in Item Exporters reference<a href="#built-in-item-exporters-reference" title="Permalink to this heading">¶</a>{.headerlink}</a></h4>
<p>Here is a list of the Item Exporters bundled with Scrapy. Some of them
contain output examples, which assume you're exporting these two items:</p>
<p>::: {.highlight-python .notranslate}
::: highlight
Item(name=&quot;Color TV&quot;, price=&quot;1200&quot;)
Item(name=&quot;DVD player&quot;, price=&quot;200&quot;)
:::
:::</p>
<p>::: {#baseitemexporter .section}</p>
<h5 id="baseitemexporterheaderlink"><a class="header" href="#baseitemexporterheaderlink">BaseItemExporter<a href="#baseitemexporter" title="Permalink to this heading">¶</a>{.headerlink}</a></h5>
<p><em>[class]{.pre}[ ]{.w}</em>[[scrapy.exporters.]{.pre}]{.sig-prename .descclassname}[[BaseItemExporter]{.pre}]{.sig-name .descname}[(]{.sig-paren}<em>[[fields_to_export]{.pre}]{.n}[[=]{.pre}]{.o}[[None]{.pre}]{.default_value}</em>, <em>[[export_empty_fields]{.pre}]{.n}[[=]{.pre}]{.o}[[False]{.pre}]{.default_value}</em>, <em>[[encoding]{.pre}]{.n}[[=]{.pre}]{.o}[['utf-8']{.pre}]{.default_value}</em>, <em>[[indent]{.pre}]{.n}[[=]{.pre}]{.o}[[0]{.pre}]{.default_value}</em>, <em>[[dont_fail]{.pre}]{.n}[[=]{.pre}]{.o}[[False]{.pre}]{.default_value}</em>[)]{.sig-paren}<a href="_modules/scrapy/exporters.html#BaseItemExporter">[[[source]]{.pre}]{.viewcode-link}</a>{.reference .internal}<a href="#scrapy.exporters.BaseItemExporter" title="Permalink to this definition">¶</a>{.headerlink}</p>
<p>:   This is the (abstract) base class for all Item Exporters. It
provides support for common features used by all (concrete) Item
Exporters, such as defining what fields to export, whether to export
empty fields, or which encoding to use.</p>
<pre><code>These features can be configured through the [`__init__`{.docutils
.literal .notranslate}]{.pre} method arguments which populate their
respective instance attributes: [[`fields_to_export`{.xref .py
.py-attr .docutils .literal
.notranslate}]{.pre}](#scrapy.exporters.BaseItemExporter.fields_to_export &quot;scrapy.exporters.BaseItemExporter.fields_to_export&quot;){.reference
.internal}, [[`export_empty_fields`{.xref .py .py-attr .docutils
.literal
.notranslate}]{.pre}](#scrapy.exporters.BaseItemExporter.export_empty_fields &quot;scrapy.exporters.BaseItemExporter.export_empty_fields&quot;){.reference
.internal}, [[`encoding`{.xref .py .py-attr .docutils .literal
.notranslate}]{.pre}](#scrapy.exporters.BaseItemExporter.encoding &quot;scrapy.exporters.BaseItemExporter.encoding&quot;){.reference
.internal}, [[`indent`{.xref .py .py-attr .docutils .literal
.notranslate}]{.pre}](#scrapy.exporters.BaseItemExporter.indent &quot;scrapy.exporters.BaseItemExporter.indent&quot;){.reference
.internal}.

::: versionadded
[New in version 2.0: ]{.versionmodified .added}The *dont_fail*
parameter.
:::

[[export_item]{.pre}]{.sig-name .descname}[(]{.sig-paren}*[[item]{.pre}]{.n}*[)]{.sig-paren}[[[\[source\]]{.pre}]{.viewcode-link}](_modules/scrapy/exporters.html#BaseItemExporter.export_item){.reference .internal}[¶](#scrapy.exporters.BaseItemExporter.export_item &quot;Permalink to this definition&quot;){.headerlink}

:   Exports the given item. This method must be implemented in
    subclasses.

[[serialize_field]{.pre}]{.sig-name .descname}[(]{.sig-paren}*[[field]{.pre}]{.n}*, *[[name]{.pre}]{.n}*, *[[value]{.pre}]{.n}*[)]{.sig-paren}[[[\[source\]]{.pre}]{.viewcode-link}](_modules/scrapy/exporters.html#BaseItemExporter.serialize_field){.reference .internal}[¶](#scrapy.exporters.BaseItemExporter.serialize_field &quot;Permalink to this definition&quot;){.headerlink}

:   Return the serialized value for the given field. You can
    override this method (in your custom Item Exporters) if you want
    to control how a particular field or value will be
    serialized/exported.

    By default, this method looks for a serializer [[declared in the
    item field]{.std
    .std-ref}](#topics-exporters-serializers){.hoverxref .tooltip
    .reference .internal} and returns the result of applying that
    serializer to the value. If no serializer is found, it returns
    the value unchanged.

    Parameters

    :   -   **field** ([`Field`{.xref .py .py-class .docutils
            .literal .notranslate}]{.pre} object or a [[`dict`{.xref
            .py .py-class .docutils .literal
            .notranslate}]{.pre}](https://docs.python.org/3/library/stdtypes.html#dict &quot;(in Python v3.12)&quot;){.reference
            .external} instance) -- the field being serialized. If
            the source [[item object]{.std
            .std-ref}](index.html#item-types){.hoverxref .tooltip
            .reference .internal} does not define field metadata,
            *field* is an empty [[`dict`{.xref .py .py-class
            .docutils .literal
            .notranslate}]{.pre}](https://docs.python.org/3/library/stdtypes.html#dict &quot;(in Python v3.12)&quot;){.reference
            .external}.

        -   **name**
            ([*str*](https://docs.python.org/3/library/stdtypes.html#str &quot;(in Python v3.12)&quot;){.reference
            .external}) -- the name of the field being serialized

        -   **value** -- the value being serialized

[[start_exporting]{.pre}]{.sig-name .descname}[(]{.sig-paren}[)]{.sig-paren}[[[\[source\]]{.pre}]{.viewcode-link}](_modules/scrapy/exporters.html#BaseItemExporter.start_exporting){.reference .internal}[¶](#scrapy.exporters.BaseItemExporter.start_exporting &quot;Permalink to this definition&quot;){.headerlink}

:   Signal the beginning of the exporting process. Some exporters
    may use this to generate some required header (for example, the
    [[`XmlItemExporter`{.xref .py .py-class .docutils .literal
    .notranslate}]{.pre}](#scrapy.exporters.XmlItemExporter &quot;scrapy.exporters.XmlItemExporter&quot;){.reference
    .internal}). You must call this method before exporting any
    items.

[[finish_exporting]{.pre}]{.sig-name .descname}[(]{.sig-paren}[)]{.sig-paren}[[[\[source\]]{.pre}]{.viewcode-link}](_modules/scrapy/exporters.html#BaseItemExporter.finish_exporting){.reference .internal}[¶](#scrapy.exporters.BaseItemExporter.finish_exporting &quot;Permalink to this definition&quot;){.headerlink}

:   Signal the end of the exporting process. Some exporters may use
    this to generate some required footer (for example, the
    [[`XmlItemExporter`{.xref .py .py-class .docutils .literal
    .notranslate}]{.pre}](#scrapy.exporters.XmlItemExporter &quot;scrapy.exporters.XmlItemExporter&quot;){.reference
    .internal}). You must always call this method after you have no
    more items to export.

[[fields_to_export]{.pre}]{.sig-name .descname}[¶](#scrapy.exporters.BaseItemExporter.fields_to_export &quot;Permalink to this definition&quot;){.headerlink}

:   Fields to export, their order [1](#id3){#id1 .footnote-reference
    .brackets} and their output names.

    Possible values are:

    -   [`None`{.docutils .literal .notranslate}]{.pre} (all fields
        [2](#id4){#id2 .footnote-reference .brackets}, default)

    -   A list of fields:

        ::: {.highlight-default .notranslate}
        ::: highlight
            ['field1', 'field2']
        :::
        :::

    -   A dict where keys are fields and values are output names:

        ::: {.highlight-default .notranslate}
        ::: highlight
            {'field1': 'Field 1', 'field2': 'Field 2'}
        :::
        :::

    [[1](#id1){.fn-backref}]{.brackets}

    :   Not all exporters respect the specified field order.

    [[2](#id2){.fn-backref}]{.brackets}

    :   When using [[item objects]{.std
        .std-ref}](index.html#item-types){.hoverxref .tooltip
        .reference .internal} that do not expose all their possible
        fields, exporters that do not support exporting a different
        subset of fields per item will only export the fields found
        in the first item exported.

[[export_empty_fields]{.pre}]{.sig-name .descname}[¶](#scrapy.exporters.BaseItemExporter.export_empty_fields &quot;Permalink to this definition&quot;){.headerlink}

:   Whether to include empty/unpopulated item fields in the exported
    data. Defaults to [`False`{.docutils .literal
    .notranslate}]{.pre}. Some exporters (like
    [[`CsvItemExporter`{.xref .py .py-class .docutils .literal
    .notranslate}]{.pre}](#scrapy.exporters.CsvItemExporter &quot;scrapy.exporters.CsvItemExporter&quot;){.reference
    .internal}) ignore this attribute and always export all empty
    fields.

    This option is ignored for dict items.

[[encoding]{.pre}]{.sig-name .descname}[¶](#scrapy.exporters.BaseItemExporter.encoding &quot;Permalink to this definition&quot;){.headerlink}

:   The output character encoding.

[[indent]{.pre}]{.sig-name .descname}[¶](#scrapy.exporters.BaseItemExporter.indent &quot;Permalink to this definition&quot;){.headerlink}

:   Amount of spaces used to indent the output on each level.
    Defaults to [`0`{.docutils .literal .notranslate}]{.pre}.

    -   [`indent=None`{.docutils .literal .notranslate}]{.pre}
        selects the most compact representation, all items in the
        same line with no indentation

    -   [`indent&lt;=0`{.docutils .literal .notranslate}]{.pre} each
        item on its own line, no indentation

    -   [`indent&gt;0`{.docutils .literal .notranslate}]{.pre} each
        item on its own line, indented with the provided numeric
        value
</code></pre>
<p>:::</p>
<p>::: {#pythonitemexporter .section}</p>
<h5 id="pythonitemexporterheaderlink"><a class="header" href="#pythonitemexporterheaderlink">PythonItemExporter<a href="#pythonitemexporter" title="Permalink to this heading">¶</a>{.headerlink}</a></h5>
<p><em>[class]{.pre}[ ]{.w}</em>[[scrapy.exporters.]{.pre}]{.sig-prename .descclassname}[[PythonItemExporter]{.pre}]{.sig-name .descname}[(]{.sig-paren}<em>[[*]{.pre}]{.o}</em>, <em>[[dont_fail]{.pre}]{.n}[[=]{.pre}]{.o}[[False]{.pre}]{.default_value}</em>, <em>[[**]{.pre}]{.o}[[kwargs]{.pre}]{.n}</em>[)]{.sig-paren}<a href="_modules/scrapy/exporters.html#PythonItemExporter">[[[source]]{.pre}]{.viewcode-link}</a>{.reference .internal}<a href="#scrapy.exporters.PythonItemExporter" title="Permalink to this definition">¶</a>{.headerlink}</p>
<p>:   This is a base class for item exporters that extends
<a href="#scrapy.exporters.BaseItemExporter" title="scrapy.exporters.BaseItemExporter">[<code>BaseItemExporter</code>{.xref .py .py-class .docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal} with support for nested items.</p>
<pre><code>It serializes items to built-in Python types, so that any
serialization library (e.g. [[`json`{.xref .py .py-mod .docutils
.literal
.notranslate}]{.pre}](https://docs.python.org/3/library/json.html#module-json &quot;(in Python v3.12)&quot;){.reference
.external} or
[msgpack](https://pypi.org/project/msgpack/){.reference .external})
can be used on top of it.
</code></pre>
<p>:::</p>
<p>::: {#xmlitemexporter .section}</p>
<h5 id="xmlitemexporterheaderlink"><a class="header" href="#xmlitemexporterheaderlink">XmlItemExporter<a href="#xmlitemexporter" title="Permalink to this heading">¶</a>{.headerlink}</a></h5>
<p><em>[class]{.pre}[ ]{.w}</em>[[scrapy.exporters.]{.pre}]{.sig-prename .descclassname}[[XmlItemExporter]{.pre}]{.sig-name .descname}[(]{.sig-paren}<em>[[file]{.pre}]{.n}</em>, <em>[[item_element]{.pre}]{.n}[[=]{.pre}]{.o}[['item']{.pre}]{.default_value}</em>, <em>[[root_element]{.pre}]{.n}[[=]{.pre}]{.o}[['items']{.pre}]{.default_value}</em>, <em>[[**]{.pre}]{.o}[[kwargs]{.pre}]{.n}</em>[)]{.sig-paren}<a href="_modules/scrapy/exporters.html#XmlItemExporter">[[[source]]{.pre}]{.viewcode-link}</a>{.reference .internal}<a href="#scrapy.exporters.XmlItemExporter" title="Permalink to this definition">¶</a>{.headerlink}</p>
<p>:   Exports items in XML format to the specified file object.</p>
<pre><code>Parameters

:   -   **file** -- the file-like object to use for exporting the
        data. Its [`write`{.docutils .literal .notranslate}]{.pre}
        method should accept [`bytes`{.docutils .literal
        .notranslate}]{.pre} (a disk file opened in binary mode, a
        [`io.BytesIO`{.docutils .literal .notranslate}]{.pre}
        object, etc)

    -   **root_element**
        ([*str*](https://docs.python.org/3/library/stdtypes.html#str &quot;(in Python v3.12)&quot;){.reference
        .external}) -- The name of root element in the exported XML.

    -   **item_element**
        ([*str*](https://docs.python.org/3/library/stdtypes.html#str &quot;(in Python v3.12)&quot;){.reference
        .external}) -- The name of each item element in the exported
        XML.

The additional keyword arguments of this [`__init__`{.docutils
.literal .notranslate}]{.pre} method are passed to the
[[`BaseItemExporter`{.xref .py .py-class .docutils .literal
.notranslate}]{.pre}](#scrapy.exporters.BaseItemExporter &quot;scrapy.exporters.BaseItemExporter&quot;){.reference
.internal} [`__init__`{.docutils .literal .notranslate}]{.pre}
method.

A typical output of this exporter would be:

::: {.highlight-none .notranslate}
::: highlight
    &lt;?xml version=&quot;1.0&quot; encoding=&quot;utf-8&quot;?&gt;
    &lt;items&gt;
      &lt;item&gt;
        &lt;name&gt;Color TV&lt;/name&gt;
        &lt;price&gt;1200&lt;/price&gt;
     &lt;/item&gt;
      &lt;item&gt;
        &lt;name&gt;DVD player&lt;/name&gt;
        &lt;price&gt;200&lt;/price&gt;
     &lt;/item&gt;
    &lt;/items&gt;
:::
:::

Unless overridden in the [`serialize_field()`{.xref .py .py-meth
.docutils .literal .notranslate}]{.pre} method, multi-valued fields
are exported by serializing each value inside a [`&lt;value&gt;`{.docutils
.literal .notranslate}]{.pre} element. This is for convenience, as
multi-valued fields are very common.

For example, the item:

::: {.highlight-none .notranslate}
::: highlight
    Item(name=['John', 'Doe'], age='23')
:::
:::

Would be serialized as:

::: {.highlight-none .notranslate}
::: highlight
    &lt;?xml version=&quot;1.0&quot; encoding=&quot;utf-8&quot;?&gt;
    &lt;items&gt;
      &lt;item&gt;
        &lt;name&gt;
          &lt;value&gt;John&lt;/value&gt;
          &lt;value&gt;Doe&lt;/value&gt;
        &lt;/name&gt;
        &lt;age&gt;23&lt;/age&gt;
      &lt;/item&gt;
    &lt;/items&gt;
:::
:::
</code></pre>
<p>:::</p>
<p>::: {#csvitemexporter .section}</p>
<h5 id="csvitemexporterheaderlink"><a class="header" href="#csvitemexporterheaderlink">CsvItemExporter<a href="#csvitemexporter" title="Permalink to this heading">¶</a>{.headerlink}</a></h5>
<p><em>[class]{.pre}[ ]{.w}</em>[[scrapy.exporters.]{.pre}]{.sig-prename .descclassname}[[CsvItemExporter]{.pre}]{.sig-name .descname}[(]{.sig-paren}<em>[[file]{.pre}]{.n}</em>, <em>[[include_headers_line]{.pre}]{.n}[[=]{.pre}]{.o}[[True]{.pre}]{.default_value}</em>, <em>[[join_multivalued]{.pre}]{.n}[[=]{.pre}]{.o}[[',']{.pre}]{.default_value}</em>, <em>[[errors]{.pre}]{.n}[[=]{.pre}]{.o}[[None]{.pre}]{.default_value}</em>, <em>[[**]{.pre}]{.o}[[kwargs]{.pre}]{.n}</em>[)]{.sig-paren}<a href="_modules/scrapy/exporters.html#CsvItemExporter">[[[source]]{.pre}]{.viewcode-link}</a>{.reference .internal}<a href="#scrapy.exporters.CsvItemExporter" title="Permalink to this definition">¶</a>{.headerlink}</p>
<p>:   Exports items in CSV format to the given file-like object. If the
[<code>fields_to_export</code>{.xref .py .py-attr .docutils .literal
.notranslate}]{.pre} attribute is set, it will be used to define the
CSV columns, their order and their column names. The
[<code>export_empty_fields</code>{.xref .py .py-attr .docutils .literal
.notranslate}]{.pre} attribute has no effect on this exporter.</p>
<pre><code>Parameters

:   -   **file** -- the file-like object to use for exporting the
        data. Its [`write`{.docutils .literal .notranslate}]{.pre}
        method should accept [`bytes`{.docutils .literal
        .notranslate}]{.pre} (a disk file opened in binary mode, a
        [`io.BytesIO`{.docutils .literal .notranslate}]{.pre}
        object, etc)

    -   **include_headers_line**
        ([*str*](https://docs.python.org/3/library/stdtypes.html#str &quot;(in Python v3.12)&quot;){.reference
        .external}) -- If enabled, makes the exporter output a
        header line with the field names taken from
        [[`BaseItemExporter.fields_to_export`{.xref .py .py-attr
        .docutils .literal
        .notranslate}]{.pre}](#scrapy.exporters.BaseItemExporter.fields_to_export &quot;scrapy.exporters.BaseItemExporter.fields_to_export&quot;){.reference
        .internal} or the first exported item fields.

    -   **join_multivalued** -- The char (or chars) that will be
        used for joining multi-valued fields, if found.

    -   **errors**
        ([*str*](https://docs.python.org/3/library/stdtypes.html#str &quot;(in Python v3.12)&quot;){.reference
        .external}) -- The optional string that specifies how
        encoding and decoding errors are to be handled. For more
        information see [[`io.TextIOWrapper`{.xref .py .py-class
        .docutils .literal
        .notranslate}]{.pre}](https://docs.python.org/3/library/io.html#io.TextIOWrapper &quot;(in Python v3.12)&quot;){.reference
        .external}.

The additional keyword arguments of this [`__init__`{.docutils
.literal .notranslate}]{.pre} method are passed to the
[[`BaseItemExporter`{.xref .py .py-class .docutils .literal
.notranslate}]{.pre}](#scrapy.exporters.BaseItemExporter &quot;scrapy.exporters.BaseItemExporter&quot;){.reference
.internal} [`__init__`{.docutils .literal .notranslate}]{.pre}
method, and the leftover arguments to the [[`csv.writer()`{.xref .py
.py-func .docutils .literal
.notranslate}]{.pre}](https://docs.python.org/3/library/csv.html#csv.writer &quot;(in Python v3.12)&quot;){.reference
.external} function, so you can use any [[`csv.writer()`{.xref .py
.py-func .docutils .literal
.notranslate}]{.pre}](https://docs.python.org/3/library/csv.html#csv.writer &quot;(in Python v3.12)&quot;){.reference
.external} function argument to customize this exporter.

A typical output of this exporter would be:

::: {.highlight-none .notranslate}
::: highlight
    product,price
    Color TV,1200
    DVD player,200
:::
:::
</code></pre>
<p>:::</p>
<p>::: {#pickleitemexporter .section}</p>
<h5 id="pickleitemexporterheaderlink"><a class="header" href="#pickleitemexporterheaderlink">PickleItemExporter<a href="#pickleitemexporter" title="Permalink to this heading">¶</a>{.headerlink}</a></h5>
<p><em>[class]{.pre}[ ]{.w}</em>[[scrapy.exporters.]{.pre}]{.sig-prename .descclassname}[[PickleItemExporter]{.pre}]{.sig-name .descname}[(]{.sig-paren}<em>[[file]{.pre}]{.n}</em>, <em>[[protocol]{.pre}]{.n}[[=]{.pre}]{.o}[[0]{.pre}]{.default_value}</em>, <em>[[**]{.pre}]{.o}[[kwargs]{.pre}]{.n}</em>[)]{.sig-paren}<a href="_modules/scrapy/exporters.html#PickleItemExporter">[[[source]]{.pre}]{.viewcode-link}</a>{.reference .internal}<a href="#scrapy.exporters.PickleItemExporter" title="Permalink to this definition">¶</a>{.headerlink}</p>
<p>:   Exports items in pickle format to the given file-like object.</p>
<pre><code>Parameters

:   -   **file** -- the file-like object to use for exporting the
        data. Its [`write`{.docutils .literal .notranslate}]{.pre}
        method should accept [`bytes`{.docutils .literal
        .notranslate}]{.pre} (a disk file opened in binary mode, a
        [`io.BytesIO`{.docutils .literal .notranslate}]{.pre}
        object, etc)

    -   **protocol**
        ([*int*](https://docs.python.org/3/library/functions.html#int &quot;(in Python v3.12)&quot;){.reference
        .external}) -- The pickle protocol to use.

For more information, see [[`pickle`{.xref .py .py-mod .docutils
.literal
.notranslate}]{.pre}](https://docs.python.org/3/library/pickle.html#module-pickle &quot;(in Python v3.12)&quot;){.reference
.external}.

The additional keyword arguments of this [`__init__`{.docutils
.literal .notranslate}]{.pre} method are passed to the
[[`BaseItemExporter`{.xref .py .py-class .docutils .literal
.notranslate}]{.pre}](#scrapy.exporters.BaseItemExporter &quot;scrapy.exporters.BaseItemExporter&quot;){.reference
.internal} [`__init__`{.docutils .literal .notranslate}]{.pre}
method.

Pickle isn't a human readable format, so no output examples are
provided.
</code></pre>
<p>:::</p>
<p>::: {#pprintitemexporter .section}</p>
<h5 id="pprintitemexporterheaderlink"><a class="header" href="#pprintitemexporterheaderlink">PprintItemExporter<a href="#pprintitemexporter" title="Permalink to this heading">¶</a>{.headerlink}</a></h5>
<p><em>[class]{.pre}[ ]{.w}</em>[[scrapy.exporters.]{.pre}]{.sig-prename .descclassname}[[PprintItemExporter]{.pre}]{.sig-name .descname}[(]{.sig-paren}<em>[[file]{.pre}]{.n}</em>, <em>[[**]{.pre}]{.o}[[kwargs]{.pre}]{.n}</em>[)]{.sig-paren}<a href="_modules/scrapy/exporters.html#PprintItemExporter">[[[source]]{.pre}]{.viewcode-link}</a>{.reference .internal}<a href="#scrapy.exporters.PprintItemExporter" title="Permalink to this definition">¶</a>{.headerlink}</p>
<p>:   Exports items in pretty print format to the specified file object.</p>
<pre><code>Parameters

:   **file** -- the file-like object to use for exporting the data.
    Its [`write`{.docutils .literal .notranslate}]{.pre} method
    should accept [`bytes`{.docutils .literal .notranslate}]{.pre}
    (a disk file opened in binary mode, a [`io.BytesIO`{.docutils
    .literal .notranslate}]{.pre} object, etc)

The additional keyword arguments of this [`__init__`{.docutils
.literal .notranslate}]{.pre} method are passed to the
[[`BaseItemExporter`{.xref .py .py-class .docutils .literal
.notranslate}]{.pre}](#scrapy.exporters.BaseItemExporter &quot;scrapy.exporters.BaseItemExporter&quot;){.reference
.internal} [`__init__`{.docutils .literal .notranslate}]{.pre}
method.

A typical output of this exporter would be:

::: {.highlight-none .notranslate}
::: highlight
    {'name': 'Color TV', 'price': '1200'}
    {'name': 'DVD player', 'price': '200'}
:::
:::

Longer lines (when present) are pretty-formatted.
</code></pre>
<p>:::</p>
<p>::: {#jsonitemexporter .section}</p>
<h5 id="jsonitemexporterheaderlink"><a class="header" href="#jsonitemexporterheaderlink">JsonItemExporter<a href="#jsonitemexporter" title="Permalink to this heading">¶</a>{.headerlink}</a></h5>
<p><em>[class]{.pre}[ ]{.w}</em>[[scrapy.exporters.]{.pre}]{.sig-prename .descclassname}[[JsonItemExporter]{.pre}]{.sig-name .descname}[(]{.sig-paren}<em>[[file]{.pre}]{.n}</em>, <em>[[**]{.pre}]{.o}[[kwargs]{.pre}]{.n}</em>[)]{.sig-paren}<a href="_modules/scrapy/exporters.html#JsonItemExporter">[[[source]]{.pre}]{.viewcode-link}</a>{.reference .internal}<a href="#scrapy.exporters.JsonItemExporter" title="Permalink to this definition">¶</a>{.headerlink}</p>
<p>:   Exports items in JSON format to the specified file-like object,
writing all objects as a list of objects. The additional
[<code>__init__</code>{.docutils .literal .notranslate}]{.pre} method arguments
are passed to the <a href="#scrapy.exporters.BaseItemExporter" title="scrapy.exporters.BaseItemExporter">[<code>BaseItemExporter</code>{.xref .py .py-class .docutils
.literal
.notranslate}]{.pre}</a>{.reference
.internal} [<code>__init__</code>{.docutils .literal .notranslate}]{.pre}
method, and the leftover arguments to the <a href="https://docs.python.org/3/library/json.html#json.JSONEncoder" title="(in Python v3.12)">[<code>JSONEncoder</code>{.xref .py
.py-class .docutils .literal
.notranslate}]{.pre}</a>{.reference
.external} [<code>__init__</code>{.docutils .literal .notranslate}]{.pre}
method, so you can use any <a href="https://docs.python.org/3/library/json.html#json.JSONEncoder" title="(in Python v3.12)">[<code>JSONEncoder</code>{.xref .py .py-class
.docutils .literal
.notranslate}]{.pre}</a>{.reference
.external} [<code>__init__</code>{.docutils .literal .notranslate}]{.pre}
method argument to customize this exporter.</p>
<pre><code>Parameters

:   **file** -- the file-like object to use for exporting the data.
    Its [`write`{.docutils .literal .notranslate}]{.pre} method
    should accept [`bytes`{.docutils .literal .notranslate}]{.pre}
    (a disk file opened in binary mode, a [`io.BytesIO`{.docutils
    .literal .notranslate}]{.pre} object, etc)

A typical output of this exporter would be:

::: {.highlight-none .notranslate}
::: highlight
    [{&quot;name&quot;: &quot;Color TV&quot;, &quot;price&quot;: &quot;1200&quot;},
    {&quot;name&quot;: &quot;DVD player&quot;, &quot;price&quot;: &quot;200&quot;}]
:::
:::

::: {#json-with-large-data .admonition .warning}
Warning

JSON is very simple and flexible serialization format, but it
doesn't scale well for large amounts of data since incremental (aka.
stream-mode) parsing is not well supported (if at all) among JSON
parsers (on any language), and most of them just parse the entire
object in memory. If you want the power and simplicity of JSON with
a more stream-friendly format, consider using
[[`JsonLinesItemExporter`{.xref .py .py-class .docutils .literal
.notranslate}]{.pre}](#scrapy.exporters.JsonLinesItemExporter &quot;scrapy.exporters.JsonLinesItemExporter&quot;){.reference
.internal} instead, or splitting the output in multiple chunks.
:::
</code></pre>
<p>:::</p>
<p>::: {#jsonlinesitemexporter .section}</p>
<h5 id="jsonlinesitemexporterheaderlink"><a class="header" href="#jsonlinesitemexporterheaderlink">JsonLinesItemExporter<a href="#jsonlinesitemexporter" title="Permalink to this heading">¶</a>{.headerlink}</a></h5>
<p><em>[class]{.pre}[ ]{.w}</em>[[scrapy.exporters.]{.pre}]{.sig-prename .descclassname}[[JsonLinesItemExporter]{.pre}]{.sig-name .descname}[(]{.sig-paren}<em>[[file]{.pre}]{.n}</em>, <em>[[**]{.pre}]{.o}[[kwargs]{.pre}]{.n}</em>[)]{.sig-paren}<a href="_modules/scrapy/exporters.html#JsonLinesItemExporter">[[[source]]{.pre}]{.viewcode-link}</a>{.reference .internal}<a href="#scrapy.exporters.JsonLinesItemExporter" title="Permalink to this definition">¶</a>{.headerlink}</p>
<p>:   Exports items in JSON format to the specified file-like object,
writing one JSON-encoded item per line. The additional
[<code>__init__</code>{.docutils .literal .notranslate}]{.pre} method arguments
are passed to the <a href="#scrapy.exporters.BaseItemExporter" title="scrapy.exporters.BaseItemExporter">[<code>BaseItemExporter</code>{.xref .py .py-class .docutils
.literal
.notranslate}]{.pre}</a>{.reference
.internal} [<code>__init__</code>{.docutils .literal .notranslate}]{.pre}
method, and the leftover arguments to the <a href="https://docs.python.org/3/library/json.html#json.JSONEncoder" title="(in Python v3.12)">[<code>JSONEncoder</code>{.xref .py
.py-class .docutils .literal
.notranslate}]{.pre}</a>{.reference
.external} [<code>__init__</code>{.docutils .literal .notranslate}]{.pre}
method, so you can use any <a href="https://docs.python.org/3/library/json.html#json.JSONEncoder" title="(in Python v3.12)">[<code>JSONEncoder</code>{.xref .py .py-class
.docutils .literal
.notranslate}]{.pre}</a>{.reference
.external} [<code>__init__</code>{.docutils .literal .notranslate}]{.pre}
method argument to customize this exporter.</p>
<pre><code>Parameters

:   **file** -- the file-like object to use for exporting the data.
    Its [`write`{.docutils .literal .notranslate}]{.pre} method
    should accept [`bytes`{.docutils .literal .notranslate}]{.pre}
    (a disk file opened in binary mode, a [`io.BytesIO`{.docutils
    .literal .notranslate}]{.pre} object, etc)

A typical output of this exporter would be:

::: {.highlight-none .notranslate}
::: highlight
    {&quot;name&quot;: &quot;Color TV&quot;, &quot;price&quot;: &quot;1200&quot;}
    {&quot;name&quot;: &quot;DVD player&quot;, &quot;price&quot;: &quot;200&quot;}
:::
:::

Unlike the one produced by [[`JsonItemExporter`{.xref .py .py-class
.docutils .literal
.notranslate}]{.pre}](#scrapy.exporters.JsonItemExporter &quot;scrapy.exporters.JsonItemExporter&quot;){.reference
.internal}, the format produced by this exporter is well suited for
serializing large amounts of data.
</code></pre>
<p>:::</p>
<p>::: {#marshalitemexporter .section}</p>
<h5 id="marshalitemexporterheaderlink"><a class="header" href="#marshalitemexporterheaderlink">MarshalItemExporter<a href="#marshalitemexporter" title="Permalink to this heading">¶</a>{.headerlink}</a></h5>
<p><em>[class]{.pre}[ ]{.w}</em>[[scrapy.exporters.]{.pre}]{.sig-prename .descclassname}[[MarshalItemExporter]{.pre}]{.sig-name .descname}[(]{.sig-paren}<em>[[file]{.pre}]{.n}</em>, <em>[[**]{.pre}]{.o}[[kwargs]{.pre}]{.n}</em>[)]{.sig-paren}<a href="_modules/scrapy/exporters.html#MarshalItemExporter">[[[source]]{.pre}]{.viewcode-link}</a>{.reference .internal}<a href="#scrapy.exporters.MarshalItemExporter" title="Permalink to this definition">¶</a>{.headerlink}</p>
<p>:   Exports items in a Python-specific binary format (see
<a href="https://docs.python.org/3/library/marshal.html#module-marshal" title="(in Python v3.12)">[<code>marshal</code>{.xref .py .py-mod .docutils .literal
.notranslate}]{.pre}</a>{.reference
.external}).</p>
<pre><code>Parameters

:   **file** -- The file-like object to use for exporting the data.
    Its [`write`{.docutils .literal .notranslate}]{.pre} method
    should accept [[`bytes`{.xref .py .py-class .docutils .literal
    .notranslate}]{.pre}](https://docs.python.org/3/library/stdtypes.html#bytes &quot;(in Python v3.12)&quot;){.reference
    .external} (a disk file opened in binary mode, a
    [[`BytesIO`{.xref .py .py-class .docutils .literal
    .notranslate}]{.pre}](https://docs.python.org/3/library/io.html#io.BytesIO &quot;(in Python v3.12)&quot;){.reference
    .external} object, etc)
</code></pre>
<p>:::
:::
:::</p>
<p>[]{#document-topics/components}</p>
<p>::: {#components .section}
[]{#topics-components}</p>
<h3 id="componentsheaderlink"><a class="header" href="#componentsheaderlink">Components<a href="#components" title="Permalink to this heading">¶</a>{.headerlink}</a></h3>
<p>A Scrapy component is any class whose objects are created using
[<code>scrapy.utils.misc.create_instance()</code>{.xref .py .py-func .docutils
.literal .notranslate}]{.pre}.</p>
<p>That includes the classes that you may assign to the following settings:</p>
<ul>
<li>
<p><a href="index.html#std-setting-DNS_RESOLVER">[<code>DNS_RESOLVER</code>{.xref .std .std-setting .docutils .literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal}</p>
</li>
<li>
<p><a href="index.html#std-setting-DOWNLOAD_HANDLERS">[<code>DOWNLOAD_HANDLERS</code>{.xref .std .std-setting .docutils .literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal}</p>
</li>
<li>
<p><a href="index.html#std-setting-DOWNLOADER_CLIENTCONTEXTFACTORY">[<code>DOWNLOADER_CLIENTCONTEXTFACTORY</code>{.xref .std .std-setting
.docutils .literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal}</p>
</li>
<li>
<p><a href="index.html#std-setting-DOWNLOADER_MIDDLEWARES">[<code>DOWNLOADER_MIDDLEWARES</code>{.xref .std .std-setting .docutils
.literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal}</p>
</li>
<li>
<p><a href="index.html#std-setting-DUPEFILTER_CLASS">[<code>DUPEFILTER_CLASS</code>{.xref .std .std-setting .docutils .literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal}</p>
</li>
<li>
<p><a href="index.html#std-setting-EXTENSIONS">[<code>EXTENSIONS</code>{.xref .std .std-setting .docutils .literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal}</p>
</li>
<li>
<p><a href="index.html#std-setting-FEED_EXPORTERS">[<code>FEED_EXPORTERS</code>{.xref .std .std-setting .docutils .literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal}</p>
</li>
<li>
<p><a href="index.html#std-setting-FEED_STORAGES">[<code>FEED_STORAGES</code>{.xref .std .std-setting .docutils .literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal}</p>
</li>
<li>
<p><a href="index.html#std-setting-ITEM_PIPELINES">[<code>ITEM_PIPELINES</code>{.xref .std .std-setting .docutils .literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal}</p>
</li>
<li>
<p><a href="index.html#std-setting-SCHEDULER">[<code>SCHEDULER</code>{.xref .std .std-setting .docutils .literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal}</p>
</li>
<li>
<p><a href="index.html#std-setting-SCHEDULER_DISK_QUEUE">[<code>SCHEDULER_DISK_QUEUE</code>{.xref .std .std-setting .docutils .literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal}</p>
</li>
<li>
<p><a href="index.html#std-setting-SCHEDULER_MEMORY_QUEUE">[<code>SCHEDULER_MEMORY_QUEUE</code>{.xref .std .std-setting .docutils
.literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal}</p>
</li>
<li>
<p><a href="index.html#std-setting-SCHEDULER_PRIORITY_QUEUE">[<code>SCHEDULER_PRIORITY_QUEUE</code>{.xref .std .std-setting .docutils
.literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal}</p>
</li>
<li>
<p><a href="index.html#std-setting-SPIDER_MIDDLEWARES">[<code>SPIDER_MIDDLEWARES</code>{.xref .std .std-setting .docutils .literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal}</p>
</li>
</ul>
<p>Third-party Scrapy components may also let you define additional Scrapy
components, usually configurable through <a href="index.html#topics-settings">[settings]{.std
.std-ref}</a>{.hoverxref .tooltip .reference
.internal}, to modify their behavior.</p>
<p>::: {#enforcing-component-requirements .section}
[]{#enforce-component-requirements}</p>
<h4 id="enforcing-component-requirementsheaderlink"><a class="header" href="#enforcing-component-requirementsheaderlink">Enforcing component requirements<a href="#enforcing-component-requirements" title="Permalink to this heading">¶</a>{.headerlink}</a></h4>
<p>Sometimes, your components may only be intended to work under certain
conditions. For example, they may require a minimum version of Scrapy to
work as intended, or they may require certain settings to have specific
values.</p>
<p>In addition to describing those conditions in the documentation of your
component, it is a good practice to raise an exception from the
[<code>__init__</code>{.docutils .literal .notranslate}]{.pre} method of your
component if those conditions are not met at run time.</p>
<p>In the case of <a href="index.html#topics-downloader-middleware">[downloader middlewares]{.std
.std-ref}</a>{.hoverxref .tooltip
.reference .internal}, <a href="index.html#topics-extensions">[extensions]{.std
.std-ref}</a>{.hoverxref .tooltip .reference
.internal}, <a href="index.html#topics-item-pipeline">[item pipelines]{.std
.std-ref}</a>{.hoverxref .tooltip
.reference .internal}, and <a href="index.html#topics-spider-middleware">[spider middlewares]{.std
.std-ref}</a>{.hoverxref .tooltip
.reference .internal}, you should raise
<a href="index.html#scrapy.exceptions.NotConfigured" title="scrapy.exceptions.NotConfigured">[<code>scrapy.exceptions.NotConfigured</code>{.xref .py .py-exc .docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal}, passing a description of the issue as a parameter to the
exception so that it is printed in the logs, for the user to see. For
other components, feel free to raise whatever other exception feels
right to you; for example, <a href="https://docs.python.org/3/library/exceptions.html#RuntimeError" title="(in Python v3.12)">[<code>RuntimeError</code>{.xref .py .py-exc .docutils
.literal
.notranslate}]{.pre}</a>{.reference
.external} would make sense for a Scrapy version mismatch, while
<a href="https://docs.python.org/3/library/exceptions.html#ValueError" title="(in Python v3.12)">[<code>ValueError</code>{.xref .py .py-exc .docutils .literal
.notranslate}]{.pre}</a>{.reference
.external} may be better if the issue is the value of a setting.</p>
<p>If your requirement is a minimum Scrapy version, you may use
[<code>scrapy.__version__</code>{.xref .py .py-attr .docutils .literal
.notranslate}]{.pre} to enforce your requirement. For example:</p>
<p>::: {.highlight-python .notranslate}
::: highlight
from packaging.version import parse as parse_version</p>
<pre><code>import scrapy


class MyComponent:
    def __init__(self):
        if parse_version(scrapy.__version__) &lt; parse_version(&quot;2.7&quot;):
            raise RuntimeError(
                f&quot;{MyComponent.__qualname__} requires Scrapy 2.7 or &quot;
                f&quot;later, which allow defining the process_spider_output &quot;
                f&quot;method of spider middlewares as an asynchronous &quot;
                f&quot;generator.&quot;
            )
</code></pre>
<p>:::
:::
:::
:::</p>
<p>[]{#document-topics/api}</p>
<p>::: {#core-api .section}
[]{#topics-api}</p>
<h3 id="core-apiheaderlink"><a class="header" href="#core-apiheaderlink">Core API<a href="#core-api" title="Permalink to this heading">¶</a>{.headerlink}</a></h3>
<p>This section documents the Scrapy core API, and it's intended for
developers of extensions and middlewares.</p>
<p>::: {#crawler-api .section}
[]{#topics-api-crawler}</p>
<h4 id="crawler-apiheaderlink"><a class="header" href="#crawler-apiheaderlink">Crawler API<a href="#crawler-api" title="Permalink to this heading">¶</a>{.headerlink}</a></h4>
<p>The main entry point to Scrapy API is the <a href="#scrapy.crawler.Crawler" title="scrapy.crawler.Crawler">[<code>Crawler</code>{.xref .py
.py-class .docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal} object, passed to extensions through the
[<code>from_crawler</code>{.docutils .literal .notranslate}]{.pre} class method.
This object provides access to all Scrapy core components, and it's the
only way for extensions to access them and hook their functionality into
Scrapy.</p>
<p>[]{#module-scrapy.crawler .target}</p>
<p>The Extension Manager is responsible for loading and keeping track of
installed extensions and it's configured through the
<a href="index.html#std-setting-EXTENSIONS">[<code>EXTENSIONS</code>{.xref .std .std-setting .docutils .literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal} setting which contains a dictionary of
all available extensions and their order similar to how you <a href="index.html#topics-downloader-middleware-setting">[configure
the downloader middlewares]{.std
.std-ref}</a>{.hoverxref
.tooltip .reference .internal}.</p>
<p><em>[class]{.pre}[ ]{.w}</em>[[scrapy.crawler.]{.pre}]{.sig-prename .descclassname}[[Crawler]{.pre}]{.sig-name .descname}[(]{.sig-paren}<em>[[spidercls]{.pre}]{.n}</em>, <em>[[settings]{.pre}]{.n}</em>[)]{.sig-paren}<a href="_modules/scrapy/crawler.html#Crawler">[[[source]]{.pre}]{.viewcode-link}</a>{.reference .internal}<a href="#scrapy.crawler.Crawler" title="Permalink to this definition">¶</a>{.headerlink}</p>
<p>:   The Crawler object must be instantiated with a
<a href="index.html#scrapy.Spider" title="scrapy.Spider">[<code>scrapy.Spider</code>{.xref .py .py-class .docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal} subclass and a <a href="#scrapy.settings.Settings" title="scrapy.settings.Settings">[<code>scrapy.settings.Settings</code>{.xref .py
.py-class .docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal} object.</p>
<pre><code>[[request_fingerprinter]{.pre}]{.sig-name .descname}[¶](#scrapy.crawler.Crawler.request_fingerprinter &quot;Permalink to this definition&quot;){.headerlink}

:   The request fingerprint builder of this crawler.

    This is used from extensions and middlewares to build short,
    unique identifiers for requests. See [[Request
    fingerprints]{.std
    .std-ref}](index.html#request-fingerprints){.hoverxref .tooltip
    .reference .internal}.

[[settings]{.pre}]{.sig-name .descname}[¶](#scrapy.crawler.Crawler.settings &quot;Permalink to this definition&quot;){.headerlink}

:   The settings manager of this crawler.

    This is used by extensions &amp; middlewares to access the Scrapy
    settings of this crawler.

    For an introduction on Scrapy settings see [[Settings]{.std
    .std-ref}](index.html#topics-settings){.hoverxref .tooltip
    .reference .internal}.

    For the API see [[`Settings`{.xref .py .py-class .docutils
    .literal
    .notranslate}]{.pre}](#scrapy.settings.Settings &quot;scrapy.settings.Settings&quot;){.reference
    .internal} class.

[[signals]{.pre}]{.sig-name .descname}[¶](#scrapy.crawler.Crawler.signals &quot;Permalink to this definition&quot;){.headerlink}

:   The signals manager of this crawler.

    This is used by extensions &amp; middlewares to hook themselves into
    Scrapy functionality.

    For an introduction on signals see [[Signals]{.std
    .std-ref}](index.html#topics-signals){.hoverxref .tooltip
    .reference .internal}.

    For the API see [[`SignalManager`{.xref .py .py-class .docutils
    .literal
    .notranslate}]{.pre}](#scrapy.signalmanager.SignalManager &quot;scrapy.signalmanager.SignalManager&quot;){.reference
    .internal} class.

[[stats]{.pre}]{.sig-name .descname}[¶](#scrapy.crawler.Crawler.stats &quot;Permalink to this definition&quot;){.headerlink}

:   The stats collector of this crawler.

    This is used from extensions &amp; middlewares to record stats of
    their behaviour, or access stats collected by other extensions.

    For an introduction on stats collection see [[Stats
    Collection]{.std .std-ref}](index.html#topics-stats){.hoverxref
    .tooltip .reference .internal}.

    For the API see [[`StatsCollector`{.xref .py .py-class .docutils
    .literal
    .notranslate}]{.pre}](#scrapy.statscollectors.StatsCollector &quot;scrapy.statscollectors.StatsCollector&quot;){.reference
    .internal} class.

[[extensions]{.pre}]{.sig-name .descname}[¶](#scrapy.crawler.Crawler.extensions &quot;Permalink to this definition&quot;){.headerlink}

:   The extension manager that keeps track of enabled extensions.

    Most extensions won't need to access this attribute.

    For an introduction on extensions and a list of available
    extensions on Scrapy see [[Extensions]{.std
    .std-ref}](index.html#topics-extensions){.hoverxref .tooltip
    .reference .internal}.

[[engine]{.pre}]{.sig-name .descname}[¶](#scrapy.crawler.Crawler.engine &quot;Permalink to this definition&quot;){.headerlink}

:   The execution engine, which coordinates the core crawling logic
    between the scheduler, downloader and spiders.

    Some extension may want to access the Scrapy engine, to inspect
    or modify the downloader and scheduler behaviour, although this
    is an advanced use and this API is not yet stable.

[[spider]{.pre}]{.sig-name .descname}[¶](#scrapy.crawler.Crawler.spider &quot;Permalink to this definition&quot;){.headerlink}

:   Spider currently being crawled. This is an instance of the
    spider class provided while constructing the crawler, and it is
    created after the arguments given in the [[`crawl()`{.xref .py
    .py-meth .docutils .literal
    .notranslate}]{.pre}](#scrapy.crawler.Crawler.crawl &quot;scrapy.crawler.Crawler.crawl&quot;){.reference
    .internal} method.

[[crawl]{.pre}]{.sig-name .descname}[(]{.sig-paren}*[[\*]{.pre}]{.o}[[args]{.pre}]{.n}*, *[[\*\*]{.pre}]{.o}[[kwargs]{.pre}]{.n}*[)]{.sig-paren}[[[\[source\]]{.pre}]{.viewcode-link}](_modules/scrapy/crawler.html#Crawler.crawl){.reference .internal}[¶](#scrapy.crawler.Crawler.crawl &quot;Permalink to this definition&quot;){.headerlink}

:   Starts the crawler by instantiating its spider class with the
    given [`args`{.docutils .literal .notranslate}]{.pre} and
    [`kwargs`{.docutils .literal .notranslate}]{.pre} arguments,
    while setting the execution engine in motion. Should be called
    only once.

    Returns a deferred that is fired when the crawl is finished.

[[stop]{.pre}]{.sig-name .descname}[(]{.sig-paren}[)]{.sig-paren} [[→]{.sig-return-icon} [[[Generator]{.pre}](https://docs.python.org/3/library/typing.html#typing.Generator &quot;(in Python v3.12)&quot;){.reference .external}[[\[]{.pre}]{.p}[[Deferred]{.pre}](https://docs.twisted.org/en/stable/api/twisted.internet.defer.Deferred.html &quot;(in Twisted)&quot;){.reference .external}[[,]{.pre}]{.p}[ ]{.w}[[Any]{.pre}](https://docs.python.org/3/library/typing.html#typing.Any &quot;(in Python v3.12)&quot;){.reference .external}[[,]{.pre}]{.p}[ ]{.w}[[None]{.pre}](https://docs.python.org/3/library/constants.html#None &quot;(in Python v3.12)&quot;){.reference .external}[[\]]{.pre}]{.p}]{.sig-return-typehint}]{.sig-return}[[[\[source\]]{.pre}]{.viewcode-link}](_modules/scrapy/crawler.html#Crawler.stop){.reference .internal}[¶](#scrapy.crawler.Crawler.stop &quot;Permalink to this definition&quot;){.headerlink}

:   Starts a graceful stop of the crawler and returns a deferred
    that is fired when the crawler is stopped.
</code></pre>
<pre><code class="language-{=html}">&lt;!-- --&gt;
</code></pre>
<p><em>[class]{.pre}[ ]{.w}</em>[[scrapy.crawler.]{.pre}]{.sig-prename .descclassname}[[CrawlerRunner]{.pre}]{.sig-name .descname}[(]{.sig-paren}<em>[[settings]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[<a href="https://docs.python.org/3/library/typing.html#typing.Optional" title="(in Python v3.12)">[Optional]{.pre}</a>{.reference .external}[[[]{.pre}]{.p}<a href="https://docs.python.org/3/library/typing.html#typing.Union" title="(in Python v3.12)">[Union]{.pre}</a>{.reference .external}[[[]{.pre}]{.p}<a href="https://docs.python.org/3/library/typing.html#typing.Dict" title="(in Python v3.12)">[Dict]{.pre}</a>{.reference .external}[[[]{.pre}]{.p}<a href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.12)">[str]{.pre}</a>{.reference .external}[[,]{.pre}]{.p}[ ]{.w}<a href="https://docs.python.org/3/library/typing.html#typing.Any" title="(in Python v3.12)">[Any]{.pre}</a>{.reference .external}[[]]{.pre}]{.p}[[,]{.pre}]{.p}[ ]{.w}<a href="index.html#scrapy.settings.Settings" title="scrapy.settings.Settings">[Settings]{.pre}</a>{.reference .internal}[[]]{.pre}]{.p}[[]]{.pre}]{.p}]{.n}[ ]{.w}[[=]{.pre}]{.o}[ ]{.w}[[None]{.pre}]{.default_value}</em>[)]{.sig-paren}<a href="_modules/scrapy/crawler.html#CrawlerRunner">[[[source]]{.pre}]{.viewcode-link}</a>{.reference .internal}<a href="#scrapy.crawler.CrawlerRunner" title="Permalink to this definition">¶</a>{.headerlink}</p>
<p>:   This is a convenient helper class that keeps track of, manages and
runs crawlers inside an already setup <a href="https://docs.twisted.org/en/stable/api/twisted.internet.reactor.html" title="(in Twisted)">[<code>reactor</code>{.xref .py .py-mod
.docutils .literal
.notranslate}]{.pre}</a>{.reference
.external}.</p>
<pre><code>The CrawlerRunner object must be instantiated with a
[[`Settings`{.xref .py .py-class .docutils .literal
.notranslate}]{.pre}](#scrapy.settings.Settings &quot;scrapy.settings.Settings&quot;){.reference
.internal} object.

This class shouldn't be needed (since Scrapy is responsible of using
it accordingly) unless writing scripts that manually handle the
crawling process. See [[Run Scrapy from a script]{.std
.std-ref}](index.html#run-from-script){.hoverxref .tooltip
.reference .internal} for an example.

[[crawl]{.pre}]{.sig-name .descname}[(]{.sig-paren}*[[crawler_or_spidercls]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[[[Union]{.pre}](https://docs.python.org/3/library/typing.html#typing.Union &quot;(in Python v3.12)&quot;){.reference .external}[[\[]{.pre}]{.p}[[Type]{.pre}](https://docs.python.org/3/library/typing.html#typing.Type &quot;(in Python v3.12)&quot;){.reference .external}[[\[]{.pre}]{.p}[[Spider]{.pre}](index.html#scrapy.spiders.Spider &quot;scrapy.spiders.Spider&quot;){.reference .internal}[[\]]{.pre}]{.p}[[,]{.pre}]{.p}[ ]{.w}[[str]{.pre}](https://docs.python.org/3/library/stdtypes.html#str &quot;(in Python v3.12)&quot;){.reference .external}[[,]{.pre}]{.p}[ ]{.w}[[Crawler]{.pre}](index.html#scrapy.crawler.Crawler &quot;scrapy.crawler.Crawler&quot;){.reference .internal}[[\]]{.pre}]{.p}]{.n}*, *[[\*]{.pre}]{.o}[[args]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[[[Any]{.pre}](https://docs.python.org/3/library/typing.html#typing.Any &quot;(in Python v3.12)&quot;){.reference .external}]{.n}*, *[[\*\*]{.pre}]{.o}[[kwargs]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[[[Any]{.pre}](https://docs.python.org/3/library/typing.html#typing.Any &quot;(in Python v3.12)&quot;){.reference .external}]{.n}*[)]{.sig-paren} [[→]{.sig-return-icon} [[[Deferred]{.pre}](https://docs.twisted.org/en/stable/api/twisted.internet.defer.Deferred.html &quot;(in Twisted)&quot;){.reference .external}]{.sig-return-typehint}]{.sig-return}[[[\[source\]]{.pre}]{.viewcode-link}](_modules/scrapy/crawler.html#CrawlerRunner.crawl){.reference .internal}[¶](#scrapy.crawler.CrawlerRunner.crawl &quot;Permalink to this definition&quot;){.headerlink}

:   Run a crawler with the provided arguments.

    It will call the given Crawler's [[`crawl()`{.xref .py .py-meth
    .docutils .literal
    .notranslate}]{.pre}](#scrapy.crawler.Crawler.crawl &quot;scrapy.crawler.Crawler.crawl&quot;){.reference
    .internal} method, while keeping track of it so it can be
    stopped later.

    If [`crawler_or_spidercls`{.docutils .literal
    .notranslate}]{.pre} isn't a [[`Crawler`{.xref .py .py-class
    .docutils .literal
    .notranslate}]{.pre}](#scrapy.crawler.Crawler &quot;scrapy.crawler.Crawler&quot;){.reference
    .internal} instance, this method will try to create one using
    this parameter as the spider class given to it.

    Returns a deferred that is fired when the crawling is finished.

    Parameters

    :   -   **crawler_or_spidercls** ([[`Crawler`{.xref .py
            .py-class .docutils .literal
            .notranslate}]{.pre}](#scrapy.crawler.Crawler &quot;scrapy.crawler.Crawler&quot;){.reference
            .internal} instance, [[`Spider`{.xref .py .py-class
            .docutils .literal
            .notranslate}]{.pre}](index.html#scrapy.spiders.Spider &quot;scrapy.spiders.Spider&quot;){.reference
            .internal} subclass or string) -- already created
            crawler, or a spider class or spider's name inside the
            project to create it

        -   **args** -- arguments to initialize the spider

        -   **kwargs** -- keyword arguments to initialize the spider

*[property]{.pre}[ ]{.w}*[[crawlers]{.pre}]{.sig-name .descname}[¶](#scrapy.crawler.CrawlerRunner.crawlers &quot;Permalink to this definition&quot;){.headerlink}

:   Set of [[`crawlers`{.xref .py .py-class .docutils .literal
    .notranslate}]{.pre}](#scrapy.crawler.Crawler &quot;scrapy.crawler.Crawler&quot;){.reference
    .internal} started by [[`crawl()`{.xref .py .py-meth .docutils
    .literal
    .notranslate}]{.pre}](#scrapy.crawler.CrawlerRunner.crawl &quot;scrapy.crawler.CrawlerRunner.crawl&quot;){.reference
    .internal} and managed by this class.

[[create_crawler]{.pre}]{.sig-name .descname}[(]{.sig-paren}*[[crawler_or_spidercls]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[[[Union]{.pre}](https://docs.python.org/3/library/typing.html#typing.Union &quot;(in Python v3.12)&quot;){.reference .external}[[\[]{.pre}]{.p}[[Type]{.pre}](https://docs.python.org/3/library/typing.html#typing.Type &quot;(in Python v3.12)&quot;){.reference .external}[[\[]{.pre}]{.p}[[Spider]{.pre}](index.html#scrapy.spiders.Spider &quot;scrapy.spiders.Spider&quot;){.reference .internal}[[\]]{.pre}]{.p}[[,]{.pre}]{.p}[ ]{.w}[[str]{.pre}](https://docs.python.org/3/library/stdtypes.html#str &quot;(in Python v3.12)&quot;){.reference .external}[[,]{.pre}]{.p}[ ]{.w}[[Crawler]{.pre}](index.html#scrapy.crawler.Crawler &quot;scrapy.crawler.Crawler&quot;){.reference .internal}[[\]]{.pre}]{.p}]{.n}*[)]{.sig-paren} [[→]{.sig-return-icon} [[[Crawler]{.pre}](index.html#scrapy.crawler.Crawler &quot;scrapy.crawler.Crawler&quot;){.reference .internal}]{.sig-return-typehint}]{.sig-return}[[[\[source\]]{.pre}]{.viewcode-link}](_modules/scrapy/crawler.html#CrawlerRunner.create_crawler){.reference .internal}[¶](#scrapy.crawler.CrawlerRunner.create_crawler &quot;Permalink to this definition&quot;){.headerlink}

:   Return a [[`Crawler`{.xref .py .py-class .docutils .literal
    .notranslate}]{.pre}](#scrapy.crawler.Crawler &quot;scrapy.crawler.Crawler&quot;){.reference
    .internal} object.

    -   If [`crawler_or_spidercls`{.docutils .literal
        .notranslate}]{.pre} is a Crawler, it is returned as-is.

    -   If [`crawler_or_spidercls`{.docutils .literal
        .notranslate}]{.pre} is a Spider subclass, a new Crawler is
        constructed for it.

    -   If [`crawler_or_spidercls`{.docutils .literal
        .notranslate}]{.pre} is a string, this function finds a
        spider with this name in a Scrapy project (using spider
        loader), then creates a Crawler instance for it.

[[join]{.pre}]{.sig-name .descname}[(]{.sig-paren}[)]{.sig-paren}[[[\[source\]]{.pre}]{.viewcode-link}](_modules/scrapy/crawler.html#CrawlerRunner.join){.reference .internal}[¶](#scrapy.crawler.CrawlerRunner.join &quot;Permalink to this definition&quot;){.headerlink}

:   Returns a deferred that is fired when all managed
    [[`crawlers`{.xref .py .py-attr .docutils .literal
    .notranslate}]{.pre}](#scrapy.crawler.CrawlerRunner.crawlers &quot;scrapy.crawler.CrawlerRunner.crawlers&quot;){.reference
    .internal} have completed their executions.

[[stop]{.pre}]{.sig-name .descname}[(]{.sig-paren}[)]{.sig-paren} [[→]{.sig-return-icon} [[[Deferred]{.pre}](https://docs.twisted.org/en/stable/api/twisted.internet.defer.Deferred.html &quot;(in Twisted)&quot;){.reference .external}]{.sig-return-typehint}]{.sig-return}[[[\[source\]]{.pre}]{.viewcode-link}](_modules/scrapy/crawler.html#CrawlerRunner.stop){.reference .internal}[¶](#scrapy.crawler.CrawlerRunner.stop &quot;Permalink to this definition&quot;){.headerlink}

:   Stops simultaneously all the crawling jobs taking place.

    Returns a deferred that is fired when they all have ended.
</code></pre>
<pre><code class="language-{=html}">&lt;!-- --&gt;
</code></pre>
<p><em>[class]{.pre}[ ]{.w}</em>[[scrapy.crawler.]{.pre}]{.sig-prename .descclassname}[[CrawlerProcess]{.pre}]{.sig-name .descname}[(]{.sig-paren}<em>[[settings]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[<a href="https://docs.python.org/3/library/typing.html#typing.Optional" title="(in Python v3.12)">[Optional]{.pre}</a>{.reference .external}[[[]{.pre}]{.p}<a href="https://docs.python.org/3/library/typing.html#typing.Union" title="(in Python v3.12)">[Union]{.pre}</a>{.reference .external}[[[]{.pre}]{.p}<a href="https://docs.python.org/3/library/typing.html#typing.Dict" title="(in Python v3.12)">[Dict]{.pre}</a>{.reference .external}[[[]{.pre}]{.p}<a href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.12)">[str]{.pre}</a>{.reference .external}[[,]{.pre}]{.p}[ ]{.w}<a href="https://docs.python.org/3/library/typing.html#typing.Any" title="(in Python v3.12)">[Any]{.pre}</a>{.reference .external}[[]]{.pre}]{.p}[[,]{.pre}]{.p}[ ]{.w}<a href="index.html#scrapy.settings.Settings" title="scrapy.settings.Settings">[Settings]{.pre}</a>{.reference .internal}[[]]{.pre}]{.p}[[]]{.pre}]{.p}]{.n}[ ]{.w}[[=]{.pre}]{.o}[ ]{.w}[[None]{.pre}]{.default_value}</em>, <em>[[install_root_handler]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[<a href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.12)">[bool]{.pre}</a>{.reference .external}]{.n}[ ]{.w}[[=]{.pre}]{.o}[ ]{.w}[[True]{.pre}]{.default_value}</em>[)]{.sig-paren}<a href="_modules/scrapy/crawler.html#CrawlerProcess">[[[source]]{.pre}]{.viewcode-link}</a>{.reference .internal}<a href="#scrapy.crawler.CrawlerProcess" title="Permalink to this definition">¶</a>{.headerlink}</p>
<p>:   Bases: <a href="#scrapy.crawler.CrawlerRunner" title="scrapy.crawler.CrawlerRunner">[<code>CrawlerRunner</code>{.xref .py .py-class .docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal}</p>
<pre><code>A class to run multiple scrapy crawlers in a process simultaneously.

This class extends [[`CrawlerRunner`{.xref .py .py-class .docutils
.literal
.notranslate}]{.pre}](#scrapy.crawler.CrawlerRunner &quot;scrapy.crawler.CrawlerRunner&quot;){.reference
.internal} by adding support for starting a [[`reactor`{.xref .py
.py-mod .docutils .literal
.notranslate}]{.pre}](https://docs.twisted.org/en/stable/api/twisted.internet.reactor.html &quot;(in Twisted)&quot;){.reference
.external} and handling shutdown signals, like the keyboard
interrupt command Ctrl-C. It also configures top-level logging.

This utility should be a better fit than [[`CrawlerRunner`{.xref .py
.py-class .docutils .literal
.notranslate}]{.pre}](#scrapy.crawler.CrawlerRunner &quot;scrapy.crawler.CrawlerRunner&quot;){.reference
.internal} if you aren't running another [[`reactor`{.xref .py
.py-mod .docutils .literal
.notranslate}]{.pre}](https://docs.twisted.org/en/stable/api/twisted.internet.reactor.html &quot;(in Twisted)&quot;){.reference
.external} within your application.

The CrawlerProcess object must be instantiated with a
[[`Settings`{.xref .py .py-class .docutils .literal
.notranslate}]{.pre}](#scrapy.settings.Settings &quot;scrapy.settings.Settings&quot;){.reference
.internal} object.

Parameters

:   **install_root_handler** -- whether to install root logging
    handler (default: True)

This class shouldn't be needed (since Scrapy is responsible of using
it accordingly) unless writing scripts that manually handle the
crawling process. See [[Run Scrapy from a script]{.std
.std-ref}](index.html#run-from-script){.hoverxref .tooltip
.reference .internal} for an example.

[[crawl]{.pre}]{.sig-name .descname}[(]{.sig-paren}*[[crawler_or_spidercls]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[[[Union]{.pre}](https://docs.python.org/3/library/typing.html#typing.Union &quot;(in Python v3.12)&quot;){.reference .external}[[\[]{.pre}]{.p}[[Type]{.pre}](https://docs.python.org/3/library/typing.html#typing.Type &quot;(in Python v3.12)&quot;){.reference .external}[[\[]{.pre}]{.p}[[Spider]{.pre}](index.html#scrapy.spiders.Spider &quot;scrapy.spiders.Spider&quot;){.reference .internal}[[\]]{.pre}]{.p}[[,]{.pre}]{.p}[ ]{.w}[[str]{.pre}](https://docs.python.org/3/library/stdtypes.html#str &quot;(in Python v3.12)&quot;){.reference .external}[[,]{.pre}]{.p}[ ]{.w}[[Crawler]{.pre}](index.html#scrapy.crawler.Crawler &quot;scrapy.crawler.Crawler&quot;){.reference .internal}[[\]]{.pre}]{.p}]{.n}*, *[[\*]{.pre}]{.o}[[args]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[[[Any]{.pre}](https://docs.python.org/3/library/typing.html#typing.Any &quot;(in Python v3.12)&quot;){.reference .external}]{.n}*, *[[\*\*]{.pre}]{.o}[[kwargs]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[[[Any]{.pre}](https://docs.python.org/3/library/typing.html#typing.Any &quot;(in Python v3.12)&quot;){.reference .external}]{.n}*[)]{.sig-paren} [[→]{.sig-return-icon} [[[Deferred]{.pre}](https://docs.twisted.org/en/stable/api/twisted.internet.defer.Deferred.html &quot;(in Twisted)&quot;){.reference .external}]{.sig-return-typehint}]{.sig-return}[¶](#scrapy.crawler.CrawlerProcess.crawl &quot;Permalink to this definition&quot;){.headerlink}

:   Run a crawler with the provided arguments.

    It will call the given Crawler's [[`crawl()`{.xref .py .py-meth
    .docutils .literal
    .notranslate}]{.pre}](#scrapy.crawler.Crawler.crawl &quot;scrapy.crawler.Crawler.crawl&quot;){.reference
    .internal} method, while keeping track of it so it can be
    stopped later.

    If [`crawler_or_spidercls`{.docutils .literal
    .notranslate}]{.pre} isn't a [[`Crawler`{.xref .py .py-class
    .docutils .literal
    .notranslate}]{.pre}](#scrapy.crawler.Crawler &quot;scrapy.crawler.Crawler&quot;){.reference
    .internal} instance, this method will try to create one using
    this parameter as the spider class given to it.

    Returns a deferred that is fired when the crawling is finished.

    Parameters

    :   -   **crawler_or_spidercls** ([[`Crawler`{.xref .py
            .py-class .docutils .literal
            .notranslate}]{.pre}](#scrapy.crawler.Crawler &quot;scrapy.crawler.Crawler&quot;){.reference
            .internal} instance, [[`Spider`{.xref .py .py-class
            .docutils .literal
            .notranslate}]{.pre}](index.html#scrapy.spiders.Spider &quot;scrapy.spiders.Spider&quot;){.reference
            .internal} subclass or string) -- already created
            crawler, or a spider class or spider's name inside the
            project to create it

        -   **args** -- arguments to initialize the spider

        -   **kwargs** -- keyword arguments to initialize the spider

*[property]{.pre}[ ]{.w}*[[crawlers]{.pre}]{.sig-name .descname}[¶](#scrapy.crawler.CrawlerProcess.crawlers &quot;Permalink to this definition&quot;){.headerlink}

:   Set of [[`crawlers`{.xref .py .py-class .docutils .literal
    .notranslate}]{.pre}](#scrapy.crawler.Crawler &quot;scrapy.crawler.Crawler&quot;){.reference
    .internal} started by [[`crawl()`{.xref .py .py-meth .docutils
    .literal
    .notranslate}]{.pre}](#scrapy.crawler.CrawlerProcess.crawl &quot;scrapy.crawler.CrawlerProcess.crawl&quot;){.reference
    .internal} and managed by this class.

[[create_crawler]{.pre}]{.sig-name .descname}[(]{.sig-paren}*[[crawler_or_spidercls]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[[[Union]{.pre}](https://docs.python.org/3/library/typing.html#typing.Union &quot;(in Python v3.12)&quot;){.reference .external}[[\[]{.pre}]{.p}[[Type]{.pre}](https://docs.python.org/3/library/typing.html#typing.Type &quot;(in Python v3.12)&quot;){.reference .external}[[\[]{.pre}]{.p}[[Spider]{.pre}](index.html#scrapy.spiders.Spider &quot;scrapy.spiders.Spider&quot;){.reference .internal}[[\]]{.pre}]{.p}[[,]{.pre}]{.p}[ ]{.w}[[str]{.pre}](https://docs.python.org/3/library/stdtypes.html#str &quot;(in Python v3.12)&quot;){.reference .external}[[,]{.pre}]{.p}[ ]{.w}[[Crawler]{.pre}](index.html#scrapy.crawler.Crawler &quot;scrapy.crawler.Crawler&quot;){.reference .internal}[[\]]{.pre}]{.p}]{.n}*[)]{.sig-paren} [[→]{.sig-return-icon} [[[Crawler]{.pre}](index.html#scrapy.crawler.Crawler &quot;scrapy.crawler.Crawler&quot;){.reference .internal}]{.sig-return-typehint}]{.sig-return}[¶](#scrapy.crawler.CrawlerProcess.create_crawler &quot;Permalink to this definition&quot;){.headerlink}

:   Return a [[`Crawler`{.xref .py .py-class .docutils .literal
    .notranslate}]{.pre}](#scrapy.crawler.Crawler &quot;scrapy.crawler.Crawler&quot;){.reference
    .internal} object.

    -   If [`crawler_or_spidercls`{.docutils .literal
        .notranslate}]{.pre} is a Crawler, it is returned as-is.

    -   If [`crawler_or_spidercls`{.docutils .literal
        .notranslate}]{.pre} is a Spider subclass, a new Crawler is
        constructed for it.

    -   If [`crawler_or_spidercls`{.docutils .literal
        .notranslate}]{.pre} is a string, this function finds a
        spider with this name in a Scrapy project (using spider
        loader), then creates a Crawler instance for it.

[[join]{.pre}]{.sig-name .descname}[(]{.sig-paren}[)]{.sig-paren}[¶](#scrapy.crawler.CrawlerProcess.join &quot;Permalink to this definition&quot;){.headerlink}

:   Returns a deferred that is fired when all managed
    [[`crawlers`{.xref .py .py-attr .docutils .literal
    .notranslate}]{.pre}](#scrapy.crawler.CrawlerProcess.crawlers &quot;scrapy.crawler.CrawlerProcess.crawlers&quot;){.reference
    .internal} have completed their executions.

[[start]{.pre}]{.sig-name .descname}[(]{.sig-paren}*[[stop_after_crawl]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[[[bool]{.pre}](https://docs.python.org/3/library/functions.html#bool &quot;(in Python v3.12)&quot;){.reference .external}]{.n}[ ]{.w}[[=]{.pre}]{.o}[ ]{.w}[[True]{.pre}]{.default_value}*, *[[install_signal_handlers]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[[[bool]{.pre}](https://docs.python.org/3/library/functions.html#bool &quot;(in Python v3.12)&quot;){.reference .external}]{.n}[ ]{.w}[[=]{.pre}]{.o}[ ]{.w}[[True]{.pre}]{.default_value}*[)]{.sig-paren} [[→]{.sig-return-icon} [[[None]{.pre}](https://docs.python.org/3/library/constants.html#None &quot;(in Python v3.12)&quot;){.reference .external}]{.sig-return-typehint}]{.sig-return}[[[\[source\]]{.pre}]{.viewcode-link}](_modules/scrapy/crawler.html#CrawlerProcess.start){.reference .internal}[¶](#scrapy.crawler.CrawlerProcess.start &quot;Permalink to this definition&quot;){.headerlink}

:   This method starts a [[`reactor`{.xref .py .py-mod .docutils
    .literal
    .notranslate}]{.pre}](https://docs.twisted.org/en/stable/api/twisted.internet.reactor.html &quot;(in Twisted)&quot;){.reference
    .external}, adjusts its pool size to
    [[`REACTOR_THREADPOOL_MAXSIZE`{.xref .std .std-setting .docutils
    .literal
    .notranslate}]{.pre}](index.html#std-setting-REACTOR_THREADPOOL_MAXSIZE){.hoverxref
    .tooltip .reference .internal}, and installs a DNS cache based
    on [[`DNSCACHE_ENABLED`{.xref .std .std-setting .docutils
    .literal
    .notranslate}]{.pre}](index.html#std-setting-DNSCACHE_ENABLED){.hoverxref
    .tooltip .reference .internal} and [[`DNSCACHE_SIZE`{.xref .std
    .std-setting .docutils .literal
    .notranslate}]{.pre}](index.html#std-setting-DNSCACHE_SIZE){.hoverxref
    .tooltip .reference .internal}.

    If [`stop_after_crawl`{.docutils .literal .notranslate}]{.pre}
    is True, the reactor will be stopped after all crawlers have
    finished, using [[`join()`{.xref .py .py-meth .docutils .literal
    .notranslate}]{.pre}](#scrapy.crawler.CrawlerProcess.join &quot;scrapy.crawler.CrawlerProcess.join&quot;){.reference
    .internal}.

    Parameters

    :   -   **stop_after_crawl**
            ([*bool*](https://docs.python.org/3/library/functions.html#bool &quot;(in Python v3.12)&quot;){.reference
            .external}) -- stop or not the reactor when all crawlers
            have finished

        -   **install_signal_handlers**
            ([*bool*](https://docs.python.org/3/library/functions.html#bool &quot;(in Python v3.12)&quot;){.reference
            .external}) -- whether to install the OS signal handlers
            from Twisted and Scrapy (default: True)

[[stop]{.pre}]{.sig-name .descname}[(]{.sig-paren}[)]{.sig-paren} [[→]{.sig-return-icon} [[[Deferred]{.pre}](https://docs.twisted.org/en/stable/api/twisted.internet.defer.Deferred.html &quot;(in Twisted)&quot;){.reference .external}]{.sig-return-typehint}]{.sig-return}[¶](#scrapy.crawler.CrawlerProcess.stop &quot;Permalink to this definition&quot;){.headerlink}

:   Stops simultaneously all the crawling jobs taking place.

    Returns a deferred that is fired when they all have ended.
</code></pre>
<p>:::</p>
<p>::: {#module-scrapy.settings .section}
[]{#settings-api}[]{#topics-api-settings}</p>
<h4 id="settings-apiheaderlink"><a class="header" href="#settings-apiheaderlink">Settings API<a href="#module-scrapy.settings" title="Permalink to this heading">¶</a>{.headerlink}</a></h4>
<p>[[scrapy.settings.]{.pre}]{.sig-prename .descclassname}[[SETTINGS_PRIORITIES]{.pre}]{.sig-name .descname}<a href="#scrapy.settings.SETTINGS_PRIORITIES" title="Permalink to this definition">¶</a>{.headerlink}</p>
<p>:   Dictionary that sets the key name and priority level of the default
settings priorities used in Scrapy.</p>
<pre><code>Each item defines a settings entry point, giving it a code name for
identification and an integer priority. Greater priorities take more
precedence over lesser ones when setting and retrieving values in
the [[`Settings`{.xref .py .py-class .docutils .literal
.notranslate}]{.pre}](#scrapy.settings.Settings &quot;scrapy.settings.Settings&quot;){.reference
.internal} class.

::: {.highlight-python .notranslate}
::: highlight
    SETTINGS_PRIORITIES = {
        &quot;default&quot;: 0,
        &quot;command&quot;: 10,
        &quot;addon&quot;: 15,
        &quot;project&quot;: 20,
        &quot;spider&quot;: 30,
        &quot;cmdline&quot;: 40,
    }
:::
:::

For a detailed explanation on each settings sources, see:
[[Settings]{.std .std-ref}](index.html#topics-settings){.hoverxref
.tooltip .reference .internal}.
</code></pre>
<pre><code class="language-{=html}">&lt;!-- --&gt;
</code></pre>
<p>[[scrapy.settings.]{.pre}]{.sig-prename .descclassname}[[get_settings_priority]{.pre}]{.sig-name .descname}[(]{.sig-paren}<em>[[priority]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[<a href="https://docs.python.org/3/library/typing.html#typing.Union" title="(in Python v3.12)">[Union]{.pre}</a>{.reference .external}[[[]{.pre}]{.p}<a href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.12)">[int]{.pre}</a>{.reference .external}[[,]{.pre}]{.p}[ ]{.w}<a href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.12)">[str]{.pre}</a>{.reference .external}[[]]{.pre}]{.p}]{.n}</em>[)]{.sig-paren} [[→]{.sig-return-icon} [<a href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.12)">[int]{.pre}</a>{.reference .external}]{.sig-return-typehint}]{.sig-return}<a href="_modules/scrapy/settings.html#get_settings_priority">[[[source]]{.pre}]{.viewcode-link}</a>{.reference .internal}<a href="#scrapy.settings.get_settings_priority" title="Permalink to this definition">¶</a>{.headerlink}</p>
<p>:   Small helper function that looks up a given string priority in the
<a href="#scrapy.settings.SETTINGS_PRIORITIES" title="scrapy.settings.SETTINGS_PRIORITIES">[<code>SETTINGS_PRIORITIES</code>{.xref .py .py-attr .docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal} dictionary and returns its numerical value, or directly
returns a given numerical priority.</p>
<pre><code class="language-{=html}">&lt;!-- --&gt;
</code></pre>
<p><em>[class]{.pre}[ ]{.w}</em>[[scrapy.settings.]{.pre}]{.sig-prename .descclassname}[[Settings]{.pre}]{.sig-name .descname}[(]{.sig-paren}<em>[[values]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[[_SettingsInputT]{.pre}]{.n}[ ]{.w}[[=]{.pre}]{.o}[ ]{.w}[[None]{.pre}]{.default_value}</em>, <em>[[priority]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[[Union]{.pre}[[[]{.pre}]{.p}<a href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.12)">[int]{.pre}</a>{.reference .external}[[,]{.pre}]{.p}[ ]{.w}<a href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.12)">[str]{.pre}</a>{.reference .external}[[]]{.pre}]{.p}]{.n}[ ]{.w}[[=]{.pre}]{.o}[ ]{.w}[['project']{.pre}]{.default_value}</em>[)]{.sig-paren}<a href="_modules/scrapy/settings.html#Settings">[[[source]]{.pre}]{.viewcode-link}</a>{.reference .internal}<a href="#scrapy.settings.Settings" title="Permalink to this definition">¶</a>{.headerlink}</p>
<p>:   Bases: <a href="#scrapy.settings.BaseSettings" title="scrapy.settings.BaseSettings">[<code>BaseSettings</code>{.xref .py .py-class .docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal}</p>
<pre><code>This object stores Scrapy settings for the configuration of internal
components, and can be used for any further customization.

It is a direct subclass and supports all methods of
[[`BaseSettings`{.xref .py .py-class .docutils .literal
.notranslate}]{.pre}](#scrapy.settings.BaseSettings &quot;scrapy.settings.BaseSettings&quot;){.reference
.internal}. Additionally, after instantiation of this class, the new
object will have the global default settings described on [[Built-in
settings reference]{.std
.std-ref}](index.html#topics-settings-ref){.hoverxref .tooltip
.reference .internal} already populated.
</code></pre>
<pre><code class="language-{=html}">&lt;!-- --&gt;
</code></pre>
<p><em>[class]{.pre}[ ]{.w}</em>[[scrapy.settings.]{.pre}]{.sig-prename .descclassname}[[BaseSettings]{.pre}]{.sig-name .descname}[(]{.sig-paren}<em>[[values]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[[_SettingsInputT]{.pre}]{.n}[ ]{.w}[[=]{.pre}]{.o}[ ]{.w}[[None]{.pre}]{.default_value}</em>, <em>[[priority]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[[Union]{.pre}[[[]{.pre}]{.p}<a href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.12)">[int]{.pre}</a>{.reference .external}[[,]{.pre}]{.p}[ ]{.w}<a href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.12)">[str]{.pre}</a>{.reference .external}[[]]{.pre}]{.p}]{.n}[ ]{.w}[[=]{.pre}]{.o}[ ]{.w}[['project']{.pre}]{.default_value}</em>[)]{.sig-paren}<a href="_modules/scrapy/settings.html#BaseSettings">[[[source]]{.pre}]{.viewcode-link}</a>{.reference .internal}<a href="#scrapy.settings.BaseSettings" title="Permalink to this definition">¶</a>{.headerlink}</p>
<p>:   Instances of this class behave like dictionaries, but store
priorities along with their [<code>(key,</code>{.docutils .literal
.notranslate}]{.pre}<code> </code>{.docutils .literal
.notranslate}[<code>value)</code>{.docutils .literal .notranslate}]{.pre}
pairs, and can be frozen (i.e. marked immutable).</p>
<pre><code>Key-value entries can be passed on initialization with the
[`values`{.docutils .literal .notranslate}]{.pre} argument, and they
would take the [`priority`{.docutils .literal .notranslate}]{.pre}
level (unless [`values`{.docutils .literal .notranslate}]{.pre} is
already an instance of [[`BaseSettings`{.xref .py .py-class
.docutils .literal
.notranslate}]{.pre}](#scrapy.settings.BaseSettings &quot;scrapy.settings.BaseSettings&quot;){.reference
.internal}, in which case the existing priority levels will be
kept). If the [`priority`{.docutils .literal .notranslate}]{.pre}
argument is a string, the priority name will be looked up in
[[`SETTINGS_PRIORITIES`{.xref .py .py-attr .docutils .literal
.notranslate}]{.pre}](#scrapy.settings.SETTINGS_PRIORITIES &quot;scrapy.settings.SETTINGS_PRIORITIES&quot;){.reference
.internal}. Otherwise, a specific integer should be provided.

Once the object is created, new settings can be loaded or updated
with the [[`set()`{.xref .py .py-meth .docutils .literal
.notranslate}]{.pre}](#scrapy.settings.BaseSettings.set &quot;scrapy.settings.BaseSettings.set&quot;){.reference
.internal} method, and can be accessed with the square bracket
notation of dictionaries, or with the [[`get()`{.xref .py .py-meth
.docutils .literal
.notranslate}]{.pre}](#scrapy.settings.BaseSettings.get &quot;scrapy.settings.BaseSettings.get&quot;){.reference
.internal} method of the instance and its value conversion variants.
When requesting a stored key, the value with the highest priority
will be retrieved.

[[copy]{.pre}]{.sig-name .descname}[(]{.sig-paren}[)]{.sig-paren} [[→]{.sig-return-icon} [[Self]{.pre}]{.sig-return-typehint}]{.sig-return}[[[\[source\]]{.pre}]{.viewcode-link}](_modules/scrapy/settings.html#BaseSettings.copy){.reference .internal}[¶](#scrapy.settings.BaseSettings.copy &quot;Permalink to this definition&quot;){.headerlink}

:   Make a deep copy of current settings.

    This method returns a new instance of the [[`Settings`{.xref .py
    .py-class .docutils .literal
    .notranslate}]{.pre}](#scrapy.settings.Settings &quot;scrapy.settings.Settings&quot;){.reference
    .internal} class, populated with the same values and their
    priorities.

    Modifications to the new object won't be reflected on the
    original settings.

[[copy_to_dict]{.pre}]{.sig-name .descname}[(]{.sig-paren}[)]{.sig-paren} [[→]{.sig-return-icon} [[[Dict]{.pre}](https://docs.python.org/3/library/typing.html#typing.Dict &quot;(in Python v3.12)&quot;){.reference .external}[[\[]{.pre}]{.p}[[Optional]{.pre}](https://docs.python.org/3/library/typing.html#typing.Optional &quot;(in Python v3.12)&quot;){.reference .external}[[\[]{.pre}]{.p}[[Union]{.pre}](https://docs.python.org/3/library/typing.html#typing.Union &quot;(in Python v3.12)&quot;){.reference .external}[[\[]{.pre}]{.p}[[bool]{.pre}](https://docs.python.org/3/library/functions.html#bool &quot;(in Python v3.12)&quot;){.reference .external}[[,]{.pre}]{.p}[ ]{.w}[[float]{.pre}](https://docs.python.org/3/library/functions.html#float &quot;(in Python v3.12)&quot;){.reference .external}[[,]{.pre}]{.p}[ ]{.w}[[int]{.pre}](https://docs.python.org/3/library/functions.html#int &quot;(in Python v3.12)&quot;){.reference .external}[[,]{.pre}]{.p}[ ]{.w}[[str]{.pre}](https://docs.python.org/3/library/stdtypes.html#str &quot;(in Python v3.12)&quot;){.reference .external}[[\]]{.pre}]{.p}[[\]]{.pre}]{.p}[[,]{.pre}]{.p}[ ]{.w}[[Any]{.pre}](https://docs.python.org/3/library/typing.html#typing.Any &quot;(in Python v3.12)&quot;){.reference .external}[[\]]{.pre}]{.p}]{.sig-return-typehint}]{.sig-return}[[[\[source\]]{.pre}]{.viewcode-link}](_modules/scrapy/settings.html#BaseSettings.copy_to_dict){.reference .internal}[¶](#scrapy.settings.BaseSettings.copy_to_dict &quot;Permalink to this definition&quot;){.headerlink}

:   Make a copy of current settings and convert to a dict.

    This method returns a new dict populated with the same values
    and their priorities as the current settings.

    Modifications to the returned dict won't be reflected on the
    original settings.

    This method can be useful for example for printing settings in
    Scrapy shell.

[[freeze]{.pre}]{.sig-name .descname}[(]{.sig-paren}[)]{.sig-paren} [[→]{.sig-return-icon} [[[None]{.pre}](https://docs.python.org/3/library/constants.html#None &quot;(in Python v3.12)&quot;){.reference .external}]{.sig-return-typehint}]{.sig-return}[[[\[source\]]{.pre}]{.viewcode-link}](_modules/scrapy/settings.html#BaseSettings.freeze){.reference .internal}[¶](#scrapy.settings.BaseSettings.freeze &quot;Permalink to this definition&quot;){.headerlink}

:   Disable further changes to the current settings.

    After calling this method, the present state of the settings
    will become immutable. Trying to change values through the
    [[`set()`{.xref .py .py-meth .docutils .literal
    .notranslate}]{.pre}](#scrapy.settings.BaseSettings.set &quot;scrapy.settings.BaseSettings.set&quot;){.reference
    .internal} method and its variants won't be possible and will be
    alerted.

[[frozencopy]{.pre}]{.sig-name .descname}[(]{.sig-paren}[)]{.sig-paren} [[→]{.sig-return-icon} [[Self]{.pre}]{.sig-return-typehint}]{.sig-return}[[[\[source\]]{.pre}]{.viewcode-link}](_modules/scrapy/settings.html#BaseSettings.frozencopy){.reference .internal}[¶](#scrapy.settings.BaseSettings.frozencopy &quot;Permalink to this definition&quot;){.headerlink}

:   Return an immutable copy of the current settings.

    Alias for a [[`freeze()`{.xref .py .py-meth .docutils .literal
    .notranslate}]{.pre}](#scrapy.settings.BaseSettings.freeze &quot;scrapy.settings.BaseSettings.freeze&quot;){.reference
    .internal} call in the object returned by [[`copy()`{.xref .py
    .py-meth .docutils .literal
    .notranslate}]{.pre}](#scrapy.settings.BaseSettings.copy &quot;scrapy.settings.BaseSettings.copy&quot;){.reference
    .internal}.

[[get]{.pre}]{.sig-name .descname}[(]{.sig-paren}*[[name]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[[[Optional]{.pre}](https://docs.python.org/3/library/typing.html#typing.Optional &quot;(in Python v3.12)&quot;){.reference .external}[[\[]{.pre}]{.p}[[Union]{.pre}](https://docs.python.org/3/library/typing.html#typing.Union &quot;(in Python v3.12)&quot;){.reference .external}[[\[]{.pre}]{.p}[[bool]{.pre}](https://docs.python.org/3/library/functions.html#bool &quot;(in Python v3.12)&quot;){.reference .external}[[,]{.pre}]{.p}[ ]{.w}[[float]{.pre}](https://docs.python.org/3/library/functions.html#float &quot;(in Python v3.12)&quot;){.reference .external}[[,]{.pre}]{.p}[ ]{.w}[[int]{.pre}](https://docs.python.org/3/library/functions.html#int &quot;(in Python v3.12)&quot;){.reference .external}[[,]{.pre}]{.p}[ ]{.w}[[str]{.pre}](https://docs.python.org/3/library/stdtypes.html#str &quot;(in Python v3.12)&quot;){.reference .external}[[\]]{.pre}]{.p}[[\]]{.pre}]{.p}]{.n}*, *[[default]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[[[Any]{.pre}](https://docs.python.org/3/library/typing.html#typing.Any &quot;(in Python v3.12)&quot;){.reference .external}]{.n}[ ]{.w}[[=]{.pre}]{.o}[ ]{.w}[[None]{.pre}]{.default_value}*[)]{.sig-paren} [[→]{.sig-return-icon} [[[Any]{.pre}](https://docs.python.org/3/library/typing.html#typing.Any &quot;(in Python v3.12)&quot;){.reference .external}]{.sig-return-typehint}]{.sig-return}[[[\[source\]]{.pre}]{.viewcode-link}](_modules/scrapy/settings.html#BaseSettings.get){.reference .internal}[¶](#scrapy.settings.BaseSettings.get &quot;Permalink to this definition&quot;){.headerlink}

:   Get a setting value without affecting its original type.

    Parameters

    :   -   **name**
            ([*str*](https://docs.python.org/3/library/stdtypes.html#str &quot;(in Python v3.12)&quot;){.reference
            .external}) -- the setting name

        -   **default**
            ([*object*](https://docs.python.org/3/library/functions.html#object &quot;(in Python v3.12)&quot;){.reference
            .external}) -- the value to return if no setting is
            found

[[getbool]{.pre}]{.sig-name .descname}[(]{.sig-paren}*[[name]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[[[Optional]{.pre}](https://docs.python.org/3/library/typing.html#typing.Optional &quot;(in Python v3.12)&quot;){.reference .external}[[\[]{.pre}]{.p}[[Union]{.pre}](https://docs.python.org/3/library/typing.html#typing.Union &quot;(in Python v3.12)&quot;){.reference .external}[[\[]{.pre}]{.p}[[bool]{.pre}](https://docs.python.org/3/library/functions.html#bool &quot;(in Python v3.12)&quot;){.reference .external}[[,]{.pre}]{.p}[ ]{.w}[[float]{.pre}](https://docs.python.org/3/library/functions.html#float &quot;(in Python v3.12)&quot;){.reference .external}[[,]{.pre}]{.p}[ ]{.w}[[int]{.pre}](https://docs.python.org/3/library/functions.html#int &quot;(in Python v3.12)&quot;){.reference .external}[[,]{.pre}]{.p}[ ]{.w}[[str]{.pre}](https://docs.python.org/3/library/stdtypes.html#str &quot;(in Python v3.12)&quot;){.reference .external}[[\]]{.pre}]{.p}[[\]]{.pre}]{.p}]{.n}*, *[[default]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[[[bool]{.pre}](https://docs.python.org/3/library/functions.html#bool &quot;(in Python v3.12)&quot;){.reference .external}]{.n}[ ]{.w}[[=]{.pre}]{.o}[ ]{.w}[[False]{.pre}]{.default_value}*[)]{.sig-paren} [[→]{.sig-return-icon} [[[bool]{.pre}](https://docs.python.org/3/library/functions.html#bool &quot;(in Python v3.12)&quot;){.reference .external}]{.sig-return-typehint}]{.sig-return}[[[\[source\]]{.pre}]{.viewcode-link}](_modules/scrapy/settings.html#BaseSettings.getbool){.reference .internal}[¶](#scrapy.settings.BaseSettings.getbool &quot;Permalink to this definition&quot;){.headerlink}

:   Get a setting value as a boolean.

    [`1`{.docutils .literal .notranslate}]{.pre}, [`'1'`{.docutils
    .literal .notranslate}]{.pre}, True\` and [`'True'`{.docutils
    .literal .notranslate}]{.pre} return [`True`{.docutils .literal
    .notranslate}]{.pre}, while [`0`{.docutils .literal
    .notranslate}]{.pre}, [`'0'`{.docutils .literal
    .notranslate}]{.pre}, [`False`{.docutils .literal
    .notranslate}]{.pre}, [`'False'`{.docutils .literal
    .notranslate}]{.pre} and [`None`{.docutils .literal
    .notranslate}]{.pre} return [`False`{.docutils .literal
    .notranslate}]{.pre}.

    For example, settings populated through environment variables
    set to [`'0'`{.docutils .literal .notranslate}]{.pre} will
    return [`False`{.docutils .literal .notranslate}]{.pre} when
    using this method.

    Parameters

    :   -   **name**
            ([*str*](https://docs.python.org/3/library/stdtypes.html#str &quot;(in Python v3.12)&quot;){.reference
            .external}) -- the setting name

        -   **default**
            ([*object*](https://docs.python.org/3/library/functions.html#object &quot;(in Python v3.12)&quot;){.reference
            .external}) -- the value to return if no setting is
            found

[[getdict]{.pre}]{.sig-name .descname}[(]{.sig-paren}*[[name]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[[[Optional]{.pre}](https://docs.python.org/3/library/typing.html#typing.Optional &quot;(in Python v3.12)&quot;){.reference .external}[[\[]{.pre}]{.p}[[Union]{.pre}](https://docs.python.org/3/library/typing.html#typing.Union &quot;(in Python v3.12)&quot;){.reference .external}[[\[]{.pre}]{.p}[[bool]{.pre}](https://docs.python.org/3/library/functions.html#bool &quot;(in Python v3.12)&quot;){.reference .external}[[,]{.pre}]{.p}[ ]{.w}[[float]{.pre}](https://docs.python.org/3/library/functions.html#float &quot;(in Python v3.12)&quot;){.reference .external}[[,]{.pre}]{.p}[ ]{.w}[[int]{.pre}](https://docs.python.org/3/library/functions.html#int &quot;(in Python v3.12)&quot;){.reference .external}[[,]{.pre}]{.p}[ ]{.w}[[str]{.pre}](https://docs.python.org/3/library/stdtypes.html#str &quot;(in Python v3.12)&quot;){.reference .external}[[\]]{.pre}]{.p}[[\]]{.pre}]{.p}]{.n}*, *[[default]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[[[Optional]{.pre}](https://docs.python.org/3/library/typing.html#typing.Optional &quot;(in Python v3.12)&quot;){.reference .external}[[\[]{.pre}]{.p}[[Dict]{.pre}](https://docs.python.org/3/library/typing.html#typing.Dict &quot;(in Python v3.12)&quot;){.reference .external}[[\[]{.pre}]{.p}[[Any]{.pre}](https://docs.python.org/3/library/typing.html#typing.Any &quot;(in Python v3.12)&quot;){.reference .external}[[,]{.pre}]{.p}[ ]{.w}[[Any]{.pre}](https://docs.python.org/3/library/typing.html#typing.Any &quot;(in Python v3.12)&quot;){.reference .external}[[\]]{.pre}]{.p}[[\]]{.pre}]{.p}]{.n}[ ]{.w}[[=]{.pre}]{.o}[ ]{.w}[[None]{.pre}]{.default_value}*[)]{.sig-paren} [[→]{.sig-return-icon} [[[Dict]{.pre}](https://docs.python.org/3/library/typing.html#typing.Dict &quot;(in Python v3.12)&quot;){.reference .external}[[\[]{.pre}]{.p}[[Any]{.pre}](https://docs.python.org/3/library/typing.html#typing.Any &quot;(in Python v3.12)&quot;){.reference .external}[[,]{.pre}]{.p}[ ]{.w}[[Any]{.pre}](https://docs.python.org/3/library/typing.html#typing.Any &quot;(in Python v3.12)&quot;){.reference .external}[[\]]{.pre}]{.p}]{.sig-return-typehint}]{.sig-return}[[[\[source\]]{.pre}]{.viewcode-link}](_modules/scrapy/settings.html#BaseSettings.getdict){.reference .internal}[¶](#scrapy.settings.BaseSettings.getdict &quot;Permalink to this definition&quot;){.headerlink}

:   Get a setting value as a dictionary. If the setting original
    type is a dictionary, a copy of it will be returned. If it is a
    string it will be evaluated as a JSON dictionary. In the case
    that it is a [[`BaseSettings`{.xref .py .py-class .docutils
    .literal
    .notranslate}]{.pre}](#scrapy.settings.BaseSettings &quot;scrapy.settings.BaseSettings&quot;){.reference
    .internal} instance itself, it will be converted to a
    dictionary, containing all its current settings values as they
    would be returned by [[`get()`{.xref .py .py-meth .docutils
    .literal
    .notranslate}]{.pre}](#scrapy.settings.BaseSettings.get &quot;scrapy.settings.BaseSettings.get&quot;){.reference
    .internal}, and losing all information about priority and
    mutability.

    Parameters

    :   -   **name**
            ([*str*](https://docs.python.org/3/library/stdtypes.html#str &quot;(in Python v3.12)&quot;){.reference
            .external}) -- the setting name

        -   **default**
            ([*object*](https://docs.python.org/3/library/functions.html#object &quot;(in Python v3.12)&quot;){.reference
            .external}) -- the value to return if no setting is
            found

[[getdictorlist]{.pre}]{.sig-name .descname}[(]{.sig-paren}*[[name]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[[[Optional]{.pre}](https://docs.python.org/3/library/typing.html#typing.Optional &quot;(in Python v3.12)&quot;){.reference .external}[[\[]{.pre}]{.p}[[Union]{.pre}](https://docs.python.org/3/library/typing.html#typing.Union &quot;(in Python v3.12)&quot;){.reference .external}[[\[]{.pre}]{.p}[[bool]{.pre}](https://docs.python.org/3/library/functions.html#bool &quot;(in Python v3.12)&quot;){.reference .external}[[,]{.pre}]{.p}[ ]{.w}[[float]{.pre}](https://docs.python.org/3/library/functions.html#float &quot;(in Python v3.12)&quot;){.reference .external}[[,]{.pre}]{.p}[ ]{.w}[[int]{.pre}](https://docs.python.org/3/library/functions.html#int &quot;(in Python v3.12)&quot;){.reference .external}[[,]{.pre}]{.p}[ ]{.w}[[str]{.pre}](https://docs.python.org/3/library/stdtypes.html#str &quot;(in Python v3.12)&quot;){.reference .external}[[\]]{.pre}]{.p}[[\]]{.pre}]{.p}]{.n}*, *[[default]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[[[Optional]{.pre}](https://docs.python.org/3/library/typing.html#typing.Optional &quot;(in Python v3.12)&quot;){.reference .external}[[\[]{.pre}]{.p}[[Union]{.pre}](https://docs.python.org/3/library/typing.html#typing.Union &quot;(in Python v3.12)&quot;){.reference .external}[[\[]{.pre}]{.p}[[Dict]{.pre}](https://docs.python.org/3/library/typing.html#typing.Dict &quot;(in Python v3.12)&quot;){.reference .external}[[\[]{.pre}]{.p}[[Any]{.pre}](https://docs.python.org/3/library/typing.html#typing.Any &quot;(in Python v3.12)&quot;){.reference .external}[[,]{.pre}]{.p}[ ]{.w}[[Any]{.pre}](https://docs.python.org/3/library/typing.html#typing.Any &quot;(in Python v3.12)&quot;){.reference .external}[[\]]{.pre}]{.p}[[,]{.pre}]{.p}[ ]{.w}[[List]{.pre}](https://docs.python.org/3/library/typing.html#typing.List &quot;(in Python v3.12)&quot;){.reference .external}[[\[]{.pre}]{.p}[[Any]{.pre}](https://docs.python.org/3/library/typing.html#typing.Any &quot;(in Python v3.12)&quot;){.reference .external}[[\]]{.pre}]{.p}[[,]{.pre}]{.p}[ ]{.w}[[Tuple]{.pre}](https://docs.python.org/3/library/typing.html#typing.Tuple &quot;(in Python v3.12)&quot;){.reference .external}[[\[]{.pre}]{.p}[[Any]{.pre}](https://docs.python.org/3/library/typing.html#typing.Any &quot;(in Python v3.12)&quot;){.reference .external}[[\]]{.pre}]{.p}[[\]]{.pre}]{.p}[[\]]{.pre}]{.p}]{.n}[ ]{.w}[[=]{.pre}]{.o}[ ]{.w}[[None]{.pre}]{.default_value}*[)]{.sig-paren} [[→]{.sig-return-icon} [[[Union]{.pre}](https://docs.python.org/3/library/typing.html#typing.Union &quot;(in Python v3.12)&quot;){.reference .external}[[\[]{.pre}]{.p}[[Dict]{.pre}](https://docs.python.org/3/library/typing.html#typing.Dict &quot;(in Python v3.12)&quot;){.reference .external}[[\[]{.pre}]{.p}[[Any]{.pre}](https://docs.python.org/3/library/typing.html#typing.Any &quot;(in Python v3.12)&quot;){.reference .external}[[,]{.pre}]{.p}[ ]{.w}[[Any]{.pre}](https://docs.python.org/3/library/typing.html#typing.Any &quot;(in Python v3.12)&quot;){.reference .external}[[\]]{.pre}]{.p}[[,]{.pre}]{.p}[ ]{.w}[[List]{.pre}](https://docs.python.org/3/library/typing.html#typing.List &quot;(in Python v3.12)&quot;){.reference .external}[[\[]{.pre}]{.p}[[Any]{.pre}](https://docs.python.org/3/library/typing.html#typing.Any &quot;(in Python v3.12)&quot;){.reference .external}[[\]]{.pre}]{.p}[[\]]{.pre}]{.p}]{.sig-return-typehint}]{.sig-return}[[[\[source\]]{.pre}]{.viewcode-link}](_modules/scrapy/settings.html#BaseSettings.getdictorlist){.reference .internal}[¶](#scrapy.settings.BaseSettings.getdictorlist &quot;Permalink to this definition&quot;){.headerlink}

:   Get a setting value as either a [[`dict`{.xref .py .py-class
    .docutils .literal
    .notranslate}]{.pre}](https://docs.python.org/3/library/stdtypes.html#dict &quot;(in Python v3.12)&quot;){.reference
    .external} or a [[`list`{.xref .py .py-class .docutils .literal
    .notranslate}]{.pre}](https://docs.python.org/3/library/stdtypes.html#list &quot;(in Python v3.12)&quot;){.reference
    .external}.

    If the setting is already a dict or a list, a copy of it will be
    returned.

    If it is a string it will be evaluated as JSON, or as a
    comma-separated list of strings as a fallback.

    For example, settings populated from the command line will
    return:

    -   [`{'key1':`{.docutils .literal
        .notranslate}]{.pre}` `{.docutils .literal
        .notranslate}[`'value1',`{.docutils .literal
        .notranslate}]{.pre}` `{.docutils .literal
        .notranslate}[`'key2':`{.docutils .literal
        .notranslate}]{.pre}` `{.docutils .literal
        .notranslate}[`'value2'}`{.docutils .literal
        .notranslate}]{.pre} if set to [`'{&quot;key1&quot;:`{.docutils
        .literal .notranslate}]{.pre}` `{.docutils .literal
        .notranslate}[`&quot;value1&quot;,`{.docutils .literal
        .notranslate}]{.pre}` `{.docutils .literal
        .notranslate}[`&quot;key2&quot;:`{.docutils .literal
        .notranslate}]{.pre}` `{.docutils .literal
        .notranslate}[`&quot;value2&quot;}'`{.docutils .literal
        .notranslate}]{.pre}

    -   [`['one',`{.docutils .literal
        .notranslate}]{.pre}` `{.docutils .literal
        .notranslate}[`'two']`{.docutils .literal
        .notranslate}]{.pre} if set to [`'[&quot;one&quot;,`{.docutils
        .literal .notranslate}]{.pre}` `{.docutils .literal
        .notranslate}[`&quot;two&quot;]'`{.docutils .literal
        .notranslate}]{.pre} or [`'one,two'`{.docutils .literal
        .notranslate}]{.pre}

    Parameters

    :   -   **name** (*string*) -- the setting name

        -   **default** (*any*) -- the value to return if no setting
            is found

[[getfloat]{.pre}]{.sig-name .descname}[(]{.sig-paren}*[[name]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[[[Optional]{.pre}](https://docs.python.org/3/library/typing.html#typing.Optional &quot;(in Python v3.12)&quot;){.reference .external}[[\[]{.pre}]{.p}[[Union]{.pre}](https://docs.python.org/3/library/typing.html#typing.Union &quot;(in Python v3.12)&quot;){.reference .external}[[\[]{.pre}]{.p}[[bool]{.pre}](https://docs.python.org/3/library/functions.html#bool &quot;(in Python v3.12)&quot;){.reference .external}[[,]{.pre}]{.p}[ ]{.w}[[float]{.pre}](https://docs.python.org/3/library/functions.html#float &quot;(in Python v3.12)&quot;){.reference .external}[[,]{.pre}]{.p}[ ]{.w}[[int]{.pre}](https://docs.python.org/3/library/functions.html#int &quot;(in Python v3.12)&quot;){.reference .external}[[,]{.pre}]{.p}[ ]{.w}[[str]{.pre}](https://docs.python.org/3/library/stdtypes.html#str &quot;(in Python v3.12)&quot;){.reference .external}[[\]]{.pre}]{.p}[[\]]{.pre}]{.p}]{.n}*, *[[default]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[[[float]{.pre}](https://docs.python.org/3/library/functions.html#float &quot;(in Python v3.12)&quot;){.reference .external}]{.n}[ ]{.w}[[=]{.pre}]{.o}[ ]{.w}[[0.0]{.pre}]{.default_value}*[)]{.sig-paren} [[→]{.sig-return-icon} [[[float]{.pre}](https://docs.python.org/3/library/functions.html#float &quot;(in Python v3.12)&quot;){.reference .external}]{.sig-return-typehint}]{.sig-return}[[[\[source\]]{.pre}]{.viewcode-link}](_modules/scrapy/settings.html#BaseSettings.getfloat){.reference .internal}[¶](#scrapy.settings.BaseSettings.getfloat &quot;Permalink to this definition&quot;){.headerlink}

:   Get a setting value as a float.

    Parameters

    :   -   **name**
            ([*str*](https://docs.python.org/3/library/stdtypes.html#str &quot;(in Python v3.12)&quot;){.reference
            .external}) -- the setting name

        -   **default**
            ([*object*](https://docs.python.org/3/library/functions.html#object &quot;(in Python v3.12)&quot;){.reference
            .external}) -- the value to return if no setting is
            found

[[getint]{.pre}]{.sig-name .descname}[(]{.sig-paren}*[[name]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[[[Optional]{.pre}](https://docs.python.org/3/library/typing.html#typing.Optional &quot;(in Python v3.12)&quot;){.reference .external}[[\[]{.pre}]{.p}[[Union]{.pre}](https://docs.python.org/3/library/typing.html#typing.Union &quot;(in Python v3.12)&quot;){.reference .external}[[\[]{.pre}]{.p}[[bool]{.pre}](https://docs.python.org/3/library/functions.html#bool &quot;(in Python v3.12)&quot;){.reference .external}[[,]{.pre}]{.p}[ ]{.w}[[float]{.pre}](https://docs.python.org/3/library/functions.html#float &quot;(in Python v3.12)&quot;){.reference .external}[[,]{.pre}]{.p}[ ]{.w}[[int]{.pre}](https://docs.python.org/3/library/functions.html#int &quot;(in Python v3.12)&quot;){.reference .external}[[,]{.pre}]{.p}[ ]{.w}[[str]{.pre}](https://docs.python.org/3/library/stdtypes.html#str &quot;(in Python v3.12)&quot;){.reference .external}[[\]]{.pre}]{.p}[[\]]{.pre}]{.p}]{.n}*, *[[default]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[[[int]{.pre}](https://docs.python.org/3/library/functions.html#int &quot;(in Python v3.12)&quot;){.reference .external}]{.n}[ ]{.w}[[=]{.pre}]{.o}[ ]{.w}[[0]{.pre}]{.default_value}*[)]{.sig-paren} [[→]{.sig-return-icon} [[[int]{.pre}](https://docs.python.org/3/library/functions.html#int &quot;(in Python v3.12)&quot;){.reference .external}]{.sig-return-typehint}]{.sig-return}[[[\[source\]]{.pre}]{.viewcode-link}](_modules/scrapy/settings.html#BaseSettings.getint){.reference .internal}[¶](#scrapy.settings.BaseSettings.getint &quot;Permalink to this definition&quot;){.headerlink}

:   Get a setting value as an int.

    Parameters

    :   -   **name**
            ([*str*](https://docs.python.org/3/library/stdtypes.html#str &quot;(in Python v3.12)&quot;){.reference
            .external}) -- the setting name

        -   **default**
            ([*object*](https://docs.python.org/3/library/functions.html#object &quot;(in Python v3.12)&quot;){.reference
            .external}) -- the value to return if no setting is
            found

[[getlist]{.pre}]{.sig-name .descname}[(]{.sig-paren}*[[name]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[[[Optional]{.pre}](https://docs.python.org/3/library/typing.html#typing.Optional &quot;(in Python v3.12)&quot;){.reference .external}[[\[]{.pre}]{.p}[[Union]{.pre}](https://docs.python.org/3/library/typing.html#typing.Union &quot;(in Python v3.12)&quot;){.reference .external}[[\[]{.pre}]{.p}[[bool]{.pre}](https://docs.python.org/3/library/functions.html#bool &quot;(in Python v3.12)&quot;){.reference .external}[[,]{.pre}]{.p}[ ]{.w}[[float]{.pre}](https://docs.python.org/3/library/functions.html#float &quot;(in Python v3.12)&quot;){.reference .external}[[,]{.pre}]{.p}[ ]{.w}[[int]{.pre}](https://docs.python.org/3/library/functions.html#int &quot;(in Python v3.12)&quot;){.reference .external}[[,]{.pre}]{.p}[ ]{.w}[[str]{.pre}](https://docs.python.org/3/library/stdtypes.html#str &quot;(in Python v3.12)&quot;){.reference .external}[[\]]{.pre}]{.p}[[\]]{.pre}]{.p}]{.n}*, *[[default]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[[[Optional]{.pre}](https://docs.python.org/3/library/typing.html#typing.Optional &quot;(in Python v3.12)&quot;){.reference .external}[[\[]{.pre}]{.p}[[List]{.pre}](https://docs.python.org/3/library/typing.html#typing.List &quot;(in Python v3.12)&quot;){.reference .external}[[\[]{.pre}]{.p}[[Any]{.pre}](https://docs.python.org/3/library/typing.html#typing.Any &quot;(in Python v3.12)&quot;){.reference .external}[[\]]{.pre}]{.p}[[\]]{.pre}]{.p}]{.n}[ ]{.w}[[=]{.pre}]{.o}[ ]{.w}[[None]{.pre}]{.default_value}*[)]{.sig-paren} [[→]{.sig-return-icon} [[[List]{.pre}](https://docs.python.org/3/library/typing.html#typing.List &quot;(in Python v3.12)&quot;){.reference .external}[[\[]{.pre}]{.p}[[Any]{.pre}](https://docs.python.org/3/library/typing.html#typing.Any &quot;(in Python v3.12)&quot;){.reference .external}[[\]]{.pre}]{.p}]{.sig-return-typehint}]{.sig-return}[[[\[source\]]{.pre}]{.viewcode-link}](_modules/scrapy/settings.html#BaseSettings.getlist){.reference .internal}[¶](#scrapy.settings.BaseSettings.getlist &quot;Permalink to this definition&quot;){.headerlink}

:   Get a setting value as a list. If the setting original type is a
    list, a copy of it will be returned. If it's a string it will be
    split by &quot;,&quot;.

    For example, settings populated through environment variables
    set to [`'one,two'`{.docutils .literal .notranslate}]{.pre} will
    return a list \['one', 'two'\] when using this method.

    Parameters

    :   -   **name**
            ([*str*](https://docs.python.org/3/library/stdtypes.html#str &quot;(in Python v3.12)&quot;){.reference
            .external}) -- the setting name

        -   **default**
            ([*object*](https://docs.python.org/3/library/functions.html#object &quot;(in Python v3.12)&quot;){.reference
            .external}) -- the value to return if no setting is
            found

[[getpriority]{.pre}]{.sig-name .descname}[(]{.sig-paren}*[[name]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[[[Optional]{.pre}](https://docs.python.org/3/library/typing.html#typing.Optional &quot;(in Python v3.12)&quot;){.reference .external}[[\[]{.pre}]{.p}[[Union]{.pre}](https://docs.python.org/3/library/typing.html#typing.Union &quot;(in Python v3.12)&quot;){.reference .external}[[\[]{.pre}]{.p}[[bool]{.pre}](https://docs.python.org/3/library/functions.html#bool &quot;(in Python v3.12)&quot;){.reference .external}[[,]{.pre}]{.p}[ ]{.w}[[float]{.pre}](https://docs.python.org/3/library/functions.html#float &quot;(in Python v3.12)&quot;){.reference .external}[[,]{.pre}]{.p}[ ]{.w}[[int]{.pre}](https://docs.python.org/3/library/functions.html#int &quot;(in Python v3.12)&quot;){.reference .external}[[,]{.pre}]{.p}[ ]{.w}[[str]{.pre}](https://docs.python.org/3/library/stdtypes.html#str &quot;(in Python v3.12)&quot;){.reference .external}[[\]]{.pre}]{.p}[[\]]{.pre}]{.p}]{.n}*[)]{.sig-paren} [[→]{.sig-return-icon} [[[Optional]{.pre}](https://docs.python.org/3/library/typing.html#typing.Optional &quot;(in Python v3.12)&quot;){.reference .external}[[\[]{.pre}]{.p}[[int]{.pre}](https://docs.python.org/3/library/functions.html#int &quot;(in Python v3.12)&quot;){.reference .external}[[\]]{.pre}]{.p}]{.sig-return-typehint}]{.sig-return}[[[\[source\]]{.pre}]{.viewcode-link}](_modules/scrapy/settings.html#BaseSettings.getpriority){.reference .internal}[¶](#scrapy.settings.BaseSettings.getpriority &quot;Permalink to this definition&quot;){.headerlink}

:   Return the current numerical priority value of a setting, or
    [`None`{.docutils .literal .notranslate}]{.pre} if the given
    [`name`{.docutils .literal .notranslate}]{.pre} does not exist.

    Parameters

    :   **name**
        ([*str*](https://docs.python.org/3/library/stdtypes.html#str &quot;(in Python v3.12)&quot;){.reference
        .external}) -- the setting name

[[getwithbase]{.pre}]{.sig-name .descname}[(]{.sig-paren}*[[name]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[[[Optional]{.pre}](https://docs.python.org/3/library/typing.html#typing.Optional &quot;(in Python v3.12)&quot;){.reference .external}[[\[]{.pre}]{.p}[[Union]{.pre}](https://docs.python.org/3/library/typing.html#typing.Union &quot;(in Python v3.12)&quot;){.reference .external}[[\[]{.pre}]{.p}[[bool]{.pre}](https://docs.python.org/3/library/functions.html#bool &quot;(in Python v3.12)&quot;){.reference .external}[[,]{.pre}]{.p}[ ]{.w}[[float]{.pre}](https://docs.python.org/3/library/functions.html#float &quot;(in Python v3.12)&quot;){.reference .external}[[,]{.pre}]{.p}[ ]{.w}[[int]{.pre}](https://docs.python.org/3/library/functions.html#int &quot;(in Python v3.12)&quot;){.reference .external}[[,]{.pre}]{.p}[ ]{.w}[[str]{.pre}](https://docs.python.org/3/library/stdtypes.html#str &quot;(in Python v3.12)&quot;){.reference .external}[[\]]{.pre}]{.p}[[\]]{.pre}]{.p}]{.n}*[)]{.sig-paren} [[→]{.sig-return-icon} [[[BaseSettings]{.pre}](index.html#scrapy.settings.BaseSettings &quot;scrapy.settings.BaseSettings&quot;){.reference .internal}]{.sig-return-typehint}]{.sig-return}[[[\[source\]]{.pre}]{.viewcode-link}](_modules/scrapy/settings.html#BaseSettings.getwithbase){.reference .internal}[¶](#scrapy.settings.BaseSettings.getwithbase &quot;Permalink to this definition&quot;){.headerlink}

:   Get a composition of a dictionary-like setting and its \_BASE
    counterpart.

    Parameters

    :   **name**
        ([*str*](https://docs.python.org/3/library/stdtypes.html#str &quot;(in Python v3.12)&quot;){.reference
        .external}) -- name of the dictionary-like setting

[[maxpriority]{.pre}]{.sig-name .descname}[(]{.sig-paren}[)]{.sig-paren} [[→]{.sig-return-icon} [[[int]{.pre}](https://docs.python.org/3/library/functions.html#int &quot;(in Python v3.12)&quot;){.reference .external}]{.sig-return-typehint}]{.sig-return}[[[\[source\]]{.pre}]{.viewcode-link}](_modules/scrapy/settings.html#BaseSettings.maxpriority){.reference .internal}[¶](#scrapy.settings.BaseSettings.maxpriority &quot;Permalink to this definition&quot;){.headerlink}

:   Return the numerical value of the highest priority present
    throughout all settings, or the numerical value for
    [`default`{.docutils .literal .notranslate}]{.pre} from
    [[`SETTINGS_PRIORITIES`{.xref .py .py-attr .docutils .literal
    .notranslate}]{.pre}](#scrapy.settings.SETTINGS_PRIORITIES &quot;scrapy.settings.SETTINGS_PRIORITIES&quot;){.reference
    .internal} if there are no settings stored.

[[pop]{.pre}]{.sig-name .descname}[(]{.sig-paren}*[[k]{.pre}]{.n}*[\[]{.optional}, *[[d]{.pre}]{.n}*[\]]{.optional}[)]{.sig-paren} [[→]{.sig-return-icon} [[v,]{.pre} [remove]{.pre} [specified]{.pre} [key]{.pre} [and]{.pre} [return]{.pre} [the]{.pre} [corresponding]{.pre} [value.]{.pre}]{.sig-return-typehint}]{.sig-return}[[[\[source\]]{.pre}]{.viewcode-link}](_modules/scrapy/settings.html#BaseSettings.pop){.reference .internal}[¶](#scrapy.settings.BaseSettings.pop &quot;Permalink to this definition&quot;){.headerlink}

:   If key is not found, d is returned if given, otherwise KeyError
    is raised.

[[set]{.pre}]{.sig-name .descname}[(]{.sig-paren}*[[name]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[[[Optional]{.pre}](https://docs.python.org/3/library/typing.html#typing.Optional &quot;(in Python v3.12)&quot;){.reference .external}[[\[]{.pre}]{.p}[[Union]{.pre}](https://docs.python.org/3/library/typing.html#typing.Union &quot;(in Python v3.12)&quot;){.reference .external}[[\[]{.pre}]{.p}[[bool]{.pre}](https://docs.python.org/3/library/functions.html#bool &quot;(in Python v3.12)&quot;){.reference .external}[[,]{.pre}]{.p}[ ]{.w}[[float]{.pre}](https://docs.python.org/3/library/functions.html#float &quot;(in Python v3.12)&quot;){.reference .external}[[,]{.pre}]{.p}[ ]{.w}[[int]{.pre}](https://docs.python.org/3/library/functions.html#int &quot;(in Python v3.12)&quot;){.reference .external}[[,]{.pre}]{.p}[ ]{.w}[[str]{.pre}](https://docs.python.org/3/library/stdtypes.html#str &quot;(in Python v3.12)&quot;){.reference .external}[[\]]{.pre}]{.p}[[\]]{.pre}]{.p}]{.n}*, *[[value]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[[[Any]{.pre}](https://docs.python.org/3/library/typing.html#typing.Any &quot;(in Python v3.12)&quot;){.reference .external}]{.n}*, *[[priority]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[[[Union]{.pre}](https://docs.python.org/3/library/typing.html#typing.Union &quot;(in Python v3.12)&quot;){.reference .external}[[\[]{.pre}]{.p}[[int]{.pre}](https://docs.python.org/3/library/functions.html#int &quot;(in Python v3.12)&quot;){.reference .external}[[,]{.pre}]{.p}[ ]{.w}[[str]{.pre}](https://docs.python.org/3/library/stdtypes.html#str &quot;(in Python v3.12)&quot;){.reference .external}[[\]]{.pre}]{.p}]{.n}[ ]{.w}[[=]{.pre}]{.o}[ ]{.w}[[\'project\']{.pre}]{.default_value}*[)]{.sig-paren} [[→]{.sig-return-icon} [[[None]{.pre}](https://docs.python.org/3/library/constants.html#None &quot;(in Python v3.12)&quot;){.reference .external}]{.sig-return-typehint}]{.sig-return}[[[\[source\]]{.pre}]{.viewcode-link}](_modules/scrapy/settings.html#BaseSettings.set){.reference .internal}[¶](#scrapy.settings.BaseSettings.set &quot;Permalink to this definition&quot;){.headerlink}

:   Store a key/value attribute with a given priority.

    Settings should be populated *before* configuring the Crawler
    object (through the [`configure()`{.xref .py .py-meth .docutils
    .literal .notranslate}]{.pre} method), otherwise they won't have
    any effect.

    Parameters

    :   -   **name**
            ([*str*](https://docs.python.org/3/library/stdtypes.html#str &quot;(in Python v3.12)&quot;){.reference
            .external}) -- the setting name

        -   **value**
            ([*object*](https://docs.python.org/3/library/functions.html#object &quot;(in Python v3.12)&quot;){.reference
            .external}) -- the value to associate with the setting

        -   **priority**
            ([*str*](https://docs.python.org/3/library/stdtypes.html#str &quot;(in Python v3.12)&quot;){.reference
            .external} *or*
            [*int*](https://docs.python.org/3/library/functions.html#int &quot;(in Python v3.12)&quot;){.reference
            .external}) -- the priority of the setting. Should be a
            key of [[`SETTINGS_PRIORITIES`{.xref .py .py-attr
            .docutils .literal
            .notranslate}]{.pre}](#scrapy.settings.SETTINGS_PRIORITIES &quot;scrapy.settings.SETTINGS_PRIORITIES&quot;){.reference
            .internal} or an integer

[[setdefault]{.pre}]{.sig-name .descname}[(]{.sig-paren}*[[k]{.pre}]{.n}*[\[]{.optional}, *[[d]{.pre}]{.n}*[\]]{.optional}[)]{.sig-paren} [[→]{.sig-return-icon} [[D.get(k,d),]{.pre} [also]{.pre} [set]{.pre} [D\[k\]=d]{.pre} [if]{.pre} [k]{.pre} [not]{.pre} [in]{.pre} [D]{.pre}]{.sig-return-typehint}]{.sig-return}[[[\[source\]]{.pre}]{.viewcode-link}](_modules/scrapy/settings.html#BaseSettings.setdefault){.reference .internal}[¶](#scrapy.settings.BaseSettings.setdefault &quot;Permalink to this definition&quot;){.headerlink}

:   

[[setmodule]{.pre}]{.sig-name .descname}[(]{.sig-paren}*[[module]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[[[Union]{.pre}](https://docs.python.org/3/library/typing.html#typing.Union &quot;(in Python v3.12)&quot;){.reference .external}[[\[]{.pre}]{.p}[module]{.pre}[[,]{.pre}]{.p}[ ]{.w}[[str]{.pre}](https://docs.python.org/3/library/stdtypes.html#str &quot;(in Python v3.12)&quot;){.reference .external}[[\]]{.pre}]{.p}]{.n}*, *[[priority]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[[[Union]{.pre}](https://docs.python.org/3/library/typing.html#typing.Union &quot;(in Python v3.12)&quot;){.reference .external}[[\[]{.pre}]{.p}[[int]{.pre}](https://docs.python.org/3/library/functions.html#int &quot;(in Python v3.12)&quot;){.reference .external}[[,]{.pre}]{.p}[ ]{.w}[[str]{.pre}](https://docs.python.org/3/library/stdtypes.html#str &quot;(in Python v3.12)&quot;){.reference .external}[[\]]{.pre}]{.p}]{.n}[ ]{.w}[[=]{.pre}]{.o}[ ]{.w}[[\'project\']{.pre}]{.default_value}*[)]{.sig-paren} [[→]{.sig-return-icon} [[[None]{.pre}](https://docs.python.org/3/library/constants.html#None &quot;(in Python v3.12)&quot;){.reference .external}]{.sig-return-typehint}]{.sig-return}[[[\[source\]]{.pre}]{.viewcode-link}](_modules/scrapy/settings.html#BaseSettings.setmodule){.reference .internal}[¶](#scrapy.settings.BaseSettings.setmodule &quot;Permalink to this definition&quot;){.headerlink}

:   Store settings from a module with a given priority.

    This is a helper function that calls [[`set()`{.xref .py
    .py-meth .docutils .literal
    .notranslate}]{.pre}](#scrapy.settings.BaseSettings.set &quot;scrapy.settings.BaseSettings.set&quot;){.reference
    .internal} for every globally declared uppercase variable of
    [`module`{.docutils .literal .notranslate}]{.pre} with the
    provided [`priority`{.docutils .literal .notranslate}]{.pre}.

    Parameters

    :   -   **module**
            ([*types.ModuleType*](https://docs.python.org/3/library/types.html#types.ModuleType &quot;(in Python v3.12)&quot;){.reference
            .external} *or*
            [*str*](https://docs.python.org/3/library/stdtypes.html#str &quot;(in Python v3.12)&quot;){.reference
            .external}) -- the module or the path of the module

        -   **priority**
            ([*str*](https://docs.python.org/3/library/stdtypes.html#str &quot;(in Python v3.12)&quot;){.reference
            .external} *or*
            [*int*](https://docs.python.org/3/library/functions.html#int &quot;(in Python v3.12)&quot;){.reference
            .external}) -- the priority of the settings. Should be a
            key of [[`SETTINGS_PRIORITIES`{.xref .py .py-attr
            .docutils .literal
            .notranslate}]{.pre}](#scrapy.settings.SETTINGS_PRIORITIES &quot;scrapy.settings.SETTINGS_PRIORITIES&quot;){.reference
            .internal} or an integer

[[update]{.pre}]{.sig-name .descname}[(]{.sig-paren}*[[values]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[[\_SettingsInputT]{.pre}]{.n}*, *[[priority]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[[Union]{.pre}[[\[]{.pre}]{.p}[[int]{.pre}](https://docs.python.org/3/library/functions.html#int &quot;(in Python v3.12)&quot;){.reference .external}[[,]{.pre}]{.p}[ ]{.w}[[str]{.pre}](https://docs.python.org/3/library/stdtypes.html#str &quot;(in Python v3.12)&quot;){.reference .external}[[\]]{.pre}]{.p}]{.n}[ ]{.w}[[=]{.pre}]{.o}[ ]{.w}[[\'project\']{.pre}]{.default_value}*[)]{.sig-paren} [[→]{.sig-return-icon} [[[None]{.pre}](https://docs.python.org/3/library/constants.html#None &quot;(in Python v3.12)&quot;){.reference .external}]{.sig-return-typehint}]{.sig-return}[[[\[source\]]{.pre}]{.viewcode-link}](_modules/scrapy/settings.html#BaseSettings.update){.reference .internal}[¶](#scrapy.settings.BaseSettings.update &quot;Permalink to this definition&quot;){.headerlink}

:   Store key/value pairs with a given priority.

    This is a helper function that calls [[`set()`{.xref .py
    .py-meth .docutils .literal
    .notranslate}]{.pre}](#scrapy.settings.BaseSettings.set &quot;scrapy.settings.BaseSettings.set&quot;){.reference
    .internal} for every item of [`values`{.docutils .literal
    .notranslate}]{.pre} with the provided [`priority`{.docutils
    .literal .notranslate}]{.pre}.

    If [`values`{.docutils .literal .notranslate}]{.pre} is a
    string, it is assumed to be JSON-encoded and parsed into a dict
    with [`json.loads()`{.docutils .literal .notranslate}]{.pre}
    first. If it is a [[`BaseSettings`{.xref .py .py-class .docutils
    .literal
    .notranslate}]{.pre}](#scrapy.settings.BaseSettings &quot;scrapy.settings.BaseSettings&quot;){.reference
    .internal} instance, the per-key priorities will be used and the
    [`priority`{.docutils .literal .notranslate}]{.pre} parameter
    ignored. This allows inserting/updating settings with different
    priorities with a single command.

    Parameters

    :   -   **values** (dict or string or [[`BaseSettings`{.xref .py
            .py-class .docutils .literal
            .notranslate}]{.pre}](#scrapy.settings.BaseSettings &quot;scrapy.settings.BaseSettings&quot;){.reference
            .internal}) -- the settings names and values

        -   **priority**
            ([*str*](https://docs.python.org/3/library/stdtypes.html#str &quot;(in Python v3.12)&quot;){.reference
            .external} *or*
            [*int*](https://docs.python.org/3/library/functions.html#int &quot;(in Python v3.12)&quot;){.reference
            .external}) -- the priority of the settings. Should be a
            key of [[`SETTINGS_PRIORITIES`{.xref .py .py-attr
            .docutils .literal
            .notranslate}]{.pre}](#scrapy.settings.SETTINGS_PRIORITIES &quot;scrapy.settings.SETTINGS_PRIORITIES&quot;){.reference
            .internal} or an integer
</code></pre>
<p>:::</p>
<p>::: {#module-scrapy.spiderloader .section}
[]{#spiderloader-api}[]{#topics-api-spiderloader}</p>
<h4 id="spiderloader-apiheaderlink"><a class="header" href="#spiderloader-apiheaderlink">SpiderLoader API<a href="#module-scrapy.spiderloader" title="Permalink to this heading">¶</a>{.headerlink}</a></h4>
<p><em>[class]{.pre}[ ]{.w}</em>[[scrapy.spiderloader.]{.pre}]{.sig-prename .descclassname}[[SpiderLoader]{.pre}]{.sig-name .descname}<a href="_modules/scrapy/spiderloader.html#SpiderLoader">[[[source]]{.pre}]{.viewcode-link}</a>{.reference .internal}<a href="#scrapy.spiderloader.SpiderLoader" title="Permalink to this definition">¶</a>{.headerlink}</p>
<p>:   This class is in charge of retrieving and handling the spider
classes defined across the project.</p>
<pre><code>Custom spider loaders can be employed by specifying their path in
the [[`SPIDER_LOADER_CLASS`{.xref .std .std-setting .docutils
.literal
.notranslate}]{.pre}](index.html#std-setting-SPIDER_LOADER_CLASS){.hoverxref
.tooltip .reference .internal} project setting. They must fully
implement the [`scrapy.interfaces.ISpiderLoader`{.xref .py .py-class
.docutils .literal .notranslate}]{.pre} interface to guarantee an
errorless execution.

[[from_settings]{.pre}]{.sig-name .descname}[(]{.sig-paren}*[[settings]{.pre}]{.n}*[)]{.sig-paren}[[[\[source\]]{.pre}]{.viewcode-link}](_modules/scrapy/spiderloader.html#SpiderLoader.from_settings){.reference .internal}[¶](#scrapy.spiderloader.SpiderLoader.from_settings &quot;Permalink to this definition&quot;){.headerlink}

:   This class method is used by Scrapy to create an instance of the
    class. It's called with the current project settings, and it
    loads the spiders found recursively in the modules of the
    [[`SPIDER_MODULES`{.xref .std .std-setting .docutils .literal
    .notranslate}]{.pre}](index.html#std-setting-SPIDER_MODULES){.hoverxref
    .tooltip .reference .internal} setting.

    Parameters

    :   **settings** ([[`Settings`{.xref .py .py-class .docutils
        .literal
        .notranslate}]{.pre}](#scrapy.settings.Settings &quot;scrapy.settings.Settings&quot;){.reference
        .internal} instance) -- project settings

[[load]{.pre}]{.sig-name .descname}[(]{.sig-paren}*[[spider_name]{.pre}]{.n}*[)]{.sig-paren}[[[\[source\]]{.pre}]{.viewcode-link}](_modules/scrapy/spiderloader.html#SpiderLoader.load){.reference .internal}[¶](#scrapy.spiderloader.SpiderLoader.load &quot;Permalink to this definition&quot;){.headerlink}

:   Get the Spider class with the given name. It'll look into the
    previously loaded spiders for a spider class with name
    [`spider_name`{.docutils .literal .notranslate}]{.pre} and will
    raise a KeyError if not found.

    Parameters

    :   **spider_name**
        ([*str*](https://docs.python.org/3/library/stdtypes.html#str &quot;(in Python v3.12)&quot;){.reference
        .external}) -- spider class name

[[list]{.pre}]{.sig-name .descname}[(]{.sig-paren}[)]{.sig-paren}[[[\[source\]]{.pre}]{.viewcode-link}](_modules/scrapy/spiderloader.html#SpiderLoader.list){.reference .internal}[¶](#scrapy.spiderloader.SpiderLoader.list &quot;Permalink to this definition&quot;){.headerlink}

:   Get the names of the available spiders in the project.

[[find_by_request]{.pre}]{.sig-name .descname}[(]{.sig-paren}*[[request]{.pre}]{.n}*[)]{.sig-paren}[[[\[source\]]{.pre}]{.viewcode-link}](_modules/scrapy/spiderloader.html#SpiderLoader.find_by_request){.reference .internal}[¶](#scrapy.spiderloader.SpiderLoader.find_by_request &quot;Permalink to this definition&quot;){.headerlink}

:   List the spiders' names that can handle the given request. Will
    try to match the request's url against the domains of the
    spiders.

    Parameters

    :   **request** ([`Request`{.xref .py .py-class .docutils
        .literal .notranslate}]{.pre} instance) -- queried request
</code></pre>
<p>:::</p>
<p>::: {#module-scrapy.signalmanager .section}
[]{#signals-api}[]{#topics-api-signals}</p>
<h4 id="signals-apiheaderlink"><a class="header" href="#signals-apiheaderlink">Signals API<a href="#module-scrapy.signalmanager" title="Permalink to this heading">¶</a>{.headerlink}</a></h4>
<p><em>[class]{.pre}[ ]{.w}</em>[[scrapy.signalmanager.]{.pre}]{.sig-prename .descclassname}[[SignalManager]{.pre}]{.sig-name .descname}[(]{.sig-paren}<em>[[sender]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[<a href="https://docs.python.org/3/library/typing.html#typing.Any" title="(in Python v3.12)">[Any]{.pre}</a>{.reference .external}]{.n}[ ]{.w}[[=]{.pre}]{.o}[ ]{.w}[[_Anonymous]{.pre}]{.default_value}</em>[)]{.sig-paren}<a href="_modules/scrapy/signalmanager.html#SignalManager">[[[source]]{.pre}]{.viewcode-link}</a>{.reference .internal}<a href="#scrapy.signalmanager.SignalManager" title="Permalink to this definition">¶</a>{.headerlink}</p>
<p>:</p>
<pre><code>[[connect]{.pre}]{.sig-name .descname}[(]{.sig-paren}*[[receiver]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[[[Any]{.pre}](https://docs.python.org/3/library/typing.html#typing.Any &quot;(in Python v3.12)&quot;){.reference .external}]{.n}*, *[[signal]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[[[Any]{.pre}](https://docs.python.org/3/library/typing.html#typing.Any &quot;(in Python v3.12)&quot;){.reference .external}]{.n}*, *[[\*\*]{.pre}]{.o}[[kwargs]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[[[Any]{.pre}](https://docs.python.org/3/library/typing.html#typing.Any &quot;(in Python v3.12)&quot;){.reference .external}]{.n}*[)]{.sig-paren} [[→]{.sig-return-icon} [[[None]{.pre}](https://docs.python.org/3/library/constants.html#None &quot;(in Python v3.12)&quot;){.reference .external}]{.sig-return-typehint}]{.sig-return}[[[\[source\]]{.pre}]{.viewcode-link}](_modules/scrapy/signalmanager.html#SignalManager.connect){.reference .internal}[¶](#scrapy.signalmanager.SignalManager.connect &quot;Permalink to this definition&quot;){.headerlink}

:   Connect a receiver function to a signal.

    The signal can be any object, although Scrapy comes with some
    predefined signals that are documented in the [[Signals]{.std
    .std-ref}](index.html#topics-signals){.hoverxref .tooltip
    .reference .internal} section.

    Parameters

    :   -   **receiver**
            ([*collections.abc.Callable*](https://docs.python.org/3/library/collections.abc.html#collections.abc.Callable &quot;(in Python v3.12)&quot;){.reference
            .external}) -- the function to be connected

        -   **signal**
            ([*object*](https://docs.python.org/3/library/functions.html#object &quot;(in Python v3.12)&quot;){.reference
            .external}) -- the signal to connect to

[[disconnect]{.pre}]{.sig-name .descname}[(]{.sig-paren}*[[receiver]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[[[Any]{.pre}](https://docs.python.org/3/library/typing.html#typing.Any &quot;(in Python v3.12)&quot;){.reference .external}]{.n}*, *[[signal]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[[[Any]{.pre}](https://docs.python.org/3/library/typing.html#typing.Any &quot;(in Python v3.12)&quot;){.reference .external}]{.n}*, *[[\*\*]{.pre}]{.o}[[kwargs]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[[[Any]{.pre}](https://docs.python.org/3/library/typing.html#typing.Any &quot;(in Python v3.12)&quot;){.reference .external}]{.n}*[)]{.sig-paren} [[→]{.sig-return-icon} [[[None]{.pre}](https://docs.python.org/3/library/constants.html#None &quot;(in Python v3.12)&quot;){.reference .external}]{.sig-return-typehint}]{.sig-return}[[[\[source\]]{.pre}]{.viewcode-link}](_modules/scrapy/signalmanager.html#SignalManager.disconnect){.reference .internal}[¶](#scrapy.signalmanager.SignalManager.disconnect &quot;Permalink to this definition&quot;){.headerlink}

:   Disconnect a receiver function from a signal. This has the
    opposite effect of the [[`connect()`{.xref .py .py-meth
    .docutils .literal
    .notranslate}]{.pre}](#scrapy.signalmanager.SignalManager.connect &quot;scrapy.signalmanager.SignalManager.connect&quot;){.reference
    .internal} method, and the arguments are the same.

[[disconnect_all]{.pre}]{.sig-name .descname}[(]{.sig-paren}*[[signal]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[[[Any]{.pre}](https://docs.python.org/3/library/typing.html#typing.Any &quot;(in Python v3.12)&quot;){.reference .external}]{.n}*, *[[\*\*]{.pre}]{.o}[[kwargs]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[[[Any]{.pre}](https://docs.python.org/3/library/typing.html#typing.Any &quot;(in Python v3.12)&quot;){.reference .external}]{.n}*[)]{.sig-paren} [[→]{.sig-return-icon} [[[None]{.pre}](https://docs.python.org/3/library/constants.html#None &quot;(in Python v3.12)&quot;){.reference .external}]{.sig-return-typehint}]{.sig-return}[[[\[source\]]{.pre}]{.viewcode-link}](_modules/scrapy/signalmanager.html#SignalManager.disconnect_all){.reference .internal}[¶](#scrapy.signalmanager.SignalManager.disconnect_all &quot;Permalink to this definition&quot;){.headerlink}

:   Disconnect all receivers from the given signal.

    Parameters

    :   **signal**
        ([*object*](https://docs.python.org/3/library/functions.html#object &quot;(in Python v3.12)&quot;){.reference
        .external}) -- the signal to disconnect from

[[send_catch_log]{.pre}]{.sig-name .descname}[(]{.sig-paren}*[[signal]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[[[Any]{.pre}](https://docs.python.org/3/library/typing.html#typing.Any &quot;(in Python v3.12)&quot;){.reference .external}]{.n}*, *[[\*\*]{.pre}]{.o}[[kwargs]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[[[Any]{.pre}](https://docs.python.org/3/library/typing.html#typing.Any &quot;(in Python v3.12)&quot;){.reference .external}]{.n}*[)]{.sig-paren} [[→]{.sig-return-icon} [[[List]{.pre}](https://docs.python.org/3/library/typing.html#typing.List &quot;(in Python v3.12)&quot;){.reference .external}[[\[]{.pre}]{.p}[[Tuple]{.pre}](https://docs.python.org/3/library/typing.html#typing.Tuple &quot;(in Python v3.12)&quot;){.reference .external}[[\[]{.pre}]{.p}[[Any]{.pre}](https://docs.python.org/3/library/typing.html#typing.Any &quot;(in Python v3.12)&quot;){.reference .external}[[,]{.pre}]{.p}[ ]{.w}[[Any]{.pre}](https://docs.python.org/3/library/typing.html#typing.Any &quot;(in Python v3.12)&quot;){.reference .external}[[\]]{.pre}]{.p}[[\]]{.pre}]{.p}]{.sig-return-typehint}]{.sig-return}[[[\[source\]]{.pre}]{.viewcode-link}](_modules/scrapy/signalmanager.html#SignalManager.send_catch_log){.reference .internal}[¶](#scrapy.signalmanager.SignalManager.send_catch_log &quot;Permalink to this definition&quot;){.headerlink}

:   Send a signal, catch exceptions and log them.

    The keyword arguments are passed to the signal handlers
    (connected through the [[`connect()`{.xref .py .py-meth
    .docutils .literal
    .notranslate}]{.pre}](#scrapy.signalmanager.SignalManager.connect &quot;scrapy.signalmanager.SignalManager.connect&quot;){.reference
    .internal} method).

[[send_catch_log_deferred]{.pre}]{.sig-name .descname}[(]{.sig-paren}*[[signal]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[[[Any]{.pre}](https://docs.python.org/3/library/typing.html#typing.Any &quot;(in Python v3.12)&quot;){.reference .external}]{.n}*, *[[\*\*]{.pre}]{.o}[[kwargs]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[[[Any]{.pre}](https://docs.python.org/3/library/typing.html#typing.Any &quot;(in Python v3.12)&quot;){.reference .external}]{.n}*[)]{.sig-paren} [[→]{.sig-return-icon} [[[Deferred]{.pre}](https://docs.twisted.org/en/stable/api/twisted.internet.defer.Deferred.html &quot;(in Twisted)&quot;){.reference .external}]{.sig-return-typehint}]{.sig-return}[[[\[source\]]{.pre}]{.viewcode-link}](_modules/scrapy/signalmanager.html#SignalManager.send_catch_log_deferred){.reference .internal}[¶](#scrapy.signalmanager.SignalManager.send_catch_log_deferred &quot;Permalink to this definition&quot;){.headerlink}

:   Like [[`send_catch_log()`{.xref .py .py-meth .docutils .literal
    .notranslate}]{.pre}](#scrapy.signalmanager.SignalManager.send_catch_log &quot;scrapy.signalmanager.SignalManager.send_catch_log&quot;){.reference
    .internal} but supports returning [[`Deferred`{.xref .py
    .py-class .docutils .literal
    .notranslate}]{.pre}](https://docs.twisted.org/en/stable/api/twisted.internet.defer.Deferred.html &quot;(in Twisted)&quot;){.reference
    .external} objects from signal handlers.

    Returns a Deferred that gets fired once all signal handlers
    deferreds were fired. Send a signal, catch exceptions and log
    them.

    The keyword arguments are passed to the signal handlers
    (connected through the [[`connect()`{.xref .py .py-meth
    .docutils .literal
    .notranslate}]{.pre}](#scrapy.signalmanager.SignalManager.connect &quot;scrapy.signalmanager.SignalManager.connect&quot;){.reference
    .internal} method).
</code></pre>
<p>:::</p>
<p>::: {#stats-collector-api .section}
[]{#topics-api-stats}</p>
<h4 id="stats-collector-apiheaderlink"><a class="header" href="#stats-collector-apiheaderlink">Stats Collector API<a href="#stats-collector-api" title="Permalink to this heading">¶</a>{.headerlink}</a></h4>
<p>There are several Stats Collectors available under the
<a href="#module-scrapy.statscollectors" title="scrapy.statscollectors: Stats Collectors">[<code>scrapy.statscollectors</code>{.xref .py .py-mod .docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal} module and they all implement the Stats Collector API defined
by the <a href="#scrapy.statscollectors.StatsCollector" title="scrapy.statscollectors.StatsCollector">[<code>StatsCollector</code>{.xref .py .py-class .docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal} class (which they all inherit from).</p>
<p>[]{#module-scrapy.statscollectors .target}</p>
<p><em>[class]{.pre}[ ]{.w}</em>[[scrapy.statscollectors.]{.pre}]{.sig-prename .descclassname}[[StatsCollector]{.pre}]{.sig-name .descname}<a href="_modules/scrapy/statscollectors.html#StatsCollector">[[[source]]{.pre}]{.viewcode-link}</a>{.reference .internal}<a href="#scrapy.statscollectors.StatsCollector" title="Permalink to this definition">¶</a>{.headerlink}</p>
<p>:</p>
<pre><code>[[get_value]{.pre}]{.sig-name .descname}[(]{.sig-paren}*[[key]{.pre}]{.n}*, *[[default]{.pre}]{.n}[[=]{.pre}]{.o}[[None]{.pre}]{.default_value}*[)]{.sig-paren}[[[\[source\]]{.pre}]{.viewcode-link}](_modules/scrapy/statscollectors.html#StatsCollector.get_value){.reference .internal}[¶](#scrapy.statscollectors.StatsCollector.get_value &quot;Permalink to this definition&quot;){.headerlink}

:   Return the value for the given stats key or default if it
    doesn't exist.

[[get_stats]{.pre}]{.sig-name .descname}[(]{.sig-paren}[)]{.sig-paren}[[[\[source\]]{.pre}]{.viewcode-link}](_modules/scrapy/statscollectors.html#StatsCollector.get_stats){.reference .internal}[¶](#scrapy.statscollectors.StatsCollector.get_stats &quot;Permalink to this definition&quot;){.headerlink}

:   Get all stats from the currently running spider as a dict.

[[set_value]{.pre}]{.sig-name .descname}[(]{.sig-paren}*[[key]{.pre}]{.n}*, *[[value]{.pre}]{.n}*[)]{.sig-paren}[[[\[source\]]{.pre}]{.viewcode-link}](_modules/scrapy/statscollectors.html#StatsCollector.set_value){.reference .internal}[¶](#scrapy.statscollectors.StatsCollector.set_value &quot;Permalink to this definition&quot;){.headerlink}

:   Set the given value for the given stats key.

[[set_stats]{.pre}]{.sig-name .descname}[(]{.sig-paren}*[[stats]{.pre}]{.n}*[)]{.sig-paren}[[[\[source\]]{.pre}]{.viewcode-link}](_modules/scrapy/statscollectors.html#StatsCollector.set_stats){.reference .internal}[¶](#scrapy.statscollectors.StatsCollector.set_stats &quot;Permalink to this definition&quot;){.headerlink}

:   Override the current stats with the dict passed in
    [`stats`{.docutils .literal .notranslate}]{.pre} argument.

[[inc_value]{.pre}]{.sig-name .descname}[(]{.sig-paren}*[[key]{.pre}]{.n}*, *[[count]{.pre}]{.n}[[=]{.pre}]{.o}[[1]{.pre}]{.default_value}*, *[[start]{.pre}]{.n}[[=]{.pre}]{.o}[[0]{.pre}]{.default_value}*[)]{.sig-paren}[[[\[source\]]{.pre}]{.viewcode-link}](_modules/scrapy/statscollectors.html#StatsCollector.inc_value){.reference .internal}[¶](#scrapy.statscollectors.StatsCollector.inc_value &quot;Permalink to this definition&quot;){.headerlink}

:   Increment the value of the given stats key, by the given count,
    assuming the start value given (when it's not set).

[[max_value]{.pre}]{.sig-name .descname}[(]{.sig-paren}*[[key]{.pre}]{.n}*, *[[value]{.pre}]{.n}*[)]{.sig-paren}[[[\[source\]]{.pre}]{.viewcode-link}](_modules/scrapy/statscollectors.html#StatsCollector.max_value){.reference .internal}[¶](#scrapy.statscollectors.StatsCollector.max_value &quot;Permalink to this definition&quot;){.headerlink}

:   Set the given value for the given key only if current value for
    the same key is lower than value. If there is no current value
    for the given key, the value is always set.

[[min_value]{.pre}]{.sig-name .descname}[(]{.sig-paren}*[[key]{.pre}]{.n}*, *[[value]{.pre}]{.n}*[)]{.sig-paren}[[[\[source\]]{.pre}]{.viewcode-link}](_modules/scrapy/statscollectors.html#StatsCollector.min_value){.reference .internal}[¶](#scrapy.statscollectors.StatsCollector.min_value &quot;Permalink to this definition&quot;){.headerlink}

:   Set the given value for the given key only if current value for
    the same key is greater than value. If there is no current value
    for the given key, the value is always set.

[[clear_stats]{.pre}]{.sig-name .descname}[(]{.sig-paren}[)]{.sig-paren}[[[\[source\]]{.pre}]{.viewcode-link}](_modules/scrapy/statscollectors.html#StatsCollector.clear_stats){.reference .internal}[¶](#scrapy.statscollectors.StatsCollector.clear_stats &quot;Permalink to this definition&quot;){.headerlink}

:   Clear all stats.

The following methods are not part of the stats collection api but
instead used when implementing custom stats collectors:

[[open_spider]{.pre}]{.sig-name .descname}[(]{.sig-paren}*[[spider]{.pre}]{.n}*[)]{.sig-paren}[[[\[source\]]{.pre}]{.viewcode-link}](_modules/scrapy/statscollectors.html#StatsCollector.open_spider){.reference .internal}[¶](#scrapy.statscollectors.StatsCollector.open_spider &quot;Permalink to this definition&quot;){.headerlink}

:   Open the given spider for stats collection.

[[close_spider]{.pre}]{.sig-name .descname}[(]{.sig-paren}*[[spider]{.pre}]{.n}*[)]{.sig-paren}[[[\[source\]]{.pre}]{.viewcode-link}](_modules/scrapy/statscollectors.html#StatsCollector.close_spider){.reference .internal}[¶](#scrapy.statscollectors.StatsCollector.close_spider &quot;Permalink to this definition&quot;){.headerlink}

:   Close the given spider. After this is called, no more specific
    stats can be accessed or collected.
</code></pre>
<p>:::
:::
:::</p>
<p><a href="index.html#document-topics/architecture">[Architecture overview]{.doc}</a>{.reference .internal}</p>
<p>:   Understand the Scrapy architecture.</p>
<p><a href="index.html#document-topics/addons">[Add-ons]{.doc}</a>{.reference .internal}</p>
<p>:   Enable and configure third-party extensions.</p>
<p><a href="index.html#document-topics/downloader-middleware">[Downloader Middleware]{.doc}</a>{.reference .internal}</p>
<p>:   Customize how pages get requested and downloaded.</p>
<p><a href="index.html#document-topics/spider-middleware">[Spider Middleware]{.doc}</a>{.reference .internal}</p>
<p>:   Customize the input and output of your spiders.</p>
<p><a href="index.html#document-topics/extensions">[Extensions]{.doc}</a>{.reference .internal}</p>
<p>:   Extend Scrapy with your custom functionality</p>
<p><a href="index.html#document-topics/signals">[Signals]{.doc}</a>{.reference .internal}</p>
<p>:   See all available signals and how to work with them.</p>
<p><a href="index.html#document-topics/scheduler">[Scheduler]{.doc}</a>{.reference .internal}</p>
<p>:   Understand the scheduler component.</p>
<p><a href="index.html#document-topics/exporters">[Item Exporters]{.doc}</a>{.reference .internal}</p>
<p>:   Quickly export your scraped items to a file (XML, CSV, etc).</p>
<p><a href="index.html#document-topics/components">[Components]{.doc}</a>{.reference .internal}</p>
<p>:   Learn the common API and some good practices when building custom
Scrapy components.</p>
<p><a href="index.html#document-topics/api">[Core API]{.doc}</a>{.reference .internal}</p>
<p>:   Use it on extensions and middlewares to extend Scrapy functionality.
:::</p>
<p>::: {#all-the-rest .section}</p>
<h2 id="all-the-restheaderlink"><a class="header" href="#all-the-restheaderlink">All the rest<a href="#all-the-rest" title="Permalink to this heading">¶</a>{.headerlink}</a></h2>
<p>::: {.toctree-wrapper .compound}
[]{#document-news}</p>
<p>::: {#release-notes .section}
[]{#news}</p>
<h3 id="release-notesheaderlink"><a class="header" href="#release-notesheaderlink">Release notes<a href="#release-notes" title="Permalink to this heading">¶</a>{.headerlink}</a></h3>
<p>::: {#scrapy-2-11-0-2023-09-18 .section}
[]{#release-2-11-0}</p>
<h4 id="scrapy-2110-2023-09-18headerlink"><a class="header" href="#scrapy-2110-2023-09-18headerlink">Scrapy 2.11.0 (2023-09-18)<a href="#scrapy-2-11-0-2023-09-18" title="Permalink to this heading">¶</a>{.headerlink}</a></h4>
<p>Highlights:</p>
<ul>
<li>
<p>Spiders can now modify <a href="index.html#topics-settings">[settings]{.std
.std-ref}</a>{.hoverxref .tooltip
.reference .internal} in their <a href="index.html#scrapy.Spider.from_crawler" title="scrapy.Spider.from_crawler">[<code>from_crawler()</code>{.xref .py .py-meth
.docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal} methods, e.g. based on <a href="index.html#spiderargs">[spider arguments]{.std
.std-ref}</a>{.hoverxref .tooltip .reference
.internal}.</p>
</li>
<li>
<p>Periodic logging of stats.</p>
</li>
</ul>
<p>::: {#backward-incompatible-changes .section}</p>
<h5 id="backward-incompatible-changesheaderlink"><a class="header" href="#backward-incompatible-changesheaderlink">Backward-incompatible changes<a href="#backward-incompatible-changes" title="Permalink to this heading">¶</a>{.headerlink}</a></h5>
<ul>
<li>
<p>Most of the initialization of <a href="index.html#scrapy.crawler.Crawler" title="scrapy.crawler.Crawler">[<code>scrapy.crawler.Crawler</code>{.xref .py
.py-class .docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal} instances is now done in <a href="index.html#scrapy.crawler.Crawler.crawl" title="scrapy.crawler.Crawler.crawl">[<code>crawl()</code>{.xref .py .py-meth
.docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal}, so the state of instances before that method is called
is now different compared to older Scrapy versions. We do not
recommend using the <a href="index.html#scrapy.crawler.Crawler" title="scrapy.crawler.Crawler">[<code>Crawler</code>{.xref .py .py-class .docutils
.literal
.notranslate}]{.pre}</a>{.reference
.internal} instances before <a href="index.html#scrapy.crawler.Crawler.crawl" title="scrapy.crawler.Crawler.crawl">[<code>crawl()</code>{.xref .py .py-meth .docutils
.literal
.notranslate}]{.pre}</a>{.reference
.internal} is called. (<a href="https://github.com/scrapy/scrapy/issues/6038">issue
6038</a>{.reference
.external})</p>
</li>
<li>
<p><a href="index.html#scrapy.Spider.from_crawler" title="scrapy.Spider.from_crawler">[<code>scrapy.Spider.from_crawler()</code>{.xref .py .py-meth .docutils
.literal
.notranslate}]{.pre}</a>{.reference
.internal} is now called before the initialization of various
components previously initialized in
[<code>scrapy.crawler.Crawler.__init__()</code>{.xref .py .py-meth .docutils
.literal .notranslate}]{.pre} and before the settings are finalized
and frozen. This change was needed to allow changing the settings in
<a href="index.html#scrapy.Spider.from_crawler" title="scrapy.Spider.from_crawler">[<code>scrapy.Spider.from_crawler()</code>{.xref .py .py-meth .docutils
.literal
.notranslate}]{.pre}</a>{.reference
.internal}. If you want to access the final setting values and the
initialized <a href="index.html#scrapy.crawler.Crawler" title="scrapy.crawler.Crawler">[<code>Crawler</code>{.xref .py .py-class .docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal} attributes in the spider code as early as possible you
can do this in <a href="index.html#scrapy.Spider.start_requests" title="scrapy.Spider.start_requests">[<code>start_requests()</code>{.xref .py .py-meth .docutils
.literal
.notranslate}]{.pre}</a>{.reference
.internal} or in a handler of the <a href="index.html#std-signal-engine_started">[<code>engine_started</code>{.xref .std
.std-signal .docutils .literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal} signal. (<a href="https://github.com/scrapy/scrapy/issues/6038">issue
6038</a>{.reference
.external})</p>
</li>
<li>
<p>The <a href="index.html#scrapy.http.TextResponse.json" title="scrapy.http.TextResponse.json">[<code>TextResponse.json</code>{.xref .py .py-meth .docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal} method now requires the response to be in a valid JSON
encoding (UTF-8, UTF-16, or UTF-32). If you need to deal with JSON
documents in an invalid encoding, use
[<code>json.loads(response.text)</code>{.docutils .literal .notranslate}]{.pre}
instead. (<a href="https://github.com/scrapy/scrapy/issues/6016">issue
6016</a>{.reference
.external})</p>
</li>
<li>
<p><a href="index.html#scrapy.exporters.PythonItemExporter" title="scrapy.exporters.PythonItemExporter">[<code>PythonItemExporter</code>{.xref .py .py-class .docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal} used the binary output by default but it no longer does.
(<a href="https://github.com/scrapy/scrapy/issues/6006">issue
6006</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/6007">issue
6007</a>{.reference
.external})
:::</p>
</li>
</ul>
<p>::: {#deprecation-removals .section}</p>
<h5 id="deprecation-removalsheaderlink"><a class="header" href="#deprecation-removalsheaderlink">Deprecation removals<a href="#deprecation-removals" title="Permalink to this heading">¶</a>{.headerlink}</a></h5>
<ul>
<li>
<p>Removed the binary export mode of <a href="index.html#scrapy.exporters.PythonItemExporter" title="scrapy.exporters.PythonItemExporter">[<code>PythonItemExporter</code>{.xref .py
.py-class .docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal}, deprecated in Scrapy 1.1.0. (<a href="https://github.com/scrapy/scrapy/issues/6006">issue
6006</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/6007">issue
6007</a>{.reference
.external})</p>
<p>::: {.admonition .note}
Note</p>
<p>If you are using this Scrapy version on Scrapy Cloud with a stack
that includes an older Scrapy version and get a &quot;TypeError:
Unexpected options: binary&quot; error, you may need to add
[<code>scrapinghub-entrypoint-scrapy</code>{.docutils .literal
.notranslate}]{.pre}<code> </code>{.docutils .literal
.notranslate}[<code>&gt;=</code>{.docutils .literal
.notranslate}]{.pre}<code> </code>{.docutils .literal
.notranslate}[<code>0.14.1</code>{.docutils .literal .notranslate}]{.pre} to
your project requirements or switch to a stack that includes Scrapy
2.11.
:::</p>
</li>
<li>
<p>Removed the [<code>CrawlerRunner.spiders</code>{.docutils .literal
.notranslate}]{.pre} attribute, deprecated in Scrapy 1.0.0, use
[<code>CrawlerRunner.spider_loader</code>{.xref .py .py-attr .docutils .literal
.notranslate}]{.pre} instead. (<a href="https://github.com/scrapy/scrapy/issues/6010">issue
6010</a>{.reference
.external})
:::</p>
</li>
</ul>
<p>::: {#deprecations .section}</p>
<h5 id="deprecationsheaderlink"><a class="header" href="#deprecationsheaderlink">Deprecations<a href="#deprecations" title="Permalink to this heading">¶</a>{.headerlink}</a></h5>
<ul>
<li>Running <a href="index.html#scrapy.crawler.Crawler.crawl" title="scrapy.crawler.Crawler.crawl">[<code>crawl()</code>{.xref .py .py-meth .docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal} more than once on the same
<a href="index.html#scrapy.crawler.Crawler" title="scrapy.crawler.Crawler">[<code>scrapy.crawler.Crawler</code>{.xref .py .py-class .docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal} instance is now deprecated. (<a href="https://github.com/scrapy/scrapy/issues/1587">issue
1587</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/6040">issue
6040</a>{.reference
.external})
:::</li>
</ul>
<p>::: {#new-features .section}</p>
<h5 id="new-featuresheaderlink"><a class="header" href="#new-featuresheaderlink">New features<a href="#new-features" title="Permalink to this heading">¶</a>{.headerlink}</a></h5>
<ul>
<li>
<p>Spiders can now modify settings in their <a href="index.html#scrapy.Spider.from_crawler" title="scrapy.Spider.from_crawler">[<code>from_crawler()</code>{.xref
.py .py-meth .docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal} method, e.g. based on <a href="index.html#spiderargs">[spider arguments]{.std
.std-ref}</a>{.hoverxref .tooltip .reference
.internal}. (<a href="https://github.com/scrapy/scrapy/issues/1305">issue
1305</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/1580">issue
1580</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/2392">issue
2392</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/3663">issue
3663</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/6038">issue
6038</a>{.reference
.external})</p>
</li>
<li>
<p>Added the <a href="index.html#scrapy.extensions.periodic_log.PeriodicLog" title="scrapy.extensions.periodic_log.PeriodicLog">[<code>PeriodicLog</code>{.xref .py .py-class .docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal} extension which can be enabled to log stats and/or their
differences periodically. (<a href="https://github.com/scrapy/scrapy/issues/5926">issue
5926</a>{.reference
.external})</p>
</li>
<li>
<p>Optimized the memory usage in <a href="index.html#scrapy.http.TextResponse.json" title="scrapy.http.TextResponse.json">[<code>TextResponse.json</code>{.xref .py
.py-meth .docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal} by removing unnecessary body decoding. (<a href="https://github.com/scrapy/scrapy/issues/5968">issue
5968</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/6016">issue
6016</a>{.reference
.external})</p>
</li>
<li>
<p>Links to [<code>.webp</code>{.docutils .literal .notranslate}]{.pre} files are
now ignored by <a href="index.html#topics-link-extractors">[link extractors]{.std
.std-ref}</a>{.hoverxref .tooltip
.reference .internal}. (<a href="https://github.com/scrapy/scrapy/issues/6021">issue
6021</a>{.reference
.external})
:::</p>
</li>
</ul>
<p>::: {#bug-fixes .section}</p>
<h5 id="bug-fixesheaderlink"><a class="header" href="#bug-fixesheaderlink">Bug fixes<a href="#bug-fixes" title="Permalink to this heading">¶</a>{.headerlink}</a></h5>
<ul>
<li>
<p>Fixed logging enabled add-ons. (<a href="https://github.com/scrapy/scrapy/issues/6036">issue
6036</a>{.reference
.external})</p>
</li>
<li>
<p>Fixed <a href="index.html#scrapy.mail.MailSender" title="scrapy.mail.MailSender">[<code>MailSender</code>{.xref .py .py-class .docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal} producing invalid message bodies when the
[<code>charset</code>{.docutils .literal .notranslate}]{.pre} argument is
passed to <a href="index.html#scrapy.mail.MailSender.send" title="scrapy.mail.MailSender.send">[<code>send()</code>{.xref .py .py-meth .docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal}. (<a href="https://github.com/scrapy/scrapy/issues/5096">issue
5096</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/5118">issue
5118</a>{.reference
.external})</p>
</li>
<li>
<p>Fixed an exception when accessing
[<code>self.EXCEPTIONS_TO_RETRY</code>{.docutils .literal .notranslate}]{.pre}
from a subclass of <a href="index.html#scrapy.downloadermiddlewares.retry.RetryMiddleware" title="scrapy.downloadermiddlewares.retry.RetryMiddleware">[<code>RetryMiddleware</code>{.xref .py .py-class .docutils
.literal
.notranslate}]{.pre}</a>{.reference
.internal}. (<a href="https://github.com/scrapy/scrapy/issues/6049">issue
6049</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/6050">issue
6050</a>{.reference
.external})</p>
</li>
<li>
<p><a href="index.html#scrapy.settings.BaseSettings.getdictorlist" title="scrapy.settings.BaseSettings.getdictorlist">[<code>scrapy.settings.BaseSettings.getdictorlist()</code>{.xref .py .py-meth
.docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal}, used to parse <a href="index.html#std-setting-FEED_EXPORT_FIELDS">[<code>FEED_EXPORT_FIELDS</code>{.xref .std
.std-setting .docutils .literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal}, now handles tuple values. (<a href="https://github.com/scrapy/scrapy/issues/6011">issue
6011</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/6013">issue
6013</a>{.reference
.external})</p>
</li>
<li>
<p>Calls to [<code>datetime.utcnow()</code>{.docutils .literal
.notranslate}]{.pre}, no longer recommended to be used, have been
replaced with calls to [<code>datetime.now()</code>{.docutils .literal
.notranslate}]{.pre} with a timezone. (<a href="https://github.com/scrapy/scrapy/issues/6014">issue
6014</a>{.reference
.external})
:::</p>
</li>
</ul>
<p>::: {#documentation .section}</p>
<h5 id="documentationheaderlink"><a class="header" href="#documentationheaderlink">Documentation<a href="#documentation" title="Permalink to this heading">¶</a>{.headerlink}</a></h5>
<ul>
<li>Updated a deprecated function call in a pipeline example. (<a href="https://github.com/scrapy/scrapy/issues/6008">issue
6008</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/6009">issue
6009</a>{.reference
.external})
:::</li>
</ul>
<p>::: {#quality-assurance .section}</p>
<h5 id="quality-assuranceheaderlink"><a class="header" href="#quality-assuranceheaderlink">Quality assurance<a href="#quality-assurance" title="Permalink to this heading">¶</a>{.headerlink}</a></h5>
<ul>
<li>
<p>Extended typing hints. (<a href="https://github.com/scrapy/scrapy/issues/6003">issue
6003</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/6005">issue
6005</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/6031">issue
6031</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/6034">issue
6034</a>{.reference
.external})</p>
</li>
<li>
<p>Pinned <a href="https://github.com/google/brotli">brotli</a>{.reference
.external} to 1.0.9 for the PyPy tests as 1.1.0 breaks them. (<a href="https://github.com/scrapy/scrapy/issues/6044">issue
6044</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/6045">issue
6045</a>{.reference
.external})</p>
</li>
<li>
<p>Other CI and pre-commit improvements. (<a href="https://github.com/scrapy/scrapy/issues/6002">issue
6002</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/6013">issue
6013</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/6046">issue
6046</a>{.reference
.external})
:::
:::</p>
</li>
</ul>
<p>::: {#scrapy-2-10-1-2023-08-30 .section}
[]{#release-2-10-1}</p>
<h4 id="scrapy-2101-2023-08-30headerlink"><a class="header" href="#scrapy-2101-2023-08-30headerlink">Scrapy 2.10.1 (2023-08-30)<a href="#scrapy-2-10-1-2023-08-30" title="Permalink to this heading">¶</a>{.headerlink}</a></h4>
<p>Marked [<code>Twisted</code>{.docutils .literal .notranslate}]{.pre}<code> </code>{.docutils
.literal .notranslate}[<code>&gt;=</code>{.docutils .literal
.notranslate}]{.pre}<code> </code>{.docutils .literal
.notranslate}[<code>23.8.0</code>{.docutils .literal .notranslate}]{.pre} as
unsupported. (<a href="https://github.com/scrapy/scrapy/issues/6024">issue
6024</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/6026">issue
6026</a>{.reference
.external})
:::</p>
<p>::: {#scrapy-2-10-0-2023-08-04 .section}
[]{#release-2-10-0}</p>
<h4 id="scrapy-2100-2023-08-04headerlink"><a class="header" href="#scrapy-2100-2023-08-04headerlink">Scrapy 2.10.0 (2023-08-04)<a href="#scrapy-2-10-0-2023-08-04" title="Permalink to this heading">¶</a>{.headerlink}</a></h4>
<p>Highlights:</p>
<ul>
<li>
<p>Added Python 3.12 support, dropped Python 3.7 support.</p>
</li>
<li>
<p>The new add-ons framework simplifies configuring 3rd-party
components that support it.</p>
</li>
<li>
<p>Exceptions to retry can now be configured.</p>
</li>
<li>
<p>Many fixes and improvements for feed exports.</p>
</li>
</ul>
<p>::: {#modified-requirements .section}</p>
<h5 id="modified-requirementsheaderlink"><a class="header" href="#modified-requirementsheaderlink">Modified requirements<a href="#modified-requirements" title="Permalink to this heading">¶</a>{.headerlink}</a></h5>
<ul>
<li>
<p>Dropped support for Python 3.7. (<a href="https://github.com/scrapy/scrapy/issues/5953">issue
5953</a>{.reference
.external})</p>
</li>
<li>
<p>Added support for the upcoming Python 3.12. (<a href="https://github.com/scrapy/scrapy/issues/5984">issue
5984</a>{.reference
.external})</p>
</li>
<li>
<p>Minimum versions increased for these dependencies:</p>
<ul>
<li>
<p><a href="https://lxml.de/">lxml</a>{.reference .external}: 4.3.0 → 4.4.1</p>
</li>
<li>
<p><a href="https://cryptography.io/en/latest/">cryptography</a>{.reference
.external}: 3.4.6 → 36.0.0</p>
</li>
</ul>
</li>
<li>
<p>[<code>pkg_resources</code>{.docutils .literal .notranslate}]{.pre} is no
longer used. (<a href="https://github.com/scrapy/scrapy/issues/5956">issue
5956</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/5958">issue
5958</a>{.reference
.external})</p>
</li>
<li>
<p><a href="https://github.com/boto/boto3">boto3</a>{.reference .external} is now
recommended instead of
<a href="https://github.com/boto/botocore">botocore</a>{.reference .external}
for exporting to S3. (<a href="https://github.com/scrapy/scrapy/issues/5833">issue
5833</a>{.reference
.external}).
:::</p>
</li>
</ul>
<p>::: {#id1 .section}</p>
<h5 id="backward-incompatible-changesheaderlink-1"><a class="header" href="#backward-incompatible-changesheaderlink-1">Backward-incompatible changes<a href="#id1" title="Permalink to this heading">¶</a>{.headerlink}</a></h5>
<ul>
<li>The value of the <a href="index.html#std-setting-FEED_STORE_EMPTY">[<code>FEED_STORE_EMPTY</code>{.xref .std .std-setting
.docutils .literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal} setting is now [<code>True</code>{.docutils
.literal .notranslate}]{.pre} instead of [<code>False</code>{.docutils .literal
.notranslate}]{.pre}. In earlier Scrapy versions empty files were
created even when this setting was [<code>False</code>{.docutils .literal
.notranslate}]{.pre} (which was a bug that is now fixed), so the new
default should keep the old behavior. (<a href="https://github.com/scrapy/scrapy/issues/872">issue
872</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/5847">issue
5847</a>{.reference
.external})
:::</li>
</ul>
<p>::: {#id2 .section}</p>
<h5 id="deprecation-removalsheaderlink-1"><a class="header" href="#deprecation-removalsheaderlink-1">Deprecation removals<a href="#id2" title="Permalink to this heading">¶</a>{.headerlink}</a></h5>
<ul>
<li>
<p>When a function is assigned to the <a href="index.html#std-setting-FEED_URI_PARAMS">[<code>FEED_URI_PARAMS</code>{.xref .std
.std-setting .docutils .literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal} setting, returning [<code>None</code>{.docutils
.literal .notranslate}]{.pre} or modifying the [<code>params</code>{.docutils
.literal .notranslate}]{.pre} input parameter, deprecated in Scrapy
2.6, is no longer supported. (<a href="https://github.com/scrapy/scrapy/issues/5994">issue
5994</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/5996">issue
5996</a>{.reference
.external})</p>
</li>
<li>
<p>The [<code>scrapy.utils.reqser</code>{.docutils .literal .notranslate}]{.pre}
module, deprecated in Scrapy 2.6, is removed. (<a href="https://github.com/scrapy/scrapy/issues/5994">issue
5994</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/5996">issue
5996</a>{.reference
.external})</p>
</li>
<li>
<p>The [<code>scrapy.squeues</code>{.docutils .literal .notranslate}]{.pre}
classes [<code>PickleFifoDiskQueueNonRequest</code>{.docutils .literal
.notranslate}]{.pre}, [<code>PickleLifoDiskQueueNonRequest</code>{.docutils
.literal .notranslate}]{.pre},
[<code>MarshalFifoDiskQueueNonRequest</code>{.docutils .literal
.notranslate}]{.pre}, and
[<code>MarshalLifoDiskQueueNonRequest</code>{.docutils .literal
.notranslate}]{.pre}, deprecated in Scrapy 2.6, are removed. (<a href="https://github.com/scrapy/scrapy/issues/5994">issue
5994</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/5996">issue
5996</a>{.reference
.external})</p>
</li>
<li>
<p>The property [<code>open_spiders</code>{.docutils .literal .notranslate}]{.pre}
and the methods [<code>has_capacity</code>{.docutils .literal
.notranslate}]{.pre} and [<code>schedule</code>{.docutils .literal
.notranslate}]{.pre} of [<code>scrapy.core.engine.ExecutionEngine</code>{.xref
.py .py-class .docutils .literal .notranslate}]{.pre}, deprecated in
Scrapy 2.6, are removed. (<a href="https://github.com/scrapy/scrapy/issues/5994">issue
5994</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/5998">issue
5998</a>{.reference
.external})</p>
</li>
<li>
<p>Passing a [<code>spider</code>{.docutils .literal .notranslate}]{.pre} argument
to the [<code>spider_is_idle()</code>{.xref .py .py-meth .docutils .literal
.notranslate}]{.pre}, [<code>crawl()</code>{.xref .py .py-meth .docutils
.literal .notranslate}]{.pre} and [<code>download()</code>{.xref .py .py-meth
.docutils .literal .notranslate}]{.pre} methods of
[<code>scrapy.core.engine.ExecutionEngine</code>{.xref .py .py-class .docutils
.literal .notranslate}]{.pre}, deprecated in Scrapy 2.6, is no
longer supported. (<a href="https://github.com/scrapy/scrapy/issues/5994">issue
5994</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/5998">issue
5998</a>{.reference
.external})
:::</p>
</li>
</ul>
<p>::: {#id3 .section}</p>
<h5 id="deprecationsheaderlink-1"><a class="header" href="#deprecationsheaderlink-1">Deprecations<a href="#id3" title="Permalink to this heading">¶</a>{.headerlink}</a></h5>
<ul>
<li>
<p>[<code>scrapy.utils.datatypes.CaselessDict</code>{.xref .py .py-class .docutils
.literal .notranslate}]{.pre} is deprecated, use
[<code>scrapy.utils.datatypes.CaseInsensitiveDict</code>{.xref .py .py-class
.docutils .literal .notranslate}]{.pre} instead. (<a href="https://github.com/scrapy/scrapy/issues/5146">issue
5146</a>{.reference
.external})</p>
</li>
<li>
<p>Passing the [<code>custom</code>{.docutils .literal .notranslate}]{.pre}
argument to [<code>scrapy.utils.conf.build_component_list()</code>{.xref .py
.py-func .docutils .literal .notranslate}]{.pre} is deprecated, it
was used in the past to merge [<code>FOO</code>{.docutils .literal
.notranslate}]{.pre} and [<code>FOO_BASE</code>{.docutils .literal
.notranslate}]{.pre} setting values but now Scrapy uses
<a href="index.html#scrapy.settings.BaseSettings.getwithbase" title="scrapy.settings.BaseSettings.getwithbase">[<code>scrapy.settings.BaseSettings.getwithbase()</code>{.xref .py .py-func
.docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal} to do the same. Code that uses this argument and cannot
be switched to [<code>getwithbase()</code>{.docutils .literal
.notranslate}]{.pre} can be switched to merging the values
explicitly. (<a href="https://github.com/scrapy/scrapy/issues/5726">issue
5726</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/5923">issue
5923</a>{.reference
.external})
:::</p>
</li>
</ul>
<p>::: {#id4 .section}</p>
<h5 id="new-featuresheaderlink-1"><a class="header" href="#new-featuresheaderlink-1">New features<a href="#id4" title="Permalink to this heading">¶</a>{.headerlink}</a></h5>
<ul>
<li>
<p>Added support for <a href="index.html#topics-addons">[Scrapy add-ons]{.std
.std-ref}</a>{.hoverxref .tooltip .reference
.internal}. (<a href="https://github.com/scrapy/scrapy/issues/5950">issue
5950</a>{.reference
.external})</p>
</li>
<li>
<p>Added the <a href="index.html#std-setting-RETRY_EXCEPTIONS">[<code>RETRY_EXCEPTIONS</code>{.xref .std .std-setting .docutils
.literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal} setting that configures which
exceptions will be retried by <a href="index.html#scrapy.downloadermiddlewares.retry.RetryMiddleware" title="scrapy.downloadermiddlewares.retry.RetryMiddleware">[<code>RetryMiddleware</code>{.xref .py
.py-class .docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal}. (<a href="https://github.com/scrapy/scrapy/issues/2701">issue
2701</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/5929">issue
5929</a>{.reference
.external})</p>
</li>
<li>
<p>Added the possiiblity to close the spider if no items were produced
in the specified time, configured by
<a href="index.html#std-setting-CLOSESPIDER_TIMEOUT_NO_ITEM">[<code>CLOSESPIDER_TIMEOUT_NO_ITEM</code>{.xref .std .std-setting .docutils
.literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal}. (<a href="https://github.com/scrapy/scrapy/issues/5979">issue
5979</a>{.reference
.external})</p>
</li>
<li>
<p>Added support for the <a href="index.html#std-setting-AWS_REGION_NAME">[<code>AWS_REGION_NAME</code>{.xref .std .std-setting
.docutils .literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal} setting to feed exports. (<a href="https://github.com/scrapy/scrapy/issues/5980">issue
5980</a>{.reference
.external})</p>
</li>
<li>
<p>Added support for using <a href="https://docs.python.org/3/library/pathlib.html#pathlib.Path" title="(in Python v3.12)">[<code>pathlib.Path</code>{.xref .py .py-class
.docutils .literal
.notranslate}]{.pre}</a>{.reference
.external} objects that refer to absolute Windows paths in the
<a href="index.html#std-setting-FEEDS">[<code>FEEDS</code>{.xref .std .std-setting .docutils .literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal} setting. (<a href="https://github.com/scrapy/scrapy/issues/5939">issue
5939</a>{.reference
.external})
:::</p>
</li>
</ul>
<p>::: {#id5 .section}</p>
<h5 id="bug-fixesheaderlink-1"><a class="header" href="#bug-fixesheaderlink-1">Bug fixes<a href="#id5" title="Permalink to this heading">¶</a>{.headerlink}</a></h5>
<ul>
<li>
<p>Fixed creating empty feeds even with
[<code>FEED_STORE_EMPTY=False</code>{.docutils .literal .notranslate}]{.pre}.
(<a href="https://github.com/scrapy/scrapy/issues/872">issue 872</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/5847">issue
5847</a>{.reference
.external})</p>
</li>
<li>
<p>Fixed using absolute Windows paths when specifying output files.
(<a href="https://github.com/scrapy/scrapy/issues/5969">issue
5969</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/5971">issue
5971</a>{.reference
.external})</p>
</li>
<li>
<p>Fixed problems with uploading large files to S3 by switching to
multipart uploads (requires
<a href="https://github.com/boto/boto3">boto3</a>{.reference .external}).
(<a href="https://github.com/scrapy/scrapy/issues/960">issue 960</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/5735">issue
5735</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/5833">issue
5833</a>{.reference
.external})</p>
</li>
<li>
<p>Fixed the JSON exporter writing extra commas when some exceptions
occur. (<a href="https://github.com/scrapy/scrapy/issues/3090">issue
3090</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/5952">issue
5952</a>{.reference
.external})</p>
</li>
<li>
<p>Fixed the &quot;read of closed file&quot; error in the CSV exporter. (<a href="https://github.com/scrapy/scrapy/issues/5043">issue
5043</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/5705">issue
5705</a>{.reference
.external})</p>
</li>
<li>
<p>Fixed an error when a component added by the class object throws
<a href="index.html#scrapy.exceptions.NotConfigured" title="scrapy.exceptions.NotConfigured">[<code>NotConfigured</code>{.xref .py .py-exc .docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal} with a message. (<a href="https://github.com/scrapy/scrapy/issues/5950">issue
5950</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/5992">issue
5992</a>{.reference
.external})</p>
</li>
<li>
<p>Added the missing <a href="index.html#scrapy.settings.BaseSettings.pop" title="scrapy.settings.BaseSettings.pop">[<code>scrapy.settings.BaseSettings.pop()</code>{.xref .py
.py-meth .docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal} method. (<a href="https://github.com/scrapy/scrapy/issues/5959">issue
5959</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/5960">issue
5960</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/5963">issue
5963</a>{.reference
.external})</p>
</li>
<li>
<p>Added [<code>CaseInsensitiveDict</code>{.xref .py .py-class .docutils .literal
.notranslate}]{.pre} as a replacement for [<code>CaselessDict</code>{.xref .py
.py-class .docutils .literal .notranslate}]{.pre} that fixes some
API inconsistencies. (<a href="https://github.com/scrapy/scrapy/issues/5146">issue
5146</a>{.reference
.external})
:::</p>
</li>
</ul>
<p>::: {#id6 .section}</p>
<h5 id="documentationheaderlink-1"><a class="header" href="#documentationheaderlink-1">Documentation<a href="#id6" title="Permalink to this heading">¶</a>{.headerlink}</a></h5>
<ul>
<li>
<p>Documented <a href="index.html#scrapy.Spider.update_settings" title="scrapy.Spider.update_settings">[<code>scrapy.Spider.update_settings()</code>{.xref .py .py-meth
.docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal}. (<a href="https://github.com/scrapy/scrapy/issues/5745">issue
5745</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/5846">issue
5846</a>{.reference
.external})</p>
</li>
<li>
<p>Documented possible problems with early Twisted reactor installation
and their solutions. (<a href="https://github.com/scrapy/scrapy/issues/5981">issue
5981</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/6000">issue
6000</a>{.reference
.external})</p>
</li>
<li>
<p>Added examples of making additional requests in callbacks. (<a href="https://github.com/scrapy/scrapy/issues/5927">issue
5927</a>{.reference
.external})</p>
</li>
<li>
<p>Improved the feed export docs. (<a href="https://github.com/scrapy/scrapy/issues/5579">issue
5579</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/5931">issue
5931</a>{.reference
.external})</p>
</li>
<li>
<p>Clarified the docs about request objects on redirection. (<a href="https://github.com/scrapy/scrapy/issues/5707">issue
5707</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/5937">issue
5937</a>{.reference
.external})
:::</p>
</li>
</ul>
<p>::: {#id7 .section}</p>
<h5 id="quality-assuranceheaderlink-1"><a class="header" href="#quality-assuranceheaderlink-1">Quality assurance<a href="#id7" title="Permalink to this heading">¶</a>{.headerlink}</a></h5>
<ul>
<li>
<p>Added support for running tests against the installed Scrapy
version. (<a href="https://github.com/scrapy/scrapy/issues/4914">issue
4914</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/5949">issue
5949</a>{.reference
.external})</p>
</li>
<li>
<p>Extended typing hints. (<a href="https://github.com/scrapy/scrapy/issues/5925">issue
5925</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/5977">issue
5977</a>{.reference
.external})</p>
</li>
<li>
<p>Fixed the
[<code>test_utils_asyncio.AsyncioTest.test_set_asyncio_event_loop</code>{.docutils
.literal .notranslate}]{.pre} test. (<a href="https://github.com/scrapy/scrapy/issues/5951">issue
5951</a>{.reference
.external})</p>
</li>
<li>
<p>Fixed the
[<code>test_feedexport.BatchDeliveriesTest.test_batch_path_differ</code>{.docutils
.literal .notranslate}]{.pre} test on Windows. (<a href="https://github.com/scrapy/scrapy/issues/5847">issue
5847</a>{.reference
.external})</p>
</li>
<li>
<p>Enabled CI runs for Python 3.11 on Windows. (<a href="https://github.com/scrapy/scrapy/issues/5999">issue
5999</a>{.reference
.external})</p>
</li>
<li>
<p>Simplified skipping tests that depend on [<code>uvloop</code>{.docutils
.literal .notranslate}]{.pre}. (<a href="https://github.com/scrapy/scrapy/issues/5984">issue
5984</a>{.reference
.external})</p>
</li>
<li>
<p>Fixed the [<code>extra-deps-pinned</code>{.docutils .literal
.notranslate}]{.pre} tox env. (<a href="https://github.com/scrapy/scrapy/issues/5948">issue
5948</a>{.reference
.external})</p>
</li>
<li>
<p>Implemented cleanups. (<a href="https://github.com/scrapy/scrapy/issues/5965">issue
5965</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/5986">issue
5986</a>{.reference
.external})
:::
:::</p>
</li>
</ul>
<p>::: {#scrapy-2-9-0-2023-05-08 .section}
[]{#release-2-9-0}</p>
<h4 id="scrapy-290-2023-05-08headerlink"><a class="header" href="#scrapy-290-2023-05-08headerlink">Scrapy 2.9.0 (2023-05-08)<a href="#scrapy-2-9-0-2023-05-08" title="Permalink to this heading">¶</a>{.headerlink}</a></h4>
<p>Highlights:</p>
<ul>
<li>
<p>Per-domain download settings.</p>
</li>
<li>
<p>Compatibility with new
<a href="https://cryptography.io/en/latest/">cryptography</a>{.reference
.external} and new
<a href="https://github.com/scrapy/parsel">parsel</a>{.reference .external}.</p>
</li>
<li>
<p>JMESPath selectors from the new
<a href="https://github.com/scrapy/parsel">parsel</a>{.reference .external}.</p>
</li>
<li>
<p>Bug fixes.</p>
</li>
</ul>
<p>::: {#id8 .section}</p>
<h5 id="deprecationsheaderlink-2"><a class="header" href="#deprecationsheaderlink-2">Deprecations<a href="#id8" title="Permalink to this heading">¶</a>{.headerlink}</a></h5>
<ul>
<li>[<code>scrapy.extensions.feedexport._FeedSlot</code>{.xref .py .py-class
.docutils .literal .notranslate}]{.pre} is renamed to
[<code>scrapy.extensions.feedexport.FeedSlot</code>{.xref .py .py-class
.docutils .literal .notranslate}]{.pre} and the old name is
deprecated. (<a href="https://github.com/scrapy/scrapy/issues/5876">issue
5876</a>{.reference
.external})
:::</li>
</ul>
<p>::: {#id9 .section}</p>
<h5 id="new-featuresheaderlink-2"><a class="header" href="#new-featuresheaderlink-2">New features<a href="#id9" title="Permalink to this heading">¶</a>{.headerlink}</a></h5>
<ul>
<li>
<p>Settings corresponding to <a href="index.html#std-setting-DOWNLOAD_DELAY">[<code>DOWNLOAD_DELAY</code>{.xref .std .std-setting
.docutils .literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal},
<a href="index.html#std-setting-CONCURRENT_REQUESTS_PER_DOMAIN">[<code>CONCURRENT_REQUESTS_PER_DOMAIN</code>{.xref .std .std-setting .docutils
.literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal} and
<a href="index.html#std-setting-RANDOMIZE_DOWNLOAD_DELAY">[<code>RANDOMIZE_DOWNLOAD_DELAY</code>{.xref .std .std-setting .docutils
.literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal} can now be set on a per-domain basis
via the new <a href="index.html#std-setting-DOWNLOAD_SLOTS">[<code>DOWNLOAD_SLOTS</code>{.xref .std .std-setting .docutils
.literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal} setting. (<a href="https://github.com/scrapy/scrapy/issues/5328">issue
5328</a>{.reference
.external})</p>
</li>
<li>
<p>Added [<code>TextResponse.jmespath()</code>{.xref .py .py-meth .docutils
.literal .notranslate}]{.pre}, a shortcut for JMESPath selectors
available since
<a href="https://github.com/scrapy/parsel">parsel</a>{.reference .external}
1.8.1. (<a href="https://github.com/scrapy/scrapy/issues/5894">issue
5894</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/5915">issue
5915</a>{.reference
.external})</p>
</li>
<li>
<p>Added <a href="index.html#std-signal-feed_slot_closed">[<code>feed_slot_closed</code>{.xref .std .std-signal .docutils .literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal} and <a href="index.html#std-signal-feed_exporter_closed">[<code>feed_exporter_closed</code>{.xref
.std .std-signal .docutils .literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal} signals. (<a href="https://github.com/scrapy/scrapy/issues/5876">issue
5876</a>{.reference
.external})</p>
</li>
<li>
<p>Added [<code>scrapy.utils.request.request_to_curl()</code>{.xref .py .py-func
.docutils .literal .notranslate}]{.pre}, a function to produce a
curl command from a [<code>Request</code>{.xref .py .py-class .docutils
.literal .notranslate}]{.pre} object. (<a href="https://github.com/scrapy/scrapy/issues/5892">issue
5892</a>{.reference
.external})</p>
</li>
<li>
<p>Values of <a href="index.html#std-setting-FILES_STORE">[<code>FILES_STORE</code>{.xref .std .std-setting .docutils .literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal} and <a href="index.html#std-setting-IMAGES_STORE">[<code>IMAGES_STORE</code>{.xref .std
.std-setting .docutils .literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal} can now be <a href="https://docs.python.org/3/library/pathlib.html#pathlib.Path" title="(in Python v3.12)">[<code>pathlib.Path</code>{.xref .py
.py-class .docutils .literal
.notranslate}]{.pre}</a>{.reference
.external} instances. (<a href="https://github.com/scrapy/scrapy/issues/5801">issue
5801</a>{.reference
.external})
:::</p>
</li>
</ul>
<p>::: {#id10 .section}</p>
<h5 id="bug-fixesheaderlink-2"><a class="header" href="#bug-fixesheaderlink-2">Bug fixes<a href="#id10" title="Permalink to this heading">¶</a>{.headerlink}</a></h5>
<ul>
<li>
<p>Fixed a warning with Parsel 1.8.1+. (<a href="https://github.com/scrapy/scrapy/issues/5903">issue
5903</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/5918">issue
5918</a>{.reference
.external})</p>
</li>
<li>
<p>Fixed an error when using feed postprocessing with S3 storage.
(<a href="https://github.com/scrapy/scrapy/issues/5500">issue
5500</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/5581">issue
5581</a>{.reference
.external})</p>
</li>
<li>
<p>Added the missing
<a href="index.html#scrapy.settings.BaseSettings.setdefault" title="scrapy.settings.BaseSettings.setdefault">[<code>scrapy.settings.BaseSettings.setdefault()</code>{.xref .py .py-meth
.docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal} method. (<a href="https://github.com/scrapy/scrapy/issues/5811">issue
5811</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/5821">issue
5821</a>{.reference
.external})</p>
</li>
<li>
<p>Fixed an error when using
<a href="https://cryptography.io/en/latest/">cryptography</a>{.reference
.external} 40.0.0+ and
<a href="index.html#std-setting-DOWNLOADER_CLIENT_TLS_VERBOSE_LOGGING">[<code>DOWNLOADER_CLIENT_TLS_VERBOSE_LOGGING</code>{.xref .std .std-setting
.docutils .literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal} is enabled. (<a href="https://github.com/scrapy/scrapy/issues/5857">issue
5857</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/5858">issue
5858</a>{.reference
.external})</p>
</li>
<li>
<p>The checksums returned by <a href="index.html#scrapy.pipelines.files.FilesPipeline" title="scrapy.pipelines.files.FilesPipeline">[<code>FilesPipeline</code>{.xref .py .py-class
.docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal} for files on Google Cloud Storage are no longer
Base64-encoded. (<a href="https://github.com/scrapy/scrapy/issues/5874">issue
5874</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/5891">issue
5891</a>{.reference
.external})</p>
</li>
<li>
<p>[<code>scrapy.utils.request.request_from_curl()</code>{.xref .py .py-func
.docutils .literal .notranslate}]{.pre} now supports $-prefixed
string values for the curl [<code>--data-raw</code>{.docutils .literal
.notranslate}]{.pre} argument, which are produced by browsers for
data that includes certain symbols. (<a href="https://github.com/scrapy/scrapy/issues/5899">issue
5899</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/5901">issue
5901</a>{.reference
.external})</p>
</li>
<li>
<p>The <a href="index.html#std-command-parse">[<code>parse</code>{.xref .std .std-command .docutils .literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal} command now also works with async
generator callbacks. (<a href="https://github.com/scrapy/scrapy/issues/5819">issue
5819</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/5824">issue
5824</a>{.reference
.external})</p>
</li>
<li>
<p>The <a href="index.html#std-command-genspider">[<code>genspider</code>{.xref .std .std-command .docutils .literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal} command now properly works with HTTPS
URLs. (<a href="https://github.com/scrapy/scrapy/issues/3553">issue
3553</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/5808">issue
5808</a>{.reference
.external})</p>
</li>
<li>
<p>Improved handling of asyncio loops. (<a href="https://github.com/scrapy/scrapy/issues/5831">issue
5831</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/5832">issue
5832</a>{.reference
.external})</p>
</li>
<li>
<p><a href="index.html#scrapy.linkextractors.lxmlhtml.LxmlLinkExtractor" title="scrapy.linkextractors.lxmlhtml.LxmlLinkExtractor">[<code>LinkExtractor</code>{.xref .py .py-class .docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal} now skips certain malformed URLs instead of raising an
exception. (<a href="https://github.com/scrapy/scrapy/issues/5881">issue
5881</a>{.reference
.external})</p>
</li>
<li>
<p>[<code>scrapy.utils.python.get_func_args()</code>{.xref .py .py-func .docutils
.literal .notranslate}]{.pre} now supports more types of callables.
(<a href="https://github.com/scrapy/scrapy/issues/5872">issue
5872</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/5885">issue
5885</a>{.reference
.external})</p>
</li>
<li>
<p>Fixed an error when processing non-UTF8 values of
[<code>Content-Type</code>{.docutils .literal .notranslate}]{.pre} headers.
(<a href="https://github.com/scrapy/scrapy/issues/5914">issue
5914</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/5917">issue
5917</a>{.reference
.external})</p>
</li>
<li>
<p>Fixed an error breaking user handling of send failures in
<a href="index.html#scrapy.mail.MailSender.send" title="scrapy.mail.MailSender.send">[<code>scrapy.mail.MailSender.send()</code>{.xref .py .py-meth .docutils
.literal
.notranslate}]{.pre}</a>{.reference
.internal}. (<a href="https://github.com/scrapy/scrapy/issues/1611">issue
1611</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/5880">issue
5880</a>{.reference
.external})
:::</p>
</li>
</ul>
<p>::: {#id11 .section}</p>
<h5 id="documentationheaderlink-2"><a class="header" href="#documentationheaderlink-2">Documentation<a href="#id11" title="Permalink to this heading">¶</a>{.headerlink}</a></h5>
<ul>
<li>
<p>Expanded contributing docs. (<a href="https://github.com/scrapy/scrapy/issues/5109">issue
5109</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/5851">issue
5851</a>{.reference
.external})</p>
</li>
<li>
<p>Added
<a href="https://github.com/adamchainz/blacken-docs">blacken-docs</a>{.reference
.external} to pre-commit and reformatted the docs with it. (<a href="https://github.com/scrapy/scrapy/issues/5813">issue
5813</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/5816">issue
5816</a>{.reference
.external})</p>
</li>
<li>
<p>Fixed a JS issue. (<a href="https://github.com/scrapy/scrapy/issues/5875">issue
5875</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/5877">issue
5877</a>{.reference
.external})</p>
</li>
<li>
<p>Fixed [<code>make</code>{.docutils .literal .notranslate}]{.pre}<code> </code>{.docutils
.literal .notranslate}[<code>htmlview</code>{.docutils .literal
.notranslate}]{.pre}. (<a href="https://github.com/scrapy/scrapy/issues/5878">issue
5878</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/5879">issue
5879</a>{.reference
.external})</p>
</li>
<li>
<p>Fixed typos and other small errors. (<a href="https://github.com/scrapy/scrapy/issues/5827">issue
5827</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/5839">issue
5839</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/5883">issue
5883</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/5890">issue
5890</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/5895">issue
5895</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/5904">issue
5904</a>{.reference
.external})
:::</p>
</li>
</ul>
<p>::: {#id12 .section}</p>
<h5 id="quality-assuranceheaderlink-2"><a class="header" href="#quality-assuranceheaderlink-2">Quality assurance<a href="#id12" title="Permalink to this heading">¶</a>{.headerlink}</a></h5>
<ul>
<li>
<p>Extended typing hints. (<a href="https://github.com/scrapy/scrapy/issues/5805">issue
5805</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/5889">issue
5889</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/5896">issue
5896</a>{.reference
.external})</p>
</li>
<li>
<p>Tests for most of the examples in the docs are now run as a part of
CI, found problems were fixed. (<a href="https://github.com/scrapy/scrapy/issues/5816">issue
5816</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/5826">issue
5826</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/5919">issue
5919</a>{.reference
.external})</p>
</li>
<li>
<p>Removed usage of deprecated Python classes. (<a href="https://github.com/scrapy/scrapy/issues/5849">issue
5849</a>{.reference
.external})</p>
</li>
<li>
<p>Silenced [<code>include-ignored</code>{.docutils .literal .notranslate}]{.pre}
warnings from coverage. (<a href="https://github.com/scrapy/scrapy/issues/5820">issue
5820</a>{.reference
.external})</p>
</li>
<li>
<p>Fixed a random failure of the
[<code>test_feedexport.test_batch_path_differ</code>{.docutils .literal
.notranslate}]{.pre} test. (<a href="https://github.com/scrapy/scrapy/issues/5855">issue
5855</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/5898">issue
5898</a>{.reference
.external})</p>
</li>
<li>
<p>Updated docstrings to match output produced by
<a href="https://github.com/scrapy/parsel">parsel</a>{.reference .external}
1.8.1 so that they don't cause test failures. (<a href="https://github.com/scrapy/scrapy/issues/5902">issue
5902</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/5919">issue
5919</a>{.reference
.external})</p>
</li>
<li>
<p>Other CI and pre-commit improvements. (<a href="https://github.com/scrapy/scrapy/issues/5802">issue
5802</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/5823">issue
5823</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/5908">issue
5908</a>{.reference
.external})
:::
:::</p>
</li>
</ul>
<p>::: {#scrapy-2-8-0-2023-02-02 .section}
[]{#release-2-8-0}</p>
<h4 id="scrapy-280-2023-02-02headerlink"><a class="header" href="#scrapy-280-2023-02-02headerlink">Scrapy 2.8.0 (2023-02-02)<a href="#scrapy-2-8-0-2023-02-02" title="Permalink to this heading">¶</a>{.headerlink}</a></h4>
<p>This is a maintenance release, with minor features, bug fixes, and
cleanups.</p>
<p>::: {#id13 .section}</p>
<h5 id="deprecation-removalsheaderlink-2"><a class="header" href="#deprecation-removalsheaderlink-2">Deprecation removals<a href="#id13" title="Permalink to this heading">¶</a>{.headerlink}</a></h5>
<ul>
<li>
<p>The [<code>scrapy.utils.gz.read1</code>{.docutils .literal .notranslate}]{.pre}
function, deprecated in Scrapy 2.0, has now been removed. Use the
<a href="https://docs.python.org/3/library/io.html#io.BufferedIOBase.read1" title="(in Python v3.12)">[<code>read1()</code>{.xref .py .py-meth .docutils .literal
.notranslate}]{.pre}</a>{.reference
.external} method of <a href="https://docs.python.org/3/library/gzip.html#gzip.GzipFile" title="(in Python v3.12)">[<code>GzipFile</code>{.xref .py .py-class .docutils
.literal
.notranslate}]{.pre}</a>{.reference
.external} instead. (<a href="https://github.com/scrapy/scrapy/issues/5719">issue
5719</a>{.reference
.external})</p>
</li>
<li>
<p>The [<code>scrapy.utils.python.to_native_str</code>{.docutils .literal
.notranslate}]{.pre} function, deprecated in Scrapy 2.0, has now
been removed. Use [<code>scrapy.utils.python.to_unicode()</code>{.xref .py
.py-func .docutils .literal .notranslate}]{.pre} instead. (<a href="https://github.com/scrapy/scrapy/issues/5719">issue
5719</a>{.reference
.external})</p>
</li>
<li>
<p>The [<code>scrapy.utils.python.MutableChain.next</code>{.docutils .literal
.notranslate}]{.pre} method, deprecated in Scrapy 2.0, has now been
removed. Use [<code>__next__()</code>{.xref .py .py-meth .docutils .literal
.notranslate}]{.pre} instead. (<a href="https://github.com/scrapy/scrapy/issues/5719">issue
5719</a>{.reference
.external})</p>
</li>
<li>
<p>The [<code>scrapy.linkextractors.FilteringLinkExtractor</code>{.docutils
.literal .notranslate}]{.pre} class, deprecated in Scrapy 2.0, has
now been removed. Use <a href="index.html#scrapy.linkextractors.lxmlhtml.LxmlLinkExtractor" title="scrapy.linkextractors.lxmlhtml.LxmlLinkExtractor">[<code>LinkExtractor</code>{.xref .py .py-class
.docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal} instead. (<a href="https://github.com/scrapy/scrapy/issues/5720">issue
5720</a>{.reference
.external})</p>
</li>
<li>
<p>Support for using environment variables prefixed with
[<code>SCRAPY_</code>{.docutils .literal .notranslate}]{.pre} to override
settings, deprecated in Scrapy 2.0, has now been removed. (<a href="https://github.com/scrapy/scrapy/issues/5724">issue
5724</a>{.reference
.external})</p>
</li>
<li>
<p>Support for the [<code>noconnect</code>{.docutils .literal .notranslate}]{.pre}
query string argument in proxy URLs, deprecated in Scrapy 2.0, has
now been removed. We expect proxies that used to need it to work
fine without it. (<a href="https://github.com/scrapy/scrapy/issues/5731">issue
5731</a>{.reference
.external})</p>
</li>
<li>
<p>The [<code>scrapy.utils.python.retry_on_eintr</code>{.docutils .literal
.notranslate}]{.pre} function, deprecated in Scrapy 2.3, has now
been removed. (<a href="https://github.com/scrapy/scrapy/issues/5719">issue
5719</a>{.reference
.external})</p>
</li>
<li>
<p>The [<code>scrapy.utils.python.WeakKeyCache</code>{.docutils .literal
.notranslate}]{.pre} class, deprecated in Scrapy 2.4, has now been
removed. (<a href="https://github.com/scrapy/scrapy/issues/5719">issue
5719</a>{.reference
.external})</p>
</li>
<li>
<p>The [<code>scrapy.utils.boto.is_botocore()</code>{.docutils .literal
.notranslate}]{.pre} function, deprecated in Scrapy 2.4, has now
been removed. (<a href="https://github.com/scrapy/scrapy/issues/5719">issue
5719</a>{.reference
.external})
:::</p>
</li>
</ul>
<p>::: {#id14 .section}</p>
<h5 id="deprecationsheaderlink-3"><a class="header" href="#deprecationsheaderlink-3">Deprecations<a href="#id14" title="Permalink to this heading">¶</a>{.headerlink}</a></h5>
<ul>
<li>
<p>[<code>scrapy.pipelines.images.NoimagesDrop</code>{.xref .py .py-exc .docutils
.literal .notranslate}]{.pre} is now deprecated. (<a href="https://github.com/scrapy/scrapy/issues/5368">issue
5368</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/5489">issue
5489</a>{.reference
.external})</p>
</li>
<li>
<p>[<code>ImagesPipeline.convert_image</code>{.xref .py .py-meth .docutils
.literal .notranslate}]{.pre} must now accept a
[<code>response_body</code>{.docutils .literal .notranslate}]{.pre} parameter.
(<a href="https://github.com/scrapy/scrapy/issues/3055">issue
3055</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/3689">issue
3689</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/4753">issue
4753</a>{.reference
.external})
:::</p>
</li>
</ul>
<p>::: {#id15 .section}</p>
<h5 id="new-featuresheaderlink-3"><a class="header" href="#new-featuresheaderlink-3">New features<a href="#id15" title="Permalink to this heading">¶</a>{.headerlink}</a></h5>
<ul>
<li>
<p>Applied <a href="https://black.readthedocs.io/en/stable/">black</a>{.reference
.external} coding style to files generated with the
<a href="index.html#std-command-genspider">[<code>genspider</code>{.xref .std .std-command .docutils .literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal} and <a href="index.html#std-command-startproject">[<code>startproject</code>{.xref .std
.std-command .docutils .literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal} commands. (<a href="https://github.com/scrapy/scrapy/issues/5809">issue
5809</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/5814">issue
5814</a>{.reference
.external})</p>
</li>
<li>
<p><a href="index.html#std-setting-FEED_EXPORT_ENCODING">[<code>FEED_EXPORT_ENCODING</code>{.xref .std .std-setting .docutils .literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal} is now set to [<code>&quot;utf-8&quot;</code>{.docutils
.literal .notranslate}]{.pre} in the [<code>settings.py</code>{.docutils
.literal .notranslate}]{.pre} file that the <a href="index.html#std-command-startproject">[<code>startproject</code>{.xref
.std .std-command .docutils .literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal} command generates. With this value,
JSON exports won't force the use of escape sequences for non-ASCII
characters. (<a href="https://github.com/scrapy/scrapy/issues/5797">issue
5797</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/5800">issue
5800</a>{.reference
.external})</p>
</li>
<li>
<p>The <a href="index.html#scrapy.extensions.memusage.MemoryUsage" title="scrapy.extensions.memusage.MemoryUsage">[<code>MemoryUsage</code>{.xref .py .py-class .docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal} extension now logs the peak memory usage during checks,
and the binary unit MiB is now used to avoid confusion. (<a href="https://github.com/scrapy/scrapy/issues/5717">issue
5717</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/5722">issue
5722</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/5727">issue
5727</a>{.reference
.external})</p>
</li>
<li>
<p>The [<code>callback</code>{.docutils .literal .notranslate}]{.pre} parameter of
<a href="index.html#scrapy.http.Request" title="scrapy.http.Request">[<code>Request</code>{.xref .py .py-class .docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal} can now be set to
<a href="index.html#scrapy.http.request.NO_CALLBACK" title="scrapy.http.request.NO_CALLBACK">[<code>scrapy.http.request.NO_CALLBACK()</code>{.xref .py .py-func .docutils
.literal
.notranslate}]{.pre}</a>{.reference
.internal}, to distinguish it from [<code>None</code>{.docutils .literal
.notranslate}]{.pre}, as the latter indicates that the default
spider callback (<a href="index.html#scrapy.Spider.parse" title="scrapy.Spider.parse">[<code>parse()</code>{.xref .py .py-meth .docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal}) is to be used. (<a href="https://github.com/scrapy/scrapy/issues/5798">issue
5798</a>{.reference
.external})
:::</p>
</li>
</ul>
<p>::: {#id16 .section}</p>
<h5 id="bug-fixesheaderlink-3"><a class="header" href="#bug-fixesheaderlink-3">Bug fixes<a href="#id16" title="Permalink to this heading">¶</a>{.headerlink}</a></h5>
<ul>
<li>
<p>Enabled unsafe legacy SSL renegotiation to fix access to some
outdated websites. (<a href="https://github.com/scrapy/scrapy/issues/5491">issue
5491</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/5790">issue
5790</a>{.reference
.external})</p>
</li>
<li>
<p>Fixed STARTTLS-based email delivery not working with Twisted 21.2.0
and better. (<a href="https://github.com/scrapy/scrapy/issues/5386">issue
5386</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/5406">issue
5406</a>{.reference
.external})</p>
</li>
<li>
<p>Fixed the [<code>finish_exporting()</code>{.xref .py .py-meth .docutils
.literal .notranslate}]{.pre} method of <a href="index.html#topics-exporters">[item exporters]{.std
.std-ref}</a>{.hoverxref .tooltip
.reference .internal} not being called for empty files. (<a href="https://github.com/scrapy/scrapy/issues/5537">issue
5537</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/5758">issue
5758</a>{.reference
.external})</p>
</li>
<li>
<p>Fixed HTTP/2 responses getting only the last value for a header when
multiple headers with the same name are received. (<a href="https://github.com/scrapy/scrapy/issues/5777">issue
5777</a>{.reference
.external})</p>
</li>
<li>
<p>Fixed an exception raised by the <a href="index.html#std-command-shell">[<code>shell</code>{.xref .std .std-command
.docutils .literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal} command on some cases when <a href="index.html#using-asyncio">[using
asyncio]{.std .std-ref}</a>{.hoverxref
.tooltip .reference .internal}. (<a href="https://github.com/scrapy/scrapy/issues/5740">issue
5740</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/5742">issue
5742</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/5748">issue
5748</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/5759">issue
5759</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/5760">issue
5760</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/5771">issue
5771</a>{.reference
.external})</p>
</li>
<li>
<p>When using <a href="index.html#scrapy.spiders.CrawlSpider" title="scrapy.spiders.CrawlSpider">[<code>CrawlSpider</code>{.xref .py .py-class .docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal}, callback keyword arguments ([<code>cb_kwargs</code>{.docutils
.literal .notranslate}]{.pre}) added to a request in the
[<code>process_request</code>{.docutils .literal .notranslate}]{.pre} callback
of a <a href="index.html#scrapy.spiders.Rule" title="scrapy.spiders.Rule">[<code>Rule</code>{.xref .py .py-class .docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal} will no longer be ignored. (<a href="https://github.com/scrapy/scrapy/issues/5699">issue
5699</a>{.reference
.external})</p>
</li>
<li>
<p>The <a href="index.html#images-pipeline">[images pipeline]{.std
.std-ref}</a>{.hoverxref .tooltip
.reference .internal} no longer re-encodes JPEG files. (<a href="https://github.com/scrapy/scrapy/issues/3055">issue
3055</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/3689">issue
3689</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/4753">issue
4753</a>{.reference
.external})</p>
</li>
<li>
<p>Fixed the handling of transparent WebP images by the <a href="index.html#images-pipeline">[images
pipeline]{.std .std-ref}</a>{.hoverxref
.tooltip .reference .internal}. (<a href="https://github.com/scrapy/scrapy/issues/3072">issue
3072</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/5766">issue
5766</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/5767">issue
5767</a>{.reference
.external})</p>
</li>
<li>
<p>[<code>scrapy.shell.inspect_response()</code>{.xref .py .py-func .docutils
.literal .notranslate}]{.pre} no longer inhibits [<code>SIGINT</code>{.docutils
.literal .notranslate}]{.pre} (Ctrl+C). (<a href="https://github.com/scrapy/scrapy/issues/2918">issue
2918</a>{.reference
.external})</p>
</li>
<li>
<p><a href="index.html#scrapy.linkextractors.lxmlhtml.LxmlLinkExtractor" title="scrapy.linkextractors.lxmlhtml.LxmlLinkExtractor">[<code>LinkExtractor</code>{.xref .py .py-class .docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal} with [<code>unique=False</code>{.docutils .literal
.notranslate}]{.pre} no longer filters out links that have identical
URL <em>and</em> text. (<a href="https://github.com/scrapy/scrapy/issues/3798">issue
3798</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/3799">issue
3799</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/4695">issue
4695</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/5458">issue
5458</a>{.reference
.external})</p>
</li>
<li>
<p><a href="index.html#scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware" title="scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware">[<code>RobotsTxtMiddleware</code>{.xref .py .py-class .docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal} now ignores URL protocols that do not support
[<code>robots.txt</code>{.docutils .literal .notranslate}]{.pre}
([<code>data://</code>{.docutils .literal .notranslate}]{.pre},
[<code>file://</code>{.docutils .literal .notranslate}]{.pre}). (<a href="https://github.com/scrapy/scrapy/issues/5807">issue
5807</a>{.reference
.external})</p>
</li>
<li>
<p>Silenced the [<code>filelock</code>{.docutils .literal .notranslate}]{.pre}
debug log messages introduced in Scrapy 2.6. (<a href="https://github.com/scrapy/scrapy/issues/5753">issue
5753</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/5754">issue
5754</a>{.reference
.external})</p>
</li>
<li>
<p>Fixed the output of [<code>scrapy</code>{.docutils .literal
.notranslate}]{.pre}<code> </code>{.docutils .literal
.notranslate}[<code>-h</code>{.docutils .literal .notranslate}]{.pre} showing
an unintended [<code>**commands**</code>{.docutils .literal
.notranslate}]{.pre} line. (<a href="https://github.com/scrapy/scrapy/issues/5709">issue
5709</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/5711">issue
5711</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/5712">issue
5712</a>{.reference
.external})</p>
</li>
<li>
<p>Made the active project indication in the output of <a href="index.html#topics-commands">[commands]{.std
.std-ref}</a>{.hoverxref .tooltip
.reference .internal} more clear. (<a href="https://github.com/scrapy/scrapy/issues/5715">issue
5715</a>{.reference
.external})
:::</p>
</li>
</ul>
<p>::: {#id17 .section}</p>
<h5 id="documentationheaderlink-3"><a class="header" href="#documentationheaderlink-3">Documentation<a href="#id17" title="Permalink to this heading">¶</a>{.headerlink}</a></h5>
<ul>
<li>
<p>Documented how to <a href="index.html#debug-vscode">[debug spiders from Visual Studio Code]{.std
.std-ref}</a>{.hoverxref .tooltip .reference
.internal}. (<a href="https://github.com/scrapy/scrapy/issues/5721">issue
5721</a>{.reference
.external})</p>
</li>
<li>
<p>Documented how <a href="index.html#std-setting-DOWNLOAD_DELAY">[<code>DOWNLOAD_DELAY</code>{.xref .std .std-setting .docutils
.literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal} affects per-domain concurrency.
(<a href="https://github.com/scrapy/scrapy/issues/5083">issue
5083</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/5540">issue
5540</a>{.reference
.external})</p>
</li>
<li>
<p>Improved consistency. (<a href="https://github.com/scrapy/scrapy/issues/5761">issue
5761</a>{.reference
.external})</p>
</li>
<li>
<p>Fixed typos. (<a href="https://github.com/scrapy/scrapy/issues/5714">issue
5714</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/5744">issue
5744</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/5764">issue
5764</a>{.reference
.external})
:::</p>
</li>
</ul>
<p>::: {#id18 .section}</p>
<h5 id="quality-assuranceheaderlink-3"><a class="header" href="#quality-assuranceheaderlink-3">Quality assurance<a href="#id18" title="Permalink to this heading">¶</a>{.headerlink}</a></h5>
<ul>
<li>
<p>Applied <a href="index.html#coding-style">[black coding style]{.std
.std-ref}</a>{.hoverxref .tooltip .reference
.internal}, sorted import statements, and introduced
<a href="index.html#scrapy-pre-commit">[pre-commit]{.std
.std-ref}</a>{.hoverxref .tooltip
.reference .internal}. (<a href="https://github.com/scrapy/scrapy/issues/4654">issue
4654</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/4658">issue
4658</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/5734">issue
5734</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/5737">issue
5737</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/5806">issue
5806</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/5810">issue
5810</a>{.reference
.external})</p>
</li>
<li>
<p>Switched from <a href="https://docs.python.org/3/library/os.path.html#module-os.path" title="(in Python v3.12)">[<code>os.path</code>{.xref .py .py-mod .docutils .literal
.notranslate}]{.pre}</a>{.reference
.external} to <a href="https://docs.python.org/3/library/pathlib.html#module-pathlib" title="(in Python v3.12)">[<code>pathlib</code>{.xref .py .py-mod .docutils .literal
.notranslate}]{.pre}</a>{.reference
.external}. (<a href="https://github.com/scrapy/scrapy/issues/4916">issue
4916</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/4497">issue
4497</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/5682">issue
5682</a>{.reference
.external})</p>
</li>
<li>
<p>Addressed many issues reported by Pylint. (<a href="https://github.com/scrapy/scrapy/issues/5677">issue
5677</a>{.reference
.external})</p>
</li>
<li>
<p>Improved code readability. (<a href="https://github.com/scrapy/scrapy/issues/5736">issue
5736</a>{.reference
.external})</p>
</li>
<li>
<p>Improved package metadata. (<a href="https://github.com/scrapy/scrapy/issues/5768">issue
5768</a>{.reference
.external})</p>
</li>
<li>
<p>Removed direct invocations of [<code>setup.py</code>{.docutils .literal
.notranslate}]{.pre}. (<a href="https://github.com/scrapy/scrapy/issues/5774">issue
5774</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/5776">issue
5776</a>{.reference
.external})</p>
</li>
<li>
<p>Removed unnecessary <a href="https://docs.python.org/3/library/collections.html#collections.OrderedDict" title="(in Python v3.12)">[<code>OrderedDict</code>{.xref .py .py-class .docutils
.literal
.notranslate}]{.pre}</a>{.reference
.external} usages. (<a href="https://github.com/scrapy/scrapy/issues/5795">issue
5795</a>{.reference
.external})</p>
</li>
<li>
<p>Removed unnecessary [<code>__str__</code>{.docutils .literal
.notranslate}]{.pre} definitions. (<a href="https://github.com/scrapy/scrapy/issues/5150">issue
5150</a>{.reference
.external})</p>
</li>
<li>
<p>Removed obsolete code and comments. (<a href="https://github.com/scrapy/scrapy/issues/5725">issue
5725</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/5729">issue
5729</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/5730">issue
5730</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/5732">issue
5732</a>{.reference
.external})</p>
</li>
<li>
<p>Fixed test and CI issues. (<a href="https://github.com/scrapy/scrapy/issues/5749">issue
5749</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/5750">issue
5750</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/5756">issue
5756</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/5762">issue
5762</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/5765">issue
5765</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/5780">issue
5780</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/5781">issue
5781</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/5782">issue
5782</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/5783">issue
5783</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/5785">issue
5785</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/5786">issue
5786</a>{.reference
.external})
:::
:::</p>
</li>
</ul>
<p>::: {#scrapy-2-7-1-2022-11-02 .section}
[]{#release-2-7-1}</p>
<h4 id="scrapy-271-2022-11-02headerlink"><a class="header" href="#scrapy-271-2022-11-02headerlink">Scrapy 2.7.1 (2022-11-02)<a href="#scrapy-2-7-1-2022-11-02" title="Permalink to this heading">¶</a>{.headerlink}</a></h4>
<p>::: {#id19 .section}</p>
<h5 id="new-featuresheaderlink-4"><a class="header" href="#new-featuresheaderlink-4">New features<a href="#id19" title="Permalink to this heading">¶</a>{.headerlink}</a></h5>
<ul>
<li>Relaxed the restriction introduced in 2.6.2 so that the
[<code>Proxy-Authorization</code>{.docutils .literal .notranslate}]{.pre}
header can again be set explicitly, as long as the proxy URL in the
<a href="index.html#std-reqmeta-proxy">[<code>proxy</code>{.xref .std .std-reqmeta .docutils .literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal} metadata has no other credentials,
and for as long as that proxy URL remains the same; this restores
compatibility with scrapy-zyte-smartproxy 2.1.0 and older (<a href="https://github.com/scrapy/scrapy/issues/5626">issue
5626</a>{.reference
.external}).
:::</li>
</ul>
<p>::: {#id20 .section}</p>
<h5 id="bug-fixesheaderlink-4"><a class="header" href="#bug-fixesheaderlink-4">Bug fixes<a href="#id20" title="Permalink to this heading">¶</a>{.headerlink}</a></h5>
<ul>
<li>
<p>Using [<code>-O</code>{.docutils .literal
.notranslate}]{.pre}/[<code>--overwrite-output</code>{.docutils .literal
.notranslate}]{.pre} and [<code>-t</code>{.docutils .literal
.notranslate}]{.pre}/[<code>--output-format</code>{.docutils .literal
.notranslate}]{.pre} options together now produces an error instead
of ignoring the former option (<a href="https://github.com/scrapy/scrapy/issues/5516">issue
5516</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/5605">issue
5605</a>{.reference
.external}).</p>
</li>
<li>
<p>Replaced deprecated <a href="https://docs.python.org/3/library/asyncio.html#module-asyncio" title="(in Python v3.12)">[<code>asyncio</code>{.xref .py .py-mod .docutils .literal
.notranslate}]{.pre}</a>{.reference
.external} APIs that implicitly use the current event loop with code
that explicitly requests a loop from the event loop policy (<a href="https://github.com/scrapy/scrapy/issues/5685">issue
5685</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/5689">issue
5689</a>{.reference
.external}).</p>
</li>
<li>
<p>Fixed uses of deprecated Scrapy APIs in Scrapy itself (<a href="https://github.com/scrapy/scrapy/issues/5588">issue
5588</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/5589">issue
5589</a>{.reference
.external}).</p>
</li>
<li>
<p>Fixed uses of a deprecated Pillow API (<a href="https://github.com/scrapy/scrapy/issues/5684">issue
5684</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/5692">issue
5692</a>{.reference
.external}).</p>
</li>
<li>
<p>Improved code that checks if generators return values, so that it no
longer fails on decorated methods and partial methods (<a href="https://github.com/scrapy/scrapy/issues/5323">issue
5323</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/5592">issue
5592</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/5599">issue
5599</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/5691">issue
5691</a>{.reference
.external}).
:::</p>
</li>
</ul>
<p>::: {#id21 .section}</p>
<h5 id="documentationheaderlink-4"><a class="header" href="#documentationheaderlink-4">Documentation<a href="#id21" title="Permalink to this heading">¶</a>{.headerlink}</a></h5>
<ul>
<li>
<p>Upgraded the Code of Conduct to Contributor Covenant v2.1 (<a href="https://github.com/scrapy/scrapy/issues/5698">issue
5698</a>{.reference
.external}).</p>
</li>
<li>
<p>Fixed typos (<a href="https://github.com/scrapy/scrapy/issues/5681">issue
5681</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/5694">issue
5694</a>{.reference
.external}).
:::</p>
</li>
</ul>
<p>::: {#id22 .section}</p>
<h5 id="quality-assuranceheaderlink-4"><a class="header" href="#quality-assuranceheaderlink-4">Quality assurance<a href="#id22" title="Permalink to this heading">¶</a>{.headerlink}</a></h5>
<ul>
<li>
<p>Re-enabled some erroneously disabled flake8 checks (<a href="https://github.com/scrapy/scrapy/issues/5688">issue
5688</a>{.reference
.external}).</p>
</li>
<li>
<p>Ignored harmless deprecation warnings from <a href="https://docs.python.org/3/library/typing.html#module-typing" title="(in Python v3.12)">[<code>typing</code>{.xref .py
.py-mod .docutils .literal
.notranslate}]{.pre}</a>{.reference
.external} in tests (<a href="https://github.com/scrapy/scrapy/issues/5686">issue
5686</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/5697">issue
5697</a>{.reference
.external}).</p>
</li>
<li>
<p>Modernized our CI configuration (<a href="https://github.com/scrapy/scrapy/issues/5695">issue
5695</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/5696">issue
5696</a>{.reference
.external}).
:::
:::</p>
</li>
</ul>
<p>::: {#scrapy-2-7-0-2022-10-17 .section}
[]{#release-2-7-0}</p>
<h4 id="scrapy-270-2022-10-17headerlink"><a class="header" href="#scrapy-270-2022-10-17headerlink">Scrapy 2.7.0 (2022-10-17)<a href="#scrapy-2-7-0-2022-10-17" title="Permalink to this heading">¶</a>{.headerlink}</a></h4>
<p>Highlights:</p>
<ul>
<li>
<p>Added Python 3.11 support, dropped Python 3.6 support</p>
</li>
<li>
<p>Improved support for <a href="index.html#topics-coroutines">[asynchronous callbacks]{.std
.std-ref}</a>{.hoverxref .tooltip
.reference .internal}</p>
</li>
<li>
<p><a href="index.html#using-asyncio">[Asyncio support]{.std
.std-ref}</a>{.hoverxref .tooltip .reference
.internal} is enabled by default on new projects</p>
</li>
<li>
<p>Output names of item fields can now be arbitrary strings</p>
</li>
<li>
<p>Centralized <a href="index.html#request-fingerprints">[request fingerprinting]{.std
.std-ref}</a>{.hoverxref .tooltip
.reference .internal} configuration is now possible</p>
</li>
</ul>
<p>::: {#id23 .section}</p>
<h5 id="modified-requirementsheaderlink-1"><a class="header" href="#modified-requirementsheaderlink-1">Modified requirements<a href="#id23" title="Permalink to this heading">¶</a>{.headerlink}</a></h5>
<p>Python 3.7 or greater is now required; support for Python 3.6 has been
dropped. Support for the upcoming Python 3.11 has been added.</p>
<p>The minimum required version of some dependencies has changed as well:</p>
<ul>
<li>
<p><a href="https://lxml.de/">lxml</a>{.reference .external}: 3.5.0 → 4.3.0</p>
</li>
<li>
<p><a href="https://python-pillow.org/">Pillow</a>{.reference .external} (<a href="index.html#images-pipeline">[images
pipeline]{.std .std-ref}</a>{.hoverxref
.tooltip .reference .internal}): 4.0.0 → 7.1.0</p>
</li>
<li>
<p><a href="https://zopeinterface.readthedocs.io/en/latest/">zope.interface</a>{.reference
.external}: 5.0.0 → 5.1.0</p>
</li>
</ul>
<p>(<a href="https://github.com/scrapy/scrapy/issues/5512">issue 5512</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/5514">issue
5514</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/5524">issue
5524</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/5563">issue
5563</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/5664">issue
5664</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/5670">issue
5670</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/5678">issue
5678</a>{.reference
.external})
:::</p>
<p>::: {#id24 .section}</p>
<h5 id="deprecationsheaderlink-4"><a class="header" href="#deprecationsheaderlink-4">Deprecations<a href="#id24" title="Permalink to this heading">¶</a>{.headerlink}</a></h5>
<ul>
<li>
<p><a href="index.html#scrapy.pipelines.images.ImagesPipeline.thumb_path" title="scrapy.pipelines.images.ImagesPipeline.thumb_path">[<code>ImagesPipeline.thumb_path</code>{.xref .py .py-meth .docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal} must now accept an [<code>item</code>{.docutils .literal
.notranslate}]{.pre} parameter (<a href="https://github.com/scrapy/scrapy/issues/5504">issue
5504</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/5508">issue
5508</a>{.reference
.external}).</p>
</li>
<li>
<p>The [<code>scrapy.downloadermiddlewares.decompression</code>{.docutils .literal
.notranslate}]{.pre} module is now deprecated (<a href="https://github.com/scrapy/scrapy/issues/5546">issue
5546</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/5547">issue
5547</a>{.reference
.external}).
:::</p>
</li>
</ul>
<p>::: {#id25 .section}</p>
<h5 id="new-featuresheaderlink-5"><a class="header" href="#new-featuresheaderlink-5">New features<a href="#id25" title="Permalink to this heading">¶</a>{.headerlink}</a></h5>
<ul>
<li>
<p>The <a href="index.html#scrapy.spidermiddlewares.SpiderMiddleware.process_spider_output" title="scrapy.spidermiddlewares.SpiderMiddleware.process_spider_output">[<code>process_spider_output()</code>{.xref .py .py-meth .docutils
.literal
.notranslate}]{.pre}</a>{.reference
.internal} method of <a href="index.html#topics-spider-middleware">[spider middlewares]{.std
.std-ref}</a>{.hoverxref .tooltip
.reference .internal} can now be defined as an <a href="https://docs.python.org/3/glossary.html#term-asynchronous-generator" title="(in Python v3.12)">[asynchronous
generator]{.xref .std
.std-term}</a>{.reference
.external} (<a href="https://github.com/scrapy/scrapy/issues/4978">issue
4978</a>{.reference
.external}).</p>
</li>
<li>
<p>The output of [<code>Request</code>{.xref .py .py-class .docutils .literal
.notranslate}]{.pre} callbacks defined as <a href="index.html#topics-coroutines">[coroutines]{.std
.std-ref}</a>{.hoverxref .tooltip
.reference .internal} is now processed asynchronously (<a href="https://github.com/scrapy/scrapy/issues/4978">issue
4978</a>{.reference
.external}).</p>
</li>
<li>
<p>[<code>CrawlSpider</code>{.xref .py .py-class .docutils .literal
.notranslate}]{.pre} now supports <a href="index.html#topics-coroutines">[asynchronous callbacks]{.std
.std-ref}</a>{.hoverxref .tooltip
.reference .internal} (<a href="https://github.com/scrapy/scrapy/issues/5657">issue
5657</a>{.reference
.external}).</p>
</li>
<li>
<p>New projects created with the <a href="index.html#std-command-startproject">[<code>startproject</code>{.xref .std
.std-command .docutils .literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal} command have <a href="index.html#using-asyncio">[asyncio support]{.std
.std-ref}</a>{.hoverxref .tooltip .reference
.internal} enabled by default (<a href="https://github.com/scrapy/scrapy/issues/5590">issue
5590</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/5679">issue
5679</a>{.reference
.external}).</p>
</li>
<li>
<p>The <a href="index.html#std-setting-FEED_EXPORT_FIELDS">[<code>FEED_EXPORT_FIELDS</code>{.xref .std .std-setting .docutils
.literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal} setting can now be defined as a
dictionary to customize the output name of item fields, lifting the
restriction that required output names to be valid Python
identifiers, e.g. preventing them to have whitespace (<a href="https://github.com/scrapy/scrapy/issues/1008">issue
1008</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/3266">issue
3266</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/3696">issue
3696</a>{.reference
.external}).</p>
</li>
<li>
<p>You can now customize <a href="index.html#request-fingerprints">[request fingerprinting]{.std
.std-ref}</a>{.hoverxref .tooltip
.reference .internal} through the new
<a href="index.html#std-setting-REQUEST_FINGERPRINTER_CLASS">[<code>REQUEST_FINGERPRINTER_CLASS</code>{.xref .std .std-setting .docutils
.literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal} setting, instead of having to change
it on every Scrapy component that relies on request fingerprinting
(<a href="https://github.com/scrapy/scrapy/issues/900">issue 900</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/3420">issue
3420</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/4113">issue
4113</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/4762">issue
4762</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/4524">issue
4524</a>{.reference
.external}).</p>
</li>
<li>
<p>[<code>jsonl</code>{.docutils .literal .notranslate}]{.pre} is now supported
and encouraged as a file extension for <a href="https://jsonlines.org/">JSON
Lines</a>{.reference .external} files (<a href="https://github.com/scrapy/scrapy/issues/4848">issue
4848</a>{.reference
.external}).</p>
</li>
<li>
<p><a href="index.html#scrapy.pipelines.images.ImagesPipeline.thumb_path" title="scrapy.pipelines.images.ImagesPipeline.thumb_path">[<code>ImagesPipeline.thumb_path</code>{.xref .py .py-meth .docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal} now receives the source <a href="index.html#topics-items">[item]{.std
.std-ref}</a>{.hoverxref .tooltip .reference
.internal} (<a href="https://github.com/scrapy/scrapy/issues/5504">issue
5504</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/5508">issue
5508</a>{.reference
.external}).
:::</p>
</li>
</ul>
<p>::: {#id26 .section}</p>
<h5 id="bug-fixesheaderlink-5"><a class="header" href="#bug-fixesheaderlink-5">Bug fixes<a href="#id26" title="Permalink to this heading">¶</a>{.headerlink}</a></h5>
<ul>
<li>
<p>When using Google Cloud Storage with a <a href="index.html#topics-media-pipeline">[media pipeline]{.std
.std-ref}</a>{.hoverxref .tooltip
.reference .internal}, <a href="index.html#std-setting-FILES_EXPIRES">[<code>FILES_EXPIRES</code>{.xref .std .std-setting
.docutils .literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal} now also works when
<a href="index.html#std-setting-FILES_STORE">[<code>FILES_STORE</code>{.xref .std .std-setting .docutils .literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal} does not point at the root of your
Google Cloud Storage bucket (<a href="https://github.com/scrapy/scrapy/issues/5317">issue
5317</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/5318">issue
5318</a>{.reference
.external}).</p>
</li>
<li>
<p>The <a href="index.html#std-command-parse">[<code>parse</code>{.xref .std .std-command .docutils .literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal} command now supports <a href="index.html#topics-coroutines">[asynchronous
callbacks]{.std .std-ref}</a>{.hoverxref
.tooltip .reference .internal} (<a href="https://github.com/scrapy/scrapy/issues/5424">issue
5424</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/5577">issue
5577</a>{.reference
.external}).</p>
</li>
<li>
<p>When using the <a href="index.html#std-command-parse">[<code>parse</code>{.xref .std .std-command .docutils .literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal} command with a URL for which there is
no available spider, an exception is no longer raised (<a href="https://github.com/scrapy/scrapy/issues/3264">issue
3264</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/3265">issue
3265</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/5375">issue
5375</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/5376">issue
5376</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/5497">issue
5497</a>{.reference
.external}).</p>
</li>
<li>
<p><a href="index.html#scrapy.http.TextResponse" title="scrapy.http.TextResponse">[<code>TextResponse</code>{.xref .py .py-class .docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal} now gives higher priority to the <a href="https://en.wikipedia.org/wiki/Byte_order_mark">byte order
mark</a>{.reference
.external} when determining the text encoding of the response body,
following the <a href="https://html.spec.whatwg.org/multipage/parsing.html#determining-the-character-encoding">HTML living
standard</a>{.reference
.external} (<a href="https://github.com/scrapy/scrapy/issues/5601">issue
5601</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/5611">issue
5611</a>{.reference
.external}).</p>
</li>
<li>
<p>MIME sniffing takes the response body into account in FTP and
HTTP/1.0 requests, as well as in cached requests (<a href="https://github.com/scrapy/scrapy/issues/4873">issue
4873</a>{.reference
.external}).</p>
</li>
<li>
<p>MIME sniffing now detects valid HTML 5 documents even if the
[<code>html</code>{.docutils .literal .notranslate}]{.pre} tag is missing
(<a href="https://github.com/scrapy/scrapy/issues/4873">issue
4873</a>{.reference</p>
</li>
</ul>

                    </main>

                    <nav class="nav-wrapper" aria-label="Page navigation">
                        <!-- Mobile navigation buttons -->
                            <a rel="prev" href="../Scrapy/Scrapy6.html" class="mobile-nav-chapters previous" title="Previous chapter" aria-label="Previous chapter" aria-keyshortcuts="Left">
                                <i class="fa fa-angle-left"></i>
                            </a>

                            <a rel="next" href="../Scrapy/Scrapy8.html" class="mobile-nav-chapters next" title="Next chapter" aria-label="Next chapter" aria-keyshortcuts="Right">
                                <i class="fa fa-angle-right"></i>
                            </a>

                        <div style="clear: both"></div>
                    </nav>
                </div>
            </div>

            <nav class="nav-wide-wrapper" aria-label="Page navigation">
                    <a rel="prev" href="../Scrapy/Scrapy6.html" class="nav-chapters previous" title="Previous chapter" aria-label="Previous chapter" aria-keyshortcuts="Left">
                        <i class="fa fa-angle-left"></i>
                    </a>

                    <a rel="next" href="../Scrapy/Scrapy8.html" class="nav-chapters next" title="Next chapter" aria-label="Next chapter" aria-keyshortcuts="Right">
                        <i class="fa fa-angle-right"></i>
                    </a>
            </nav>

        </div>




        <script>
            window.playground_copyable = true;
        </script>



        <script src="../clipboard.min.js"></script>
        <script src="../highlight.js"></script>
        <script src="../book.js"></script>

        <!-- Custom JS scripts -->


    </div>
    </body>
</html>
