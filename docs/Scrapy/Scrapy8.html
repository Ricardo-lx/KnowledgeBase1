<!DOCTYPE HTML>
<html lang="en" class="sidebar-visible no-js light">
    <head>
        <!-- Book generated using mdBook -->
        <meta charset="UTF-8">
        <title>Scrapy8 - KnowledgeBase</title>


        <!-- Custom HTML head -->
        
        <meta name="description" content="">
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <meta name="theme-color" content="#ffffff" />

        <link rel="icon" href="../favicon.svg">
        <link rel="shortcut icon" href="../favicon.png">
        <link rel="stylesheet" href="../css/variables.css">
        <link rel="stylesheet" href="../css/general.css">
        <link rel="stylesheet" href="../css/chrome.css">
        <link rel="stylesheet" href="../css/print.css" media="print">

        <!-- Fonts -->
        <link rel="stylesheet" href="../FontAwesome/css/font-awesome.css">
        <link rel="stylesheet" href="../fonts/fonts.css">

        <!-- Highlight.js Stylesheets -->
        <link rel="stylesheet" href="../highlight.css">
        <link rel="stylesheet" href="../tomorrow-night.css">
        <link rel="stylesheet" href="../ayu-highlight.css">

        <!-- Custom theme stylesheets -->

    </head>
    <body>
    <div id="body-container">
        <!-- Provide site root to javascript -->
        <script>
            var path_to_root = "../";
            var default_theme = window.matchMedia("(prefers-color-scheme: dark)").matches ? "navy" : "light";
        </script>

        <!-- Work around some values being stored in localStorage wrapped in quotes -->
        <script>
            try {
                var theme = localStorage.getItem('mdbook-theme');
                var sidebar = localStorage.getItem('mdbook-sidebar');

                if (theme.startsWith('"') && theme.endsWith('"')) {
                    localStorage.setItem('mdbook-theme', theme.slice(1, theme.length - 1));
                }

                if (sidebar.startsWith('"') && sidebar.endsWith('"')) {
                    localStorage.setItem('mdbook-sidebar', sidebar.slice(1, sidebar.length - 1));
                }
            } catch (e) { }
        </script>

        <!-- Set the theme before any content is loaded, prevents flash -->
        <script>
            var theme;
            try { theme = localStorage.getItem('mdbook-theme'); } catch(e) { }
            if (theme === null || theme === undefined) { theme = default_theme; }
            var html = document.querySelector('html');
            html.classList.remove('no-js')
            html.classList.remove('light')
            html.classList.add(theme);
            html.classList.add('js');
        </script>

        <!-- Hide / unhide sidebar before it is displayed -->
        <script>
            var html = document.querySelector('html');
            var sidebar = null;
            if (document.body.clientWidth >= 1080) {
                try { sidebar = localStorage.getItem('mdbook-sidebar'); } catch(e) { }
                sidebar = sidebar || 'visible';
            } else {
                sidebar = 'hidden';
            }
            html.classList.remove('sidebar-visible');
            html.classList.add("sidebar-" + sidebar);
        </script>

        <nav id="sidebar" class="sidebar" aria-label="Table of contents">
            <div class="sidebar-scrollbox">
                <ol class="chapter"><li class="chapter-item expanded "><a href="../Scrapy/Scrapy.html"><strong aria-hidden="true">1.</strong> Scrapy</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="../Scrapy/Scrapy1.html"><strong aria-hidden="true">1.1.</strong> Scrapy1</a></li><li class="chapter-item expanded "><a href="../Scrapy/Scrapy2.html"><strong aria-hidden="true">1.2.</strong> Scrapy2</a></li><li class="chapter-item expanded "><a href="../Scrapy/Scrapy3.html"><strong aria-hidden="true">1.3.</strong> Scrapy3</a></li><li class="chapter-item expanded "><a href="../Scrapy/Scrapy4.html"><strong aria-hidden="true">1.4.</strong> Scrapy4</a></li><li class="chapter-item expanded "><a href="../Scrapy/Scrapy5.html"><strong aria-hidden="true">1.5.</strong> Scrapy5</a></li><li class="chapter-item expanded "><a href="../Scrapy/Scrapy6.html"><strong aria-hidden="true">1.6.</strong> Scrapy6</a></li><li class="chapter-item expanded "><a href="../Scrapy/Scrapy7.html"><strong aria-hidden="true">1.7.</strong> Scrapy7</a></li><li class="chapter-item expanded "><a href="../Scrapy/Scrapy8.html" class="active"><strong aria-hidden="true">1.8.</strong> Scrapy8</a></li><li class="chapter-item expanded "><a href="../Scrapy/Scrapy9.html"><strong aria-hidden="true">1.9.</strong> Scrapy9</a></li><li class="chapter-item expanded "><a href="../Scrapy/Scrapy10.html"><strong aria-hidden="true">1.10.</strong> Scrapy10</a></li></ol></li><li class="chapter-item expanded "><a href="../ThinkPython/ThinkPython.html"><strong aria-hidden="true">2.</strong> ThinkPython</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="../ThinkPython/part_1.html"><strong aria-hidden="true">2.1.</strong> ThinkPython1</a></li><li class="chapter-item expanded "><a href="../ThinkPython/part_2.html"><strong aria-hidden="true">2.2.</strong> ThinkPython2</a></li><li class="chapter-item expanded "><a href="../ThinkPython/part_3.html"><strong aria-hidden="true">2.3.</strong> ThinkPython3</a></li><li class="chapter-item expanded "><a href="../ThinkPython/part_4.html"><strong aria-hidden="true">2.4.</strong> ThinkPython4</a></li><li class="chapter-item expanded "><a href="../ThinkPython/part_5.html"><strong aria-hidden="true">2.5.</strong> ThinkPython5</a></li><li class="chapter-item expanded "><a href="../ThinkPython/part_6.html"><strong aria-hidden="true">2.6.</strong> ThinkPython6</a></li><li class="chapter-item expanded "><a href="../ThinkPython/part_7.html"><strong aria-hidden="true">2.7.</strong> ThinkPython7</a></li><li class="chapter-item expanded "><a href="../ThinkPython/part_8.html"><strong aria-hidden="true">2.8.</strong> ThinkPython8</a></li><li class="chapter-item expanded "><a href="../ThinkPython/part_9.html"><strong aria-hidden="true">2.9.</strong> ThinkPython9</a></li><li class="chapter-item expanded "><a href="../ThinkPython/part_10.html"><strong aria-hidden="true">2.10.</strong> ThinkPython10</a></li></ol></li><li class="chapter-item expanded "><a href="../C-sharp-docs/C-sharp-docs.html"><strong aria-hidden="true">3.</strong> C-sharp-docs</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="../C-sharp-docs/part1.html"><strong aria-hidden="true">3.1.</strong> Csharp1</a></li><li class="chapter-item expanded "><a href="../C-sharp-docs/part2.html"><strong aria-hidden="true">3.2.</strong> Csharp2</a></li><li class="chapter-item expanded "><a href="../C-sharp-docs/part3.html"><strong aria-hidden="true">3.3.</strong> Csharp3</a></li><li class="chapter-item expanded "><a href="../C-sharp-docs/part4.html"><strong aria-hidden="true">3.4.</strong> Csharp4</a></li><li class="chapter-item expanded "><a href="../C-sharp-docs/part5.html"><strong aria-hidden="true">3.5.</strong> Csharp5</a></li><li class="chapter-item expanded "><a href="../C-sharp-docs/part6.html"><strong aria-hidden="true">3.6.</strong> Csharp6</a></li><li class="chapter-item expanded "><a href="../C-sharp-docs/part7.html"><strong aria-hidden="true">3.7.</strong> Csharp7</a></li><li class="chapter-item expanded "><a href="../C-sharp-docs/part8.html"><strong aria-hidden="true">3.8.</strong> Csharp8</a></li><li class="chapter-item expanded "><a href="../C-sharp-docs/part9.html"><strong aria-hidden="true">3.9.</strong> Csharp9</a></li><li class="chapter-item expanded "><a href="../C-sharp-docs/part10.html"><strong aria-hidden="true">3.10.</strong> Csharp10</a></li></ol></li></ol>
            </div>
            <div id="sidebar-resize-handle" class="sidebar-resize-handle"></div>
        </nav>

        <div id="page-wrapper" class="page-wrapper">

            <div class="page">
                                <div id="menu-bar-hover-placeholder"></div>
                <div id="menu-bar" class="menu-bar sticky bordered">
                    <div class="left-buttons">
                        <button id="sidebar-toggle" class="icon-button" type="button" title="Toggle Table of Contents" aria-label="Toggle Table of Contents" aria-controls="sidebar">
                            <i class="fa fa-bars"></i>
                        </button>
                        <button id="theme-toggle" class="icon-button" type="button" title="Change theme" aria-label="Change theme" aria-haspopup="true" aria-expanded="false" aria-controls="theme-list">
                            <i class="fa fa-paint-brush"></i>
                        </button>
                        <ul id="theme-list" class="theme-popup" aria-label="Themes" role="menu">
                            <li role="none"><button role="menuitem" class="theme" id="light">Light</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="rust">Rust</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="coal">Coal</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="navy">Navy</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="ayu">Ayu</button></li>
                        </ul>
                    </div>

                    <h1 class="menu-title">KnowledgeBase</h1>

                    <div class="right-buttons">
                        <a href="../print.html" title="Print this book" aria-label="Print this book">
                            <i id="print-button" class="fa fa-print"></i>
                        </a>

                    </div>
                </div>


                <!-- Apply ARIA attributes after the sidebar and the sidebar toggle button are added to the DOM -->
                <script>
                    document.getElementById('sidebar-toggle').setAttribute('aria-expanded', sidebar === 'visible');
                    document.getElementById('sidebar').setAttribute('aria-hidden', sidebar !== 'visible');
                    Array.from(document.querySelectorAll('#sidebar a')).forEach(function(link) {
                        link.setAttribute('tabIndex', sidebar === 'visible' ? 0 : -1);
                    });
                </script>

                <div id="content" class="content">
                    <main>
                        <pre><code>.external}).
</code></pre>
<ul>
<li>
<p>An exception is now raised if <a href="index.html#std-setting-ASYNCIO_EVENT_LOOP">[<code>ASYNCIO_EVENT_LOOP</code>{.xref .std
.std-setting .docutils .literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal} has a value that does not match the
asyncio event loop actually installed (<a href="https://github.com/scrapy/scrapy/issues/5529">issue
5529</a>{.reference
.external}).</p>
</li>
<li>
<p>Fixed [<code>Headers.getlist</code>{.xref .py .py-meth .docutils .literal
.notranslate}]{.pre} returning only the last header (<a href="https://github.com/scrapy/scrapy/issues/5515">issue
5515</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/5526">issue
5526</a>{.reference
.external}).</p>
</li>
<li>
<p>Fixed <a href="index.html#scrapy.linkextractors.lxmlhtml.LxmlLinkExtractor" title="scrapy.linkextractors.lxmlhtml.LxmlLinkExtractor">[<code>LinkExtractor</code>{.xref .py .py-class .docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal} not ignoring the [<code>tar.gz</code>{.docutils .literal
.notranslate}]{.pre} file extension by default (<a href="https://github.com/scrapy/scrapy/issues/1837">issue
1837</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/2067">issue
2067</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/4066">issue
4066</a>{.reference
.external})
:::</p>
</li>
</ul>
<p>::: {#id27 .section}</p>
<h5 id="documentationheaderlink"><a class="header" href="#documentationheaderlink">Documentation<a href="#id27" title="Permalink to this heading">¶</a>{.headerlink}</a></h5>
<ul>
<li>
<p>Clarified the return type of <a href="index.html#scrapy.Spider.parse" title="scrapy.Spider.parse">[<code>Spider.parse</code>{.xref .py .py-meth
.docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal} (<a href="https://github.com/scrapy/scrapy/issues/5602">issue
5602</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/5608">issue
5608</a>{.reference
.external}).</p>
</li>
<li>
<p>To enable <a href="index.html#scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware" title="scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware">[<code>HttpCompressionMiddleware</code>{.xref .py .py-class
.docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal} to do <a href="https://www.ietf.org/rfc/rfc7932.txt">brotli
compression</a>{.reference
.external}, installing
<a href="https://github.com/google/brotli">brotli</a>{.reference .external} is
now recommended instead of installing
<a href="https://github.com/python-hyper/brotlipy/">brotlipy</a>{.reference
.external}, as the former provides a more recent version of brotli.</p>
</li>
<li>
<p><a href="index.html#topics-signals">[Signal documentation]{.std
.std-ref}</a>{.hoverxref .tooltip .reference
.internal} now mentions <a href="index.html#topics-coroutines">[coroutine support]{.std
.std-ref}</a>{.hoverxref .tooltip
.reference .internal} and uses it in code examples (<a href="https://github.com/scrapy/scrapy/issues/4852">issue
4852</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/5358">issue
5358</a>{.reference
.external}).</p>
</li>
<li>
<p><a href="index.html#bans">[Avoiding getting banned]{.std
.std-ref}</a>{.hoverxref .tooltip .reference
.internal} now recommends <a href="https://commoncrawl.org/">Common
Crawl</a>{.reference .external} instead of
<a href="http://www.googleguide.com/cached_pages.html">Google
cache</a>{.reference
.external} (<a href="https://github.com/scrapy/scrapy/issues/3582">issue
3582</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/5432">issue
5432</a>{.reference
.external}).</p>
</li>
<li>
<p>The new <a href="index.html#topics-components">[Components]{.std
.std-ref}</a>{.hoverxref .tooltip
.reference .internal} topic covers enforcing requirements on Scrapy
components, like <a href="index.html#topics-downloader-middleware">[downloader middlewares]{.std
.std-ref}</a>{.hoverxref
.tooltip .reference .internal}, <a href="index.html#topics-extensions">[extensions]{.std
.std-ref}</a>{.hoverxref .tooltip
.reference .internal}, <a href="index.html#topics-item-pipeline">[item pipelines]{.std
.std-ref}</a>{.hoverxref .tooltip
.reference .internal}, <a href="index.html#topics-spider-middleware">[spider middlewares]{.std
.std-ref}</a>{.hoverxref .tooltip
.reference .internal}, and more; <a href="index.html#enforce-asyncio-requirement">[Enforcing asyncio as a
requirement]{.std
.std-ref}</a>{.hoverxref
.tooltip .reference .internal} has also been added (<a href="https://github.com/scrapy/scrapy/issues/4978">issue
4978</a>{.reference
.external}).</p>
</li>
<li>
<p><a href="index.html#topics-settings">[Settings]{.std .std-ref}</a>{.hoverxref
.tooltip .reference .internal} now indicates that setting values
must be <a href="https://docs.python.org/3/library/pickle.html#pickle-picklable" title="(in Python v3.12)">[picklable]{.xref .std
.std-ref}</a>{.reference
.external} (<a href="https://github.com/scrapy/scrapy/issues/5607">issue
5607</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/5629">issue
5629</a>{.reference
.external}).</p>
</li>
<li>
<p>Removed outdated documentation (<a href="https://github.com/scrapy/scrapy/issues/5446">issue
5446</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/5373">issue
5373</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/5369">issue
5369</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/5370">issue
5370</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/5554">issue
5554</a>{.reference
.external}).</p>
</li>
<li>
<p>Fixed typos (<a href="https://github.com/scrapy/scrapy/issues/5442">issue
5442</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/5455">issue
5455</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/5457">issue
5457</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/5461">issue
5461</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/5538">issue
5538</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/5553">issue
5553</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/5558">issue
5558</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/5624">issue
5624</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/5631">issue
5631</a>{.reference
.external}).</p>
</li>
<li>
<p>Fixed other issues (<a href="https://github.com/scrapy/scrapy/issues/5283">issue
5283</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/5284">issue
5284</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/5559">issue
5559</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/5567">issue
5567</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/5648">issue
5648</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/5659">issue
5659</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/5665">issue
5665</a>{.reference
.external}).
:::</p>
</li>
</ul>
<p>::: {#id28 .section}</p>
<h5 id="quality-assuranceheaderlink"><a class="header" href="#quality-assuranceheaderlink">Quality assurance<a href="#id28" title="Permalink to this heading">¶</a>{.headerlink}</a></h5>
<ul>
<li>
<p>Added a continuous integration job to run <a href="https://twine.readthedocs.io/en/stable/#twine-check">twine
check</a>{.reference
.external} (<a href="https://github.com/scrapy/scrapy/issues/5655">issue
5655</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/5656">issue
5656</a>{.reference
.external}).</p>
</li>
<li>
<p>Addressed test issues and warnings (<a href="https://github.com/scrapy/scrapy/issues/5560">issue
5560</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/5561">issue
5561</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/5612">issue
5612</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/5617">issue
5617</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/5639">issue
5639</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/5645">issue
5645</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/5662">issue
5662</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/5671">issue
5671</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/5675">issue
5675</a>{.reference
.external}).</p>
</li>
<li>
<p>Cleaned up code (<a href="https://github.com/scrapy/scrapy/issues/4991">issue
4991</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/4995">issue
4995</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/5451">issue
5451</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/5487">issue
5487</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/5542">issue
5542</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/5667">issue
5667</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/5668">issue
5668</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/5672">issue
5672</a>{.reference
.external}).</p>
</li>
<li>
<p>Applied minor code improvements (<a href="https://github.com/scrapy/scrapy/issues/5661">issue
5661</a>{.reference
.external}).
:::
:::</p>
</li>
</ul>
<p>::: {#scrapy-2-6-3-2022-09-27 .section}
[]{#release-2-6-3}</p>
<h4 id="scrapy-263-2022-09-27headerlink"><a class="header" href="#scrapy-263-2022-09-27headerlink">Scrapy 2.6.3 (2022-09-27)<a href="#scrapy-2-6-3-2022-09-27" title="Permalink to this heading">¶</a>{.headerlink}</a></h4>
<ul>
<li>
<p>Added support for
<a href="https://www.pyopenssl.org/en/stable/">pyOpenSSL</a>{.reference
.external} 22.1.0, removing support for SSLv3 (<a href="https://github.com/scrapy/scrapy/issues/5634">issue
5634</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/5635">issue
5635</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/5636">issue
5636</a>{.reference
.external}).</p>
</li>
<li>
<p>Upgraded the minimum versions of the following dependencies:</p>
<ul>
<li>
<p><a href="https://cryptography.io/en/latest/">cryptography</a>{.reference
.external}: 2.0 → 3.3</p>
</li>
<li>
<p><a href="https://www.pyopenssl.org/en/stable/">pyOpenSSL</a>{.reference
.external}: 16.2.0 → 21.0.0</p>
</li>
<li>
<p><a href="https://service-identity.readthedocs.io/en/stable/">service_identity</a>{.reference
.external}: 16.0.0 → 18.1.0</p>
</li>
<li>
<p><a href="https://twistedmatrix.com/trac/">Twisted</a>{.reference
.external}: 17.9.0 → 18.9.0</p>
</li>
<li>
<p><a href="https://zopeinterface.readthedocs.io/en/latest/">zope.interface</a>{.reference
.external}: 4.1.3 → 5.0.0</p>
</li>
</ul>
<p>(<a href="https://github.com/scrapy/scrapy/issues/5621">issue
5621</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/5632">issue
5632</a>{.reference
.external})</p>
</li>
<li>
<p>Fixes test and documentation issues (<a href="https://github.com/scrapy/scrapy/issues/5612">issue
5612</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/5617">issue
5617</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/5631">issue
5631</a>{.reference
.external}).
:::</p>
</li>
</ul>
<p>::: {#scrapy-2-6-2-2022-07-25 .section}
[]{#release-2-6-2}</p>
<h4 id="scrapy-262-2022-07-25headerlink"><a class="header" href="#scrapy-262-2022-07-25headerlink">Scrapy 2.6.2 (2022-07-25)<a href="#scrapy-2-6-2-2022-07-25" title="Permalink to this heading">¶</a>{.headerlink}</a></h4>
<p><strong>Security bug fix:</strong></p>
<ul>
<li>
<p>When <a href="index.html#scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware" title="scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware">[<code>HttpProxyMiddleware</code>{.xref .py .py-class .docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal} processes a request with <a href="index.html#std-reqmeta-proxy">[<code>proxy</code>{.xref .std
.std-reqmeta .docutils .literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal} metadata, and that <a href="index.html#std-reqmeta-proxy">[<code>proxy</code>{.xref
.std .std-reqmeta .docutils .literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal} metadata includes proxy credentials,
<a href="index.html#scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware" title="scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware">[<code>HttpProxyMiddleware</code>{.xref .py .py-class .docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal} sets the [<code>Proxy-Authorization</code>{.docutils .literal
.notranslate}]{.pre} header, but only if that header is not already
set.</p>
<p>There are third-party proxy-rotation downloader middlewares that set
different <a href="index.html#std-reqmeta-proxy">[<code>proxy</code>{.xref .std .std-reqmeta .docutils .literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal} metadata every time they process a
request.</p>
<p>Because of request retries and redirects, the same request can be
processed by downloader middlewares more than once, including both
<a href="index.html#scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware" title="scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware">[<code>HttpProxyMiddleware</code>{.xref .py .py-class .docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal} and any third-party proxy-rotation downloader middleware.</p>
<p>These third-party proxy-rotation downloader middlewares could change
the <a href="index.html#std-reqmeta-proxy">[<code>proxy</code>{.xref .std .std-reqmeta .docutils .literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal} metadata of a request to a new value,
but fail to remove the [<code>Proxy-Authorization</code>{.docutils .literal
.notranslate}]{.pre} header from the previous value of the
<a href="index.html#std-reqmeta-proxy">[<code>proxy</code>{.xref .std .std-reqmeta .docutils .literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal} metadata, causing the credentials of
one proxy to be sent to a different proxy.</p>
<p>To prevent the unintended leaking of proxy credentials, the behavior
of <a href="index.html#scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware" title="scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware">[<code>HttpProxyMiddleware</code>{.xref .py .py-class .docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal} is now as follows when processing a request:</p>
<ul>
<li>
<p>If the request being processed defines <a href="index.html#std-reqmeta-proxy">[<code>proxy</code>{.xref .std
.std-reqmeta .docutils .literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal} metadata that includes
credentials, the [<code>Proxy-Authorization</code>{.docutils .literal
.notranslate}]{.pre} header is always updated to feature those
credentials.</p>
</li>
<li>
<p>If the request being processed defines <a href="index.html#std-reqmeta-proxy">[<code>proxy</code>{.xref .std
.std-reqmeta .docutils .literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal} metadata without credentials, the
[<code>Proxy-Authorization</code>{.docutils .literal .notranslate}]{.pre}
header is removed <em>unless</em> it was originally defined for the
same proxy URL.</p>
<p>To remove proxy credentials while keeping the same proxy URL,
remove the [<code>Proxy-Authorization</code>{.docutils .literal
.notranslate}]{.pre} header.</p>
</li>
<li>
<p>If the request has no <a href="index.html#std-reqmeta-proxy">[<code>proxy</code>{.xref .std .std-reqmeta
.docutils .literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal} metadata, or that metadata is a
falsy value (e.g. [<code>None</code>{.docutils .literal
.notranslate}]{.pre}), the [<code>Proxy-Authorization</code>{.docutils
.literal .notranslate}]{.pre} header is removed.</p>
<p>It is no longer possible to set a proxy URL through the
<a href="index.html#std-reqmeta-proxy">[<code>proxy</code>{.xref .std .std-reqmeta .docutils .literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal} metadata but set the credentials
through the [<code>Proxy-Authorization</code>{.docutils .literal
.notranslate}]{.pre} header. Set proxy credentials through the
<a href="index.html#std-reqmeta-proxy">[<code>proxy</code>{.xref .std .std-reqmeta .docutils .literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal} metadata instead.</p>
</li>
</ul>
</li>
</ul>
<p>Also fixes the following regressions introduced in 2.6.0:</p>
<ul>
<li>
<p><a href="index.html#scrapy.crawler.CrawlerProcess" title="scrapy.crawler.CrawlerProcess">[<code>CrawlerProcess</code>{.xref .py .py-class .docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal} supports again crawling multiple spiders (<a href="https://github.com/scrapy/scrapy/issues/5435">issue
5435</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/5436">issue
5436</a>{.reference
.external})</p>
</li>
<li>
<p>Installing a Twisted reactor before Scrapy does (e.g. importing
<a href="https://docs.twisted.org/en/stable/api/twisted.internet.reactor.html" title="(in Twisted)">[<code>twisted.internet.reactor</code>{.xref .py .py-mod .docutils .literal
.notranslate}]{.pre}</a>{.reference
.external} somewhere at the module level) no longer prevents Scrapy
from starting, as long as a different reactor is not specified in
<a href="index.html#std-setting-TWISTED_REACTOR">[<code>TWISTED_REACTOR</code>{.xref .std .std-setting .docutils .literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal} (<a href="https://github.com/scrapy/scrapy/issues/5525">issue
5525</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/5528">issue
5528</a>{.reference
.external})</p>
</li>
<li>
<p>Fixed an exception that was being logged after the spider finished
under certain conditions (<a href="https://github.com/scrapy/scrapy/issues/5437">issue
5437</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/5440">issue
5440</a>{.reference
.external})</p>
</li>
<li>
<p>The [<code>--output</code>{.docutils .literal
.notranslate}]{.pre}/[<code>-o</code>{.docutils .literal .notranslate}]{.pre}
command-line parameter supports again a value starting with a hyphen
(<a href="https://github.com/scrapy/scrapy/issues/5444">issue
5444</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/5445">issue
5445</a>{.reference
.external})</p>
</li>
<li>
<p>The [<code>scrapy</code>{.docutils .literal .notranslate}]{.pre}<code> </code>{.docutils
.literal .notranslate}[<code>parse</code>{.docutils .literal
.notranslate}]{.pre}<code> </code>{.docutils .literal
.notranslate}[<code>-h</code>{.docutils .literal .notranslate}]{.pre} command
no longer throws an error (<a href="https://github.com/scrapy/scrapy/issues/5481">issue
5481</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/5482">issue
5482</a>{.reference
.external})
:::</p>
</li>
</ul>
<p>::: {#scrapy-2-6-1-2022-03-01 .section}
[]{#release-2-6-1}</p>
<h4 id="scrapy-261-2022-03-01headerlink"><a class="header" href="#scrapy-261-2022-03-01headerlink">Scrapy 2.6.1 (2022-03-01)<a href="#scrapy-2-6-1-2022-03-01" title="Permalink to this heading">¶</a>{.headerlink}</a></h4>
<p>Fixes a regression introduced in 2.6.0 that would unset the request
method when following redirects.
:::</p>
<p>::: {#scrapy-2-6-0-2022-03-01 .section}
[]{#release-2-6-0}</p>
<h4 id="scrapy-260-2022-03-01headerlink"><a class="header" href="#scrapy-260-2022-03-01headerlink">Scrapy 2.6.0 (2022-03-01)<a href="#scrapy-2-6-0-2022-03-01" title="Permalink to this heading">¶</a>{.headerlink}</a></h4>
<p>Highlights:</p>
<ul>
<li>
<p><a href="#security-fixes">[Security fixes for cookie handling]{.std
.std-ref}</a>{.hoverxref .tooltip .reference
.internal}</p>
</li>
<li>
<p>Python 3.10 support</p>
</li>
<li>
<p><a href="index.html#using-asyncio">[asyncio support]{.std
.std-ref}</a>{.hoverxref .tooltip .reference
.internal} is no longer considered experimental, and works
out-of-the-box on Windows regardless of your Python version</p>
</li>
<li>
<p>Feed exports now support <a href="https://docs.python.org/3/library/pathlib.html#pathlib.Path" title="(in Python v3.12)">[<code>pathlib.Path</code>{.xref .py .py-class
.docutils .literal
.notranslate}]{.pre}</a>{.reference
.external} output paths and per-feed <a href="index.html#item-filter">[item filtering]{.std
.std-ref}</a>{.hoverxref .tooltip .reference
.internal} and <a href="index.html#post-processing">[post-processing]{.std
.std-ref}</a>{.hoverxref .tooltip
.reference .internal}</p>
</li>
</ul>
<p>::: {#security-bug-fixes .section}
[]{#security-fixes}</p>
<h5 id="security-bug-fixesheaderlink"><a class="header" href="#security-bug-fixesheaderlink">Security bug fixes<a href="#security-bug-fixes" title="Permalink to this heading">¶</a>{.headerlink}</a></h5>
<ul>
<li>
<p>When a <a href="index.html#scrapy.http.Request" title="scrapy.http.Request">[<code>Request</code>{.xref .py .py-class .docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal} object with cookies defined gets a redirect response
causing a new <a href="index.html#scrapy.http.Request" title="scrapy.http.Request">[<code>Request</code>{.xref .py .py-class .docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal} object to be scheduled, the cookies defined in the
original <a href="index.html#scrapy.http.Request" title="scrapy.http.Request">[<code>Request</code>{.xref .py .py-class .docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal} object are no longer copied into the new
<a href="index.html#scrapy.http.Request" title="scrapy.http.Request">[<code>Request</code>{.xref .py .py-class .docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal} object.</p>
<p>If you manually set the [<code>Cookie</code>{.docutils .literal
.notranslate}]{.pre} header on a <a href="index.html#scrapy.http.Request" title="scrapy.http.Request">[<code>Request</code>{.xref .py .py-class
.docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal} object and the domain name of the redirect URL is not an
exact match for the domain of the URL of the original
<a href="index.html#scrapy.http.Request" title="scrapy.http.Request">[<code>Request</code>{.xref .py .py-class .docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal} object, your [<code>Cookie</code>{.docutils .literal
.notranslate}]{.pre} header is now dropped from the new
<a href="index.html#scrapy.http.Request" title="scrapy.http.Request">[<code>Request</code>{.xref .py .py-class .docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal} object.</p>
<p>The old behavior could be exploited by an attacker to gain access to
your cookies. Please, see the <a href="https://github.com/scrapy/scrapy/security/advisories/GHSA-cjvr-mfj7-j4j8">cjvr-mfj7-j4j8 security
advisory</a>{.reference
.external} for more information.</p>
<p>::: {.admonition .note}
Note</p>
<p>It is still possible to enable the sharing of cookies between
different domains with a shared domain suffix (e.g.
[<code>example.com</code>{.docutils .literal .notranslate}]{.pre} and any
subdomain) by defining the shared domain suffix (e.g.
[<code>example.com</code>{.docutils .literal .notranslate}]{.pre}) as the
cookie domain when defining your cookies. See the documentation of
the <a href="index.html#scrapy.http.Request" title="scrapy.http.Request">[<code>Request</code>{.xref .py .py-class .docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal} class for more information.
:::</p>
</li>
<li>
<p>When the domain of a cookie, either received in the
[<code>Set-Cookie</code>{.docutils .literal .notranslate}]{.pre} header of a
response or defined in a <a href="index.html#scrapy.http.Request" title="scrapy.http.Request">[<code>Request</code>{.xref .py .py-class .docutils
.literal
.notranslate}]{.pre}</a>{.reference
.internal} object, is set to a <a href="https://publicsuffix.org/">public
suffix</a>{.reference .external}, the cookie
is now ignored unless the cookie domain is the same as the request
domain.</p>
<p>The old behavior could be exploited by an attacker to inject cookies
from a controlled domain into your cookiejar that could be sent to
other domains not controlled by the attacker. Please, see the
<a href="https://github.com/scrapy/scrapy/security/advisories/GHSA-mfjm-vh54-3f96">mfjm-vh54-3f96 security
advisory</a>{.reference
.external} for more information.
:::</p>
</li>
</ul>
<p>::: {#id29 .section}</p>
<h5 id="modified-requirementsheaderlink"><a class="header" href="#modified-requirementsheaderlink">Modified requirements<a href="#id29" title="Permalink to this heading">¶</a>{.headerlink}</a></h5>
<ul>
<li>The <a href="https://pypi.org/project/h2/">h2</a>{.reference .external}
dependency is now optional, only needed to <a href="index.html#http2">[enable HTTP/2
support]{.std .std-ref}</a>{.hoverxref .tooltip
.reference .internal}. (<a href="https://github.com/scrapy/scrapy/issues/5113">issue
5113</a>{.reference
.external})
:::</li>
</ul>
<p>::: {#id30 .section}</p>
<h5 id="backward-incompatible-changesheaderlink"><a class="header" href="#backward-incompatible-changesheaderlink">Backward-incompatible changes<a href="#id30" title="Permalink to this heading">¶</a>{.headerlink}</a></h5>
<ul>
<li>
<p>The [<code>formdata</code>{.docutils .literal .notranslate}]{.pre} parameter of
[<code>FormRequest</code>{.xref .py .py-class .docutils .literal
.notranslate}]{.pre}, if specified for a non-POST request, now
overrides the URL query string, instead of being appended to it.
(<a href="https://github.com/scrapy/scrapy/issues/2919">issue
2919</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/3579">issue
3579</a>{.reference
.external})</p>
</li>
<li>
<p>When a function is assigned to the <a href="index.html#std-setting-FEED_URI_PARAMS">[<code>FEED_URI_PARAMS</code>{.xref .std
.std-setting .docutils .literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal} setting, now the return value of that
function, and not the [<code>params</code>{.docutils .literal
.notranslate}]{.pre} input parameter, will determine the feed URI
parameters, unless that return value is [<code>None</code>{.docutils .literal
.notranslate}]{.pre}. (<a href="https://github.com/scrapy/scrapy/issues/4962">issue
4962</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/4966">issue
4966</a>{.reference
.external})</p>
</li>
<li>
<p>In [<code>scrapy.core.engine.ExecutionEngine</code>{.xref .py .py-class
.docutils .literal .notranslate}]{.pre}, methods [<code>crawl()</code>{.xref
.py .py-meth .docutils .literal .notranslate}]{.pre},
[<code>download()</code>{.xref .py .py-meth .docutils .literal
.notranslate}]{.pre}, [<code>schedule()</code>{.xref .py .py-meth .docutils
.literal .notranslate}]{.pre}, and [<code>spider_is_idle()</code>{.xref .py
.py-meth .docutils .literal .notranslate}]{.pre} now raise
<a href="https://docs.python.org/3/library/exceptions.html#RuntimeError" title="(in Python v3.12)">[<code>RuntimeError</code>{.xref .py .py-exc .docutils .literal
.notranslate}]{.pre}</a>{.reference
.external} if called before [<code>open_spider()</code>{.xref .py .py-meth
.docutils .literal .notranslate}]{.pre}. (<a href="https://github.com/scrapy/scrapy/issues/5090">issue
5090</a>{.reference
.external})</p>
<p>These methods used to assume that [<code>ExecutionEngine.slot</code>{.xref .py
.py-attr .docutils .literal .notranslate}]{.pre} had been defined by
a prior call to [<code>open_spider()</code>{.xref .py .py-meth .docutils
.literal .notranslate}]{.pre}, so they were raising
<a href="https://docs.python.org/3/library/exceptions.html#AttributeError" title="(in Python v3.12)">[<code>AttributeError</code>{.xref .py .py-exc .docutils .literal
.notranslate}]{.pre}</a>{.reference
.external} instead.</p>
</li>
<li>
<p>If the API of the configured <a href="index.html#topics-scheduler">[scheduler]{.std
.std-ref}</a>{.hoverxref .tooltip
.reference .internal} does not meet expectations,
<a href="https://docs.python.org/3/library/exceptions.html#TypeError" title="(in Python v3.12)">[<code>TypeError</code>{.xref .py .py-exc .docutils .literal
.notranslate}]{.pre}</a>{.reference
.external} is now raised at startup time. Before, other exceptions
would be raised at run time. (<a href="https://github.com/scrapy/scrapy/issues/3559">issue
3559</a>{.reference
.external})</p>
</li>
<li>
<p>The [<code>_encoding</code>{.docutils .literal .notranslate}]{.pre} field of
serialized <a href="index.html#scrapy.http.Request" title="scrapy.http.Request">[<code>Request</code>{.xref .py .py-class .docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal} objects is now named [<code>encoding</code>{.docutils .literal
.notranslate}]{.pre}, in line with all other fields (<a href="https://github.com/scrapy/scrapy/issues/5130">issue
5130</a>{.reference
.external})
:::</p>
</li>
</ul>
<p>::: {#id31 .section}</p>
<h5 id="deprecation-removalsheaderlink"><a class="header" href="#deprecation-removalsheaderlink">Deprecation removals<a href="#id31" title="Permalink to this heading">¶</a>{.headerlink}</a></h5>
<ul>
<li>
<p>[<code>scrapy.http.TextResponse.body_as_unicode</code>{.docutils .literal
.notranslate}]{.pre}, deprecated in Scrapy 2.2, has now been
removed. (<a href="https://github.com/scrapy/scrapy/issues/5393">issue
5393</a>{.reference
.external})</p>
</li>
<li>
<p>[<code>scrapy.item.BaseItem</code>{.docutils .literal .notranslate}]{.pre},
deprecated in Scrapy 2.2, has now been removed. (<a href="https://github.com/scrapy/scrapy/issues/5398">issue
5398</a>{.reference
.external})</p>
</li>
<li>
<p>[<code>scrapy.item.DictItem</code>{.docutils .literal .notranslate}]{.pre},
deprecated in Scrapy 1.8, has now been removed. (<a href="https://github.com/scrapy/scrapy/issues/5398">issue
5398</a>{.reference
.external})</p>
</li>
<li>
<p>[<code>scrapy.Spider.make_requests_from_url</code>{.docutils .literal
.notranslate}]{.pre}, deprecated in Scrapy 1.4, has now been
removed. (<a href="https://github.com/scrapy/scrapy/issues/4178">issue
4178</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/4356">issue
4356</a>{.reference
.external})
:::</p>
</li>
</ul>
<p>::: {#id32 .section}</p>
<h5 id="deprecationsheaderlink"><a class="header" href="#deprecationsheaderlink">Deprecations<a href="#id32" title="Permalink to this heading">¶</a>{.headerlink}</a></h5>
<ul>
<li>
<p>When a function is assigned to the <a href="index.html#std-setting-FEED_URI_PARAMS">[<code>FEED_URI_PARAMS</code>{.xref .std
.std-setting .docutils .literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal} setting, returning [<code>None</code>{.docutils
.literal .notranslate}]{.pre} or modifying the [<code>params</code>{.docutils
.literal .notranslate}]{.pre} input parameter is now deprecated.
Return a new dictionary instead. (<a href="https://github.com/scrapy/scrapy/issues/4962">issue
4962</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/4966">issue
4966</a>{.reference
.external})</p>
</li>
<li>
<p>[<code>scrapy.utils.reqser</code>{.xref .py .py-mod .docutils .literal
.notranslate}]{.pre} is deprecated. (<a href="https://github.com/scrapy/scrapy/issues/5130">issue
5130</a>{.reference
.external})</p>
<ul>
<li>
<p>Instead of [<code>request_to_dict()</code>{.xref .py .py-func .docutils
.literal .notranslate}]{.pre}, use the new
<a href="index.html#scrapy.http.Request.to_dict" title="scrapy.http.Request.to_dict">[<code>Request.to_dict</code>{.xref .py .py-meth .docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal} method.</p>
</li>
<li>
<p>Instead of [<code>request_from_dict()</code>{.xref .py .py-func .docutils
.literal .notranslate}]{.pre}, use the new
<a href="index.html#scrapy.utils.request.request_from_dict" title="scrapy.utils.request.request_from_dict">[<code>scrapy.utils.request.request_from_dict()</code>{.xref .py .py-func
.docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal} function.</p>
</li>
</ul>
</li>
<li>
<p>In [<code>scrapy.squeues</code>{.xref .py .py-mod .docutils .literal
.notranslate}]{.pre}, the following queue classes are deprecated:
[<code>PickleFifoDiskQueueNonRequest</code>{.xref .py .py-class .docutils
.literal .notranslate}]{.pre},
[<code>PickleLifoDiskQueueNonRequest</code>{.xref .py .py-class .docutils
.literal .notranslate}]{.pre},
[<code>MarshalFifoDiskQueueNonRequest</code>{.xref .py .py-class .docutils
.literal .notranslate}]{.pre}, and
[<code>MarshalLifoDiskQueueNonRequest</code>{.xref .py .py-class .docutils
.literal .notranslate}]{.pre}. You should instead use:
[<code>PickleFifoDiskQueue</code>{.xref .py .py-class .docutils .literal
.notranslate}]{.pre}, [<code>PickleLifoDiskQueue</code>{.xref .py .py-class
.docutils .literal .notranslate}]{.pre},
[<code>MarshalFifoDiskQueue</code>{.xref .py .py-class .docutils .literal
.notranslate}]{.pre}, and [<code>MarshalLifoDiskQueue</code>{.xref .py
.py-class .docutils .literal .notranslate}]{.pre}. (<a href="https://github.com/scrapy/scrapy/issues/5117">issue
5117</a>{.reference
.external})</p>
</li>
<li>
<p>Many aspects of [<code>scrapy.core.engine.ExecutionEngine</code>{.xref .py
.py-class .docutils .literal .notranslate}]{.pre} that come from a
time when this class could handle multiple <a href="index.html#scrapy.Spider" title="scrapy.Spider">[<code>Spider</code>{.xref .py
.py-class .docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal} objects at a time have been deprecated. (<a href="https://github.com/scrapy/scrapy/issues/5090">issue
5090</a>{.reference
.external})</p>
<ul>
<li>
<p>The [<code>has_capacity()</code>{.xref .py .py-meth .docutils .literal
.notranslate}]{.pre} method is deprecated.</p>
</li>
<li>
<p>The [<code>schedule()</code>{.xref .py .py-meth .docutils .literal
.notranslate}]{.pre} method is deprecated, use [<code>crawl()</code>{.xref
.py .py-meth .docutils .literal .notranslate}]{.pre} or
[<code>download()</code>{.xref .py .py-meth .docutils .literal
.notranslate}]{.pre} instead.</p>
</li>
<li>
<p>The [<code>open_spiders</code>{.xref .py .py-attr .docutils .literal
.notranslate}]{.pre} attribute is deprecated, use
[<code>spider</code>{.xref .py .py-attr .docutils .literal
.notranslate}]{.pre} instead.</p>
</li>
<li>
<p>The [<code>spider</code>{.docutils .literal .notranslate}]{.pre} parameter
is deprecated for the following methods:</p>
<ul>
<li>
<p>[<code>spider_is_idle()</code>{.xref .py .py-meth .docutils .literal
.notranslate}]{.pre}</p>
</li>
<li>
<p>[<code>crawl()</code>{.xref .py .py-meth .docutils .literal
.notranslate}]{.pre}</p>
</li>
<li>
<p>[<code>download()</code>{.xref .py .py-meth .docutils .literal
.notranslate}]{.pre}</p>
</li>
</ul>
<p>Instead, call [<code>open_spider()</code>{.xref .py .py-meth .docutils
.literal .notranslate}]{.pre} first to set the <a href="index.html#scrapy.Spider" title="scrapy.Spider">[<code>Spider</code>{.xref
.py .py-class .docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal} object.
:::</p>
</li>
</ul>
</li>
</ul>
<p>::: {#id33 .section}</p>
<h5 id="new-featuresheaderlink"><a class="header" href="#new-featuresheaderlink">New features<a href="#id33" title="Permalink to this heading">¶</a>{.headerlink}</a></h5>
<ul>
<li>
<p>You can now use <a href="index.html#item-filter">[item filtering]{.std
.std-ref}</a>{.hoverxref .tooltip .reference
.internal} to control which items are exported to each output feed.
(<a href="https://github.com/scrapy/scrapy/issues/4575">issue
4575</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/5178">issue
5178</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/5161">issue
5161</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/5203">issue
5203</a>{.reference
.external})</p>
</li>
<li>
<p>You can now apply <a href="index.html#post-processing">[post-processing]{.std
.std-ref}</a>{.hoverxref .tooltip
.reference .internal} to feeds, and <a href="index.html#builtin-plugins">[built-in post-processing
plugins]{.std .std-ref}</a>{.hoverxref
.tooltip .reference .internal} are provided for output file
compression. (<a href="https://github.com/scrapy/scrapy/issues/2174">issue
2174</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/5168">issue
5168</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/5190">issue
5190</a>{.reference
.external})</p>
</li>
<li>
<p>The <a href="index.html#std-setting-FEEDS">[<code>FEEDS</code>{.xref .std .std-setting .docutils .literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal} setting now supports
<a href="https://docs.python.org/3/library/pathlib.html#pathlib.Path" title="(in Python v3.12)">[<code>pathlib.Path</code>{.xref .py .py-class .docutils .literal
.notranslate}]{.pre}</a>{.reference
.external} objects as keys. (<a href="https://github.com/scrapy/scrapy/issues/5383">issue
5383</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/5384">issue
5384</a>{.reference
.external})</p>
</li>
<li>
<p>Enabling <a href="index.html#using-asyncio">[asyncio]{.std
.std-ref}</a>{.hoverxref .tooltip .reference
.internal} while using Windows and Python 3.8 or later will
automatically switch the asyncio event loop to one that allows
Scrapy to work. See <a href="index.html#asyncio-windows">[Windows-specific notes]{.std
.std-ref}</a>{.hoverxref .tooltip
.reference .internal}. (<a href="https://github.com/scrapy/scrapy/issues/4976">issue
4976</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/5315">issue
5315</a>{.reference
.external})</p>
</li>
<li>
<p>The <a href="index.html#std-command-genspider">[<code>genspider</code>{.xref .std .std-command .docutils .literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal} command now supports a start URL
instead of a domain name. (<a href="https://github.com/scrapy/scrapy/issues/4439">issue
4439</a>{.reference
.external})</p>
</li>
<li>
<p>[<code>scrapy.utils.defer</code>{.xref .py .py-mod .docutils .literal
.notranslate}]{.pre} gained 2 new functions,
<a href="index.html#scrapy.utils.defer.deferred_to_future" title="scrapy.utils.defer.deferred_to_future">[<code>deferred_to_future()</code>{.xref .py .py-func .docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal} and <a href="index.html#scrapy.utils.defer.maybe_deferred_to_future" title="scrapy.utils.defer.maybe_deferred_to_future">[<code>maybe_deferred_to_future()</code>{.xref .py .py-func
.docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal}, to help <a href="index.html#asyncio-await-dfd">[await on Deferreds when using the asyncio
reactor]{.std .std-ref}</a>{.hoverxref
.tooltip .reference .internal}. (<a href="https://github.com/scrapy/scrapy/issues/5288">issue
5288</a>{.reference
.external})</p>
</li>
<li>
<p><a href="index.html#topics-feed-storage-s3">[Amazon S3 feed export storage]{.std
.std-ref}</a>{.hoverxref .tooltip
.reference .internal} gained support for <a href="https://docs.aws.amazon.com/general/latest/gr/aws-sec-cred-types.html#temporary-access-keys">temporary security
credentials</a>{.reference
.external} (<a href="index.html#std-setting-AWS_SESSION_TOKEN">[<code>AWS_SESSION_TOKEN</code>{.xref .std .std-setting .docutils
.literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal}) and endpoint customization
(<a href="index.html#std-setting-AWS_ENDPOINT_URL">[<code>AWS_ENDPOINT_URL</code>{.xref .std .std-setting .docutils .literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal}). (<a href="https://github.com/scrapy/scrapy/issues/4998">issue
4998</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/5210">issue
5210</a>{.reference
.external})</p>
</li>
<li>
<p>New <a href="index.html#std-setting-LOG_FILE_APPEND">[<code>LOG_FILE_APPEND</code>{.xref .std .std-setting .docutils .literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal} setting to allow truncating the log
file. (<a href="https://github.com/scrapy/scrapy/issues/5279">issue
5279</a>{.reference
.external})</p>
</li>
<li>
<p>[<code>Request.cookies</code>{.xref .py .py-attr .docutils .literal
.notranslate}]{.pre} values that are <a href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.12)">[<code>bool</code>{.xref .py .py-class
.docutils .literal
.notranslate}]{.pre}</a>{.reference
.external}, <a href="https://docs.python.org/3/library/functions.html#float" title="(in Python v3.12)">[<code>float</code>{.xref .py .py-class .docutils .literal
.notranslate}]{.pre}</a>{.reference
.external} or <a href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.12)">[<code>int</code>{.xref .py .py-class .docutils .literal
.notranslate}]{.pre}</a>{.reference
.external} are cast to <a href="https://docs.python.org/3/library/stdtypes.html#str" title="(in Python v3.12)">[<code>str</code>{.xref .py .py-class .docutils
.literal
.notranslate}]{.pre}</a>{.reference
.external}. (<a href="https://github.com/scrapy/scrapy/issues/5252">issue
5252</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/5253">issue
5253</a>{.reference
.external})</p>
</li>
<li>
<p>You may now raise <a href="index.html#scrapy.exceptions.CloseSpider" title="scrapy.exceptions.CloseSpider">[<code>CloseSpider</code>{.xref .py .py-exc .docutils
.literal
.notranslate}]{.pre}</a>{.reference
.internal} from a handler of the <a href="index.html#std-signal-spider_idle">[<code>spider_idle</code>{.xref .std
.std-signal .docutils .literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal} signal to customize the reason why
the spider is stopping. (<a href="https://github.com/scrapy/scrapy/issues/5191">issue
5191</a>{.reference
.external})</p>
</li>
<li>
<p>When using <a href="index.html#scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware" title="scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware">[<code>HttpProxyMiddleware</code>{.xref .py .py-class .docutils
.literal
.notranslate}]{.pre}</a>{.reference
.internal}, the proxy URL for non-HTTPS HTTP/1.1 requests no longer
needs to include a URL scheme. (<a href="https://github.com/scrapy/scrapy/issues/4505">issue
4505</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/4649">issue
4649</a>{.reference
.external})</p>
</li>
<li>
<p>All built-in queues now expose a [<code>peek</code>{.docutils .literal
.notranslate}]{.pre} method that returns the next queue object (like
[<code>pop</code>{.docutils .literal .notranslate}]{.pre}) but does not remove
the returned object from the queue. (<a href="https://github.com/scrapy/scrapy/issues/5112">issue
5112</a>{.reference
.external})</p>
<p>If the underlying queue does not support peeking (e.g. because you
are not using [<code>queuelib</code>{.docutils .literal .notranslate}]{.pre}
1.6.1 or later), the [<code>peek</code>{.docutils .literal .notranslate}]{.pre}
method raises <a href="https://docs.python.org/3/library/exceptions.html#NotImplementedError" title="(in Python v3.12)">[<code>NotImplementedError</code>{.xref .py .py-exc .docutils
.literal
.notranslate}]{.pre}</a>{.reference
.external}.</p>
</li>
<li>
<p><a href="index.html#scrapy.http.Request" title="scrapy.http.Request">[<code>Request</code>{.xref .py .py-class .docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal} and <a href="index.html#scrapy.http.Response" title="scrapy.http.Response">[<code>Response</code>{.xref .py .py-class .docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal} now have an [<code>attributes</code>{.docutils .literal
.notranslate}]{.pre} attribute that makes subclassing easier. For
<a href="index.html#scrapy.http.Request" title="scrapy.http.Request">[<code>Request</code>{.xref .py .py-class .docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal}, it also allows subclasses to work with
<a href="index.html#scrapy.utils.request.request_from_dict" title="scrapy.utils.request.request_from_dict">[<code>scrapy.utils.request.request_from_dict()</code>{.xref .py .py-func
.docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal}. (<a href="https://github.com/scrapy/scrapy/issues/1877">issue
1877</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/5130">issue
5130</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/5218">issue
5218</a>{.reference
.external})</p>
</li>
<li>
<p>The <a href="index.html#scrapy.core.scheduler.BaseScheduler.open" title="scrapy.core.scheduler.BaseScheduler.open">[<code>open()</code>{.xref .py .py-meth .docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal} and <a href="index.html#scrapy.core.scheduler.BaseScheduler.close" title="scrapy.core.scheduler.BaseScheduler.close">[<code>close()</code>{.xref .py .py-meth .docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal} methods of the <a href="index.html#topics-scheduler">[scheduler]{.std
.std-ref}</a>{.hoverxref .tooltip
.reference .internal} are now optional. (<a href="https://github.com/scrapy/scrapy/issues/3559">issue
3559</a>{.reference
.external})</p>
</li>
<li>
<p>HTTP/1.1 [<code>TunnelError</code>{.xref .py .py-exc .docutils .literal
.notranslate}]{.pre} exceptions now only truncate response bodies
longer than 1000 characters, instead of those longer than 32
characters, making it easier to debug such errors. (<a href="https://github.com/scrapy/scrapy/issues/4881">issue
4881</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/5007">issue
5007</a>{.reference
.external})</p>
</li>
<li>
<p><a href="index.html#scrapy.loader.ItemLoader" title="scrapy.loader.ItemLoader">[<code>ItemLoader</code>{.xref .py .py-class .docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal} now supports non-text responses. (<a href="https://github.com/scrapy/scrapy/issues/5145">issue
5145</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/5269">issue
5269</a>{.reference
.external})
:::</p>
</li>
</ul>
<p>::: {#id34 .section}</p>
<h5 id="bug-fixesheaderlink"><a class="header" href="#bug-fixesheaderlink">Bug fixes<a href="#id34" title="Permalink to this heading">¶</a>{.headerlink}</a></h5>
<ul>
<li>
<p>The <a href="index.html#std-setting-TWISTED_REACTOR">[<code>TWISTED_REACTOR</code>{.xref .std .std-setting .docutils .literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal} and <a href="index.html#std-setting-ASYNCIO_EVENT_LOOP">[<code>ASYNCIO_EVENT_LOOP</code>{.xref .std
.std-setting .docutils .literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal} settings are no longer ignored if
defined in <a href="index.html#scrapy.Spider.custom_settings" title="scrapy.Spider.custom_settings">[<code>custom_settings</code>{.xref .py .py-attr .docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal}. (<a href="https://github.com/scrapy/scrapy/issues/4485">issue
4485</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/5352">issue
5352</a>{.reference
.external})</p>
</li>
<li>
<p>Removed a module-level Twisted reactor import that could prevent
<a href="index.html#using-asyncio">[using the asyncio reactor]{.std
.std-ref}</a>{.hoverxref .tooltip .reference
.internal}. (<a href="https://github.com/scrapy/scrapy/issues/5357">issue
5357</a>{.reference
.external})</p>
</li>
<li>
<p>The <a href="index.html#std-command-startproject">[<code>startproject</code>{.xref .std .std-command .docutils .literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal} command works with existing folders
again. (<a href="https://github.com/scrapy/scrapy/issues/4665">issue
4665</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/4676">issue
4676</a>{.reference
.external})</p>
</li>
<li>
<p>The <a href="index.html#std-setting-FEED_URI_PARAMS">[<code>FEED_URI_PARAMS</code>{.xref .std .std-setting .docutils .literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal} setting now behaves as documented.
(<a href="https://github.com/scrapy/scrapy/issues/4962">issue
4962</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/4966">issue
4966</a>{.reference
.external})</p>
</li>
<li>
<p>[<code>Request.cb_kwargs</code>{.xref .py .py-attr .docutils .literal
.notranslate}]{.pre} once again allows the [<code>callback</code>{.docutils
.literal .notranslate}]{.pre} keyword. (<a href="https://github.com/scrapy/scrapy/issues/5237">issue
5237</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/5251">issue
5251</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/5264">issue
5264</a>{.reference
.external})</p>
</li>
<li>
<p>Made [<code>scrapy.utils.response.open_in_browser()</code>{.xref .py .py-func
.docutils .literal .notranslate}]{.pre} support more complex HTML.
(<a href="https://github.com/scrapy/scrapy/issues/5319">issue
5319</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/5320">issue
5320</a>{.reference
.external})</p>
</li>
<li>
<p>Fixed <a href="index.html#scrapy.spiders.CSVFeedSpider.quotechar" title="scrapy.spiders.CSVFeedSpider.quotechar">[<code>CSVFeedSpider.quotechar</code>{.xref .py .py-attr .docutils
.literal
.notranslate}]{.pre}</a>{.reference
.internal} being interpreted as the CSV file encoding. (<a href="https://github.com/scrapy/scrapy/issues/5391">issue
5391</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/5394">issue
5394</a>{.reference
.external})</p>
</li>
<li>
<p>Added missing
<a href="https://pypi.org/project/setuptools/">setuptools</a>{.reference
.external} to the list of dependencies. (<a href="https://github.com/scrapy/scrapy/issues/5122">issue
5122</a>{.reference
.external})</p>
</li>
<li>
<p><a href="index.html#scrapy.linkextractors.lxmlhtml.LxmlLinkExtractor" title="scrapy.linkextractors.lxmlhtml.LxmlLinkExtractor">[<code>LinkExtractor</code>{.xref .py .py-class .docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal} now also works as expected with links that have
comma-separated [<code>rel</code>{.docutils .literal .notranslate}]{.pre}
attribute values including [<code>nofollow</code>{.docutils .literal
.notranslate}]{.pre}. (<a href="https://github.com/scrapy/scrapy/issues/5225">issue
5225</a>{.reference
.external})</p>
</li>
<li>
<p>Fixed a <a href="https://docs.python.org/3/library/exceptions.html#TypeError" title="(in Python v3.12)">[<code>TypeError</code>{.xref .py .py-exc .docutils .literal
.notranslate}]{.pre}</a>{.reference
.external} that could be raised during <a href="index.html#topics-feed-exports">[feed export]{.std
.std-ref}</a>{.hoverxref .tooltip
.reference .internal} parameter parsing. (<a href="https://github.com/scrapy/scrapy/issues/5359">issue
5359</a>{.reference
.external})
:::</p>
</li>
</ul>
<p>::: {#id35 .section}</p>
<h5 id="documentationheaderlink-1"><a class="header" href="#documentationheaderlink-1">Documentation<a href="#id35" title="Permalink to this heading">¶</a>{.headerlink}</a></h5>
<ul>
<li>
<p><a href="index.html#using-asyncio">[asyncio support]{.std
.std-ref}</a>{.hoverxref .tooltip .reference
.internal} is no longer considered experimental. (<a href="https://github.com/scrapy/scrapy/issues/5332">issue
5332</a>{.reference
.external})</p>
</li>
<li>
<p>Included <a href="index.html#asyncio-windows">[Windows-specific help for asyncio usage]{.std
.std-ref}</a>{.hoverxref .tooltip
.reference .internal}. (<a href="https://github.com/scrapy/scrapy/issues/4976">issue
4976</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/5315">issue
5315</a>{.reference
.external})</p>
</li>
<li>
<p>Rewrote <a href="index.html#topics-headless-browsing">[Using a headless browser]{.std
.std-ref}</a>{.hoverxref .tooltip
.reference .internal} with up-to-date best practices. (<a href="https://github.com/scrapy/scrapy/issues/4484">issue
4484</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/4613">issue
4613</a>{.reference
.external})</p>
</li>
<li>
<p>Documented <a href="index.html#topics-file-naming">[local file naming in media pipelines]{.std
.std-ref}</a>{.hoverxref .tooltip
.reference .internal}. (<a href="https://github.com/scrapy/scrapy/issues/5069">issue
5069</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/5152">issue
5152</a>{.reference
.external})</p>
</li>
<li>
<p><a href="index.html#faq">[Frequently Asked Questions]{.std
.std-ref}</a>{.hoverxref .tooltip .reference .internal}
now covers spider file name collision issues. (<a href="https://github.com/scrapy/scrapy/issues/2680">issue
2680</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/3669">issue
3669</a>{.reference
.external})</p>
</li>
<li>
<p>Provided better context and instructions to disable the
<a href="index.html#std-setting-URLLENGTH_LIMIT">[<code>URLLENGTH_LIMIT</code>{.xref .std .std-setting .docutils .literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal} setting. (<a href="https://github.com/scrapy/scrapy/issues/5135">issue
5135</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/5250">issue
5250</a>{.reference
.external})</p>
</li>
<li>
<p>Documented that <a href="index.html#reppy-parser">[Reppy parser]{.std
.std-ref}</a>{.hoverxref .tooltip .reference
.internal} does not support Python 3.9+. (<a href="https://github.com/scrapy/scrapy/issues/5226">issue
5226</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/5231">issue
5231</a>{.reference
.external})</p>
</li>
<li>
<p>Documented <a href="index.html#topics-scheduler">[the scheduler component]{.std
.std-ref}</a>{.hoverxref .tooltip
.reference .internal}. (<a href="https://github.com/scrapy/scrapy/issues/3537">issue
3537</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/3559">issue
3559</a>{.reference
.external})</p>
</li>
<li>
<p>Documented the method used by <a href="index.html#topics-media-pipeline">[media pipelines]{.std
.std-ref}</a>{.hoverxref .tooltip
.reference .internal} to <a href="index.html#file-expiration">[determine if a file has expired]{.std
.std-ref}</a>{.hoverxref .tooltip
.reference .internal}. (<a href="https://github.com/scrapy/scrapy/issues/5120">issue
5120</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/5254">issue
5254</a>{.reference
.external})</p>
</li>
<li>
<p><a href="index.html#run-multiple-spiders">[Running multiple spiders in the same process]{.std
.std-ref}</a>{.hoverxref .tooltip
.reference .internal} now features
[<code>scrapy.utils.project.get_project_settings()</code>{.xref .py .py-func
.docutils .literal .notranslate}]{.pre} usage. (<a href="https://github.com/scrapy/scrapy/issues/5070">issue
5070</a>{.reference
.external})</p>
</li>
<li>
<p><a href="index.html#run-multiple-spiders">[Running multiple spiders in the same process]{.std
.std-ref}</a>{.hoverxref .tooltip
.reference .internal} now covers what happens when you define
different per-spider values for some settings that cannot differ at
run time. (<a href="https://github.com/scrapy/scrapy/issues/4485">issue
4485</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/5352">issue
5352</a>{.reference
.external})</p>
</li>
<li>
<p>Extended the documentation of the <a href="index.html#scrapy.extensions.statsmailer.StatsMailer" title="scrapy.extensions.statsmailer.StatsMailer">[<code>StatsMailer</code>{.xref .py
.py-class .docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal} extension. (<a href="https://github.com/scrapy/scrapy/issues/5199">issue
5199</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/5217">issue
5217</a>{.reference
.external})</p>
</li>
<li>
<p>Added <a href="index.html#std-setting-JOBDIR">[<code>JOBDIR</code>{.xref .std .std-setting .docutils .literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal} to <a href="index.html#topics-settings">[Settings]{.std
.std-ref}</a>{.hoverxref .tooltip
.reference .internal}. (<a href="https://github.com/scrapy/scrapy/issues/5173">issue
5173</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/5224">issue
5224</a>{.reference
.external})</p>
</li>
<li>
<p>Documented [<code>Spider.attribute</code>{.xref .py .py-attr .docutils .literal
.notranslate}]{.pre}. (<a href="https://github.com/scrapy/scrapy/issues/5174">issue
5174</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/5244">issue
5244</a>{.reference
.external})</p>
</li>
<li>
<p>Documented <a href="index.html#scrapy.http.TextResponse.urljoin" title="scrapy.http.TextResponse.urljoin">[<code>TextResponse.urljoin</code>{.xref .py .py-attr .docutils
.literal
.notranslate}]{.pre}</a>{.reference
.internal}. (<a href="https://github.com/scrapy/scrapy/issues/1582">issue
1582</a>{.reference
.external})</p>
</li>
<li>
<p>Added the [<code>body_length</code>{.docutils .literal .notranslate}]{.pre}
parameter to the documented signature of the
<a href="index.html#std-signal-headers_received">[<code>headers_received</code>{.xref .std .std-signal .docutils .literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal} signal. (<a href="https://github.com/scrapy/scrapy/issues/5270">issue
5270</a>{.reference
.external})</p>
</li>
<li>
<p>Clarified <a href="index.html#scrapy.selector.SelectorList.get" title="scrapy.selector.SelectorList.get">[<code>SelectorList.get</code>{.xref .py .py-meth .docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal} usage in the <a href="index.html#intro-tutorial">[tutorial]{.std
.std-ref}</a>{.hoverxref .tooltip .reference
.internal}. (<a href="https://github.com/scrapy/scrapy/issues/5256">issue
5256</a>{.reference
.external})</p>
</li>
<li>
<p>The documentation now features the shortest import path of classes
with multiple import paths. (<a href="https://github.com/scrapy/scrapy/issues/2733">issue
2733</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/5099">issue
5099</a>{.reference
.external})</p>
</li>
<li>
<p>[<code>quotes.toscrape.com</code>{.docutils .literal .notranslate}]{.pre}
references now use HTTPS instead of HTTP. (<a href="https://github.com/scrapy/scrapy/issues/5395">issue
5395</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/5396">issue
5396</a>{.reference
.external})</p>
</li>
<li>
<p>Added a link to <a href="https://discord.gg/mv3yErfpvq">our Discord
server</a>{.reference .external} to
<a href="index.html#getting-help">[Getting help]{.std .std-ref}</a>{.hoverxref
.tooltip .reference .internal}. (<a href="https://github.com/scrapy/scrapy/issues/5421">issue
5421</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/5422">issue
5422</a>{.reference
.external})</p>
</li>
<li>
<p>The pronunciation of the project name is now <a href="index.html#intro-overview">[officially]{.std
.std-ref}</a>{.hoverxref .tooltip .reference
.internal} /ˈskreɪpaɪ/. (<a href="https://github.com/scrapy/scrapy/issues/5280">issue
5280</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/5281">issue
5281</a>{.reference
.external})</p>
</li>
<li>
<p>Added the Scrapy logo to the README. (<a href="https://github.com/scrapy/scrapy/issues/5255">issue
5255</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/5258">issue
5258</a>{.reference
.external})</p>
</li>
<li>
<p>Fixed issues and implemented minor improvements. (<a href="https://github.com/scrapy/scrapy/issues/3155">issue
3155</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/4335">issue
4335</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/5074">issue
5074</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/5098">issue
5098</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/5134">issue
5134</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/5180">issue
5180</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/5194">issue
5194</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/5239">issue
5239</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/5266">issue
5266</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/5271">issue
5271</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/5273">issue
5273</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/5274">issue
5274</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/5276">issue
5276</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/5347">issue
5347</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/5356">issue
5356</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/5414">issue
5414</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/5415">issue
5415</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/5416">issue
5416</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/5419">issue
5419</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/5420">issue
5420</a>{.reference
.external})
:::</p>
</li>
</ul>
<p>::: {#id36 .section}</p>
<h5 id="quality-assuranceheaderlink-1"><a class="header" href="#quality-assuranceheaderlink-1">Quality Assurance<a href="#id36" title="Permalink to this heading">¶</a>{.headerlink}</a></h5>
<ul>
<li>
<p>Added support for Python 3.10. (<a href="https://github.com/scrapy/scrapy/issues/5212">issue
5212</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/5221">issue
5221</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/5265">issue
5265</a>{.reference
.external})</p>
</li>
<li>
<p>Significantly reduced memory usage by
[<code>scrapy.utils.response.response_httprepr()</code>{.xref .py .py-func
.docutils .literal .notranslate}]{.pre}, used by the
<a href="index.html#scrapy.downloadermiddlewares.stats.DownloaderStats" title="scrapy.downloadermiddlewares.stats.DownloaderStats">[<code>DownloaderStats</code>{.xref .py .py-class .docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal} downloader middleware, which is enabled by default.
(<a href="https://github.com/scrapy/scrapy/issues/4964">issue
4964</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/4972">issue
4972</a>{.reference
.external})</p>
</li>
<li>
<p>Removed uses of the deprecated <a href="https://docs.python.org/3/library/optparse.html#module-optparse" title="(in Python v3.12)">[<code>optparse</code>{.xref .py .py-mod
.docutils .literal
.notranslate}]{.pre}</a>{.reference
.external} module. (<a href="https://github.com/scrapy/scrapy/issues/5366">issue
5366</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/5374">issue
5374</a>{.reference
.external})</p>
</li>
<li>
<p>Extended typing hints. (<a href="https://github.com/scrapy/scrapy/issues/5077">issue
5077</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/5090">issue
5090</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/5100">issue
5100</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/5108">issue
5108</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/5171">issue
5171</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/5215">issue
5215</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/5334">issue
5334</a>{.reference
.external})</p>
</li>
<li>
<p>Improved tests, fixed CI issues, removed unused code. (<a href="https://github.com/scrapy/scrapy/issues/5094">issue
5094</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/5157">issue
5157</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/5162">issue
5162</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/5198">issue
5198</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/5207">issue
5207</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/5208">issue
5208</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/5229">issue
5229</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/5298">issue
5298</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/5299">issue
5299</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/5310">issue
5310</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/5316">issue
5316</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/5333">issue
5333</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/5388">issue
5388</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/5389">issue
5389</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/5400">issue
5400</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/5401">issue
5401</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/5404">issue
5404</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/5405">issue
5405</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/5407">issue
5407</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/5410">issue
5410</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/5412">issue
5412</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/5425">issue
5425</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/5427">issue
5427</a>{.reference
.external})</p>
</li>
<li>
<p>Implemented improvements for contributors. (<a href="https://github.com/scrapy/scrapy/issues/5080">issue
5080</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/5082">issue
5082</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/5177">issue
5177</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/5200">issue
5200</a>{.reference
.external})</p>
</li>
<li>
<p>Implemented cleanups. (<a href="https://github.com/scrapy/scrapy/issues/5095">issue
5095</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/5106">issue
5106</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/5209">issue
5209</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/5228">issue
5228</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/5235">issue
5235</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/5245">issue
5245</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/5246">issue
5246</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/5292">issue
5292</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/5314">issue
5314</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/5322">issue
5322</a>{.reference
.external})
:::
:::</p>
</li>
</ul>
<p>::: {#scrapy-2-5-1-2021-10-05 .section}
[]{#release-2-5-1}</p>
<h4 id="scrapy-251-2021-10-05headerlink"><a class="header" href="#scrapy-251-2021-10-05headerlink">Scrapy 2.5.1 (2021-10-05)<a href="#scrapy-2-5-1-2021-10-05" title="Permalink to this heading">¶</a>{.headerlink}</a></h4>
<ul>
<li>
<p><strong>Security bug fix:</strong></p>
<p>If you use <a href="index.html#scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware" title="scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware">[<code>HttpAuthMiddleware</code>{.xref .py .py-class .docutils
.literal
.notranslate}]{.pre}</a>{.reference
.internal} (i.e. the [<code>http_user</code>{.docutils .literal
.notranslate}]{.pre} and [<code>http_pass</code>{.docutils .literal
.notranslate}]{.pre} spider attributes) for HTTP authentication, any
request exposes your credentials to the request target.</p>
<p>To prevent unintended exposure of authentication credentials to
unintended domains, you must now additionally set a new, additional
spider attribute, [<code>http_auth_domain</code>{.docutils .literal
.notranslate}]{.pre}, and point it to the specific domain to which
the authentication credentials must be sent.</p>
<p>If the [<code>http_auth_domain</code>{.docutils .literal .notranslate}]{.pre}
spider attribute is not set, the domain of the first request will be
considered the HTTP authentication target, and authentication
credentials will only be sent in requests targeting that domain.</p>
<p>If you need to send the same HTTP authentication credentials to
multiple domains, you can use
<a href="https://w3lib.readthedocs.io/en/latest/w3lib.html#w3lib.http.basic_auth_header" title="(in w3lib v2.1)">[<code>w3lib.http.basic_auth_header()</code>{.xref .py .py-func .docutils
.literal
.notranslate}]{.pre}</a>{.reference
.external} instead to set the value of the
[<code>Authorization</code>{.docutils .literal .notranslate}]{.pre} header of
your requests.</p>
<p>If you <em>really</em> want your spider to send the same HTTP
authentication credentials to any domain, set the
[<code>http_auth_domain</code>{.docutils .literal .notranslate}]{.pre} spider
attribute to [<code>None</code>{.docutils .literal .notranslate}]{.pre}.</p>
<p>Finally, if you are a user of
<a href="https://github.com/scrapy-plugins/scrapy-splash">scrapy-splash</a>{.reference
.external}, know that this version of Scrapy breaks compatibility
with scrapy-splash 0.7.2 and earlier. You will need to upgrade
scrapy-splash to a greater version for it to continue to work.
:::</p>
</li>
</ul>
<p>::: {#scrapy-2-5-0-2021-04-06 .section}
[]{#release-2-5-0}</p>
<h4 id="scrapy-250-2021-04-06headerlink"><a class="header" href="#scrapy-250-2021-04-06headerlink">Scrapy 2.5.0 (2021-04-06)<a href="#scrapy-2-5-0-2021-04-06" title="Permalink to this heading">¶</a>{.headerlink}</a></h4>
<p>Highlights:</p>
<ul>
<li>
<p>Official Python 3.9 support</p>
</li>
<li>
<p>Experimental <a href="index.html#http2">[HTTP/2 support]{.std
.std-ref}</a>{.hoverxref .tooltip .reference
.internal}</p>
</li>
<li>
<p>New <a href="index.html#scrapy.downloadermiddlewares.retry.get_retry_request" title="scrapy.downloadermiddlewares.retry.get_retry_request">[<code>get_retry_request()</code>{.xref .py .py-func .docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal} function to retry requests from spider callbacks</p>
</li>
<li>
<p>New <a href="index.html#scrapy.signals.headers_received" title="scrapy.signals.headers_received">[<code>headers_received</code>{.xref .py .py-class .docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal} signal that allows stopping downloads early</p>
</li>
<li>
<p>New <a href="index.html#scrapy.http.Response.protocol" title="scrapy.http.Response.protocol">[<code>Response.protocol</code>{.xref .py .py-class .docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal} attribute</p>
</li>
</ul>
<p>::: {#id37 .section}</p>
<h5 id="deprecation-removalsheaderlink-1"><a class="header" href="#deprecation-removalsheaderlink-1">Deprecation removals<a href="#id37" title="Permalink to this heading">¶</a>{.headerlink}</a></h5>
<ul>
<li>
<p>Removed all code that <a href="#id96">[was deprecated in 1.7.0]{.std
.std-ref}</a>{.hoverxref .tooltip .reference .internal} and had
not <a href="#id45">[already been removed in 2.4.0]{.std
.std-ref}</a>{.hoverxref .tooltip .reference .internal}. (<a href="https://github.com/scrapy/scrapy/issues/4901">issue
4901</a>{.reference
.external})</p>
</li>
<li>
<p>Removed support for the
[<code>SCRAPY_PICKLED_SETTINGS_TO_OVERRIDE</code>{.docutils .literal
.notranslate}]{.pre} environment variable, <a href="#id88">[deprecated in
1.8.0]{.std .std-ref}</a>{.hoverxref .tooltip .reference
.internal}. (<a href="https://github.com/scrapy/scrapy/issues/4912">issue
4912</a>{.reference
.external})
:::</p>
</li>
</ul>
<p>::: {#id38 .section}</p>
<h5 id="deprecationsheaderlink-1"><a class="header" href="#deprecationsheaderlink-1">Deprecations<a href="#id38" title="Permalink to this heading">¶</a>{.headerlink}</a></h5>
<ul>
<li>The [<code>scrapy.utils.py36</code>{.xref .py .py-mod .docutils .literal
.notranslate}]{.pre} module is now deprecated in favor of
[<code>scrapy.utils.asyncgen</code>{.xref .py .py-mod .docutils .literal
.notranslate}]{.pre}. (<a href="https://github.com/scrapy/scrapy/issues/4900">issue
4900</a>{.reference
.external})
:::</li>
</ul>
<p>::: {#id39 .section}</p>
<h5 id="new-featuresheaderlink-1"><a class="header" href="#new-featuresheaderlink-1">New features<a href="#id39" title="Permalink to this heading">¶</a>{.headerlink}</a></h5>
<ul>
<li>
<p>Experimental <a href="index.html#http2">[HTTP/2 support]{.std
.std-ref}</a>{.hoverxref .tooltip .reference
.internal} through a new download handler that can be assigned to
the [<code>https</code>{.docutils .literal .notranslate}]{.pre} protocol in the
<a href="index.html#std-setting-DOWNLOAD_HANDLERS">[<code>DOWNLOAD_HANDLERS</code>{.xref .std .std-setting .docutils .literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal} setting. (<a href="https://github.com/scrapy/scrapy/issues/1854">issue
1854</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/4769">issue
4769</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/5058">issue
5058</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/5059">issue
5059</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/5066">issue
5066</a>{.reference
.external})</p>
</li>
<li>
<p>The new
<a href="index.html#scrapy.downloadermiddlewares.retry.get_retry_request" title="scrapy.downloadermiddlewares.retry.get_retry_request">[<code>scrapy.downloadermiddlewares.retry.get_retry_request()</code>{.xref .py
.py-func .docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal} function may be used from spider callbacks or middlewares
to handle the retrying of a request beyond the scenarios that
<a href="index.html#scrapy.downloadermiddlewares.retry.RetryMiddleware" title="scrapy.downloadermiddlewares.retry.RetryMiddleware">[<code>RetryMiddleware</code>{.xref .py .py-class .docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal} supports. (<a href="https://github.com/scrapy/scrapy/issues/3590">issue
3590</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/3685">issue
3685</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/4902">issue
4902</a>{.reference
.external})</p>
</li>
<li>
<p>The new <a href="index.html#scrapy.signals.headers_received" title="scrapy.signals.headers_received">[<code>headers_received</code>{.xref .py .py-class .docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal} signal gives early access to response headers and allows
<a href="index.html#topics-stop-response-download">[stopping downloads]{.std
.std-ref}</a>{.hoverxref
.tooltip .reference .internal}. (<a href="https://github.com/scrapy/scrapy/issues/1772">issue
1772</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/4897">issue
4897</a>{.reference
.external})</p>
</li>
<li>
<p>The new <a href="index.html#scrapy.http.Response.protocol" title="scrapy.http.Response.protocol">[<code>Response.protocol</code>{.xref .py .py-attr .docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal} attribute gives access to the string that identifies the
protocol used to download a response. (<a href="https://github.com/scrapy/scrapy/issues/4878">issue
4878</a>{.reference
.external})</p>
</li>
<li>
<p><a href="index.html#topics-stats">[Stats]{.std .std-ref}</a>{.hoverxref
.tooltip .reference .internal} now include the following entries
that indicate the number of successes and failures in storing
<a href="index.html#topics-feed-exports">[feeds]{.std .std-ref}</a>{.hoverxref
.tooltip .reference .internal}:</p>
<p>::: {.highlight-default .notranslate}
::: highlight
feedexport/success_count/<storage type>
feedexport/failed_count/<storage type>
:::
:::</p>
<p>Where [<code>&lt;storage</code>{.docutils .literal
.notranslate}]{.pre}<code> </code>{.docutils .literal
.notranslate}[<code>type&gt;</code>{.docutils .literal .notranslate}]{.pre} is the
feed storage backend class name, such as [<code>FileFeedStorage</code>{.xref
.py .py-class .docutils .literal .notranslate}]{.pre} or
[<code>FTPFeedStorage</code>{.xref .py .py-class .docutils .literal
.notranslate}]{.pre}.</p>
<p>(<a href="https://github.com/scrapy/scrapy/issues/3947">issue
3947</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/4850">issue
4850</a>{.reference
.external})</p>
</li>
<li>
<p>The <a href="index.html#scrapy.spidermiddlewares.urllength.UrlLengthMiddleware" title="scrapy.spidermiddlewares.urllength.UrlLengthMiddleware">[<code>UrlLengthMiddleware</code>{.xref .py .py-class .docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal} spider middleware now logs ignored URLs with
[<code>INFO</code>{.docutils .literal .notranslate}]{.pre} <a href="https://docs.python.org/3/library/logging.html#levels" title="(in Python v3.12)">[logging
level]{.xref .std
.std-ref}</a>{.reference
.external} instead of [<code>DEBUG</code>{.docutils .literal
.notranslate}]{.pre}, and it now includes the following entry into
<a href="index.html#topics-stats">[stats]{.std .std-ref}</a>{.hoverxref
.tooltip .reference .internal} to keep track of the number of
ignored URLs:</p>
<p>::: {.highlight-default .notranslate}
::: highlight
urllength/request_ignored_count
:::
:::</p>
<p>(<a href="https://github.com/scrapy/scrapy/issues/5036">issue
5036</a>{.reference
.external})</p>
</li>
<li>
<p>The <a href="index.html#scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware" title="scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware">[<code>HttpCompressionMiddleware</code>{.xref .py .py-class .docutils
.literal
.notranslate}]{.pre}</a>{.reference
.internal} downloader middleware now logs the number of decompressed
responses and the total count of resulting bytes:</p>
<p>::: {.highlight-default .notranslate}
::: highlight
httpcompression/response_bytes
httpcompression/response_count
:::
:::</p>
<p>(<a href="https://github.com/scrapy/scrapy/issues/4797">issue
4797</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/4799">issue
4799</a>{.reference
.external})
:::</p>
</li>
</ul>
<p>::: {#id40 .section}</p>
<h5 id="bug-fixesheaderlink-1"><a class="header" href="#bug-fixesheaderlink-1">Bug fixes<a href="#id40" title="Permalink to this heading">¶</a>{.headerlink}</a></h5>
<ul>
<li>
<p>Fixed installation on PyPy installing PyDispatcher in addition to
PyPyDispatcher, which could prevent Scrapy from working depending on
which package got imported. (<a href="https://github.com/scrapy/scrapy/issues/4710">issue
4710</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/4814">issue
4814</a>{.reference
.external})</p>
</li>
<li>
<p>When inspecting a callback to check if it is a generator that also
returns a value, an exception is no longer raised if the callback
has a docstring with lower indentation than the following code.
(<a href="https://github.com/scrapy/scrapy/issues/4477">issue
4477</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/4935">issue
4935</a>{.reference
.external})</p>
</li>
<li>
<p>The
<a href="https://tools.ietf.org/html/rfc2616#section-14.13">Content-Length</a>{.reference
.external} header is no longer omitted from responses when using the
default, HTTP/1.1 download handler (see <a href="index.html#std-setting-DOWNLOAD_HANDLERS">[<code>DOWNLOAD_HANDLERS</code>{.xref
.std .std-setting .docutils .literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal}). (<a href="https://github.com/scrapy/scrapy/issues/5009">issue
5009</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/5034">issue
5034</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/5045">issue
5045</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/5057">issue
5057</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/5062">issue
5062</a>{.reference
.external})</p>
</li>
<li>
<p>Setting the <a href="index.html#std-reqmeta-handle_httpstatus_all">[<code>handle_httpstatus_all</code>{.xref .std .std-reqmeta
.docutils .literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal} request meta key to
[<code>False</code>{.docutils .literal .notranslate}]{.pre} now has the same
effect as not setting it at all, instead of having the same effect
as setting it to [<code>True</code>{.docutils .literal .notranslate}]{.pre}.
(<a href="https://github.com/scrapy/scrapy/issues/3851">issue
3851</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/4694">issue
4694</a>{.reference
.external})
:::</p>
</li>
</ul>
<p>::: {#id41 .section}</p>
<h5 id="documentationheaderlink-2"><a class="header" href="#documentationheaderlink-2">Documentation<a href="#id41" title="Permalink to this heading">¶</a>{.headerlink}</a></h5>
<ul>
<li>
<p>Added instructions to <a href="index.html#intro-install-windows">[install Scrapy in Windows using pip]{.std
.std-ref}</a>{.hoverxref .tooltip
.reference .internal}. (<a href="https://github.com/scrapy/scrapy/issues/4715">issue
4715</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/4736">issue
4736</a>{.reference
.external})</p>
</li>
<li>
<p>Logging documentation now includes <a href="index.html#topics-logging-advanced-customization">[additional ways to filter
logs]{.std
.std-ref}</a>{.hoverxref
.tooltip .reference .internal}. (<a href="https://github.com/scrapy/scrapy/issues/4216">issue
4216</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/4257">issue
4257</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/4965">issue
4965</a>{.reference
.external})</p>
</li>
<li>
<p>Covered how to deal with long lists of allowed domains in the
<a href="index.html#faq">[FAQ]{.std .std-ref}</a>{.hoverxref .tooltip
.reference .internal}. (<a href="https://github.com/scrapy/scrapy/issues/2263">issue
2263</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/3667">issue
3667</a>{.reference
.external})</p>
</li>
<li>
<p>Covered
<a href="https://github.com/scrapy/scrapy-bench">scrapy-bench</a>{.reference
.external} in <a href="index.html#benchmarking">[Benchmarking]{.std
.std-ref}</a>{.hoverxref .tooltip .reference
.internal}. (<a href="https://github.com/scrapy/scrapy/issues/4996">issue
4996</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/5016">issue
5016</a>{.reference
.external})</p>
</li>
<li>
<p>Clarified that one <a href="index.html#topics-extensions">[extension]{.std
.std-ref}</a>{.hoverxref .tooltip
.reference .internal} instance is created per crawler. (<a href="https://github.com/scrapy/scrapy/issues/5014">issue
5014</a>{.reference
.external})</p>
</li>
<li>
<p>Fixed some errors in examples. (<a href="https://github.com/scrapy/scrapy/issues/4829">issue
4829</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/4830">issue
4830</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/4907">issue
4907</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/4909">issue
4909</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/5008">issue
5008</a>{.reference
.external})</p>
</li>
<li>
<p>Fixed some external links, typos, and so on. (<a href="https://github.com/scrapy/scrapy/issues/4892">issue
4892</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/4899">issue
4899</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/4936">issue
4936</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/4942">issue
4942</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/5005">issue
5005</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/5063">issue
5063</a>{.reference
.external})</p>
</li>
<li>
<p>The <a href="index.html#topics-request-meta">[list of Request.meta keys]{.std
.std-ref}</a>{.hoverxref .tooltip
.reference .internal} is now sorted alphabetically. (<a href="https://github.com/scrapy/scrapy/issues/5061">issue
5061</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/5065">issue
5065</a>{.reference
.external})</p>
</li>
<li>
<p>Updated references to Scrapinghub, which is now called Zyte. (<a href="https://github.com/scrapy/scrapy/issues/4973">issue
4973</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/5072">issue
5072</a>{.reference
.external})</p>
</li>
<li>
<p>Added a mention to contributors in the README. (<a href="https://github.com/scrapy/scrapy/issues/4956">issue
4956</a>{.reference
.external})</p>
</li>
<li>
<p>Reduced the top margin of lists. (<a href="https://github.com/scrapy/scrapy/issues/4974">issue
4974</a>{.reference
.external})
:::</p>
</li>
</ul>
<p>::: {#id42 .section}</p>
<h5 id="quality-assuranceheaderlink-2"><a class="header" href="#quality-assuranceheaderlink-2">Quality Assurance<a href="#id42" title="Permalink to this heading">¶</a>{.headerlink}</a></h5>
<ul>
<li>
<p>Made Python 3.9 support official (<a href="https://github.com/scrapy/scrapy/issues/4757">issue
4757</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/4759">issue
4759</a>{.reference
.external})</p>
</li>
<li>
<p>Extended typing hints (<a href="https://github.com/scrapy/scrapy/issues/4895">issue
4895</a>{.reference
.external})</p>
</li>
<li>
<p>Fixed deprecated uses of the Twisted API. (<a href="https://github.com/scrapy/scrapy/issues/4940">issue
4940</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/4950">issue
4950</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/5073">issue
5073</a>{.reference
.external})</p>
</li>
<li>
<p>Made our tests run with the new pip resolver. (<a href="https://github.com/scrapy/scrapy/issues/4710">issue
4710</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/4814">issue
4814</a>{.reference
.external})</p>
</li>
<li>
<p>Added tests to ensure that <a href="index.html#coroutine-support">[coroutine support]{.std
.std-ref}</a>{.hoverxref .tooltip
.reference .internal} is tested. (<a href="https://github.com/scrapy/scrapy/issues/4987">issue
4987</a>{.reference
.external})</p>
</li>
<li>
<p>Migrated from Travis CI to GitHub Actions. (<a href="https://github.com/scrapy/scrapy/issues/4924">issue
4924</a>{.reference
.external})</p>
</li>
<li>
<p>Fixed CI issues. (<a href="https://github.com/scrapy/scrapy/issues/4986">issue
4986</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/5020">issue
5020</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/5022">issue
5022</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/5027">issue
5027</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/5052">issue
5052</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/5053">issue
5053</a>{.reference
.external})</p>
</li>
<li>
<p>Implemented code refactorings, style fixes and cleanups. (<a href="https://github.com/scrapy/scrapy/issues/4911">issue
4911</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/4982">issue
4982</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/5001">issue
5001</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/5002">issue
5002</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/5076">issue
5076</a>{.reference
.external})
:::
:::</p>
</li>
</ul>
<p>::: {#scrapy-2-4-1-2020-11-17 .section}
[]{#release-2-4-1}</p>
<h4 id="scrapy-241-2020-11-17headerlink"><a class="header" href="#scrapy-241-2020-11-17headerlink">Scrapy 2.4.1 (2020-11-17)<a href="#scrapy-2-4-1-2020-11-17" title="Permalink to this heading">¶</a>{.headerlink}</a></h4>
<ul>
<li>
<p>Fixed <a href="index.html#topics-feed-exports">[feed exports]{.std
.std-ref}</a>{.hoverxref .tooltip
.reference .internal} overwrite support (<a href="https://github.com/scrapy/scrapy/issues/4845">issue
4845</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/4857">issue
4857</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/4859">issue
4859</a>{.reference
.external})</p>
</li>
<li>
<p>Fixed the AsyncIO event loop handling, which could make code hang
(<a href="https://github.com/scrapy/scrapy/issues/4855">issue
4855</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/4872">issue
4872</a>{.reference
.external})</p>
</li>
<li>
<p>Fixed the IPv6-capable DNS resolver [<code>CachingHostnameResolver</code>{.xref
.py .py-class .docutils .literal .notranslate}]{.pre} for download
handlers that call <a href="https://docs.twisted.org/en/stable/api/twisted.internet.interfaces.IReactorCore.html#resolve" title="(in Twisted)">[<code>reactor.resolve</code>{.xref .py .py-meth .docutils
.literal
.notranslate}]{.pre}</a>{.reference
.external} (<a href="https://github.com/scrapy/scrapy/issues/4802">issue
4802</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/4803">issue
4803</a>{.reference
.external})</p>
</li>
<li>
<p>Fixed the output of the <a href="index.html#std-command-genspider">[<code>genspider</code>{.xref .std .std-command
.docutils .literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal} command showing placeholders instead
of the import path of the generated spider module (<a href="https://github.com/scrapy/scrapy/issues/4874">issue
4874</a>{.reference
.external})</p>
</li>
<li>
<p>Migrated Windows CI from Azure Pipelines to GitHub Actions (<a href="https://github.com/scrapy/scrapy/issues/4869">issue
4869</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/4876">issue
4876</a>{.reference
.external})
:::</p>
</li>
</ul>
<p>::: {#scrapy-2-4-0-2020-10-11 .section}
[]{#release-2-4-0}</p>
<h4 id="scrapy-240-2020-10-11headerlink"><a class="header" href="#scrapy-240-2020-10-11headerlink">Scrapy 2.4.0 (2020-10-11)<a href="#scrapy-2-4-0-2020-10-11" title="Permalink to this heading">¶</a>{.headerlink}</a></h4>
<p>Highlights:</p>
<ul>
<li>
<p>Python 3.5 support has been dropped.</p>
</li>
<li>
<p>The [<code>file_path</code>{.docutils .literal .notranslate}]{.pre} method of
<a href="index.html#topics-media-pipeline">[media pipelines]{.std
.std-ref}</a>{.hoverxref .tooltip
.reference .internal} can now access the source <a href="index.html#topics-items">[item]{.std
.std-ref}</a>{.hoverxref .tooltip .reference
.internal}.</p>
<p>This allows you to set a download file path based on item data.</p>
</li>
<li>
<p>The new [<code>item_export_kwargs</code>{.docutils .literal
.notranslate}]{.pre} key of the <a href="index.html#std-setting-FEEDS">[<code>FEEDS</code>{.xref .std .std-setting
.docutils .literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal} setting allows to define keyword
parameters to pass to <a href="index.html#topics-exporters">[item exporter classes]{.std
.std-ref}</a>{.hoverxref .tooltip
.reference .internal}</p>
</li>
<li>
<p>You can now choose whether <a href="index.html#topics-feed-exports">[feed exports]{.std
.std-ref}</a>{.hoverxref .tooltip
.reference .internal} overwrite or append to the output file.</p>
<p>For example, when using the <a href="index.html#std-command-crawl">[<code>crawl</code>{.xref .std .std-command
.docutils .literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal} or <a href="index.html#std-command-runspider">[<code>runspider</code>{.xref .std
.std-command .docutils .literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal} commands, you can use the
[<code>-O</code>{.docutils .literal .notranslate}]{.pre} option instead of
[<code>-o</code>{.docutils .literal .notranslate}]{.pre} to overwrite the
output file.</p>
</li>
<li>
<p>Zstd-compressed responses are now supported if
<a href="https://pypi.org/project/zstandard/">zstandard</a>{.reference
.external} is installed.</p>
</li>
<li>
<p>In settings, where the import path of a class is required, it is now
possible to pass a class object instead.</p>
</li>
</ul>
<p>::: {#id43 .section}</p>
<h5 id="modified-requirementsheaderlink-1"><a class="header" href="#modified-requirementsheaderlink-1">Modified requirements<a href="#id43" title="Permalink to this heading">¶</a>{.headerlink}</a></h5>
<ul>
<li>
<p>Python 3.6 or greater is now required; support for Python 3.5 has
been dropped</p>
<p>As a result:</p>
<ul>
<li>
<p>When using PyPy, PyPy 7.2.0 or greater <a href="index.html#faq-python-versions">[is now required]{.std
.std-ref}</a>{.hoverxref .tooltip
.reference .internal}</p>
</li>
<li>
<p>For Amazon S3 storage support in <a href="index.html#topics-feed-storage-s3">[feed exports]{.std
.std-ref}</a>{.hoverxref
.tooltip .reference .internal} or <a href="index.html#media-pipelines-s3">[media pipelines]{.std
.std-ref}</a>{.hoverxref .tooltip
.reference .internal},
<a href="https://github.com/boto/botocore">botocore</a>{.reference
.external} 1.4.87 or greater is now required</p>
</li>
<li>
<p>To use the <a href="index.html#images-pipeline">[images pipeline]{.std
.std-ref}</a>{.hoverxref .tooltip
.reference .internal},
<a href="https://python-pillow.org/">Pillow</a>{.reference .external} 4.0.0
or greater is now required</p>
</li>
</ul>
<p>(<a href="https://github.com/scrapy/scrapy/issues/4718">issue
4718</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/4732">issue
4732</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/4733">issue
4733</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/4742">issue
4742</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/4743">issue
4743</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/4764">issue
4764</a>{.reference
.external})
:::</p>
</li>
</ul>
<p>::: {#id44 .section}</p>
<h5 id="backward-incompatible-changesheaderlink-1"><a class="header" href="#backward-incompatible-changesheaderlink-1">Backward-incompatible changes<a href="#id44" title="Permalink to this heading">¶</a>{.headerlink}</a></h5>
<ul>
<li>
<p><a href="index.html#scrapy.downloadermiddlewares.cookies.CookiesMiddleware" title="scrapy.downloadermiddlewares.cookies.CookiesMiddleware">[<code>CookiesMiddleware</code>{.xref .py .py-class .docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal} once again discards cookies defined in
<a href="index.html#scrapy.http.Request.headers" title="scrapy.http.Request.headers">[<code>Request.headers</code>{.xref .py .py-attr .docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal}.</p>
<p>We decided to revert this bug fix, introduced in Scrapy 2.2.0,
because it was reported that the current implementation could break
existing code.</p>
<p>If you need to set cookies for a request, use the
<a href="index.html#scrapy.http.Request" title="scrapy.http.Request">[<code>Request.cookies</code>{.xref .py .py-class .docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal} parameter.</p>
<p>A future version of Scrapy will include a new, better implementation
of the reverted bug fix.</p>
<p>(<a href="https://github.com/scrapy/scrapy/issues/4717">issue
4717</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/4823">issue
4823</a>{.reference
.external})
:::</p>
</li>
</ul>
<p>::: {#id45 .section}
[]{#id46}</p>
<h5 id="deprecation-removalsheaderlink-2"><a class="header" href="#deprecation-removalsheaderlink-2">Deprecation removals<a href="#id45" title="Permalink to this heading">¶</a>{.headerlink}</a></h5>
<ul>
<li>
<p>[<code>scrapy.extensions.feedexport.S3FeedStorage</code>{.xref .py .py-class
.docutils .literal .notranslate}]{.pre} no longer reads the values
of [<code>access_key</code>{.docutils .literal .notranslate}]{.pre} and
[<code>secret_key</code>{.docutils .literal .notranslate}]{.pre} from the
running project settings when they are not passed to its
[<code>__init__</code>{.docutils .literal .notranslate}]{.pre} method; you must
either pass those parameters to its [<code>__init__</code>{.docutils .literal
.notranslate}]{.pre} method or use
[<code>S3FeedStorage.from_crawler</code>{.xref .py .py-class .docutils .literal
.notranslate}]{.pre} (<a href="https://github.com/scrapy/scrapy/issues/4356">issue
4356</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/4411">issue
4411</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/4688">issue
4688</a>{.reference
.external})</p>
</li>
<li>
<p>[<code>Rule.process_request</code>{.xref .py .py-attr .docutils .literal
.notranslate}]{.pre} no longer admits callables which expect a
single [<code>request</code>{.docutils .literal .notranslate}]{.pre} parameter,
rather than both [<code>request</code>{.docutils .literal .notranslate}]{.pre}
and [<code>response</code>{.docutils .literal .notranslate}]{.pre} (<a href="https://github.com/scrapy/scrapy/issues/4818">issue
4818</a>{.reference
.external})
:::</p>
</li>
</ul>
<p>::: {#id47 .section}</p>
<h5 id="deprecationsheaderlink-2"><a class="header" href="#deprecationsheaderlink-2">Deprecations<a href="#id47" title="Permalink to this heading">¶</a>{.headerlink}</a></h5>
<ul>
<li>
<p>In custom <a href="index.html#topics-media-pipeline">[media pipelines]{.std
.std-ref}</a>{.hoverxref .tooltip
.reference .internal}, signatures that do not accept a keyword-only
[<code>item</code>{.docutils .literal .notranslate}]{.pre} parameter in any of
the methods that <a href="#media-pipeline-item-parameter">[now support this parameter]{.std
.std-ref}</a>{.hoverxref .tooltip
.reference .internal} are now deprecated (<a href="https://github.com/scrapy/scrapy/issues/4628">issue
4628</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/4686">issue
4686</a>{.reference
.external})</p>
</li>
<li>
<p>In custom <a href="index.html#topics-feed-storage">[feed storage backend classes]{.std
.std-ref}</a>{.hoverxref .tooltip
.reference .internal}, [<code>__init__</code>{.docutils .literal
.notranslate}]{.pre} method signatures that do not accept a
keyword-only [<code>feed_options</code>{.docutils .literal .notranslate}]{.pre}
parameter are now deprecated (<a href="https://github.com/scrapy/scrapy/issues/547">issue
547</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/716">issue
716</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/4512">issue
4512</a>{.reference
.external})</p>
</li>
<li>
<p>The [<code>scrapy.utils.python.WeakKeyCache</code>{.xref .py .py-class
.docutils .literal .notranslate}]{.pre} class is now deprecated
(<a href="https://github.com/scrapy/scrapy/issues/4684">issue
4684</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/4701">issue
4701</a>{.reference
.external})</p>
</li>
<li>
<p>The [<code>scrapy.utils.boto.is_botocore()</code>{.xref .py .py-func .docutils
.literal .notranslate}]{.pre} function is now deprecated, use
[<code>scrapy.utils.boto.is_botocore_available()</code>{.xref .py .py-func
.docutils .literal .notranslate}]{.pre} instead (<a href="https://github.com/scrapy/scrapy/issues/4734">issue
4734</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/4776">issue
4776</a>{.reference
.external})
:::</p>
</li>
</ul>
<p>::: {#id48 .section}</p>
<h5 id="new-featuresheaderlink-2"><a class="header" href="#new-featuresheaderlink-2">New features<a href="#id48" title="Permalink to this heading">¶</a>{.headerlink}</a></h5>
<ul>
<li>
<p>The following methods of <a href="index.html#topics-media-pipeline">[media pipelines]{.std
.std-ref}</a>{.hoverxref .tooltip
.reference .internal} now accept an [<code>item</code>{.docutils .literal
.notranslate}]{.pre} keyword-only parameter containing the source
<a href="index.html#topics-items">[item]{.std .std-ref}</a>{.hoverxref .tooltip
.reference .internal}:</p>
<ul>
<li>
<p>In <a href="index.html#scrapy.pipelines.files.FilesPipeline" title="scrapy.pipelines.files.FilesPipeline">[<code>scrapy.pipelines.files.FilesPipeline</code>{.xref .py .py-class
.docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal}:</p>
<ul>
<li>
<p>[<code>file_downloaded()</code>{.xref .py .py-meth .docutils .literal
.notranslate}]{.pre}</p>
</li>
<li>
<p><a href="index.html#scrapy.pipelines.files.FilesPipeline.file_path" title="scrapy.pipelines.files.FilesPipeline.file_path">[<code>file_path()</code>{.xref .py .py-meth .docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal}</p>
</li>
<li>
<p>[<code>media_downloaded()</code>{.xref .py .py-meth .docutils .literal
.notranslate}]{.pre}</p>
</li>
<li>
<p>[<code>media_to_download()</code>{.xref .py .py-meth .docutils .literal
.notranslate}]{.pre}</p>
</li>
</ul>
</li>
<li>
<p>In <a href="index.html#scrapy.pipelines.images.ImagesPipeline" title="scrapy.pipelines.images.ImagesPipeline">[<code>scrapy.pipelines.images.ImagesPipeline</code>{.xref .py
.py-class .docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal}:</p>
<ul>
<li>
<p>[<code>file_downloaded()</code>{.xref .py .py-meth .docutils .literal
.notranslate}]{.pre}</p>
</li>
<li>
<p><a href="index.html#scrapy.pipelines.images.ImagesPipeline.file_path" title="scrapy.pipelines.images.ImagesPipeline.file_path">[<code>file_path()</code>{.xref .py .py-meth .docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal}</p>
</li>
<li>
<p>[<code>get_images()</code>{.xref .py .py-meth .docutils .literal
.notranslate}]{.pre}</p>
</li>
<li>
<p>[<code>image_downloaded()</code>{.xref .py .py-meth .docutils .literal
.notranslate}]{.pre}</p>
</li>
<li>
<p>[<code>media_downloaded()</code>{.xref .py .py-meth .docutils .literal
.notranslate}]{.pre}</p>
</li>
<li>
<p>[<code>media_to_download()</code>{.xref .py .py-meth .docutils .literal
.notranslate}]{.pre}</p>
</li>
</ul>
</li>
</ul>
<p>(<a href="https://github.com/scrapy/scrapy/issues/4628">issue
4628</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/4686">issue
4686</a>{.reference
.external})</p>
</li>
<li>
<p>The new [<code>item_export_kwargs</code>{.docutils .literal
.notranslate}]{.pre} key of the <a href="index.html#std-setting-FEEDS">[<code>FEEDS</code>{.xref .std .std-setting
.docutils .literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal} setting allows to define keyword
parameters to pass to <a href="index.html#topics-exporters">[item exporter classes]{.std
.std-ref}</a>{.hoverxref .tooltip
.reference .internal} (<a href="https://github.com/scrapy/scrapy/issues/4606">issue
4606</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/4768">issue
4768</a>{.reference
.external})</p>
</li>
<li>
<p><a href="index.html#topics-feed-exports">[Feed exports]{.std
.std-ref}</a>{.hoverxref .tooltip
.reference .internal} gained overwrite support:</p>
<ul>
<li>
<p>When using the <a href="index.html#std-command-crawl">[<code>crawl</code>{.xref .std .std-command .docutils
.literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal} or <a href="index.html#std-command-runspider">[<code>runspider</code>{.xref .std
.std-command .docutils .literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal} commands, you can use the
[<code>-O</code>{.docutils .literal .notranslate}]{.pre} option instead of
[<code>-o</code>{.docutils .literal .notranslate}]{.pre} to overwrite the
output file</p>
</li>
<li>
<p>You can use the [<code>overwrite</code>{.docutils .literal
.notranslate}]{.pre} key in the <a href="index.html#std-setting-FEEDS">[<code>FEEDS</code>{.xref .std
.std-setting .docutils .literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal} setting to configure whether to
overwrite the output file ([<code>True</code>{.docutils .literal
.notranslate}]{.pre}) or append to its content
([<code>False</code>{.docutils .literal .notranslate}]{.pre})</p>
</li>
<li>
<p>The [<code>__init__</code>{.docutils .literal .notranslate}]{.pre} and
[<code>from_crawler</code>{.docutils .literal .notranslate}]{.pre} methods
of <a href="index.html#topics-feed-storage">[feed storage backend classes]{.std
.std-ref}</a>{.hoverxref .tooltip
.reference .internal} now receive a new keyword-only parameter,
[<code>feed_options</code>{.docutils .literal .notranslate}]{.pre}, which
is a dictionary of <a href="index.html#feed-options">[feed options]{.std
.std-ref}</a>{.hoverxref .tooltip
.reference .internal}</p>
</li>
</ul>
<p>(<a href="https://github.com/scrapy/scrapy/issues/547">issue 547</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/716">issue
716</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/4512">issue
4512</a>{.reference
.external})</p>
</li>
<li>
<p>Zstd-compressed responses are now supported if
<a href="https://pypi.org/project/zstandard/">zstandard</a>{.reference
.external} is installed (<a href="https://github.com/scrapy/scrapy/issues/4831">issue
4831</a>{.reference
.external})</p>
</li>
<li>
<p>In settings, where the import path of a class is required, it is now
possible to pass a class object instead (<a href="https://github.com/scrapy/scrapy/issues/3870">issue
3870</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/3873">issue
3873</a>{.reference
.external}).</p>
<p>This includes also settings where only part of its value is made of
an import path, such as <a href="index.html#std-setting-DOWNLOADER_MIDDLEWARES">[<code>DOWNLOADER_MIDDLEWARES</code>{.xref .std
.std-setting .docutils .literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal} or <a href="index.html#std-setting-DOWNLOAD_HANDLERS">[<code>DOWNLOAD_HANDLERS</code>{.xref .std
.std-setting .docutils .literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal}.</p>
</li>
<li>
<p><a href="index.html#topics-downloader-middleware">[Downloader middlewares]{.std
.std-ref}</a>{.hoverxref
.tooltip .reference .internal} can now override
<a href="index.html#scrapy.http.Response.request" title="scrapy.http.Response.request">[<code>response.request</code>{.xref .py .py-class .docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal}.</p>
<p>If a <a href="index.html#topics-downloader-middleware">[downloader middleware]{.std
.std-ref}</a>{.hoverxref
.tooltip .reference .internal} returns a <a href="index.html#scrapy.http.Response" title="scrapy.http.Response">[<code>Response</code>{.xref .py
.py-class .docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal} object from <a href="index.html#scrapy.downloadermiddlewares.DownloaderMiddleware.process_response" title="scrapy.downloadermiddlewares.DownloaderMiddleware.process_response">[<code>process_response()</code>{.xref .py .py-meth
.docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal} or <a href="index.html#scrapy.downloadermiddlewares.DownloaderMiddleware.process_exception" title="scrapy.downloadermiddlewares.DownloaderMiddleware.process_exception">[<code>process_exception()</code>{.xref .py .py-meth .docutils
.literal
.notranslate}]{.pre}</a>{.reference
.internal} with a custom <a href="index.html#scrapy.http.Request" title="scrapy.http.Request">[<code>Request</code>{.xref .py .py-class .docutils
.literal
.notranslate}]{.pre}</a>{.reference
.internal} object assigned to <a href="index.html#scrapy.http.Response.request" title="scrapy.http.Response.request">[<code>response.request</code>{.xref .py
.py-class .docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal}:</p>
<ul>
<li>
<p>The response is handled by the callback of that custom
<a href="index.html#scrapy.http.Request" title="scrapy.http.Request">[<code>Request</code>{.xref .py .py-class .docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal} object, instead of being handled by the callback of
the original <a href="index.html#scrapy.http.Request" title="scrapy.http.Request">[<code>Request</code>{.xref .py .py-class .docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal} object</p>
</li>
<li>
<p>That custom <a href="index.html#scrapy.http.Request" title="scrapy.http.Request">[<code>Request</code>{.xref .py .py-class .docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal} object is now sent as the [<code>request</code>{.docutils
.literal .notranslate}]{.pre} argument to the
<a href="index.html#std-signal-response_received">[<code>response_received</code>{.xref .std .std-signal .docutils .literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal} signal, instead of the original
<a href="index.html#scrapy.http.Request" title="scrapy.http.Request">[<code>Request</code>{.xref .py .py-class .docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal} object</p>
</li>
</ul>
<p>(<a href="https://github.com/scrapy/scrapy/issues/4529">issue
4529</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/4632">issue
4632</a>{.reference
.external})</p>
</li>
<li>
<p>When using the <a href="index.html#topics-feed-storage-ftp">[FTP feed storage backend]{.std
.std-ref}</a>{.hoverxref .tooltip
.reference .internal}:</p>
<ul>
<li>
<p>It is now possible to set the new [<code>overwrite</code>{.docutils
.literal .notranslate}]{.pre} <a href="index.html#feed-options">[feed option]{.std
.std-ref}</a>{.hoverxref .tooltip
.reference .internal} to [<code>False</code>{.docutils .literal
.notranslate}]{.pre} to append to an existing file instead of
overwriting it</p>
</li>
<li>
<p>The FTP password can now be omitted if it is not necessary</p>
</li>
</ul>
<p>(<a href="https://github.com/scrapy/scrapy/issues/547">issue 547</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/716">issue
716</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/4512">issue
4512</a>{.reference
.external})</p>
</li>
<li>
<p>The [<code>__init__</code>{.docutils .literal .notranslate}]{.pre} method of
<a href="index.html#scrapy.exporters.CsvItemExporter" title="scrapy.exporters.CsvItemExporter">[<code>CsvItemExporter</code>{.xref .py .py-class .docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal} now supports an [<code>errors</code>{.docutils .literal
.notranslate}]{.pre} parameter to indicate how to handle encoding
errors (<a href="https://github.com/scrapy/scrapy/issues/4755">issue
4755</a>{.reference
.external})</p>
</li>
<li>
<p>When <a href="index.html#using-asyncio">[using asyncio]{.std
.std-ref}</a>{.hoverxref .tooltip .reference
.internal}, it is now possible to <a href="index.html#using-custom-loops">[set a custom asyncio loop]{.std
.std-ref}</a>{.hoverxref .tooltip
.reference .internal} (<a href="https://github.com/scrapy/scrapy/issues/4306">issue
4306</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/4414">issue
4414</a>{.reference
.external})</p>
</li>
<li>
<p>Serialized requests (see <a href="index.html#topics-jobs">[Jobs: pausing and resuming crawls]{.std
.std-ref}</a>{.hoverxref .tooltip .reference
.internal}) now support callbacks that are spider methods that
delegate on other callable (<a href="https://github.com/scrapy/scrapy/issues/4756">issue
4756</a>{.reference
.external})</p>
</li>
<li>
<p>When a response is larger than <a href="index.html#std-setting-DOWNLOAD_MAXSIZE">[<code>DOWNLOAD_MAXSIZE</code>{.xref .std
.std-setting .docutils .literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal}, the logged message is now a warning,
instead of an error (<a href="https://github.com/scrapy/scrapy/issues/3874">issue
3874</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/3886">issue
3886</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/4752">issue
4752</a>{.reference
.external})
:::</p>
</li>
</ul>
<p>::: {#id49 .section}</p>
<h5 id="bug-fixesheaderlink-2"><a class="header" href="#bug-fixesheaderlink-2">Bug fixes<a href="#id49" title="Permalink to this heading">¶</a>{.headerlink}</a></h5>
<ul>
<li>
<p>The <a href="index.html#std-command-genspider">[<code>genspider</code>{.xref .std .std-command .docutils .literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal} command no longer overwrites existing
files unless the [<code>--force</code>{.docutils .literal .notranslate}]{.pre}
option is used (<a href="https://github.com/scrapy/scrapy/issues/4561">issue
4561</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/4616">issue
4616</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/4623">issue
4623</a>{.reference
.external})</p>
</li>
<li>
<p>Cookies with an empty value are no longer considered invalid cookies
(<a href="https://github.com/scrapy/scrapy/issues/4772">issue
4772</a>{.reference
.external})</p>
</li>
<li>
<p>The <a href="index.html#std-command-runspider">[<code>runspider</code>{.xref .std .std-command .docutils .literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal} command now supports files with the
[<code>.pyw</code>{.docutils .literal .notranslate}]{.pre} file extension
(<a href="https://github.com/scrapy/scrapy/issues/4643">issue
4643</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/4646">issue
4646</a>{.reference
.external})</p>
</li>
<li>
<p>The <a href="index.html#scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware" title="scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware">[<code>HttpProxyMiddleware</code>{.xref .py .py-class .docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal} middleware now simply ignores unsupported proxy values
(<a href="https://github.com/scrapy/scrapy/issues/3331">issue
3331</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/4778">issue
4778</a>{.reference
.external})</p>
</li>
<li>
<p>Checks for generator callbacks with a [<code>return</code>{.docutils .literal
.notranslate}]{.pre} statement no longer warn about
[<code>return</code>{.docutils .literal .notranslate}]{.pre} statements in
nested functions (<a href="https://github.com/scrapy/scrapy/issues/4720">issue
4720</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/4721">issue
4721</a>{.reference
.external})</p>
</li>
<li>
<p>The system file mode creation mask no longer affects the permissions
of files generated using the <a href="index.html#std-command-startproject">[<code>startproject</code>{.xref .std
.std-command .docutils .literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal} command (<a href="https://github.com/scrapy/scrapy/issues/4722">issue
4722</a>{.reference
.external})</p>
</li>
<li>
<p>[<code>scrapy.utils.iterators.xmliter()</code>{.xref .py .py-func .docutils
.literal .notranslate}]{.pre} now supports namespaced node names
(<a href="https://github.com/scrapy/scrapy/issues/861">issue 861</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/4746">issue
4746</a>{.reference
.external})</p>
</li>
<li>
<p>[<code>Request</code>{.xref .py .py-class .docutils .literal
.notranslate}]{.pre} objects can now have [<code>about:</code>{.docutils
.literal .notranslate}]{.pre} URLs, which can work when using a
headless browser (<a href="https://github.com/scrapy/scrapy/issues/4835">issue
4835</a>{.reference
.external})
:::</p>
</li>
</ul>
<p>::: {#id50 .section}</p>
<h5 id="documentationheaderlink-3"><a class="header" href="#documentationheaderlink-3">Documentation<a href="#id50" title="Permalink to this heading">¶</a>{.headerlink}</a></h5>
<ul>
<li>
<p>The <a href="index.html#std-setting-FEED_URI_PARAMS">[<code>FEED_URI_PARAMS</code>{.xref .std .std-setting .docutils .literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal} setting is now documented (<a href="https://github.com/scrapy/scrapy/issues/4671">issue
4671</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/4724">issue
4724</a>{.reference
.external})</p>
</li>
<li>
<p>Improved the documentation of <a href="index.html#topics-link-extractors">[link extractors]{.std
.std-ref}</a>{.hoverxref .tooltip
.reference .internal} with an usage example from a spider callback
and reference documentation for the <a href="index.html#scrapy.link.Link" title="scrapy.link.Link">[<code>Link</code>{.xref .py .py-class
.docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal} class (<a href="https://github.com/scrapy/scrapy/issues/4751">issue
4751</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/4775">issue
4775</a>{.reference
.external})</p>
</li>
<li>
<p>Clarified the impact of <a href="index.html#std-setting-CONCURRENT_REQUESTS">[<code>CONCURRENT_REQUESTS</code>{.xref .std
.std-setting .docutils .literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal} when using the <a href="index.html#scrapy.extensions.closespider.CloseSpider" title="scrapy.extensions.closespider.CloseSpider">[<code>CloseSpider</code>{.xref
.py .py-class .docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal} extension (<a href="https://github.com/scrapy/scrapy/issues/4836">issue
4836</a>{.reference
.external})</p>
</li>
<li>
<p>Removed references to Python 2's [<code>unicode</code>{.docutils .literal
.notranslate}]{.pre} type (<a href="https://github.com/scrapy/scrapy/issues/4547">issue
4547</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/4703">issue
4703</a>{.reference
.external})</p>
</li>
<li>
<p>We now have an <a href="index.html#deprecation-policy">[official deprecation policy]{.std
.std-ref}</a>{.hoverxref .tooltip
.reference .internal} (<a href="https://github.com/scrapy/scrapy/issues/4705">issue
4705</a>{.reference
.external})</p>
</li>
<li>
<p>Our <a href="index.html#documentation-policies">[documentation policies]{.std
.std-ref}</a>{.hoverxref .tooltip
.reference .internal} now cover usage of Sphinx's
<a href="https://www.sphinx-doc.org/en/master/usage/restructuredtext/directives.html#directive-versionadded" title="(in Sphinx v7.3.0)">[<code>versionadded</code>{.xref .rst .rst-dir .docutils .literal
.notranslate}]{.pre}</a>{.reference
.external} and <a href="https://www.sphinx-doc.org/en/master/usage/restructuredtext/directives.html#directive-versionchanged" title="(in Sphinx v7.3.0)">[<code>versionchanged</code>{.xref .rst .rst-dir .docutils
.literal
.notranslate}]{.pre}</a>{.reference
.external} directives, and we have removed usages referencing Scrapy
1.4.0 and earlier versions (<a href="https://github.com/scrapy/scrapy/issues/3971">issue
3971</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/4310">issue
4310</a>{.reference
.external})</p>
</li>
<li>
<p>Other documentation cleanups (<a href="https://github.com/scrapy/scrapy/issues/4090">issue
4090</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/4782">issue
4782</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/4800">issue
4800</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/4801">issue
4801</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/4809">issue
4809</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/4816">issue
4816</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/4825">issue
4825</a>{.reference
.external})
:::</p>
</li>
</ul>
<p>::: {#id51 .section}</p>
<h5 id="quality-assuranceheaderlink-3"><a class="header" href="#quality-assuranceheaderlink-3">Quality assurance<a href="#id51" title="Permalink to this heading">¶</a>{.headerlink}</a></h5>
<ul>
<li>
<p>Extended typing hints (<a href="https://github.com/scrapy/scrapy/issues/4243">issue
4243</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/4691">issue
4691</a>{.reference
.external})</p>
</li>
<li>
<p>Added tests for the <a href="index.html#std-command-check">[<code>check</code>{.xref .std .std-command .docutils
.literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal} command (<a href="https://github.com/scrapy/scrapy/issues/4663">issue
4663</a>{.reference
.external})</p>
</li>
<li>
<p>Fixed test failures on Debian (<a href="https://github.com/scrapy/scrapy/issues/4726">issue
4726</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/4727">issue
4727</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/4735">issue
4735</a>{.reference
.external})</p>
</li>
<li>
<p>Improved Windows test coverage (<a href="https://github.com/scrapy/scrapy/issues/4723">issue
4723</a>{.reference
.external})</p>
</li>
<li>
<p>Switched to <a href="https://docs.python.org/3/reference/lexical_analysis.html#f-strings" title="(in Python v3.12)">[formatted string literals]{.xref .std
.std-ref}</a>{.reference
.external} where possible (<a href="https://github.com/scrapy/scrapy/issues/4307">issue
4307</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/4324">issue
4324</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/4672">issue
4672</a>{.reference
.external})</p>
</li>
<li>
<p>Modernized [<code>super()</code>{.xref .py .py-func .docutils .literal
.notranslate}]{.pre} usage (<a href="https://github.com/scrapy/scrapy/issues/4707">issue
4707</a>{.reference
.external})</p>
</li>
<li>
<p>Other code and test cleanups (<a href="https://github.com/scrapy/scrapy/issues/1790">issue
1790</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/3288">issue
3288</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/4165">issue
4165</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/4564">issue
4564</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/4651">issue
4651</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/4714">issue
4714</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/4738">issue
4738</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/4745">issue
4745</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/4747">issue
4747</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/4761">issue
4761</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/4765">issue
4765</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/4804">issue
4804</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/4817">issue
4817</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/4820">issue
4820</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/4822">issue
4822</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/4839">issue
4839</a>{.reference
.external})
:::
:::</p>
</li>
</ul>
<p>::: {#scrapy-2-3-0-2020-08-04 .section}
[]{#release-2-3-0}</p>
<h4 id="scrapy-230-2020-08-04headerlink"><a class="header" href="#scrapy-230-2020-08-04headerlink">Scrapy 2.3.0 (2020-08-04)<a href="#scrapy-2-3-0-2020-08-04" title="Permalink to this heading">¶</a>{.headerlink}</a></h4>
<p>Highlights:</p>
<ul>
<li>
<p><a href="index.html#topics-feed-exports">[Feed exports]{.std
.std-ref}</a>{.hoverxref .tooltip
.reference .internal} now support <a href="index.html#topics-feed-storage-gcs">[Google Cloud Storage]{.std
.std-ref}</a>{.hoverxref .tooltip
.reference .internal} as a storage backend</p>
</li>
<li>
<p>The new <a href="index.html#std-setting-FEED_EXPORT_BATCH_ITEM_COUNT">[<code>FEED_EXPORT_BATCH_ITEM_COUNT</code>{.xref .std .std-setting
.docutils .literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal} setting allows to deliver output
items in batches of up to the specified number of items.</p>
<p>It also serves as a workaround for <a href="index.html#delayed-file-delivery">[delayed file delivery]{.std
.std-ref}</a>{.hoverxref .tooltip
.reference .internal}, which causes Scrapy to only start item
delivery after the crawl has finished when using certain storage
backends (<a href="index.html#topics-feed-storage-s3">[S3]{.std
.std-ref}</a>{.hoverxref .tooltip
.reference .internal}, <a href="index.html#topics-feed-storage-ftp">[FTP]{.std
.std-ref}</a>{.hoverxref .tooltip
.reference .internal}, and now <a href="index.html#topics-feed-storage-gcs">[GCS]{.std
.std-ref}</a>{.hoverxref .tooltip
.reference .internal}).</p>
</li>
<li>
<p>The base implementation of <a href="index.html#topics-loaders">[item loaders]{.std
.std-ref}</a>{.hoverxref .tooltip .reference
.internal} has been moved into a separate library,
<a href="https://itemloaders.readthedocs.io/en/latest/index.html" title="(in itemloaders)">[itemloaders]{.xref .std
.std-doc}</a>{.reference
.external}, allowing usage from outside Scrapy and a separate
release schedule</p>
</li>
</ul>
<p>::: {#id52 .section}</p>
<h5 id="deprecation-removalsheaderlink-3"><a class="header" href="#deprecation-removalsheaderlink-3">Deprecation removals<a href="#id52" title="Permalink to this heading">¶</a>{.headerlink}</a></h5>
<ul>
<li>
<p>Removed the following classes and their parent modules from
[<code>scrapy.linkextractors</code>{.docutils .literal .notranslate}]{.pre}:</p>
<ul>
<li>
<p>[<code>htmlparser.HtmlParserLinkExtractor</code>{.docutils .literal
.notranslate}]{.pre}</p>
</li>
<li>
<p>[<code>regex.RegexLinkExtractor</code>{.docutils .literal
.notranslate}]{.pre}</p>
</li>
<li>
<p>[<code>sgml.BaseSgmlLinkExtractor</code>{.docutils .literal
.notranslate}]{.pre}</p>
</li>
<li>
<p>[<code>sgml.SgmlLinkExtractor</code>{.docutils .literal
.notranslate}]{.pre}</p>
</li>
</ul>
<p>Use <a href="index.html#scrapy.linkextractors.lxmlhtml.LxmlLinkExtractor" title="scrapy.linkextractors.lxmlhtml.LxmlLinkExtractor">[<code>LinkExtractor</code>{.xref .py .py-class .docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal} instead (<a href="https://github.com/scrapy/scrapy/issues/4356">issue
4356</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/4679">issue
4679</a>{.reference
.external})
:::</p>
</li>
</ul>
<p>::: {#id53 .section}</p>
<h5 id="deprecationsheaderlink-3"><a class="header" href="#deprecationsheaderlink-3">Deprecations<a href="#id53" title="Permalink to this heading">¶</a>{.headerlink}</a></h5>
<ul>
<li>The [<code>scrapy.utils.python.retry_on_eintr</code>{.docutils .literal
.notranslate}]{.pre} function is now deprecated (<a href="https://github.com/scrapy/scrapy/issues/4683">issue
4683</a>{.reference
.external})
:::</li>
</ul>
<p>::: {#id54 .section}</p>
<h5 id="new-featuresheaderlink-3"><a class="header" href="#new-featuresheaderlink-3">New features<a href="#id54" title="Permalink to this heading">¶</a>{.headerlink}</a></h5>
<ul>
<li>
<p><a href="index.html#topics-feed-exports">[Feed exports]{.std
.std-ref}</a>{.hoverxref .tooltip
.reference .internal} support <a href="index.html#topics-feed-storage-gcs">[Google Cloud Storage]{.std
.std-ref}</a>{.hoverxref .tooltip
.reference .internal} (<a href="https://github.com/scrapy/scrapy/issues/685">issue
685</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/3608">issue
3608</a>{.reference
.external})</p>
</li>
<li>
<p>New <a href="index.html#std-setting-FEED_EXPORT_BATCH_ITEM_COUNT">[<code>FEED_EXPORT_BATCH_ITEM_COUNT</code>{.xref .std .std-setting
.docutils .literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal} setting for batch deliveries (<a href="https://github.com/scrapy/scrapy/issues/4250">issue
4250</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/4434">issue
4434</a>{.reference
.external})</p>
</li>
<li>
<p>The <a href="index.html#std-command-parse">[<code>parse</code>{.xref .std .std-command .docutils .literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal} command now allows specifying an
output file (<a href="https://github.com/scrapy/scrapy/issues/4317">issue
4317</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/4377">issue
4377</a>{.reference
.external})</p>
</li>
<li>
<p><a href="index.html#scrapy.http.Request.from_curl" title="scrapy.http.Request.from_curl">[<code>Request.from_curl</code>{.xref .py .py-meth .docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal} and <a href="index.html#scrapy.utils.curl.curl_to_request_kwargs" title="scrapy.utils.curl.curl_to_request_kwargs">[<code>curl_to_request_kwargs()</code>{.xref .py .py-func
.docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal} now also support [<code>--data-raw</code>{.docutils .literal
.notranslate}]{.pre} (<a href="https://github.com/scrapy/scrapy/issues/4612">issue
4612</a>{.reference
.external})</p>
</li>
<li>
<p>A [<code>parse</code>{.docutils .literal .notranslate}]{.pre} callback may now
be used in built-in spider subclasses, such as <a href="index.html#scrapy.spiders.CrawlSpider" title="scrapy.spiders.CrawlSpider">[<code>CrawlSpider</code>{.xref
.py .py-class .docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal} (<a href="https://github.com/scrapy/scrapy/issues/712">issue
712</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/732">issue
732</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/781">issue
781</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/4254">issue
4254</a>{.reference
.external} )
:::</p>
</li>
</ul>
<p>::: {#id55 .section}</p>
<h5 id="bug-fixesheaderlink-3"><a class="header" href="#bug-fixesheaderlink-3">Bug fixes<a href="#id55" title="Permalink to this heading">¶</a>{.headerlink}</a></h5>
<ul>
<li>
<p>Fixed the <a href="index.html#topics-feed-format-csv">[CSV exporting]{.std
.std-ref}</a>{.hoverxref .tooltip
.reference .internal} of <a href="index.html#dataclass-items">[dataclass items]{.std
.std-ref}</a>{.hoverxref .tooltip
.reference .internal} and <a href="index.html#attrs-items">[attr.s items]{.std
.std-ref}</a>{.hoverxref .tooltip .reference
.internal} (<a href="https://github.com/scrapy/scrapy/issues/4667">issue
4667</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/4668">issue
4668</a>{.reference
.external})</p>
</li>
<li>
<p><a href="index.html#scrapy.http.Request.from_curl" title="scrapy.http.Request.from_curl">[<code>Request.from_curl</code>{.xref .py .py-meth .docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal} and <a href="index.html#scrapy.utils.curl.curl_to_request_kwargs" title="scrapy.utils.curl.curl_to_request_kwargs">[<code>curl_to_request_kwargs()</code>{.xref .py .py-func
.docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal} now set the request method to [<code>POST</code>{.docutils .literal
.notranslate}]{.pre} when a request body is specified and no request
method is specified (<a href="https://github.com/scrapy/scrapy/issues/4612">issue
4612</a>{.reference
.external})</p>
</li>
<li>
<p>The processing of ANSI escape sequences in enabled in Windows
10.0.14393 and later, where it is required for colored output
(<a href="https://github.com/scrapy/scrapy/issues/4393">issue
4393</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/4403">issue
4403</a>{.reference
.external})
:::</p>
</li>
</ul>
<p>::: {#id56 .section}</p>
<h5 id="documentationheaderlink-4"><a class="header" href="#documentationheaderlink-4">Documentation<a href="#id56" title="Permalink to this heading">¶</a>{.headerlink}</a></h5>
<ul>
<li>
<p>Updated the <a href="https://www.openssl.org/docs/manmaster/man1/openssl-ciphers.html#CIPHER-LIST-FORMAT">OpenSSL cipher list
format</a>{.reference
.external} link in the documentation about the
<a href="index.html#std-setting-DOWNLOADER_CLIENT_TLS_CIPHERS">[<code>DOWNLOADER_CLIENT_TLS_CIPHERS</code>{.xref .std .std-setting .docutils
.literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal} setting (<a href="https://github.com/scrapy/scrapy/issues/4653">issue
4653</a>{.reference
.external})</p>
</li>
<li>
<p>Simplified the code example in <a href="index.html#topics-loaders-dataclass">[Working with dataclass items]{.std
.std-ref}</a>{.hoverxref .tooltip
.reference .internal} (<a href="https://github.com/scrapy/scrapy/issues/4652">issue
4652</a>{.reference
.external})
:::</p>
</li>
</ul>
<p>::: {#id57 .section}</p>
<h5 id="quality-assuranceheaderlink-4"><a class="header" href="#quality-assuranceheaderlink-4">Quality assurance<a href="#id57" title="Permalink to this heading">¶</a>{.headerlink}</a></h5>
<ul>
<li>
<p>The base implementation of <a href="index.html#topics-loaders">[item loaders]{.std
.std-ref}</a>{.hoverxref .tooltip .reference
.internal} has been moved into <a href="https://itemloaders.readthedocs.io/en/latest/index.html" title="(in itemloaders)">[itemloaders]{.xref .std
.std-doc}</a>{.reference
.external} (<a href="https://github.com/scrapy/scrapy/issues/4005">issue
4005</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/4516">issue
4516</a>{.reference
.external})</p>
</li>
<li>
<p>Fixed a silenced error in some scheduler tests (<a href="https://github.com/scrapy/scrapy/issues/4644">issue
4644</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/4645">issue
4645</a>{.reference
.external})</p>
</li>
<li>
<p>Renewed the localhost certificate used for SSL tests (<a href="https://github.com/scrapy/scrapy/issues/4650">issue
4650</a>{.reference
.external})</p>
</li>
<li>
<p>Removed cookie-handling code specific to Python 2 (<a href="https://github.com/scrapy/scrapy/issues/4682">issue
4682</a>{.reference
.external})</p>
</li>
<li>
<p>Stopped using Python 2 unicode literal syntax (<a href="https://github.com/scrapy/scrapy/issues/4704">issue
4704</a>{.reference
.external})</p>
</li>
<li>
<p>Stopped using a backlash for line continuation (<a href="https://github.com/scrapy/scrapy/issues/4673">issue
4673</a>{.reference
.external})</p>
</li>
<li>
<p>Removed unneeded entries from the MyPy exception list (<a href="https://github.com/scrapy/scrapy/issues/4690">issue
4690</a>{.reference
.external})</p>
</li>
<li>
<p>Automated tests now pass on Windows as part of our continuous
integration system (<a href="https://github.com/scrapy/scrapy/issues/4458">issue
4458</a>{.reference
.external})</p>
</li>
<li>
<p>Automated tests now pass on the latest PyPy version for supported
Python versions in our continuous integration system (<a href="https://github.com/scrapy/scrapy/issues/4504">issue
4504</a>{.reference
.external})
:::
:::</p>
</li>
</ul>
<p>::: {#scrapy-2-2-1-2020-07-17 .section}
[]{#release-2-2-1}</p>
<h4 id="scrapy-221-2020-07-17headerlink"><a class="header" href="#scrapy-221-2020-07-17headerlink">Scrapy 2.2.1 (2020-07-17)<a href="#scrapy-2-2-1-2020-07-17" title="Permalink to this heading">¶</a>{.headerlink}</a></h4>
<ul>
<li>The <a href="index.html#std-command-startproject">[<code>startproject</code>{.xref .std .std-command .docutils .literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal} command no longer makes unintended
changes to the permissions of files in the destination folder, such
as removing execution permissions (<a href="https://github.com/scrapy/scrapy/issues/4662">issue
4662</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/4666">issue
4666</a>{.reference
.external})
:::</li>
</ul>
<p>::: {#scrapy-2-2-0-2020-06-24 .section}
[]{#release-2-2-0}</p>
<h4 id="scrapy-220-2020-06-24headerlink"><a class="header" href="#scrapy-220-2020-06-24headerlink">Scrapy 2.2.0 (2020-06-24)<a href="#scrapy-2-2-0-2020-06-24" title="Permalink to this heading">¶</a>{.headerlink}</a></h4>
<p>Highlights:</p>
<ul>
<li>
<p>Python 3.5.2+ is required now</p>
</li>
<li>
<p><a href="index.html#dataclass-items">[dataclass objects]{.std
.std-ref}</a>{.hoverxref .tooltip
.reference .internal} and <a href="index.html#attrs-items">[attrs objects]{.std
.std-ref}</a>{.hoverxref .tooltip .reference
.internal} are now valid <a href="index.html#item-types">[item types]{.std
.std-ref}</a>{.hoverxref .tooltip .reference
.internal}</p>
</li>
<li>
<p>New <a href="index.html#scrapy.http.TextResponse.json" title="scrapy.http.TextResponse.json">[<code>TextResponse.json</code>{.xref .py .py-meth .docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal} method</p>
</li>
<li>
<p>New <a href="index.html#std-signal-bytes_received">[<code>bytes_received</code>{.xref .std .std-signal .docutils .literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal} signal that allows canceling response
download</p>
</li>
<li>
<p><a href="index.html#scrapy.downloadermiddlewares.cookies.CookiesMiddleware" title="scrapy.downloadermiddlewares.cookies.CookiesMiddleware">[<code>CookiesMiddleware</code>{.xref .py .py-class .docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal} fixes</p>
</li>
</ul>
<p>::: {#id58 .section}</p>
<h5 id="backward-incompatible-changesheaderlink-2"><a class="header" href="#backward-incompatible-changesheaderlink-2">Backward-incompatible changes<a href="#id58" title="Permalink to this heading">¶</a>{.headerlink}</a></h5>
<ul>
<li>Support for Python 3.5.0 and 3.5.1 has been dropped; Scrapy now
refuses to run with a Python version lower than 3.5.2, which
introduced <a href="https://docs.python.org/3/library/typing.html#typing.Type" title="(in Python v3.12)">[<code>typing.Type</code>{.xref .py .py-class .docutils .literal
.notranslate}]{.pre}</a>{.reference
.external} (<a href="https://github.com/scrapy/scrapy/issues/4615">issue
4615</a>{.reference
.external})
:::</li>
</ul>
<p>::: {#id59 .section}</p>
<h5 id="deprecationsheaderlink-4"><a class="header" href="#deprecationsheaderlink-4">Deprecations<a href="#id59" title="Permalink to this heading">¶</a>{.headerlink}</a></h5>
<ul>
<li>
<p>[<code>TextResponse.body_as_unicode</code>{.xref .py .py-meth .docutils
.literal .notranslate}]{.pre} is now deprecated, use
<a href="index.html#scrapy.http.TextResponse.text" title="scrapy.http.TextResponse.text">[<code>TextResponse.text</code>{.xref .py .py-attr .docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal} instead (<a href="https://github.com/scrapy/scrapy/issues/4546">issue
4546</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/4555">issue
4555</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/4579">issue
4579</a>{.reference
.external})</p>
</li>
<li>
<p>[<code>scrapy.item.BaseItem</code>{.xref .py .py-class .docutils .literal
.notranslate}]{.pre} is now deprecated, use
[<code>scrapy.item.Item</code>{.xref .py .py-class .docutils .literal
.notranslate}]{.pre} instead (<a href="https://github.com/scrapy/scrapy/issues/4534">issue
4534</a>{.reference
.external})
:::</p>
</li>
</ul>
<p>::: {#id60 .section}</p>
<h5 id="new-featuresheaderlink-4"><a class="header" href="#new-featuresheaderlink-4">New features<a href="#id60" title="Permalink to this heading">¶</a>{.headerlink}</a></h5>
<ul>
<li>
<p><a href="index.html#dataclass-items">[dataclass objects]{.std
.std-ref}</a>{.hoverxref .tooltip
.reference .internal} and <a href="index.html#attrs-items">[attrs objects]{.std
.std-ref}</a>{.hoverxref .tooltip .reference
.internal} are now valid <a href="index.html#item-types">[item types]{.std
.std-ref}</a>{.hoverxref .tooltip .reference
.internal}, and a new
<a href="https://github.com/scrapy/itemadapter">itemadapter</a>{.reference
.external} library makes it easy to write code that <a href="index.html#supporting-item-types">[supports any
item type]{.std
.std-ref}</a>{.hoverxref .tooltip
.reference .internal} (<a href="https://github.com/scrapy/scrapy/issues/2749">issue
2749</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/2807">issue
2807</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/3761">issue
3761</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/3881">issue
3881</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/4642">issue
4642</a>{.reference
.external})</p>
</li>
<li>
<p>A new <a href="index.html#scrapy.http.TextResponse.json" title="scrapy.http.TextResponse.json">[<code>TextResponse.json</code>{.xref .py .py-meth .docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal} method allows to deserialize JSON responses (<a href="https://github.com/scrapy/scrapy/issues/2444">issue
2444</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/4460">issue
4460</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/4574">issue
4574</a>{.reference
.external})</p>
</li>
<li>
<p>A new <a href="index.html#std-signal-bytes_received">[<code>bytes_received</code>{.xref .std .std-signal .docutils .literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal} signal allows monitoring response
download progress and <a href="index.html#topics-stop-response-download">[stopping downloads]{.std
.std-ref}</a>{.hoverxref
.tooltip .reference .internal} (<a href="https://github.com/scrapy/scrapy/issues/4205">issue
4205</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/4559">issue
4559</a>{.reference
.external})</p>
</li>
<li>
<p>The dictionaries in the result list of a <a href="index.html#topics-media-pipeline">[media pipeline]{.std
.std-ref}</a>{.hoverxref .tooltip
.reference .internal} now include a new key, [<code>status</code>{.docutils
.literal .notranslate}]{.pre}, which indicates if the file was
downloaded or, if the file was not downloaded, why it was not
downloaded; see <a href="index.html#scrapy.pipelines.files.FilesPipeline.get_media_requests" title="scrapy.pipelines.files.FilesPipeline.get_media_requests">[<code>FilesPipeline.get_media_requests</code>{.xref .py
.py-meth .docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal} for more information (<a href="https://github.com/scrapy/scrapy/issues/2893">issue
2893</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/4486">issue
4486</a>{.reference
.external})</p>
</li>
<li>
<p>When using <a href="index.html#media-pipeline-gcs">[Google Cloud Storage]{.std
.std-ref}</a>{.hoverxref .tooltip
.reference .internal} for a <a href="index.html#topics-media-pipeline">[media pipeline]{.std
.std-ref}</a>{.hoverxref .tooltip
.reference .internal}, a warning is now logged if the configured
credentials do not grant the required permissions (<a href="https://github.com/scrapy/scrapy/issues/4346">issue
4346</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/4508">issue
4508</a>{.reference
.external})</p>
</li>
<li>
<p><a href="index.html#topics-link-extractors">[Link extractors]{.std
.std-ref}</a>{.hoverxref .tooltip
.reference .internal} are now serializable, as long as you do not
use <a href="https://docs.python.org/3/reference/expressions.html#lambda" title="(in Python v3.12)">[lambdas]{.xref .std
.std-ref}</a>{.reference
.external} for parameters; for example, you can now pass link
extractors in <a href="index.html#scrapy.http.Request.cb_kwargs" title="scrapy.http.Request.cb_kwargs">[<code>Request.cb_kwargs</code>{.xref .py .py-attr .docutils
.literal
.notranslate}]{.pre}</a>{.reference
.internal} or <a href="index.html#scrapy.http.Request.meta" title="scrapy.http.Request.meta">[<code>Request.meta</code>{.xref .py .py-attr .docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal} when <a href="index.html#topics-jobs">[persisting scheduled requests]{.std
.std-ref}</a>{.hoverxref .tooltip .reference
.internal} (<a href="https://github.com/scrapy/scrapy/issues/4554">issue
4554</a>{.reference
.external})</p>
</li>
<li>
<p>Upgraded the <a href="https://docs.python.org/3/library/pickle.html#pickle-protocols" title="(in Python v3.12)">[pickle protocol]{.xref .std
.std-ref}</a>{.reference
.external} that Scrapy uses from protocol 2 to protocol 4, improving
serialization capabilities and performance (<a href="https://github.com/scrapy/scrapy/issues/4135">issue
4135</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/4541">issue
4541</a>{.reference
.external})</p>
</li>
<li>
<p>[<code>scrapy.utils.misc.create_instance()</code>{.xref .py .py-func .docutils
.literal .notranslate}]{.pre} now raises a <a href="https://docs.python.org/3/library/exceptions.html#TypeError" title="(in Python v3.12)">[<code>TypeError</code>{.xref .py
.py-exc .docutils .literal
.notranslate}]{.pre}</a>{.reference
.external} exception if the resulting instance is [<code>None</code>{.docutils
.literal .notranslate}]{.pre} (<a href="https://github.com/scrapy/scrapy/issues/4528">issue
4528</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/4532">issue
4532</a>{.reference
.external})
:::</p>
</li>
</ul>
<p>::: {#id61 .section}</p>
<h5 id="bug-fixesheaderlink-4"><a class="header" href="#bug-fixesheaderlink-4">Bug fixes<a href="#id61" title="Permalink to this heading">¶</a>{.headerlink}</a></h5>
<ul>
<li>
<p><a href="index.html#scrapy.downloadermiddlewares.cookies.CookiesMiddleware" title="scrapy.downloadermiddlewares.cookies.CookiesMiddleware">[<code>CookiesMiddleware</code>{.xref .py .py-class .docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal} no longer discards cookies defined in
<a href="index.html#scrapy.http.Request.headers" title="scrapy.http.Request.headers">[<code>Request.headers</code>{.xref .py .py-attr .docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal} (<a href="https://github.com/scrapy/scrapy/issues/1992">issue
1992</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/2400">issue
2400</a>{.reference
.external})</p>
</li>
<li>
<p><a href="index.html#scrapy.downloadermiddlewares.cookies.CookiesMiddleware" title="scrapy.downloadermiddlewares.cookies.CookiesMiddleware">[<code>CookiesMiddleware</code>{.xref .py .py-class .docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal} no longer re-encodes cookies defined as <a href="https://docs.python.org/3/library/stdtypes.html#bytes" title="(in Python v3.12)">[<code>bytes</code>{.xref
.py .py-class .docutils .literal
.notranslate}]{.pre}</a>{.reference
.external} in the [<code>cookies</code>{.docutils .literal .notranslate}]{.pre}
parameter of the [<code>__init__</code>{.docutils .literal .notranslate}]{.pre}
method of <a href="index.html#scrapy.http.Request" title="scrapy.http.Request">[<code>Request</code>{.xref .py .py-class .docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal} (<a href="https://github.com/scrapy/scrapy/issues/2400">issue
2400</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/3575">issue
3575</a>{.reference
.external})</p>
</li>
<li>
<p>When <a href="index.html#std-setting-FEEDS">[<code>FEEDS</code>{.xref .std .std-setting .docutils .literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal} defines multiple URIs,
<a href="index.html#std-setting-FEED_STORE_EMPTY">[<code>FEED_STORE_EMPTY</code>{.xref .std .std-setting .docutils .literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal} is [<code>False</code>{.docutils .literal
.notranslate}]{.pre} and the crawl yields no items, Scrapy no longer
stops feed exports after the first URI (<a href="https://github.com/scrapy/scrapy/issues/4621">issue
4621</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/4626">issue
4626</a>{.reference
.external})</p>
</li>
<li>
<p><a href="index.html#scrapy.spiders.Spider" title="scrapy.spiders.Spider">[<code>Spider</code>{.xref .py .py-class .docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal} callbacks defined using <a href="index.html#document-topics/coroutines">[coroutine
syntax]{.doc}</a>{.reference
.internal} no longer need to return an iterable, and may instead
return a <a href="index.html#scrapy.http.Request" title="scrapy.http.Request">[<code>Request</code>{.xref .py .py-class .docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal} object, an <a href="index.html#topics-items">[item]{.std
.std-ref}</a>{.hoverxref .tooltip .reference
.internal}, or [<code>None</code>{.docutils .literal .notranslate}]{.pre}
(<a href="https://github.com/scrapy/scrapy/issues/4609">issue
4609</a>{.reference
.external})</p>
</li>
<li>
<p>The <a href="index.html#std-command-startproject">[<code>startproject</code>{.xref .std .std-command .docutils .literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal} command now ensures that the
generated project folders and files have the right permissions
(<a href="https://github.com/scrapy/scrapy/issues/4604">issue
4604</a>{.reference
.external})</p>
</li>
<li>
<p>Fix a <a href="https://docs.python.org/3/library/exceptions.html#KeyError" title="(in Python v3.12)">[<code>KeyError</code>{.xref .py .py-exc .docutils .literal
.notranslate}]{.pre}</a>{.reference
.external} exception being sometimes raised from
[<code>scrapy.utils.datatypes.LocalWeakReferencedCache</code>{.xref .py
.py-class .docutils .literal .notranslate}]{.pre} (<a href="https://github.com/scrapy/scrapy/issues/4597">issue
4597</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/4599">issue
4599</a>{.reference
.external})</p>
</li>
<li>
<p>When <a href="index.html#std-setting-FEEDS">[<code>FEEDS</code>{.xref .std .std-setting .docutils .literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal} defines multiple URIs, log messages
about items being stored now contain information from the
corresponding feed, instead of always containing information about
only one of the feeds (<a href="https://github.com/scrapy/scrapy/issues/4619">issue
4619</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/4629">issue
4629</a>{.reference
.external})
:::</p>
</li>
</ul>
<p>::: {#id62 .section}</p>
<h5 id="documentationheaderlink-5"><a class="header" href="#documentationheaderlink-5">Documentation<a href="#id62" title="Permalink to this heading">¶</a>{.headerlink}</a></h5>
<ul>
<li>
<p>Added a new section about <a href="index.html#errback-cb-kwargs">[accessing cb_kwargs from errbacks]{.std
.std-ref}</a>{.hoverxref .tooltip
.reference .internal} (<a href="https://github.com/scrapy/scrapy/issues/4598">issue
4598</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/4634">issue
4634</a>{.reference
.external})</p>
</li>
<li>
<p>Covered <a href="https://github.com/Nykakin/chompjs">chompjs</a>{.reference
.external} in <a href="index.html#topics-parsing-javascript">[Parsing JavaScript code]{.std
.std-ref}</a>{.hoverxref .tooltip
.reference .internal} (<a href="https://github.com/scrapy/scrapy/issues/4556">issue
4556</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/4562">issue
4562</a>{.reference
.external})</p>
</li>
<li>
<p>Removed from
<a href="index.html#document-topics/coroutines">[Coroutines]{.doc}</a>{.reference
.internal} the warning about the API being experimental (<a href="https://github.com/scrapy/scrapy/issues/4511">issue
4511</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/4513">issue
4513</a>{.reference
.external})</p>
</li>
<li>
<p>Removed references to unsupported versions of <a href="https://docs.twisted.org/en/stable/index.html" title="(in Twisted v23.10)">[Twisted]{.xref .std
.std-doc}</a>{.reference
.external} (<a href="https://github.com/scrapy/scrapy/issues/4533">issue
4533</a>{.reference
.external})</p>
</li>
<li>
<p>Updated the description of the <a href="index.html#screenshotpipeline">[screenshot pipeline example]{.std
.std-ref}</a>{.hoverxref .tooltip
.reference .internal}, which now uses <a href="index.html#document-topics/coroutines">[coroutine
syntax]{.doc}</a>{.reference
.internal} instead of returning a <a href="https://docs.twisted.org/en/stable/api/twisted.internet.defer.Deferred.html" title="(in Twisted)">[<code>Deferred</code>{.xref .py .py-class
.docutils .literal
.notranslate}]{.pre}</a>{.reference
.external} (<a href="https://github.com/scrapy/scrapy/issues/4514">issue
4514</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/4593">issue
4593</a>{.reference
.external})</p>
</li>
<li>
<p>Removed a misleading import line from the
<a href="index.html#scrapy.utils.log.configure_logging" title="scrapy.utils.log.configure_logging">[<code>scrapy.utils.log.configure_logging()</code>{.xref .py .py-func
.docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal} code example (<a href="https://github.com/scrapy/scrapy/issues/4510">issue
4510</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/4587">issue
4587</a>{.reference
.external})</p>
</li>
<li>
<p>The display-on-hover behavior of internal documentation references
now also covers links to <a href="index.html#topics-commands">[commands]{.std
.std-ref}</a>{.hoverxref .tooltip
.reference .internal}, <a href="index.html#scrapy.http.Request.meta" title="scrapy.http.Request.meta">[<code>Request.meta</code>{.xref .py .py-attr .docutils
.literal
.notranslate}]{.pre}</a>{.reference
.internal} keys, <a href="index.html#topics-settings">[settings]{.std
.std-ref}</a>{.hoverxref .tooltip
.reference .internal} and <a href="index.html#topics-signals">[signals]{.std
.std-ref}</a>{.hoverxref .tooltip .reference
.internal} (<a href="https://github.com/scrapy/scrapy/issues/4495">issue
4495</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/4563">issue
4563</a>{.reference
.external})</p>
</li>
<li>
<p>It is again possible to download the documentation for offline
reading (<a href="https://github.com/scrapy/scrapy/issues/4578">issue
4578</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/4585">issue
4585</a>{.reference
.external})</p>
</li>
<li>
<p>Removed backslashes preceding [<code>*args</code>{.docutils .literal
.notranslate}]{.pre} and [<code>**kwargs</code>{.docutils .literal
.notranslate}]{.pre} in some function and method signatures (<a href="https://github.com/scrapy/scrapy/issues/4592">issue
4592</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/4596">issue
4596</a>{.reference
.external})
:::</p>
</li>
</ul>
<p>::: {#id63 .section}</p>
<h5 id="quality-assuranceheaderlink-5"><a class="header" href="#quality-assuranceheaderlink-5">Quality assurance<a href="#id63" title="Permalink to this heading">¶</a>{.headerlink}</a></h5>
<ul>
<li>
<p>Adjusted the code base further to our <a href="index.html#coding-style">[style guidelines]{.std
.std-ref}</a>{.hoverxref .tooltip .reference
.internal} (<a href="https://github.com/scrapy/scrapy/issues/4237">issue
4237</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/4525">issue
4525</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/4538">issue
4538</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/4539">issue
4539</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/4540">issue
4540</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/4542">issue
4542</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/4543">issue
4543</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/4544">issue
4544</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/4545">issue
4545</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/4557">issue
4557</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/4558">issue
4558</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/4566">issue
4566</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/4568">issue
4568</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/4572">issue
4572</a>{.reference
.external})</p>
</li>
<li>
<p>Removed remnants of Python 2 support (<a href="https://github.com/scrapy/scrapy/issues/4550">issue
4550</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/4553">issue
4553</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/4568">issue
4568</a>{.reference
.external})</p>
</li>
<li>
<p>Improved code sharing between the <a href="index.html#std-command-crawl">[<code>crawl</code>{.xref .std .std-command
.docutils .literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal} and <a href="index.html#std-command-runspider">[<code>runspider</code>{.xref .std
.std-command .docutils .literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal} commands (<a href="https://github.com/scrapy/scrapy/issues/4548">issue
4548</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/4552">issue
4552</a>{.reference
.external})</p>
</li>
<li>
<p>Replaced [<code>chain(*iterable)</code>{.docutils .literal .notranslate}]{.pre}
with [<code>chain.from_iterable(iterable)</code>{.docutils .literal
.notranslate}]{.pre} (<a href="https://github.com/scrapy/scrapy/issues/4635">issue
4635</a>{.reference
.external})</p>
</li>
<li>
<p>You may now run the <a href="https://docs.python.org/3/library/asyncio.html#module-asyncio" title="(in Python v3.12)">[<code>asyncio</code>{.xref .py .py-mod .docutils .literal
.notranslate}]{.pre}</a>{.reference
.external} tests with Tox on any Python version (<a href="https://github.com/scrapy/scrapy/issues/4521">issue
4521</a>{.reference
.external})</p>
</li>
<li>
<p>Updated test requirements to reflect an incompatibility with pytest
5.4 and 5.4.1 (<a href="https://github.com/scrapy/scrapy/issues/4588">issue
4588</a>{.reference
.external})</p>
</li>
<li>
<p>Improved <a href="index.html#scrapy.spiderloader.SpiderLoader" title="scrapy.spiderloader.SpiderLoader">[<code>SpiderLoader</code>{.xref .py .py-class .docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal} test coverage for scenarios involving duplicate spider
names (<a href="https://github.com/scrapy/scrapy/issues/4549">issue
4549</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/4560">issue
4560</a>{.reference
.external})</p>
</li>
<li>
<p>Configured Travis CI to also run the tests with Python 3.5.2 (<a href="https://github.com/scrapy/scrapy/issues/4518">issue
4518</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/4615">issue
4615</a>{.reference
.external})</p>
</li>
<li>
<p>Added a <a href="https://www.pylint.org/">Pylint</a>{.reference .external} job
to Travis CI (<a href="https://github.com/scrapy/scrapy/issues/3727">issue
3727</a>{.reference
.external})</p>
</li>
<li>
<p>Added a <a href="http://mypy-lang.org/">Mypy</a>{.reference .external} job to
Travis CI (<a href="https://github.com/scrapy/scrapy/issues/4637">issue
4637</a>{.reference
.external})</p>
</li>
<li>
<p>Made use of set literals in tests (<a href="https://github.com/scrapy/scrapy/issues/4573">issue
4573</a>{.reference
.external})</p>
</li>
<li>
<p>Cleaned up the Travis CI configuration (<a href="https://github.com/scrapy/scrapy/issues/4517">issue
4517</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/4519">issue
4519</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/4522">issue
4522</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/4537">issue
4537</a>{.reference
.external})
:::
:::</p>
</li>
</ul>
<p>::: {#scrapy-2-1-0-2020-04-24 .section}
[]{#release-2-1-0}</p>
<h4 id="scrapy-210-2020-04-24headerlink"><a class="header" href="#scrapy-210-2020-04-24headerlink">Scrapy 2.1.0 (2020-04-24)<a href="#scrapy-2-1-0-2020-04-24" title="Permalink to this heading">¶</a>{.headerlink}</a></h4>
<p>Highlights:</p>
<ul>
<li>
<p>New <a href="index.html#std-setting-FEEDS">[<code>FEEDS</code>{.xref .std .std-setting .docutils .literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal} setting to export to multiple feeds</p>
</li>
<li>
<p>New <a href="index.html#scrapy.http.Response.ip_address" title="scrapy.http.Response.ip_address">[<code>Response.ip_address</code>{.xref .py .py-attr .docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal} attribute</p>
</li>
</ul>
<p>::: {#id64 .section}</p>
<h5 id="backward-incompatible-changesheaderlink-3"><a class="header" href="#backward-incompatible-changesheaderlink-3">Backward-incompatible changes<a href="#id64" title="Permalink to this heading">¶</a>{.headerlink}</a></h5>
<ul>
<li>
<p><a href="https://docs.python.org/3/library/exceptions.html#AssertionError" title="(in Python v3.12)">[<code>AssertionError</code>{.xref .py .py-exc .docutils .literal
.notranslate}]{.pre}</a>{.reference
.external} exceptions triggered by <a href="https://docs.python.org/3/reference/simple_stmts.html#assert" title="(in Python v3.12)">[assert]{.xref .std
.std-ref}</a>{.reference
.external} statements have been replaced by new exception types, to
support running Python in optimized mode (see <a href="https://docs.python.org/3/using/cmdline.html#cmdoption-O" title="(in Python v3.12)">[<code>-O</code>{.xref .std
.std-option .docutils .literal
.notranslate}]{.pre}</a>{.reference
.external}) without changing Scrapy's behavior in any unexpected
ways.</p>
<p>If you catch an <a href="https://docs.python.org/3/library/exceptions.html#AssertionError" title="(in Python v3.12)">[<code>AssertionError</code>{.xref .py .py-exc .docutils
.literal
.notranslate}]{.pre}</a>{.reference
.external} exception from Scrapy, update your code to catch the
corresponding new exception.</p>
<p>(<a href="https://github.com/scrapy/scrapy/issues/4440">issue
4440</a>{.reference
.external})
:::</p>
</li>
</ul>
<p>::: {#id65 .section}</p>
<h5 id="deprecation-removalsheaderlink-4"><a class="header" href="#deprecation-removalsheaderlink-4">Deprecation removals<a href="#id65" title="Permalink to this heading">¶</a>{.headerlink}</a></h5>
<ul>
<li>
<p>The [<code>LOG_UNSERIALIZABLE_REQUESTS</code>{.docutils .literal
.notranslate}]{.pre} setting is no longer supported, use
<a href="index.html#std-setting-SCHEDULER_DEBUG">[<code>SCHEDULER_DEBUG</code>{.xref .std .std-setting .docutils .literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal} instead (<a href="https://github.com/scrapy/scrapy/issues/4385">issue
4385</a>{.reference
.external})</p>
</li>
<li>
<p>The [<code>REDIRECT_MAX_METAREFRESH_DELAY</code>{.docutils .literal
.notranslate}]{.pre} setting is no longer supported, use
<a href="index.html#std-setting-METAREFRESH_MAXDELAY">[<code>METAREFRESH_MAXDELAY</code>{.xref .std .std-setting .docutils .literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal} instead (<a href="https://github.com/scrapy/scrapy/issues/4385">issue
4385</a>{.reference
.external})</p>
</li>
<li>
<p>The [<code>ChunkedTransferMiddleware</code>{.xref .py .py-class .docutils
.literal .notranslate}]{.pre} middleware has been removed, including
the entire [<code>scrapy.downloadermiddlewares.chunked</code>{.xref .py
.py-class .docutils .literal .notranslate}]{.pre} module; chunked
transfers work out of the box (<a href="https://github.com/scrapy/scrapy/issues/4431">issue
4431</a>{.reference
.external})</p>
</li>
<li>
<p>The [<code>spiders</code>{.docutils .literal .notranslate}]{.pre} property has
been removed from <a href="index.html#scrapy.crawler.Crawler" title="scrapy.crawler.Crawler">[<code>Crawler</code>{.xref .py .py-class .docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal}, use [<code>CrawlerRunner.spider_loader</code>{.xref .py .py-class
.docutils .literal .notranslate}]{.pre} or instantiate
<a href="index.html#std-setting-SPIDER_LOADER_CLASS">[<code>SPIDER_LOADER_CLASS</code>{.xref .std .std-setting .docutils .literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal} with your settings instead (<a href="https://github.com/scrapy/scrapy/issues/4398">issue
4398</a>{.reference
.external})</p>
</li>
<li>
<p>The [<code>MultiValueDict</code>{.docutils .literal .notranslate}]{.pre},
[<code>MultiValueDictKeyError</code>{.docutils .literal .notranslate}]{.pre},
and [<code>SiteNode</code>{.docutils .literal .notranslate}]{.pre} classes have
been removed from [<code>scrapy.utils.datatypes</code>{.xref .py .py-mod
.docutils .literal .notranslate}]{.pre} (<a href="https://github.com/scrapy/scrapy/issues/4400">issue
4400</a>{.reference
.external})
:::</p>
</li>
</ul>
<p>::: {#id66 .section}</p>
<h5 id="deprecationsheaderlink-5"><a class="header" href="#deprecationsheaderlink-5">Deprecations<a href="#id66" title="Permalink to this heading">¶</a>{.headerlink}</a></h5>
<ul>
<li>The [<code>FEED_FORMAT</code>{.docutils .literal .notranslate}]{.pre} and
[<code>FEED_URI</code>{.docutils .literal .notranslate}]{.pre} settings have
been deprecated in favor of the new <a href="index.html#std-setting-FEEDS">[<code>FEEDS</code>{.xref .std
.std-setting .docutils .literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal} setting (<a href="https://github.com/scrapy/scrapy/issues/1336">issue
1336</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/3858">issue
3858</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/4507">issue
4507</a>{.reference
.external})
:::</li>
</ul>
<p>::: {#id67 .section}</p>
<h5 id="new-featuresheaderlink-5"><a class="header" href="#new-featuresheaderlink-5">New features<a href="#id67" title="Permalink to this heading">¶</a>{.headerlink}</a></h5>
<ul>
<li>
<p>A new setting, <a href="index.html#std-setting-FEEDS">[<code>FEEDS</code>{.xref .std .std-setting .docutils .literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal}, allows configuring multiple output
feeds with different settings each (<a href="https://github.com/scrapy/scrapy/issues/1336">issue
1336</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/3858">issue
3858</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/4507">issue
4507</a>{.reference
.external})</p>
</li>
<li>
<p>The <a href="index.html#std-command-crawl">[<code>crawl</code>{.xref .std .std-command .docutils .literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal} and <a href="index.html#std-command-runspider">[<code>runspider</code>{.xref .std
.std-command .docutils .literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal} commands now support multiple
[<code>-o</code>{.docutils .literal .notranslate}]{.pre} parameters (<a href="https://github.com/scrapy/scrapy/issues/1336">issue
1336</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/3858">issue
3858</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/4507">issue
4507</a>{.reference
.external})</p>
</li>
<li>
<p>The <a href="index.html#std-command-crawl">[<code>crawl</code>{.xref .std .std-command .docutils .literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal} and <a href="index.html#std-command-runspider">[<code>runspider</code>{.xref .std
.std-command .docutils .literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal} commands now support specifying an
output format by appending [<code>:&lt;format&gt;</code>{.docutils .literal
.notranslate}]{.pre} to the output file (<a href="https://github.com/scrapy/scrapy/issues/1336">issue
1336</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/3858">issue
3858</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/4507">issue
4507</a>{.reference
.external})</p>
</li>
<li>
<p>The new <a href="index.html#scrapy.http.Response.ip_address" title="scrapy.http.Response.ip_address">[<code>Response.ip_address</code>{.xref .py .py-attr .docutils
.literal
.notranslate}]{.pre}</a>{.reference
.internal} attribute gives access to the IP address that originated
a response (<a href="https://github.com/scrapy/scrapy/issues/3903">issue
3903</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/3940">issue
3940</a>{.reference
.external})</p>
</li>
<li>
<p>A warning is now issued when a value in [<code>allowed_domains</code>{.xref .py
.py-attr .docutils .literal .notranslate}]{.pre} includes a port
(<a href="https://github.com/scrapy/scrapy/issues/50">issue 50</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/3198">issue
3198</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/4413">issue
4413</a>{.reference
.external})</p>
</li>
<li>
<p>Zsh completion now excludes used option aliases from the completion
list (<a href="https://github.com/scrapy/scrapy/issues/4438">issue
4438</a>{.reference
.external})
:::</p>
</li>
</ul>
<p>::: {#id68 .section}</p>
<h5 id="bug-fixesheaderlink-5"><a class="header" href="#bug-fixesheaderlink-5">Bug fixes<a href="#id68" title="Permalink to this heading">¶</a>{.headerlink}</a></h5>
<ul>
<li>
<p><a href="index.html#request-serialization">[Request serialization]{.std
.std-ref}</a>{.hoverxref .tooltip
.reference .internal} no longer breaks for callbacks that are spider
attributes which are assigned a function with a different name
(<a href="https://github.com/scrapy/scrapy/issues/4500">issue
4500</a>{.reference
.external})</p>
</li>
<li>
<p>[<code>None</code>{.docutils .literal .notranslate}]{.pre} values in
[<code>allowed_domains</code>{.xref .py .py-attr .docutils .literal
.notranslate}]{.pre} no longer cause a <a href="https://docs.python.org/3/library/exceptions.html#TypeError" title="(in Python v3.12)">[<code>TypeError</code>{.xref .py
.py-exc .docutils .literal
.notranslate}]{.pre}</a>{.reference
.external} exception (<a href="https://github.com/scrapy/scrapy/issues/4410">issue
4410</a>{.reference
.external})</p>
</li>
<li>
<p>Zsh completion no longer allows options after arguments (<a href="https://github.com/scrapy/scrapy/issues/4438">issue
4438</a>{.reference
.external})</p>
</li>
<li>
<p>zope.interface 5.0.0 and later versions are now supported (<a href="https://github.com/scrapy/scrapy/issues/4447">issue
4447</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/4448">issue
4448</a>{.reference
.external})</p>
</li>
<li>
<p>[<code>Spider.make_requests_from_url</code>{.docutils .literal
.notranslate}]{.pre}, deprecated in Scrapy 1.4.0, now issues a
warning when used (<a href="https://github.com/scrapy/scrapy/issues/4412">issue
4412</a>{.reference
.external})
:::</p>
</li>
</ul>
<p>::: {#id69 .section}</p>
<h5 id="documentationheaderlink-6"><a class="header" href="#documentationheaderlink-6">Documentation<a href="#id69" title="Permalink to this heading">¶</a>{.headerlink}</a></h5>
<ul>
<li>
<p>Improved the documentation about signals that allow their handlers
to return a <a href="https://docs.twisted.org/en/stable/api/twisted.internet.defer.Deferred.html" title="(in Twisted)">[<code>Deferred</code>{.xref .py .py-class .docutils .literal
.notranslate}]{.pre}</a>{.reference
.external} (<a href="https://github.com/scrapy/scrapy/issues/4295">issue
4295</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/4390">issue
4390</a>{.reference
.external})</p>
</li>
<li>
<p>Our PyPI entry now includes links for our documentation, our source
code repository and our issue tracker (<a href="https://github.com/scrapy/scrapy/issues/4456">issue
4456</a>{.reference
.external})</p>
</li>
<li>
<p>Covered the
<a href="https://michael-shub.github.io/curl2scrapy/">curl2scrapy</a>{.reference
.external} service in the documentation (<a href="https://github.com/scrapy/scrapy/issues/4206">issue
4206</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/4455">issue
4455</a>{.reference
.external})</p>
</li>
<li>
<p>Removed references to the Guppy library, which only works in Python
2 (<a href="https://github.com/scrapy/scrapy/issues/4285">issue
4285</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/4343">issue
4343</a>{.reference
.external})</p>
</li>
<li>
<p>Extended use of InterSphinx to link to Python 3 documentation
(<a href="https://github.com/scrapy/scrapy/issues/4444">issue
4444</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/4445">issue
4445</a>{.reference
.external})</p>
</li>
<li>
<p>Added support for Sphinx 3.0 and later (<a href="https://github.com/scrapy/scrapy/issues/4475">issue
4475</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/4480">issue
4480</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/4496">issue
4496</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/4503">issue
4503</a>{.reference
.external})
:::</p>
</li>
</ul>
<p>::: {#id70 .section}</p>
<h5 id="quality-assuranceheaderlink-6"><a class="header" href="#quality-assuranceheaderlink-6">Quality assurance<a href="#id70" title="Permalink to this heading">¶</a>{.headerlink}</a></h5>
<ul>
<li>
<p>Removed warnings about using old, removed settings (<a href="https://github.com/scrapy/scrapy/issues/4404">issue
4404</a>{.reference
.external})</p>
</li>
<li>
<p>Removed a warning about importing <a href="https://docs.twisted.org/en/stable/api/twisted.internet.testing.StringTransport.html" title="(in Twisted)">[<code>StringTransport</code>{.xref .py
.py-class .docutils .literal
.notranslate}]{.pre}</a>{.reference
.external} from [<code>twisted.test.proto_helpers</code>{.docutils .literal
.notranslate}]{.pre} in Twisted 19.7.0 or newer (<a href="https://github.com/scrapy/scrapy/issues/4409">issue
4409</a>{.reference
.external})</p>
</li>
<li>
<p>Removed outdated Debian package build files (<a href="https://github.com/scrapy/scrapy/issues/4384">issue
4384</a>{.reference
.external})</p>
</li>
<li>
<p>Removed <a href="https://docs.python.org/3/library/functions.html#object" title="(in Python v3.12)">[<code>object</code>{.xref .py .py-class .docutils .literal
.notranslate}]{.pre}</a>{.reference
.external} usage as a base class (<a href="https://github.com/scrapy/scrapy/issues/4430">issue
4430</a>{.reference
.external})</p>
</li>
<li>
<p>Removed code that added support for old versions of Twisted that we
no longer support (<a href="https://github.com/scrapy/scrapy/issues/4472">issue
4472</a>{.reference
.external})</p>
</li>
<li>
<p>Fixed code style issues (<a href="https://github.com/scrapy/scrapy/issues/4468">issue
4468</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/4469">issue
4469</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/4471">issue
4471</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/4481">issue
4481</a>{.reference
.external})</p>
</li>
<li>
<p>Removed <a href="https://docs.twisted.org/en/stable/api/twisted.internet.defer.html#returnValue" title="(in Twisted)">[<code>twisted.internet.defer.returnValue()</code>{.xref .py .py-func
.docutils .literal
.notranslate}]{.pre}</a>{.reference
.external} calls (<a href="https://github.com/scrapy/scrapy/issues/4443">issue
4443</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/4446">issue
4446</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/4489">issue
4489</a>{.reference
.external})
:::
:::</p>
</li>
</ul>
<p>::: {#scrapy-2-0-1-2020-03-18 .section}
[]{#release-2-0-1}</p>
<h4 id="scrapy-201-2020-03-18headerlink"><a class="header" href="#scrapy-201-2020-03-18headerlink">Scrapy 2.0.1 (2020-03-18)<a href="#scrapy-2-0-1-2020-03-18" title="Permalink to this heading">¶</a>{.headerlink}</a></h4>
<ul>
<li>
<p><a href="index.html#scrapy.http.Response.follow_all" title="scrapy.http.Response.follow_all">[<code>Response.follow_all</code>{.xref .py .py-meth .docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal} now supports an empty URL iterable as input (<a href="https://github.com/scrapy/scrapy/issues/4408">issue
4408</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/4420">issue
4420</a>{.reference
.external})</p>
</li>
<li>
<p>Removed top-level <a href="https://docs.twisted.org/en/stable/api/twisted.internet.reactor.html" title="(in Twisted)">[<code>reactor</code>{.xref .py .py-mod .docutils .literal
.notranslate}]{.pre}</a>{.reference
.external} imports to prevent errors about the wrong Twisted reactor
being installed when setting a different Twisted reactor using
<a href="index.html#std-setting-TWISTED_REACTOR">[<code>TWISTED_REACTOR</code>{.xref .std .std-setting .docutils .literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal} (<a href="https://github.com/scrapy/scrapy/issues/4401">issue
4401</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/4406">issue
4406</a>{.reference
.external})</p>
</li>
<li>
<p>Fixed tests (<a href="https://github.com/scrapy/scrapy/issues/4422">issue
4422</a>{.reference
.external})
:::</p>
</li>
</ul>
<p>::: {#scrapy-2-0-0-2020-03-03 .section}
[]{#release-2-0-0}</p>
<h4 id="scrapy-200-2020-03-03headerlink"><a class="header" href="#scrapy-200-2020-03-03headerlink">Scrapy 2.0.0 (2020-03-03)<a href="#scrapy-2-0-0-2020-03-03" title="Permalink to this heading">¶</a>{.headerlink}</a></h4>
<p>Highlights:</p>
<ul>
<li>
<p>Python 2 support has been removed</p>
</li>
<li>
<p><a href="index.html#document-topics/coroutines">[Partial]{.doc}</a>{.reference
.internal} <a href="https://docs.python.org/3/reference/compound_stmts.html#async" title="(in Python v3.12)">[coroutine syntax]{.xref .std
.std-ref}</a>{.reference
.external} support and
<a href="index.html#document-topics/asyncio">[experimental]{.doc}</a>{.reference
.internal} <a href="https://docs.python.org/3/library/asyncio.html#module-asyncio" title="(in Python v3.12)">[<code>asyncio</code>{.xref .py .py-mod .docutils .literal
.notranslate}]{.pre}</a>{.reference
.external} support</p>
</li>
<li>
<p>New <a href="index.html#scrapy.http.Response.follow_all" title="scrapy.http.Response.follow_all">[<code>Response.follow_all</code>{.xref .py .py-meth .docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal} method</p>
</li>
<li>
<p><a href="index.html#media-pipeline-ftp">[FTP support]{.std
.std-ref}</a>{.hoverxref .tooltip
.reference .internal} for media pipelines</p>
</li>
<li>
<p>New <a href="index.html#scrapy.http.Response.certificate" title="scrapy.http.Response.certificate">[<code>Response.certificate</code>{.xref .py .py-attr .docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal} attribute</p>
</li>
<li>
<p>IPv6 support through <a href="index.html#std-setting-DNS_RESOLVER">[<code>DNS_RESOLVER</code>{.xref .std .std-setting
.docutils .literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal}</p>
</li>
</ul>
<p>::: {#id71 .section}</p>
<h5 id="backward-incompatible-changesheaderlink-4"><a class="header" href="#backward-incompatible-changesheaderlink-4">Backward-incompatible changes<a href="#id71" title="Permalink to this heading">¶</a>{.headerlink}</a></h5>
<ul>
<li>
<p>Python 2 support has been removed, following <a href="https://www.python.org/doc/sunset-python-2/">Python 2 end-of-life
on January 1,
2020</a>{.reference
.external} (<a href="https://github.com/scrapy/scrapy/issues/4091">issue
4091</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/4114">issue
4114</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/4115">issue
4115</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/4121">issue
4121</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/4138">issue
4138</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/4231">issue
4231</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/4242">issue
4242</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/4304">issue
4304</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/4309">issue
4309</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/4373">issue
4373</a>{.reference
.external})</p>
</li>
<li>
<p>Retry gaveups (see <a href="index.html#std-setting-RETRY_TIMES">[<code>RETRY_TIMES</code>{.xref .std .std-setting .docutils
.literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal}) are now logged as errors instead of
as debug information (<a href="https://github.com/scrapy/scrapy/issues/3171">issue
3171</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/3566">issue
3566</a>{.reference
.external})</p>
</li>
<li>
<p>File extensions that <a href="index.html#scrapy.linkextractors.lxmlhtml.LxmlLinkExtractor" title="scrapy.linkextractors.lxmlhtml.LxmlLinkExtractor">[<code>LinkExtractor</code>{.xref .py .py-class .docutils
.literal
.notranslate}]{.pre}</a>{.reference
.internal} ignores by default now also include [<code>7z</code>{.docutils
.literal .notranslate}]{.pre}, [<code>7zip</code>{.docutils .literal
.notranslate}]{.pre}, [<code>apk</code>{.docutils .literal
.notranslate}]{.pre}, [<code>bz2</code>{.docutils .literal
.notranslate}]{.pre}, [<code>cdr</code>{.docutils .literal
.notranslate}]{.pre}, [<code>dmg</code>{.docutils .literal
.notranslate}]{.pre}, [<code>ico</code>{.docutils .literal
.notranslate}]{.pre}, [<code>iso</code>{.docutils .literal
.notranslate}]{.pre}, [<code>tar</code>{.docutils .literal
.notranslate}]{.pre}, [<code>tar.gz</code>{.docutils .literal
.notranslate}]{.pre}, [<code>webm</code>{.docutils .literal
.notranslate}]{.pre}, and [<code>xz</code>{.docutils .literal
.notranslate}]{.pre} (<a href="https://github.com/scrapy/scrapy/issues/1837">issue
1837</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/2067">issue
2067</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/4066">issue
4066</a>{.reference
.external})</p>
</li>
<li>
<p>The <a href="index.html#std-setting-METAREFRESH_IGNORE_TAGS">[<code>METAREFRESH_IGNORE_TAGS</code>{.xref .std .std-setting .docutils
.literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal} setting is now an empty list by
default, following web browser behavior (<a href="https://github.com/scrapy/scrapy/issues/3844">issue
3844</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/4311">issue
4311</a>{.reference
.external})</p>
</li>
<li>
<p>The <a href="index.html#scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware" title="scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware">[<code>HttpCompressionMiddleware</code>{.xref .py .py-class .docutils
.literal
.notranslate}]{.pre}</a>{.reference
.internal} now includes spaces after commas in the value of the
[<code>Accept-Encoding</code>{.docutils .literal .notranslate}]{.pre} header
that it sets, following web browser behavior (<a href="https://github.com/scrapy/scrapy/issues/4293">issue
4293</a>{.reference
.external})</p>
</li>
<li>
<p>The [<code>__init__</code>{.docutils .literal .notranslate}]{.pre} method of
custom download handlers (see <a href="index.html#std-setting-DOWNLOAD_HANDLERS">[<code>DOWNLOAD_HANDLERS</code>{.xref .std
.std-setting .docutils .literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal}) or subclasses of the following
downloader handlers no longer receives a [<code>settings</code>{.docutils
.literal .notranslate}]{.pre} parameter:</p>
<ul>
<li>
<p>[<code>scrapy.core.downloader.handlers.datauri.DataURIDownloadHandler</code>{.xref
.py .py-class .docutils .literal .notranslate}]{.pre}</p>
</li>
<li>
<p>[<code>scrapy.core.downloader.handlers.file.FileDownloadHandler</code>{.xref
.py .py-class .docutils .literal .notranslate}]{.pre}</p>
</li>
</ul>
<p>Use the [<code>from_settings</code>{.docutils .literal .notranslate}]{.pre} or
[<code>from_crawler</code>{.docutils .literal .notranslate}]{.pre} class
methods to expose such a parameter to your custom download handlers.</p>
<p>(<a href="https://github.com/scrapy/scrapy/issues/4126">issue
4126</a>{.reference
.external})</p>
</li>
<li>
<p>We have refactored the <a href="index.html#scrapy.core.scheduler.Scheduler" title="scrapy.core.scheduler.Scheduler">[<code>scrapy.core.scheduler.Scheduler</code>{.xref .py
.py-class .docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal} class and related queue classes (see
<a href="index.html#std-setting-SCHEDULER_PRIORITY_QUEUE">[<code>SCHEDULER_PRIORITY_QUEUE</code>{.xref .std .std-setting .docutils
.literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal}, <a href="index.html#std-setting-SCHEDULER_DISK_QUEUE">[<code>SCHEDULER_DISK_QUEUE</code>{.xref .std
.std-setting .docutils .literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal} and <a href="index.html#std-setting-SCHEDULER_MEMORY_QUEUE">[<code>SCHEDULER_MEMORY_QUEUE</code>{.xref
.std .std-setting .docutils .literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal}) to make it easier to implement
custom scheduler queue classes. See <a href="#scheduler-queue-changes">[Changes to scheduler queue
classes]{.std .std-ref}</a>{.hoverxref
.tooltip .reference .internal} below for details.</p>
</li>
<li>
<p>Overridden settings are now logged in a different format. This is
more in line with similar information logged at startup (<a href="https://github.com/scrapy/scrapy/issues/4199">issue
4199</a>{.reference
.external})
:::</p>
</li>
</ul>
<p>::: {#id72 .section}</p>
<h5 id="deprecation-removalsheaderlink-5"><a class="header" href="#deprecation-removalsheaderlink-5">Deprecation removals<a href="#id72" title="Permalink to this heading">¶</a>{.headerlink}</a></h5>
<ul>
<li>
<p>The <a href="index.html#topics-shell">[Scrapy shell]{.std
.std-ref}</a>{.hoverxref .tooltip .reference
.internal} no longer provides a sel proxy object, use
[<code>response.selector</code>{.xref .py .py-meth .docutils .literal
.notranslate}]{.pre} instead (<a href="https://github.com/scrapy/scrapy/issues/4347">issue
4347</a>{.reference
.external})</p>
</li>
<li>
<p>LevelDB support has been removed (<a href="https://github.com/scrapy/scrapy/issues/4112">issue
4112</a>{.reference
.external})</p>
</li>
<li>
<p>The following functions have been removed from
[<code>scrapy.utils.python</code>{.xref .py .py-mod .docutils .literal
.notranslate}]{.pre}: [<code>isbinarytext</code>{.docutils .literal
.notranslate}]{.pre}, [<code>is_writable</code>{.docutils .literal
.notranslate}]{.pre}, [<code>setattr_default</code>{.docutils .literal
.notranslate}]{.pre}, [<code>stringify_dict</code>{.docutils .literal
.notranslate}]{.pre} (<a href="https://github.com/scrapy/scrapy/issues/4362">issue
4362</a>{.reference
.external})
:::</p>
</li>
</ul>
<p>::: {#id73 .section}</p>
<h5 id="deprecationsheaderlink-6"><a class="header" href="#deprecationsheaderlink-6">Deprecations<a href="#id73" title="Permalink to this heading">¶</a>{.headerlink}</a></h5>
<ul>
<li>
<p>Using environment variables prefixed with [<code>SCRAPY_</code>{.docutils
.literal .notranslate}]{.pre} to override settings is deprecated
(<a href="https://github.com/scrapy/scrapy/issues/4300">issue
4300</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/4374">issue
4374</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/4375">issue
4375</a>{.reference
.external})</p>
</li>
<li>
<p>[<code>scrapy.linkextractors.FilteringLinkExtractor</code>{.xref .py .py-class
.docutils .literal .notranslate}]{.pre} is deprecated, use
<a href="index.html#scrapy.linkextractors.lxmlhtml.LxmlLinkExtractor" title="scrapy.linkextractors.lxmlhtml.LxmlLinkExtractor">[<code>scrapy.linkextractors.LinkExtractor</code>{.xref .py .py-class
.docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal} instead (<a href="https://github.com/scrapy/scrapy/issues/4045">issue
4045</a>{.reference
.external})</p>
</li>
<li>
<p>The [<code>noconnect</code>{.docutils .literal .notranslate}]{.pre} query
string argument of proxy URLs is deprecated and should be removed
from proxy URLs (<a href="https://github.com/scrapy/scrapy/issues/4198">issue
4198</a>{.reference
.external})</p>
</li>
<li>
<p>The [<code>next</code>{.xref .py .py-meth .docutils .literal
.notranslate}]{.pre} method of
[<code>scrapy.utils.python.MutableChain</code>{.xref .py .py-class .docutils
.literal .notranslate}]{.pre} is deprecated, use the global
<a href="https://docs.python.org/3/library/functions.html#next" title="(in Python v3.12)">[<code>next()</code>{.xref .py .py-func .docutils .literal
.notranslate}]{.pre}</a>{.reference
.external} function or [<code>MutableChain.__next__</code>{.xref .py .py-meth
.docutils .literal .notranslate}]{.pre} instead (<a href="https://github.com/scrapy/scrapy/issues/4153">issue
4153</a>{.reference
.external})
:::</p>
</li>
</ul>
<p>::: {#id74 .section}</p>
<h5 id="new-featuresheaderlink-6"><a class="header" href="#new-featuresheaderlink-6">New features<a href="#id74" title="Permalink to this heading">¶</a>{.headerlink}</a></h5>
<ul>
<li>
<p>Added <a href="index.html#document-topics/coroutines">[partial
support]{.doc}</a>{.reference
.internal} for Python's <a href="https://docs.python.org/3/reference/compound_stmts.html#async" title="(in Python v3.12)">[coroutine syntax]{.xref .std
.std-ref}</a>{.reference
.external} and <a href="index.html#document-topics/asyncio">[experimental
support]{.doc}</a>{.reference
.internal} for <a href="https://docs.python.org/3/library/asyncio.html#module-asyncio" title="(in Python v3.12)">[<code>asyncio</code>{.xref .py .py-mod .docutils .literal
.notranslate}]{.pre}</a>{.reference
.external} and <a href="https://docs.python.org/3/library/asyncio.html#module-asyncio" title="(in Python v3.12)">[<code>asyncio</code>{.xref .py .py-mod .docutils .literal
.notranslate}]{.pre}</a>{.reference
.external}-powered libraries (<a href="https://github.com/scrapy/scrapy/issues/4010">issue
4010</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/4259">issue
4259</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/4269">issue
4269</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/4270">issue
4270</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/4271">issue
4271</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/4316">issue
4316</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/4318">issue
4318</a>{.reference
.external})</p>
</li>
<li>
<p>The new <a href="index.html#scrapy.http.Response.follow_all" title="scrapy.http.Response.follow_all">[<code>Response.follow_all</code>{.xref .py .py-meth .docutils
.literal
.notranslate}]{.pre}</a>{.reference
.internal} method offers the same functionality as
<a href="index.html#scrapy.http.Response.follow" title="scrapy.http.Response.follow">[<code>Response.follow</code>{.xref .py .py-meth .docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal} but supports an iterable of URLs as input and returns an
iterable of requests (<a href="https://github.com/scrapy/scrapy/issues/2582">issue
2582</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/4057">issue
4057</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/4286">issue
4286</a>{.reference
.external})</p>
</li>
<li>
<p><a href="index.html#topics-media-pipeline">[Media pipelines]{.std
.std-ref}</a>{.hoverxref .tooltip
.reference .internal} now support <a href="index.html#media-pipeline-ftp">[FTP storage]{.std
.std-ref}</a>{.hoverxref .tooltip
.reference .internal} (<a href="https://github.com/scrapy/scrapy/issues/3928">issue
3928</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/3961">issue
3961</a>{.reference
.external})</p>
</li>
<li>
<p>The new <a href="index.html#scrapy.http.Response.certificate" title="scrapy.http.Response.certificate">[<code>Response.certificate</code>{.xref .py .py-attr .docutils
.literal
.notranslate}]{.pre}</a>{.reference
.internal} attribute exposes the SSL certificate of the server as a
<a href="https://docs.twisted.org/en/stable/api/twisted.internet.ssl.Certificate.html" title="(in Twisted)">[<code>twisted.internet.ssl.Certificate</code>{.xref .py .py-class .docutils
.literal
.notranslate}]{.pre}</a>{.reference
.external} object for HTTPS responses (<a href="https://github.com/scrapy/scrapy/issues/2726">issue
2726</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/4054">issue
4054</a>{.reference
.external})</p>
</li>
<li>
<p>A new <a href="index.html#std-setting-DNS_RESOLVER">[<code>DNS_RESOLVER</code>{.xref .std .std-setting .docutils .literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal} setting allows enabling IPv6 support
(<a href="https://github.com/scrapy/scrapy/issues/1031">issue
1031</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/4227">issue
4227</a>{.reference
.external})</p>
</li>
<li>
<p>A new <a href="index.html#std-setting-SCRAPER_SLOT_MAX_ACTIVE_SIZE">[<code>SCRAPER_SLOT_MAX_ACTIVE_SIZE</code>{.xref .std .std-setting
.docutils .literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal} setting allows configuring the
existing soft limit that pauses request downloads when the total
response data being processed is too high (<a href="https://github.com/scrapy/scrapy/issues/1410">issue
1410</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/3551">issue
3551</a>{.reference
.external})</p>
</li>
<li>
<p>A new <a href="index.html#std-setting-TWISTED_REACTOR">[<code>TWISTED_REACTOR</code>{.xref .std .std-setting .docutils .literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal} setting allows customizing the
<a href="https://docs.twisted.org/en/stable/api/twisted.internet.reactor.html" title="(in Twisted)">[<code>reactor</code>{.xref .py .py-mod .docutils .literal
.notranslate}]{.pre}</a>{.reference
.external} that Scrapy uses, allowing to <a href="index.html#document-topics/asyncio">[enable asyncio
support]{.doc}</a>{.reference
.internal} or deal with a <a href="index.html#faq-specific-reactor">[common macOS issue]{.std
.std-ref}</a>{.hoverxref .tooltip
.reference .internal} (<a href="https://github.com/scrapy/scrapy/issues/2905">issue
2905</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/4294">issue
4294</a>{.reference
.external})</p>
</li>
<li>
<p>Scheduler disk and memory queues may now use the class methods
[<code>from_crawler</code>{.docutils .literal .notranslate}]{.pre} or
[<code>from_settings</code>{.docutils .literal .notranslate}]{.pre} (<a href="https://github.com/scrapy/scrapy/issues/3884">issue
3884</a>{.reference
.external})</p>
</li>
<li>
<p>The new <a href="index.html#scrapy.http.Response.cb_kwargs" title="scrapy.http.Response.cb_kwargs">[<code>Response.cb_kwargs</code>{.xref .py .py-attr .docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal} attribute serves as a shortcut for
<a href="index.html#scrapy.http.Request.cb_kwargs" title="scrapy.http.Request.cb_kwargs">[<code>Response.request.cb_kwargs</code>{.xref .py .py-attr .docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal} (<a href="https://github.com/scrapy/scrapy/issues/4331">issue
4331</a>{.reference
.external})</p>
</li>
<li>
<p><a href="index.html#scrapy.http.Response.follow" title="scrapy.http.Response.follow">[<code>Response.follow</code>{.xref .py .py-meth .docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal} now supports a [<code>flags</code>{.docutils .literal
.notranslate}]{.pre} parameter, for consistency with
<a href="index.html#scrapy.http.Request" title="scrapy.http.Request">[<code>Request</code>{.xref .py .py-class .docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal} (<a href="https://github.com/scrapy/scrapy/issues/4277">issue
4277</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/4279">issue
4279</a>{.reference
.external})</p>
</li>
<li>
<p><a href="index.html#topics-loaders-processors">[Item loader processors]{.std
.std-ref}</a>{.hoverxref .tooltip
.reference .internal} can now be regular functions, they no longer
need to be methods (<a href="https://github.com/scrapy/scrapy/issues/3899">issue
3899</a>{.reference
.external})</p>
</li>
<li>
<p><a href="index.html#scrapy.spiders.Rule" title="scrapy.spiders.Rule">[<code>Rule</code>{.xref .py .py-class .docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal} now accepts an [<code>errback</code>{.docutils .literal
.notranslate}]{.pre} parameter (<a href="https://github.com/scrapy/scrapy/issues/4000">issue
4000</a>{.reference
.external})</p>
</li>
<li>
<p><a href="index.html#scrapy.http.Request" title="scrapy.http.Request">[<code>Request</code>{.xref .py .py-class .docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal} no longer requires a [<code>callback</code>{.docutils .literal
.notranslate}]{.pre} parameter when an [<code>errback</code>{.docutils .literal
.notranslate}]{.pre} parameter is specified (<a href="https://github.com/scrapy/scrapy/issues/3586">issue
3586</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/4008">issue
4008</a>{.reference
.external})</p>
</li>
<li>
<p><a href="index.html#scrapy.logformatter.LogFormatter" title="scrapy.logformatter.LogFormatter">[<code>LogFormatter</code>{.xref .py .py-class .docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal} now supports some additional methods:</p>
<ul>
<li>
<p><a href="index.html#scrapy.logformatter.LogFormatter.download_error" title="scrapy.logformatter.LogFormatter.download_error">[<code>download_error</code>{.xref .py .py-class .docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal} for download errors</p>
</li>
<li>
<p><a href="index.html#scrapy.logformatter.LogFormatter.item_error" title="scrapy.logformatter.LogFormatter.item_error">[<code>item_error</code>{.xref .py .py-class .docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal} for exceptions raised during item processing by
<a href="index.html#topics-item-pipeline">[item pipelines]{.std
.std-ref}</a>{.hoverxref .tooltip
.reference .internal}</p>
</li>
<li>
<p><a href="index.html#scrapy.logformatter.LogFormatter.spider_error" title="scrapy.logformatter.LogFormatter.spider_error">[<code>spider_error</code>{.xref .py .py-class .docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal} for exceptions raised from <a href="index.html#topics-spiders">[spider callbacks]{.std
.std-ref}</a>{.hoverxref .tooltip
.reference .internal}</p>
</li>
</ul>
<p>(<a href="https://github.com/scrapy/scrapy/issues/374">issue 374</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/3986">issue
3986</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/3989">issue
3989</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/4176">issue
4176</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/4188">issue
4188</a>{.reference
.external})</p>
</li>
<li>
<p>The [<code>FEED_URI</code>{.xref .std .std-setting .docutils .literal
.notranslate}]{.pre} setting now supports <a href="https://docs.python.org/3/library/pathlib.html#pathlib.Path" title="(in Python v3.12)">[<code>pathlib.Path</code>{.xref .py
.py-class .docutils .literal
.notranslate}]{.pre}</a>{.reference
.external} values (<a href="https://github.com/scrapy/scrapy/issues/3731">issue
3731</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/4074">issue
4074</a>{.reference
.external})</p>
</li>
<li>
<p>A new <a href="index.html#std-signal-request_left_downloader">[<code>request_left_downloader</code>{.xref .std .std-signal .docutils
.literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal} signal is sent when a request leaves
the downloader (<a href="https://github.com/scrapy/scrapy/issues/4303">issue
4303</a>{.reference
.external})</p>
</li>
<li>
<p>Scrapy logs a warning when it detects a request callback or errback
that uses [<code>yield</code>{.docutils .literal .notranslate}]{.pre} but also
returns a value, since the returned value would be lost (<a href="https://github.com/scrapy/scrapy/issues/3484">issue
3484</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/3869">issue
3869</a>{.reference
.external})</p>
</li>
<li>
<p><a href="index.html#scrapy.spiders.Spider" title="scrapy.spiders.Spider">[<code>Spider</code>{.xref .py .py-class .docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal} objects now raise an <a href="https://docs.python.org/3/library/exceptions.html#AttributeError" title="(in Python v3.12)">[<code>AttributeError</code>{.xref .py .py-exc
.docutils .literal
.notranslate}]{.pre}</a>{.reference
.external} exception if they do not have a [<code>start_urls</code>{.xref .py
.py-class .docutils .literal .notranslate}]{.pre} attribute nor
reimplement [<code>start_requests</code>{.xref .py .py-class .docutils .literal
.notranslate}]{.pre}, but have a [<code>start_url</code>{.docutils .literal
.notranslate}]{.pre} attribute (<a href="https://github.com/scrapy/scrapy/issues/4133">issue
4133</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/4170">issue
4170</a>{.reference
.external})</p>
</li>
<li>
<p><a href="index.html#scrapy.exporters.BaseItemExporter" title="scrapy.exporters.BaseItemExporter">[<code>BaseItemExporter</code>{.xref .py .py-class .docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal} subclasses may now use
[<code>super().__init__(**kwargs)</code>{.docutils .literal
.notranslate}]{.pre} instead of [<code>self._configure(kwargs)</code>{.docutils
.literal .notranslate}]{.pre} in their [<code>__init__</code>{.docutils
.literal .notranslate}]{.pre} method, passing
[<code>dont_fail=True</code>{.docutils .literal .notranslate}]{.pre} to the
parent [<code>__init__</code>{.docutils .literal .notranslate}]{.pre} method if
needed, and accessing [<code>kwargs</code>{.docutils .literal
.notranslate}]{.pre} at [<code>self._kwargs</code>{.docutils .literal
.notranslate}]{.pre} after calling their parent
[<code>__init__</code>{.docutils .literal .notranslate}]{.pre} method (<a href="https://github.com/scrapy/scrapy/issues/4193">issue
4193</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/4370">issue
4370</a>{.reference
.external})</p>
</li>
<li>
<p>A new [<code>keep_fragments</code>{.docutils .literal .notranslate}]{.pre}
parameter of [<code>scrapy.utils.request.request_fingerprint</code>{.docutils
.literal .notranslate}]{.pre} allows to generate different
fingerprints for requests with different fragments in their URL
(<a href="https://github.com/scrapy/scrapy/issues/4104">issue
4104</a>{.reference
.external})</p>
</li>
<li>
<p>Download handlers (see <a href="index.html#std-setting-DOWNLOAD_HANDLERS">[<code>DOWNLOAD_HANDLERS</code>{.xref .std .std-setting
.docutils .literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal}) may now use the
[<code>from_settings</code>{.docutils .literal .notranslate}]{.pre} and
[<code>from_crawler</code>{.docutils .literal .notranslate}]{.pre} class
methods that other Scrapy components already supported (<a href="https://github.com/scrapy/scrapy/issues/4126">issue
4126</a>{.reference
.external})</p>
</li>
<li>
<p>[<code>scrapy.utils.python.MutableChain.__iter__</code>{.xref .py .py-class
.docutils .literal .notranslate}]{.pre} now returns
[<code>self</code>{.docutils .literal .notranslate}]{.pre}, <a href="https://lgtm.com/rules/4850080/">allowing it to be
used as a sequence</a>{.reference
.external} (<a href="https://github.com/scrapy/scrapy/issues/4153">issue
4153</a>{.reference
.external})
:::</p>
</li>
</ul>
<p>::: {#id75 .section}</p>
<h5 id="bug-fixesheaderlink-6"><a class="header" href="#bug-fixesheaderlink-6">Bug fixes<a href="#id75" title="Permalink to this heading">¶</a>{.headerlink}</a></h5>
<ul>
<li>
<p>The <a href="index.html#std-command-crawl">[<code>crawl</code>{.xref .std .std-command .docutils .literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal} command now also exits with exit code
1 when an exception happens before the crawling starts (<a href="https://github.com/scrapy/scrapy/issues/4175">issue
4175</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/4207">issue
4207</a>{.reference
.external})</p>
</li>
<li>
<p><a href="index.html#scrapy.linkextractors.lxmlhtml.LxmlLinkExtractor.extract_links" title="scrapy.linkextractors.lxmlhtml.LxmlLinkExtractor.extract_links">[<code>LinkExtractor.extract_links</code>{.xref .py .py-class .docutils
.literal
.notranslate}]{.pre}</a>{.reference
.internal} no longer re-encodes the query string or URLs from
non-UTF-8 responses in UTF-8 (<a href="https://github.com/scrapy/scrapy/issues/998">issue
998</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/1403">issue
1403</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/1949">issue
1949</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/4321">issue
4321</a>{.reference
.external})</p>
</li>
<li>
<p>The first spider middleware (see <a href="index.html#std-setting-SPIDER_MIDDLEWARES">[<code>SPIDER_MIDDLEWARES</code>{.xref .std
.std-setting .docutils .literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal}) now also processes exceptions raised
from callbacks that are generators (<a href="https://github.com/scrapy/scrapy/issues/4260">issue
4260</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/4272">issue
4272</a>{.reference
.external})</p>
</li>
<li>
<p>Redirects to URLs starting with 3 slashes ([<code>///</code>{.docutils .literal
.notranslate}]{.pre}) are now supported (<a href="https://github.com/scrapy/scrapy/issues/4032">issue
4032</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/4042">issue
4042</a>{.reference
.external})</p>
</li>
<li>
<p><a href="index.html#scrapy.http.Request" title="scrapy.http.Request">[<code>Request</code>{.xref .py .py-class .docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal} no longer accepts strings as [<code>url</code>{.docutils .literal
.notranslate}]{.pre} simply because they have a colon (<a href="https://github.com/scrapy/scrapy/issues/2552">issue
2552</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/4094">issue
4094</a>{.reference
.external})</p>
</li>
<li>
<p>The correct encoding is now used for attach names in
<a href="index.html#scrapy.mail.MailSender" title="scrapy.mail.MailSender">[<code>MailSender</code>{.xref .py .py-class .docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal} (<a href="https://github.com/scrapy/scrapy/issues/4229">issue
4229</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/4239">issue
4239</a>{.reference
.external})</p>
</li>
<li>
<p>[<code>RFPDupeFilter</code>{.xref .py .py-class .docutils .literal
.notranslate}]{.pre}, the default <a href="index.html#std-setting-DUPEFILTER_CLASS">[<code>DUPEFILTER_CLASS</code>{.xref .std
.std-setting .docutils .literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal}, no longer writes an extra
[<code>\r</code>{.docutils .literal .notranslate}]{.pre} character on each line
in Windows, which made the size of the [<code>requests.seen</code>{.docutils
.literal .notranslate}]{.pre} file unnecessarily large on that
platform (<a href="https://github.com/scrapy/scrapy/issues/4283">issue
4283</a>{.reference
.external})</p>
</li>
<li>
<p>Z shell auto-completion now looks for [<code>.html</code>{.docutils .literal
.notranslate}]{.pre} files, not [<code>.http</code>{.docutils .literal
.notranslate}]{.pre} files, and covers the [<code>-h</code>{.docutils .literal
.notranslate}]{.pre} command-line switch (<a href="https://github.com/scrapy/scrapy/issues/4122">issue
4122</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/4291">issue
4291</a>{.reference
.external})</p>
</li>
<li>
<p>Adding items to a [<code>scrapy.utils.datatypes.LocalCache</code>{.xref .py
.py-class .docutils .literal .notranslate}]{.pre} object without a
[<code>limit</code>{.docutils .literal .notranslate}]{.pre} defined no longer
raises a <a href="https://docs.python.org/3/library/exceptions.html#TypeError" title="(in Python v3.12)">[<code>TypeError</code>{.xref .py .py-exc .docutils .literal
.notranslate}]{.pre}</a>{.reference
.external} exception (<a href="https://github.com/scrapy/scrapy/issues/4123">issue
4123</a>{.reference
.external})</p>
</li>
<li>
<p>Fixed a typo in the message of the <a href="https://docs.python.org/3/library/exceptions.html#ValueError" title="(in Python v3.12)">[<code>ValueError</code>{.xref .py .py-exc
.docutils .literal
.notranslate}]{.pre}</a>{.reference
.external} exception raised when
[<code>scrapy.utils.misc.create_instance()</code>{.xref .py .py-func .docutils
.literal .notranslate}]{.pre} gets both [<code>settings</code>{.docutils
.literal .notranslate}]{.pre} and [<code>crawler</code>{.docutils .literal
.notranslate}]{.pre} set to [<code>None</code>{.docutils .literal
.notranslate}]{.pre} (<a href="https://github.com/scrapy/scrapy/issues/4128">issue
4128</a>{.reference
.external})
:::</p>
</li>
</ul>
<p>::: {#id76 .section}</p>
<h5 id="documentationheaderlink-7"><a class="header" href="#documentationheaderlink-7">Documentation<a href="#id76" title="Permalink to this heading">¶</a>{.headerlink}</a></h5>
<ul>
<li>
<p>API documentation now links to an online, syntax-highlighted view of
the corresponding source code (<a href="https://github.com/scrapy/scrapy/issues/4148">issue
4148</a>{.reference
.external})</p>
</li>
<li>
<p>Links to unexisting documentation pages now allow access to the
sidebar (<a href="https://github.com/scrapy/scrapy/issues/4152">issue
4152</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/4169">issue
4169</a>{.reference
.external})</p>
</li>
<li>
<p>Cross-references within our documentation now display a tooltip when
hovered (<a href="https://github.com/scrapy/scrapy/issues/4173">issue
4173</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/4183">issue
4183</a>{.reference
.external})</p>
</li>
<li>
<p>Improved the documentation about
<a href="index.html#scrapy.linkextractors.lxmlhtml.LxmlLinkExtractor.extract_links" title="scrapy.linkextractors.lxmlhtml.LxmlLinkExtractor.extract_links">[<code>LinkExtractor.extract_links</code>{.xref .py .py-meth .docutils
.literal
.notranslate}]{.pre}</a>{.reference
.internal} and simplified <a href="index.html#topics-link-extractors">[Link Extractors]{.std
.std-ref}</a>{.hoverxref .tooltip
.reference .internal} (<a href="https://github.com/scrapy/scrapy/issues/4045">issue
4045</a>{.reference
.external})</p>
</li>
<li>
<p>Clarified how <a href="index.html#scrapy.loader.ItemLoader.item" title="scrapy.loader.ItemLoader.item">[<code>ItemLoader.item</code>{.xref .py .py-class .docutils
.literal
.notranslate}]{.pre}</a>{.reference
.internal} works (<a href="https://github.com/scrapy/scrapy/issues/3574">issue
3574</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/4099">issue
4099</a>{.reference
.external})</p>
</li>
<li>
<p>Clarified that <a href="https://docs.python.org/3/library/logging.html#logging.basicConfig" title="(in Python v3.12)">[<code>logging.basicConfig()</code>{.xref .py .py-func
.docutils .literal
.notranslate}]{.pre}</a>{.reference
.external} should not be used when also using
<a href="index.html#scrapy.crawler.CrawlerProcess" title="scrapy.crawler.CrawlerProcess">[<code>CrawlerProcess</code>{.xref .py .py-class .docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal} (<a href="https://github.com/scrapy/scrapy/issues/2149">issue
2149</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/2352">issue
2352</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/3146">issue
3146</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/3960">issue
3960</a>{.reference
.external})</p>
</li>
<li>
<p>Clarified the requirements for <a href="index.html#scrapy.http.Request" title="scrapy.http.Request">[<code>Request</code>{.xref .py .py-class
.docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal} objects <a href="index.html#request-serialization">[when using persistence]{.std
.std-ref}</a>{.hoverxref .tooltip
.reference .internal} (<a href="https://github.com/scrapy/scrapy/issues/4124">issue
4124</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/4139">issue
4139</a>{.reference
.external})</p>
</li>
<li>
<p>Clarified how to install a <a href="index.html#media-pipeline-example">[custom image pipeline]{.std
.std-ref}</a>{.hoverxref .tooltip
.reference .internal} (<a href="https://github.com/scrapy/scrapy/issues/4034">issue
4034</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/4252">issue
4252</a>{.reference
.external})</p>
</li>
<li>
<p>Fixed the signatures of the [<code>file_path</code>{.docutils .literal
.notranslate}]{.pre} method in <a href="index.html#topics-media-pipeline">[media pipeline]{.std
.std-ref}</a>{.hoverxref .tooltip
.reference .internal} examples (<a href="https://github.com/scrapy/scrapy/issues/4290">issue
4290</a>{.reference
.external})</p>
</li>
<li>
<p>Covered a backward-incompatible change in Scrapy 1.7.0 affecting
custom <a href="index.html#scrapy.core.scheduler.Scheduler" title="scrapy.core.scheduler.Scheduler">[<code>scrapy.core.scheduler.Scheduler</code>{.xref .py .py-class
.docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal} subclasses (<a href="https://github.com/scrapy/scrapy/issues/4274">issue
4274</a>{.reference
.external})</p>
</li>
<li>
<p>Improved the [<code>README.rst</code>{.docutils .literal .notranslate}]{.pre}
and [<code>CODE_OF_CONDUCT.md</code>{.docutils .literal .notranslate}]{.pre}
files (<a href="https://github.com/scrapy/scrapy/issues/4059">issue
4059</a>{.reference
.external})</p>
</li>
<li>
<p>Documentation examples are now checked as part of our test suite and
we have fixed some of the issues detected (<a href="https://github.com/scrapy/scrapy/issues/4142">issue
4142</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/4146">issue
4146</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/4171">issue
4171</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/4184">issue
4184</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/4190">issue
4190</a>{.reference
.external})</p>
</li>
<li>
<p>Fixed logic issues, broken links and typos (<a href="https://github.com/scrapy/scrapy/issues/4247">issue
4247</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/4258">issue
4258</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/4282">issue
4282</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/4288">issue
4288</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/4305">issue
4305</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/4308">issue
4308</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/4323">issue
4323</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/4338">issue
4338</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/4359">issue
4359</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/4361">issue
4361</a>{.reference
.external})</p>
</li>
<li>
<p>Improved consistency when referring to the [<code>__init__</code>{.docutils
.literal .notranslate}]{.pre} method of an object (<a href="https://github.com/scrapy/scrapy/issues/4086">issue
4086</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/4088">issue
4088</a>{.reference
.external})</p>
</li>
<li>
<p>Fixed an inconsistency between code and output in <a href="index.html#intro-overview">[Scrapy at a
glance]{.std .std-ref}</a>{.hoverxref
.tooltip .reference .internal} (<a href="https://github.com/scrapy/scrapy/issues/4213">issue
4213</a>{.reference
.external})</p>
</li>
<li>
<p>Extended <a href="https://www.sphinx-doc.org/en/master/usage/extensions/intersphinx.html#module-sphinx.ext.intersphinx" title="(in Sphinx v7.3.0)">[<code>intersphinx</code>{.xref .py .py-mod .docutils .literal
.notranslate}]{.pre}</a>{.reference
.external} usage (<a href="https://github.com/scrapy/scrapy/issues/4147">issue
4147</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/4172">issue
4172</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/4185">issue
4185</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/4194">issue
4194</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/4197">issue
4197</a>{.reference
.external})</p>
</li>
<li>
<p>We now use a recent version of Python to build the documentation
(<a href="https://github.com/scrapy/scrapy/issues/4140">issue
4140</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/4249">issue
4249</a>{.reference
.external})</p>
</li>
<li>
<p>Cleaned up documentation (<a href="https://github.com/scrapy/scrapy/issues/4143">issue
4143</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/4275">issue
4275</a>{.reference
.external})
:::</p>
</li>
</ul>
<p>::: {#id77 .section}</p>
<h5 id="quality-assuranceheaderlink-7"><a class="header" href="#quality-assuranceheaderlink-7">Quality assurance<a href="#id77" title="Permalink to this heading">¶</a>{.headerlink}</a></h5>
<ul>
<li>
<p>Re-enabled proxy [<code>CONNECT</code>{.docutils .literal .notranslate}]{.pre}
tests (<a href="https://github.com/scrapy/scrapy/issues/2545">issue
2545</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/4114">issue
4114</a>{.reference
.external})</p>
</li>
<li>
<p>Added <a href="https://bandit.readthedocs.io/">Bandit</a>{.reference .external}
security checks to our test suite (<a href="https://github.com/scrapy/scrapy/issues/4162">issue
4162</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/4181">issue
4181</a>{.reference
.external})</p>
</li>
<li>
<p>Added <a href="https://flake8.pycqa.org/en/latest/">Flake8</a>{.reference
.external} style checks to our test suite and applied many of the
corresponding changes (<a href="https://github.com/scrapy/scrapy/issues/3944">issue
3944</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/3945">issue
3945</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/4137">issue
4137</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/4157">issue
4157</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/4167">issue
4167</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/4174">issue
4174</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/4186">issue
4186</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/4195">issue
4195</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/4238">issue
4238</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/4246">issue
4246</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/4355">issue
4355</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/4360">issue
4360</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/4365">issue
4365</a>{.reference
.external})</p>
</li>
<li>
<p>Improved test coverage (<a href="https://github.com/scrapy/scrapy/issues/4097">issue
4097</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/4218">issue
4218</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/4236">issue
4236</a>{.reference
.external})</p>
</li>
<li>
<p>Started reporting slowest tests, and improved the performance of
some of them (<a href="https://github.com/scrapy/scrapy/issues/4163">issue
4163</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/4164">issue
4164</a>{.reference
.external})</p>
</li>
<li>
<p>Fixed broken tests and refactored some tests (<a href="https://github.com/scrapy/scrapy/issues/4014">issue
4014</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/4095">issue
4095</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/4244">issue
4244</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/4268">issue
4268</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/4372">issue
4372</a>{.reference
.external})</p>
</li>
<li>
<p>Modified the <a href="https://tox.wiki/en/latest/index.html" title="(in Python v4.11)">[tox]{.xref .std
.std-doc}</a>{.reference
.external} configuration to allow running tests with any Python
version, run <a href="https://bandit.readthedocs.io/">Bandit</a>{.reference
.external} and
<a href="https://flake8.pycqa.org/en/latest/">Flake8</a>{.reference .external}
tests by default, and enforce a minimum tox version programmatically
(<a href="https://github.com/scrapy/scrapy/issues/4179">issue
4179</a>{.reference
.external})</p>
</li>
<li>
<p>Cleaned up code (<a href="https://github.com/scrapy/scrapy/issues/3937">issue
3937</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/4208">issue
4208</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/4209">issue
4209</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/4210">issue
4210</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/4212">issue
4212</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/4369">issue
4369</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/4376">issue
4376</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/4378">issue
4378</a>{.reference
.external})
:::</p>
</li>
</ul>
<p>::: {#changes-to-scheduler-queue-classes .section}
[]{#scheduler-queue-changes}</p>
<h5 id="changes-to-scheduler-queue-classesheaderlink"><a class="header" href="#changes-to-scheduler-queue-classesheaderlink">Changes to scheduler queue classes<a href="#changes-to-scheduler-queue-classes" title="Permalink to this heading">¶</a>{.headerlink}</a></h5>
<p>The following changes may impact any custom queue classes of all types:</p>
<ul>
<li>The [<code>push</code>{.docutils .literal .notranslate}]{.pre} method no longer
receives a second positional parameter containing
[<code>request.priority</code>{.docutils .literal
.notranslate}]{.pre}<code> </code>{.docutils .literal
.notranslate}[<code>*</code>{.docutils .literal
.notranslate}]{.pre}<code> </code>{.docutils .literal
.notranslate}[<code>-1</code>{.docutils .literal .notranslate}]{.pre}. If you
need that value, get it from the first positional parameter,
[<code>request</code>{.docutils .literal .notranslate}]{.pre}, instead, or use
the new [<code>priority()</code>{.xref .py .py-meth .docutils .literal
.notranslate}]{.pre} method in
[<code>scrapy.core.scheduler.ScrapyPriorityQueue</code>{.xref .py .py-class
.docutils .literal .notranslate}]{.pre} subclasses.</li>
</ul>
<p>The following changes may impact custom priority queue classes:</p>
<ul>
<li>
<p>In the [<code>__init__</code>{.docutils .literal .notranslate}]{.pre} method or
the [<code>from_crawler</code>{.docutils .literal .notranslate}]{.pre} or
[<code>from_settings</code>{.docutils .literal .notranslate}]{.pre} class
methods:</p>
<ul>
<li>
<p>The parameter that used to contain a factory function,
[<code>qfactory</code>{.docutils .literal .notranslate}]{.pre}, is now
passed as a keyword parameter named
[<code>downstream_queue_cls</code>{.docutils .literal .notranslate}]{.pre}.</p>
</li>
<li>
<p>A new keyword parameter has been added: [<code>key</code>{.docutils
.literal .notranslate}]{.pre}. It is a string that is always an
empty string for memory queues and indicates the
[<code>JOB_DIR</code>{.xref .std .std-setting .docutils .literal
.notranslate}]{.pre} value for disk queues.</p>
</li>
<li>
<p>The parameter for disk queues that contains data from the
previous crawl, [<code>startprios</code>{.docutils .literal
.notranslate}]{.pre} or [<code>slot_startprios</code>{.docutils .literal
.notranslate}]{.pre}, is now passed as a keyword parameter named
[<code>startprios</code>{.docutils .literal .notranslate}]{.pre}.</p>
</li>
<li>
<p>The [<code>serialize</code>{.docutils .literal .notranslate}]{.pre}
parameter is no longer passed. The disk queue class must take
care of request serialization on its own before writing to disk,
using the [<code>request_to_dict()</code>{.xref .py .py-func .docutils
.literal .notranslate}]{.pre} and [<code>request_from_dict()</code>{.xref
.py .py-func .docutils .literal .notranslate}]{.pre} functions
from the [<code>scrapy.utils.reqser</code>{.xref .py .py-mod .docutils
.literal .notranslate}]{.pre} module.</p>
</li>
</ul>
</li>
</ul>
<p>The following changes may impact custom disk and memory queue classes:</p>
<ul>
<li>The signature of the [<code>__init__</code>{.docutils .literal
.notranslate}]{.pre} method is now [<code>__init__(self,</code>{.docutils
.literal .notranslate}]{.pre}<code> </code>{.docutils .literal
.notranslate}[<code>crawler,</code>{.docutils .literal
.notranslate}]{.pre}<code> </code>{.docutils .literal
.notranslate}[<code>key)</code>{.docutils .literal .notranslate}]{.pre}.</li>
</ul>
<p>The following changes affect specifically the
[<code>ScrapyPriorityQueue</code>{.xref .py .py-class .docutils .literal
.notranslate}]{.pre} and [<code>DownloaderAwarePriorityQueue</code>{.xref .py
.py-class .docutils .literal .notranslate}]{.pre} classes from
<a href="index.html#module-scrapy.core.scheduler" title="scrapy.core.scheduler">[<code>scrapy.core.scheduler</code>{.xref .py .py-mod .docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal} and may affect subclasses:</p>
<ul>
<li>
<p>In the [<code>__init__</code>{.docutils .literal .notranslate}]{.pre} method,
most of the changes described above apply.</p>
<p>[<code>__init__</code>{.docutils .literal .notranslate}]{.pre} may still
receive all parameters as positional parameters, however:</p>
<ul>
<li>
<p>[<code>downstream_queue_cls</code>{.docutils .literal .notranslate}]{.pre},
which replaced [<code>qfactory</code>{.docutils .literal
.notranslate}]{.pre}, must be instantiated differently.</p>
<p>[<code>qfactory</code>{.docutils .literal .notranslate}]{.pre} was
instantiated with a priority value (integer).</p>
<p>Instances of [<code>downstream_queue_cls</code>{.docutils .literal
.notranslate}]{.pre} should be created using the new
[<code>ScrapyPriorityQueue.qfactory</code>{.xref .py .py-meth .docutils
.literal .notranslate}]{.pre} or
[<code>DownloaderAwarePriorityQueue.pqfactory</code>{.xref .py .py-meth
.docutils .literal .notranslate}]{.pre} methods.</p>
</li>
<li>
<p>The new [<code>key</code>{.docutils .literal .notranslate}]{.pre} parameter
displaced the [<code>startprios</code>{.docutils .literal
.notranslate}]{.pre} parameter 1 position to the right.</p>
</li>
</ul>
</li>
<li>
<p>The following class attributes have been added:</p>
<ul>
<li>
<p>[<code>crawler</code>{.xref .py .py-attr .docutils .literal
.notranslate}]{.pre}</p>
</li>
<li>
<p>[<code>downstream_queue_cls</code>{.xref .py .py-attr .docutils .literal
.notranslate}]{.pre} (details above)</p>
</li>
<li>
<p>[<code>key</code>{.xref .py .py-attr .docutils .literal
.notranslate}]{.pre} (details above)</p>
</li>
</ul>
</li>
<li>
<p>The [<code>serialize</code>{.docutils .literal .notranslate}]{.pre} attribute
has been removed (details above)</p>
</li>
</ul>
<p>The following changes affect specifically the
[<code>ScrapyPriorityQueue</code>{.xref .py .py-class .docutils .literal
.notranslate}]{.pre} class and may affect subclasses:</p>
<ul>
<li>A new [<code>priority()</code>{.xref .py .py-meth .docutils .literal
.notranslate}]{.pre} method has been added which, given a request,
returns [<code>request.priority</code>{.docutils .literal
.notranslate}]{.pre}<code> </code>{.docutils .literal</li>
</ul>

                    </main>

                    <nav class="nav-wrapper" aria-label="Page navigation">
                        <!-- Mobile navigation buttons -->
                            <a rel="prev" href="../Scrapy/Scrapy7.html" class="mobile-nav-chapters previous" title="Previous chapter" aria-label="Previous chapter" aria-keyshortcuts="Left">
                                <i class="fa fa-angle-left"></i>
                            </a>

                            <a rel="next" href="../Scrapy/Scrapy9.html" class="mobile-nav-chapters next" title="Next chapter" aria-label="Next chapter" aria-keyshortcuts="Right">
                                <i class="fa fa-angle-right"></i>
                            </a>

                        <div style="clear: both"></div>
                    </nav>
                </div>
            </div>

            <nav class="nav-wide-wrapper" aria-label="Page navigation">
                    <a rel="prev" href="../Scrapy/Scrapy7.html" class="nav-chapters previous" title="Previous chapter" aria-label="Previous chapter" aria-keyshortcuts="Left">
                        <i class="fa fa-angle-left"></i>
                    </a>

                    <a rel="next" href="../Scrapy/Scrapy9.html" class="nav-chapters next" title="Next chapter" aria-label="Next chapter" aria-keyshortcuts="Right">
                        <i class="fa fa-angle-right"></i>
                    </a>
            </nav>

        </div>




        <script>
            window.playground_copyable = true;
        </script>



        <script src="../clipboard.min.js"></script>
        <script src="../highlight.js"></script>
        <script src="../book.js"></script>

        <!-- Custom JS scripts -->


    </div>
    </body>
</html>
