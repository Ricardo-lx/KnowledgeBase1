<!DOCTYPE HTML>
<html lang="en" class="sidebar-visible no-js light">
    <head>
        <!-- Book generated using mdBook -->
        <meta charset="UTF-8">
        <title>Scrapy9 - KnowledgeBase</title>


        <!-- Custom HTML head -->
        
        <meta name="description" content="">
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <meta name="theme-color" content="#ffffff" />

        <link rel="icon" href="../favicon.svg">
        <link rel="shortcut icon" href="../favicon.png">
        <link rel="stylesheet" href="../css/variables.css">
        <link rel="stylesheet" href="../css/general.css">
        <link rel="stylesheet" href="../css/chrome.css">
        <link rel="stylesheet" href="../css/print.css" media="print">

        <!-- Fonts -->
        <link rel="stylesheet" href="../FontAwesome/css/font-awesome.css">
        <link rel="stylesheet" href="../fonts/fonts.css">

        <!-- Highlight.js Stylesheets -->
        <link rel="stylesheet" href="../highlight.css">
        <link rel="stylesheet" href="../tomorrow-night.css">
        <link rel="stylesheet" href="../ayu-highlight.css">

        <!-- Custom theme stylesheets -->

    </head>
    <body>
    <div id="body-container">
        <!-- Provide site root to javascript -->
        <script>
            var path_to_root = "../";
            var default_theme = window.matchMedia("(prefers-color-scheme: dark)").matches ? "navy" : "light";
        </script>

        <!-- Work around some values being stored in localStorage wrapped in quotes -->
        <script>
            try {
                var theme = localStorage.getItem('mdbook-theme');
                var sidebar = localStorage.getItem('mdbook-sidebar');

                if (theme.startsWith('"') && theme.endsWith('"')) {
                    localStorage.setItem('mdbook-theme', theme.slice(1, theme.length - 1));
                }

                if (sidebar.startsWith('"') && sidebar.endsWith('"')) {
                    localStorage.setItem('mdbook-sidebar', sidebar.slice(1, sidebar.length - 1));
                }
            } catch (e) { }
        </script>

        <!-- Set the theme before any content is loaded, prevents flash -->
        <script>
            var theme;
            try { theme = localStorage.getItem('mdbook-theme'); } catch(e) { }
            if (theme === null || theme === undefined) { theme = default_theme; }
            var html = document.querySelector('html');
            html.classList.remove('no-js')
            html.classList.remove('light')
            html.classList.add(theme);
            html.classList.add('js');
        </script>

        <!-- Hide / unhide sidebar before it is displayed -->
        <script>
            var html = document.querySelector('html');
            var sidebar = null;
            if (document.body.clientWidth >= 1080) {
                try { sidebar = localStorage.getItem('mdbook-sidebar'); } catch(e) { }
                sidebar = sidebar || 'visible';
            } else {
                sidebar = 'hidden';
            }
            html.classList.remove('sidebar-visible');
            html.classList.add("sidebar-" + sidebar);
        </script>

        <nav id="sidebar" class="sidebar" aria-label="Table of contents">
            <div class="sidebar-scrollbox">
                <ol class="chapter"><li class="chapter-item expanded "><a href="../Scrapy/Scrapy.html"><strong aria-hidden="true">1.</strong> Scrapy</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="../Scrapy/Scrapy1.html"><strong aria-hidden="true">1.1.</strong> Scrapy1</a></li><li class="chapter-item expanded "><a href="../Scrapy/Scrapy2.html"><strong aria-hidden="true">1.2.</strong> Scrapy2</a></li><li class="chapter-item expanded "><a href="../Scrapy/Scrapy3.html"><strong aria-hidden="true">1.3.</strong> Scrapy3</a></li><li class="chapter-item expanded "><a href="../Scrapy/Scrapy4.html"><strong aria-hidden="true">1.4.</strong> Scrapy4</a></li><li class="chapter-item expanded "><a href="../Scrapy/Scrapy5.html"><strong aria-hidden="true">1.5.</strong> Scrapy5</a></li><li class="chapter-item expanded "><a href="../Scrapy/Scrapy6.html"><strong aria-hidden="true">1.6.</strong> Scrapy6</a></li><li class="chapter-item expanded "><a href="../Scrapy/Scrapy7.html"><strong aria-hidden="true">1.7.</strong> Scrapy7</a></li><li class="chapter-item expanded "><a href="../Scrapy/Scrapy8.html"><strong aria-hidden="true">1.8.</strong> Scrapy8</a></li><li class="chapter-item expanded "><a href="../Scrapy/Scrapy9.html" class="active"><strong aria-hidden="true">1.9.</strong> Scrapy9</a></li><li class="chapter-item expanded "><a href="../Scrapy/Scrapy10.html"><strong aria-hidden="true">1.10.</strong> Scrapy10</a></li></ol></li><li class="chapter-item expanded "><a href="../ThinkPython/ThinkPython.html"><strong aria-hidden="true">2.</strong> ThinkPython</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="../ThinkPython/part_1.html"><strong aria-hidden="true">2.1.</strong> ThinkPython1</a></li><li class="chapter-item expanded "><a href="../ThinkPython/part_2.html"><strong aria-hidden="true">2.2.</strong> ThinkPython2</a></li><li class="chapter-item expanded "><a href="../ThinkPython/part_3.html"><strong aria-hidden="true">2.3.</strong> ThinkPython3</a></li><li class="chapter-item expanded "><a href="../ThinkPython/part_4.html"><strong aria-hidden="true">2.4.</strong> ThinkPython4</a></li><li class="chapter-item expanded "><a href="../ThinkPython/part_5.html"><strong aria-hidden="true">2.5.</strong> ThinkPython5</a></li><li class="chapter-item expanded "><a href="../ThinkPython/part_6.html"><strong aria-hidden="true">2.6.</strong> ThinkPython6</a></li><li class="chapter-item expanded "><a href="../ThinkPython/part_7.html"><strong aria-hidden="true">2.7.</strong> ThinkPython7</a></li><li class="chapter-item expanded "><a href="../ThinkPython/part_8.html"><strong aria-hidden="true">2.8.</strong> ThinkPython8</a></li><li class="chapter-item expanded "><a href="../ThinkPython/part_9.html"><strong aria-hidden="true">2.9.</strong> ThinkPython9</a></li><li class="chapter-item expanded "><a href="../ThinkPython/part_10.html"><strong aria-hidden="true">2.10.</strong> ThinkPython10</a></li></ol></li><li class="chapter-item expanded "><a href="../C-sharp-docs/C-sharp-docs.html"><strong aria-hidden="true">3.</strong> C-sharp-docs</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="../C-sharp-docs/part_1.html"><strong aria-hidden="true">3.1.</strong> Csharp1</a></li><li class="chapter-item expanded "><a href="../C-sharp-docs/part_2.html"><strong aria-hidden="true">3.2.</strong> Csharp2</a></li><li class="chapter-item expanded "><a href="../C-sharp-docs/part_3.html"><strong aria-hidden="true">3.3.</strong> Csharp3</a></li><li class="chapter-item expanded "><a href="../C-sharp-docs/part_4.html"><strong aria-hidden="true">3.4.</strong> Csharp4</a></li><li class="chapter-item expanded "><a href="../C-sharp-docs/part_5.html"><strong aria-hidden="true">3.5.</strong> Csharp5</a></li><li class="chapter-item expanded "><a href="../C-sharp-docs/part_6.html"><strong aria-hidden="true">3.6.</strong> Csharp6</a></li><li class="chapter-item expanded "><a href="../C-sharp-docs/part_7.html"><strong aria-hidden="true">3.7.</strong> Csharp7</a></li><li class="chapter-item expanded "><a href="../C-sharp-docs/part_8.html"><strong aria-hidden="true">3.8.</strong> Csharp8</a></li><li class="chapter-item expanded "><a href="../C-sharp-docs/part_9.html"><strong aria-hidden="true">3.9.</strong> Csharp9</a></li><li class="chapter-item expanded "><a href="../C-sharp-docs/part_10.html"><strong aria-hidden="true">3.10.</strong> Csharp10</a></li></ol></li></ol>
            </div>
            <div id="sidebar-resize-handle" class="sidebar-resize-handle"></div>
        </nav>

        <div id="page-wrapper" class="page-wrapper">

            <div class="page">
                                <div id="menu-bar-hover-placeholder"></div>
                <div id="menu-bar" class="menu-bar sticky bordered">
                    <div class="left-buttons">
                        <button id="sidebar-toggle" class="icon-button" type="button" title="Toggle Table of Contents" aria-label="Toggle Table of Contents" aria-controls="sidebar">
                            <i class="fa fa-bars"></i>
                        </button>
                        <button id="theme-toggle" class="icon-button" type="button" title="Change theme" aria-label="Change theme" aria-haspopup="true" aria-expanded="false" aria-controls="theme-list">
                            <i class="fa fa-paint-brush"></i>
                        </button>
                        <ul id="theme-list" class="theme-popup" aria-label="Themes" role="menu">
                            <li role="none"><button role="menuitem" class="theme" id="light">Light</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="rust">Rust</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="coal">Coal</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="navy">Navy</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="ayu">Ayu</button></li>
                        </ul>
                    </div>

                    <h1 class="menu-title">KnowledgeBase</h1>

                    <div class="right-buttons">
                        <a href="../print.html" title="Print this book" aria-label="Print this book">
                            <i id="print-button" class="fa fa-print"></i>
                        </a>

                    </div>
                </div>


                <!-- Apply ARIA attributes after the sidebar and the sidebar toggle button are added to the DOM -->
                <script>
                    document.getElementById('sidebar-toggle').setAttribute('aria-expanded', sidebar === 'visible');
                    document.getElementById('sidebar').setAttribute('aria-hidden', sidebar !== 'visible');
                    Array.from(document.querySelectorAll('#sidebar a')).forEach(function(link) {
                        link.setAttribute('tabIndex', sidebar === 'visible' ? 0 : -1);
                    });
                </script>

                <div id="content" class="content">
                    <main>
                        <pre><code>.notranslate}[`*`{.docutils .literal
.notranslate}]{.pre}` `{.docutils .literal
.notranslate}[`-1`{.docutils .literal .notranslate}]{.pre}.

It is used in [`push()`{.xref .py .py-meth .docutils .literal
.notranslate}]{.pre} to make up for the removal of its
[`priority`{.docutils .literal .notranslate}]{.pre} parameter.
</code></pre>
<ul>
<li>The [<code>spider</code>{.docutils .literal .notranslate}]{.pre} attribute has
been removed. Use [<code>crawler.spider</code>{.xref .py .py-attr .docutils
.literal .notranslate}]{.pre} instead.</li>
</ul>
<p>The following changes affect specifically the
[<code>DownloaderAwarePriorityQueue</code>{.xref .py .py-class .docutils .literal
.notranslate}]{.pre} class and may affect subclasses:</p>
<ul>
<li>A new [<code>pqueues</code>{.xref .py .py-attr .docutils .literal
.notranslate}]{.pre} attribute offers a mapping of downloader slot
names to the corresponding instances of
[<code>downstream_queue_cls</code>{.xref .py .py-attr .docutils .literal
.notranslate}]{.pre}.</li>
</ul>
<p>(<a href="https://github.com/scrapy/scrapy/issues/3884">issue 3884</a>{.reference
.external})
:::
:::</p>
<p>::: {#scrapy-1-8-3-2022-07-25 .section}
[]{#release-1-8-3}</p>
<h4 id="scrapy-183-2022-07-25headerlink"><a class="header" href="#scrapy-183-2022-07-25headerlink">Scrapy 1.8.3 (2022-07-25)<a href="#scrapy-1-8-3-2022-07-25" title="Permalink to this heading">¶</a>{.headerlink}</a></h4>
<p><strong>Security bug fix:</strong></p>
<ul>
<li>
<p>When <a href="index.html#scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware" title="scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware">[<code>HttpProxyMiddleware</code>{.xref .py .py-class .docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal} processes a request with <a href="index.html#std-reqmeta-proxy">[<code>proxy</code>{.xref .std
.std-reqmeta .docutils .literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal} metadata, and that <a href="index.html#std-reqmeta-proxy">[<code>proxy</code>{.xref
.std .std-reqmeta .docutils .literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal} metadata includes proxy credentials,
<a href="index.html#scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware" title="scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware">[<code>HttpProxyMiddleware</code>{.xref .py .py-class .docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal} sets the [<code>Proxy-Authorization</code>{.docutils .literal
.notranslate}]{.pre} header, but only if that header is not already
set.</p>
<p>There are third-party proxy-rotation downloader middlewares that set
different <a href="index.html#std-reqmeta-proxy">[<code>proxy</code>{.xref .std .std-reqmeta .docutils .literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal} metadata every time they process a
request.</p>
<p>Because of request retries and redirects, the same request can be
processed by downloader middlewares more than once, including both
<a href="index.html#scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware" title="scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware">[<code>HttpProxyMiddleware</code>{.xref .py .py-class .docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal} and any third-party proxy-rotation downloader middleware.</p>
<p>These third-party proxy-rotation downloader middlewares could change
the <a href="index.html#std-reqmeta-proxy">[<code>proxy</code>{.xref .std .std-reqmeta .docutils .literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal} metadata of a request to a new value,
but fail to remove the [<code>Proxy-Authorization</code>{.docutils .literal
.notranslate}]{.pre} header from the previous value of the
<a href="index.html#std-reqmeta-proxy">[<code>proxy</code>{.xref .std .std-reqmeta .docutils .literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal} metadata, causing the credentials of
one proxy to be sent to a different proxy.</p>
<p>To prevent the unintended leaking of proxy credentials, the behavior
of <a href="index.html#scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware" title="scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware">[<code>HttpProxyMiddleware</code>{.xref .py .py-class .docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal} is now as follows when processing a request:</p>
<ul>
<li>
<p>If the request being processed defines <a href="index.html#std-reqmeta-proxy">[<code>proxy</code>{.xref .std
.std-reqmeta .docutils .literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal} metadata that includes
credentials, the [<code>Proxy-Authorization</code>{.docutils .literal
.notranslate}]{.pre} header is always updated to feature those
credentials.</p>
</li>
<li>
<p>If the request being processed defines <a href="index.html#std-reqmeta-proxy">[<code>proxy</code>{.xref .std
.std-reqmeta .docutils .literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal} metadata without credentials, the
[<code>Proxy-Authorization</code>{.docutils .literal .notranslate}]{.pre}
header is removed <em>unless</em> it was originally defined for the
same proxy URL.</p>
<p>To remove proxy credentials while keeping the same proxy URL,
remove the [<code>Proxy-Authorization</code>{.docutils .literal
.notranslate}]{.pre} header.</p>
</li>
<li>
<p>If the request has no <a href="index.html#std-reqmeta-proxy">[<code>proxy</code>{.xref .std .std-reqmeta
.docutils .literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal} metadata, or that metadata is a
falsy value (e.g. [<code>None</code>{.docutils .literal
.notranslate}]{.pre}), the [<code>Proxy-Authorization</code>{.docutils
.literal .notranslate}]{.pre} header is removed.</p>
<p>It is no longer possible to set a proxy URL through the
<a href="index.html#std-reqmeta-proxy">[<code>proxy</code>{.xref .std .std-reqmeta .docutils .literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal} metadata but set the credentials
through the [<code>Proxy-Authorization</code>{.docutils .literal
.notranslate}]{.pre} header. Set proxy credentials through the
<a href="index.html#std-reqmeta-proxy">[<code>proxy</code>{.xref .std .std-reqmeta .docutils .literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal} metadata instead.
:::</p>
</li>
</ul>
</li>
</ul>
<p>::: {#scrapy-1-8-2-2022-03-01 .section}
[]{#release-1-8-2}</p>
<h4 id="scrapy-182-2022-03-01headerlink"><a class="header" href="#scrapy-182-2022-03-01headerlink">Scrapy 1.8.2 (2022-03-01)<a href="#scrapy-1-8-2-2022-03-01" title="Permalink to this heading">¶</a>{.headerlink}</a></h4>
<p><strong>Security bug fixes:</strong></p>
<ul>
<li>
<p>When a <a href="index.html#scrapy.http.Request" title="scrapy.http.Request">[<code>Request</code>{.xref .py .py-class .docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal} object with cookies defined gets a redirect response
causing a new <a href="index.html#scrapy.http.Request" title="scrapy.http.Request">[<code>Request</code>{.xref .py .py-class .docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal} object to be scheduled, the cookies defined in the
original <a href="index.html#scrapy.http.Request" title="scrapy.http.Request">[<code>Request</code>{.xref .py .py-class .docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal} object are no longer copied into the new
<a href="index.html#scrapy.http.Request" title="scrapy.http.Request">[<code>Request</code>{.xref .py .py-class .docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal} object.</p>
<p>If you manually set the [<code>Cookie</code>{.docutils .literal
.notranslate}]{.pre} header on a <a href="index.html#scrapy.http.Request" title="scrapy.http.Request">[<code>Request</code>{.xref .py .py-class
.docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal} object and the domain name of the redirect URL is not an
exact match for the domain of the URL of the original
<a href="index.html#scrapy.http.Request" title="scrapy.http.Request">[<code>Request</code>{.xref .py .py-class .docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal} object, your [<code>Cookie</code>{.docutils .literal
.notranslate}]{.pre} header is now dropped from the new
<a href="index.html#scrapy.http.Request" title="scrapy.http.Request">[<code>Request</code>{.xref .py .py-class .docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal} object.</p>
<p>The old behavior could be exploited by an attacker to gain access to
your cookies. Please, see the <a href="https://github.com/scrapy/scrapy/security/advisories/GHSA-cjvr-mfj7-j4j8">cjvr-mfj7-j4j8 security
advisory</a>{.reference
.external} for more information.</p>
<p>::: {.admonition .note}
Note</p>
<p>It is still possible to enable the sharing of cookies between
different domains with a shared domain suffix (e.g.
[<code>example.com</code>{.docutils .literal .notranslate}]{.pre} and any
subdomain) by defining the shared domain suffix (e.g.
[<code>example.com</code>{.docutils .literal .notranslate}]{.pre}) as the
cookie domain when defining your cookies. See the documentation of
the <a href="index.html#scrapy.http.Request" title="scrapy.http.Request">[<code>Request</code>{.xref .py .py-class .docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal} class for more information.
:::</p>
</li>
<li>
<p>When the domain of a cookie, either received in the
[<code>Set-Cookie</code>{.docutils .literal .notranslate}]{.pre} header of a
response or defined in a <a href="index.html#scrapy.http.Request" title="scrapy.http.Request">[<code>Request</code>{.xref .py .py-class .docutils
.literal
.notranslate}]{.pre}</a>{.reference
.internal} object, is set to a <a href="https://publicsuffix.org/">public
suffix</a>{.reference .external}, the cookie
is now ignored unless the cookie domain is the same as the request
domain.</p>
<p>The old behavior could be exploited by an attacker to inject cookies
into your requests to some other domains. Please, see the
<a href="https://github.com/scrapy/scrapy/security/advisories/GHSA-mfjm-vh54-3f96">mfjm-vh54-3f96 security
advisory</a>{.reference
.external} for more information.
:::</p>
</li>
</ul>
<p>::: {#scrapy-1-8-1-2021-10-05 .section}
[]{#release-1-8-1}</p>
<h4 id="scrapy-181-2021-10-05headerlink"><a class="header" href="#scrapy-181-2021-10-05headerlink">Scrapy 1.8.1 (2021-10-05)<a href="#scrapy-1-8-1-2021-10-05" title="Permalink to this heading">¶</a>{.headerlink}</a></h4>
<ul>
<li>
<p><strong>Security bug fix:</strong></p>
<p>If you use <a href="index.html#scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware" title="scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware">[<code>HttpAuthMiddleware</code>{.xref .py .py-class .docutils
.literal
.notranslate}]{.pre}</a>{.reference
.internal} (i.e. the [<code>http_user</code>{.docutils .literal
.notranslate}]{.pre} and [<code>http_pass</code>{.docutils .literal
.notranslate}]{.pre} spider attributes) for HTTP authentication, any
request exposes your credentials to the request target.</p>
<p>To prevent unintended exposure of authentication credentials to
unintended domains, you must now additionally set a new, additional
spider attribute, [<code>http_auth_domain</code>{.docutils .literal
.notranslate}]{.pre}, and point it to the specific domain to which
the authentication credentials must be sent.</p>
<p>If the [<code>http_auth_domain</code>{.docutils .literal .notranslate}]{.pre}
spider attribute is not set, the domain of the first request will be
considered the HTTP authentication target, and authentication
credentials will only be sent in requests targeting that domain.</p>
<p>If you need to send the same HTTP authentication credentials to
multiple domains, you can use
<a href="https://w3lib.readthedocs.io/en/latest/w3lib.html#w3lib.http.basic_auth_header" title="(in w3lib v2.1)">[<code>w3lib.http.basic_auth_header()</code>{.xref .py .py-func .docutils
.literal
.notranslate}]{.pre}</a>{.reference
.external} instead to set the value of the
[<code>Authorization</code>{.docutils .literal .notranslate}]{.pre} header of
your requests.</p>
<p>If you <em>really</em> want your spider to send the same HTTP
authentication credentials to any domain, set the
[<code>http_auth_domain</code>{.docutils .literal .notranslate}]{.pre} spider
attribute to [<code>None</code>{.docutils .literal .notranslate}]{.pre}.</p>
<p>Finally, if you are a user of
<a href="https://github.com/scrapy-plugins/scrapy-splash">scrapy-splash</a>{.reference
.external}, know that this version of Scrapy breaks compatibility
with scrapy-splash 0.7.2 and earlier. You will need to upgrade
scrapy-splash to a greater version for it to continue to work.
:::</p>
</li>
</ul>
<p>::: {#scrapy-1-8-0-2019-10-28 .section}
[]{#release-1-8-0}</p>
<h4 id="scrapy-180-2019-10-28headerlink"><a class="header" href="#scrapy-180-2019-10-28headerlink">Scrapy 1.8.0 (2019-10-28)<a href="#scrapy-1-8-0-2019-10-28" title="Permalink to this heading">¶</a>{.headerlink}</a></h4>
<p>Highlights:</p>
<ul>
<li>
<p>Dropped Python 3.4 support and updated minimum requirements; made
Python 3.8 support official</p>
</li>
<li>
<p>New <a href="index.html#scrapy.http.Request.from_curl" title="scrapy.http.Request.from_curl">[<code>Request.from_curl</code>{.xref .py .py-meth .docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal} class method</p>
</li>
<li>
<p>New <a href="index.html#std-setting-ROBOTSTXT_PARSER">[<code>ROBOTSTXT_PARSER</code>{.xref .std .std-setting .docutils .literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal} and <a href="index.html#std-setting-ROBOTSTXT_USER_AGENT">[<code>ROBOTSTXT_USER_AGENT</code>{.xref
.std .std-setting .docutils .literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal} settings</p>
</li>
<li>
<p>New <a href="index.html#std-setting-DOWNLOADER_CLIENT_TLS_CIPHERS">[<code>DOWNLOADER_CLIENT_TLS_CIPHERS</code>{.xref .std .std-setting
.docutils .literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal} and
<a href="index.html#std-setting-DOWNLOADER_CLIENT_TLS_VERBOSE_LOGGING">[<code>DOWNLOADER_CLIENT_TLS_VERBOSE_LOGGING</code>{.xref .std .std-setting
.docutils .literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal} settings</p>
</li>
</ul>
<p>::: {#id82 .section}</p>
<h5 id="backward-incompatible-changesheaderlink"><a class="header" href="#backward-incompatible-changesheaderlink">Backward-incompatible changes<a href="#id82" title="Permalink to this heading">¶</a>{.headerlink}</a></h5>
<ul>
<li>
<p>Python 3.4 is no longer supported, and some of the minimum
requirements of Scrapy have also changed:</p>
<ul>
<li>
<p><a href="https://cssselect.readthedocs.io/en/latest/index.html" title="(in cssselect v1.2.0)">[cssselect]{.xref .std
.std-doc}</a>{.reference
.external} 0.9.1</p>
</li>
<li>
<p><a href="https://cryptography.io/en/latest/">cryptography</a>{.reference
.external} 2.0</p>
</li>
<li>
<p><a href="https://lxml.de/">lxml</a>{.reference .external} 3.5.0</p>
</li>
<li>
<p><a href="https://www.pyopenssl.org/en/stable/">pyOpenSSL</a>{.reference
.external} 16.2.0</p>
</li>
<li>
<p><a href="https://github.com/scrapy/queuelib">queuelib</a>{.reference
.external} 1.4.2</p>
</li>
<li>
<p><a href="https://service-identity.readthedocs.io/en/stable/">service_identity</a>{.reference
.external} 16.0.0</p>
</li>
<li>
<p><a href="https://six.readthedocs.io/">six</a>{.reference .external} 1.10.0</p>
</li>
<li>
<p><a href="https://twistedmatrix.com/trac/">Twisted</a>{.reference .external}
17.9.0 (16.0.0 with Python 2)</p>
</li>
<li>
<p><a href="https://zopeinterface.readthedocs.io/en/latest/">zope.interface</a>{.reference
.external} 4.1.3</p>
</li>
</ul>
<p>(<a href="https://github.com/scrapy/scrapy/issues/3892">issue
3892</a>{.reference
.external})</p>
</li>
<li>
<p>[<code>JSONRequest</code>{.docutils .literal .notranslate}]{.pre} is now called
<a href="index.html#scrapy.http.JsonRequest" title="scrapy.http.JsonRequest">[<code>JsonRequest</code>{.xref .py .py-class .docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal} for consistency with similar classes (<a href="https://github.com/scrapy/scrapy/issues/3929">issue
3929</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/3982">issue
3982</a>{.reference
.external})</p>
</li>
<li>
<p>If you are using a custom context factory
(<a href="index.html#std-setting-DOWNLOADER_CLIENTCONTEXTFACTORY">[<code>DOWNLOADER_CLIENTCONTEXTFACTORY</code>{.xref .std .std-setting
.docutils .literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal}), its [<code>__init__</code>{.docutils .literal
.notranslate}]{.pre} method must accept two new parameters:
[<code>tls_verbose_logging</code>{.docutils .literal .notranslate}]{.pre} and
[<code>tls_ciphers</code>{.docutils .literal .notranslate}]{.pre} (<a href="https://github.com/scrapy/scrapy/issues/2111">issue
2111</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/3392">issue
3392</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/3442">issue
3442</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/3450">issue
3450</a>{.reference
.external})</p>
</li>
<li>
<p><a href="index.html#scrapy.loader.ItemLoader" title="scrapy.loader.ItemLoader">[<code>ItemLoader</code>{.xref .py .py-class .docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal} now turns the values of its input item into lists:</p>
<p>::: {.highlight-pycon .notranslate}
::: highlight
&gt;&gt;&gt; item = MyItem()
&gt;&gt;&gt; item[&quot;field&quot;] = &quot;value1&quot;
&gt;&gt;&gt; loader = ItemLoader(item=item)
&gt;&gt;&gt; item[&quot;field&quot;]
['value1']
:::
:::</p>
<p>This is needed to allow adding values to existing fields
([<code>loader.add_value('field',</code>{.docutils .literal
.notranslate}]{.pre}<code> </code>{.docutils .literal
.notranslate}[<code>'value2')</code>{.docutils .literal .notranslate}]{.pre}).</p>
<p>(<a href="https://github.com/scrapy/scrapy/issues/3804">issue
3804</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/3819">issue
3819</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/3897">issue
3897</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/3976">issue
3976</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/3998">issue
3998</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/4036">issue
4036</a>{.reference
.external})</p>
</li>
</ul>
<p>See also <a href="#id86">[Deprecation removals]{.std .std-ref}</a>{.hoverxref
.tooltip .reference .internal} below.
:::</p>
<p>::: {#id83 .section}</p>
<h5 id="new-featuresheaderlink"><a class="header" href="#new-featuresheaderlink">New features<a href="#id83" title="Permalink to this heading">¶</a>{.headerlink}</a></h5>
<ul>
<li>
<p>A new <a href="index.html#scrapy.http.Request.from_curl" title="scrapy.http.Request.from_curl">[<code>Request.from_curl</code>{.xref .py .py-meth .docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal} class method allows <a href="index.html#requests-from-curl">[creating a request from a cURL
command]{.std .std-ref}</a>{.hoverxref
.tooltip .reference .internal} (<a href="https://github.com/scrapy/scrapy/issues/2985">issue
2985</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/3862">issue
3862</a>{.reference
.external})</p>
</li>
<li>
<p>A new <a href="index.html#std-setting-ROBOTSTXT_PARSER">[<code>ROBOTSTXT_PARSER</code>{.xref .std .std-setting .docutils
.literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal} setting allows choosing which
<a href="https://www.robotstxt.org/">robots.txt</a>{.reference .external}
parser to use. It includes built-in support for
<a href="index.html#python-robotfileparser">[RobotFileParser]{.std
.std-ref}</a>{.hoverxref .tooltip
.reference .internal}, <a href="index.html#protego-parser">[Protego]{.std
.std-ref}</a>{.hoverxref .tooltip .reference
.internal} (default), <a href="index.html#reppy-parser">[Reppy]{.std
.std-ref}</a>{.hoverxref .tooltip .reference
.internal}, and <a href="index.html#rerp-parser">[Robotexclusionrulesparser]{.std
.std-ref}</a>{.hoverxref .tooltip .reference
.internal}, and allows you to <a href="index.html#support-for-new-robots-parser">[implement support for additional
parsers]{.std
.std-ref}</a>{.hoverxref
.tooltip .reference .internal} (<a href="https://github.com/scrapy/scrapy/issues/754">issue
754</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/2669">issue
2669</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/3796">issue
3796</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/3935">issue
3935</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/3969">issue
3969</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/4006">issue
4006</a>{.reference
.external})</p>
</li>
<li>
<p>A new <a href="index.html#std-setting-ROBOTSTXT_USER_AGENT">[<code>ROBOTSTXT_USER_AGENT</code>{.xref .std .std-setting .docutils
.literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal} setting allows defining a separate
user agent string to use for
<a href="https://www.robotstxt.org/">robots.txt</a>{.reference .external}
parsing (<a href="https://github.com/scrapy/scrapy/issues/3931">issue
3931</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/3966">issue
3966</a>{.reference
.external})</p>
</li>
<li>
<p><a href="index.html#scrapy.spiders.Rule" title="scrapy.spiders.Rule">[<code>Rule</code>{.xref .py .py-class .docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal} no longer requires a <a href="index.html#scrapy.linkextractors.lxmlhtml.LxmlLinkExtractor" title="scrapy.linkextractors.lxmlhtml.LxmlLinkExtractor">[<code>LinkExtractor</code>{.xref .py
.py-class .docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal} parameter (<a href="https://github.com/scrapy/scrapy/issues/781">issue
781</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/4016">issue
4016</a>{.reference
.external})</p>
</li>
<li>
<p>Use the new <a href="index.html#std-setting-DOWNLOADER_CLIENT_TLS_CIPHERS">[<code>DOWNLOADER_CLIENT_TLS_CIPHERS</code>{.xref .std
.std-setting .docutils .literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal} setting to customize the TLS/SSL
ciphers used by the default HTTP/1.1 downloader (<a href="https://github.com/scrapy/scrapy/issues/3392">issue
3392</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/3442">issue
3442</a>{.reference
.external})</p>
</li>
<li>
<p>Set the new <a href="index.html#std-setting-DOWNLOADER_CLIENT_TLS_VERBOSE_LOGGING">[<code>DOWNLOADER_CLIENT_TLS_VERBOSE_LOGGING</code>{.xref .std
.std-setting .docutils .literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal} setting to [<code>True</code>{.docutils .literal
.notranslate}]{.pre} to enable debug-level messages about TLS
connection parameters after establishing HTTPS connections (<a href="https://github.com/scrapy/scrapy/issues/2111">issue
2111</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/3450">issue
3450</a>{.reference
.external})</p>
</li>
<li>
<p>Callbacks that receive keyword arguments (see
<a href="index.html#scrapy.http.Request.cb_kwargs" title="scrapy.http.Request.cb_kwargs">[<code>Request.cb_kwargs</code>{.xref .py .py-attr .docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal}) can now be tested using the new <a href="index.html#scrapy.contracts.default.CallbackKeywordArgumentsContract" title="scrapy.contracts.default.CallbackKeywordArgumentsContract">[<code>@cb_kwargs</code>{.xref .py
.py-class .docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal} <a href="index.html#topics-contracts">[spider contract]{.std
.std-ref}</a>{.hoverxref .tooltip
.reference .internal} (<a href="https://github.com/scrapy/scrapy/issues/3985">issue
3985</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/3988">issue
3988</a>{.reference
.external})</p>
</li>
<li>
<p>When a <a href="index.html#scrapy.contracts.default.ScrapesContract" title="scrapy.contracts.default.ScrapesContract">[<code>@scrapes</code>{.xref .py .py-class .docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal} spider contract fails, all missing fields are now
reported (<a href="https://github.com/scrapy/scrapy/issues/766">issue
766</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/3939">issue
3939</a>{.reference
.external})</p>
</li>
<li>
<p><a href="index.html#custom-log-formats">[Custom log formats]{.std
.std-ref}</a>{.hoverxref .tooltip
.reference .internal} can now drop messages by having the
corresponding methods of the configured <a href="index.html#std-setting-LOG_FORMATTER">[<code>LOG_FORMATTER</code>{.xref .std
.std-setting .docutils .literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal} return [<code>None</code>{.docutils .literal
.notranslate}]{.pre} (<a href="https://github.com/scrapy/scrapy/issues/3984">issue
3984</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/3987">issue
3987</a>{.reference
.external})</p>
</li>
<li>
<p>A much improved completion definition is now available for
<a href="https://www.zsh.org/">Zsh</a>{.reference .external} (<a href="https://github.com/scrapy/scrapy/issues/4069">issue
4069</a>{.reference
.external})
:::</p>
</li>
</ul>
<p>::: {#id84 .section}</p>
<h5 id="bug-fixesheaderlink"><a class="header" href="#bug-fixesheaderlink">Bug fixes<a href="#id84" title="Permalink to this heading">¶</a>{.headerlink}</a></h5>
<ul>
<li>
<p><a href="index.html#scrapy.loader.ItemLoader.load_item" title="scrapy.loader.ItemLoader.load_item">[<code>ItemLoader.load_item()</code>{.xref .py .py-meth .docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal} no longer makes later calls to
<a href="index.html#scrapy.loader.ItemLoader.get_output_value" title="scrapy.loader.ItemLoader.get_output_value">[<code>ItemLoader.get_output_value()</code>{.xref .py .py-meth .docutils
.literal
.notranslate}]{.pre}</a>{.reference
.internal} or <a href="index.html#scrapy.loader.ItemLoader.load_item" title="scrapy.loader.ItemLoader.load_item">[<code>ItemLoader.load_item()</code>{.xref .py .py-meth
.docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal} return empty data (<a href="https://github.com/scrapy/scrapy/issues/3804">issue
3804</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/3819">issue
3819</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/3897">issue
3897</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/3976">issue
3976</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/3998">issue
3998</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/4036">issue
4036</a>{.reference
.external})</p>
</li>
<li>
<p>Fixed <a href="index.html#scrapy.statscollectors.DummyStatsCollector" title="scrapy.statscollectors.DummyStatsCollector">[<code>DummyStatsCollector</code>{.xref .py .py-class .docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal} raising a <a href="https://docs.python.org/3/library/exceptions.html#TypeError" title="(in Python v3.12)">[<code>TypeError</code>{.xref .py .py-exc .docutils
.literal
.notranslate}]{.pre}</a>{.reference
.external} exception (<a href="https://github.com/scrapy/scrapy/issues/4007">issue
4007</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/4052">issue
4052</a>{.reference
.external})</p>
</li>
<li>
<p><a href="index.html#scrapy.pipelines.files.FilesPipeline.file_path" title="scrapy.pipelines.files.FilesPipeline.file_path">[<code>FilesPipeline.file_path</code>{.xref .py .py-meth .docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal} and <a href="index.html#scrapy.pipelines.images.ImagesPipeline.file_path" title="scrapy.pipelines.images.ImagesPipeline.file_path">[<code>ImagesPipeline.file_path</code>{.xref .py .py-meth
.docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal} no longer choose file extensions that are not <a href="https://www.iana.org/assignments/media-types/media-types.xhtml">registered
with
IANA</a>{.reference
.external} (<a href="https://github.com/scrapy/scrapy/issues/1287">issue
1287</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/3953">issue
3953</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/3954">issue
3954</a>{.reference
.external})</p>
</li>
<li>
<p>When using <a href="https://github.com/boto/botocore">botocore</a>{.reference
.external} to persist files in S3, all botocore-supported headers
are properly mapped now (<a href="https://github.com/scrapy/scrapy/issues/3904">issue
3904</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/3905">issue
3905</a>{.reference
.external})</p>
</li>
<li>
<p>FTP passwords in [<code>FEED_URI</code>{.xref .std .std-setting .docutils
.literal .notranslate}]{.pre} containing percent-escaped characters
are now properly decoded (<a href="https://github.com/scrapy/scrapy/issues/3941">issue
3941</a>{.reference
.external})</p>
</li>
<li>
<p>A memory-handling and error-handling issue in
[<code>scrapy.utils.ssl.get_temp_key_info()</code>{.xref .py .py-func .docutils
.literal .notranslate}]{.pre} has been fixed (<a href="https://github.com/scrapy/scrapy/issues/3920">issue
3920</a>{.reference
.external})
:::</p>
</li>
</ul>
<p>::: {#id85 .section}</p>
<h5 id="documentationheaderlink"><a class="header" href="#documentationheaderlink">Documentation<a href="#id85" title="Permalink to this heading">¶</a>{.headerlink}</a></h5>
<ul>
<li>
<p>The documentation now covers how to define and configure a <a href="index.html#custom-log-formats">[custom
log format]{.std
.std-ref}</a>{.hoverxref .tooltip
.reference .internal} (<a href="https://github.com/scrapy/scrapy/issues/3616">issue
3616</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/3660">issue
3660</a>{.reference
.external})</p>
</li>
<li>
<p>API documentation added for <a href="index.html#scrapy.exporters.MarshalItemExporter" title="scrapy.exporters.MarshalItemExporter">[<code>MarshalItemExporter</code>{.xref .py
.py-class .docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal} and <a href="index.html#scrapy.exporters.PythonItemExporter" title="scrapy.exporters.PythonItemExporter">[<code>PythonItemExporter</code>{.xref .py .py-class .docutils
.literal
.notranslate}]{.pre}</a>{.reference
.internal} (<a href="https://github.com/scrapy/scrapy/issues/3973">issue
3973</a>{.reference
.external})</p>
</li>
<li>
<p>API documentation added for [<code>BaseItem</code>{.xref .py .py-class
.docutils .literal .notranslate}]{.pre} and <a href="index.html#scrapy.item.ItemMeta" title="scrapy.item.ItemMeta">[<code>ItemMeta</code>{.xref .py
.py-class .docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal} (<a href="https://github.com/scrapy/scrapy/issues/3999">issue
3999</a>{.reference
.external})</p>
</li>
<li>
<p>Minor documentation fixes (<a href="https://github.com/scrapy/scrapy/issues/2998">issue
2998</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/3398">issue
3398</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/3597">issue
3597</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/3894">issue
3894</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/3934">issue
3934</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/3978">issue
3978</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/3993">issue
3993</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/4022">issue
4022</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/4028">issue
4028</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/4033">issue
4033</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/4046">issue
4046</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/4050">issue
4050</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/4055">issue
4055</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/4056">issue
4056</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/4061">issue
4061</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/4072">issue
4072</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/4071">issue
4071</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/4079">issue
4079</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/4081">issue
4081</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/4089">issue
4089</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/4093">issue
4093</a>{.reference
.external})
:::</p>
</li>
</ul>
<p>::: {#id86 .section}
[]{#id87}</p>
<h5 id="deprecation-removalsheaderlink"><a class="header" href="#deprecation-removalsheaderlink">Deprecation removals<a href="#id86" title="Permalink to this heading">¶</a>{.headerlink}</a></h5>
<ul>
<li>[<code>scrapy.xlib</code>{.docutils .literal .notranslate}]{.pre} has been
removed (<a href="https://github.com/scrapy/scrapy/issues/4015">issue
4015</a>{.reference
.external})
:::</li>
</ul>
<p>::: {#id88 .section}
[]{#id89}</p>
<h5 id="deprecationsheaderlink"><a class="header" href="#deprecationsheaderlink">Deprecations<a href="#id88" title="Permalink to this heading">¶</a>{.headerlink}</a></h5>
<ul>
<li>
<p>The <a href="https://github.com/google/leveldb">LevelDB</a>{.reference
.external} storage backend
([<code>scrapy.extensions.httpcache.LeveldbCacheStorage</code>{.docutils
.literal .notranslate}]{.pre}) of <a href="index.html#scrapy.downloadermiddlewares.httpcache.HttpCacheMiddleware" title="scrapy.downloadermiddlewares.httpcache.HttpCacheMiddleware">[<code>HttpCacheMiddleware</code>{.xref .py
.py-class .docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal} is deprecated (<a href="https://github.com/scrapy/scrapy/issues/4085">issue
4085</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/4092">issue
4092</a>{.reference
.external})</p>
</li>
<li>
<p>Use of the undocumented
[<code>SCRAPY_PICKLED_SETTINGS_TO_OVERRIDE</code>{.docutils .literal
.notranslate}]{.pre} environment variable is deprecated (<a href="https://github.com/scrapy/scrapy/issues/3910">issue
3910</a>{.reference
.external})</p>
</li>
<li>
<p>[<code>scrapy.item.DictItem</code>{.docutils .literal .notranslate}]{.pre} is
deprecated, use [<code>Item</code>{.xref .py .py-class .docutils .literal
.notranslate}]{.pre} instead (<a href="https://github.com/scrapy/scrapy/issues/3999">issue
3999</a>{.reference
.external})
:::</p>
</li>
</ul>
<p>::: {#other-changes .section}</p>
<h5 id="other-changesheaderlink"><a class="header" href="#other-changesheaderlink">Other changes<a href="#other-changes" title="Permalink to this heading">¶</a>{.headerlink}</a></h5>
<ul>
<li>
<p>Minimum versions of optional Scrapy requirements that are covered by
continuous integration tests have been updated:</p>
<ul>
<li>
<p><a href="https://github.com/boto/botocore">botocore</a>{.reference
.external} 1.3.23</p>
</li>
<li>
<p><a href="https://python-pillow.org/">Pillow</a>{.reference .external} 3.4.2</p>
</li>
</ul>
<p>Lower versions of these optional requirements may work, but it is
not guaranteed (<a href="https://github.com/scrapy/scrapy/issues/3892">issue
3892</a>{.reference
.external})</p>
</li>
<li>
<p>GitHub templates for bug reports and feature requests (<a href="https://github.com/scrapy/scrapy/issues/3126">issue
3126</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/3471">issue
3471</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/3749">issue
3749</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/3754">issue
3754</a>{.reference
.external})</p>
</li>
<li>
<p>Continuous integration fixes (<a href="https://github.com/scrapy/scrapy/issues/3923">issue
3923</a>{.reference
.external})</p>
</li>
<li>
<p>Code cleanup (<a href="https://github.com/scrapy/scrapy/issues/3391">issue
3391</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/3907">issue
3907</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/3946">issue
3946</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/3950">issue
3950</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/4023">issue
4023</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/4031">issue
4031</a>{.reference
.external})
:::
:::</p>
</li>
</ul>
<p>::: {#scrapy-1-7-4-2019-10-21 .section}
[]{#release-1-7-4}</p>
<h4 id="scrapy-174-2019-10-21headerlink"><a class="header" href="#scrapy-174-2019-10-21headerlink">Scrapy 1.7.4 (2019-10-21)<a href="#scrapy-1-7-4-2019-10-21" title="Permalink to this heading">¶</a>{.headerlink}</a></h4>
<p>Revert the fix for <a href="https://github.com/scrapy/scrapy/issues/3804">issue
3804</a>{.reference
.external} (<a href="https://github.com/scrapy/scrapy/issues/3819">issue
3819</a>{.reference
.external}), which has a few undesired side effects (<a href="https://github.com/scrapy/scrapy/issues/3897">issue
3897</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/3976">issue
3976</a>{.reference
.external}).</p>
<p>As a result, when an item loader is initialized with an item,
<a href="index.html#scrapy.loader.ItemLoader.load_item" title="scrapy.loader.ItemLoader.load_item">[<code>ItemLoader.load_item()</code>{.xref .py .py-meth .docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal} once again makes later calls to
<a href="index.html#scrapy.loader.ItemLoader.get_output_value" title="scrapy.loader.ItemLoader.get_output_value">[<code>ItemLoader.get_output_value()</code>{.xref .py .py-meth .docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal} or <a href="index.html#scrapy.loader.ItemLoader.load_item" title="scrapy.loader.ItemLoader.load_item">[<code>ItemLoader.load_item()</code>{.xref .py .py-meth .docutils
.literal
.notranslate}]{.pre}</a>{.reference
.internal} return empty data.
:::</p>
<p>::: {#scrapy-1-7-3-2019-08-01 .section}
[]{#release-1-7-3}</p>
<h4 id="scrapy-173-2019-08-01headerlink"><a class="header" href="#scrapy-173-2019-08-01headerlink">Scrapy 1.7.3 (2019-08-01)<a href="#scrapy-1-7-3-2019-08-01" title="Permalink to this heading">¶</a>{.headerlink}</a></h4>
<p>Enforce lxml 4.3.5 or lower for Python 3.4 (<a href="https://github.com/scrapy/scrapy/issues/3912">issue
3912</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/3918">issue
3918</a>{.reference
.external}).
:::</p>
<p>::: {#scrapy-1-7-2-2019-07-23 .section}
[]{#release-1-7-2}</p>
<h4 id="scrapy-172-2019-07-23headerlink"><a class="header" href="#scrapy-172-2019-07-23headerlink">Scrapy 1.7.2 (2019-07-23)<a href="#scrapy-1-7-2-2019-07-23" title="Permalink to this heading">¶</a>{.headerlink}</a></h4>
<p>Fix Python 2 support (<a href="https://github.com/scrapy/scrapy/issues/3889">issue
3889</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/3893">issue
3893</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/3896">issue
3896</a>{.reference
.external}).
:::</p>
<p>::: {#scrapy-1-7-1-2019-07-18 .section}
[]{#release-1-7-1}</p>
<h4 id="scrapy-171-2019-07-18headerlink"><a class="header" href="#scrapy-171-2019-07-18headerlink">Scrapy 1.7.1 (2019-07-18)<a href="#scrapy-1-7-1-2019-07-18" title="Permalink to this heading">¶</a>{.headerlink}</a></h4>
<p>Re-packaging of Scrapy 1.7.0, which was missing some changes in PyPI.
:::</p>
<p>::: {#scrapy-1-7-0-2019-07-18 .section}
[]{#release-1-7-0}</p>
<h4 id="scrapy-170-2019-07-18headerlink"><a class="header" href="#scrapy-170-2019-07-18headerlink">Scrapy 1.7.0 (2019-07-18)<a href="#scrapy-1-7-0-2019-07-18" title="Permalink to this heading">¶</a>{.headerlink}</a></h4>
<p>::: {.admonition .note}
Note</p>
<p>Make sure you install Scrapy 1.7.1. The Scrapy 1.7.0 package in PyPI is
the result of an erroneous commit tagging and does not include all the
changes described below.
:::</p>
<p>Highlights:</p>
<ul>
<li>
<p>Improvements for crawls targeting multiple domains</p>
</li>
<li>
<p>A cleaner way to pass arguments to callbacks</p>
</li>
<li>
<p>A new class for JSON requests</p>
</li>
<li>
<p>Improvements for rule-based spiders</p>
</li>
<li>
<p>New features for feed exports</p>
</li>
</ul>
<p>::: {#id90 .section}</p>
<h5 id="backward-incompatible-changesheaderlink-1"><a class="header" href="#backward-incompatible-changesheaderlink-1">Backward-incompatible changes<a href="#id90" title="Permalink to this heading">¶</a>{.headerlink}</a></h5>
<ul>
<li>
<p>[<code>429</code>{.docutils .literal .notranslate}]{.pre} is now part of the
<a href="index.html#std-setting-RETRY_HTTP_CODES">[<code>RETRY_HTTP_CODES</code>{.xref .std .std-setting .docutils .literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal} setting by default</p>
<p>This change is <strong>backward incompatible</strong>. If you don't want to retry
[<code>429</code>{.docutils .literal .notranslate}]{.pre}, you must override
<a href="index.html#std-setting-RETRY_HTTP_CODES">[<code>RETRY_HTTP_CODES</code>{.xref .std .std-setting .docutils .literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal} accordingly.</p>
</li>
<li>
<p><a href="index.html#scrapy.crawler.Crawler" title="scrapy.crawler.Crawler">[<code>Crawler</code>{.xref .py .py-class .docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal}, <a href="index.html#scrapy.crawler.CrawlerRunner.crawl" title="scrapy.crawler.CrawlerRunner.crawl">[<code>CrawlerRunner.crawl</code>{.xref .py .py-class .docutils
.literal
.notranslate}]{.pre}</a>{.reference
.internal} and <a href="index.html#scrapy.crawler.CrawlerRunner.create_crawler" title="scrapy.crawler.CrawlerRunner.create_crawler">[<code>CrawlerRunner.create_crawler</code>{.xref .py .py-class
.docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal} no longer accept a <a href="index.html#scrapy.spiders.Spider" title="scrapy.spiders.Spider">[<code>Spider</code>{.xref .py .py-class
.docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal} subclass instance, they only accept a <a href="index.html#scrapy.spiders.Spider" title="scrapy.spiders.Spider">[<code>Spider</code>{.xref
.py .py-class .docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal} subclass now.</p>
<p><a href="index.html#scrapy.spiders.Spider" title="scrapy.spiders.Spider">[<code>Spider</code>{.xref .py .py-class .docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal} subclass instances were never meant to work, and they
were not working as one would expect: instead of using the passed
<a href="index.html#scrapy.spiders.Spider" title="scrapy.spiders.Spider">[<code>Spider</code>{.xref .py .py-class .docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal} subclass instance, their [<code>from_crawler</code>{.xref .py
.py-class .docutils .literal .notranslate}]{.pre} method was called
to generate a new instance.</p>
</li>
<li>
<p>Non-default values for the <a href="index.html#std-setting-SCHEDULER_PRIORITY_QUEUE">[<code>SCHEDULER_PRIORITY_QUEUE</code>{.xref .std
.std-setting .docutils .literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal} setting may stop working. Scheduler
priority queue classes now need to handle <a href="index.html#scrapy.http.Request" title="scrapy.http.Request">[<code>Request</code>{.xref .py
.py-class .docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal} objects instead of arbitrary Python data structures.</p>
</li>
<li>
<p>An additional [<code>crawler</code>{.docutils .literal .notranslate}]{.pre}
parameter has been added to the [<code>__init__</code>{.docutils .literal
.notranslate}]{.pre} method of the <a href="index.html#scrapy.core.scheduler.Scheduler" title="scrapy.core.scheduler.Scheduler">[<code>Scheduler</code>{.xref .py .py-class
.docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal} class. Custom scheduler subclasses which don't accept
arbitrary parameters in their [<code>__init__</code>{.docutils .literal
.notranslate}]{.pre} method might break because of this change.</p>
<p>For more information, see <a href="index.html#std-setting-SCHEDULER">[<code>SCHEDULER</code>{.xref .std .std-setting
.docutils .literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal}.</p>
</li>
</ul>
<p>See also <a href="#id94">[Deprecation removals]{.std .std-ref}</a>{.hoverxref
.tooltip .reference .internal} below.
:::</p>
<p>::: {#id91 .section}</p>
<h5 id="new-featuresheaderlink-1"><a class="header" href="#new-featuresheaderlink-1">New features<a href="#id91" title="Permalink to this heading">¶</a>{.headerlink}</a></h5>
<ul>
<li>
<p>A new scheduler priority queue,
[<code>scrapy.pqueues.DownloaderAwarePriorityQueue</code>{.docutils .literal
.notranslate}]{.pre}, may be <a href="index.html#broad-crawls-scheduler-priority-queue">[enabled]{.std
.std-ref}</a>{.hoverxref
.tooltip .reference .internal} for a significant scheduling
improvement on crawls targeting multiple web domains, at the cost of
no <a href="index.html#std-setting-CONCURRENT_REQUESTS_PER_IP">[<code>CONCURRENT_REQUESTS_PER_IP</code>{.xref .std .std-setting .docutils
.literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal} support (<a href="https://github.com/scrapy/scrapy/issues/3520">issue
3520</a>{.reference
.external})</p>
</li>
<li>
<p>A new <a href="index.html#scrapy.http.Request.cb_kwargs" title="scrapy.http.Request.cb_kwargs">[<code>Request.cb_kwargs</code>{.xref .py .py-attr .docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal} attribute provides a cleaner way to pass keyword
arguments to callback methods (<a href="https://github.com/scrapy/scrapy/issues/1138">issue
1138</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/3563">issue
3563</a>{.reference
.external})</p>
</li>
<li>
<p>A new <a href="index.html#scrapy.http.JsonRequest" title="scrapy.http.JsonRequest">[<code>JSONRequest</code>{.xref .py .py-class .docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal} class offers a more convenient way to build JSON requests
(<a href="https://github.com/scrapy/scrapy/issues/3504">issue
3504</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/3505">issue
3505</a>{.reference
.external})</p>
</li>
<li>
<p>A [<code>process_request</code>{.docutils .literal .notranslate}]{.pre}
callback passed to the <a href="index.html#scrapy.spiders.Rule" title="scrapy.spiders.Rule">[<code>Rule</code>{.xref .py .py-class .docutils
.literal
.notranslate}]{.pre}</a>{.reference
.internal} [<code>__init__</code>{.docutils .literal .notranslate}]{.pre}
method now receives the <a href="index.html#scrapy.http.Response" title="scrapy.http.Response">[<code>Response</code>{.xref .py .py-class .docutils
.literal
.notranslate}]{.pre}</a>{.reference
.internal} object that originated the request as its second argument
(<a href="https://github.com/scrapy/scrapy/issues/3682">issue
3682</a>{.reference
.external})</p>
</li>
<li>
<p>A new [<code>restrict_text</code>{.docutils .literal .notranslate}]{.pre}
parameter for the <a href="index.html#scrapy.linkextractors.lxmlhtml.LxmlLinkExtractor" title="scrapy.linkextractors.lxmlhtml.LxmlLinkExtractor">[<code>LinkExtractor</code>{.xref .py .py-attr .docutils
.literal
.notranslate}]{.pre}</a>{.reference
.internal} [<code>__init__</code>{.docutils .literal .notranslate}]{.pre}
method allows filtering links by linking text (<a href="https://github.com/scrapy/scrapy/issues/3622">issue
3622</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/3635">issue
3635</a>{.reference
.external})</p>
</li>
<li>
<p>A new <a href="index.html#std-setting-FEED_STORAGE_S3_ACL">[<code>FEED_STORAGE_S3_ACL</code>{.xref .std .std-setting .docutils
.literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal} setting allows defining a custom ACL
for feeds exported to Amazon S3 (<a href="https://github.com/scrapy/scrapy/issues/3607">issue
3607</a>{.reference
.external})</p>
</li>
<li>
<p>A new <a href="index.html#std-setting-FEED_STORAGE_FTP_ACTIVE">[<code>FEED_STORAGE_FTP_ACTIVE</code>{.xref .std .std-setting .docutils
.literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal} setting allows using FTP's active
connection mode for feeds exported to FTP servers (<a href="https://github.com/scrapy/scrapy/issues/3829">issue
3829</a>{.reference
.external})</p>
</li>
<li>
<p>A new <a href="index.html#std-setting-METAREFRESH_IGNORE_TAGS">[<code>METAREFRESH_IGNORE_TAGS</code>{.xref .std .std-setting .docutils
.literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal} setting allows overriding which HTML
tags are ignored when searching a response for HTML meta tags that
trigger a redirect (<a href="https://github.com/scrapy/scrapy/issues/1422">issue
1422</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/3768">issue
3768</a>{.reference
.external})</p>
</li>
<li>
<p>A new <a href="index.html#std-reqmeta-redirect_reasons">[<code>redirect_reasons</code>{.xref .std .std-reqmeta .docutils
.literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal} request meta key exposes the reason
(status code, meta refresh) behind every followed redirect (<a href="https://github.com/scrapy/scrapy/issues/3581">issue
3581</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/3687">issue
3687</a>{.reference
.external})</p>
</li>
<li>
<p>The [<code>SCRAPY_CHECK</code>{.docutils .literal .notranslate}]{.pre} variable
is now set to the [<code>true</code>{.docutils .literal .notranslate}]{.pre}
string during runs of the <a href="index.html#std-command-check">[<code>check</code>{.xref .std .std-command
.docutils .literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal} command, which allows <a href="index.html#detecting-contract-check-runs">[detecting
contract check runs from code]{.std
.std-ref}</a>{.hoverxref
.tooltip .reference .internal} (<a href="https://github.com/scrapy/scrapy/issues/3704">issue
3704</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/3739">issue
3739</a>{.reference
.external})</p>
</li>
<li>
<p>A new [<code>Item.deepcopy()</code>{.xref .py .py-meth .docutils .literal
.notranslate}]{.pre} method makes it easier to <a href="index.html#copying-items">[deep-copy
items]{.std .std-ref}</a>{.hoverxref .tooltip
.reference .internal} (<a href="https://github.com/scrapy/scrapy/issues/1493">issue
1493</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/3671">issue
3671</a>{.reference
.external})</p>
</li>
<li>
<p><a href="index.html#scrapy.extensions.corestats.CoreStats" title="scrapy.extensions.corestats.CoreStats">[<code>CoreStats</code>{.xref .py .py-class .docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal} also logs [<code>elapsed_time_seconds</code>{.docutils .literal
.notranslate}]{.pre} now (<a href="https://github.com/scrapy/scrapy/issues/3638">issue
3638</a>{.reference
.external})</p>
</li>
<li>
<p>Exceptions from <a href="index.html#scrapy.loader.ItemLoader" title="scrapy.loader.ItemLoader">[<code>ItemLoader</code>{.xref .py .py-class .docutils
.literal
.notranslate}]{.pre}</a>{.reference
.internal} <a href="index.html#topics-loaders-processors">[input and output processors]{.std
.std-ref}</a>{.hoverxref .tooltip
.reference .internal} are now more verbose (<a href="https://github.com/scrapy/scrapy/issues/3836">issue
3836</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/3840">issue
3840</a>{.reference
.external})</p>
</li>
<li>
<p><a href="index.html#scrapy.crawler.Crawler" title="scrapy.crawler.Crawler">[<code>Crawler</code>{.xref .py .py-class .docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal}, <a href="index.html#scrapy.crawler.CrawlerRunner.crawl" title="scrapy.crawler.CrawlerRunner.crawl">[<code>CrawlerRunner.crawl</code>{.xref .py .py-class .docutils
.literal
.notranslate}]{.pre}</a>{.reference
.internal} and <a href="index.html#scrapy.crawler.CrawlerRunner.create_crawler" title="scrapy.crawler.CrawlerRunner.create_crawler">[<code>CrawlerRunner.create_crawler</code>{.xref .py .py-class
.docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal} now fail gracefully if they receive a <a href="index.html#scrapy.spiders.Spider" title="scrapy.spiders.Spider">[<code>Spider</code>{.xref
.py .py-class .docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal} subclass instance instead of the subclass itself (<a href="https://github.com/scrapy/scrapy/issues/2283">issue
2283</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/3610">issue
3610</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/3872">issue
3872</a>{.reference
.external})
:::</p>
</li>
</ul>
<p>::: {#id92 .section}</p>
<h5 id="bug-fixesheaderlink-1"><a class="header" href="#bug-fixesheaderlink-1">Bug fixes<a href="#id92" title="Permalink to this heading">¶</a>{.headerlink}</a></h5>
<ul>
<li>
<p><a href="index.html#scrapy.spidermiddlewares.SpiderMiddleware.process_spider_exception" title="scrapy.spidermiddlewares.SpiderMiddleware.process_spider_exception">[<code>process_spider_exception()</code>{.xref .py .py-meth .docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal} is now also invoked for generators (<a href="https://github.com/scrapy/scrapy/issues/220">issue
220</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/2061">issue
2061</a>{.reference
.external})</p>
</li>
<li>
<p>System exceptions like
<a href="https://docs.python.org/3/library/exceptions.html#KeyboardInterrupt">KeyboardInterrupt</a>{.reference
.external} are no longer caught (<a href="https://github.com/scrapy/scrapy/issues/3726">issue
3726</a>{.reference
.external})</p>
</li>
<li>
<p><a href="index.html#scrapy.loader.ItemLoader.load_item" title="scrapy.loader.ItemLoader.load_item">[<code>ItemLoader.load_item()</code>{.xref .py .py-meth .docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal} no longer makes later calls to
<a href="index.html#scrapy.loader.ItemLoader.get_output_value" title="scrapy.loader.ItemLoader.get_output_value">[<code>ItemLoader.get_output_value()</code>{.xref .py .py-meth .docutils
.literal
.notranslate}]{.pre}</a>{.reference
.internal} or <a href="index.html#scrapy.loader.ItemLoader.load_item" title="scrapy.loader.ItemLoader.load_item">[<code>ItemLoader.load_item()</code>{.xref .py .py-meth
.docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal} return empty data (<a href="https://github.com/scrapy/scrapy/issues/3804">issue
3804</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/3819">issue
3819</a>{.reference
.external})</p>
</li>
<li>
<p>The images pipeline (<a href="index.html#scrapy.pipelines.images.ImagesPipeline" title="scrapy.pipelines.images.ImagesPipeline">[<code>ImagesPipeline</code>{.xref .py .py-class
.docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal}) no longer ignores these Amazon S3 settings:
<a href="index.html#std-setting-AWS_ENDPOINT_URL">[<code>AWS_ENDPOINT_URL</code>{.xref .std .std-setting .docutils .literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal}, <a href="index.html#std-setting-AWS_REGION_NAME">[<code>AWS_REGION_NAME</code>{.xref .std
.std-setting .docutils .literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal}, <a href="index.html#std-setting-AWS_USE_SSL">[<code>AWS_USE_SSL</code>{.xref .std
.std-setting .docutils .literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal}, <a href="index.html#std-setting-AWS_VERIFY">[<code>AWS_VERIFY</code>{.xref .std
.std-setting .docutils .literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal} (<a href="https://github.com/scrapy/scrapy/issues/3625">issue
3625</a>{.reference
.external})</p>
</li>
<li>
<p>Fixed a memory leak in
[<code>scrapy.pipelines.media.MediaPipeline</code>{.docutils .literal
.notranslate}]{.pre} affecting, for example, non-200 responses and
exceptions from custom middlewares (<a href="https://github.com/scrapy/scrapy/issues/3813">issue
3813</a>{.reference
.external})</p>
</li>
<li>
<p>Requests with private callbacks are now correctly unserialized from
disk (<a href="https://github.com/scrapy/scrapy/issues/3790">issue
3790</a>{.reference
.external})</p>
</li>
<li>
<p>[<code>FormRequest.from_response()</code>{.xref .py .py-meth .docutils .literal
.notranslate}]{.pre} now handles invalid methods like major web
browsers (<a href="https://github.com/scrapy/scrapy/issues/3777">issue
3777</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/3794">issue
3794</a>{.reference
.external})
:::</p>
</li>
</ul>
<p>::: {#id93 .section}</p>
<h5 id="documentationheaderlink-1"><a class="header" href="#documentationheaderlink-1">Documentation<a href="#id93" title="Permalink to this heading">¶</a>{.headerlink}</a></h5>
<ul>
<li>
<p>A new topic, <a href="index.html#topics-dynamic-content">[Selecting dynamically-loaded content]{.std
.std-ref}</a>{.hoverxref .tooltip
.reference .internal}, covers recommended approaches to read
dynamically-loaded data (<a href="https://github.com/scrapy/scrapy/issues/3703">issue
3703</a>{.reference
.external})</p>
</li>
<li>
<p><a href="index.html#topics-broad-crawls">[Broad Crawls]{.std
.std-ref}</a>{.hoverxref .tooltip
.reference .internal} now features information about memory usage
(<a href="https://github.com/scrapy/scrapy/issues/1264">issue
1264</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/3866">issue
3866</a>{.reference
.external})</p>
</li>
<li>
<p>The documentation of <a href="index.html#scrapy.spiders.Rule" title="scrapy.spiders.Rule">[<code>Rule</code>{.xref .py .py-class .docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal} now covers how to access the text of a link when using
<a href="index.html#scrapy.spiders.CrawlSpider" title="scrapy.spiders.CrawlSpider">[<code>CrawlSpider</code>{.xref .py .py-class .docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal} (<a href="https://github.com/scrapy/scrapy/issues/3711">issue
3711</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/3712">issue
3712</a>{.reference
.external})</p>
</li>
<li>
<p>A new section, <a href="index.html#httpcache-storage-custom">[Writing your own storage backend]{.std
.std-ref}</a>{.hoverxref .tooltip
.reference .internal}, covers writing a custom cache storage backend
for <a href="index.html#scrapy.downloadermiddlewares.httpcache.HttpCacheMiddleware" title="scrapy.downloadermiddlewares.httpcache.HttpCacheMiddleware">[<code>HttpCacheMiddleware</code>{.xref .py .py-class .docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal} (<a href="https://github.com/scrapy/scrapy/issues/3683">issue
3683</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/3692">issue
3692</a>{.reference
.external})</p>
</li>
<li>
<p>A new <a href="index.html#faq">[FAQ]{.std .std-ref}</a>{.hoverxref .tooltip
.reference .internal} entry, <a href="index.html#faq-split-item">[How to split an item into multiple
items in an item pipeline?]{.std
.std-ref}</a>{.hoverxref .tooltip .reference
.internal}, explains what to do when you want to split an item into
multiple items from an item pipeline (<a href="https://github.com/scrapy/scrapy/issues/2240">issue
2240</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/3672">issue
3672</a>{.reference
.external})</p>
</li>
<li>
<p>Updated the <a href="index.html#faq-bfo-dfo">[FAQ entry about crawl order]{.std
.std-ref}</a>{.hoverxref .tooltip .reference
.internal} to explain why the first few requests rarely follow the
desired order (<a href="https://github.com/scrapy/scrapy/issues/1739">issue
1739</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/3621">issue
3621</a>{.reference
.external})</p>
</li>
<li>
<p>The <a href="index.html#std-setting-LOGSTATS_INTERVAL">[<code>LOGSTATS_INTERVAL</code>{.xref .std .std-setting .docutils .literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal} setting (<a href="https://github.com/scrapy/scrapy/issues/3730">issue
3730</a>{.reference
.external}), the <a href="index.html#scrapy.pipelines.files.FilesPipeline.file_path" title="scrapy.pipelines.files.FilesPipeline.file_path">[<code>FilesPipeline.file_path</code>{.xref .py .py-meth
.docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal} and <a href="index.html#scrapy.pipelines.images.ImagesPipeline.file_path" title="scrapy.pipelines.images.ImagesPipeline.file_path">[<code>ImagesPipeline.file_path</code>{.xref .py .py-meth
.docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal} methods (<a href="https://github.com/scrapy/scrapy/issues/2253">issue
2253</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/3609">issue
3609</a>{.reference
.external}) and the <a href="index.html#scrapy.crawler.Crawler.stop" title="scrapy.crawler.Crawler.stop">[<code>Crawler.stop()</code>{.xref .py .py-meth .docutils
.literal
.notranslate}]{.pre}</a>{.reference
.internal} method (<a href="https://github.com/scrapy/scrapy/issues/3842">issue
3842</a>{.reference
.external}) are now documented</p>
</li>
<li>
<p>Some parts of the documentation that were confusing or misleading
are now clearer (<a href="https://github.com/scrapy/scrapy/issues/1347">issue
1347</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/1789">issue
1789</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/2289">issue
2289</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/3069">issue
3069</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/3615">issue
3615</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/3626">issue
3626</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/3668">issue
3668</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/3670">issue
3670</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/3673">issue
3673</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/3728">issue
3728</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/3762">issue
3762</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/3861">issue
3861</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/3882">issue
3882</a>{.reference
.external})</p>
</li>
<li>
<p>Minor documentation fixes (<a href="https://github.com/scrapy/scrapy/issues/3648">issue
3648</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/3649">issue
3649</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/3662">issue
3662</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/3674">issue
3674</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/3676">issue
3676</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/3694">issue
3694</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/3724">issue
3724</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/3764">issue
3764</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/3767">issue
3767</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/3791">issue
3791</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/3797">issue
3797</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/3806">issue
3806</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/3812">issue
3812</a>{.reference
.external})
:::</p>
</li>
</ul>
<p>::: {#id94 .section}
[]{#id95}</p>
<h5 id="deprecation-removalsheaderlink-1"><a class="header" href="#deprecation-removalsheaderlink-1">Deprecation removals<a href="#id94" title="Permalink to this heading">¶</a>{.headerlink}</a></h5>
<p>The following deprecated APIs have been removed (<a href="https://github.com/scrapy/scrapy/issues/3578">issue
3578</a>{.reference
.external}):</p>
<ul>
<li>
<p>[<code>scrapy.conf</code>{.docutils .literal .notranslate}]{.pre} (use
<a href="index.html#scrapy.crawler.Crawler.settings" title="scrapy.crawler.Crawler.settings">[<code>Crawler.settings</code>{.xref .py .py-attr .docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal})</p>
</li>
<li>
<p>From [<code>scrapy.core.downloader.handlers</code>{.docutils .literal
.notranslate}]{.pre}:</p>
<ul>
<li>[<code>http.HttpDownloadHandler</code>{.docutils .literal
.notranslate}]{.pre} (use
[<code>http10.HTTP10DownloadHandler</code>{.docutils .literal
.notranslate}]{.pre})</li>
</ul>
</li>
<li>
<p>[<code>scrapy.loader.ItemLoader._get_values</code>{.docutils .literal
.notranslate}]{.pre} (use [<code>_get_xpathvalues</code>{.docutils .literal
.notranslate}]{.pre})</p>
</li>
<li>
<p>[<code>scrapy.loader.XPathItemLoader</code>{.docutils .literal
.notranslate}]{.pre} (use <a href="index.html#scrapy.loader.ItemLoader" title="scrapy.loader.ItemLoader">[<code>ItemLoader</code>{.xref .py .py-class
.docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal})</p>
</li>
<li>
<p>[<code>scrapy.log</code>{.docutils .literal .notranslate}]{.pre} (see
<a href="index.html#topics-logging">[Logging]{.std .std-ref}</a>{.hoverxref
.tooltip .reference .internal})</p>
</li>
<li>
<p>From [<code>scrapy.pipelines</code>{.docutils .literal .notranslate}]{.pre}:</p>
<ul>
<li>
<p>[<code>files.FilesPipeline.file_key</code>{.docutils .literal
.notranslate}]{.pre} (use [<code>file_path</code>{.docutils .literal
.notranslate}]{.pre})</p>
</li>
<li>
<p>[<code>images.ImagesPipeline.file_key</code>{.docutils .literal
.notranslate}]{.pre} (use [<code>file_path</code>{.docutils .literal
.notranslate}]{.pre})</p>
</li>
<li>
<p>[<code>images.ImagesPipeline.image_key</code>{.docutils .literal
.notranslate}]{.pre} (use [<code>file_path</code>{.docutils .literal
.notranslate}]{.pre})</p>
</li>
<li>
<p>[<code>images.ImagesPipeline.thumb_key</code>{.docutils .literal
.notranslate}]{.pre} (use [<code>thumb_path</code>{.docutils .literal
.notranslate}]{.pre})</p>
</li>
</ul>
</li>
<li>
<p>From both [<code>scrapy.selector</code>{.docutils .literal .notranslate}]{.pre}
and [<code>scrapy.selector.lxmlsel</code>{.docutils .literal
.notranslate}]{.pre}:</p>
<ul>
<li>
<p>[<code>HtmlXPathSelector</code>{.docutils .literal .notranslate}]{.pre}
(use <a href="index.html#scrapy.selector.Selector" title="scrapy.selector.Selector">[<code>Selector</code>{.xref .py .py-class .docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal})</p>
</li>
<li>
<p>[<code>XmlXPathSelector</code>{.docutils .literal .notranslate}]{.pre} (use
<a href="index.html#scrapy.selector.Selector" title="scrapy.selector.Selector">[<code>Selector</code>{.xref .py .py-class .docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal})</p>
</li>
<li>
<p>[<code>XPathSelector</code>{.docutils .literal .notranslate}]{.pre} (use
<a href="index.html#scrapy.selector.Selector" title="scrapy.selector.Selector">[<code>Selector</code>{.xref .py .py-class .docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal})</p>
</li>
<li>
<p>[<code>XPathSelectorList</code>{.docutils .literal .notranslate}]{.pre}
(use <a href="index.html#scrapy.selector.Selector" title="scrapy.selector.Selector">[<code>Selector</code>{.xref .py .py-class .docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal})</p>
</li>
</ul>
</li>
<li>
<p>From [<code>scrapy.selector.csstranslator</code>{.docutils .literal
.notranslate}]{.pre}:</p>
<ul>
<li>
<p>[<code>ScrapyGenericTranslator</code>{.docutils .literal
.notranslate}]{.pre} (use
<a href="https://parsel.readthedocs.io/en/latest/parsel.html#parsel.csstranslator.GenericTranslator">parsel.csstranslator.GenericTranslator</a>{.reference
.external})</p>
</li>
<li>
<p>[<code>ScrapyHTMLTranslator</code>{.docutils .literal .notranslate}]{.pre}
(use
<a href="https://parsel.readthedocs.io/en/latest/parsel.html#parsel.csstranslator.HTMLTranslator">parsel.csstranslator.HTMLTranslator</a>{.reference
.external})</p>
</li>
<li>
<p>[<code>ScrapyXPathExpr</code>{.docutils .literal .notranslate}]{.pre} (use
<a href="https://parsel.readthedocs.io/en/latest/parsel.html#parsel.csstranslator.XPathExpr">parsel.csstranslator.XPathExpr</a>{.reference
.external})</p>
</li>
</ul>
</li>
<li>
<p>From <a href="index.html#scrapy.selector.Selector" title="scrapy.selector.Selector">[<code>Selector</code>{.xref .py .py-class .docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal}:</p>
<ul>
<li>
<p>[<code>_root</code>{.docutils .literal .notranslate}]{.pre} (both the
[<code>__init__</code>{.docutils .literal .notranslate}]{.pre} method
argument and the object property, use [<code>root</code>{.docutils .literal
.notranslate}]{.pre})</p>
</li>
<li>
<p>[<code>extract_unquoted</code>{.docutils .literal .notranslate}]{.pre} (use
[<code>getall</code>{.docutils .literal .notranslate}]{.pre})</p>
</li>
<li>
<p>[<code>select</code>{.docutils .literal .notranslate}]{.pre} (use
[<code>xpath</code>{.docutils .literal .notranslate}]{.pre})</p>
</li>
</ul>
</li>
<li>
<p>From <a href="index.html#scrapy.selector.SelectorList" title="scrapy.selector.SelectorList">[<code>SelectorList</code>{.xref .py .py-class .docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal}:</p>
<ul>
<li>
<p>[<code>extract_unquoted</code>{.docutils .literal .notranslate}]{.pre} (use
[<code>getall</code>{.docutils .literal .notranslate}]{.pre})</p>
</li>
<li>
<p>[<code>select</code>{.docutils .literal .notranslate}]{.pre} (use
[<code>xpath</code>{.docutils .literal .notranslate}]{.pre})</p>
</li>
<li>
<p>[<code>x</code>{.docutils .literal .notranslate}]{.pre} (use
[<code>xpath</code>{.docutils .literal .notranslate}]{.pre})</p>
</li>
</ul>
</li>
<li>
<p>[<code>scrapy.spiders.BaseSpider</code>{.docutils .literal .notranslate}]{.pre}
(use <a href="index.html#scrapy.spiders.Spider" title="scrapy.spiders.Spider">[<code>Spider</code>{.xref .py .py-class .docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal})</p>
</li>
<li>
<p>From <a href="index.html#scrapy.spiders.Spider" title="scrapy.spiders.Spider">[<code>Spider</code>{.xref .py .py-class .docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal} (and subclasses):</p>
<ul>
<li>
<p>[<code>DOWNLOAD_DELAY</code>{.docutils .literal .notranslate}]{.pre} (use
<a href="index.html#spider-download-delay-attribute">[download_delay]{.std
.std-ref}</a>{.hoverxref
.tooltip .reference .internal})</p>
</li>
<li>
<p>[<code>set_crawler</code>{.docutils .literal .notranslate}]{.pre} (use
[<code>from_crawler()</code>{.xref .py .py-meth .docutils .literal
.notranslate}]{.pre})</p>
</li>
</ul>
</li>
<li>
<p>[<code>scrapy.spiders.spiders</code>{.docutils .literal .notranslate}]{.pre}
(use <a href="index.html#scrapy.spiderloader.SpiderLoader" title="scrapy.spiderloader.SpiderLoader">[<code>SpiderLoader</code>{.xref .py .py-class .docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal})</p>
</li>
<li>
<p>[<code>scrapy.telnet</code>{.docutils .literal .notranslate}]{.pre} (use
<a href="index.html#module-scrapy.extensions.telnet" title="scrapy.extensions.telnet: Telnet console">[<code>scrapy.extensions.telnet</code>{.xref .py .py-mod .docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal})</p>
</li>
<li>
<p>From [<code>scrapy.utils.python</code>{.docutils .literal .notranslate}]{.pre}:</p>
<ul>
<li>
<p>[<code>str_to_unicode</code>{.docutils .literal .notranslate}]{.pre} (use
[<code>to_unicode</code>{.docutils .literal .notranslate}]{.pre})</p>
</li>
<li>
<p>[<code>unicode_to_str</code>{.docutils .literal .notranslate}]{.pre} (use
[<code>to_bytes</code>{.docutils .literal .notranslate}]{.pre})</p>
</li>
</ul>
</li>
<li>
<p>[<code>scrapy.utils.response.body_or_str</code>{.docutils .literal
.notranslate}]{.pre}</p>
</li>
</ul>
<p>The following deprecated settings have also been removed (<a href="https://github.com/scrapy/scrapy/issues/3578">issue
3578</a>{.reference
.external}):</p>
<ul>
<li>[<code>SPIDER_MANAGER_CLASS</code>{.docutils .literal .notranslate}]{.pre} (use
<a href="index.html#std-setting-SPIDER_LOADER_CLASS">[<code>SPIDER_LOADER_CLASS</code>{.xref .std .std-setting .docutils .literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal})
:::</li>
</ul>
<p>::: {#id96 .section}
[]{#id97}</p>
<h5 id="deprecationsheaderlink-1"><a class="header" href="#deprecationsheaderlink-1">Deprecations<a href="#id96" title="Permalink to this heading">¶</a>{.headerlink}</a></h5>
<ul>
<li>
<p>The [<code>queuelib.PriorityQueue</code>{.docutils .literal
.notranslate}]{.pre} value for the
<a href="index.html#std-setting-SCHEDULER_PRIORITY_QUEUE">[<code>SCHEDULER_PRIORITY_QUEUE</code>{.xref .std .std-setting .docutils
.literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal} setting is deprecated. Use
[<code>scrapy.pqueues.ScrapyPriorityQueue</code>{.docutils .literal
.notranslate}]{.pre} instead.</p>
</li>
<li>
<p>[<code>process_request</code>{.docutils .literal .notranslate}]{.pre} callbacks
passed to <a href="index.html#scrapy.spiders.Rule" title="scrapy.spiders.Rule">[<code>Rule</code>{.xref .py .py-class .docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal} that do not accept two arguments are deprecated.</p>
</li>
<li>
<p>The following modules are deprecated:</p>
<ul>
<li>
<p>[<code>scrapy.utils.http</code>{.docutils .literal .notranslate}]{.pre}
(use
<a href="https://w3lib.readthedocs.io/en/latest/w3lib.html#module-w3lib.http">w3lib.http</a>{.reference
.external})</p>
</li>
<li>
<p>[<code>scrapy.utils.markup</code>{.docutils .literal .notranslate}]{.pre}
(use
<a href="https://w3lib.readthedocs.io/en/latest/w3lib.html#module-w3lib.html">w3lib.html</a>{.reference
.external})</p>
</li>
<li>
<p>[<code>scrapy.utils.multipart</code>{.docutils .literal
.notranslate}]{.pre} (use
<a href="https://urllib3.readthedocs.io/en/latest/index.html">urllib3</a>{.reference
.external})</p>
</li>
</ul>
</li>
<li>
<p>The [<code>scrapy.utils.datatypes.MergeDict</code>{.docutils .literal
.notranslate}]{.pre} class is deprecated for Python 3 code bases.
Use <a href="https://docs.python.org/3/library/collections.html#collections.ChainMap" title="(in Python v3.12)">[<code>ChainMap</code>{.xref .py .py-class .docutils .literal
.notranslate}]{.pre}</a>{.reference
.external} instead. (<a href="https://github.com/scrapy/scrapy/issues/3878">issue
3878</a>{.reference
.external})</p>
</li>
<li>
<p>The [<code>scrapy.utils.gz.is_gzipped</code>{.docutils .literal
.notranslate}]{.pre} function is deprecated. Use
[<code>scrapy.utils.gz.gzip_magic_number</code>{.docutils .literal
.notranslate}]{.pre} instead.
:::</p>
</li>
</ul>
<p>::: {#id98 .section}</p>
<h5 id="other-changesheaderlink-1"><a class="header" href="#other-changesheaderlink-1">Other changes<a href="#id98" title="Permalink to this heading">¶</a>{.headerlink}</a></h5>
<ul>
<li>
<p>It is now possible to run all tests from the same
<a href="https://pypi.org/project/tox/">tox</a>{.reference .external}
environment in parallel; the documentation now covers <a href="index.html#running-tests">[this and
other ways to run tests]{.std
.std-ref}</a>{.hoverxref .tooltip .reference
.internal} (<a href="https://github.com/scrapy/scrapy/issues/3707">issue
3707</a>{.reference
.external})</p>
</li>
<li>
<p>It is now possible to generate an API documentation coverage report
(<a href="https://github.com/scrapy/scrapy/issues/3806">issue
3806</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/3810">issue
3810</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/3860">issue
3860</a>{.reference
.external})</p>
</li>
<li>
<p>The <a href="index.html#documentation-policies">[documentation policies]{.std
.std-ref}</a>{.hoverxref .tooltip
.reference .internal} now require
<a href="https://docs.python.org/3/glossary.html#term-docstring">docstrings</a>{.reference
.external} (<a href="https://github.com/scrapy/scrapy/issues/3701">issue
3701</a>{.reference
.external}) that follow <a href="https://www.python.org/dev/peps/pep-0257/">PEP
257</a>{.reference
.external} (<a href="https://github.com/scrapy/scrapy/issues/3748">issue
3748</a>{.reference
.external})</p>
</li>
<li>
<p>Internal fixes and cleanup (<a href="https://github.com/scrapy/scrapy/issues/3629">issue
3629</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/3643">issue
3643</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/3684">issue
3684</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/3698">issue
3698</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/3734">issue
3734</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/3735">issue
3735</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/3736">issue
3736</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/3737">issue
3737</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/3809">issue
3809</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/3821">issue
3821</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/3825">issue
3825</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/3827">issue
3827</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/3833">issue
3833</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/3857">issue
3857</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/3877">issue
3877</a>{.reference
.external})
:::
:::</p>
</li>
</ul>
<p>::: {#scrapy-1-6-0-2019-01-30 .section}
[]{#release-1-6-0}</p>
<h4 id="scrapy-160-2019-01-30headerlink"><a class="header" href="#scrapy-160-2019-01-30headerlink">Scrapy 1.6.0 (2019-01-30)<a href="#scrapy-1-6-0-2019-01-30" title="Permalink to this heading">¶</a>{.headerlink}</a></h4>
<p>Highlights:</p>
<ul>
<li>
<p>better Windows support;</p>
</li>
<li>
<p>Python 3.7 compatibility;</p>
</li>
<li>
<p>big documentation improvements, including a switch from
[<code>.extract_first()</code>{.docutils .literal .notranslate}]{.pre} +
[<code>.extract()</code>{.docutils .literal .notranslate}]{.pre} API to
[<code>.get()</code>{.docutils .literal .notranslate}]{.pre} +
[<code>.getall()</code>{.docutils .literal .notranslate}]{.pre} API;</p>
</li>
<li>
<p>feed exports, FilePipeline and MediaPipeline improvements;</p>
</li>
<li>
<p>better extensibility: <a href="index.html#std-signal-item_error">[<code>item_error</code>{.xref .std .std-signal
.docutils .literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal} and
<a href="index.html#std-signal-request_reached_downloader">[<code>request_reached_downloader</code>{.xref .std .std-signal .docutils
.literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal} signals; [<code>from_crawler</code>{.docutils
.literal .notranslate}]{.pre} support for feed exporters, feed
storages and dupefilters.</p>
</li>
<li>
<p>[<code>scrapy.contracts</code>{.docutils .literal .notranslate}]{.pre} fixes
and new features;</p>
</li>
<li>
<p>telnet console security improvements, first released as a backport
in <a href="#release-1-5-2">[Scrapy 1.5.2 (2019-01-22)]{.std
.std-ref}</a>{.hoverxref .tooltip .reference
.internal};</p>
</li>
<li>
<p>clean-up of the deprecated code;</p>
</li>
<li>
<p>various bug fixes, small new features and usability improvements
across the codebase.</p>
</li>
</ul>
<p>::: {#selector-api-changes .section}</p>
<h5 id="selector-api-changesheaderlink"><a class="header" href="#selector-api-changesheaderlink">Selector API changes<a href="#selector-api-changes" title="Permalink to this heading">¶</a>{.headerlink}</a></h5>
<p>While these are not changes in Scrapy itself, but rather in the
<a href="https://github.com/scrapy/parsel">parsel</a>{.reference .external} library
which Scrapy uses for xpath/css selectors, these changes are worth
mentioning here. Scrapy now depends on parsel &gt;= 1.5, and Scrapy
documentation is updated to follow recent [<code>parsel</code>{.docutils .literal
.notranslate}]{.pre} API conventions.</p>
<p>Most visible change is that [<code>.get()</code>{.docutils .literal
.notranslate}]{.pre} and [<code>.getall()</code>{.docutils .literal
.notranslate}]{.pre} selector methods are now preferred over
[<code>.extract_first()</code>{.docutils .literal .notranslate}]{.pre} and
[<code>.extract()</code>{.docutils .literal .notranslate}]{.pre}. We feel that
these new methods result in a more concise and readable code. See
<a href="index.html#old-extraction-api">[extract() and extract_first()]{.std
.std-ref}</a>{.hoverxref .tooltip .reference
.internal} for more details.</p>
<p>::: {.admonition .note}
Note</p>
<p>There are currently <strong>no plans</strong> to deprecate [<code>.extract()</code>{.docutils
.literal .notranslate}]{.pre} and [<code>.extract_first()</code>{.docutils .literal
.notranslate}]{.pre} methods.
:::</p>
<p>Another useful new feature is the introduction of
[<code>Selector.attrib</code>{.docutils .literal .notranslate}]{.pre} and
[<code>SelectorList.attrib</code>{.docutils .literal .notranslate}]{.pre}
properties, which make it easier to get attributes of HTML elements. See
<a href="index.html#selecting-attributes">[Selecting element attributes]{.std
.std-ref}</a>{.hoverxref .tooltip
.reference .internal}.</p>
<p>CSS selectors are cached in parsel &gt;= 1.5, which makes them faster when
the same CSS path is used many times. This is very common in case of
Scrapy spiders: callbacks are usually called several times, on different
pages.</p>
<p>If you're using custom [<code>Selector</code>{.docutils .literal
.notranslate}]{.pre} or [<code>SelectorList</code>{.docutils .literal
.notranslate}]{.pre} subclasses, a <strong>backward incompatible</strong> change in
parsel may affect your code. See <a href="https://parsel.readthedocs.io/en/latest/history.html">parsel
changelog</a>{.reference
.external} for a detailed description, as well as for the full list of
improvements.
:::</p>
<p>::: {#telnet-console .section}</p>
<h5 id="telnet-consoleheaderlink"><a class="header" href="#telnet-consoleheaderlink">Telnet console<a href="#telnet-console" title="Permalink to this heading">¶</a>{.headerlink}</a></h5>
<p><strong>Backward incompatible</strong>: Scrapy's telnet console now requires username
and password. See <a href="index.html#topics-telnetconsole">[Telnet Console]{.std
.std-ref}</a>{.hoverxref .tooltip
.reference .internal} for more details. This change fixes a <strong>security
issue</strong>; see <a href="#release-1-5-2">[Scrapy 1.5.2 (2019-01-22)]{.std
.std-ref}</a>{.hoverxref .tooltip .reference .internal}
release notes for details.
:::</p>
<p>::: {#new-extensibility-features .section}</p>
<h5 id="new-extensibility-featuresheaderlink"><a class="header" href="#new-extensibility-featuresheaderlink">New extensibility features<a href="#new-extensibility-features" title="Permalink to this heading">¶</a>{.headerlink}</a></h5>
<ul>
<li>
<p>[<code>from_crawler</code>{.docutils .literal .notranslate}]{.pre} support is
added to feed exporters and feed storages. This, among other things,
allows to access Scrapy settings from custom feed storages and
exporters (<a href="https://github.com/scrapy/scrapy/issues/1605">issue
1605</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/3348">issue
3348</a>{.reference
.external}).</p>
</li>
<li>
<p>[<code>from_crawler</code>{.docutils .literal .notranslate}]{.pre} support is
added to dupefilters (<a href="https://github.com/scrapy/scrapy/issues/2956">issue
2956</a>{.reference
.external}); this allows to access e.g. settings or a spider from a
dupefilter.</p>
</li>
<li>
<p><a href="index.html#std-signal-item_error">[<code>item_error</code>{.xref .std .std-signal .docutils .literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal} is fired when an error happens in a
pipeline (<a href="https://github.com/scrapy/scrapy/issues/3256">issue
3256</a>{.reference
.external});</p>
</li>
<li>
<p><a href="index.html#std-signal-request_reached_downloader">[<code>request_reached_downloader</code>{.xref .std .std-signal .docutils
.literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal} is fired when Downloader gets a new
Request; this signal can be useful e.g. for custom Schedulers
(<a href="https://github.com/scrapy/scrapy/issues/3393">issue
3393</a>{.reference
.external}).</p>
</li>
<li>
<p>new SitemapSpider <a href="index.html#scrapy.spiders.SitemapSpider.sitemap_filter" title="scrapy.spiders.SitemapSpider.sitemap_filter">[<code>sitemap_filter()</code>{.xref .py .py-meth .docutils
.literal
.notranslate}]{.pre}</a>{.reference
.internal} method which allows to select sitemap entries based on
their attributes in SitemapSpider subclasses (<a href="https://github.com/scrapy/scrapy/issues/3512">issue
3512</a>{.reference
.external}).</p>
</li>
<li>
<p>Lazy loading of Downloader Handlers is now optional; this enables
better initialization error handling in custom Downloader Handlers
(<a href="https://github.com/scrapy/scrapy/issues/3394">issue
3394</a>{.reference
.external}).
:::</p>
</li>
</ul>
<p>::: {#new-filepipeline-and-mediapipeline-features .section}</p>
<h5 id="new-filepipeline-and-mediapipeline-featuresheaderlink"><a class="header" href="#new-filepipeline-and-mediapipeline-featuresheaderlink">New FilePipeline and MediaPipeline features<a href="#new-filepipeline-and-mediapipeline-features" title="Permalink to this heading">¶</a>{.headerlink}</a></h5>
<ul>
<li>
<p>Expose more options for S3FilesStore: <a href="index.html#std-setting-AWS_ENDPOINT_URL">[<code>AWS_ENDPOINT_URL</code>{.xref
.std .std-setting .docutils .literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal}, <a href="index.html#std-setting-AWS_USE_SSL">[<code>AWS_USE_SSL</code>{.xref .std
.std-setting .docutils .literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal}, <a href="index.html#std-setting-AWS_VERIFY">[<code>AWS_VERIFY</code>{.xref .std
.std-setting .docutils .literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal}, <a href="index.html#std-setting-AWS_REGION_NAME">[<code>AWS_REGION_NAME</code>{.xref .std
.std-setting .docutils .literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal}. For example, this allows to use
alternative or self-hosted AWS-compatible providers (<a href="https://github.com/scrapy/scrapy/issues/2609">issue
2609</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/3548">issue
3548</a>{.reference
.external}).</p>
</li>
<li>
<p>ACL support for Google Cloud Storage: <a href="index.html#std-setting-FILES_STORE_GCS_ACL">[<code>FILES_STORE_GCS_ACL</code>{.xref
.std .std-setting .docutils .literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal} and <a href="index.html#std-setting-IMAGES_STORE_GCS_ACL">[<code>IMAGES_STORE_GCS_ACL</code>{.xref
.std .std-setting .docutils .literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal} (<a href="https://github.com/scrapy/scrapy/issues/3199">issue
3199</a>{.reference
.external}).
:::</p>
</li>
</ul>
<p>::: {#scrapy-contracts-improvements .section}</p>
<h5 id="scrapycontractsdocutils-literal-notranslatepre-improvementsheaderlink"><a class="header" href="#scrapycontractsdocutils-literal-notranslatepre-improvementsheaderlink">[<code>scrapy.contracts</code>{.docutils .literal .notranslate}]{.pre} improvements<a href="#scrapy-contracts-improvements" title="Permalink to this heading">¶</a>{.headerlink}</a></h5>
<ul>
<li>
<p>Exceptions in contracts code are handled better (<a href="https://github.com/scrapy/scrapy/issues/3377">issue
3377</a>{.reference
.external});</p>
</li>
<li>
<p>[<code>dont_filter=True</code>{.docutils .literal .notranslate}]{.pre} is used
for contract requests, which allows to test different callbacks with
the same URL (<a href="https://github.com/scrapy/scrapy/issues/3381">issue
3381</a>{.reference
.external});</p>
</li>
<li>
<p>[<code>request_cls</code>{.docutils .literal .notranslate}]{.pre} attribute in
Contract subclasses allow to use different Request classes in
contracts, for example FormRequest (<a href="https://github.com/scrapy/scrapy/issues/3383">issue
3383</a>{.reference
.external}).</p>
</li>
<li>
<p>Fixed errback handling in contracts, e.g. for cases where a contract
is executed for URL which returns non-200 response (<a href="https://github.com/scrapy/scrapy/issues/3371">issue
3371</a>{.reference
.external}).
:::</p>
</li>
</ul>
<p>::: {#usability-improvements .section}</p>
<h5 id="usability-improvementsheaderlink"><a class="header" href="#usability-improvementsheaderlink">Usability improvements<a href="#usability-improvements" title="Permalink to this heading">¶</a>{.headerlink}</a></h5>
<ul>
<li>
<p>more stats for RobotsTxtMiddleware (<a href="https://github.com/scrapy/scrapy/issues/3100">issue
3100</a>{.reference
.external})</p>
</li>
<li>
<p>INFO log level is used to show telnet host/port (<a href="https://github.com/scrapy/scrapy/issues/3115">issue
3115</a>{.reference
.external})</p>
</li>
<li>
<p>a message is added to IgnoreRequest in RobotsTxtMiddleware (<a href="https://github.com/scrapy/scrapy/issues/3113">issue
3113</a>{.reference
.external})</p>
</li>
<li>
<p>better validation of [<code>url</code>{.docutils .literal .notranslate}]{.pre}
argument in [<code>Response.follow</code>{.docutils .literal
.notranslate}]{.pre} (<a href="https://github.com/scrapy/scrapy/issues/3131">issue
3131</a>{.reference
.external})</p>
</li>
<li>
<p>non-zero exit code is returned from Scrapy commands when error
happens on spider initialization (<a href="https://github.com/scrapy/scrapy/issues/3226">issue
3226</a>{.reference
.external})</p>
</li>
<li>
<p>Link extraction improvements: &quot;ftp&quot; is added to scheme list (<a href="https://github.com/scrapy/scrapy/issues/3152">issue
3152</a>{.reference
.external}); &quot;flv&quot; is added to common video extensions (<a href="https://github.com/scrapy/scrapy/issues/3165">issue
3165</a>{.reference
.external})</p>
</li>
<li>
<p>better error message when an exporter is disabled (<a href="https://github.com/scrapy/scrapy/issues/3358">issue
3358</a>{.reference
.external});</p>
</li>
<li>
<p>[<code>scrapy</code>{.docutils .literal .notranslate}]{.pre}<code> </code>{.docutils
.literal .notranslate}[<code>shell</code>{.docutils .literal
.notranslate}]{.pre}<code> </code>{.docutils .literal
.notranslate}[<code>--help</code>{.docutils .literal .notranslate}]{.pre}
mentions syntax required for local files ([<code>./file.html</code>{.docutils
.literal .notranslate}]{.pre}) - <a href="https://github.com/scrapy/scrapy/issues/3496">issue
3496</a>{.reference
.external}.</p>
</li>
<li>
<p>Referer header value is added to RFPDupeFilter log messages (<a href="https://github.com/scrapy/scrapy/issues/3588">issue
3588</a>{.reference
.external})
:::</p>
</li>
</ul>
<p>::: {#id99 .section}</p>
<h5 id="bug-fixesheaderlink-2"><a class="header" href="#bug-fixesheaderlink-2">Bug fixes<a href="#id99" title="Permalink to this heading">¶</a>{.headerlink}</a></h5>
<ul>
<li>
<p>fixed issue with extra blank lines in .csv exports under Windows
(<a href="https://github.com/scrapy/scrapy/issues/3039">issue
3039</a>{.reference
.external});</p>
</li>
<li>
<p>proper handling of pickling errors in Python 3 when serializing
objects for disk queues (<a href="https://github.com/scrapy/scrapy/issues/3082">issue
3082</a>{.reference
.external})</p>
</li>
<li>
<p>flags are now preserved when copying Requests (<a href="https://github.com/scrapy/scrapy/issues/3342">issue
3342</a>{.reference
.external});</p>
</li>
<li>
<p>FormRequest.from_response clickdata shouldn't ignore elements with
[<code>input[type=image]</code>{.docutils .literal .notranslate}]{.pre} (<a href="https://github.com/scrapy/scrapy/issues/3153">issue
3153</a>{.reference
.external}).</p>
</li>
<li>
<p>FormRequest.from_response should preserve duplicate keys (<a href="https://github.com/scrapy/scrapy/issues/3247">issue
3247</a>{.reference
.external})
:::</p>
</li>
</ul>
<p>::: {#documentation-improvements .section}</p>
<h5 id="documentation-improvementsheaderlink"><a class="header" href="#documentation-improvementsheaderlink">Documentation improvements<a href="#documentation-improvements" title="Permalink to this heading">¶</a>{.headerlink}</a></h5>
<ul>
<li>
<p>Docs are re-written to suggest .get/.getall API instead of
.extract/.extract_first. Also, <a href="index.html#topics-selectors">[Selectors]{.std
.std-ref}</a>{.hoverxref .tooltip
.reference .internal} docs are updated and re-structured to match
latest parsel docs; they now contain more topics, such as
<a href="index.html#selecting-attributes">[Selecting element attributes]{.std
.std-ref}</a>{.hoverxref .tooltip
.reference .internal} or <a href="index.html#topics-selectors-css-extensions">[Extensions to CSS Selectors]{.std
.std-ref}</a>{.hoverxref
.tooltip .reference .internal} (<a href="https://github.com/scrapy/scrapy/issues/3390">issue
3390</a>{.reference
.external}).</p>
</li>
<li>
<p><a href="index.html#topics-developer-tools">[Using your browser's Developer Tools for scraping]{.std
.std-ref}</a>{.hoverxref .tooltip
.reference .internal} is a new tutorial which replaces old Firefox
and Firebug tutorials (<a href="https://github.com/scrapy/scrapy/issues/3400">issue
3400</a>{.reference
.external}).</p>
</li>
<li>
<p>SCRAPY_PROJECT environment variable is documented (<a href="https://github.com/scrapy/scrapy/issues/3518">issue
3518</a>{.reference
.external});</p>
</li>
<li>
<p>troubleshooting section is added to install instructions (<a href="https://github.com/scrapy/scrapy/issues/3517">issue
3517</a>{.reference
.external});</p>
</li>
<li>
<p>improved links to beginner resources in the tutorial (<a href="https://github.com/scrapy/scrapy/issues/3367">issue
3367</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/3468">issue
3468</a>{.reference
.external});</p>
</li>
<li>
<p>fixed <a href="index.html#std-setting-RETRY_HTTP_CODES">[<code>RETRY_HTTP_CODES</code>{.xref .std .std-setting .docutils
.literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal} default values in docs (<a href="https://github.com/scrapy/scrapy/issues/3335">issue
3335</a>{.reference
.external});</p>
</li>
<li>
<p>remove unused [<code>DEPTH_STATS</code>{.docutils .literal .notranslate}]{.pre}
option from docs (<a href="https://github.com/scrapy/scrapy/issues/3245">issue
3245</a>{.reference
.external});</p>
</li>
<li>
<p>other cleanups (<a href="https://github.com/scrapy/scrapy/issues/3347">issue
3347</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/3350">issue
3350</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/3445">issue
3445</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/3544">issue
3544</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/3605">issue
3605</a>{.reference
.external}).
:::</p>
</li>
</ul>
<p>::: {#id100 .section}</p>
<h5 id="deprecation-removalsheaderlink-2"><a class="header" href="#deprecation-removalsheaderlink-2">Deprecation removals<a href="#id100" title="Permalink to this heading">¶</a>{.headerlink}</a></h5>
<p>Compatibility shims for pre-1.0 Scrapy module names are removed (<a href="https://github.com/scrapy/scrapy/issues/3318">issue
3318</a>{.reference
.external}):</p>
<ul>
<li>
<p>[<code>scrapy.command</code>{.docutils .literal .notranslate}]{.pre}</p>
</li>
<li>
<p>[<code>scrapy.contrib</code>{.docutils .literal .notranslate}]{.pre} (with all
submodules)</p>
</li>
<li>
<p>[<code>scrapy.contrib_exp</code>{.docutils .literal .notranslate}]{.pre} (with
all submodules)</p>
</li>
<li>
<p>[<code>scrapy.dupefilter</code>{.docutils .literal .notranslate}]{.pre}</p>
</li>
<li>
<p>[<code>scrapy.linkextractor</code>{.docutils .literal .notranslate}]{.pre}</p>
</li>
<li>
<p>[<code>scrapy.project</code>{.docutils .literal .notranslate}]{.pre}</p>
</li>
<li>
<p>[<code>scrapy.spider</code>{.docutils .literal .notranslate}]{.pre}</p>
</li>
<li>
<p>[<code>scrapy.spidermanager</code>{.docutils .literal .notranslate}]{.pre}</p>
</li>
<li>
<p>[<code>scrapy.squeue</code>{.docutils .literal .notranslate}]{.pre}</p>
</li>
<li>
<p>[<code>scrapy.stats</code>{.docutils .literal .notranslate}]{.pre}</p>
</li>
<li>
<p>[<code>scrapy.statscol</code>{.docutils .literal .notranslate}]{.pre}</p>
</li>
<li>
<p>[<code>scrapy.utils.decorator</code>{.docutils .literal .notranslate}]{.pre}</p>
</li>
</ul>
<p>See <a href="#module-relocations">[Module Relocations]{.std
.std-ref}</a>{.hoverxref .tooltip .reference
.internal} for more information, or use suggestions from Scrapy 1.5.x
deprecation warnings to update your code.</p>
<p>Other deprecation removals:</p>
<ul>
<li>
<p>Deprecated scrapy.interfaces.ISpiderManager is removed; please use
scrapy.interfaces.ISpiderLoader.</p>
</li>
<li>
<p>Deprecated [<code>CrawlerSettings</code>{.docutils .literal
.notranslate}]{.pre} class is removed (<a href="https://github.com/scrapy/scrapy/issues/3327">issue
3327</a>{.reference
.external}).</p>
</li>
<li>
<p>Deprecated [<code>Settings.overrides</code>{.docutils .literal
.notranslate}]{.pre} and [<code>Settings.defaults</code>{.docutils .literal
.notranslate}]{.pre} attributes are removed (<a href="https://github.com/scrapy/scrapy/issues/3327">issue
3327</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/3359">issue
3359</a>{.reference
.external}).
:::</p>
</li>
</ul>
<p>::: {#other-improvements-cleanups .section}</p>
<h5 id="other-improvements-cleanupsheaderlink"><a class="header" href="#other-improvements-cleanupsheaderlink">Other improvements, cleanups<a href="#other-improvements-cleanups" title="Permalink to this heading">¶</a>{.headerlink}</a></h5>
<ul>
<li>
<p>All Scrapy tests now pass on Windows; Scrapy testing suite is
executed in a Windows environment on CI (<a href="https://github.com/scrapy/scrapy/issues/3315">issue
3315</a>{.reference
.external}).</p>
</li>
<li>
<p>Python 3.7 support (<a href="https://github.com/scrapy/scrapy/issues/3326">issue
3326</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/3150">issue
3150</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/3547">issue
3547</a>{.reference
.external}).</p>
</li>
<li>
<p>Testing and CI fixes (<a href="https://github.com/scrapy/scrapy/issues/3526">issue
3526</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/3538">issue
3538</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/3308">issue
3308</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/3311">issue
3311</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/3309">issue
3309</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/3305">issue
3305</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/3210">issue
3210</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/3299">issue
3299</a>{.reference
.external})</p>
</li>
<li>
<p>[<code>scrapy.http.cookies.CookieJar.clear</code>{.docutils .literal
.notranslate}]{.pre} accepts &quot;domain&quot;, &quot;path&quot; and &quot;name&quot; optional
arguments (<a href="https://github.com/scrapy/scrapy/issues/3231">issue
3231</a>{.reference
.external}).</p>
</li>
<li>
<p>additional files are included to sdist (<a href="https://github.com/scrapy/scrapy/issues/3495">issue
3495</a>{.reference
.external});</p>
</li>
<li>
<p>code style fixes (<a href="https://github.com/scrapy/scrapy/issues/3405">issue
3405</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/3304">issue
3304</a>{.reference
.external});</p>
</li>
<li>
<p>unneeded .strip() call is removed (<a href="https://github.com/scrapy/scrapy/issues/3519">issue
3519</a>{.reference
.external});</p>
</li>
<li>
<p>collections.deque is used to store MiddlewareManager methods instead
of a list (<a href="https://github.com/scrapy/scrapy/issues/3476">issue
3476</a>{.reference
.external})
:::
:::</p>
</li>
</ul>
<p>::: {#scrapy-1-5-2-2019-01-22 .section}
[]{#release-1-5-2}</p>
<h4 id="scrapy-152-2019-01-22headerlink"><a class="header" href="#scrapy-152-2019-01-22headerlink">Scrapy 1.5.2 (2019-01-22)<a href="#scrapy-1-5-2-2019-01-22" title="Permalink to this heading">¶</a>{.headerlink}</a></h4>
<ul>
<li>
<p><em>Security bugfix</em>: Telnet console extension can be easily exploited
by rogue websites POSTing content to
<a href="http://localhost:6023">http://localhost:6023</a>{.reference
.external}, we haven't found a way to exploit it from Scrapy, but it
is very easy to trick a browser to do so and elevates the risk for
local development environment.</p>
<p><em>The fix is backward incompatible</em>, it enables telnet user-password
authentication by default with a random generated password. If you
can't upgrade right away, please consider setting
<a href="index.html#std-setting-TELNETCONSOLE_PORT">[<code>TELNETCONSOLE_PORT</code>{.xref .std .std-setting .docutils .literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal} out of its default value.</p>
<p>See <a href="index.html#topics-telnetconsole">[telnet console]{.std
.std-ref}</a>{.hoverxref .tooltip
.reference .internal} documentation for more info</p>
</li>
<li>
<p>Backport CI build failure under GCE environment due to boto import
error.
:::</p>
</li>
</ul>
<p>::: {#scrapy-1-5-1-2018-07-12 .section}
[]{#release-1-5-1}</p>
<h4 id="scrapy-151-2018-07-12headerlink"><a class="header" href="#scrapy-151-2018-07-12headerlink">Scrapy 1.5.1 (2018-07-12)<a href="#scrapy-1-5-1-2018-07-12" title="Permalink to this heading">¶</a>{.headerlink}</a></h4>
<p>This is a maintenance release with important bug fixes, but no new
features:</p>
<ul>
<li>
<p>[<code>O(N^2)</code>{.docutils .literal .notranslate}]{.pre} gzip decompression
issue which affected Python 3 and PyPy is fixed (<a href="https://github.com/scrapy/scrapy/issues/3281">issue
3281</a>{.reference
.external});</p>
</li>
<li>
<p>skipping of TLS validation errors is improved (<a href="https://github.com/scrapy/scrapy/issues/3166">issue
3166</a>{.reference
.external});</p>
</li>
<li>
<p>Ctrl-C handling is fixed in Python 3.5+ (<a href="https://github.com/scrapy/scrapy/issues/3096">issue
3096</a>{.reference
.external});</p>
</li>
<li>
<p>testing fixes (<a href="https://github.com/scrapy/scrapy/issues/3092">issue
3092</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/3263">issue
3263</a>{.reference
.external});</p>
</li>
<li>
<p>documentation improvements (<a href="https://github.com/scrapy/scrapy/issues/3058">issue
3058</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/3059">issue
3059</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/3089">issue
3089</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/3123">issue
3123</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/3127">issue
3127</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/3189">issue
3189</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/3224">issue
3224</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/3280">issue
3280</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/3279">issue
3279</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/3201">issue
3201</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/3260">issue
3260</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/3284">issue
3284</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/3298">issue
3298</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/3294">issue
3294</a>{.reference
.external}).
:::</p>
</li>
</ul>
<p>::: {#scrapy-1-5-0-2017-12-29 .section}
[]{#release-1-5-0}</p>
<h4 id="scrapy-150-2017-12-29headerlink"><a class="header" href="#scrapy-150-2017-12-29headerlink">Scrapy 1.5.0 (2017-12-29)<a href="#scrapy-1-5-0-2017-12-29" title="Permalink to this heading">¶</a>{.headerlink}</a></h4>
<p>This release brings small new features and improvements across the
codebase. Some highlights:</p>
<ul>
<li>
<p>Google Cloud Storage is supported in FilesPipeline and
ImagesPipeline.</p>
</li>
<li>
<p>Crawling with proxy servers becomes more efficient, as connections
to proxies can be reused now.</p>
</li>
<li>
<p>Warnings, exception and logging messages are improved to make
debugging easier.</p>
</li>
<li>
<p>[<code>scrapy</code>{.docutils .literal .notranslate}]{.pre}<code> </code>{.docutils
.literal .notranslate}[<code>parse</code>{.docutils .literal
.notranslate}]{.pre} command now allows to set custom request meta
via [<code>--meta</code>{.docutils .literal .notranslate}]{.pre} argument.</p>
</li>
<li>
<p>Compatibility with Python 3.6, PyPy and PyPy3 is improved; PyPy and
PyPy3 are now supported officially, by running tests on CI.</p>
</li>
<li>
<p>Better default handling of HTTP 308, 522 and 524 status codes.</p>
</li>
<li>
<p>Documentation is improved, as usual.</p>
</li>
</ul>
<p>::: {#id101 .section}</p>
<h5 id="backward-incompatible-changesheaderlink-2"><a class="header" href="#backward-incompatible-changesheaderlink-2">Backward Incompatible Changes<a href="#id101" title="Permalink to this heading">¶</a>{.headerlink}</a></h5>
<ul>
<li>
<p>Scrapy 1.5 drops support for Python 3.3.</p>
</li>
<li>
<p>Default Scrapy User-Agent now uses https link to scrapy.org (<a href="https://github.com/scrapy/scrapy/issues/2983">issue
2983</a>{.reference
.external}). <strong>This is technically backward-incompatible</strong>; override
<a href="index.html#std-setting-USER_AGENT">[<code>USER_AGENT</code>{.xref .std .std-setting .docutils .literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal} if you relied on old value.</p>
</li>
<li>
<p>Logging of settings overridden by [<code>custom_settings</code>{.docutils
.literal .notranslate}]{.pre} is fixed; <strong>this is technically
backward-incompatible</strong> because the logger changes from
[<code>[scrapy.utils.log]</code>{.docutils .literal .notranslate}]{.pre} to
[<code>[scrapy.crawler]</code>{.docutils .literal .notranslate}]{.pre}. If
you're parsing Scrapy logs, please update your log parsers (<a href="https://github.com/scrapy/scrapy/issues/1343">issue
1343</a>{.reference
.external}).</p>
</li>
<li>
<p>LinkExtractor now ignores [<code>m4v</code>{.docutils .literal
.notranslate}]{.pre} extension by default, this is change in
behavior.</p>
</li>
<li>
<p>522 and 524 status codes are added to [<code>RETRY_HTTP_CODES</code>{.docutils
.literal .notranslate}]{.pre} (<a href="https://github.com/scrapy/scrapy/issues/2851">issue
2851</a>{.reference
.external})
:::</p>
</li>
</ul>
<p>::: {#id102 .section}</p>
<h5 id="new-featuresheaderlink-2"><a class="header" href="#new-featuresheaderlink-2">New features<a href="#id102" title="Permalink to this heading">¶</a>{.headerlink}</a></h5>
<ul>
<li>
<p>Support [<code>&lt;link&gt;</code>{.docutils .literal .notranslate}]{.pre} tags in
[<code>Response.follow</code>{.docutils .literal .notranslate}]{.pre} (<a href="https://github.com/scrapy/scrapy/issues/2785">issue
2785</a>{.reference
.external})</p>
</li>
<li>
<p>Support for [<code>ptpython</code>{.docutils .literal .notranslate}]{.pre} REPL
(<a href="https://github.com/scrapy/scrapy/issues/2654">issue
2654</a>{.reference
.external})</p>
</li>
<li>
<p>Google Cloud Storage support for FilesPipeline and ImagesPipeline
(<a href="https://github.com/scrapy/scrapy/issues/2923">issue
2923</a>{.reference
.external}).</p>
</li>
<li>
<p>New [<code>--meta</code>{.docutils .literal .notranslate}]{.pre} option of the
&quot;scrapy parse&quot; command allows to pass additional request.meta
(<a href="https://github.com/scrapy/scrapy/issues/2883">issue
2883</a>{.reference
.external})</p>
</li>
<li>
<p>Populate spider variable when using
[<code>shell.inspect_response</code>{.docutils .literal .notranslate}]{.pre}
(<a href="https://github.com/scrapy/scrapy/issues/2812">issue
2812</a>{.reference
.external})</p>
</li>
<li>
<p>Handle HTTP 308 Permanent Redirect (<a href="https://github.com/scrapy/scrapy/issues/2844">issue
2844</a>{.reference
.external})</p>
</li>
<li>
<p>Add 522 and 524 to [<code>RETRY_HTTP_CODES</code>{.docutils .literal
.notranslate}]{.pre} (<a href="https://github.com/scrapy/scrapy/issues/2851">issue
2851</a>{.reference
.external})</p>
</li>
<li>
<p>Log versions information at startup (<a href="https://github.com/scrapy/scrapy/issues/2857">issue
2857</a>{.reference
.external})</p>
</li>
<li>
<p>[<code>scrapy.mail.MailSender</code>{.docutils .literal .notranslate}]{.pre}
now works in Python 3 (it requires Twisted 17.9.0)</p>
</li>
<li>
<p>Connections to proxy servers are reused (<a href="https://github.com/scrapy/scrapy/issues/2743">issue
2743</a>{.reference
.external})</p>
</li>
<li>
<p>Add template for a downloader middleware (<a href="https://github.com/scrapy/scrapy/issues/2755">issue
2755</a>{.reference
.external})</p>
</li>
<li>
<p>Explicit message for NotImplementedError when parse callback not
defined (<a href="https://github.com/scrapy/scrapy/issues/2831">issue
2831</a>{.reference
.external})</p>
</li>
<li>
<p>CrawlerProcess got an option to disable installation of root log
handler (<a href="https://github.com/scrapy/scrapy/issues/2921">issue
2921</a>{.reference
.external})</p>
</li>
<li>
<p>LinkExtractor now ignores [<code>m4v</code>{.docutils .literal
.notranslate}]{.pre} extension by default</p>
</li>
<li>
<p>Better log messages for responses over <a href="index.html#std-setting-DOWNLOAD_WARNSIZE">[<code>DOWNLOAD_WARNSIZE</code>{.xref
.std .std-setting .docutils .literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal} and <a href="index.html#std-setting-DOWNLOAD_MAXSIZE">[<code>DOWNLOAD_MAXSIZE</code>{.xref .std
.std-setting .docutils .literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal} limits (<a href="https://github.com/scrapy/scrapy/issues/2927">issue
2927</a>{.reference
.external})</p>
</li>
<li>
<p>Show warning when a URL is put to
[<code>Spider.allowed_domains</code>{.docutils .literal .notranslate}]{.pre}
instead of a domain (<a href="https://github.com/scrapy/scrapy/issues/2250">issue
2250</a>{.reference
.external}).
:::</p>
</li>
</ul>
<p>::: {#id103 .section}</p>
<h5 id="bug-fixesheaderlink-3"><a class="header" href="#bug-fixesheaderlink-3">Bug fixes<a href="#id103" title="Permalink to this heading">¶</a>{.headerlink}</a></h5>
<ul>
<li>
<p>Fix logging of settings overridden by [<code>custom_settings</code>{.docutils
.literal .notranslate}]{.pre}; <strong>this is technically
backward-incompatible</strong> because the logger changes from
[<code>[scrapy.utils.log]</code>{.docutils .literal .notranslate}]{.pre} to
[<code>[scrapy.crawler]</code>{.docutils .literal .notranslate}]{.pre}, so
please update your log parsers if needed (<a href="https://github.com/scrapy/scrapy/issues/1343">issue
1343</a>{.reference
.external})</p>
</li>
<li>
<p>Default Scrapy User-Agent now uses https link to scrapy.org (<a href="https://github.com/scrapy/scrapy/issues/2983">issue
2983</a>{.reference
.external}). <strong>This is technically backward-incompatible</strong>; override
<a href="index.html#std-setting-USER_AGENT">[<code>USER_AGENT</code>{.xref .std .std-setting .docutils .literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal} if you relied on old value.</p>
</li>
<li>
<p>Fix PyPy and PyPy3 test failures, support them officially (<a href="https://github.com/scrapy/scrapy/issues/2793">issue
2793</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/2935">issue
2935</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/2990">issue
2990</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/3050">issue
3050</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/2213">issue
2213</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/3048">issue
3048</a>{.reference
.external})</p>
</li>
<li>
<p>Fix DNS resolver when [<code>DNSCACHE_ENABLED=False</code>{.docutils .literal
.notranslate}]{.pre} (<a href="https://github.com/scrapy/scrapy/issues/2811">issue
2811</a>{.reference
.external})</p>
</li>
<li>
<p>Add [<code>cryptography</code>{.docutils .literal .notranslate}]{.pre} for
Debian Jessie tox test env (<a href="https://github.com/scrapy/scrapy/issues/2848">issue
2848</a>{.reference
.external})</p>
</li>
<li>
<p>Add verification to check if Request callback is callable (<a href="https://github.com/scrapy/scrapy/issues/2766">issue
2766</a>{.reference
.external})</p>
</li>
<li>
<p>Port [<code>extras/qpsclient.py</code>{.docutils .literal .notranslate}]{.pre}
to Python 3 (<a href="https://github.com/scrapy/scrapy/issues/2849">issue
2849</a>{.reference
.external})</p>
</li>
<li>
<p>Use getfullargspec under the scenes for Python 3 to stop
DeprecationWarning (<a href="https://github.com/scrapy/scrapy/issues/2862">issue
2862</a>{.reference
.external})</p>
</li>
<li>
<p>Update deprecated test aliases (<a href="https://github.com/scrapy/scrapy/issues/2876">issue
2876</a>{.reference
.external})</p>
</li>
<li>
<p>Fix [<code>SitemapSpider</code>{.docutils .literal .notranslate}]{.pre} support
for alternate links (<a href="https://github.com/scrapy/scrapy/issues/2853">issue
2853</a>{.reference
.external})
:::</p>
</li>
</ul>
<p>::: {#docs .section}</p>
<h5 id="docsheaderlink"><a class="header" href="#docsheaderlink">Docs<a href="#docs" title="Permalink to this heading">¶</a>{.headerlink}</a></h5>
<ul>
<li>
<p>Added missing bullet point for the
[<code>AUTOTHROTTLE_TARGET_CONCURRENCY</code>{.docutils .literal
.notranslate}]{.pre} setting. (<a href="https://github.com/scrapy/scrapy/issues/2756">issue
2756</a>{.reference
.external})</p>
</li>
<li>
<p>Update Contributing docs, document new support channels (<a href="https://github.com/scrapy/scrapy/issues/2762">issue
2762</a>{.reference
.external}, issue:3038)</p>
</li>
<li>
<p>Include references to Scrapy subreddit in the docs</p>
</li>
<li>
<p>Fix broken links; use <a href="https://">https://</a>{.reference .external} for
external links (<a href="https://github.com/scrapy/scrapy/issues/2978">issue
2978</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/2982">issue
2982</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/2958">issue
2958</a>{.reference
.external})</p>
</li>
<li>
<p>Document CloseSpider extension better (<a href="https://github.com/scrapy/scrapy/issues/2759">issue
2759</a>{.reference
.external})</p>
</li>
<li>
<p>Use [<code>pymongo.collection.Collection.insert_one()</code>{.docutils .literal
.notranslate}]{.pre} in MongoDB example (<a href="https://github.com/scrapy/scrapy/issues/2781">issue
2781</a>{.reference
.external})</p>
</li>
<li>
<p>Spelling mistake and typos (<a href="https://github.com/scrapy/scrapy/issues/2828">issue
2828</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/2837">issue
2837</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/2884">issue
2884</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/2924">issue
2924</a>{.reference
.external})</p>
</li>
<li>
<p>Clarify [<code>CSVFeedSpider.headers</code>{.docutils .literal
.notranslate}]{.pre} documentation (<a href="https://github.com/scrapy/scrapy/issues/2826">issue
2826</a>{.reference
.external})</p>
</li>
<li>
<p>Document [<code>DontCloseSpider</code>{.docutils .literal .notranslate}]{.pre}
exception and clarify [<code>spider_idle</code>{.docutils .literal
.notranslate}]{.pre} (<a href="https://github.com/scrapy/scrapy/issues/2791">issue
2791</a>{.reference
.external})</p>
</li>
<li>
<p>Update &quot;Releases&quot; section in README (<a href="https://github.com/scrapy/scrapy/issues/2764">issue
2764</a>{.reference
.external})</p>
</li>
<li>
<p>Fix rst syntax in [<code>DOWNLOAD_FAIL_ON_DATALOSS</code>{.docutils .literal
.notranslate}]{.pre} docs (<a href="https://github.com/scrapy/scrapy/issues/2763">issue
2763</a>{.reference
.external})</p>
</li>
<li>
<p>Small fix in description of startproject arguments (<a href="https://github.com/scrapy/scrapy/issues/2866">issue
2866</a>{.reference
.external})</p>
</li>
<li>
<p>Clarify data types in Response.body docs (<a href="https://github.com/scrapy/scrapy/issues/2922">issue
2922</a>{.reference
.external})</p>
</li>
<li>
<p>Add a note about [<code>request.meta['depth']</code>{.docutils .literal
.notranslate}]{.pre} to DepthMiddleware docs (<a href="https://github.com/scrapy/scrapy/issues/2374">issue
2374</a>{.reference
.external})</p>
</li>
<li>
<p>Add a note about [<code>request.meta['dont_merge_cookies']</code>{.docutils
.literal .notranslate}]{.pre} to CookiesMiddleware docs (<a href="https://github.com/scrapy/scrapy/issues/2999">issue
2999</a>{.reference
.external})</p>
</li>
<li>
<p>Up-to-date example of project structure (<a href="https://github.com/scrapy/scrapy/issues/2964">issue
2964</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/2976">issue
2976</a>{.reference
.external})</p>
</li>
<li>
<p>A better example of ItemExporters usage (<a href="https://github.com/scrapy/scrapy/issues/2989">issue
2989</a>{.reference
.external})</p>
</li>
<li>
<p>Document [<code>from_crawler</code>{.docutils .literal .notranslate}]{.pre}
methods for spider and downloader middlewares (<a href="https://github.com/scrapy/scrapy/issues/3019">issue
3019</a>{.reference
.external})
:::
:::</p>
</li>
</ul>
<p>::: {#scrapy-1-4-0-2017-05-18 .section}
[]{#release-1-4-0}</p>
<h4 id="scrapy-140-2017-05-18headerlink"><a class="header" href="#scrapy-140-2017-05-18headerlink">Scrapy 1.4.0 (2017-05-18)<a href="#scrapy-1-4-0-2017-05-18" title="Permalink to this heading">¶</a>{.headerlink}</a></h4>
<p>Scrapy 1.4 does not bring that many breathtaking new features but quite
a few handy improvements nonetheless.</p>
<p>Scrapy now supports anonymous FTP sessions with customizable user and
password via the new <a href="index.html#std-setting-FTP_USER">[<code>FTP_USER</code>{.xref .std .std-setting .docutils
.literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal} and <a href="index.html#std-setting-FTP_PASSWORD">[<code>FTP_PASSWORD</code>{.xref .std
.std-setting .docutils .literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal} settings. And if you're using Twisted
version 17.1.0 or above, FTP is now available with Python 3.</p>
<p>There's a new <a href="index.html#scrapy.http.TextResponse.follow" title="scrapy.http.TextResponse.follow">[<code>response.follow</code>{.xref .py .py-meth .docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal} method for creating requests; <strong>it is now a recommended way
to create Requests in Scrapy spiders</strong>. This method makes it easier to
write correct spiders; [<code>response.follow</code>{.docutils .literal
.notranslate}]{.pre} has several advantages over creating
[<code>scrapy.Request</code>{.docutils .literal .notranslate}]{.pre} objects
directly:</p>
<ul>
<li>
<p>it handles relative URLs;</p>
</li>
<li>
<p>it works properly with non-ascii URLs on non-UTF8 pages;</p>
</li>
<li>
<p>in addition to absolute and relative URLs it supports Selectors; for
[<code>&lt;a&gt;</code>{.docutils .literal .notranslate}]{.pre} elements it can also
extract their href values.</p>
</li>
</ul>
<p>For example, instead of this:</p>
<p>::: {.highlight-default .notranslate}
::: highlight
for href in response.css('li.page a::attr(href)').extract():
url = response.urljoin(href)
yield scrapy.Request(url, self.parse, encoding=response.encoding)
:::
:::</p>
<p>One can now write this:</p>
<p>::: {.highlight-default .notranslate}
::: highlight
for a in response.css('li.page a'):
yield response.follow(a, self.parse)
:::
:::</p>
<p>Link extractors are also improved. They work similarly to what a regular
modern browser would do: leading and trailing whitespace are removed
from attributes (think [<code>href=&quot;</code>{.docutils .literal
.notranslate}]{.pre}<code>   </code>{.docutils .literal
.notranslate}[<code>http://example.com&quot;</code>{.docutils .literal
.notranslate}]{.pre}) when building [<code>Link</code>{.docutils .literal
.notranslate}]{.pre} objects. This whitespace-stripping also happens for
[<code>action</code>{.docutils .literal .notranslate}]{.pre} attributes with
[<code>FormRequest</code>{.docutils .literal .notranslate}]{.pre}.</p>
<p><strong>Please also note that link extractors do not canonicalize URLs by
default anymore.</strong> This was puzzling users every now and then, and it's
not what browsers do in fact, so we removed that extra transformation on
extracted links.</p>
<p>For those of you wanting more control on the [<code>Referer:</code>{.docutils
.literal .notranslate}]{.pre} header that Scrapy sends when following
links, you can set your own [<code>Referrer</code>{.docutils .literal
.notranslate}]{.pre}<code> </code>{.docutils .literal
.notranslate}[<code>Policy</code>{.docutils .literal .notranslate}]{.pre}. Prior to
Scrapy 1.4, the default [<code>RefererMiddleware</code>{.docutils .literal
.notranslate}]{.pre} would simply and blindly set it to the URL of the
response that generated the HTTP request (which could leak information
on your URL seeds). By default, Scrapy now behaves much like your
regular browser does. And this policy is fully customizable with W3C
standard values (or with something really custom of your own if you
wish). See <a href="index.html#std-setting-REFERRER_POLICY">[<code>REFERRER_POLICY</code>{.xref .std .std-setting .docutils
.literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal} for details.</p>
<p>To make Scrapy spiders easier to debug, Scrapy logs more stats by
default in 1.4: memory usage stats, detailed retry stats, detailed HTTP
error code stats. A similar change is that HTTP cache path is also
visible in logs now.</p>
<p>Last but not least, Scrapy now has the option to make JSON and XML items
more human-readable, with newlines between items and even custom
indenting offset, using the new <a href="index.html#std-setting-FEED_EXPORT_INDENT">[<code>FEED_EXPORT_INDENT</code>{.xref .std
.std-setting .docutils .literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal} setting.</p>
<p>Enjoy! (Or read on for the rest of changes in this release.)</p>
<p>::: {#deprecations-and-backward-incompatible-changes .section}</p>
<h5 id="deprecations-and-backward-incompatible-changesheaderlink"><a class="header" href="#deprecations-and-backward-incompatible-changesheaderlink">Deprecations and Backward Incompatible Changes<a href="#deprecations-and-backward-incompatible-changes" title="Permalink to this heading">¶</a>{.headerlink}</a></h5>
<ul>
<li>
<p>Default to [<code>canonicalize=False</code>{.docutils .literal
.notranslate}]{.pre} in
<a href="index.html#scrapy.linkextractors.lxmlhtml.LxmlLinkExtractor" title="scrapy.linkextractors.lxmlhtml.LxmlLinkExtractor">[<code>scrapy.linkextractors.LinkExtractor</code>{.xref .py .py-class
.docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal} (<a href="https://github.com/scrapy/scrapy/issues/2537">issue
2537</a>{.reference
.external}, fixes <a href="https://github.com/scrapy/scrapy/issues/1941">issue
1941</a>{.reference
.external} and <a href="https://github.com/scrapy/scrapy/issues/1982">issue
1982</a>{.reference
.external}): <strong>warning, this is technically backward-incompatible</strong></p>
</li>
<li>
<p>Enable memusage extension by default (<a href="https://github.com/scrapy/scrapy/issues/2539">issue
2539</a>{.reference
.external}, fixes <a href="https://github.com/scrapy/scrapy/issues/2187">issue
2187</a>{.reference
.external}); <strong>this is technically backward-incompatible</strong> so please
check if you have any non-default [<code>MEMUSAGE_***</code>{.docutils .literal
.notranslate}]{.pre} options set.</p>
</li>
<li>
<p>[<code>EDITOR</code>{.docutils .literal .notranslate}]{.pre} environment
variable now takes precedence over [<code>EDITOR</code>{.docutils .literal
.notranslate}]{.pre} option defined in settings.py (<a href="https://github.com/scrapy/scrapy/issues/1829">issue
1829</a>{.reference
.external}); Scrapy default settings no longer depend on environment
variables. <strong>This is technically a backward incompatible change</strong>.</p>
</li>
<li>
<p>[<code>Spider.make_requests_from_url</code>{.docutils .literal
.notranslate}]{.pre} is deprecated (<a href="https://github.com/scrapy/scrapy/issues/1728">issue
1728</a>{.reference
.external}, fixes <a href="https://github.com/scrapy/scrapy/issues/1495">issue
1495</a>{.reference
.external}).
:::</p>
</li>
</ul>
<p>::: {#id104 .section}</p>
<h5 id="new-featuresheaderlink-3"><a class="header" href="#new-featuresheaderlink-3">New Features<a href="#id104" title="Permalink to this heading">¶</a>{.headerlink}</a></h5>
<ul>
<li>
<p>Accept proxy credentials in <a href="index.html#std-reqmeta-proxy">[<code>proxy</code>{.xref .std .std-reqmeta
.docutils .literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal} request meta key (<a href="https://github.com/scrapy/scrapy/issues/2526">issue
2526</a>{.reference
.external})</p>
</li>
<li>
<p>Support
<a href="https://www.ietf.org/rfc/rfc7932.txt">brotli-compressed</a>{.reference
.external} content; requires optional
<a href="https://github.com/python-hyper/brotlipy/">brotlipy</a>{.reference
.external} (<a href="https://github.com/scrapy/scrapy/issues/2535">issue
2535</a>{.reference
.external})</p>
</li>
<li>
<p>New <a href="index.html#response-follow-example">[response.follow]{.std
.std-ref}</a>{.hoverxref .tooltip
.reference .internal} shortcut for creating requests (<a href="https://github.com/scrapy/scrapy/issues/1940">issue
1940</a>{.reference
.external})</p>
</li>
<li>
<p>Added [<code>flags</code>{.docutils .literal .notranslate}]{.pre} argument and
attribute to <a href="index.html#scrapy.http.Request" title="scrapy.http.Request">[<code>Request</code>{.xref .py .py-class .docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal} objects (<a href="https://github.com/scrapy/scrapy/issues/2047">issue
2047</a>{.reference
.external})</p>
</li>
<li>
<p>Support Anonymous FTP (<a href="https://github.com/scrapy/scrapy/issues/2342">issue
2342</a>{.reference
.external})</p>
</li>
<li>
<p>Added [<code>retry/count</code>{.docutils .literal .notranslate}]{.pre},
[<code>retry/max_reached</code>{.docutils .literal .notranslate}]{.pre} and
[<code>retry/reason_count/&lt;reason&gt;</code>{.docutils .literal
.notranslate}]{.pre} stats to <a href="index.html#scrapy.downloadermiddlewares.retry.RetryMiddleware" title="scrapy.downloadermiddlewares.retry.RetryMiddleware">[<code>RetryMiddleware</code>{.xref .py
.py-class .docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal} (<a href="https://github.com/scrapy/scrapy/issues/2543">issue
2543</a>{.reference
.external})</p>
</li>
<li>
<p>Added [<code>httperror/response_ignored_count</code>{.docutils .literal
.notranslate}]{.pre} and
[<code>httperror/response_ignored_status_count/&lt;status&gt;</code>{.docutils
.literal .notranslate}]{.pre} stats to <a href="index.html#scrapy.spidermiddlewares.httperror.HttpErrorMiddleware" title="scrapy.spidermiddlewares.httperror.HttpErrorMiddleware">[<code>HttpErrorMiddleware</code>{.xref
.py .py-class .docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal} (<a href="https://github.com/scrapy/scrapy/issues/2566">issue
2566</a>{.reference
.external})</p>
</li>
<li>
<p>Customizable <a href="index.html#std-setting-REFERRER_POLICY">[<code>Referrer</code>{.xref .std .std-setting .docutils .literal
.notranslate}]{.pre}<code> </code>{.xref .std .std-setting .docutils .literal
.notranslate}[<code>policy</code>{.xref .std .std-setting .docutils .literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal} in <a href="index.html#scrapy.spidermiddlewares.referer.RefererMiddleware" title="scrapy.spidermiddlewares.referer.RefererMiddleware">[<code>RefererMiddleware</code>{.xref .py
.py-class .docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal} (<a href="https://github.com/scrapy/scrapy/issues/2306">issue
2306</a>{.reference
.external})</p>
</li>
<li>
<p>New [<code>data:</code>{.docutils .literal .notranslate}]{.pre} URI download
handler (<a href="https://github.com/scrapy/scrapy/issues/2334">issue
2334</a>{.reference
.external}, fixes <a href="https://github.com/scrapy/scrapy/issues/2156">issue
2156</a>{.reference
.external})</p>
</li>
<li>
<p>Log cache directory when HTTP Cache is used (<a href="https://github.com/scrapy/scrapy/issues/2611">issue
2611</a>{.reference
.external}, fixes <a href="https://github.com/scrapy/scrapy/issues/2604">issue
2604</a>{.reference
.external})</p>
</li>
<li>
<p>Warn users when project contains duplicate spider names (fixes
<a href="https://github.com/scrapy/scrapy/issues/2181">issue
2181</a>{.reference
.external})</p>
</li>
<li>
<p>[<code>scrapy.utils.datatypes.CaselessDict</code>{.docutils .literal
.notranslate}]{.pre} now accepts [<code>Mapping</code>{.docutils .literal
.notranslate}]{.pre} instances and not only dicts (<a href="https://github.com/scrapy/scrapy/issues/2646">issue
2646</a>{.reference
.external})</p>
</li>
<li>
<p><a href="index.html#topics-media-pipeline">[Media downloads]{.std
.std-ref}</a>{.hoverxref .tooltip
.reference .internal}, with <a href="index.html#scrapy.pipelines.files.FilesPipeline" title="scrapy.pipelines.files.FilesPipeline">[<code>FilesPipeline</code>{.xref .py .py-class
.docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal} or <a href="index.html#scrapy.pipelines.images.ImagesPipeline" title="scrapy.pipelines.images.ImagesPipeline">[<code>ImagesPipeline</code>{.xref .py .py-class .docutils
.literal
.notranslate}]{.pre}</a>{.reference
.internal}, can now optionally handle HTTP redirects using the new
<a href="index.html#std-setting-MEDIA_ALLOW_REDIRECTS">[<code>MEDIA_ALLOW_REDIRECTS</code>{.xref .std .std-setting .docutils .literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal} setting (<a href="https://github.com/scrapy/scrapy/issues/2616">issue
2616</a>{.reference
.external}, fixes <a href="https://github.com/scrapy/scrapy/issues/2004">issue
2004</a>{.reference
.external})</p>
</li>
<li>
<p>Accept non-complete responses from websites using a new
<a href="index.html#std-setting-DOWNLOAD_FAIL_ON_DATALOSS">[<code>DOWNLOAD_FAIL_ON_DATALOSS</code>{.xref .std .std-setting .docutils
.literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal} setting (<a href="https://github.com/scrapy/scrapy/issues/2590">issue
2590</a>{.reference
.external}, fixes <a href="https://github.com/scrapy/scrapy/issues/2586">issue
2586</a>{.reference
.external})</p>
</li>
<li>
<p>Optional pretty-printing of JSON and XML items via
<a href="index.html#std-setting-FEED_EXPORT_INDENT">[<code>FEED_EXPORT_INDENT</code>{.xref .std .std-setting .docutils .literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal} setting (<a href="https://github.com/scrapy/scrapy/issues/2456">issue
2456</a>{.reference
.external}, fixes <a href="https://github.com/scrapy/scrapy/issues/1327">issue
1327</a>{.reference
.external})</p>
</li>
<li>
<p>Allow dropping fields in [<code>FormRequest.from_response</code>{.docutils
.literal .notranslate}]{.pre} formdata when [<code>None</code>{.docutils
.literal .notranslate}]{.pre} value is passed (<a href="https://github.com/scrapy/scrapy/issues/667">issue
667</a>{.reference
.external})</p>
</li>
<li>
<p>Per-request retry times with the new <a href="index.html#std-reqmeta-max_retry_times">[<code>max_retry_times</code>{.xref .std
.std-reqmeta .docutils .literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal} meta key (<a href="https://github.com/scrapy/scrapy/issues/2642">issue
2642</a>{.reference
.external})</p>
</li>
<li>
<p>[<code>python</code>{.docutils .literal .notranslate}]{.pre}<code> </code>{.docutils
.literal .notranslate}[<code>-m</code>{.docutils .literal
.notranslate}]{.pre}<code> </code>{.docutils .literal
.notranslate}[<code>scrapy</code>{.docutils .literal .notranslate}]{.pre} as a
more explicit alternative to [<code>scrapy</code>{.docutils .literal
.notranslate}]{.pre} command (<a href="https://github.com/scrapy/scrapy/issues/2740">issue
2740</a>{.reference
.external})
:::</p>
</li>
</ul>
<p>::: {#id105 .section}</p>
<h5 id="bug-fixesheaderlink-4"><a class="header" href="#bug-fixesheaderlink-4">Bug fixes<a href="#id105" title="Permalink to this heading">¶</a>{.headerlink}</a></h5>
<ul>
<li>
<p>LinkExtractor now strips leading and trailing whitespaces from
attributes (<a href="https://github.com/scrapy/scrapy/issues/2547">issue
2547</a>{.reference
.external}, fixes <a href="https://github.com/scrapy/scrapy/issues/1614">issue
1614</a>{.reference
.external})</p>
</li>
<li>
<p>Properly handle whitespaces in action attribute in
[<code>FormRequest</code>{.xref .py .py-class .docutils .literal
.notranslate}]{.pre} (<a href="https://github.com/scrapy/scrapy/issues/2548">issue
2548</a>{.reference
.external})</p>
</li>
<li>
<p>Buffer CONNECT response bytes from proxy until all HTTP headers are
received (<a href="https://github.com/scrapy/scrapy/issues/2495">issue
2495</a>{.reference
.external}, fixes <a href="https://github.com/scrapy/scrapy/issues/2491">issue
2491</a>{.reference
.external})</p>
</li>
<li>
<p>FTP downloader now works on Python 3, provided you use
Twisted&gt;=17.1 (<a href="https://github.com/scrapy/scrapy/issues/2599">issue
2599</a>{.reference
.external})</p>
</li>
<li>
<p>Use body to choose response type after decompressing content (<a href="https://github.com/scrapy/scrapy/issues/2393">issue
2393</a>{.reference
.external}, fixes <a href="https://github.com/scrapy/scrapy/issues/2145">issue
2145</a>{.reference
.external})</p>
</li>
<li>
<p>Always decompress [<code>Content-Encoding:</code>{.docutils .literal
.notranslate}]{.pre}<code> </code>{.docutils .literal
.notranslate}[<code>gzip</code>{.docutils .literal .notranslate}]{.pre} at
<a href="index.html#scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware" title="scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware">[<code>HttpCompressionMiddleware</code>{.xref .py .py-class .docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal} stage (<a href="https://github.com/scrapy/scrapy/issues/2391">issue
2391</a>{.reference
.external})</p>
</li>
<li>
<p>Respect custom log level in [<code>Spider.custom_settings</code>{.docutils
.literal .notranslate}]{.pre} (<a href="https://github.com/scrapy/scrapy/issues/2581">issue
2581</a>{.reference
.external}, fixes <a href="https://github.com/scrapy/scrapy/issues/1612">issue
1612</a>{.reference
.external})</p>
</li>
<li>
<p>'make htmlview' fix for macOS (<a href="https://github.com/scrapy/scrapy/issues/2661">issue
2661</a>{.reference
.external})</p>
</li>
<li>
<p>Remove &quot;commands&quot; from the command list (<a href="https://github.com/scrapy/scrapy/issues/2695">issue
2695</a>{.reference
.external})</p>
</li>
<li>
<p>Fix duplicate Content-Length header for POST requests with empty
body (<a href="https://github.com/scrapy/scrapy/issues/2677">issue
2677</a>{.reference
.external})</p>
</li>
<li>
<p>Properly cancel large downloads, i.e. above
<a href="index.html#std-setting-DOWNLOAD_MAXSIZE">[<code>DOWNLOAD_MAXSIZE</code>{.xref .std .std-setting .docutils .literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal} (<a href="https://github.com/scrapy/scrapy/issues/1616">issue
1616</a>{.reference
.external})</p>
</li>
<li>
<p>ImagesPipeline: fixed processing of transparent PNG images with
palette (<a href="https://github.com/scrapy/scrapy/issues/2675">issue
2675</a>{.reference
.external})
:::</p>
</li>
</ul>
<p>::: {#cleanups-refactoring .section}</p>
<h5 id="cleanups--refactoringheaderlink"><a class="header" href="#cleanups--refactoringheaderlink">Cleanups &amp; Refactoring<a href="#cleanups-refactoring" title="Permalink to this heading">¶</a>{.headerlink}</a></h5>
<ul>
<li>
<p>Tests: remove temp files and folders (<a href="https://github.com/scrapy/scrapy/issues/2570">issue
2570</a>{.reference
.external}), fixed ProjectUtilsTest on macOS (<a href="https://github.com/scrapy/scrapy/issues/2569">issue
2569</a>{.reference
.external}), use portable pypy for Linux on Travis CI (<a href="https://github.com/scrapy/scrapy/issues/2710">issue
2710</a>{.reference
.external})</p>
</li>
<li>
<p>Separate building request from [<code>_requests_to_follow</code>{.docutils
.literal .notranslate}]{.pre} in CrawlSpider (<a href="https://github.com/scrapy/scrapy/issues/2562">issue
2562</a>{.reference
.external})</p>
</li>
<li>
<p>Remove &quot;Python 3 progress&quot; badge (<a href="https://github.com/scrapy/scrapy/issues/2567">issue
2567</a>{.reference
.external})</p>
</li>
<li>
<p>Add a couple more lines to [<code>.gitignore</code>{.docutils .literal
.notranslate}]{.pre} (<a href="https://github.com/scrapy/scrapy/issues/2557">issue
2557</a>{.reference
.external})</p>
</li>
<li>
<p>Remove bumpversion prerelease configuration (<a href="https://github.com/scrapy/scrapy/issues/2159">issue
2159</a>{.reference
.external})</p>
</li>
<li>
<p>Add codecov.yml file (<a href="https://github.com/scrapy/scrapy/issues/2750">issue
2750</a>{.reference
.external})</p>
</li>
<li>
<p>Set context factory implementation based on Twisted version (<a href="https://github.com/scrapy/scrapy/issues/2577">issue
2577</a>{.reference
.external}, fixes <a href="https://github.com/scrapy/scrapy/issues/2560">issue
2560</a>{.reference
.external})</p>
</li>
<li>
<p>Add omitted [<code>self</code>{.docutils .literal .notranslate}]{.pre}
arguments in default project middleware template (<a href="https://github.com/scrapy/scrapy/issues/2595">issue
2595</a>{.reference
.external})</p>
</li>
<li>
<p>Remove redundant [<code>slot.add_request()</code>{.docutils .literal
.notranslate}]{.pre} call in ExecutionEngine (<a href="https://github.com/scrapy/scrapy/issues/2617">issue
2617</a>{.reference
.external})</p>
</li>
<li>
<p>Catch more specific [<code>os.error</code>{.docutils .literal
.notranslate}]{.pre} exception in
[<code>scrapy.pipelines.files.FSFilesStore</code>{.docutils .literal
.notranslate}]{.pre} (<a href="https://github.com/scrapy/scrapy/issues/2644">issue
2644</a>{.reference
.external})</p>
</li>
<li>
<p>Change &quot;localhost&quot; test server certificate (<a href="https://github.com/scrapy/scrapy/issues/2720">issue
2720</a>{.reference
.external})</p>
</li>
<li>
<p>Remove unused [<code>MEMUSAGE_REPORT</code>{.docutils .literal
.notranslate}]{.pre} setting (<a href="https://github.com/scrapy/scrapy/issues/2576">issue
2576</a>{.reference
.external})
:::</p>
</li>
</ul>
<p>::: {#id106 .section}</p>
<h5 id="documentationheaderlink-2"><a class="header" href="#documentationheaderlink-2">Documentation<a href="#id106" title="Permalink to this heading">¶</a>{.headerlink}</a></h5>
<ul>
<li>
<p>Binary mode is required for exporters (<a href="https://github.com/scrapy/scrapy/issues/2564">issue
2564</a>{.reference
.external}, fixes <a href="https://github.com/scrapy/scrapy/issues/2553">issue
2553</a>{.reference
.external})</p>
</li>
<li>
<p>Mention issue with [<code>FormRequest.from_response</code>{.xref .py .py-meth
.docutils .literal .notranslate}]{.pre} due to bug in lxml (<a href="https://github.com/scrapy/scrapy/issues/2572">issue
2572</a>{.reference
.external})</p>
</li>
<li>
<p>Use single quotes uniformly in templates (<a href="https://github.com/scrapy/scrapy/issues/2596">issue
2596</a>{.reference
.external})</p>
</li>
<li>
<p>Document <a href="index.html#std-reqmeta-ftp_user">[<code>ftp_user</code>{.xref .std .std-reqmeta .docutils .literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal} and <a href="index.html#std-reqmeta-ftp_password">[<code>ftp_password</code>{.xref .std
.std-reqmeta .docutils .literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal} meta keys (<a href="https://github.com/scrapy/scrapy/issues/2587">issue
2587</a>{.reference
.external})</p>
</li>
<li>
<p>Removed section on deprecated [<code>contrib/</code>{.docutils .literal
.notranslate}]{.pre} (<a href="https://github.com/scrapy/scrapy/issues/2636">issue
2636</a>{.reference
.external})</p>
</li>
<li>
<p>Recommend Anaconda when installing Scrapy on Windows (<a href="https://github.com/scrapy/scrapy/issues/2477">issue
2477</a>{.reference
.external}, fixes <a href="https://github.com/scrapy/scrapy/issues/2475">issue
2475</a>{.reference
.external})</p>
</li>
<li>
<p>FAQ: rewrite note on Python 3 support on Windows (<a href="https://github.com/scrapy/scrapy/issues/2690">issue
2690</a>{.reference
.external})</p>
</li>
<li>
<p>Rearrange selector sections (<a href="https://github.com/scrapy/scrapy/issues/2705">issue
2705</a>{.reference
.external})</p>
</li>
<li>
<p>Remove [<code>__nonzero__</code>{.docutils .literal .notranslate}]{.pre} from
<a href="index.html#scrapy.selector.SelectorList" title="scrapy.selector.SelectorList">[<code>SelectorList</code>{.xref .py .py-class .docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal} docs (<a href="https://github.com/scrapy/scrapy/issues/2683">issue
2683</a>{.reference
.external})</p>
</li>
<li>
<p>Mention how to disable request filtering in documentation of
<a href="index.html#std-setting-DUPEFILTER_CLASS">[<code>DUPEFILTER_CLASS</code>{.xref .std .std-setting .docutils .literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal} setting (<a href="https://github.com/scrapy/scrapy/issues/2714">issue
2714</a>{.reference
.external})</p>
</li>
<li>
<p>Add sphinx_rtd_theme to docs setup readme (<a href="https://github.com/scrapy/scrapy/issues/2668">issue
2668</a>{.reference
.external})</p>
</li>
<li>
<p>Open file in text mode in JSON item writer example (<a href="https://github.com/scrapy/scrapy/issues/2729">issue
2729</a>{.reference
.external})</p>
</li>
<li>
<p>Clarify [<code>allowed_domains</code>{.docutils .literal .notranslate}]{.pre}
example (<a href="https://github.com/scrapy/scrapy/issues/2670">issue
2670</a>{.reference
.external})
:::
:::</p>
</li>
</ul>
<p>::: {#scrapy-1-3-3-2017-03-10 .section}
[]{#release-1-3-3}</p>
<h4 id="scrapy-133-2017-03-10headerlink"><a class="header" href="#scrapy-133-2017-03-10headerlink">Scrapy 1.3.3 (2017-03-10)<a href="#scrapy-1-3-3-2017-03-10" title="Permalink to this heading">¶</a>{.headerlink}</a></h4>
<p>::: {#id107 .section}</p>
<h5 id="bug-fixesheaderlink-5"><a class="header" href="#bug-fixesheaderlink-5">Bug fixes<a href="#id107" title="Permalink to this heading">¶</a>{.headerlink}</a></h5>
<ul>
<li>Make [<code>SpiderLoader</code>{.docutils .literal .notranslate}]{.pre} raise
[<code>ImportError</code>{.docutils .literal .notranslate}]{.pre} again by
default for missing dependencies and wrong <a href="index.html#std-setting-SPIDER_MODULES">[<code>SPIDER_MODULES</code>{.xref
.std .std-setting .docutils .literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal}. These exceptions were silenced as
warnings since 1.3.0. A new setting is introduced to toggle between
warning or exception if needed ; see
<a href="index.html#std-setting-SPIDER_LOADER_WARN_ONLY">[<code>SPIDER_LOADER_WARN_ONLY</code>{.xref .std .std-setting .docutils
.literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal} for details.
:::
:::</li>
</ul>
<p>::: {#scrapy-1-3-2-2017-02-13 .section}
[]{#release-1-3-2}</p>
<h4 id="scrapy-132-2017-02-13headerlink"><a class="header" href="#scrapy-132-2017-02-13headerlink">Scrapy 1.3.2 (2017-02-13)<a href="#scrapy-1-3-2-2017-02-13" title="Permalink to this heading">¶</a>{.headerlink}</a></h4>
<p>::: {#id108 .section}</p>
<h5 id="bug-fixesheaderlink-6"><a class="header" href="#bug-fixesheaderlink-6">Bug fixes<a href="#id108" title="Permalink to this heading">¶</a>{.headerlink}</a></h5>
<ul>
<li>
<p>Preserve request class when converting to/from dicts (utils.reqser)
(<a href="https://github.com/scrapy/scrapy/issues/2510">issue
2510</a>{.reference
.external}).</p>
</li>
<li>
<p>Use consistent selectors for author field in tutorial (<a href="https://github.com/scrapy/scrapy/issues/2551">issue
2551</a>{.reference
.external}).</p>
</li>
<li>
<p>Fix TLS compatibility in Twisted 17+ (<a href="https://github.com/scrapy/scrapy/issues/2558">issue
2558</a>{.reference
.external})
:::
:::</p>
</li>
</ul>
<p>::: {#scrapy-1-3-1-2017-02-08 .section}
[]{#release-1-3-1}</p>
<h4 id="scrapy-131-2017-02-08headerlink"><a class="header" href="#scrapy-131-2017-02-08headerlink">Scrapy 1.3.1 (2017-02-08)<a href="#scrapy-1-3-1-2017-02-08" title="Permalink to this heading">¶</a>{.headerlink}</a></h4>
<p>::: {#id109 .section}</p>
<h5 id="new-featuresheaderlink-4"><a class="header" href="#new-featuresheaderlink-4">New features<a href="#id109" title="Permalink to this heading">¶</a>{.headerlink}</a></h5>
<ul>
<li>
<p>Support [<code>'True'</code>{.docutils .literal .notranslate}]{.pre} and
[<code>'False'</code>{.docutils .literal .notranslate}]{.pre} string values for
boolean settings (<a href="https://github.com/scrapy/scrapy/issues/2519">issue
2519</a>{.reference
.external}); you can now do something like [<code>scrapy</code>{.docutils
.literal .notranslate}]{.pre}<code> </code>{.docutils .literal
.notranslate}[<code>crawl</code>{.docutils .literal
.notranslate}]{.pre}<code> </code>{.docutils .literal
.notranslate}[<code>myspider</code>{.docutils .literal
.notranslate}]{.pre}<code> </code>{.docutils .literal
.notranslate}[<code>-s</code>{.docutils .literal
.notranslate}]{.pre}<code> </code>{.docutils .literal
.notranslate}[<code>REDIRECT_ENABLED=False</code>{.docutils .literal
.notranslate}]{.pre}.</p>
</li>
<li>
<p>Support kwargs with [<code>response.xpath()</code>{.docutils .literal
.notranslate}]{.pre} to use <a href="index.html#topics-selectors-xpath-variables">[XPath variables]{.std
.std-ref}</a>{.hoverxref
.tooltip .reference .internal} and ad-hoc namespaces declarations ;
this requires at least Parsel v1.1 (<a href="https://github.com/scrapy/scrapy/issues/2457">issue
2457</a>{.reference
.external}).</p>
</li>
<li>
<p>Add support for Python 3.6 (<a href="https://github.com/scrapy/scrapy/issues/2485">issue
2485</a>{.reference
.external}).</p>
</li>
<li>
<p>Run tests on PyPy (warning: some tests still fail, so PyPy is not
supported yet).
:::</p>
</li>
</ul>
<p>::: {#id110 .section}</p>
<h5 id="bug-fixesheaderlink-7"><a class="header" href="#bug-fixesheaderlink-7">Bug fixes<a href="#id110" title="Permalink to this heading">¶</a>{.headerlink}</a></h5>
<ul>
<li>
<p>Enforce [<code>DNS_TIMEOUT</code>{.docutils .literal .notranslate}]{.pre}
setting (<a href="https://github.com/scrapy/scrapy/issues/2496">issue
2496</a>{.reference
.external}).</p>
</li>
<li>
<p>Fix <a href="index.html#std-command-view">[<code>view</code>{.xref .std .std-command .docutils .literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal} command ; it was a regression in
v1.3.0 (<a href="https://github.com/scrapy/scrapy/issues/2503">issue
2503</a>{.reference
.external}).</p>
</li>
<li>
<p>Fix tests regarding [<code>*_EXPIRES</code>{.docutils .literal
.notranslate}]{.pre}<code> </code>{.docutils .literal
.notranslate}[<code>settings</code>{.docutils .literal .notranslate}]{.pre}
with Files/Images pipelines (<a href="https://github.com/scrapy/scrapy/issues/2460">issue
2460</a>{.reference
.external}).</p>
</li>
<li>
<p>Fix name of generated pipeline class when using basic project
template (<a href="https://github.com/scrapy/scrapy/issues/2466">issue
2466</a>{.reference
.external}).</p>
</li>
<li>
<p>Fix compatibility with Twisted 17+ (<a href="https://github.com/scrapy/scrapy/issues/2496">issue
2496</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/2528">issue
2528</a>{.reference
.external}).</p>
</li>
<li>
<p>Fix [<code>scrapy.Item</code>{.docutils .literal .notranslate}]{.pre}
inheritance on Python 3.6 (<a href="https://github.com/scrapy/scrapy/issues/2511">issue
2511</a>{.reference
.external}).</p>
</li>
<li>
<p>Enforce numeric values for components order in
[<code>SPIDER_MIDDLEWARES</code>{.docutils .literal .notranslate}]{.pre},
[<code>DOWNLOADER_MIDDLEWARES</code>{.docutils .literal .notranslate}]{.pre},
[<code>EXTENSIONS</code>{.docutils .literal .notranslate}]{.pre} and
[<code>SPIDER_CONTRACTS</code>{.docutils .literal .notranslate}]{.pre} (<a href="https://github.com/scrapy/scrapy/issues/2420">issue
2420</a>{.reference
.external}).
:::</p>
</li>
</ul>
<p>::: {#id111 .section}</p>
<h5 id="documentationheaderlink-3"><a class="header" href="#documentationheaderlink-3">Documentation<a href="#id111" title="Permalink to this heading">¶</a>{.headerlink}</a></h5>
<ul>
<li>
<p>Reword Code of Conduct section and upgrade to Contributor Covenant
v1.4 (<a href="https://github.com/scrapy/scrapy/issues/2469">issue
2469</a>{.reference
.external}).</p>
</li>
<li>
<p>Clarify that passing spider arguments converts them to spider
attributes (<a href="https://github.com/scrapy/scrapy/issues/2483">issue
2483</a>{.reference
.external}).</p>
</li>
<li>
<p>Document [<code>formid</code>{.docutils .literal .notranslate}]{.pre} argument
on [<code>FormRequest.from_response()</code>{.docutils .literal
.notranslate}]{.pre} (<a href="https://github.com/scrapy/scrapy/issues/2497">issue
2497</a>{.reference
.external}).</p>
</li>
<li>
<p>Add .rst extension to README files (<a href="https://github.com/scrapy/scrapy/issues/2507">issue
2507</a>{.reference
.external}).</p>
</li>
<li>
<p>Mention LevelDB cache storage backend (<a href="https://github.com/scrapy/scrapy/issues/2525">issue
2525</a>{.reference
.external}).</p>
</li>
<li>
<p>Use [<code>yield</code>{.docutils .literal .notranslate}]{.pre} in sample
callback code (<a href="https://github.com/scrapy/scrapy/issues/2533">issue
2533</a>{.reference
.external}).</p>
</li>
<li>
<p>Add note about HTML entities decoding with
[<code>.re()/.re_first()</code>{.docutils .literal .notranslate}]{.pre} (<a href="https://github.com/scrapy/scrapy/issues/1704">issue
1704</a>{.reference
.external}).</p>
</li>
<li>
<p>Typos (<a href="https://github.com/scrapy/scrapy/issues/2512">issue
2512</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/2534">issue
2534</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/2531">issue
2531</a>{.reference
.external}).
:::</p>
</li>
</ul>
<p>::: {#cleanups .section}</p>
<h5 id="cleanupsheaderlink"><a class="header" href="#cleanupsheaderlink">Cleanups<a href="#cleanups" title="Permalink to this heading">¶</a>{.headerlink}</a></h5>
<ul>
<li>
<p>Remove redundant check in [<code>MetaRefreshMiddleware</code>{.docutils
.literal .notranslate}]{.pre} (<a href="https://github.com/scrapy/scrapy/issues/2542">issue
2542</a>{.reference
.external}).</p>
</li>
<li>
<p>Faster checks in [<code>LinkExtractor</code>{.docutils .literal
.notranslate}]{.pre} for allow/deny patterns (<a href="https://github.com/scrapy/scrapy/issues/2538">issue
2538</a>{.reference
.external}).</p>
</li>
<li>
<p>Remove dead code supporting old Twisted versions (<a href="https://github.com/scrapy/scrapy/issues/2544">issue
2544</a>{.reference
.external}).
:::
:::</p>
</li>
</ul>
<p>::: {#scrapy-1-3-0-2016-12-21 .section}
[]{#release-1-3-0}</p>
<h4 id="scrapy-130-2016-12-21headerlink"><a class="header" href="#scrapy-130-2016-12-21headerlink">Scrapy 1.3.0 (2016-12-21)<a href="#scrapy-1-3-0-2016-12-21" title="Permalink to this heading">¶</a>{.headerlink}</a></h4>
<p>This release comes rather soon after 1.2.2 for one main reason: it was
found out that releases since 0.18 up to 1.2.2 (included) use some
backported code from Twisted ([<code>scrapy.xlib.tx.*</code>{.docutils .literal
.notranslate}]{.pre}), even if newer Twisted modules are available.
Scrapy now uses [<code>twisted.web.client</code>{.docutils .literal
.notranslate}]{.pre} and [<code>twisted.internet.endpoints</code>{.docutils
.literal .notranslate}]{.pre} directly. (See also cleanups below.)</p>
<p>As it is a major change, we wanted to get the bug fix out quickly while
not breaking any projects using the 1.2 series.</p>
<p>::: {#id112 .section}</p>
<h5 id="new-featuresheaderlink-5"><a class="header" href="#new-featuresheaderlink-5">New Features<a href="#id112" title="Permalink to this heading">¶</a>{.headerlink}</a></h5>
<ul>
<li>
<p>[<code>MailSender</code>{.docutils .literal .notranslate}]{.pre} now accepts
single strings as values for [<code>to</code>{.docutils .literal
.notranslate}]{.pre} and [<code>cc</code>{.docutils .literal
.notranslate}]{.pre} arguments (<a href="https://github.com/scrapy/scrapy/issues/2272">issue
2272</a>{.reference
.external})</p>
</li>
<li>
<p>[<code>scrapy</code>{.docutils .literal .notranslate}]{.pre}<code> </code>{.docutils
.literal .notranslate}[<code>fetch</code>{.docutils .literal
.notranslate}]{.pre}<code> </code>{.docutils .literal
.notranslate}[<code>url</code>{.docutils .literal .notranslate}]{.pre},
[<code>scrapy</code>{.docutils .literal .notranslate}]{.pre}<code> </code>{.docutils
.literal .notranslate}[<code>shell</code>{.docutils .literal
.notranslate}]{.pre}<code> </code>{.docutils .literal
.notranslate}[<code>url</code>{.docutils .literal .notranslate}]{.pre} and
[<code>fetch(url)</code>{.docutils .literal .notranslate}]{.pre} inside Scrapy
shell now follow HTTP redirections by default (<a href="https://github.com/scrapy/scrapy/issues/2290">issue
2290</a>{.reference
.external}); See <a href="index.html#std-command-fetch">[<code>fetch</code>{.xref .std .std-command .docutils
.literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal} and <a href="index.html#std-command-shell">[<code>shell</code>{.xref .std .std-command
.docutils .literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal} for details.</p>
</li>
<li>
<p>[<code>HttpErrorMiddleware</code>{.docutils .literal .notranslate}]{.pre} now
logs errors with [<code>INFO</code>{.docutils .literal .notranslate}]{.pre}
level instead of [<code>DEBUG</code>{.docutils .literal .notranslate}]{.pre};
this is technically <strong>backward incompatible</strong> so please check your
log parsers.</p>
</li>
<li>
<p>By default, logger names now use a long-form path, e.g.
[<code>[scrapy.extensions.logstats]</code>{.docutils .literal
.notranslate}]{.pre}, instead of the shorter &quot;top-level&quot; variant of
prior releases (e.g. [<code>[scrapy]</code>{.docutils .literal
.notranslate}]{.pre}); this is <strong>backward incompatible</strong> if you have
log parsers expecting the short logger name part. You can switch
back to short logger names using <a href="index.html#std-setting-LOG_SHORT_NAMES">[<code>LOG_SHORT_NAMES</code>{.xref .std
.std-setting .docutils .literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal} set to [<code>True</code>{.docutils .literal
.notranslate}]{.pre}.
:::</p>
</li>
</ul>
<p>::: {#dependencies-cleanups .section}</p>
<h5 id="dependencies--cleanupsheaderlink"><a class="header" href="#dependencies--cleanupsheaderlink">Dependencies &amp; Cleanups<a href="#dependencies-cleanups" title="Permalink to this heading">¶</a>{.headerlink}</a></h5>
<ul>
<li>
<p>Scrapy now requires Twisted &gt;= 13.1 which is the case for many
Linux distributions already.</p>
</li>
<li>
<p>As a consequence, we got rid of [<code>scrapy.xlib.tx.*</code>{.docutils
.literal .notranslate}]{.pre} modules, which copied some of Twisted
code for users stuck with an &quot;old&quot; Twisted version</p>
</li>
<li>
<p>[<code>ChunkedTransferMiddleware</code>{.docutils .literal .notranslate}]{.pre}
is deprecated and removed from the default downloader middlewares.
:::
:::</p>
</li>
</ul>
<p>::: {#scrapy-1-2-3-2017-03-03 .section}
[]{#release-1-2-3}</p>
<h4 id="scrapy-123-2017-03-03headerlink"><a class="header" href="#scrapy-123-2017-03-03headerlink">Scrapy 1.2.3 (2017-03-03)<a href="#scrapy-1-2-3-2017-03-03" title="Permalink to this heading">¶</a>{.headerlink}</a></h4>
<ul>
<li>Packaging fix: disallow unsupported Twisted versions in setup.py
:::</li>
</ul>
<p>::: {#scrapy-1-2-2-2016-12-06 .section}
[]{#release-1-2-2}</p>
<h4 id="scrapy-122-2016-12-06headerlink"><a class="header" href="#scrapy-122-2016-12-06headerlink">Scrapy 1.2.2 (2016-12-06)<a href="#scrapy-1-2-2-2016-12-06" title="Permalink to this heading">¶</a>{.headerlink}</a></h4>
<p>::: {#id113 .section}</p>
<h5 id="bug-fixesheaderlink-8"><a class="header" href="#bug-fixesheaderlink-8">Bug fixes<a href="#id113" title="Permalink to this heading">¶</a>{.headerlink}</a></h5>
<ul>
<li>
<p>Fix a cryptic traceback when a pipeline fails on
[<code>open_spider()</code>{.docutils .literal .notranslate}]{.pre} (<a href="https://github.com/scrapy/scrapy/issues/2011">issue
2011</a>{.reference
.external})</p>
</li>
<li>
<p>Fix embedded IPython shell variables (fixing <a href="https://github.com/scrapy/scrapy/issues/396">issue
396</a>{.reference
.external} that re-appeared in 1.2.0, fixed in <a href="https://github.com/scrapy/scrapy/issues/2418">issue
2418</a>{.reference
.external})</p>
</li>
<li>
<p>A couple of patches when dealing with robots.txt:</p>
<ul>
<li>
<p>handle (non-standard) relative sitemap URLs (<a href="https://github.com/scrapy/scrapy/issues/2390">issue
2390</a>{.reference
.external})</p>
</li>
<li>
<p>handle non-ASCII URLs and User-Agents in Python 2 (<a href="https://github.com/scrapy/scrapy/issues/2373">issue
2373</a>{.reference
.external})
:::</p>
</li>
</ul>
</li>
</ul>
<p>::: {#id114 .section}</p>
<h5 id="documentationheaderlink-4"><a class="header" href="#documentationheaderlink-4">Documentation<a href="#id114" title="Permalink to this heading">¶</a>{.headerlink}</a></h5>
<ul>
<li>
<p>Document [<code>&quot;download_latency&quot;</code>{.docutils .literal
.notranslate}]{.pre} key in [<code>Request</code>{.docutils .literal
.notranslate}]{.pre}'s [<code>meta</code>{.docutils .literal
.notranslate}]{.pre} dict (<a href="https://github.com/scrapy/scrapy/issues/2033">issue
2033</a>{.reference
.external})</p>
</li>
<li>
<p>Remove page on (deprecated &amp; unsupported) Ubuntu packages from ToC
(<a href="https://github.com/scrapy/scrapy/issues/2335">issue
2335</a>{.reference
.external})</p>
</li>
<li>
<p>A few fixed typos (<a href="https://github.com/scrapy/scrapy/issues/2346">issue
2346</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/2369">issue
2369</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/2369">issue
2369</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/2380">issue
2380</a>{.reference
.external}) and clarifications (<a href="https://github.com/scrapy/scrapy/issues/2354">issue
2354</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/2325">issue
2325</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/2414">issue
2414</a>{.reference
.external})
:::</p>
</li>
</ul>
<p>::: {#id115 .section}</p>
<h5 id="other-changesheaderlink-2"><a class="header" href="#other-changesheaderlink-2">Other changes<a href="#id115" title="Permalink to this heading">¶</a>{.headerlink}</a></h5>
<ul>
<li>
<p>Advertize
<a href="https://anaconda.org/conda-forge/scrapy">conda-forge</a>{.reference
.external} as Scrapy's official conda channel (<a href="https://github.com/scrapy/scrapy/issues/2387">issue
2387</a>{.reference
.external})</p>
</li>
<li>
<p>More helpful error messages when trying to use [<code>.css()</code>{.docutils
.literal .notranslate}]{.pre} or [<code>.xpath()</code>{.docutils .literal
.notranslate}]{.pre} on non-Text Responses (<a href="https://github.com/scrapy/scrapy/issues/2264">issue
2264</a>{.reference
.external})</p>
</li>
<li>
<p>[<code>startproject</code>{.docutils .literal .notranslate}]{.pre} command now
generates a sample [<code>middlewares.py</code>{.docutils .literal
.notranslate}]{.pre} file (<a href="https://github.com/scrapy/scrapy/issues/2335">issue
2335</a>{.reference
.external})</p>
</li>
<li>
<p>Add more dependencies' version info in [<code>scrapy</code>{.docutils .literal
.notranslate}]{.pre}<code> </code>{.docutils .literal
.notranslate}[<code>version</code>{.docutils .literal .notranslate}]{.pre}
verbose output (<a href="https://github.com/scrapy/scrapy/issues/2404">issue
2404</a>{.reference
.external})</p>
</li>
<li>
<p>Remove all [<code>*.pyc</code>{.docutils .literal .notranslate}]{.pre} files
from source distribution (<a href="https://github.com/scrapy/scrapy/issues/2386">issue
2386</a>{.reference
.external})
:::
:::</p>
</li>
</ul>
<p>::: {#scrapy-1-2-1-2016-10-21 .section}
[]{#release-1-2-1}</p>
<h4 id="scrapy-121-2016-10-21headerlink"><a class="header" href="#scrapy-121-2016-10-21headerlink">Scrapy 1.2.1 (2016-10-21)<a href="#scrapy-1-2-1-2016-10-21" title="Permalink to this heading">¶</a>{.headerlink}</a></h4>
<p>::: {#id116 .section}</p>
<h5 id="bug-fixesheaderlink-9"><a class="header" href="#bug-fixesheaderlink-9">Bug fixes<a href="#id116" title="Permalink to this heading">¶</a>{.headerlink}</a></h5>
<ul>
<li>
<p>Include OpenSSL's more permissive default ciphers when establishing
TLS/SSL connections (<a href="https://github.com/scrapy/scrapy/issues/2314">issue
2314</a>{.reference
.external}).</p>
</li>
<li>
<p>Fix &quot;Location&quot; HTTP header decoding on non-ASCII URL redirects
(<a href="https://github.com/scrapy/scrapy/issues/2321">issue
2321</a>{.reference
.external}).
:::</p>
</li>
</ul>
<p>::: {#id117 .section}</p>
<h5 id="documentationheaderlink-5"><a class="header" href="#documentationheaderlink-5">Documentation<a href="#id117" title="Permalink to this heading">¶</a>{.headerlink}</a></h5>
<ul>
<li>
<p>Fix JsonWriterPipeline example (<a href="https://github.com/scrapy/scrapy/issues/2302">issue
2302</a>{.reference
.external}).</p>
</li>
<li>
<p>Various notes: <a href="https://github.com/scrapy/scrapy/issues/2330">issue
2330</a>{.reference
.external} on spider names, <a href="https://github.com/scrapy/scrapy/issues/2329">issue
2329</a>{.reference
.external} on middleware methods processing order, <a href="https://github.com/scrapy/scrapy/issues/2327">issue
2327</a>{.reference
.external} on getting multi-valued HTTP headers as lists.
:::</p>
</li>
</ul>
<p>::: {#id118 .section}</p>
<h5 id="other-changesheaderlink-3"><a class="header" href="#other-changesheaderlink-3">Other changes<a href="#id118" title="Permalink to this heading">¶</a>{.headerlink}</a></h5>
<ul>
<li>Removed [<code>www.</code>{.docutils .literal .notranslate}]{.pre} from
[<code>start_urls</code>{.docutils .literal .notranslate}]{.pre} in built-in
spider templates (<a href="https://github.com/scrapy/scrapy/issues/2299">issue
2299</a>{.reference
.external}).
:::
:::</li>
</ul>
<p>::: {#scrapy-1-2-0-2016-10-03 .section}
[]{#release-1-2-0}</p>
<h4 id="scrapy-120-2016-10-03headerlink"><a class="header" href="#scrapy-120-2016-10-03headerlink">Scrapy 1.2.0 (2016-10-03)<a href="#scrapy-1-2-0-2016-10-03" title="Permalink to this heading">¶</a>{.headerlink}</a></h4>
<p>::: {#id119 .section}</p>
<h5 id="new-featuresheaderlink-6"><a class="header" href="#new-featuresheaderlink-6">New Features<a href="#id119" title="Permalink to this heading">¶</a>{.headerlink}</a></h5>
<ul>
<li>
<p>New <a href="index.html#std-setting-FEED_EXPORT_ENCODING">[<code>FEED_EXPORT_ENCODING</code>{.xref .std .std-setting .docutils
.literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal} setting to customize the encoding
used when writing items to a file. This can be used to turn off
[<code>\uXXXX</code>{.docutils .literal .notranslate}]{.pre} escapes in JSON
output. This is also useful for those wanting something else than
UTF-8 for XML or CSV output (<a href="https://github.com/scrapy/scrapy/issues/2034">issue
2034</a>{.reference
.external}).</p>
</li>
<li>
<p>[<code>startproject</code>{.docutils .literal .notranslate}]{.pre} command now
supports an optional destination directory to override the default
one based on the project name (<a href="https://github.com/scrapy/scrapy/issues/2005">issue
2005</a>{.reference
.external}).</p>
</li>
<li>
<p>New <a href="index.html#std-setting-SCHEDULER_DEBUG">[<code>SCHEDULER_DEBUG</code>{.xref .std .std-setting .docutils .literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal} setting to log requests serialization
failures (<a href="https://github.com/scrapy/scrapy/issues/1610">issue
1610</a>{.reference
.external}).</p>
</li>
<li>
<p>JSON encoder now supports serialization of [<code>set</code>{.docutils .literal
.notranslate}]{.pre} instances (<a href="https://github.com/scrapy/scrapy/issues/2058">issue
2058</a>{.reference
.external}).</p>
</li>
<li>
<p>Interpret [<code>application/json-amazonui-streaming</code>{.docutils .literal
.notranslate}]{.pre} as [<code>TextResponse</code>{.docutils .literal
.notranslate}]{.pre} (<a href="https://github.com/scrapy/scrapy/issues/1503">issue
1503</a>{.reference
.external}).</p>
</li>
<li>
<p>[<code>scrapy</code>{.docutils .literal .notranslate}]{.pre} is imported by
default when using shell tools (<a href="index.html#std-command-shell">[<code>shell</code>{.xref .std .std-command
.docutils .literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal}, <a href="index.html#topics-shell-inspect-response">[inspect_response]{.std
.std-ref}</a>{.hoverxref
.tooltip .reference .internal}) (<a href="https://github.com/scrapy/scrapy/issues/2248">issue
2248</a>{.reference
.external}).
:::</p>
</li>
</ul>
<p>::: {#id120 .section}</p>
<h5 id="bug-fixesheaderlink-10"><a class="header" href="#bug-fixesheaderlink-10">Bug fixes<a href="#id120" title="Permalink to this heading">¶</a>{.headerlink}</a></h5>
<ul>
<li>
<p>DefaultRequestHeaders middleware now runs before UserAgent
middleware (<a href="https://github.com/scrapy/scrapy/issues/2088">issue
2088</a>{.reference
.external}). <strong>Warning: this is technically backward incompatible</strong>,
though we consider this a bug fix.</p>
</li>
<li>
<p>HTTP cache extension and plugins that use the [<code>.scrapy</code>{.docutils
.literal .notranslate}]{.pre} data directory now work outside
projects (<a href="https://github.com/scrapy/scrapy/issues/1581">issue
1581</a>{.reference
.external}). <strong>Warning: this is technically backward incompatible</strong>,
though we consider this a bug fix.</p>
</li>
<li>
<p>[<code>Selector</code>{.docutils .literal .notranslate}]{.pre} does not allow
passing both [<code>response</code>{.docutils .literal .notranslate}]{.pre} and
[<code>text</code>{.docutils .literal .notranslate}]{.pre} anymore (<a href="https://github.com/scrapy/scrapy/issues/2153">issue
2153</a>{.reference
.external}).</p>
</li>
<li>
<p>Fixed logging of wrong callback name with [<code>scrapy</code>{.docutils
.literal .notranslate}]{.pre}<code> </code>{.docutils .literal
.notranslate}[<code>parse</code>{.docutils .literal .notranslate}]{.pre}
(<a href="https://github.com/scrapy/scrapy/issues/2169">issue
2169</a>{.reference
.external}).</p>
</li>
<li>
<p>Fix for an odd gzip decompression bug (<a href="https://github.com/scrapy/scrapy/issues/1606">issue
1606</a>{.reference
.external}).</p>
</li>
<li>
<p>Fix for selected callbacks when using [<code>CrawlSpider</code>{.docutils
.literal .notranslate}]{.pre} with <a href="index.html#std-command-parse">[<code>scrapy</code>{.xref .std
.std-command .docutils .literal .notranslate}]{.pre}<code> </code>{.xref .std
.std-command .docutils .literal .notranslate}[<code>parse</code>{.xref .std
.std-command .docutils .literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal} (<a href="https://github.com/scrapy/scrapy/issues/2225">issue
2225</a>{.reference
.external}).</p>
</li>
<li>
<p>Fix for invalid JSON and XML files when spider yields no items
(<a href="https://github.com/scrapy/scrapy/issues/872">issue 872</a>{.reference
.external}).</p>
</li>
<li>
<p>Implement [<code>flush()</code>{.docutils .literal .notranslate}]{.pre} for
[<code>StreamLogger</code>{.docutils .literal .notranslate}]{.pre} avoiding a
warning in logs (<a href="https://github.com/scrapy/scrapy/issues/2125">issue
2125</a>{.reference
.external}).
:::</p>
</li>
</ul>
<p>::: {#refactoring .section}</p>
<h5 id="refactoringheaderlink"><a class="header" href="#refactoringheaderlink">Refactoring<a href="#refactoring" title="Permalink to this heading">¶</a>{.headerlink}</a></h5>
<ul>
<li>[<code>canonicalize_url</code>{.docutils .literal .notranslate}]{.pre} has been
moved to
<a href="https://w3lib.readthedocs.io/en/latest/w3lib.html#w3lib.url.canonicalize_url">w3lib.url</a>{.reference
.external} (<a href="https://github.com/scrapy/scrapy/issues/2168">issue
2168</a>{.reference
.external}).
:::</li>
</ul>
<p>::: {#tests-requirements .section}</p>
<h5 id="tests--requirementsheaderlink"><a class="header" href="#tests--requirementsheaderlink">Tests &amp; Requirements<a href="#tests-requirements" title="Permalink to this heading">¶</a>{.headerlink}</a></h5>
<p>Scrapy's new requirements baseline is Debian 8 &quot;Jessie&quot;. It was
previously Ubuntu 12.04 Precise. What this means in practice is that we
run continuous integration tests with these (main) packages versions at
a minimum: Twisted 14.0, pyOpenSSL 0.14, lxml 3.4.</p>
<p>Scrapy may very well work with older versions of these packages (the
code base still has switches for older Twisted versions for example) but
it is not guaranteed (because it's not tested anymore).
:::</p>
<p>::: {#id121 .section}</p>
<h5 id="documentationheaderlink-6"><a class="header" href="#documentationheaderlink-6">Documentation<a href="#id121" title="Permalink to this heading">¶</a>{.headerlink}</a></h5>
<ul>
<li>
<p>Grammar fixes: <a href="https://github.com/scrapy/scrapy/issues/2128">issue
2128</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/1566">issue
1566</a>{.reference
.external}.</p>
</li>
<li>
<p>Download stats badge removed from README (<a href="https://github.com/scrapy/scrapy/issues/2160">issue
2160</a>{.reference
.external}).</p>
</li>
<li>
<p>New Scrapy <a href="index.html#topics-architecture">[architecture diagram]{.std
.std-ref}</a>{.hoverxref .tooltip
.reference .internal} (<a href="https://github.com/scrapy/scrapy/issues/2165">issue
2165</a>{.reference
.external}).</p>
</li>
<li>
<p>Updated [<code>Response</code>{.docutils .literal .notranslate}]{.pre}
parameters documentation (<a href="https://github.com/scrapy/scrapy/issues/2197">issue
2197</a>{.reference
.external}).</p>
</li>
<li>
<p>Reworded misleading <a href="index.html#std-setting-RANDOMIZE_DOWNLOAD_DELAY">[<code>RANDOMIZE_DOWNLOAD_DELAY</code>{.xref .std
.std-setting .docutils .literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal} description (<a href="https://github.com/scrapy/scrapy/issues/2190">issue
2190</a>{.reference
.external}).</p>
</li>
<li>
<p>Add StackOverflow as a support channel (<a href="https://github.com/scrapy/scrapy/issues/2257">issue
2257</a>{.reference
.external}).
:::
:::</p>
</li>
</ul>
<p>::: {#scrapy-1-1-4-2017-03-03 .section}
[]{#release-1-1-4}</p>
<h4 id="scrapy-114-2017-03-03headerlink"><a class="header" href="#scrapy-114-2017-03-03headerlink">Scrapy 1.1.4 (2017-03-03)<a href="#scrapy-1-1-4-2017-03-03" title="Permalink to this heading">¶</a>{.headerlink}</a></h4>
<ul>
<li>Packaging fix: disallow unsupported Twisted versions in setup.py
:::</li>
</ul>
<p>::: {#scrapy-1-1-3-2016-09-22 .section}
[]{#release-1-1-3}</p>
<h4 id="scrapy-113-2016-09-22headerlink"><a class="header" href="#scrapy-113-2016-09-22headerlink">Scrapy 1.1.3 (2016-09-22)<a href="#scrapy-1-1-3-2016-09-22" title="Permalink to this heading">¶</a>{.headerlink}</a></h4>
<p>::: {#id122 .section}</p>
<h5 id="bug-fixesheaderlink-11"><a class="header" href="#bug-fixesheaderlink-11">Bug fixes<a href="#id122" title="Permalink to this heading">¶</a>{.headerlink}</a></h5>
<ul>
<li>Class attributes for subclasses of [<code>ImagesPipeline</code>{.docutils
.literal .notranslate}]{.pre} and [<code>FilesPipeline</code>{.docutils
.literal .notranslate}]{.pre} work as they did before 1.1.1 (<a href="https://github.com/scrapy/scrapy/issues/2243">issue
2243</a>{.reference
.external}, fixes <a href="https://github.com/scrapy/scrapy/issues/2198">issue
2198</a>{.reference
.external})
:::</li>
</ul>
<p>::: {#id123 .section}</p>
<h5 id="documentationheaderlink-7"><a class="header" href="#documentationheaderlink-7">Documentation<a href="#id123" title="Permalink to this heading">¶</a>{.headerlink}</a></h5>
<ul>
<li><a href="index.html#intro-overview">[Overview]{.std .std-ref}</a>{.hoverxref
.tooltip .reference .internal} and <a href="index.html#intro-tutorial">[tutorial]{.std
.std-ref}</a>{.hoverxref .tooltip .reference
.internal} rewritten to use
<a href="http://toscrape.com">http://toscrape.com</a>{.reference .external}
websites (<a href="https://github.com/scrapy/scrapy/issues/2236">issue
2236</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/2249">issue
2249</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/2252">issue
2252</a>{.reference
.external}).
:::
:::</li>
</ul>
<p>::: {#scrapy-1-1-2-2016-08-18 .section}
[]{#release-1-1-2}</p>
<h4 id="scrapy-112-2016-08-18headerlink"><a class="header" href="#scrapy-112-2016-08-18headerlink">Scrapy 1.1.2 (2016-08-18)<a href="#scrapy-1-1-2-2016-08-18" title="Permalink to this heading">¶</a>{.headerlink}</a></h4>
<p>::: {#id124 .section}</p>
<h5 id="bug-fixesheaderlink-12"><a class="header" href="#bug-fixesheaderlink-12">Bug fixes<a href="#id124" title="Permalink to this heading">¶</a>{.headerlink}</a></h5>
<ul>
<li>
<p>Introduce a missing <a href="index.html#std-setting-IMAGES_STORE_S3_ACL">[<code>IMAGES_STORE_S3_ACL</code>{.xref .std .std-setting
.docutils .literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal} setting to override the default ACL
policy in [<code>ImagesPipeline</code>{.docutils .literal .notranslate}]{.pre}
when uploading images to S3 (note that default ACL policy is
&quot;private&quot; -- instead of &quot;public-read&quot; -- since Scrapy 1.1.0)</p>
</li>
<li>
<p><a href="index.html#std-setting-IMAGES_EXPIRES">[<code>IMAGES_EXPIRES</code>{.xref .std .std-setting .docutils .literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal} default value set back to 90 (the
regression was introduced in 1.1.1)
:::
:::</p>
</li>
</ul>
<p>::: {#scrapy-1-1-1-2016-07-13 .section}
[]{#release-1-1-1}</p>
<h4 id="scrapy-111-2016-07-13headerlink"><a class="header" href="#scrapy-111-2016-07-13headerlink">Scrapy 1.1.1 (2016-07-13)<a href="#scrapy-1-1-1-2016-07-13" title="Permalink to this heading">¶</a>{.headerlink}</a></h4>
<p>::: {#id125 .section}</p>
<h5 id="bug-fixesheaderlink-13"><a class="header" href="#bug-fixesheaderlink-13">Bug fixes<a href="#id125" title="Permalink to this heading">¶</a>{.headerlink}</a></h5>
<ul>
<li>
<p>Add &quot;Host&quot; header in CONNECT requests to HTTPS proxies (<a href="https://github.com/scrapy/scrapy/issues/2069">issue
2069</a>{.reference
.external})</p>
</li>
<li>
<p>Use response [<code>body</code>{.docutils .literal .notranslate}]{.pre} when
choosing response class (<a href="https://github.com/scrapy/scrapy/issues/2001">issue
2001</a>{.reference
.external}, fixes <a href="https://github.com/scrapy/scrapy/issues/2000">issue
2000</a>{.reference
.external})</p>
</li>
<li>
<p>Do not fail on canonicalizing URLs with wrong netlocs (<a href="https://github.com/scrapy/scrapy/issues/2038">issue
2038</a>{.reference
.external}, fixes <a href="https://github.com/scrapy/scrapy/issues/2010">issue
2010</a>{.reference
.external})</p>
</li>
<li>
<p>a few fixes for [<code>HttpCompressionMiddleware</code>{.docutils .literal
.notranslate}]{.pre} (and [<code>SitemapSpider</code>{.docutils .literal
.notranslate}]{.pre}):</p>
<ul>
<li>
<p>Do not decode HEAD responses (<a href="https://github.com/scrapy/scrapy/issues/2008">issue
2008</a>{.reference
.external}, fixes <a href="https://github.com/scrapy/scrapy/issues/1899">issue
1899</a>{.reference
.external})</p>
</li>
<li>
<p>Handle charset parameter in gzip Content-Type header (<a href="https://github.com/scrapy/scrapy/issues/2050">issue
2050</a>{.reference
.external}, fixes <a href="https://github.com/scrapy/scrapy/issues/2049">issue
2049</a>{.reference
.external})</p>
</li>
<li>
<p>Do not decompress gzip octet-stream responses (<a href="https://github.com/scrapy/scrapy/issues/2065">issue
2065</a>{.reference
.external}, fixes <a href="https://github.com/scrapy/scrapy/issues/2063">issue
2063</a>{.reference
.external})</p>
</li>
</ul>
</li>
<li>
<p>Catch (and ignore with a warning) exception when verifying
certificate against IP-address hosts (<a href="https://github.com/scrapy/scrapy/issues/2094">issue
2094</a>{.reference
.external}, fixes <a href="https://github.com/scrapy/scrapy/issues/2092">issue
2092</a>{.reference
.external})</p>
</li>
<li>
<p>Make [<code>FilesPipeline</code>{.docutils .literal .notranslate}]{.pre} and
[<code>ImagesPipeline</code>{.docutils .literal .notranslate}]{.pre} backward
compatible again regarding the use of legacy class attributes for
customization (<a href="https://github.com/scrapy/scrapy/issues/1989">issue
1989</a>{.reference
.external}, fixes <a href="https://github.com/scrapy/scrapy/issues/1985">issue
1985</a>{.reference
.external})
:::</p>
</li>
</ul>
<p>::: {#id126 .section}</p>
<h5 id="new-featuresheaderlink-7"><a class="header" href="#new-featuresheaderlink-7">New features<a href="#id126" title="Permalink to this heading">¶</a>{.headerlink}</a></h5>
<ul>
<li>
<p>Enable genspider command outside project folder (<a href="https://github.com/scrapy/scrapy/issues/2052">issue
2052</a>{.reference
.external})</p>
</li>
<li>
<p>Retry HTTPS CONNECT [<code>TunnelError</code>{.docutils .literal
.notranslate}]{.pre} by default (<a href="https://github.com/scrapy/scrapy/issues/1974">issue
1974</a>{.reference
.external})
:::</p>
</li>
</ul>
<p>::: {#id127 .section}</p>
<h5 id="documentationheaderlink-8"><a class="header" href="#documentationheaderlink-8">Documentation<a href="#id127" title="Permalink to this heading">¶</a>{.headerlink}</a></h5>
<ul>
<li>
<p>[<code>FEED_TEMPDIR</code>{.docutils .literal .notranslate}]{.pre} setting at
lexicographical position (<a href="https://github.com/scrapy/scrapy/commit/9b3c72c">commit
9b3c72c</a>{.reference
.external})</p>
</li>
<li>
<p>Use idiomatic [<code>.extract_first()</code>{.docutils .literal
.notranslate}]{.pre} in overview (<a href="https://github.com/scrapy/scrapy/issues/1994">issue
1994</a>{.reference
.external})</p>
</li>
<li>
<p>Update years in copyright notice (<a href="https://github.com/scrapy/scrapy/commit/c2c8036">commit
c2c8036</a>{.reference
.external})</p>
</li>
<li>
<p>Add information and example on errbacks (<a href="https://github.com/scrapy/scrapy/issues/1995">issue
1995</a>{.reference
.external})</p>
</li>
<li>
<p>Use &quot;url&quot; variable in downloader middleware example (<a href="https://github.com/scrapy/scrapy/issues/2015">issue
2015</a>{.reference
.external})</p>
</li>
<li>
<p>Grammar fixes (<a href="https://github.com/scrapy/scrapy/issues/2054">issue
2054</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/2120">issue
2120</a>{.reference
.external})</p>
</li>
<li>
<p>New FAQ entry on using BeautifulSoup in spider callbacks (<a href="https://github.com/scrapy/scrapy/issues/2048">issue
2048</a>{.reference
.external})</p>
</li>
<li>
<p>Add notes about Scrapy not working on Windows with Python 3 (<a href="https://github.com/scrapy/scrapy/issues/2060">issue
2060</a>{.reference
.external})</p>
</li>
<li>
<p>Encourage complete titles in pull requests (<a href="https://github.com/scrapy/scrapy/issues/2026">issue
2026</a>{.reference
.external})
:::</p>
</li>
</ul>
<p>::: {#tests .section}</p>
<h5 id="testsheaderlink"><a class="header" href="#testsheaderlink">Tests<a href="#tests" title="Permalink to this heading">¶</a>{.headerlink}</a></h5>
<ul>
<li>Upgrade py.test requirement on Travis CI and Pin pytest-cov to 2.2.1
(<a href="https://github.com/scrapy/scrapy/issues/2095">issue
2095</a>{.reference
.external})
:::
:::</li>
</ul>
<p>::: {#scrapy-1-1-0-2016-05-11 .section}
[]{#release-1-1-0}</p>
<h4 id="scrapy-110-2016-05-11headerlink"><a class="header" href="#scrapy-110-2016-05-11headerlink">Scrapy 1.1.0 (2016-05-11)<a href="#scrapy-1-1-0-2016-05-11" title="Permalink to this heading">¶</a>{.headerlink}</a></h4>
<p>This 1.1 release brings a lot of interesting features and bug fixes:</p>
<ul>
<li>
<p>Scrapy 1.1 has beta Python 3 support (requires Twisted &gt;= 15.5).
See <a href="#news-betapy3">[Beta Python 3 Support]{.std
.std-ref}</a>{.hoverxref .tooltip .reference .internal}
for more details and some limitations.</p>
</li>
<li>
<p>Hot new features:</p>
<ul>
<li>
<p>Item loaders now support nested loaders (<a href="https://github.com/scrapy/scrapy/issues/1467">issue
1467</a>{.reference
.external}).</p>
</li>
<li>
<p>[<code>FormRequest.from_response</code>{.docutils .literal
.notranslate}]{.pre} improvements (<a href="https://github.com/scrapy/scrapy/issues/1382">issue
1382</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/1137">issue
1137</a>{.reference
.external}).</p>
</li>
<li>
<p>Added setting <a href="index.html#std-setting-AUTOTHROTTLE_TARGET_CONCURRENCY">[<code>AUTOTHROTTLE_TARGET_CONCURRENCY</code>{.xref .std
.std-setting .docutils .literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal} and improved AutoThrottle docs
(<a href="https://github.com/scrapy/scrapy/issues/1324">issue
1324</a>{.reference
.external}).</p>
</li>
<li>
<p>Added [<code>response.text</code>{.docutils .literal .notranslate}]{.pre}
to get body as unicode (<a href="https://github.com/scrapy/scrapy/issues/1730">issue
1730</a>{.reference
.external}).</p>
</li>
<li>
<p>Anonymous S3 connections (<a href="https://github.com/scrapy/scrapy/issues/1358">issue
1358</a>{.reference
.external}).</p>
</li>
<li>
<p>Deferreds in downloader middlewares (<a href="https://github.com/scrapy/scrapy/issues/1473">issue
1473</a>{.reference
.external}). This enables better robots.txt handling (<a href="https://github.com/scrapy/scrapy/issues/1471">issue
1471</a>{.reference
.external}).</p>
</li>
<li>
<p>HTTP caching now follows RFC2616 more closely, added settings
<a href="index.html#std-setting-HTTPCACHE_ALWAYS_STORE">[<code>HTTPCACHE_ALWAYS_STORE</code>{.xref .std .std-setting .docutils
.literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal} and
<a href="index.html#std-setting-HTTPCACHE_IGNORE_RESPONSE_CACHE_CONTROLS">[<code>HTTPCACHE_IGNORE_RESPONSE_CACHE_CONTROLS</code>{.xref .std
.std-setting .docutils .literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal} (<a href="https://github.com/scrapy/scrapy/issues/1151">issue
1151</a>{.reference
.external}).</p>
</li>
<li>
<p>Selectors were extracted to the
<a href="https://github.com/scrapy/parsel">parsel</a>{.reference .external}
library (<a href="https://github.com/scrapy/scrapy/issues/1409">issue
1409</a>{.reference
.external}). This means you can use Scrapy Selectors without
Scrapy and also upgrade the selectors engine without needing to
upgrade Scrapy.</p>
</li>
<li>
<p>HTTPS downloader now does TLS protocol negotiation by default,
instead of forcing TLS 1.0. You can also set the SSL/TLS method
using the new <a href="index.html#std-setting-DOWNLOADER_CLIENT_TLS_METHOD">[<code>DOWNLOADER_CLIENT_TLS_METHOD</code>{.xref .std
.std-setting .docutils .literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal}.</p>
</li>
</ul>
</li>
<li>
<p>These bug fixes may require your attention:</p>
<ul>
<li>
<p>Don't retry bad requests (HTTP 400) by default (<a href="https://github.com/scrapy/scrapy/issues/1289">issue
1289</a>{.reference
.external}). If you need the old behavior, add [<code>400</code>{.docutils
.literal .notranslate}]{.pre} to <a href="index.html#std-setting-RETRY_HTTP_CODES">[<code>RETRY_HTTP_CODES</code>{.xref .std
.std-setting .docutils .literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal}.</p>
</li>
<li>
<p>Fix shell files argument handling (<a href="https://github.com/scrapy/scrapy/issues/1710">issue
1710</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/1550">issue
1550</a>{.reference
.external}). If you try [<code>scrapy</code>{.docutils .literal
.notranslate}]{.pre}<code> </code>{.docutils .literal
.notranslate}[<code>shell</code>{.docutils .literal
.notranslate}]{.pre}<code> </code>{.docutils .literal
.notranslate}[<code>index.html</code>{.docutils .literal
.notranslate}]{.pre} it will try to load the URL
<a href="http://index.html">http://index.html</a>{.reference .external},
use [<code>scrapy</code>{.docutils .literal
.notranslate}]{.pre}<code> </code>{.docutils .literal
.notranslate}[<code>shell</code>{.docutils .literal
.notranslate}]{.pre}<code> </code>{.docutils .literal
.notranslate}[<code>./index.html</code>{.docutils .literal
.notranslate}]{.pre} to load a local file.</p>
</li>
<li>
<p>Robots.txt compliance is now enabled by default for
newly-created projects (<a href="https://github.com/scrapy/scrapy/issues/1724">issue
1724</a>{.reference
.external}). Scrapy will also wait for robots.txt to be
downloaded before proceeding with the crawl (<a href="https://github.com/scrapy/scrapy/issues/1735">issue
1735</a>{.reference
.external}). If you want to disable this behavior, update
<a href="index.html#std-setting-ROBOTSTXT_OBEY">[<code>ROBOTSTXT_OBEY</code>{.xref .std .std-setting .docutils .literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal} in [<code>settings.py</code>{.docutils
.literal .notranslate}]{.pre} file after creating a new project.</p>
</li>
<li>
<p>Exporters now work on unicode, instead of bytes by default
(<a href="https://github.com/scrapy/scrapy/issues/1080">issue
1080</a>{.reference
.external}). If you use <a href="index.html#scrapy.exporters.PythonItemExporter" title="scrapy.exporters.PythonItemExporter">[<code>PythonItemExporter</code>{.xref .py
.py-class .docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal}, you may want to update your code to disable binary
mode which is now deprecated.</p>
</li>
<li>
<p>Accept XML node names containing dots as valid (<a href="https://github.com/scrapy/scrapy/issues/1533">issue
1533</a>{.reference
.external}).</p>
</li>
<li>
<p>When uploading files or images to S3 (with
[<code>FilesPipeline</code>{.docutils .literal .notranslate}]{.pre} or
[<code>ImagesPipeline</code>{.docutils .literal .notranslate}]{.pre}), the
default ACL policy is now &quot;private&quot; instead of &quot;public&quot;
<strong>Warning: backward incompatible!</strong>. You can use
<a href="index.html#std-setting-FILES_STORE_S3_ACL">[<code>FILES_STORE_S3_ACL</code>{.xref .std .std-setting .docutils
.literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal} to change it.</p>
</li>
<li>
<p>We've reimplemented [<code>canonicalize_url()</code>{.docutils .literal
.notranslate}]{.pre} for more correct output, especially for
URLs with non-ASCII characters (<a href="https://github.com/scrapy/scrapy/issues/1947">issue
1947</a>{.reference
.external}). This could change link extractors output compared
to previous Scrapy versions. This may also invalidate some cache
entries you could still have from pre-1.1 runs. <strong>Warning:
backward incompatible!</strong>.</p>
</li>
</ul>
</li>
</ul>
<p>Keep reading for more details on other improvements and bug fixes.</p>
<p>::: {#beta-python-3-support .section}
[]{#news-betapy3}</p>
<h5 id="beta-python-3-supportheaderlink"><a class="header" href="#beta-python-3-supportheaderlink">Beta Python 3 Support<a href="#beta-python-3-support" title="Permalink to this heading">¶</a>{.headerlink}</a></h5>
<p>We have been <a href="https://github.com/scrapy/scrapy/wiki/Python-3-Porting">hard at work to make Scrapy run on Python
3</a>{.reference
.external}. As a result, now you can run spiders on Python 3.3, 3.4 and
3.5 (Twisted &gt;= 15.5 required). Some features are still missing (and
some may never be ported).</p>
<p>Almost all builtin extensions/middlewares are expected to work. However,
we are aware of some limitations in Python 3:</p>
<ul>
<li>
<p>Scrapy does not work on Windows with Python 3</p>
</li>
<li>
<p>Sending emails is not supported</p>
</li>
<li>
<p>FTP download handler is not supported</p>
</li>
<li>
<p>Telnet console is not supported
:::</p>
</li>
</ul>
<p>::: {#additional-new-features-and-enhancements .section}</p>
<h5 id="additional-new-features-and-enhancementsheaderlink"><a class="header" href="#additional-new-features-and-enhancementsheaderlink">Additional New Features and Enhancements<a href="#additional-new-features-and-enhancements" title="Permalink to this heading">¶</a>{.headerlink}</a></h5>
<ul>
<li>
<p>Scrapy now has a <a href="https://github.com/scrapy/scrapy/blob/master/CODE_OF_CONDUCT.md">Code of
Conduct</a>{.reference
.external} (<a href="https://github.com/scrapy/scrapy/issues/1681">issue
1681</a>{.reference
.external}).</p>
</li>
<li>
<p>Command line tool now has completion for zsh (<a href="https://github.com/scrapy/scrapy/issues/934">issue
934</a>{.reference
.external}).</p>
</li>
<li>
<p>Improvements to [<code>scrapy</code>{.docutils .literal
.notranslate}]{.pre}<code> </code>{.docutils .literal
.notranslate}[<code>shell</code>{.docutils .literal .notranslate}]{.pre}:</p>
<ul>
<li>
<p>Support for bpython and configure preferred Python shell via
[<code>SCRAPY_PYTHON_SHELL</code>{.docutils .literal .notranslate}]{.pre}
(<a href="https://github.com/scrapy/scrapy/issues/1100">issue
1100</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/1444">issue
1444</a>{.reference
.external}).</p>
</li>
<li>
<p>Support URLs without scheme (<a href="https://github.com/scrapy/scrapy/issues/1498">issue
1498</a>{.reference
.external}) <strong>Warning: backward incompatible!</strong></p>
</li>
<li>
<p>Bring back support for relative file path (<a href="https://github.com/scrapy/scrapy/issues/1710">issue
1710</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/1550">issue
1550</a>{.reference
.external}).</p>
</li>
</ul>
</li>
<li>
<p>Added <a href="index.html#std-setting-MEMUSAGE_CHECK_INTERVAL_SECONDS">[<code>MEMUSAGE_CHECK_INTERVAL_SECONDS</code>{.xref .std .std-setting
.docutils .literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal} setting to change default check
interval (<a href="https://github.com/scrapy/scrapy/issues/1282">issue
1282</a>{.reference
.external}).</p>
</li>
<li>
<p>Download handlers are now lazy-loaded on first request using their
scheme (<a href="https://github.com/scrapy/scrapy/issues/1390">issue
1390</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/1421">issue
1421</a>{.reference
.external}).</p>
</li>
<li>
<p>HTTPS download handlers do not force TLS 1.0 anymore; instead,
OpenSSL's [<code>SSLv23_method()/TLS_method()</code>{.docutils .literal
.notranslate}]{.pre} is used allowing to try negotiating with the
remote hosts the highest TLS protocol version it can (<a href="https://github.com/scrapy/scrapy/issues/1794">issue
1794</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/1629">issue
1629</a>{.reference
.external}).</p>
</li>
<li>
<p>[<code>RedirectMiddleware</code>{.docutils .literal .notranslate}]{.pre} now
skips the status codes from [<code>handle_httpstatus_list</code>{.docutils
.literal .notranslate}]{.pre} on spider attribute or in
[<code>Request</code>{.docutils .literal .notranslate}]{.pre}'s
[<code>meta</code>{.docutils .literal .notranslate}]{.pre} key (<a href="https://github.com/scrapy/scrapy/issues/1334">issue
1334</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/1364">issue
1364</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/1447">issue
1447</a>{.reference
.external}).</p>
</li>
<li>
<p>Form submission:</p>
<ul>
<li>
<p>now works with [<code>&lt;button&gt;</code>{.docutils .literal
.notranslate}]{.pre} elements too (<a href="https://github.com/scrapy/scrapy/issues/1469">issue
1469</a>{.reference
.external}).</p>
</li>
<li>
<p>an empty string is now used for submit buttons without a value
(<a href="https://github.com/scrapy/scrapy/issues/1472">issue
1472</a>{.reference
.external})</p>
</li>
</ul>
</li>
<li>
<p>Dict-like settings now have per-key priorities (<a href="https://github.com/scrapy/scrapy/issues/1135">issue
1135</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/1149">issue
1149</a>{.reference
.external} and <a href="https://github.com/scrapy/scrapy/issues/1586">issue
1586</a>{.reference
.external}).</p>
</li>
<li>
<p>Sending non-ASCII emails (<a href="https://github.com/scrapy/scrapy/issues/1662">issue
1662</a>{.reference
.external})</p>
</li>
<li>
<p>[<code>CloseSpider</code>{.docutils .literal .notranslate}]{.pre} and
[<code>SpiderState</code>{.docutils .literal .notranslate}]{.pre} extensions
now get disabled if no relevant setting is set (<a href="https://github.com/scrapy/scrapy/issues/1723">issue
1723</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/1725">issue
1725</a>{.reference
.external}).</p>
</li>
<li>
<p>Added method [<code>ExecutionEngine.close</code>{.docutils .literal
.notranslate}]{.pre} (<a href="https://github.com/scrapy/scrapy/issues/1423">issue
1423</a>{.reference
.external}).</p>
</li>
<li>
<p>Added method [<code>CrawlerRunner.create_crawler</code>{.docutils .literal
.notranslate}]{.pre} (<a href="https://github.com/scrapy/scrapy/issues/1528">issue
1528</a>{.reference
.external}).</p>
</li>
<li>
<p>Scheduler priority queue can now be customized via
<a href="index.html#std-setting-SCHEDULER_PRIORITY_QUEUE">[<code>SCHEDULER_PRIORITY_QUEUE</code>{.xref .std .std-setting .docutils
.literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal} (<a href="https://github.com/scrapy/scrapy/issues/1822">issue
1822</a>{.reference
.external}).</p>
</li>
<li>
<p>[<code>.pps</code>{.docutils .literal .notranslate}]{.pre} links are now
ignored by default in link extractors (<a href="https://github.com/scrapy/scrapy/issues/1835">issue
1835</a>{.reference
.external}).</p>
</li>
<li>
<p>temporary data folder for FTP and S3 feed storages can be customized
using a new <a href="index.html#std-setting-FEED_TEMPDIR">[<code>FEED_TEMPDIR</code>{.xref .std .std-setting .docutils
.literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal} setting (<a href="https://github.com/scrapy/scrapy/issues/1847">issue
1847</a>{.reference
.external}).</p>
</li>
<li>
<p>[<code>FilesPipeline</code>{.docutils .literal .notranslate}]{.pre} and
[<code>ImagesPipeline</code>{.docutils .literal .notranslate}]{.pre} settings
are now instance attributes instead of class attributes, enabling
spider-specific behaviors (<a href="https://github.com/scrapy/scrapy/issues/1891">issue
1891</a>{.reference
.external}).</p>
</li>
<li>
<p>[<code>JsonItemExporter</code>{.docutils .literal .notranslate}]{.pre} now
formats opening and closing square brackets on their own line (first
and last lines of output file) (<a href="https://github.com/scrapy/scrapy/issues/1950">issue
1950</a>{.reference
.external}).</p>
</li>
<li>
<p>If available, [<code>botocore</code>{.docutils .literal .notranslate}]{.pre} is
used for [<code>S3FeedStorage</code>{.docutils .literal .notranslate}]{.pre},
[<code>S3DownloadHandler</code>{.docutils .literal .notranslate}]{.pre} and
[<code>S3FilesStore</code>{.docutils .literal .notranslate}]{.pre} (<a href="https://github.com/scrapy/scrapy/issues/1761">issue
1761</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/1883">issue
1883</a>{.reference
.external}).</p>
</li>
<li>
<p>Tons of documentation updates and related fixes (<a href="https://github.com/scrapy/scrapy/issues/1291">issue
1291</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/1302">issue
1302</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/1335">issue
1335</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/1683">issue
1683</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/1660">issue
1660</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/1642">issue
1642</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/1721">issue
1721</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/1727">issue
1727</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/1879">issue
1879</a>{.reference
.external}).</p>
</li>
<li>
<p>Other refactoring, optimizations and cleanup (<a href="https://github.com/scrapy/scrapy/issues/1476">issue
1476</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/1481">issue
1481</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/1477">issue
1477</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/1315">issue
1315</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/1290">issue
1290</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/1750">issue
1750</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/1881">issue
1881</a>{.reference
.external}).
:::</p>
</li>
</ul>
<p>::: {#deprecations-and-removals .section}</p>
<h5 id="deprecations-and-removalsheaderlink"><a class="header" href="#deprecations-and-removalsheaderlink">Deprecations and Removals<a href="#deprecations-and-removals" title="Permalink to this heading">¶</a>{.headerlink}</a></h5>
<ul>
<li>
<p>Added [<code>to_bytes</code>{.docutils .literal .notranslate}]{.pre} and
[<code>to_unicode</code>{.docutils .literal .notranslate}]{.pre}, deprecated
[<code>str_to_unicode</code>{.docutils .literal .notranslate}]{.pre} and
[<code>unicode_to_str</code>{.docutils .literal .notranslate}]{.pre} functions
(<a href="https://github.com/scrapy/scrapy/issues/778">issue 778</a>{.reference
.external}).</p>
</li>
<li>
<p>[<code>binary_is_text</code>{.docutils .literal .notranslate}]{.pre} is
introduced, to replace use of [<code>isbinarytext</code>{.docutils .literal
.notranslate}]{.pre} (but with inverse return value) (<a href="https://github.com/scrapy/scrapy/issues/1851">issue
1851</a>{.reference
.external})</p>
</li>
<li>
<p>The [<code>optional_features</code>{.docutils .literal .notranslate}]{.pre} set
has been removed (<a href="https://github.com/scrapy/scrapy/issues/1359">issue
1359</a>{.reference
.external}).</p>
</li>
<li>
<p>The [<code>--lsprof</code>{.docutils .literal .notranslate}]{.pre} command line
option has been removed (<a href="https://github.com/scrapy/scrapy/issues/1689">issue
1689</a>{.reference
.external}). <strong>Warning: backward incompatible</strong>, but doesn't break
user code.</p>
</li>
<li>
<p>The following datatypes were deprecated (<a href="https://github.com/scrapy/scrapy/issues/1720">issue
1720</a>{.reference
.external}):</p>
<ul>
<li>
<p>[<code>scrapy.utils.datatypes.MultiValueDictKeyError</code>{.docutils
.literal .notranslate}]{.pre}</p>
</li>
<li>
<p>[<code>scrapy.utils.datatypes.MultiValueDict</code>{.docutils .literal
.notranslate}]{.pre}</p>
</li>
<li>
<p>[<code>scrapy.utils.datatypes.SiteNode</code>{.docutils .literal
.notranslate}]{.pre}</p>
</li>
</ul>
</li>
<li>
<p>The previously bundled [<code>scrapy.xlib.pydispatch</code>{.docutils .literal
.notranslate}]{.pre} library was deprecated and replaced by
<a href="https://pypi.org/project/PyDispatcher/">pydispatcher</a>{.reference
.external}.
:::</p>
</li>
</ul>
<p>::: {#relocations .section}</p>
<h5 id="relocationsheaderlink"><a class="header" href="#relocationsheaderlink">Relocations<a href="#relocations" title="Permalink to this heading">¶</a>{.headerlink}</a></h5>
<ul>
<li>
<p>[<code>telnetconsole</code>{.docutils .literal .notranslate}]{.pre} was
relocated to [<code>extensions/</code>{.docutils .literal .notranslate}]{.pre}
(<a href="https://github.com/scrapy/scrapy/issues/1524">issue
1524</a>{.reference
.external}).</p>
<ul>
<li>Note: telnet is not enabled on Python 3
(<a href="https://github.com/scrapy/scrapy/pull/1524#issuecomment-146985595">https://github.com/scrapy/scrapy/pull/1524#issuecomment-146985595</a>{.reference
.external})
:::</li>
</ul>
</li>
</ul>
<p>::: {#bugfixes .section}</p>
<h5 id="bugfixesheaderlink"><a class="header" href="#bugfixesheaderlink">Bugfixes<a href="#bugfixes" title="Permalink to this heading">¶</a>{.headerlink}</a></h5>
<ul>
<li>
<p>Scrapy does not retry requests that got a [<code>HTTP</code>{.docutils .literal
.notranslate}]{.pre}<code> </code>{.docutils .literal
.notranslate}[<code>400</code>{.docutils .literal
.notranslate}]{.pre}<code> </code>{.docutils .literal
.notranslate}[<code>Bad</code>{.docutils .literal
.notranslate}]{.pre}<code> </code>{.docutils .literal
.notranslate}[<code>Request</code>{.docutils .literal .notranslate}]{.pre}
response anymore (<a href="https://github.com/scrapy/scrapy/issues/1289">issue
1289</a>{.reference
.external}). <strong>Warning: backward incompatible!</strong></p>
</li>
<li>
<p>Support empty password for http_proxy config (<a href="https://github.com/scrapy/scrapy/issues/1274">issue
1274</a>{.reference
.external}).</p>
</li>
<li>
<p>Interpret [<code>application/x-json</code>{.docutils .literal
.notranslate}]{.pre} as [<code>TextResponse</code>{.docutils .literal
.notranslate}]{.pre} (<a href="https://github.com/scrapy/scrapy/issues/1333">issue
1333</a>{.reference
.external}).</p>
</li>
<li>
<p>Support link rel attribute with multiple values (<a href="https://github.com/scrapy/scrapy/issues/1201">issue
1201</a>{.reference
.external}).</p>
</li>
<li>
<p>Fixed [<code>scrapy.http.FormRequest.from_response</code>{.docutils .literal
.notranslate}]{.pre} when there is a [<code>&lt;base&gt;</code>{.docutils .literal
.notranslate}]{.pre} tag (<a href="https://github.com/scrapy/scrapy/issues/1564">issue
1564</a>{.reference
.external}).</p>
</li>
<li>
<p>Fixed <a href="index.html#std-setting-TEMPLATES_DIR">[<code>TEMPLATES_DIR</code>{.xref .std .std-setting .docutils .literal
.notranslate}]{.pre}</a>{.hoverxref
.tooltip .reference .internal} handling (<a href="https://github.com/scrapy/scrapy/issues/1575">issue
1575</a>{.reference
.external}).</p>
</li>
<li>
<p>Various [<code>FormRequest</code>{.docutils .literal .notranslate}]{.pre} fixes
(<a href="https://github.com/scrapy/scrapy/issues/1595">issue
1595</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/1596">issue
1596</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/1597">issue
1597</a>{.reference
.external}).</p>
</li>
<li>
<p>Makes [<code>_monkeypatches</code>{.docutils .literal .notranslate}]{.pre} more
robust (<a href="https://github.com/scrapy/scrapy/issues/1634">issue
1634</a>{.reference
.external}).</p>
</li>
<li>
<p>Fixed bug on [<code>XMLItemExporter</code>{.docutils .literal
.notranslate}]{.pre} with non-string fields in items (<a href="https://github.com/scrapy/scrapy/issues/1738">issue
1738</a>{.reference
.external}).</p>
</li>
<li>
<p>Fixed startproject command in macOS (<a href="https://github.com/scrapy/scrapy/issues/1635">issue
1635</a>{.reference
.external}).</p>
</li>
<li>
<p>Fixed <a href="index.html#scrapy.exporters.PythonItemExporter" title="scrapy.exporters.PythonItemExporter">[<code>PythonItemExporter</code>{.xref .py .py-class .docutils .literal
.notranslate}]{.pre}</a>{.reference
.internal} and CSVExporter for non-string item types (<a href="https://github.com/scrapy/scrapy/issues/1737">issue
1737</a>{.reference
.external}).</p>
</li>
<li>
<p>Various logging related fixes (<a href="https://github.com/scrapy/scrapy/issues/1294">issue
1294</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/1419">issue
1419</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/1263">issue
1263</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/1624">issue
1624</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/1654">issue
1654</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/1722">issue
1722</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/issues/1726">issue
1726</a>{.reference
.external} and <a href="https://github.com/scrapy/scrapy/issues/1303">issue
1303</a>{.reference
.external}).</p>
</li>
<li>
<p>Fixed bug in [<code>utils.template.render_templatefile()</code>{.docutils
.literal .notranslate}]{.pre} (<a href="https://github.com/scrapy/scrapy/issues/1212">issue
1212</a>{.reference
.external}).</p>
</li>
<li>
<p>sitemaps extraction from [<code>robots.txt</code>{.docutils .literal
.notranslate}]{.pre} is now case-insensitive (<a href="https://github.com/scrapy/scrapy/issues/1902">issue
1902</a>{.reference
.external}).</p>
</li>
<li>
<p>HTTPS+CONNECT tunnels could get mixed up when using multiple proxies
to same remote host (<a href="https://github.com/scrapy/scrapy/issues/1912">issue
1912</a>{.reference
.external}).
:::
:::</p>
</li>
</ul>
<p>::: {#scrapy-1-0-7-2017-03-03 .section}
[]{#release-1-0-7}</p>
<h4 id="scrapy-107-2017-03-03headerlink"><a class="header" href="#scrapy-107-2017-03-03headerlink">Scrapy 1.0.7 (2017-03-03)<a href="#scrapy-1-0-7-2017-03-03" title="Permalink to this heading">¶</a>{.headerlink}</a></h4>
<ul>
<li>Packaging fix: disallow unsupported Twisted versions in setup.py
:::</li>
</ul>
<p>::: {#scrapy-1-0-6-2016-05-04 .section}
[]{#release-1-0-6}</p>
<h4 id="scrapy-106-2016-05-04headerlink"><a class="header" href="#scrapy-106-2016-05-04headerlink">Scrapy 1.0.6 (2016-05-04)<a href="#scrapy-1-0-6-2016-05-04" title="Permalink to this heading">¶</a>{.headerlink}</a></h4>
<ul>
<li>
<p>FIX: RetryMiddleware is now robust to non-standard HTTP status codes
(<a href="https://github.com/scrapy/scrapy/issues/1857">issue
1857</a>{.reference
.external})</p>
</li>
<li>
<p>FIX: Filestorage HTTP cache was checking wrong modified time (<a href="https://github.com/scrapy/scrapy/issues/1875">issue
1875</a>{.reference
.external})</p>
</li>
<li>
<p>DOC: Support for Sphinx 1.4+ (<a href="https://github.com/scrapy/scrapy/issues/1893">issue
1893</a>{.reference
.external})</p>
</li>
<li>
<p>DOC: Consistency in selectors examples (<a href="https://github.com/scrapy/scrapy/issues/1869">issue
1869</a>{.reference
.external})
:::</p>
</li>
</ul>
<p>::: {#scrapy-1-0-5-2016-02-04 .section}
[]{#release-1-0-5}</p>
<h4 id="scrapy-105-2016-02-04headerlink"><a class="header" href="#scrapy-105-2016-02-04headerlink">Scrapy 1.0.5 (2016-02-04)<a href="#scrapy-1-0-5-2016-02-04" title="Permalink to this heading">¶</a>{.headerlink}</a></h4>
<ul>
<li>
<p>FIX: [Backport] Ignore bogus links in LinkExtractors (fixes <a href="https://github.com/scrapy/scrapy/issues/907">issue
907</a>{.reference
.external}, <a href="https://github.com/scrapy/scrapy/commit/108195e">commit
108195e</a>{.reference
.external})</p>
</li>
<li>
<p>TST: Changed buildbot makefile to use 'pytest' (<a href="https://github.com/scrapy/scrapy/commit/1f3d90a">commit
1f3d90a</a>{.reference
.external})</p>
</li>
<li>
<p>DOC: Fixed typos in tutorial and media-pipeline (<a href="https://github.com/scrapy/scrapy/commit/808a9ea">commit
808a9ea</a>{.reference
.external} and <a href="https://github.com/scrapy/scrapy/commit/803bd87">commit
803bd87</a>{.reference
.external})</p>
</li>
<li>
<p>DOC: Add AjaxCrawlMiddleware to DOWNLOADER_MIDDLEWARES_BASE in
settings docs (<a href="https://github.com/scrapy/scrapy/commit/aa94121">commit
aa94121</a>{.reference
.external})
:::</p>
</li>
</ul>
<p>::: {#scrapy-1-0-4-2015-12-30 .section}
[]{#release-1-0-4}</p>
<h4 id="scrapy-104-2015-12-30headerlink"><a class="header" href="#scrapy-104-2015-12-30headerlink">Scrapy 1.0.4 (2015-12-30)<a href="#scrapy-1-0-4-2015-12-30" title="Permalink to this heading">¶</a>{.headerlink}</a></h4>
<ul>
<li>
<p>Ignoring xlib/tx folder, depending on Twisted version. (<a href="https://github.com/scrapy/scrapy/commit/7dfa979">commit
7dfa979</a>{.reference
.external})</p>
</li>
<li>
<p>Run on new travis-ci infra (<a href="https://github.com/scrapy/scrapy/commit/6e42f0b">commit
6e42f0b</a>{.reference
.external})</p>
</li>
<li>
<p>Spelling fixes (<a href="https://github.com/scrapy/scrapy/commit/823a1cc">commit
823a1cc</a>{.reference
.external})</p>
</li>
<li>
<p>escape nodename in xmliter regex (<a href="https://github.com/scrapy/scrapy/commit/da3c155">commit
da3c155</a>{.reference
.external})</p>
</li>
<li>
<p>test xml nodename with dots (<a href="https://github.com/scrapy/scrapy/commit/4418fc3">commit
4418fc3</a>{.reference
.external})</p>
</li>
<li>
<p>TST don't use broken Pillow version in tests (<a href="https://github.com/scrapy/scrapy/commit/a55078c">commit
a55078c</a>{.reference
.external})</p>
</li>
<li>
<p>disable log on version command. closes #1426 (<a href="https://github.com/scrapy/scrapy/commit/86fc330">commit
86fc330</a>{.reference
.external})</p>
</li>
<li>
<p>disable log on startproject command (<a href="https://github.com/scrapy/scrapy/commit/db4c9fe">commit
db4c9fe</a>{.reference
.external})</p>
</li>
<li>
<p>Add PyPI download stats badge (<a href="https://github.com/scrapy/scrapy/commit/df2b944">commit
df2b944</a>{.reference
.external})</p>
</li>
<li>
<p>don't run tests twice on Travis if a PR is made from a scrapy/scrapy
branch (<a href="https://github.com/scrapy/scrapy/commit/a83ab41">commit
a83ab41</a>{.reference
.external})</p>
</li>
<li>
<p>Add Python 3 porting status badge to the README (<a href="https://github.com/scrapy/scrapy/commit/73ac80d">commit
73ac80d</a>{.reference
.external})</p>
</li>
<li>
<p>fixed RFPDupeFilter persistence (<a href="https://github.com/scrapy/scrapy/commit/97d080e">commit
97d080e</a>{.reference
.external})</p>
</li>
<li>
<p>TST a test to show that dupefilter persistence is not working
(<a href="https://github.com/scrapy/scrapy/commit/97f2fb3">commit
97f2fb3</a>{.reference
.external})</p>
</li>
<li>
<p>explicit close file on <a href="file://">file://</a>{.reference .external}
scheme handler (<a href="https://github.com/scrapy/scrapy/commit/d9b4850">commit
d9b4850</a>{.reference
.external})</p>
</li>
<li>
<p>Disable dupefilter in shell (<a href="https://github.com/scrapy/scrapy/commit/c0d0734">commit
c0d0734</a>{.reference
.external})</p>
</li>
<li>
<p>DOC: Add captions to toctrees which appear in sidebar (<a href="https://github.com/scrapy/scrapy/commit/aa239ad">commit
aa239ad</a>{.reference
.external})</p>
</li>
<li>
<p>DOC Removed pywin32 from install instructions as it's already
declared as dependency. (<a href="https://github.com/scrapy/scrapy/commit/10eb400">commit
10eb400</a>{.reference
.external})</p>
</li>
<li>
<p>Added installation notes about using Conda for Windows and other
OSes. (<a href="https://github.com/scrapy/scrapy/commit/1c3600a">commit
1c3600a</a>{.reference
.external})</p>
</li>
<li>
<p>Fixed minor grammar issues. (<a href="https://github.com/scrapy/scrapy/commit/7f4ddd5">commit
7f4ddd5</a>{.reference
.external})</p>
</li>
<li>
<p>fixed a typo in the documentation. (<a href="https://github.com/scrapy/scrapy/commit/b71f677">commit
b71f677</a>{.reference
.external})</p>
</li>
<li>
<p>Version 1 now exists (<a href="https://github.com/scrapy/scrapy/commit/5456c0e">commit
5456c0e</a>{.reference
.external})</p>
</li>
<li>
<p>fix another invalid xpath error (<a href="https://github.com/scrapy/scrapy/commit/0a1366e">commit
0a1366e</a>{.reference
.external})</p>
</li>
<li>
<p>fix ValueError: Invalid XPath: //div/[id=&quot;not-exists&quot;]/text() on
selectors.rst (<a href="https://github.com/scrapy/scrapy/commit/ca8d60f">commit
ca8d60f</a>{.reference
.external})</p>
</li>
<li>
<p>Typos corrections (<a href="https://github.com/scrapy/scrapy/commit/7067117">commit
7067117</a>{.reference
.external})</p>
</li>
<li>
<p>fix typos in downloader-middleware.rst and exceptions.rst, middlware
-&gt; middleware (<a href="https://github.com/scrapy/scrapy/commit/32f115c">commit
32f115c</a>{.reference
.external})</p>
</li>
<li>
<p>Add note to Ubuntu install section about Debian compatibility
(<a href="https://github.com/scrapy/scrapy/commit/23fda69">commit
23fda69</a>{.reference
.external})</p>
</li>
<li>
<p>Replace alternative macOS install workaround with virtualenv
(<a href="https://github.com/scrapy/scrapy/commit/98b63ee">commit
98b63ee</a>{.reference
.external})</p>
</li>
<li>
<p>Reference Homebrew's homepage for installation instructions (<a href="https://github.com/scrapy/scrapy/commit/1925db1">commit
1925db1</a>{.reference
.external})</p>
</li>
<li>
<p>Add oldest supported tox version to contributing docs (<a href="https://github.com/scrapy/scrapy/commit/5d10d6d">commit
5d10d6d</a>{.reference
.external})</p>
</li>
<li>
<p>Note in install docs about pip being already included in
python&gt;=2.7.9 (<a href="https://github.com/scrapy/scrapy/commit/85c980e">commit
85c980e</a>{.reference
.external})</p>
</li>
<li>
<p>Add non-python dependencies to Ubuntu install section in the docs
(<a href="https://github.com/scrapy/scrapy/commit/fbd010d">commit
fbd010d</a>{.reference
.external})</p>
</li>
<li>
<p>Add macOS installation section to docs (<a href="https://github.com/scrapy/scrapy/commit/d8f4cba">commit
d8f4cba</a>{.reference
.external})</p>
</li>
<li>
<p>DOC(ENH): specify path to rtd theme explicitly (<a href="https://github.com/scrapy/scrapy/commit/de73b1a">commit
de73b1a</a>{.reference
.external})</p>
</li>
<li>
<p>minor: scrapy.Spider docs grammar (<a href="https://github.com/scrapy/scrapy/commit/1ddcc7b">commit
1ddcc7b</a>{.reference
.external})</p>
</li>
<li>
<p>Make common practices sample code match the comments (<a href="https://github.com/scrapy/scrapy/commit/1b85bcf">commit
1b85bcf</a>{.reference
.external})</p>
</li>
<li>
<p>nextcall repetitive calls (heartbeats). (<a href="https://github.com/scrapy/scrapy/commit/55f7104">commit
55f7104</a>{.reference
.external})</p>
</li>
<li>
<p>Backport fix compatibility with Twisted 15.4.0 (<a href="https://github.com/scrapy/scrapy/commit/b262411">commit
b262411</a>{.reference
.external})</p>
</li>
<li>
<p>pin pytest to 2.7.3 (<a href="https://github.com/scrapy/scrapy/commit/a6535c2">commit
a6535c2</a>{.reference
.external})</p>
</li>
<li>
<p>Merge pull request #1512 from mgedmin/patch-1 (<a href="https://github.com/scrapy/scrapy/commit/8876111">commit
8876111</a>{.reference
.external})</p>
</li>
<li>
<p>Merge pull request #1513 from mgedmin/patch-2 (<a href="https://github.com/scrapy/scrapy/commit/5d4daf8">commit
5d4daf8</a>{.reference
.external})</p>
</li>
<li>
<p>Typo (<a href="https://github.com/scrapy/scrapy/commit/f8d0682">commit
f8d0682</a>{.reference
.external})</p>
</li>
<li>
<p>Fix list formatting (<a href="https://github.com/scrapy/scrapy/commit/5f83a93">commit
5f83a93</a>{.reference
.external})</p>
</li>
<li>
<p>fix Scrapy squeue tests after recent changes to queuelib (<a href="https://github.com/scrapy/scrapy/commit/3365c01">commit
3365c01</a>{.reference
.external})</p>
</li>
<li>
<p>Merge pull request #1475 from rweindl/patch-1 (<a href="https://github.com/scrapy/scrapy/commit/2d688cd">commit
2d688cd</a>{.reference
.external})</p>
</li>
<li>
<p>Update tutorial.rst (<a href="https://github.com/scrapy/scrapy/commit/fbc1f25">commit
fbc1f25</a>{.reference
.external})</p>
</li>
<li>
<p>Merge pull request #1449 from rhoekman/patch-1 (<a href="https://github.com/scrapy/scrapy/commit/7d6538c">commit
7d6538c</a>{.reference
.external})</p>
</li>
<li>
<p>Small grammatical change (<a href="https://github.com/scrapy/scrapy/commit/8752294">commit
8752294</a>{.reference
.external})</p>
</li>
<li>
<p>Add openssl version to version command (<a href="https://github.com/scrapy/scrapy/commit/13c45ac">commit
13c45ac</a>{.reference
.external})
:::</p>
</li>
</ul>
<p>::: {#scrapy-1-0-3-2015-08-11 .section}
[]{#release-1-0-3}</p>
<h4 id="scrapy-103-2015-08-11headerlink"><a class="header" href="#scrapy-103-2015-08-11headerlink">Scrapy 1.0.3 (2015-08-11)<a href="#scrapy-1-0-3-2015-08-11" title="Permalink to this heading">¶</a>{.headerlink}</a></h4>
<ul>
<li>
<p>add service_identity to Scrapy install_requires (<a href="https://github.com/scrapy/scrapy/commit/cbc2501">commit
cbc2501</a>{.reference
.external})</p>
</li>
<li>
<p>Workaround for travis#296 (<a href="https://github.com/scrapy/scrapy/commit/66af9cd">commit
66af9cd</a>{.reference
.external})
:::</p>
</li>
</ul>
<p>::: {#scrapy-1-0-2-2015-08-06 .section}
[]{#release-1-0-2}</p>
<h4 id="scrapy-102-2015-08-06headerlink"><a class="header" href="#scrapy-102-2015-08-06headerlink">Scrapy 1.0.2 (2015-08-06)<a href="#scrapy-1-0-2-2015-08-06" title="Permalink to this heading">¶</a>{.headerlink}</a></h4>
<ul>
<li>
<p>Twisted 15.3.0 does not raises PicklingError serializing lambda
functions (<a href="https://github.com/scrapy/scrapy/commit/b04dd7d">commit
b04dd7d</a>{.reference
.external})</p>
</li>
<li>
<p>Minor method name fix (<a href="https://github.com/scrapy/scrapy/commit/6f85c7f">commit
6f85c7f</a>{.reference
.external})</p>
</li>
<li>
<p>minor: scrapy.Spider grammar and clarity (<a href="https://github.com/scrapy/scrapy/commit/9c9d2e0">commit
9c9d2e0</a>{.reference
.external})</p>
</li>
<li>
<p>Put a blurb about support channels in CONTRIBUTING (<a href="https://github.com/scrapy/scrapy/commit/c63882b">commit
c63882b</a>{.reference
.external})</p>
</li>
<li>
<p>Fixed typos (<a href="https://github.com/scrapy/scrapy/commit/a9ae7b0">commit
a9ae7b0</a>{.reference
.external})</p>
</li>
<li>
<p>Fix doc reference. (<a href="https://github.com/scrapy/scrapy/commit/7c8a4fe">commit
7c8a4fe</a>{.reference
.external})
:::</p>
</li>
</ul>
<p>::: {#scrapy-1-0-1-2015-07-01 .section}
[]{#release-1-0-1}</p>
<h4 id="scrapy-101-2015-07-01headerlink"><a class="header" href="#scrapy-101-2015-07-01headerlink">Scrapy 1.0.1 (2015-07-01)<a href="#scrapy-1-0-1-2015-07-01" title="Permalink to this heading">¶</a>{.headerlink}</a></h4>
<ul>
<li>Unquote request path before passing to FTPClient, it already escape
paths ([commit</li>
</ul>

                    </main>

                    <nav class="nav-wrapper" aria-label="Page navigation">
                        <!-- Mobile navigation buttons -->
                            <a rel="prev" href="../Scrapy/Scrapy8.html" class="mobile-nav-chapters previous" title="Previous chapter" aria-label="Previous chapter" aria-keyshortcuts="Left">
                                <i class="fa fa-angle-left"></i>
                            </a>

                            <a rel="next" href="../Scrapy/Scrapy10.html" class="mobile-nav-chapters next" title="Next chapter" aria-label="Next chapter" aria-keyshortcuts="Right">
                                <i class="fa fa-angle-right"></i>
                            </a>

                        <div style="clear: both"></div>
                    </nav>
                </div>
            </div>

            <nav class="nav-wide-wrapper" aria-label="Page navigation">
                    <a rel="prev" href="../Scrapy/Scrapy8.html" class="nav-chapters previous" title="Previous chapter" aria-label="Previous chapter" aria-keyshortcuts="Left">
                        <i class="fa fa-angle-left"></i>
                    </a>

                    <a rel="next" href="../Scrapy/Scrapy10.html" class="nav-chapters next" title="Next chapter" aria-label="Next chapter" aria-keyshortcuts="Right">
                        <i class="fa fa-angle-right"></i>
                    </a>
            </nav>

        </div>




        <script>
            window.playground_copyable = true;
        </script>



        <script src="../clipboard.min.js"></script>
        <script src="../highlight.js"></script>
        <script src="../book.js"></script>

        <!-- Custom JS scripts -->


    </div>
    </body>
</html>
