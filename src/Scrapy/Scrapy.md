---
generator: "Docutils 0.17.1: http://docutils.sourceforge.net/"
lang: en
title: Scrapy 2.11.0 documentation
viewport: width=device-width, initial-scale=1.0
---

::: wy-grid-for-nav
::: wy-side-scroll
::: wy-side-nav-search
[Scrapy](#){.icon .icon-home}

::: version
master
:::
:::

::: {.wy-menu .wy-menu-vertical spy="affix" role="navigation" aria-label="Navigation menu"}
[First steps]{.caption-text}

-   [Scrapy at a glance](index.html#document-intro/overview){.reference
    .internal}
-   [Installation guide](index.html#document-intro/install){.reference
    .internal}
-   [Scrapy Tutorial](index.html#document-intro/tutorial){.reference
    .internal}
-   [Examples](index.html#document-intro/examples){.reference .internal}

[Basic concepts]{.caption-text}

-   [Command line tool](index.html#document-topics/commands){.reference
    .internal}
-   [Spiders](index.html#document-topics/spiders){.reference .internal}
-   [Selectors](index.html#document-topics/selectors){.reference
    .internal}
-   [Items](index.html#document-topics/items){.reference .internal}
-   [Item Loaders](index.html#document-topics/loaders){.reference
    .internal}
-   [Scrapy shell](index.html#document-topics/shell){.reference
    .internal}
-   [Item Pipeline](index.html#document-topics/item-pipeline){.reference
    .internal}
-   [Feed exports](index.html#document-topics/feed-exports){.reference
    .internal}
-   [Requests and
    Responses](index.html#document-topics/request-response){.reference
    .internal}
-   [Link
    Extractors](index.html#document-topics/link-extractors){.reference
    .internal}
-   [Settings](index.html#document-topics/settings){.reference
    .internal}
-   [Exceptions](index.html#document-topics/exceptions){.reference
    .internal}

[Built-in services]{.caption-text}

-   [Logging](index.html#document-topics/logging){.reference .internal}
-   [Stats Collection](index.html#document-topics/stats){.reference
    .internal}
-   [Sending e-mail](index.html#document-topics/email){.reference
    .internal}
-   [Telnet
    Console](index.html#document-topics/telnetconsole){.reference
    .internal}

[Solving specific problems]{.caption-text}

-   [Frequently Asked Questions](index.html#document-faq){.reference
    .internal}
-   [Debugging Spiders](index.html#document-topics/debug){.reference
    .internal}
-   [Spiders Contracts](index.html#document-topics/contracts){.reference
    .internal}
-   [Common Practices](index.html#document-topics/practices){.reference
    .internal}
-   [Broad Crawls](index.html#document-topics/broad-crawls){.reference
    .internal}
-   [Using your browser's Developer Tools for
    scraping](index.html#document-topics/developer-tools){.reference
    .internal}
-   [Selecting dynamically-loaded
    content](index.html#document-topics/dynamic-content){.reference
    .internal}
-   [Debugging memory
    leaks](index.html#document-topics/leaks){.reference .internal}
-   [Downloading and processing files and
    images](index.html#document-topics/media-pipeline){.reference
    .internal}
-   [Deploying Spiders](index.html#document-topics/deploy){.reference
    .internal}
-   [AutoThrottle
    extension](index.html#document-topics/autothrottle){.reference
    .internal}
-   [Benchmarking](index.html#document-topics/benchmarking){.reference
    .internal}
-   [Jobs: pausing and resuming
    crawls](index.html#document-topics/jobs){.reference .internal}
-   [Coroutines](index.html#document-topics/coroutines){.reference
    .internal}
-   [asyncio](index.html#document-topics/asyncio){.reference .internal}

[Extending Scrapy]{.caption-text}

-   [Architecture
    overview](index.html#document-topics/architecture){.reference
    .internal}
-   [Add-ons](index.html#document-topics/addons){.reference .internal}
-   [Downloader
    Middleware](index.html#document-topics/downloader-middleware){.reference
    .internal}
-   [Spider
    Middleware](index.html#document-topics/spider-middleware){.reference
    .internal}
-   [Extensions](index.html#document-topics/extensions){.reference
    .internal}
-   [Signals](index.html#document-topics/signals){.reference .internal}
-   [Scheduler](index.html#document-topics/scheduler){.reference
    .internal}
-   [Item Exporters](index.html#document-topics/exporters){.reference
    .internal}
-   [Components](index.html#document-topics/components){.reference
    .internal}
-   [Core API](index.html#document-topics/api){.reference .internal}

[All the rest]{.caption-text}

-   [Release notes](index.html#document-news){.reference .internal}
-   [Contributing to
    Scrapy](index.html#document-contributing){.reference .internal}
-   [Versioning and API
    stability](index.html#document-versioning){.reference .internal}
:::
:::

::: {.section .wy-nav-content-wrap toggle="wy-nav-shift"}
[Scrapy](#)

::: wy-nav-content
::: rst-content
::: {role="navigation" aria-label="Page navigation"}
-   [](#){.icon .icon-home} »
-   Scrapy 2.11.0 documentation
-   [Edit on
    GitHub](https://github.com/scrapy/scrapy/blob/master/docs/index){.fa
    .fa-github}

------------------------------------------------------------------------
:::

::: {.document role="main" itemscope="itemscope" itemtype="http://schema.org/Article"}
::: {itemprop="articleBody"}
::: {#scrapy-version-documentation .section}
[]{#topics-index}

# Scrapy 2.11 documentation[¶](#scrapy-version-documentation "Permalink to this heading"){.headerlink}

Scrapy is a fast high-level [web
crawling](https://en.wikipedia.org/wiki/Web_crawler){.reference
.external} and [web
scraping](https://en.wikipedia.org/wiki/Web_scraping){.reference
.external} framework, used to crawl websites and extract structured data
from their pages. It can be used for a wide range of purposes, from data
mining to monitoring and automated testing.

::: {#getting-help .section}
[]{#id1}

## Getting help[¶](#getting-help "Permalink to this heading"){.headerlink}

Having trouble? We'd like to help!

-   Try the [[FAQ]{.doc}](index.html#document-faq){.reference .internal}
    -- it's got answers to some common questions.

-   Looking for specific information? Try the [[Index]{.std
    .std-ref}](genindex.html){.reference .internal} or [[Module
    Index]{.std .std-ref}](py-modindex.html){.reference .internal}.

-   Ask or search questions in [StackOverflow using the scrapy
    tag](https://stackoverflow.com/tags/scrapy){.reference .external}.

-   Ask or search questions in the [Scrapy
    subreddit](https://www.reddit.com/r/scrapy/){.reference .external}.

-   Search for questions on the archives of the [scrapy-users mailing
    list](https://groups.google.com/forum/#!forum/scrapy-users){.reference
    .external}.

-   Ask a question in the [#scrapy IRC
    channel](irc://irc.freenode.net/scrapy){.reference .external},

-   Report bugs with Scrapy in our [issue
    tracker](https://github.com/scrapy/scrapy/issues){.reference
    .external}.

-   Join the Discord community [Scrapy
    Discord](https://discord.gg/mv3yErfpvq){.reference .external}.
:::

::: {#first-steps .section}
## First steps[¶](#first-steps "Permalink to this heading"){.headerlink}

::: {.toctree-wrapper .compound}
[]{#document-intro/overview}

::: {#scrapy-at-a-glance .section}
[]{#intro-overview}

### Scrapy at a glance[¶](#scrapy-at-a-glance "Permalink to this heading"){.headerlink}

Scrapy (/ˈskreɪpaɪ/) is an application framework for crawling web sites
and extracting structured data which can be used for a wide range of
useful applications, like data mining, information processing or
historical archival.

Even though Scrapy was originally designed for [web
scraping](https://en.wikipedia.org/wiki/Web_scraping){.reference
.external}, it can also be used to extract data using APIs (such as
[Amazon Associates Web
Services](https://affiliate-program.amazon.com/gp/advertising/api/detail/main.html){.reference
.external}) or as a general purpose web crawler.

::: {#walk-through-of-an-example-spider .section}
#### Walk-through of an example spider[¶](#walk-through-of-an-example-spider "Permalink to this heading"){.headerlink}

In order to show you what Scrapy brings to the table, we'll walk you
through an example of a Scrapy Spider using the simplest way to run a
spider.

Here's the code for a spider that scrapes famous quotes from website
[https://quotes.toscrape.com](https://quotes.toscrape.com){.reference
.external}, following the pagination:

::: {.highlight-python .notranslate}
::: highlight
    import scrapy


    class QuotesSpider(scrapy.Spider):
        name = "quotes"
        start_urls = [
            "https://quotes.toscrape.com/tag/humor/",
        ]

        def parse(self, response):
            for quote in response.css("div.quote"):
                yield {
                    "author": quote.xpath("span/small/text()").get(),
                    "text": quote.css("span.text::text").get(),
                }

            next_page = response.css('li.next a::attr("href")').get()
            if next_page is not None:
                yield response.follow(next_page, self.parse)
:::
:::

Put this in a text file, name it to something like
[`quotes_spider.py`{.docutils .literal .notranslate}]{.pre} and run the
spider using the [[`runspider`{.xref .std .std-command .docutils
.literal
.notranslate}]{.pre}](index.html#std-command-runspider){.hoverxref
.tooltip .reference .internal} command:

::: {.highlight-default .notranslate}
::: highlight
    scrapy runspider quotes_spider.py -o quotes.jsonl
:::
:::

When this finishes you will have in the [`quotes.jsonl`{.docutils
.literal .notranslate}]{.pre} file a list of the quotes in JSON Lines
format, containing text and author, looking like this:

::: {.highlight-default .notranslate}
::: highlight
    {"author": "Jane Austen", "text": "\u201cThe person, be it gentleman or lady, who has not pleasure in a good novel, must be intolerably stupid.\u201d"}
    {"author": "Steve Martin", "text": "\u201cA day without sunshine is like, you know, night.\u201d"}
    {"author": "Garrison Keillor", "text": "\u201cAnyone who thinks sitting in church can make you a Christian must also think that sitting in a garage can make you a car.\u201d"}
    ...
:::
:::

::: {#what-just-happened .section}
##### What just happened?[¶](#what-just-happened "Permalink to this heading"){.headerlink}

When you ran the command [`scrapy`{.docutils .literal
.notranslate}]{.pre}` `{.docutils .literal
.notranslate}[`runspider`{.docutils .literal
.notranslate}]{.pre}` `{.docutils .literal
.notranslate}[`quotes_spider.py`{.docutils .literal
.notranslate}]{.pre}, Scrapy looked for a Spider definition inside it
and ran it through its crawler engine.

The crawl started by making requests to the URLs defined in the
[`start_urls`{.docutils .literal .notranslate}]{.pre} attribute (in this
case, only the URL for quotes in *humor* category) and called the
default callback method [`parse`{.docutils .literal
.notranslate}]{.pre}, passing the response object as an argument. In the
[`parse`{.docutils .literal .notranslate}]{.pre} callback, we loop
through the quote elements using a CSS Selector, yield a Python dict
with the extracted quote text and author, look for a link to the next
page and schedule another request using the same [`parse`{.docutils
.literal .notranslate}]{.pre} method as callback.

Here you notice one of the main advantages about Scrapy: requests are
[[scheduled and processed asynchronously]{.std
.std-ref}](index.html#topics-architecture){.hoverxref .tooltip
.reference .internal}. This means that Scrapy doesn't need to wait for a
request to be finished and processed, it can send another request or do
other things in the meantime. This also means that other requests can
keep going even if some request fails or an error happens while handling
it.

While this enables you to do very fast crawls (sending multiple
concurrent requests at the same time, in a fault-tolerant way) Scrapy
also gives you control over the politeness of the crawl through [[a few
settings]{.std .std-ref}](index.html#topics-settings-ref){.hoverxref
.tooltip .reference .internal}. You can do things like setting a
download delay between each request, limiting amount of concurrent
requests per domain or per IP, and even [[using an auto-throttling
extension]{.std .std-ref}](index.html#topics-autothrottle){.hoverxref
.tooltip .reference .internal} that tries to figure out these
automatically.

::: {.admonition .note}
Note

This is using [[feed exports]{.std
.std-ref}](index.html#topics-feed-exports){.hoverxref .tooltip
.reference .internal} to generate the JSON file, you can easily change
the export format (XML or CSV, for example) or the storage backend (FTP
or [Amazon S3](https://aws.amazon.com/s3/){.reference .external}, for
example). You can also write an [[item pipeline]{.std
.std-ref}](index.html#topics-item-pipeline){.hoverxref .tooltip
.reference .internal} to store the items in a database.
:::
:::
:::

::: {#what-else .section}
[]{#topics-whatelse}

#### What else?[¶](#what-else "Permalink to this heading"){.headerlink}

You've seen how to extract and store items from a website using Scrapy,
but this is just the surface. Scrapy provides a lot of powerful features
for making scraping easy and efficient, such as:

-   Built-in support for [[selecting and extracting]{.std
    .std-ref}](index.html#topics-selectors){.hoverxref .tooltip
    .reference .internal} data from HTML/XML sources using extended CSS
    selectors and XPath expressions, with helper methods to extract
    using regular expressions.

-   An [[interactive shell console]{.std
    .std-ref}](index.html#topics-shell){.hoverxref .tooltip .reference
    .internal} (IPython aware) for trying out the CSS and XPath
    expressions to scrape data, very useful when writing or debugging
    your spiders.

-   Built-in support for [[generating feed exports]{.std
    .std-ref}](index.html#topics-feed-exports){.hoverxref .tooltip
    .reference .internal} in multiple formats (JSON, CSV, XML) and
    storing them in multiple backends (FTP, S3, local filesystem)

-   Robust encoding support and auto-detection, for dealing with
    foreign, non-standard and broken encoding declarations.

-   [[Strong extensibility support]{.std
    .std-ref}](index.html#extending-scrapy){.hoverxref .tooltip
    .reference .internal}, allowing you to plug in your own
    functionality using [[signals]{.std
    .std-ref}](index.html#topics-signals){.hoverxref .tooltip .reference
    .internal} and a well-defined API (middlewares, [[extensions]{.std
    .std-ref}](index.html#topics-extensions){.hoverxref .tooltip
    .reference .internal}, and [[pipelines]{.std
    .std-ref}](index.html#topics-item-pipeline){.hoverxref .tooltip
    .reference .internal}).

-   Wide range of built-in extensions and middlewares for handling:

    -   cookies and session handling

    -   HTTP features like compression, authentication, caching

    -   user-agent spoofing

    -   robots.txt

    -   crawl depth restriction

    -   and more

-   A [[Telnet console]{.std
    .std-ref}](index.html#topics-telnetconsole){.hoverxref .tooltip
    .reference .internal} for hooking into a Python console running
    inside your Scrapy process, to introspect and debug your crawler

-   Plus other goodies like reusable spiders to crawl sites from
    [Sitemaps](https://www.sitemaps.org/index.html){.reference
    .external} and XML/CSV feeds, a media pipeline for [[automatically
    downloading images]{.std
    .std-ref}](index.html#topics-media-pipeline){.hoverxref .tooltip
    .reference .internal} (or any other media) associated with the
    scraped items, a caching DNS resolver, and much more!
:::

::: {#what-s-next .section}
#### What's next?[¶](#what-s-next "Permalink to this heading"){.headerlink}

The next steps for you are to [[install Scrapy]{.std
.std-ref}](index.html#intro-install){.hoverxref .tooltip .reference
.internal}, [[follow through the tutorial]{.std
.std-ref}](index.html#intro-tutorial){.hoverxref .tooltip .reference
.internal} to learn how to create a full-blown Scrapy project and [join
the community](https://scrapy.org/community/){.reference .external}.
Thanks for your interest!
:::
:::

[]{#document-intro/install}

::: {#installation-guide .section}
[]{#intro-install}

### Installation guide[¶](#installation-guide "Permalink to this heading"){.headerlink}

::: {#supported-python-versions .section}
[]{#faq-python-versions}

#### Supported Python versions[¶](#supported-python-versions "Permalink to this heading"){.headerlink}

Scrapy requires Python 3.8+, either the CPython implementation (default)
or the PyPy implementation (see [Alternate
Implementations](https://docs.python.org/3/reference/introduction.html#implementations "(in Python v3.12)"){.reference
.external}).
:::

::: {#installing-scrapy .section}
[]{#intro-install-scrapy}

#### Installing Scrapy[¶](#installing-scrapy "Permalink to this heading"){.headerlink}

If you're using
[Anaconda](https://docs.anaconda.com/anaconda/){.reference .external} or
[Miniconda](https://docs.conda.io/projects/conda/en/latest/user-guide/install/index.html){.reference
.external}, you can install the package from the
[conda-forge](https://conda-forge.org/){.reference .external} channel,
which has up-to-date packages for Linux, Windows and macOS.

To install Scrapy using [`conda`{.docutils .literal
.notranslate}]{.pre}, run:

::: {.highlight-default .notranslate}
::: highlight
    conda install -c conda-forge scrapy
:::
:::

Alternatively, if you're already familiar with installation of Python
packages, you can install Scrapy and its dependencies from PyPI with:

::: {.highlight-default .notranslate}
::: highlight
    pip install Scrapy
:::
:::

We strongly recommend that you install Scrapy in [[a dedicated
virtualenv]{.std .std-ref}](#intro-using-virtualenv){.hoverxref .tooltip
.reference .internal}, to avoid conflicting with your system packages.

Note that sometimes this may require solving compilation issues for some
Scrapy dependencies depending on your operating system, so be sure to
check the [[Platform specific installation notes]{.std
.std-ref}](#intro-install-platform-notes){.hoverxref .tooltip .reference
.internal}.

For more detailed and platform specifics instructions, as well as
troubleshooting information, read on.

::: {#things-that-are-good-to-know .section}
##### Things that are good to know[¶](#things-that-are-good-to-know "Permalink to this heading"){.headerlink}

Scrapy is written in pure Python and depends on a few key Python
packages (among others):

-   [lxml](https://lxml.de/index.html){.reference .external}, an
    efficient XML and HTML parser

-   [parsel](https://pypi.org/project/parsel/){.reference .external}, an
    HTML/XML data extraction library written on top of lxml,

-   [w3lib](https://pypi.org/project/w3lib/){.reference .external}, a
    multi-purpose helper for dealing with URLs and web page encodings

-   [twisted](https://twistedmatrix.com/trac/){.reference .external}, an
    asynchronous networking framework

-   [cryptography](https://cryptography.io/en/latest/){.reference
    .external} and
    [pyOpenSSL](https://pypi.org/project/pyOpenSSL/){.reference
    .external}, to deal with various network-level security needs

Some of these packages themselves depend on non-Python packages that
might require additional installation steps depending on your platform.
Please check [[platform-specific guides below]{.std
.std-ref}](#intro-install-platform-notes){.hoverxref .tooltip .reference
.internal}.

In case of any trouble related to these dependencies, please refer to
their respective installation instructions:

-   [lxml installation](https://lxml.de/installation.html){.reference
    .external}

-   [[cryptography installation]{.xref .std
    .std-doc}](https://cryptography.io/en/latest/installation/ "(in Cryptography v42.0.0.dev1)"){.reference
    .external}
:::

::: {#using-a-virtual-environment-recommended .section}
[]{#intro-using-virtualenv}

##### Using a virtual environment (recommended)[¶](#using-a-virtual-environment-recommended "Permalink to this heading"){.headerlink}

TL;DR: We recommend installing Scrapy inside a virtual environment on
all platforms.

Python packages can be installed either globally (a.k.a system wide), or
in user-space. We do not recommend installing Scrapy system wide.

Instead, we recommend that you install Scrapy within a so-called
"virtual environment" ([[`venv`{.xref .py .py-mod .docutils .literal
.notranslate}]{.pre}](https://docs.python.org/3/library/venv.html#module-venv "(in Python v3.12)"){.reference
.external}). Virtual environments allow you to not conflict with
already-installed Python system packages (which could break some of your
system tools and scripts), and still install packages normally with
[`pip`{.docutils .literal .notranslate}]{.pre} (without
[`sudo`{.docutils .literal .notranslate}]{.pre} and the likes).

See [Virtual Environments and
Packages](https://docs.python.org/3/tutorial/venv.html#tut-venv "(in Python v3.12)"){.reference
.external} on how to create your virtual environment.

Once you have created a virtual environment, you can install Scrapy
inside it with [`pip`{.docutils .literal .notranslate}]{.pre}, just like
any other Python package. (See [[platform-specific guides]{.std
.std-ref}](#intro-install-platform-notes){.hoverxref .tooltip .reference
.internal} below for non-Python dependencies that you may need to
install beforehand).
:::
:::

::: {#platform-specific-installation-notes .section}
[]{#intro-install-platform-notes}

#### Platform specific installation notes[¶](#platform-specific-installation-notes "Permalink to this heading"){.headerlink}

::: {#windows .section}
[]{#intro-install-windows}

##### Windows[¶](#windows "Permalink to this heading"){.headerlink}

Though it's possible to install Scrapy on Windows using pip, we
recommend you to install
[Anaconda](https://docs.anaconda.com/anaconda/){.reference .external} or
[Miniconda](https://docs.conda.io/projects/conda/en/latest/user-guide/install/index.html){.reference
.external} and use the package from the
[conda-forge](https://conda-forge.org/){.reference .external} channel,
which will avoid most installation issues.

Once you've installed
[Anaconda](https://docs.anaconda.com/anaconda/){.reference .external} or
[Miniconda](https://docs.conda.io/projects/conda/en/latest/user-guide/install/index.html){.reference
.external}, install Scrapy with:

::: {.highlight-default .notranslate}
::: highlight
    conda install -c conda-forge scrapy
:::
:::

To install Scrapy on Windows using [`pip`{.docutils .literal
.notranslate}]{.pre}:

::: {.admonition .warning}
Warning

This installation method requires "Microsoft Visual C++" for installing
some Scrapy dependencies, which demands significantly more disk space
than Anaconda.
:::

1.  Download and execute [Microsoft C++ Build
    Tools](https://visualstudio.microsoft.com/visual-cpp-build-tools/){.reference
    .external} to install the Visual Studio Installer.

2.  Run the Visual Studio Installer.

3.  Under the Workloads section, select **C++ build tools**.

4.  Check the installation details and make sure following packages are
    selected as optional components:

    > <div>
    >
    > -   **MSVC** (e.g MSVC v142 - VS 2019 C++ x64/x86 build tools
    >     (v14.23) )
    >
    > -   **Windows SDK** (e.g Windows 10 SDK (10.0.18362.0))
    >
    > </div>

5.  Install the Visual Studio Build Tools.

Now, you should be able to [[install Scrapy]{.std
.std-ref}](#intro-install-scrapy){.hoverxref .tooltip .reference
.internal} using [`pip`{.docutils .literal .notranslate}]{.pre}.
:::

::: {#ubuntu-14-04-or-above .section}
[]{#intro-install-ubuntu}

##### Ubuntu 14.04 or above[¶](#ubuntu-14-04-or-above "Permalink to this heading"){.headerlink}

Scrapy is currently tested with recent-enough versions of lxml, twisted
and pyOpenSSL, and is compatible with recent Ubuntu distributions. But
it should support older versions of Ubuntu too, like Ubuntu 14.04,
albeit with potential issues with TLS connections.

**Don't** use the [`python-scrapy`{.docutils .literal
.notranslate}]{.pre} package provided by Ubuntu, they are typically too
old and slow to catch up with latest Scrapy.

To install Scrapy on Ubuntu (or Ubuntu-based) systems, you need to
install these dependencies:

::: {.highlight-default .notranslate}
::: highlight
    sudo apt-get install python3 python3-dev python3-pip libxml2-dev libxslt1-dev zlib1g-dev libffi-dev libssl-dev
:::
:::

-   [`python3-dev`{.docutils .literal .notranslate}]{.pre},
    [`zlib1g-dev`{.docutils .literal .notranslate}]{.pre},
    [`libxml2-dev`{.docutils .literal .notranslate}]{.pre} and
    [`libxslt1-dev`{.docutils .literal .notranslate}]{.pre} are required
    for [`lxml`{.docutils .literal .notranslate}]{.pre}

-   [`libssl-dev`{.docutils .literal .notranslate}]{.pre} and
    [`libffi-dev`{.docutils .literal .notranslate}]{.pre} are required
    for [`cryptography`{.docutils .literal .notranslate}]{.pre}

Inside a [[virtualenv]{.std
.std-ref}](#intro-using-virtualenv){.hoverxref .tooltip .reference
.internal}, you can install Scrapy with [`pip`{.docutils .literal
.notranslate}]{.pre} after that:

::: {.highlight-default .notranslate}
::: highlight
    pip install scrapy
:::
:::

::: {.admonition .note}
Note

The same non-Python dependencies can be used to install Scrapy in Debian
Jessie (8.0) and above.
:::
:::

::: {#macos .section}
[]{#intro-install-macos}

##### macOS[¶](#macos "Permalink to this heading"){.headerlink}

Building Scrapy's dependencies requires the presence of a C compiler and
development headers. On macOS this is typically provided by Apple's
Xcode development tools. To install the Xcode command line tools open a
terminal window and run:

::: {.highlight-default .notranslate}
::: highlight
    xcode-select --install
:::
:::

There's a [known
issue](https://github.com/pypa/pip/issues/2468){.reference .external}
that prevents [`pip`{.docutils .literal .notranslate}]{.pre} from
updating system packages. This has to be addressed to successfully
install Scrapy and its dependencies. Here are some proposed solutions:

-   *(Recommended)* **Don't** use system Python. Install a new, updated
    version that doesn't conflict with the rest of your system. Here's
    how to do it using the [homebrew](https://brew.sh/){.reference
    .external} package manager:

    -   Install [homebrew](https://brew.sh/){.reference .external}
        following the instructions in
        [https://brew.sh/](https://brew.sh/){.reference .external}

    -   Update your [`PATH`{.docutils .literal .notranslate}]{.pre}
        variable to state that homebrew packages should be used before
        system packages (Change [`.bashrc`{.docutils .literal
        .notranslate}]{.pre} to [`.zshrc`{.docutils .literal
        .notranslate}]{.pre} accordingly if you're using
        [zsh](https://www.zsh.org/){.reference .external} as default
        shell):

        ::: {.highlight-default .notranslate}
        ::: highlight
            echo "export PATH=/usr/local/bin:/usr/local/sbin:$PATH" >> ~/.bashrc
        :::
        :::

    -   Reload [`.bashrc`{.docutils .literal .notranslate}]{.pre} to
        ensure the changes have taken place:

        ::: {.highlight-default .notranslate}
        ::: highlight
            source ~/.bashrc
        :::
        :::

    -   Install python:

        ::: {.highlight-default .notranslate}
        ::: highlight
            brew install python
        :::
        :::

    -   Latest versions of python have [`pip`{.docutils .literal
        .notranslate}]{.pre} bundled with them so you won't need to
        install it separately. If this is not the case, upgrade python:

        ::: {.highlight-default .notranslate}
        ::: highlight
            brew update; brew upgrade python
        :::
        :::

-   *(Optional)* [[Install Scrapy inside a Python virtual
    environment]{.std .std-ref}](#intro-using-virtualenv){.hoverxref
    .tooltip .reference .internal}.

> <div>
>
> This method is a workaround for the above macOS issue, but it's an
> overall good practice for managing dependencies and can complement the
> first method.
>
> </div>

After any of these workarounds you should be able to install Scrapy:

::: {.highlight-default .notranslate}
::: highlight
    pip install Scrapy
:::
:::
:::

::: {#pypy .section}
##### PyPy[¶](#pypy "Permalink to this heading"){.headerlink}

We recommend using the latest PyPy version. For PyPy3, only Linux
installation was tested.

Most Scrapy dependencies now have binary wheels for CPython, but not for
PyPy. This means that these dependencies will be built during
installation. On macOS, you are likely to face an issue with building
the Cryptography dependency. The solution to this problem is described
[here](https://github.com/pyca/cryptography/issues/2692#issuecomment-272773481){.reference
.external}, that is to [`brew`{.docutils .literal
.notranslate}]{.pre}` `{.docutils .literal
.notranslate}[`install`{.docutils .literal
.notranslate}]{.pre}` `{.docutils .literal
.notranslate}[`openssl`{.docutils .literal .notranslate}]{.pre} and then
export the flags that this command recommends (only needed when
installing Scrapy). Installing on Linux has no special issues besides
installing build dependencies. Installing Scrapy with PyPy on Windows is
not tested.

You can check that Scrapy is installed correctly by running
[`scrapy`{.docutils .literal .notranslate}]{.pre}` `{.docutils .literal
.notranslate}[`bench`{.docutils .literal .notranslate}]{.pre}. If this
command gives errors such as [`TypeError:`{.docutils .literal
.notranslate}]{.pre}` `{.docutils .literal .notranslate}[`...`{.docutils
.literal .notranslate}]{.pre}` `{.docutils .literal
.notranslate}[`got`{.docutils .literal .notranslate}]{.pre}` `{.docutils
.literal .notranslate}[`2`{.docutils .literal
.notranslate}]{.pre}` `{.docutils .literal
.notranslate}[`unexpected`{.docutils .literal
.notranslate}]{.pre}` `{.docutils .literal
.notranslate}[`keyword`{.docutils .literal
.notranslate}]{.pre}` `{.docutils .literal
.notranslate}[`arguments`{.docutils .literal .notranslate}]{.pre}, this
means that setuptools was unable to pick up one PyPy-specific
dependency. To fix this issue, run [`pip`{.docutils .literal
.notranslate}]{.pre}` `{.docutils .literal
.notranslate}[`install`{.docutils .literal
.notranslate}]{.pre}` `{.docutils .literal
.notranslate}[`'PyPyDispatcher>=2.1.0'`{.docutils .literal
.notranslate}]{.pre}.
:::
:::

::: {#troubleshooting .section}
[]{#intro-install-troubleshooting}

#### Troubleshooting[¶](#troubleshooting "Permalink to this heading"){.headerlink}

::: {#attributeerror-module-object-has-no-attribute-op-no-tlsv1-1 .section}
##### AttributeError: 'module' object has no attribute 'OP_NO_TLSv1_1'[¶](#attributeerror-module-object-has-no-attribute-op-no-tlsv1-1 "Permalink to this heading"){.headerlink}

After you install or upgrade Scrapy, Twisted or pyOpenSSL, you may get
an exception with the following traceback:

::: {.highlight-default .notranslate}
::: highlight
    […]
      File "[…]/site-packages/twisted/protocols/tls.py", line 63, in <module>
        from twisted.internet._sslverify import _setAcceptableProtocols
      File "[…]/site-packages/twisted/internet/_sslverify.py", line 38, in <module>
        TLSVersion.TLSv1_1: SSL.OP_NO_TLSv1_1,
    AttributeError: 'module' object has no attribute 'OP_NO_TLSv1_1'
:::
:::

The reason you get this exception is that your system or virtual
environment has a version of pyOpenSSL that your version of Twisted does
not support.

To install a version of pyOpenSSL that your version of Twisted supports,
reinstall Twisted with the [`tls`{.code .docutils .literal
.notranslate}]{.pre} extra option:

::: {.highlight-default .notranslate}
::: highlight
    pip install twisted[tls]
:::
:::

For details, see [Issue
#2473](https://github.com/scrapy/scrapy/issues/2473){.reference
.external}.
:::
:::
:::

[]{#document-intro/tutorial}

::: {#scrapy-tutorial .section}
[]{#intro-tutorial}

### Scrapy Tutorial[¶](#scrapy-tutorial "Permalink to this heading"){.headerlink}

In this tutorial, we'll assume that Scrapy is already installed on your
system. If that's not the case, see [[Installation guide]{.std
.std-ref}](index.html#intro-install){.hoverxref .tooltip .reference
.internal}.

We are going to scrape
[quotes.toscrape.com](https://quotes.toscrape.com/){.reference
.external}, a website that lists quotes from famous authors.

This tutorial will walk you through these tasks:

1.  Creating a new Scrapy project

2.  Writing a [[spider]{.std
    .std-ref}](index.html#topics-spiders){.hoverxref .tooltip .reference
    .internal} to crawl a site and extract data

3.  Exporting the scraped data using the command line

4.  Changing spider to recursively follow links

5.  Using spider arguments

Scrapy is written in [Python](https://www.python.org/){.reference
.external}. If you're new to the language you might want to start by
getting an idea of what the language is like, to get the most out of
Scrapy.

If you're already familiar with other languages, and want to learn
Python quickly, the [Python
Tutorial](https://docs.python.org/3/tutorial){.reference .external} is a
good resource.

If you're new to programming and want to start with Python, the
following books may be useful to you:

-   [Automate the Boring Stuff With
    Python](https://automatetheboringstuff.com/){.reference .external}

-   [How To Think Like a Computer
    Scientist](http://openbookproject.net/thinkcs/python/english3e/){.reference
    .external}

-   [Learn Python 3 The Hard
    Way](https://learnpythonthehardway.org/python3/){.reference
    .external}

You can also take a look at [this list of Python resources for
non-programmers](https://wiki.python.org/moin/BeginnersGuide/NonProgrammers){.reference
.external}, as well as the [suggested resources in the
learnpython-subreddit](https://www.reddit.com/r/learnpython/wiki/index#wiki_new_to_python.3F){.reference
.external}.

::: {#creating-a-project .section}
#### Creating a project[¶](#creating-a-project "Permalink to this heading"){.headerlink}

Before you start scraping, you will have to set up a new Scrapy project.
Enter a directory where you'd like to store your code and run:

::: {.highlight-default .notranslate}
::: highlight
    scrapy startproject tutorial
:::
:::

This will create a [`tutorial`{.docutils .literal .notranslate}]{.pre}
directory with the following contents:

::: {.highlight-default .notranslate}
::: highlight
    tutorial/
        scrapy.cfg            # deploy configuration file

        tutorial/             # project's Python module, you'll import your code from here
            __init__.py

            items.py          # project items definition file

            middlewares.py    # project middlewares file

            pipelines.py      # project pipelines file

            settings.py       # project settings file

            spiders/          # a directory where you'll later put your spiders
                __init__.py
:::
:::
:::

::: {#our-first-spider .section}
#### Our first Spider[¶](#our-first-spider "Permalink to this heading"){.headerlink}

Spiders are classes that you define and that Scrapy uses to scrape
information from a website (or a group of websites). They must subclass
[[`Spider`{.xref .py .py-class .docutils .literal
.notranslate}]{.pre}](index.html#scrapy.Spider "scrapy.Spider"){.reference
.internal} and define the initial requests to make, optionally how to
follow links in the pages, and how to parse the downloaded page content
to extract data.

This is the code for our first Spider. Save it in a file named
[`quotes_spider.py`{.docutils .literal .notranslate}]{.pre} under the
[`tutorial/spiders`{.docutils .literal .notranslate}]{.pre} directory in
your project:

::: {.highlight-python .notranslate}
::: highlight
    from pathlib import Path

    import scrapy


    class QuotesSpider(scrapy.Spider):
        name = "quotes"

        def start_requests(self):
            urls = [
                "https://quotes.toscrape.com/page/1/",
                "https://quotes.toscrape.com/page/2/",
            ]
            for url in urls:
                yield scrapy.Request(url=url, callback=self.parse)

        def parse(self, response):
            page = response.url.split("/")[-2]
            filename = f"quotes-{page}.html"
            Path(filename).write_bytes(response.body)
            self.log(f"Saved file {filename}")
:::
:::

As you can see, our Spider subclasses [[`scrapy.Spider`{.xref .py
.py-class .docutils .literal
.notranslate}]{.pre}](index.html#scrapy.Spider "scrapy.Spider"){.reference
.internal} and defines some attributes and methods:

-   [[`name`{.xref .py .py-attr .docutils .literal
    .notranslate}]{.pre}](index.html#scrapy.Spider.name "scrapy.Spider.name"){.reference
    .internal}: identifies the Spider. It must be unique within a
    project, that is, you can't set the same name for different Spiders.

-   [[`start_requests()`{.xref .py .py-meth .docutils .literal
    .notranslate}]{.pre}](index.html#scrapy.Spider.start_requests "scrapy.Spider.start_requests"){.reference
    .internal}: must return an iterable of Requests (you can return a
    list of requests or write a generator function) which the Spider
    will begin to crawl from. Subsequent requests will be generated
    successively from these initial requests.

-   [[`parse()`{.xref .py .py-meth .docutils .literal
    .notranslate}]{.pre}](index.html#scrapy.Spider.parse "scrapy.Spider.parse"){.reference
    .internal}: a method that will be called to handle the response
    downloaded for each of the requests made. The response parameter is
    an instance of [[`TextResponse`{.xref .py .py-class .docutils
    .literal
    .notranslate}]{.pre}](index.html#scrapy.http.TextResponse "scrapy.http.TextResponse"){.reference
    .internal} that holds the page content and has further helpful
    methods to handle it.

    The [[`parse()`{.xref .py .py-meth .docutils .literal
    .notranslate}]{.pre}](index.html#scrapy.Spider.parse "scrapy.Spider.parse"){.reference
    .internal} method usually parses the response, extracting the
    scraped data as dicts and also finding new URLs to follow and
    creating new requests ([`Request`{.xref .py .py-class .docutils
    .literal .notranslate}]{.pre}) from them.

::: {#how-to-run-our-spider .section}
##### How to run our spider[¶](#how-to-run-our-spider "Permalink to this heading"){.headerlink}

To put our spider to work, go to the project's top level directory and
run:

::: {.highlight-default .notranslate}
::: highlight
    scrapy crawl quotes
:::
:::

This command runs the spider with name [`quotes`{.docutils .literal
.notranslate}]{.pre} that we've just added, that will send some requests
for the [`quotes.toscrape.com`{.docutils .literal .notranslate}]{.pre}
domain. You will get an output similar to this:

::: {.highlight-default .notranslate}
::: highlight
    ... (omitted for brevity)
    2016-12-16 21:24:05 [scrapy.core.engine] INFO: Spider opened
    2016-12-16 21:24:05 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
    2016-12-16 21:24:05 [scrapy.extensions.telnet] DEBUG: Telnet console listening on 127.0.0.1:6023
    2016-12-16 21:24:05 [scrapy.core.engine] DEBUG: Crawled (404) <GET https://quotes.toscrape.com/robots.txt> (referer: None)
    2016-12-16 21:24:05 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://quotes.toscrape.com/page/1/> (referer: None)
    2016-12-16 21:24:05 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://quotes.toscrape.com/page/2/> (referer: None)
    2016-12-16 21:24:05 [quotes] DEBUG: Saved file quotes-1.html
    2016-12-16 21:24:05 [quotes] DEBUG: Saved file quotes-2.html
    2016-12-16 21:24:05 [scrapy.core.engine] INFO: Closing spider (finished)
    ...
:::
:::

Now, check the files in the current directory. You should notice that
two new files have been created: *quotes-1.html* and *quotes-2.html*,
with the content for the respective URLs, as our [`parse`{.docutils
.literal .notranslate}]{.pre} method instructs.

::: {.admonition .note}
Note

If you are wondering why we haven't parsed the HTML yet, hold on, we
will cover that soon.
:::

::: {#what-just-happened-under-the-hood .section}
###### What just happened under the hood?[¶](#what-just-happened-under-the-hood "Permalink to this heading"){.headerlink}

Scrapy schedules the [`scrapy.Request`{.xref .py .py-class .docutils
.literal .notranslate}]{.pre} objects returned by the
[`start_requests`{.docutils .literal .notranslate}]{.pre} method of the
Spider. Upon receiving a response for each one, it instantiates
[[`Response`{.xref .py .py-class .docutils .literal
.notranslate}]{.pre}](index.html#scrapy.http.Response "scrapy.http.Response"){.reference
.internal} objects and calls the callback method associated with the
request (in this case, the [`parse`{.docutils .literal
.notranslate}]{.pre} method) passing the response as argument.
:::
:::

::: {#a-shortcut-to-the-start-requests-method .section}
##### A shortcut to the start_requests method[¶](#a-shortcut-to-the-start-requests-method "Permalink to this heading"){.headerlink}

Instead of implementing a [[`start_requests()`{.xref .py .py-meth
.docutils .literal
.notranslate}]{.pre}](index.html#scrapy.Spider.start_requests "scrapy.Spider.start_requests"){.reference
.internal} method that generates [`scrapy.Request`{.xref .py .py-class
.docutils .literal .notranslate}]{.pre} objects from URLs, you can just
define a [[`start_urls`{.xref .py .py-attr .docutils .literal
.notranslate}]{.pre}](index.html#scrapy.Spider.start_urls "scrapy.Spider.start_urls"){.reference
.internal} class attribute with a list of URLs. This list will then be
used by the default implementation of [[`start_requests()`{.xref .py
.py-meth .docutils .literal
.notranslate}]{.pre}](index.html#scrapy.Spider.start_requests "scrapy.Spider.start_requests"){.reference
.internal} to create the initial requests for your spider.

::: {.highlight-python .notranslate}
::: highlight
    from pathlib import Path

    import scrapy


    class QuotesSpider(scrapy.Spider):
        name = "quotes"
        start_urls = [
            "https://quotes.toscrape.com/page/1/",
            "https://quotes.toscrape.com/page/2/",
        ]

        def parse(self, response):
            page = response.url.split("/")[-2]
            filename = f"quotes-{page}.html"
            Path(filename).write_bytes(response.body)
:::
:::

The [[`parse()`{.xref .py .py-meth .docutils .literal
.notranslate}]{.pre}](index.html#scrapy.Spider.parse "scrapy.Spider.parse"){.reference
.internal} method will be called to handle each of the requests for
those URLs, even though we haven't explicitly told Scrapy to do so. This
happens because [[`parse()`{.xref .py .py-meth .docutils .literal
.notranslate}]{.pre}](index.html#scrapy.Spider.parse "scrapy.Spider.parse"){.reference
.internal} is Scrapy's default callback method, which is called for
requests without an explicitly assigned callback.
:::

::: {#extracting-data .section}
##### Extracting data[¶](#extracting-data "Permalink to this heading"){.headerlink}

The best way to learn how to extract data with Scrapy is trying
selectors using the [[Scrapy shell]{.std
.std-ref}](index.html#topics-shell){.hoverxref .tooltip .reference
.internal}. Run:

::: {.highlight-default .notranslate}
::: highlight
    scrapy shell 'https://quotes.toscrape.com/page/1/'
:::
:::

::: {.admonition .note}
Note

Remember to always enclose urls in quotes when running Scrapy shell from
command-line, otherwise urls containing arguments (i.e. [`&`{.docutils
.literal .notranslate}]{.pre} character) will not work.

On Windows, use double quotes instead:

::: {.highlight-default .notranslate}
::: highlight
    scrapy shell "https://quotes.toscrape.com/page/1/"
:::
:::
:::

You will see something like:

::: {.highlight-default .notranslate}
::: highlight
    [ ... Scrapy log here ... ]
    2016-09-19 12:09:27 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://quotes.toscrape.com/page/1/> (referer: None)
    [s] Available Scrapy objects:
    [s]   scrapy     scrapy module (contains scrapy.Request, scrapy.Selector, etc)
    [s]   crawler    <scrapy.crawler.Crawler object at 0x7fa91d888c90>
    [s]   item       {}
    [s]   request    <GET https://quotes.toscrape.com/page/1/>
    [s]   response   <200 https://quotes.toscrape.com/page/1/>
    [s]   settings   <scrapy.settings.Settings object at 0x7fa91d888c10>
    [s]   spider     <DefaultSpider 'default' at 0x7fa91c8af990>
    [s] Useful shortcuts:
    [s]   shelp()           Shell help (print this help)
    [s]   fetch(req_or_url) Fetch request (or URL) and update local objects
    [s]   view(response)    View response in a browser
:::
:::

Using the shell, you can try selecting elements using
[CSS](https://www.w3.org/TR/selectors){.reference .external} with the
response object:

::: {.highlight-pycon .notranslate}
::: highlight
    >>> response.css("title")
    [<Selector query='descendant-or-self::title' data='<title>Quotes to Scrape</title>'>]
:::
:::

The result of running [`response.css('title')`{.docutils .literal
.notranslate}]{.pre} is a list-like object called [[`SelectorList`{.xref
.py .py-class .docutils .literal
.notranslate}]{.pre}](index.html#scrapy.selector.SelectorList "scrapy.selector.SelectorList"){.reference
.internal}, which represents a list of [`Selector`{.xref .py .py-class
.docutils .literal .notranslate}]{.pre} objects that wrap around
XML/HTML elements and allow you to run further queries to fine-grain the
selection or extract the data.

To extract the text from the title above, you can do:

::: {.highlight-pycon .notranslate}
::: highlight
    >>> response.css("title::text").getall()
    ['Quotes to Scrape']
:::
:::

There are two things to note here: one is that we've added
[`::text`{.docutils .literal .notranslate}]{.pre} to the CSS query, to
mean we want to select only the text elements directly inside
[`<title>`{.docutils .literal .notranslate}]{.pre} element. If we don't
specify [`::text`{.docutils .literal .notranslate}]{.pre}, we'd get the
full title element, including its tags:

::: {.highlight-pycon .notranslate}
::: highlight
    >>> response.css("title").getall()
    ['<title>Quotes to Scrape</title>']
:::
:::

The other thing is that the result of calling [`.getall()`{.docutils
.literal .notranslate}]{.pre} is a list: it is possible that a selector
returns more than one result, so we extract them all. When you know you
just want the first result, as in this case, you can do:

::: {.highlight-pycon .notranslate}
::: highlight
    >>> response.css("title::text").get()
    'Quotes to Scrape'
:::
:::

As an alternative, you could've written:

::: {.highlight-pycon .notranslate}
::: highlight
    >>> response.css("title::text")[0].get()
    'Quotes to Scrape'
:::
:::

Accessing an index on a [[`SelectorList`{.xref .py .py-class .docutils
.literal
.notranslate}]{.pre}](index.html#scrapy.selector.SelectorList "scrapy.selector.SelectorList"){.reference
.internal} instance will raise an [[`IndexError`{.xref .py .py-exc
.docutils .literal
.notranslate}]{.pre}](https://docs.python.org/3/library/exceptions.html#IndexError "(in Python v3.12)"){.reference
.external} exception if there are no results:

::: {.highlight-pycon .notranslate}
::: highlight
    >>> response.css("noelement")[0].get()
    Traceback (most recent call last):
    ...
    IndexError: list index out of range
:::
:::

You might want to use [`.get()`{.docutils .literal .notranslate}]{.pre}
directly on the [[`SelectorList`{.xref .py .py-class .docutils .literal
.notranslate}]{.pre}](index.html#scrapy.selector.SelectorList "scrapy.selector.SelectorList"){.reference
.internal} instance instead, which returns [`None`{.docutils .literal
.notranslate}]{.pre} if there are no results:

::: {.highlight-pycon .notranslate}
::: highlight
    >>> response.css("noelement").get()
:::
:::

There's a lesson here: for most scraping code, you want it to be
resilient to errors due to things not being found on a page, so that
even if some parts fail to be scraped, you can at least get **some**
data.

Besides the [[`getall()`{.xref .py .py-meth .docutils .literal
.notranslate}]{.pre}](index.html#scrapy.selector.SelectorList.getall "scrapy.selector.SelectorList.getall"){.reference
.internal} and [[`get()`{.xref .py .py-meth .docutils .literal
.notranslate}]{.pre}](index.html#scrapy.selector.SelectorList.get "scrapy.selector.SelectorList.get"){.reference
.internal} methods, you can also use the [[`re()`{.xref .py .py-meth
.docutils .literal
.notranslate}]{.pre}](index.html#scrapy.selector.SelectorList.re "scrapy.selector.SelectorList.re"){.reference
.internal} method to extract using [[regular expressions]{.xref .std
.std-doc}](https://docs.python.org/3/library/re.html "(in Python v3.12)"){.reference
.external}:

::: {.highlight-pycon .notranslate}
::: highlight
    >>> response.css("title::text").re(r"Quotes.*")
    ['Quotes to Scrape']
    >>> response.css("title::text").re(r"Q\w+")
    ['Quotes']
    >>> response.css("title::text").re(r"(\w+) to (\w+)")
    ['Quotes', 'Scrape']
:::
:::

In order to find the proper CSS selectors to use, you might find it
useful to open the response page from the shell in your web browser
using [`view(response)`{.docutils .literal .notranslate}]{.pre}. You can
use your browser's developer tools to inspect the HTML and come up with
a selector (see [[Using your browser's Developer Tools for
scraping]{.std .std-ref}](index.html#topics-developer-tools){.hoverxref
.tooltip .reference .internal}).

[Selector Gadget](https://selectorgadget.com/){.reference .external} is
also a nice tool to quickly find CSS selector for visually selected
elements, which works in many browsers.

::: {#xpath-a-brief-intro .section}
###### XPath: a brief intro[¶](#xpath-a-brief-intro "Permalink to this heading"){.headerlink}

Besides [CSS](https://www.w3.org/TR/selectors){.reference .external},
Scrapy selectors also support using
[XPath](https://www.w3.org/TR/xpath/all/){.reference .external}
expressions:

::: {.highlight-pycon .notranslate}
::: highlight
    >>> response.xpath("//title")
    [<Selector query='//title' data='<title>Quotes to Scrape</title>'>]
    >>> response.xpath("//title/text()").get()
    'Quotes to Scrape'
:::
:::

XPath expressions are very powerful, and are the foundation of Scrapy
Selectors. In fact, CSS selectors are converted to XPath under-the-hood.
You can see that if you read closely the text representation of the
selector objects in the shell.

While perhaps not as popular as CSS selectors, XPath expressions offer
more power because besides navigating the structure, it can also look at
the content. Using XPath, you're able to select things like: *select the
link that contains the text "Next Page"*. This makes XPath very fitting
to the task of scraping, and we encourage you to learn XPath even if you
already know how to construct CSS selectors, it will make scraping much
easier.

We won't cover much of XPath here, but you can read more about [[using
XPath with Scrapy Selectors here]{.std
.std-ref}](index.html#topics-selectors){.hoverxref .tooltip .reference
.internal}. To learn more about XPath, we recommend [this tutorial to
learn XPath through
examples](http://zvon.org/comp/r/tut-XPath_1.html){.reference
.external}, and [this tutorial to learn "how to think in
XPath"](http://plasmasturm.org/log/xpath101/){.reference .external}.
:::

::: {#extracting-quotes-and-authors .section}
###### Extracting quotes and authors[¶](#extracting-quotes-and-authors "Permalink to this heading"){.headerlink}

Now that you know a bit about selection and extraction, let's complete
our spider by writing the code to extract the quotes from the web page.

Each quote in
[https://quotes.toscrape.com](https://quotes.toscrape.com){.reference
.external} is represented by HTML elements that look like this:

::: {.highlight-html .notranslate}
::: highlight
    <div class="quote">
        <span class="text">“The world as we have created it is a process of our
        thinking. It cannot be changed without changing our thinking.”</span>
        <span>
            by <small class="author">Albert Einstein</small>
            <a href="/author/Albert-Einstein">(about)</a>
        </span>
        <div class="tags">
            Tags:
            <a class="tag" href="/tag/change/page/1/">change</a>
            <a class="tag" href="/tag/deep-thoughts/page/1/">deep-thoughts</a>
            <a class="tag" href="/tag/thinking/page/1/">thinking</a>
            <a class="tag" href="/tag/world/page/1/">world</a>
        </div>
    </div>
:::
:::

Let's open up scrapy shell and play a bit to find out how to extract the
data we want:

::: {.highlight-default .notranslate}
::: highlight
    scrapy shell 'https://quotes.toscrape.com'
:::
:::

We get a list of selectors for the quote HTML elements with:

::: {.highlight-pycon .notranslate}
::: highlight
    >>> response.css("div.quote")
    [<Selector query="descendant-or-self::div[@class and contains(concat(' ', normalize-space(@class), ' '), ' quote ')]" data='<div class="quote" itemscope itemtype...'>,
    <Selector query="descendant-or-self::div[@class and contains(concat(' ', normalize-space(@class), ' '), ' quote ')]" data='<div class="quote" itemscope itemtype...'>,
    ...]
:::
:::

Each of the selectors returned by the query above allows us to run
further queries over their sub-elements. Let's assign the first selector
to a variable, so that we can run our CSS selectors directly on a
particular quote:

::: {.highlight-pycon .notranslate}
::: highlight
    >>> quote = response.css("div.quote")[0]
:::
:::

Now, let's extract [`text`{.docutils .literal .notranslate}]{.pre},
[`author`{.docutils .literal .notranslate}]{.pre} and the
[`tags`{.docutils .literal .notranslate}]{.pre} from that quote using
the [`quote`{.docutils .literal .notranslate}]{.pre} object we just
created:

::: {.highlight-pycon .notranslate}
::: highlight
    >>> text = quote.css("span.text::text").get()
    >>> text
    '“The world as we have created it is a process of our thinking. It cannot be changed without changing our thinking.”'
    >>> author = quote.css("small.author::text").get()
    >>> author
    'Albert Einstein'
:::
:::

Given that the tags are a list of strings, we can use the
[`.getall()`{.docutils .literal .notranslate}]{.pre} method to get all
of them:

::: {.highlight-pycon .notranslate}
::: highlight
    >>> tags = quote.css("div.tags a.tag::text").getall()
    >>> tags
    ['change', 'deep-thoughts', 'thinking', 'world']
:::
:::

Having figured out how to extract each bit, we can now iterate over all
the quotes elements and put them together into a Python dictionary:

::: {.highlight-pycon .notranslate}
::: highlight
    >>> for quote in response.css("div.quote"):
    ...     text = quote.css("span.text::text").get()
    ...     author = quote.css("small.author::text").get()
    ...     tags = quote.css("div.tags a.tag::text").getall()
    ...     print(dict(text=text, author=author, tags=tags))
    ...
    {'text': '“The world as we have created it is a process of our thinking. It cannot be changed without changing our thinking.”', 'author': 'Albert Einstein', 'tags': ['change', 'deep-thoughts', 'thinking', 'world']}
    {'text': '“It is our choices, Harry, that show what we truly are, far more than our abilities.”', 'author': 'J.K. Rowling', 'tags': ['abilities', 'choices']}
    ...
:::
:::
:::
:::

::: {#extracting-data-in-our-spider .section}
##### Extracting data in our spider[¶](#extracting-data-in-our-spider "Permalink to this heading"){.headerlink}

Let's get back to our spider. Until now, it doesn't extract any data in
particular, just saves the whole HTML page to a local file. Let's
integrate the extraction logic above into our spider.

A Scrapy spider typically generates many dictionaries containing the
data extracted from the page. To do that, we use the [`yield`{.docutils
.literal .notranslate}]{.pre} Python keyword in the callback, as you can
see below:

::: {.highlight-python .notranslate}
::: highlight
    import scrapy


    class QuotesSpider(scrapy.Spider):
        name = "quotes"
        start_urls = [
            "https://quotes.toscrape.com/page/1/",
            "https://quotes.toscrape.com/page/2/",
        ]

        def parse(self, response):
            for quote in response.css("div.quote"):
                yield {
                    "text": quote.css("span.text::text").get(),
                    "author": quote.css("small.author::text").get(),
                    "tags": quote.css("div.tags a.tag::text").getall(),
                }
:::
:::

To run this spider, exit the scrapy shell by entering:

::: {.highlight-default .notranslate}
::: highlight
    quit()
:::
:::

Then, run:

::: {.highlight-default .notranslate}
::: highlight
    scrapy crawl quotes
:::
:::

Now, it should output the extracted data with the log:

::: {.highlight-default .notranslate}
::: highlight
    2016-09-19 18:57:19 [scrapy.core.scraper] DEBUG: Scraped from <200 https://quotes.toscrape.com/page/1/>
    {'tags': ['life', 'love'], 'author': 'André Gide', 'text': '“It is better to be hated for what you are than to be loved for what you are not.”'}
    2016-09-19 18:57:19 [scrapy.core.scraper] DEBUG: Scraped from <200 https://quotes.toscrape.com/page/1/>
    {'tags': ['edison', 'failure', 'inspirational', 'paraphrased'], 'author': 'Thomas A. Edison', 'text': "“I have not failed. I've just found 10,000 ways that won't work.”"}
:::
:::
:::
:::

::: {#storing-the-scraped-data .section}
[]{#storing-data}

#### Storing the scraped data[¶](#storing-the-scraped-data "Permalink to this heading"){.headerlink}

The simplest way to store the scraped data is by using [[Feed
exports]{.std .std-ref}](index.html#topics-feed-exports){.hoverxref
.tooltip .reference .internal}, with the following command:

::: {.highlight-default .notranslate}
::: highlight
    scrapy crawl quotes -O quotes.json
:::
:::

That will generate a [`quotes.json`{.docutils .literal
.notranslate}]{.pre} file containing all scraped items, serialized in
[JSON](https://en.wikipedia.org/wiki/JSON){.reference .external}.

The [`-O`{.docutils .literal .notranslate}]{.pre} command-line switch
overwrites any existing file; use [`-o`{.docutils .literal
.notranslate}]{.pre} instead to append new content to any existing file.
However, appending to a JSON file makes the file contents invalid JSON.
When appending to a file, consider using a different serialization
format, such as [JSON Lines](http://jsonlines.org){.reference
.external}:

::: {.highlight-default .notranslate}
::: highlight
    scrapy crawl quotes -o quotes.jsonl
:::
:::

The [JSON Lines](http://jsonlines.org){.reference .external} format is
useful because it's stream-like, you can easily append new records to
it. It doesn't have the same problem of JSON when you run twice. Also,
as each record is a separate line, you can process big files without
having to fit everything in memory, there are tools like
[JQ](https://stedolan.github.io/jq){.reference .external} to help do
that at the command-line.

In small projects (like the one in this tutorial), that should be
enough. However, if you want to perform more complex things with the
scraped items, you can write an [[Item Pipeline]{.std
.std-ref}](index.html#topics-item-pipeline){.hoverxref .tooltip
.reference .internal}. A placeholder file for Item Pipelines has been
set up for you when the project is created, in
[`tutorial/pipelines.py`{.docutils .literal .notranslate}]{.pre}. Though
you don't need to implement any item pipelines if you just want to store
the scraped items.
:::

::: {#following-links .section}
#### Following links[¶](#following-links "Permalink to this heading"){.headerlink}

Let's say, instead of just scraping the stuff from the first two pages
from
[https://quotes.toscrape.com](https://quotes.toscrape.com){.reference
.external}, you want quotes from all the pages in the website.

Now that you know how to extract data from pages, let's see how to
follow links from them.

First thing is to extract the link to the page we want to follow.
Examining our page, we can see there is a link to the next page with the
following markup:

::: {.highlight-html .notranslate}
::: highlight
    <ul class="pager">
        <li class="next">
            <a href="/page/2/">Next <span aria-hidden="true">&rarr;</span></a>
        </li>
    </ul>
:::
:::

We can try extracting it in the shell:

::: {.doctest .highlight-default .notranslate}
::: highlight
    >>> response.css('li.next a').get()
    '<a href="/page/2/">Next <span aria-hidden="true">→</span></a>'
:::
:::

This gets the anchor element, but we want the attribute
[`href`{.docutils .literal .notranslate}]{.pre}. For that, Scrapy
supports a CSS extension that lets you select the attribute contents,
like this:

::: {.highlight-pycon .notranslate}
::: highlight
    >>> response.css("li.next a::attr(href)").get()
    '/page/2/'
:::
:::

There is also an [`attrib`{.docutils .literal .notranslate}]{.pre}
property available (see [[Selecting element attributes]{.std
.std-ref}](index.html#selecting-attributes){.hoverxref .tooltip
.reference .internal} for more):

::: {.highlight-pycon .notranslate}
::: highlight
    >>> response.css("li.next a").attrib["href"]
    '/page/2/'
:::
:::

Let's see now our spider modified to recursively follow the link to the
next page, extracting data from it:

::: {.highlight-python .notranslate}
::: highlight
    import scrapy


    class QuotesSpider(scrapy.Spider):
        name = "quotes"
        start_urls = [
            "https://quotes.toscrape.com/page/1/",
        ]

        def parse(self, response):
            for quote in response.css("div.quote"):
                yield {
                    "text": quote.css("span.text::text").get(),
                    "author": quote.css("small.author::text").get(),
                    "tags": quote.css("div.tags a.tag::text").getall(),
                }

            next_page = response.css("li.next a::attr(href)").get()
            if next_page is not None:
                next_page = response.urljoin(next_page)
                yield scrapy.Request(next_page, callback=self.parse)
:::
:::

Now, after extracting the data, the [`parse()`{.docutils .literal
.notranslate}]{.pre} method looks for the link to the next page, builds
a full absolute URL using the [[`urljoin()`{.xref .py .py-meth .docutils
.literal
.notranslate}]{.pre}](index.html#scrapy.http.Response.urljoin "scrapy.http.Response.urljoin"){.reference
.internal} method (since the links can be relative) and yields a new
request to the next page, registering itself as callback to handle the
data extraction for the next page and to keep the crawling going through
all the pages.

What you see here is Scrapy's mechanism of following links: when you
yield a Request in a callback method, Scrapy will schedule that request
to be sent and register a callback method to be executed when that
request finishes.

Using this, you can build complex crawlers that follow links according
to rules you define, and extract different kinds of data depending on
the page it's visiting.

In our example, it creates a sort of loop, following all the links to
the next page until it doesn't find one -- handy for crawling blogs,
forums and other sites with pagination.

::: {#a-shortcut-for-creating-requests .section}
[]{#response-follow-example}

##### A shortcut for creating Requests[¶](#a-shortcut-for-creating-requests "Permalink to this heading"){.headerlink}

As a shortcut for creating Request objects you can use
[[`response.follow`{.xref .py .py-meth .docutils .literal
.notranslate}]{.pre}](index.html#scrapy.http.TextResponse.follow "scrapy.http.TextResponse.follow"){.reference
.internal}:

::: {.highlight-python .notranslate}
::: highlight
    import scrapy


    class QuotesSpider(scrapy.Spider):
        name = "quotes"
        start_urls = [
            "https://quotes.toscrape.com/page/1/",
        ]

        def parse(self, response):
            for quote in response.css("div.quote"):
                yield {
                    "text": quote.css("span.text::text").get(),
                    "author": quote.css("span small::text").get(),
                    "tags": quote.css("div.tags a.tag::text").getall(),
                }

            next_page = response.css("li.next a::attr(href)").get()
            if next_page is not None:
                yield response.follow(next_page, callback=self.parse)
:::
:::

Unlike scrapy.Request, [`response.follow`{.docutils .literal
.notranslate}]{.pre} supports relative URLs directly - no need to call
urljoin. Note that [`response.follow`{.docutils .literal
.notranslate}]{.pre} just returns a Request instance; you still have to
yield this Request.

You can also pass a selector to [`response.follow`{.docutils .literal
.notranslate}]{.pre} instead of a string; this selector should extract
necessary attributes:

::: {.highlight-python .notranslate}
::: highlight
    for href in response.css("ul.pager a::attr(href)"):
        yield response.follow(href, callback=self.parse)
:::
:::

For [`<a>`{.docutils .literal .notranslate}]{.pre} elements there is a
shortcut: [`response.follow`{.docutils .literal .notranslate}]{.pre}
uses their href attribute automatically. So the code can be shortened
further:

::: {.highlight-python .notranslate}
::: highlight
    for a in response.css("ul.pager a"):
        yield response.follow(a, callback=self.parse)
:::
:::

To create multiple requests from an iterable, you can use
[[`response.follow_all`{.xref .py .py-meth .docutils .literal
.notranslate}]{.pre}](index.html#scrapy.http.TextResponse.follow_all "scrapy.http.TextResponse.follow_all"){.reference
.internal} instead:

::: {.highlight-python .notranslate}
::: highlight
    anchors = response.css("ul.pager a")
    yield from response.follow_all(anchors, callback=self.parse)
:::
:::

or, shortening it further:

::: {.highlight-python .notranslate}
::: highlight
    yield from response.follow_all(css="ul.pager a", callback=self.parse)
:::
:::
:::

::: {#more-examples-and-patterns .section}
##### More examples and patterns[¶](#more-examples-and-patterns "Permalink to this heading"){.headerlink}

Here is another spider that illustrates callbacks and following links,
this time for scraping author information:

::: {.highlight-python .notranslate}
::: highlight
    import scrapy


    class AuthorSpider(scrapy.Spider):
        name = "author"

        start_urls = ["https://quotes.toscrape.com/"]

        def parse(self, response):
            author_page_links = response.css(".author + a")
            yield from response.follow_all(author_page_links, self.parse_author)

            pagination_links = response.css("li.next a")
            yield from response.follow_all(pagination_links, self.parse)

        def parse_author(self, response):
            def extract_with_css(query):
                return response.css(query).get(default="").strip()

            yield {
                "name": extract_with_css("h3.author-title::text"),
                "birthdate": extract_with_css(".author-born-date::text"),
                "bio": extract_with_css(".author-description::text"),
            }
:::
:::

This spider will start from the main page, it will follow all the links
to the authors pages calling the [`parse_author`{.docutils .literal
.notranslate}]{.pre} callback for each of them, and also the pagination
links with the [`parse`{.docutils .literal .notranslate}]{.pre} callback
as we saw before.

Here we're passing callbacks to [[`response.follow_all`{.xref .py
.py-meth .docutils .literal
.notranslate}]{.pre}](index.html#scrapy.http.TextResponse.follow_all "scrapy.http.TextResponse.follow_all"){.reference
.internal} as positional arguments to make the code shorter; it also
works for [`Request`{.xref .py .py-class .docutils .literal
.notranslate}]{.pre}.

The [`parse_author`{.docutils .literal .notranslate}]{.pre} callback
defines a helper function to extract and cleanup the data from a CSS
query and yields the Python dict with the author data.

Another interesting thing this spider demonstrates is that, even if
there are many quotes from the same author, we don't need to worry about
visiting the same author page multiple times. By default, Scrapy filters
out duplicated requests to URLs already visited, avoiding the problem of
hitting servers too much because of a programming mistake. This can be
configured by the setting [[`DUPEFILTER_CLASS`{.xref .std .std-setting
.docutils .literal
.notranslate}]{.pre}](index.html#std-setting-DUPEFILTER_CLASS){.hoverxref
.tooltip .reference .internal}.

Hopefully by now you have a good understanding of how to use the
mechanism of following links and callbacks with Scrapy.

As yet another example spider that leverages the mechanism of following
links, check out the [[`CrawlSpider`{.xref .py .py-class .docutils
.literal
.notranslate}]{.pre}](index.html#scrapy.spiders.CrawlSpider "scrapy.spiders.CrawlSpider"){.reference
.internal} class for a generic spider that implements a small rules
engine that you can use to write your crawlers on top of it.

Also, a common pattern is to build an item with data from more than one
page, using a [[trick to pass additional data to the callbacks]{.std
.std-ref}](index.html#topics-request-response-ref-request-callback-arguments){.hoverxref
.tooltip .reference .internal}.
:::
:::

::: {#using-spider-arguments .section}
#### Using spider arguments[¶](#using-spider-arguments "Permalink to this heading"){.headerlink}

You can provide command line arguments to your spiders by using the
[`-a`{.docutils .literal .notranslate}]{.pre} option when running them:

::: {.highlight-default .notranslate}
::: highlight
    scrapy crawl quotes -O quotes-humor.json -a tag=humor
:::
:::

These arguments are passed to the Spider's [`__init__`{.docutils
.literal .notranslate}]{.pre} method and become spider attributes by
default.

In this example, the value provided for the [`tag`{.docutils .literal
.notranslate}]{.pre} argument will be available via
[`self.tag`{.docutils .literal .notranslate}]{.pre}. You can use this to
make your spider fetch only quotes with a specific tag, building the URL
based on the argument:

::: {.highlight-python .notranslate}
::: highlight
    import scrapy


    class QuotesSpider(scrapy.Spider):
        name = "quotes"

        def start_requests(self):
            url = "https://quotes.toscrape.com/"
            tag = getattr(self, "tag", None)
            if tag is not None:
                url = url + "tag/" + tag
            yield scrapy.Request(url, self.parse)

        def parse(self, response):
            for quote in response.css("div.quote"):
                yield {
                    "text": quote.css("span.text::text").get(),
                    "author": quote.css("small.author::text").get(),
                }

            next_page = response.css("li.next a::attr(href)").get()
            if next_page is not None:
                yield response.follow(next_page, self.parse)
:::
:::

If you pass the [`tag=humor`{.docutils .literal .notranslate}]{.pre}
argument to this spider, you'll notice that it will only visit URLs from
the [`humor`{.docutils .literal .notranslate}]{.pre} tag, such as
[`https://quotes.toscrape.com/tag/humor`{.docutils .literal
.notranslate}]{.pre}.

You can [[learn more about handling spider arguments here]{.std
.std-ref}](index.html#spiderargs){.hoverxref .tooltip .reference
.internal}.
:::

::: {#next-steps .section}
#### Next steps[¶](#next-steps "Permalink to this heading"){.headerlink}

This tutorial covered only the basics of Scrapy, but there's a lot of
other features not mentioned here. Check the [[What else?]{.std
.std-ref}](index.html#topics-whatelse){.hoverxref .tooltip .reference
.internal} section in [[Scrapy at a glance]{.std
.std-ref}](index.html#intro-overview){.hoverxref .tooltip .reference
.internal} chapter for a quick overview of the most important ones.

You can continue from the section [[Basic concepts]{.std
.std-ref}](index.html#section-basics){.hoverxref .tooltip .reference
.internal} to know more about the command-line tool, spiders, selectors
and other things the tutorial hasn't covered like modeling the scraped
data. If you prefer to play with an example project, check the
[[Examples]{.std .std-ref}](index.html#intro-examples){.hoverxref
.tooltip .reference .internal} section.
:::
:::

[]{#document-intro/examples}

::: {#examples .section}
[]{#intro-examples}

### Examples[¶](#examples "Permalink to this heading"){.headerlink}

The best way to learn is with examples, and Scrapy is no exception. For
this reason, there is an example Scrapy project named
[quotesbot](https://github.com/scrapy/quotesbot){.reference .external},
that you can use to play and learn more about Scrapy. It contains two
spiders for
[https://quotes.toscrape.com](https://quotes.toscrape.com){.reference
.external}, one using CSS selectors and another one using XPath
expressions.

The [quotesbot](https://github.com/scrapy/quotesbot){.reference
.external} project is available at:
[https://github.com/scrapy/quotesbot](https://github.com/scrapy/quotesbot){.reference
.external}. You can find more information about it in the project's
README.

If you're familiar with git, you can checkout the code. Otherwise you
can download the project as a zip file by clicking
[here](https://github.com/scrapy/quotesbot/archive/master.zip){.reference
.external}.
:::
:::

[[Scrapy at a glance]{.doc}](index.html#document-intro/overview){.reference .internal}

:   Understand what Scrapy is and how it can help you.

[[Installation guide]{.doc}](index.html#document-intro/install){.reference .internal}

:   Get Scrapy installed on your computer.

[[Scrapy Tutorial]{.doc}](index.html#document-intro/tutorial){.reference .internal}

:   Write your first Scrapy project.

[[Examples]{.doc}](index.html#document-intro/examples){.reference .internal}

:   Learn more by playing with a pre-made Scrapy project.
:::

::: {#basic-concepts .section}
[]{#section-basics}

## Basic concepts[¶](#basic-concepts "Permalink to this heading"){.headerlink}

::: {.toctree-wrapper .compound}
[]{#document-topics/commands}

::: {#command-line-tool .section}
[]{#topics-commands}

### Command line tool[¶](#command-line-tool "Permalink to this heading"){.headerlink}

Scrapy is controlled through the [`scrapy`{.docutils .literal
.notranslate}]{.pre} command-line tool, to be referred here as the
"Scrapy tool" to differentiate it from the sub-commands, which we just
call "commands" or "Scrapy commands".

The Scrapy tool provides several commands, for multiple purposes, and
each one accepts a different set of arguments and options.

(The [`scrapy`{.docutils .literal .notranslate}]{.pre}` `{.docutils
.literal .notranslate}[`deploy`{.docutils .literal .notranslate}]{.pre}
command has been removed in 1.0 in favor of the standalone
[`scrapyd-deploy`{.docutils .literal .notranslate}]{.pre}. See
[Deploying your
project](https://scrapyd.readthedocs.io/en/latest/deploy.html){.reference
.external}.)

::: {#configuration-settings .section}
[]{#topics-config-settings}

#### Configuration settings[¶](#configuration-settings "Permalink to this heading"){.headerlink}

Scrapy will look for configuration parameters in ini-style
[`scrapy.cfg`{.docutils .literal .notranslate}]{.pre} files in standard
locations:

1.  [`/etc/scrapy.cfg`{.docutils .literal .notranslate}]{.pre} or
    [`c:\scrapy\scrapy.cfg`{.docutils .literal .notranslate}]{.pre}
    (system-wide),

2.  [`~/.config/scrapy.cfg`{.docutils .literal .notranslate}]{.pre}
    ([`$XDG_CONFIG_HOME`{.docutils .literal .notranslate}]{.pre}) and
    [`~/.scrapy.cfg`{.docutils .literal .notranslate}]{.pre}
    ([`$HOME`{.docutils .literal .notranslate}]{.pre}) for global
    (user-wide) settings, and

3.  [`scrapy.cfg`{.docutils .literal .notranslate}]{.pre} inside a
    Scrapy project's root (see next section).

Settings from these files are merged in the listed order of preference:
user-defined values have higher priority than system-wide defaults and
project-wide settings will override all others, when defined.

Scrapy also understands, and can be configured through, a number of
environment variables. Currently these are:

-   [`SCRAPY_SETTINGS_MODULE`{.docutils .literal .notranslate}]{.pre}
    (see [[Designating the settings]{.std
    .std-ref}](index.html#topics-settings-module-envvar){.hoverxref
    .tooltip .reference .internal})

-   [`SCRAPY_PROJECT`{.docutils .literal .notranslate}]{.pre} (see
    [[Sharing the root directory between projects]{.std
    .std-ref}](#topics-project-envvar){.hoverxref .tooltip .reference
    .internal})

-   [`SCRAPY_PYTHON_SHELL`{.docutils .literal .notranslate}]{.pre} (see
    [[Scrapy shell]{.std .std-ref}](index.html#topics-shell){.hoverxref
    .tooltip .reference .internal})
:::

::: {#default-structure-of-scrapy-projects .section}
[]{#topics-project-structure}

#### Default structure of Scrapy projects[¶](#default-structure-of-scrapy-projects "Permalink to this heading"){.headerlink}

Before delving into the command-line tool and its sub-commands, let's
first understand the directory structure of a Scrapy project.

Though it can be modified, all Scrapy projects have the same file
structure by default, similar to this:

::: {.highlight-none .notranslate}
::: highlight
    scrapy.cfg
    myproject/
        __init__.py
        items.py
        middlewares.py
        pipelines.py
        settings.py
        spiders/
            __init__.py
            spider1.py
            spider2.py
            ...
:::
:::

The directory where the [`scrapy.cfg`{.docutils .literal
.notranslate}]{.pre} file resides is known as the *project root
directory*. That file contains the name of the python module that
defines the project settings. Here is an example:

::: {.highlight-ini .notranslate}
::: highlight
    [settings]
    default = myproject.settings
:::
:::
:::

::: {#sharing-the-root-directory-between-projects .section}
[]{#topics-project-envvar}

#### Sharing the root directory between projects[¶](#sharing-the-root-directory-between-projects "Permalink to this heading"){.headerlink}

A project root directory, the one that contains the
[`scrapy.cfg`{.docutils .literal .notranslate}]{.pre}, may be shared by
multiple Scrapy projects, each with its own settings module.

In that case, you must define one or more aliases for those settings
modules under [`[settings]`{.docutils .literal .notranslate}]{.pre} in
your [`scrapy.cfg`{.docutils .literal .notranslate}]{.pre} file:

::: {.highlight-ini .notranslate}
::: highlight
    [settings]
    default = myproject1.settings
    project1 = myproject1.settings
    project2 = myproject2.settings
:::
:::

By default, the [`scrapy`{.docutils .literal .notranslate}]{.pre}
command-line tool will use the [`default`{.docutils .literal
.notranslate}]{.pre} settings. Use the [`SCRAPY_PROJECT`{.docutils
.literal .notranslate}]{.pre} environment variable to specify a
different project for [`scrapy`{.docutils .literal .notranslate}]{.pre}
to use:

::: {.highlight-none .notranslate}
::: highlight
    $ scrapy settings --get BOT_NAME
    Project 1 Bot
    $ export SCRAPY_PROJECT=project2
    $ scrapy settings --get BOT_NAME
    Project 2 Bot
:::
:::
:::

::: {#using-the-scrapy-tool .section}
#### Using the [`scrapy`{.docutils .literal .notranslate}]{.pre} tool[¶](#using-the-scrapy-tool "Permalink to this heading"){.headerlink}

You can start by running the Scrapy tool with no arguments and it will
print some usage help and the available commands:

::: {.highlight-none .notranslate}
::: highlight
    Scrapy X.Y - no active project

    Usage:
      scrapy <command> [options] [args]

    Available commands:
      crawl         Run a spider
      fetch         Fetch a URL using the Scrapy downloader
    [...]
:::
:::

The first line will print the currently active project if you're inside
a Scrapy project. In this example it was run from outside a project. If
run from inside a project it would have printed something like this:

::: {.highlight-none .notranslate}
::: highlight
    Scrapy X.Y - project: myproject

    Usage:
      scrapy <command> [options] [args]

    [...]
:::
:::

::: {#creating-projects .section}
##### Creating projects[¶](#creating-projects "Permalink to this heading"){.headerlink}

The first thing you typically do with the [`scrapy`{.docutils .literal
.notranslate}]{.pre} tool is create your Scrapy project:

::: {.highlight-none .notranslate}
::: highlight
    scrapy startproject myproject [project_dir]
:::
:::

That will create a Scrapy project under the [`project_dir`{.docutils
.literal .notranslate}]{.pre} directory. If [`project_dir`{.docutils
.literal .notranslate}]{.pre} wasn't specified, [`project_dir`{.docutils
.literal .notranslate}]{.pre} will be the same as [`myproject`{.docutils
.literal .notranslate}]{.pre}.

Next, you go inside the new project directory:

::: {.highlight-none .notranslate}
::: highlight
    cd project_dir
:::
:::

And you're ready to use the [`scrapy`{.docutils .literal
.notranslate}]{.pre} command to manage and control your project from
there.
:::

::: {#controlling-projects .section}
##### Controlling projects[¶](#controlling-projects "Permalink to this heading"){.headerlink}

You use the [`scrapy`{.docutils .literal .notranslate}]{.pre} tool from
inside your projects to control and manage them.

For example, to create a new spider:

::: {.highlight-none .notranslate}
::: highlight
    scrapy genspider mydomain mydomain.com
:::
:::

Some Scrapy commands (like [[`crawl`{.xref .std .std-command .docutils
.literal .notranslate}]{.pre}](#std-command-crawl){.hoverxref .tooltip
.reference .internal}) must be run from inside a Scrapy project. See the
[[commands reference]{.std .std-ref}](#topics-commands-ref){.hoverxref
.tooltip .reference .internal} below for more information on which
commands must be run from inside projects, and which not.

Also keep in mind that some commands may have slightly different
behaviours when running them from inside projects. For example, the
fetch command will use spider-overridden behaviours (such as the
[`user_agent`{.docutils .literal .notranslate}]{.pre} attribute to
override the user-agent) if the url being fetched is associated with
some specific spider. This is intentional, as the [`fetch`{.docutils
.literal .notranslate}]{.pre} command is meant to be used to check how
spiders are downloading pages.
:::
:::

::: {#available-tool-commands .section}
[]{#topics-commands-ref}

#### Available tool commands[¶](#available-tool-commands "Permalink to this heading"){.headerlink}

This section contains a list of the available built-in commands with a
description and some usage examples. Remember, you can always get more
info about each command by running:

::: {.highlight-none .notranslate}
::: highlight
    scrapy <command> -h
:::
:::

And you can see all available commands with:

::: {.highlight-none .notranslate}
::: highlight
    scrapy -h
:::
:::

There are two kinds of commands, those that only work from inside a
Scrapy project (Project-specific commands) and those that also work
without an active Scrapy project (Global commands), though they may
behave slightly different when running from inside a project (as they
would use the project overridden settings).

Global commands:

-   [[`startproject`{.xref .std .std-command .docutils .literal
    .notranslate}]{.pre}](#std-command-startproject){.hoverxref .tooltip
    .reference .internal}

-   [[`genspider`{.xref .std .std-command .docutils .literal
    .notranslate}]{.pre}](#std-command-genspider){.hoverxref .tooltip
    .reference .internal}

-   [[`settings`{.xref .std .std-command .docutils .literal
    .notranslate}]{.pre}](#std-command-settings){.hoverxref .tooltip
    .reference .internal}

-   [[`runspider`{.xref .std .std-command .docutils .literal
    .notranslate}]{.pre}](#std-command-runspider){.hoverxref .tooltip
    .reference .internal}

-   [[`shell`{.xref .std .std-command .docutils .literal
    .notranslate}]{.pre}](#std-command-shell){.hoverxref .tooltip
    .reference .internal}

-   [[`fetch`{.xref .std .std-command .docutils .literal
    .notranslate}]{.pre}](#std-command-fetch){.hoverxref .tooltip
    .reference .internal}

-   [[`view`{.xref .std .std-command .docutils .literal
    .notranslate}]{.pre}](#std-command-view){.hoverxref .tooltip
    .reference .internal}

-   [[`version`{.xref .std .std-command .docutils .literal
    .notranslate}]{.pre}](#std-command-version){.hoverxref .tooltip
    .reference .internal}

Project-only commands:

-   [[`crawl`{.xref .std .std-command .docutils .literal
    .notranslate}]{.pre}](#std-command-crawl){.hoverxref .tooltip
    .reference .internal}

-   [[`check`{.xref .std .std-command .docutils .literal
    .notranslate}]{.pre}](#std-command-check){.hoverxref .tooltip
    .reference .internal}

-   [[`list`{.xref .std .std-command .docutils .literal
    .notranslate}]{.pre}](#std-command-list){.hoverxref .tooltip
    .reference .internal}

-   [[`edit`{.xref .std .std-command .docutils .literal
    .notranslate}]{.pre}](#std-command-edit){.hoverxref .tooltip
    .reference .internal}

-   [[`parse`{.xref .std .std-command .docutils .literal
    .notranslate}]{.pre}](#std-command-parse){.hoverxref .tooltip
    .reference .internal}

-   [[`bench`{.xref .std .std-command .docutils .literal
    .notranslate}]{.pre}](#std-command-bench){.hoverxref .tooltip
    .reference .internal}

::: {#startproject .section}
[]{#std-command-startproject}

##### startproject[¶](#startproject "Permalink to this heading"){.headerlink}

-   Syntax: [`scrapy`{.docutils .literal
    .notranslate}]{.pre}` `{.docutils .literal
    .notranslate}[`startproject`{.docutils .literal
    .notranslate}]{.pre}` `{.docutils .literal
    .notranslate}[`<project_name>`{.docutils .literal
    .notranslate}]{.pre}` `{.docutils .literal
    .notranslate}[`[project_dir]`{.docutils .literal
    .notranslate}]{.pre}

-   Requires project: *no*

Creates a new Scrapy project named [`project_name`{.docutils .literal
.notranslate}]{.pre}, under the [`project_dir`{.docutils .literal
.notranslate}]{.pre} directory. If [`project_dir`{.docutils .literal
.notranslate}]{.pre} wasn't specified, [`project_dir`{.docutils .literal
.notranslate}]{.pre} will be the same as [`project_name`{.docutils
.literal .notranslate}]{.pre}.

Usage example:

::: {.highlight-none .notranslate}
::: highlight
    $ scrapy startproject myproject
:::
:::
:::

::: {#genspider .section}
[]{#std-command-genspider}

##### genspider[¶](#genspider "Permalink to this heading"){.headerlink}

-   Syntax: [`scrapy`{.docutils .literal
    .notranslate}]{.pre}` `{.docutils .literal
    .notranslate}[`genspider`{.docutils .literal
    .notranslate}]{.pre}` `{.docutils .literal
    .notranslate}[`[-t`{.docutils .literal
    .notranslate}]{.pre}` `{.docutils .literal
    .notranslate}[`template]`{.docutils .literal
    .notranslate}]{.pre}` `{.docutils .literal
    .notranslate}[`<name>`{.docutils .literal
    .notranslate}]{.pre}` `{.docutils .literal
    .notranslate}[`<domain`{.docutils .literal
    .notranslate}]{.pre}` `{.docutils .literal
    .notranslate}[`or`{.docutils .literal
    .notranslate}]{.pre}` `{.docutils .literal
    .notranslate}[`URL>`{.docutils .literal .notranslate}]{.pre}

-   Requires project: *no*

::: versionadded
[New in version 2.6.0: ]{.versionmodified .added}The ability to pass a
URL instead of a domain.
:::

Create a new spider in the current folder or in the current project's
[`spiders`{.docutils .literal .notranslate}]{.pre} folder, if called
from inside a project. The [`<name>`{.docutils .literal
.notranslate}]{.pre} parameter is set as the spider's [`name`{.docutils
.literal .notranslate}]{.pre}, while [`<domain`{.docutils .literal
.notranslate}]{.pre}` `{.docutils .literal .notranslate}[`or`{.docutils
.literal .notranslate}]{.pre}` `{.docutils .literal
.notranslate}[`URL>`{.docutils .literal .notranslate}]{.pre} is used to
generate the [`allowed_domains`{.docutils .literal .notranslate}]{.pre}
and [`start_urls`{.docutils .literal .notranslate}]{.pre} spider's
attributes.

Usage example:

::: {.highlight-none .notranslate}
::: highlight
    $ scrapy genspider -l
    Available templates:
      basic
      crawl
      csvfeed
      xmlfeed

    $ scrapy genspider example example.com
    Created spider 'example' using template 'basic'

    $ scrapy genspider -t crawl scrapyorg scrapy.org
    Created spider 'scrapyorg' using template 'crawl'
:::
:::

This is just a convenience shortcut command for creating spiders based
on pre-defined templates, but certainly not the only way to create
spiders. You can just create the spider source code files yourself,
instead of using this command.
:::

::: {#crawl .section}
[]{#std-command-crawl}

##### crawl[¶](#crawl "Permalink to this heading"){.headerlink}

-   Syntax: [`scrapy`{.docutils .literal
    .notranslate}]{.pre}` `{.docutils .literal
    .notranslate}[`crawl`{.docutils .literal
    .notranslate}]{.pre}` `{.docutils .literal
    .notranslate}[`<spider>`{.docutils .literal .notranslate}]{.pre}

-   Requires project: *yes*

Start crawling using a spider.

Supported options:

-   [`-h,`{.docutils .literal .notranslate}]{.pre}` `{.docutils .literal
    .notranslate}[`--help`{.docutils .literal .notranslate}]{.pre}: show
    a help message and exit

-   [`-a`{.docutils .literal .notranslate}]{.pre}` `{.docutils .literal
    .notranslate}[`NAME=VALUE`{.docutils .literal .notranslate}]{.pre}:
    set a spider argument (may be repeated)

-   [`--output`{.docutils .literal .notranslate}]{.pre}` `{.docutils
    .literal .notranslate}[`FILE`{.docutils .literal
    .notranslate}]{.pre} or [`-o`{.docutils .literal
    .notranslate}]{.pre}` `{.docutils .literal
    .notranslate}[`FILE`{.docutils .literal .notranslate}]{.pre}: append
    scraped items to the end of FILE (use - for stdout), to define
    format set a colon at the end of the output URI (i.e.
    [`-o`{.docutils .literal .notranslate}]{.pre}` `{.docutils .literal
    .notranslate}[`FILE:FORMAT`{.docutils .literal .notranslate}]{.pre})

-   [`--overwrite-output`{.docutils .literal
    .notranslate}]{.pre}` `{.docutils .literal
    .notranslate}[`FILE`{.docutils .literal .notranslate}]{.pre} or
    [`-O`{.docutils .literal .notranslate}]{.pre}` `{.docutils .literal
    .notranslate}[`FILE`{.docutils .literal .notranslate}]{.pre}: dump
    scraped items into FILE, overwriting any existing file, to define
    format set a colon at the end of the output URI (i.e.
    [`-O`{.docutils .literal .notranslate}]{.pre}` `{.docutils .literal
    .notranslate}[`FILE:FORMAT`{.docutils .literal .notranslate}]{.pre})

-   [`--output-format`{.docutils .literal
    .notranslate}]{.pre}` `{.docutils .literal
    .notranslate}[`FORMAT`{.docutils .literal .notranslate}]{.pre} or
    [`-t`{.docutils .literal .notranslate}]{.pre}` `{.docutils .literal
    .notranslate}[`FORMAT`{.docutils .literal .notranslate}]{.pre}:
    deprecated way to define format to use for dumping items, does not
    work in combination with [`-O`{.docutils .literal
    .notranslate}]{.pre}

Usage examples:

::: {.highlight-none .notranslate}
::: highlight
    $ scrapy crawl myspider
    [ ... myspider starts crawling ... ]

    $ scrapy crawl -o myfile:csv myspider
    [ ... myspider starts crawling and appends the result to the file myfile in csv format ... ]

    $ scrapy crawl -O myfile:json myspider
    [ ... myspider starts crawling and saves the result in myfile in json format overwriting the original content... ]

    $ scrapy crawl -o myfile -t csv myspider
    [ ... myspider starts crawling and appends the result to the file myfile in csv format ... ]
:::
:::
:::

::: {#check .section}
[]{#std-command-check}

##### check[¶](#check "Permalink to this heading"){.headerlink}

-   Syntax: [`scrapy`{.docutils .literal
    .notranslate}]{.pre}` `{.docutils .literal
    .notranslate}[`check`{.docutils .literal
    .notranslate}]{.pre}` `{.docutils .literal
    .notranslate}[`[-l]`{.docutils .literal
    .notranslate}]{.pre}` `{.docutils .literal
    .notranslate}[`<spider>`{.docutils .literal .notranslate}]{.pre}

-   Requires project: *yes*

Run contract checks.

Usage examples:

::: {.highlight-none .notranslate}
::: highlight
    $ scrapy check -l
    first_spider
      * parse
      * parse_item
    second_spider
      * parse
      * parse_item

    $ scrapy check
    [FAILED] first_spider:parse_item
    >>> 'RetailPricex' field is missing

    [FAILED] first_spider:parse
    >>> Returned 92 requests, expected 0..4
:::
:::
:::

::: {#list .section}
[]{#std-command-list}

##### list[¶](#list "Permalink to this heading"){.headerlink}

-   Syntax: [`scrapy`{.docutils .literal
    .notranslate}]{.pre}` `{.docutils .literal
    .notranslate}[`list`{.docutils .literal .notranslate}]{.pre}

-   Requires project: *yes*

List all available spiders in the current project. The output is one
spider per line.

Usage example:

::: {.highlight-none .notranslate}
::: highlight
    $ scrapy list
    spider1
    spider2
:::
:::
:::

::: {#edit .section}
[]{#std-command-edit}

##### edit[¶](#edit "Permalink to this heading"){.headerlink}

-   Syntax: [`scrapy`{.docutils .literal
    .notranslate}]{.pre}` `{.docutils .literal
    .notranslate}[`edit`{.docutils .literal
    .notranslate}]{.pre}` `{.docutils .literal
    .notranslate}[`<spider>`{.docutils .literal .notranslate}]{.pre}

-   Requires project: *yes*

Edit the given spider using the editor defined in the
[`EDITOR`{.docutils .literal .notranslate}]{.pre} environment variable
or (if unset) the [[`EDITOR`{.xref .std .std-setting .docutils .literal
.notranslate}]{.pre}](index.html#std-setting-EDITOR){.hoverxref .tooltip
.reference .internal} setting.

This command is provided only as a convenience shortcut for the most
common case, the developer is of course free to choose any tool or IDE
to write and debug spiders.

Usage example:

::: {.highlight-none .notranslate}
::: highlight
    $ scrapy edit spider1
:::
:::
:::

::: {#fetch .section}
[]{#std-command-fetch}

##### fetch[¶](#fetch "Permalink to this heading"){.headerlink}

-   Syntax: [`scrapy`{.docutils .literal
    .notranslate}]{.pre}` `{.docutils .literal
    .notranslate}[`fetch`{.docutils .literal
    .notranslate}]{.pre}` `{.docutils .literal
    .notranslate}[`<url>`{.docutils .literal .notranslate}]{.pre}

-   Requires project: *no*

Downloads the given URL using the Scrapy downloader and writes the
contents to standard output.

The interesting thing about this command is that it fetches the page how
the spider would download it. For example, if the spider has a
[`USER_AGENT`{.docutils .literal .notranslate}]{.pre} attribute which
overrides the User Agent, it will use that one.

So this command can be used to "see" how your spider would fetch a
certain page.

If used outside a project, no particular per-spider behaviour would be
applied and it will just use the default Scrapy downloader settings.

Supported options:

-   [`--spider=SPIDER`{.docutils .literal .notranslate}]{.pre}: bypass
    spider autodetection and force use of specific spider

-   [`--headers`{.docutils .literal .notranslate}]{.pre}: print the
    response's HTTP headers instead of the response's body

-   [`--no-redirect`{.docutils .literal .notranslate}]{.pre}: do not
    follow HTTP 3xx redirects (default is to follow them)

Usage examples:

::: {.highlight-none .notranslate}
::: highlight
    $ scrapy fetch --nolog http://www.example.com/some/page.html
    [ ... html content here ... ]

    $ scrapy fetch --nolog --headers http://www.example.com/
    {'Accept-Ranges': ['bytes'],
     'Age': ['1263   '],
     'Connection': ['close     '],
     'Content-Length': ['596'],
     'Content-Type': ['text/html; charset=UTF-8'],
     'Date': ['Wed, 18 Aug 2010 23:59:46 GMT'],
     'Etag': ['"573c1-254-48c9c87349680"'],
     'Last-Modified': ['Fri, 30 Jul 2010 15:30:18 GMT'],
     'Server': ['Apache/2.2.3 (CentOS)']}
:::
:::
:::

::: {#view .section}
[]{#std-command-view}

##### view[¶](#view "Permalink to this heading"){.headerlink}

-   Syntax: [`scrapy`{.docutils .literal
    .notranslate}]{.pre}` `{.docutils .literal
    .notranslate}[`view`{.docutils .literal
    .notranslate}]{.pre}` `{.docutils .literal
    .notranslate}[`<url>`{.docutils .literal .notranslate}]{.pre}

-   Requires project: *no*

Opens the given URL in a browser, as your Scrapy spider would "see" it.
Sometimes spiders see pages differently from regular users, so this can
be used to check what the spider "sees" and confirm it's what you
expect.

Supported options:

-   [`--spider=SPIDER`{.docutils .literal .notranslate}]{.pre}: bypass
    spider autodetection and force use of specific spider

-   [`--no-redirect`{.docutils .literal .notranslate}]{.pre}: do not
    follow HTTP 3xx redirects (default is to follow them)

Usage example:

::: {.highlight-none .notranslate}
::: highlight
    $ scrapy view http://www.example.com/some/page.html
    [ ... browser starts ... ]
:::
:::
:::

::: {#shell .section}
[]{#std-command-shell}

##### shell[¶](#shell "Permalink to this heading"){.headerlink}

-   Syntax: [`scrapy`{.docutils .literal
    .notranslate}]{.pre}` `{.docutils .literal
    .notranslate}[`shell`{.docutils .literal
    .notranslate}]{.pre}` `{.docutils .literal
    .notranslate}[`[url]`{.docutils .literal .notranslate}]{.pre}

-   Requires project: *no*

Starts the Scrapy shell for the given URL (if given) or empty if no URL
is given. Also supports UNIX-style local file paths, either relative
with [`./`{.docutils .literal .notranslate}]{.pre} or [`../`{.docutils
.literal .notranslate}]{.pre} prefixes or absolute file paths. See
[[Scrapy shell]{.std .std-ref}](index.html#topics-shell){.hoverxref
.tooltip .reference .internal} for more info.

Supported options:

-   [`--spider=SPIDER`{.docutils .literal .notranslate}]{.pre}: bypass
    spider autodetection and force use of specific spider

-   [`-c`{.docutils .literal .notranslate}]{.pre}` `{.docutils .literal
    .notranslate}[`code`{.docutils .literal .notranslate}]{.pre}:
    evaluate the code in the shell, print the result and exit

-   [`--no-redirect`{.docutils .literal .notranslate}]{.pre}: do not
    follow HTTP 3xx redirects (default is to follow them); this only
    affects the URL you may pass as argument on the command line; once
    you are inside the shell, [`fetch(url)`{.docutils .literal
    .notranslate}]{.pre} will still follow HTTP redirects by default.

Usage example:

::: {.highlight-none .notranslate}
::: highlight
    $ scrapy shell http://www.example.com/some/page.html
    [ ... scrapy shell starts ... ]

    $ scrapy shell --nolog http://www.example.com/ -c '(response.status, response.url)'
    (200, 'http://www.example.com/')

    # shell follows HTTP redirects by default
    $ scrapy shell --nolog http://httpbin.org/redirect-to?url=http%3A%2F%2Fexample.com%2F -c '(response.status, response.url)'
    (200, 'http://example.com/')

    # you can disable this with --no-redirect
    # (only for the URL passed as command line argument)
    $ scrapy shell --no-redirect --nolog http://httpbin.org/redirect-to?url=http%3A%2F%2Fexample.com%2F -c '(response.status, response.url)'
    (302, 'http://httpbin.org/redirect-to?url=http%3A%2F%2Fexample.com%2F')
:::
:::
:::

::: {#parse .section}
[]{#std-command-parse}

##### parse[¶](#parse "Permalink to this heading"){.headerlink}

-   Syntax: [`scrapy`{.docutils .literal
    .notranslate}]{.pre}` `{.docutils .literal
    .notranslate}[`parse`{.docutils .literal
    .notranslate}]{.pre}` `{.docutils .literal
    .notranslate}[`<url>`{.docutils .literal
    .notranslate}]{.pre}` `{.docutils .literal
    .notranslate}[`[options]`{.docutils .literal .notranslate}]{.pre}

-   Requires project: *yes*

Fetches the given URL and parses it with the spider that handles it,
using the method passed with the [`--callback`{.docutils .literal
.notranslate}]{.pre} option, or [`parse`{.docutils .literal
.notranslate}]{.pre} if not given.

Supported options:

-   [`--spider=SPIDER`{.docutils .literal .notranslate}]{.pre}: bypass
    spider autodetection and force use of specific spider

-   [`--a`{.docutils .literal .notranslate}]{.pre}` `{.docutils .literal
    .notranslate}[`NAME=VALUE`{.docutils .literal .notranslate}]{.pre}:
    set spider argument (may be repeated)

-   [`--callback`{.docutils .literal .notranslate}]{.pre} or
    [`-c`{.docutils .literal .notranslate}]{.pre}: spider method to use
    as callback for parsing the response

-   [`--meta`{.docutils .literal .notranslate}]{.pre} or [`-m`{.docutils
    .literal .notranslate}]{.pre}: additional request meta that will be
    passed to the callback request. This must be a valid json string.
    Example: --meta='{"foo" : "bar"}'

-   [`--cbkwargs`{.docutils .literal .notranslate}]{.pre}: additional
    keyword arguments that will be passed to the callback. This must be
    a valid json string. Example: --cbkwargs='{"foo" : "bar"}'

-   [`--pipelines`{.docutils .literal .notranslate}]{.pre}: process
    items through pipelines

-   [`--rules`{.docutils .literal .notranslate}]{.pre} or
    [`-r`{.docutils .literal .notranslate}]{.pre}: use
    [[`CrawlSpider`{.xref .py .py-class .docutils .literal
    .notranslate}]{.pre}](index.html#scrapy.spiders.CrawlSpider "scrapy.spiders.CrawlSpider"){.reference
    .internal} rules to discover the callback (i.e. spider method) to
    use for parsing the response

-   [`--noitems`{.docutils .literal .notranslate}]{.pre}: don't show
    scraped items

-   [`--nolinks`{.docutils .literal .notranslate}]{.pre}: don't show
    extracted links

-   [`--nocolour`{.docutils .literal .notranslate}]{.pre}: avoid using
    pygments to colorize the output

-   [`--depth`{.docutils .literal .notranslate}]{.pre} or
    [`-d`{.docutils .literal .notranslate}]{.pre}: depth level for which
    the requests should be followed recursively (default: 1)

-   [`--verbose`{.docutils .literal .notranslate}]{.pre} or
    [`-v`{.docutils .literal .notranslate}]{.pre}: display information
    for each depth level

-   [`--output`{.docutils .literal .notranslate}]{.pre} or
    [`-o`{.docutils .literal .notranslate}]{.pre}: dump scraped items to
    a file

    ::: versionadded
    [New in version 2.3.]{.versionmodified .added}
    :::

Usage example:

::: {.highlight-none .notranslate}
::: highlight
    $ scrapy parse http://www.example.com/ -c parse_item
    [ ... scrapy log lines crawling example.com spider ... ]

    >>> STATUS DEPTH LEVEL 1 <<<
    # Scraped Items  ------------------------------------------------------------
    [{'name': 'Example item',
     'category': 'Furniture',
     'length': '12 cm'}]

    # Requests  -----------------------------------------------------------------
    []
:::
:::
:::

::: {#settings .section}
[]{#std-command-settings}

##### settings[¶](#settings "Permalink to this heading"){.headerlink}

-   Syntax: [`scrapy`{.docutils .literal
    .notranslate}]{.pre}` `{.docutils .literal
    .notranslate}[`settings`{.docutils .literal
    .notranslate}]{.pre}` `{.docutils .literal
    .notranslate}[`[options]`{.docutils .literal .notranslate}]{.pre}

-   Requires project: *no*

Get the value of a Scrapy setting.

If used inside a project it'll show the project setting value, otherwise
it'll show the default Scrapy value for that setting.

Example usage:

::: {.highlight-none .notranslate}
::: highlight
    $ scrapy settings --get BOT_NAME
    scrapybot
    $ scrapy settings --get DOWNLOAD_DELAY
    0
:::
:::
:::

::: {#runspider .section}
[]{#std-command-runspider}

##### runspider[¶](#runspider "Permalink to this heading"){.headerlink}

-   Syntax: [`scrapy`{.docutils .literal
    .notranslate}]{.pre}` `{.docutils .literal
    .notranslate}[`runspider`{.docutils .literal
    .notranslate}]{.pre}` `{.docutils .literal
    .notranslate}[`<spider_file.py>`{.docutils .literal
    .notranslate}]{.pre}

-   Requires project: *no*

Run a spider self-contained in a Python file, without having to create a
project.

Example usage:

::: {.highlight-none .notranslate}
::: highlight
    $ scrapy runspider myspider.py
    [ ... spider starts crawling ... ]
:::
:::
:::

::: {#version .section}
[]{#std-command-version}

##### version[¶](#version "Permalink to this heading"){.headerlink}

-   Syntax: [`scrapy`{.docutils .literal
    .notranslate}]{.pre}` `{.docutils .literal
    .notranslate}[`version`{.docutils .literal
    .notranslate}]{.pre}` `{.docutils .literal
    .notranslate}[`[-v]`{.docutils .literal .notranslate}]{.pre}

-   Requires project: *no*

Prints the Scrapy version. If used with [`-v`{.docutils .literal
.notranslate}]{.pre} it also prints Python, Twisted and Platform info,
which is useful for bug reports.
:::

::: {#bench .section}
[]{#std-command-bench}

##### bench[¶](#bench "Permalink to this heading"){.headerlink}

-   Syntax: [`scrapy`{.docutils .literal
    .notranslate}]{.pre}` `{.docutils .literal
    .notranslate}[`bench`{.docutils .literal .notranslate}]{.pre}

-   Requires project: *no*

Run a quick benchmark test. [[Benchmarking]{.std
.std-ref}](index.html#benchmarking){.hoverxref .tooltip .reference
.internal}.
:::
:::

::: {#custom-project-commands .section}
#### Custom project commands[¶](#custom-project-commands "Permalink to this heading"){.headerlink}

You can also add your custom project commands by using the
[[`COMMANDS_MODULE`{.xref .std .std-setting .docutils .literal
.notranslate}]{.pre}](#std-setting-COMMANDS_MODULE){.hoverxref .tooltip
.reference .internal} setting. See the Scrapy commands in
[scrapy/commands](https://github.com/scrapy/scrapy/tree/master/scrapy/commands){.reference
.external} for examples on how to implement your commands.

::: {#commands-module .section}
[]{#std-setting-COMMANDS_MODULE}

##### COMMANDS_MODULE[¶](#commands-module "Permalink to this heading"){.headerlink}

Default: [`''`{.docutils .literal .notranslate}]{.pre} (empty string)

A module to use for looking up custom Scrapy commands. This is used to
add custom commands for your Scrapy project.

Example:

::: {.highlight-python .notranslate}
::: highlight
    COMMANDS_MODULE = "mybot.commands"
:::
:::
:::

::: {#register-commands-via-setup-py-entry-points .section}
##### Register commands via setup.py entry points[¶](#register-commands-via-setup-py-entry-points "Permalink to this heading"){.headerlink}

You can also add Scrapy commands from an external library by adding a
[`scrapy.commands`{.docutils .literal .notranslate}]{.pre} section in
the entry points of the library [`setup.py`{.docutils .literal
.notranslate}]{.pre} file.

The following example adds [`my_command`{.docutils .literal
.notranslate}]{.pre} command:

::: {.highlight-python .notranslate}
::: highlight
    from setuptools import setup, find_packages

    setup(
        name="scrapy-mymodule",
        entry_points={
            "scrapy.commands": [
                "my_command=my_scrapy_module.commands:MyCommand",
            ],
        },
    )
:::
:::
:::
:::
:::

[]{#document-topics/spiders}

::: {#spiders .section}
[]{#topics-spiders}

### Spiders[¶](#spiders "Permalink to this heading"){.headerlink}

Spiders are classes which define how a certain site (or a group of
sites) will be scraped, including how to perform the crawl (i.e. follow
links) and how to extract structured data from their pages (i.e.
scraping items). In other words, Spiders are the place where you define
the custom behaviour for crawling and parsing pages for a particular
site (or, in some cases, a group of sites).

For spiders, the scraping cycle goes through something like this:

1.  You start by generating the initial Requests to crawl the first
    URLs, and specify a callback function to be called with the response
    downloaded from those requests.

    The first requests to perform are obtained by calling the
    [[`start_requests()`{.xref .py .py-meth .docutils .literal
    .notranslate}]{.pre}](#scrapy.Spider.start_requests "scrapy.Spider.start_requests"){.reference
    .internal} method which (by default) generates [`Request`{.xref .py
    .py-class .docutils .literal .notranslate}]{.pre} for the URLs
    specified in the [[`start_urls`{.xref .py .py-attr .docutils
    .literal
    .notranslate}]{.pre}](#scrapy.Spider.start_urls "scrapy.Spider.start_urls"){.reference
    .internal} and the [[`parse`{.xref .py .py-attr .docutils .literal
    .notranslate}]{.pre}](#scrapy.Spider.parse "scrapy.Spider.parse"){.reference
    .internal} method as callback function for the Requests.

2.  In the callback function, you parse the response (web page) and
    return [[item objects]{.std
    .std-ref}](index.html#topics-items){.hoverxref .tooltip .reference
    .internal}, [`Request`{.xref .py .py-class .docutils .literal
    .notranslate}]{.pre} objects, or an iterable of these objects. Those
    Requests will also contain a callback (maybe the same) and will then
    be downloaded by Scrapy and then their response handled by the
    specified callback.

3.  In callback functions, you parse the page contents, typically using
    [[Selectors]{.std .std-ref}](index.html#topics-selectors){.hoverxref
    .tooltip .reference .internal} (but you can also use BeautifulSoup,
    lxml or whatever mechanism you prefer) and generate items with the
    parsed data.

4.  Finally, the items returned from the spider will be typically
    persisted to a database (in some [[Item Pipeline]{.std
    .std-ref}](index.html#topics-item-pipeline){.hoverxref .tooltip
    .reference .internal}) or written to a file using [[Feed
    exports]{.std .std-ref}](index.html#topics-feed-exports){.hoverxref
    .tooltip .reference .internal}.

Even though this cycle applies (more or less) to any kind of spider,
there are different kinds of default spiders bundled into Scrapy for
different purposes. We will talk about those types here.

::: {#scrapy-spider .section}
[]{#topics-spiders-ref}

#### scrapy.Spider[¶](#scrapy-spider "Permalink to this heading"){.headerlink}

*[class]{.pre}[ ]{.w}*[[scrapy.spiders.]{.pre}]{.sig-prename .descclassname}[[Spider]{.pre}]{.sig-name .descname}[¶](#scrapy.spiders.Spider "Permalink to this definition"){.headerlink}

:   

```{=html}
<!-- -->
```

*[class]{.pre}[ ]{.w}*[[scrapy.]{.pre}]{.sig-prename .descclassname}[[Spider]{.pre}]{.sig-name .descname}[¶](#scrapy.Spider "Permalink to this definition"){.headerlink}

:   This is the simplest spider, and the one from which every other
    spider must inherit (including spiders that come bundled with
    Scrapy, as well as spiders that you write yourself). It doesn't
    provide any special functionality. It just provides a default
    [[`start_requests()`{.xref .py .py-meth .docutils .literal
    .notranslate}]{.pre}](#scrapy.Spider.start_requests "scrapy.Spider.start_requests"){.reference
    .internal} implementation which sends requests from the
    [[`start_urls`{.xref .py .py-attr .docutils .literal
    .notranslate}]{.pre}](#scrapy.Spider.start_urls "scrapy.Spider.start_urls"){.reference
    .internal} spider attribute and calls the spider's method
    [`parse`{.docutils .literal .notranslate}]{.pre} for each of the
    resulting responses.

    [[name]{.pre}]{.sig-name .descname}[¶](#scrapy.Spider.name "Permalink to this definition"){.headerlink}

    :   A string which defines the name for this spider. The spider name
        is how the spider is located (and instantiated) by Scrapy, so it
        must be unique. However, nothing prevents you from instantiating
        more than one instance of the same spider. This is the most
        important spider attribute and it's required.

        If the spider scrapes a single domain, a common practice is to
        name the spider after the domain, with or without the
        [TLD](https://en.wikipedia.org/wiki/Top-level_domain){.reference
        .external}. So, for example, a spider that crawls
        [`mywebsite.com`{.docutils .literal .notranslate}]{.pre} would
        often be called [`mywebsite`{.docutils .literal
        .notranslate}]{.pre}.

    [[allowed_domains]{.pre}]{.sig-name .descname}[¶](#scrapy.Spider.allowed_domains "Permalink to this definition"){.headerlink}

    :   An optional list of strings containing domains that this spider
        is allowed to crawl. Requests for URLs not belonging to the
        domain names specified in this list (or their subdomains) won't
        be followed if [[`OffsiteMiddleware`{.xref .py .py-class
        .docutils .literal
        .notranslate}]{.pre}](index.html#scrapy.spidermiddlewares.offsite.OffsiteMiddleware "scrapy.spidermiddlewares.offsite.OffsiteMiddleware"){.reference
        .internal} is enabled.

        Let's say your target url is
        [`https://www.example.com/1.html`{.docutils .literal
        .notranslate}]{.pre}, then add [`'example.com'`{.docutils
        .literal .notranslate}]{.pre} to the list.

    [[start_urls]{.pre}]{.sig-name .descname}[¶](#scrapy.Spider.start_urls "Permalink to this definition"){.headerlink}

    :   A list of URLs where the spider will begin to crawl from, when
        no particular URLs are specified. So, the first pages downloaded
        will be those listed here. The subsequent [`Request`{.xref .py
        .py-class .docutils .literal .notranslate}]{.pre} will be
        generated successively from data contained in the start URLs.

    [[custom_settings]{.pre}]{.sig-name .descname}[¶](#scrapy.Spider.custom_settings "Permalink to this definition"){.headerlink}

    :   A dictionary of settings that will be overridden from the
        project wide configuration when running this spider. It must be
        defined as a class attribute since the settings are updated
        before instantiation.

        For a list of available built-in settings see: [[Built-in
        settings reference]{.std
        .std-ref}](index.html#topics-settings-ref){.hoverxref .tooltip
        .reference .internal}.

    [[crawler]{.pre}]{.sig-name .descname}[¶](#scrapy.Spider.crawler "Permalink to this definition"){.headerlink}

    :   This attribute is set by the [[`from_crawler()`{.xref .py
        .py-meth .docutils .literal
        .notranslate}]{.pre}](index.html#from_crawler "from_crawler"){.reference
        .internal} class method after initializing the class, and links
        to the [[`Crawler`{.xref .py .py-class .docutils .literal
        .notranslate}]{.pre}](index.html#scrapy.crawler.Crawler "scrapy.crawler.Crawler"){.reference
        .internal} object to which this spider instance is bound.

        Crawlers encapsulate a lot of components in the project for
        their single entry access (such as extensions, middlewares,
        signals managers, etc). See [[Crawler API]{.std
        .std-ref}](index.html#topics-api-crawler){.hoverxref .tooltip
        .reference .internal} to know more about them.

    [[settings]{.pre}]{.sig-name .descname}[¶](#scrapy.Spider.settings "Permalink to this definition"){.headerlink}

    :   Configuration for running this spider. This is a
        [[`Settings`{.xref .py .py-class .docutils .literal
        .notranslate}]{.pre}](index.html#scrapy.settings.Settings "scrapy.settings.Settings"){.reference
        .internal} instance, see the [[Settings]{.std
        .std-ref}](index.html#topics-settings){.hoverxref .tooltip
        .reference .internal} topic for a detailed introduction on this
        subject.

    [[logger]{.pre}]{.sig-name .descname}[¶](#scrapy.Spider.logger "Permalink to this definition"){.headerlink}

    :   Python logger created with the Spider's [[`name`{.xref .py
        .py-attr .docutils .literal
        .notranslate}]{.pre}](#scrapy.Spider.name "scrapy.Spider.name"){.reference
        .internal}. You can use it to send log messages through it as
        described on [[Logging from Spiders]{.std
        .std-ref}](index.html#topics-logging-from-spiders){.hoverxref
        .tooltip .reference .internal}.

    [[state]{.pre}]{.sig-name .descname}[¶](#scrapy.Spider.state "Permalink to this definition"){.headerlink}

    :   A dict you can use to persist some spider state between batches.
        See [[Keeping persistent state between batches]{.std
        .std-ref}](index.html#topics-keeping-persistent-state-between-batches){.hoverxref
        .tooltip .reference .internal} to know more about it.

    [[from_crawler]{.pre}]{.sig-name .descname}[(]{.sig-paren}*[[crawler]{.pre}]{.n}*, *[[\*]{.pre}]{.o}[[args]{.pre}]{.n}*, *[[\*\*]{.pre}]{.o}[[kwargs]{.pre}]{.n}*[)]{.sig-paren}[¶](#scrapy.Spider.from_crawler "Permalink to this definition"){.headerlink}

    :   This is the class method used by Scrapy to create your spiders.

        You probably won't need to override this directly because the
        default implementation acts as a proxy to the
        [[`__init__()`{.xref .py .py-meth .docutils .literal
        .notranslate}]{.pre}](index.html#init__ "__init__"){.reference
        .internal} method, calling it with the given arguments
        [`args`{.docutils .literal .notranslate}]{.pre} and named
        arguments [`kwargs`{.docutils .literal .notranslate}]{.pre}.

        Nonetheless, this method sets the [[`crawler`{.xref .py .py-attr
        .docutils .literal
        .notranslate}]{.pre}](#scrapy.Spider.crawler "scrapy.Spider.crawler"){.reference
        .internal} and [[`settings`{.xref .py .py-attr .docutils
        .literal
        .notranslate}]{.pre}](#scrapy.Spider.settings "scrapy.Spider.settings"){.reference
        .internal} attributes in the new instance so they can be
        accessed later inside the spider's code.

        ::: versionchanged
        [Changed in version 2.11: ]{.versionmodified .changed}The
        settings in [`crawler.settings`{.docutils .literal
        .notranslate}]{.pre} can now be modified in this method, which
        is handy if you want to modify them based on arguments. As a
        consequence, these settings aren't the final values as they can
        be modified later by e.g. [[add-ons]{.std
        .std-ref}](index.html#topics-addons){.hoverxref .tooltip
        .reference .internal}. For the same reason, most of the
        [[`Crawler`{.xref .py .py-class .docutils .literal
        .notranslate}]{.pre}](index.html#scrapy.crawler.Crawler "scrapy.crawler.Crawler"){.reference
        .internal} attributes aren't initialized at this point.

        The final settings and the initialized [[`Crawler`{.xref .py
        .py-class .docutils .literal
        .notranslate}]{.pre}](index.html#scrapy.crawler.Crawler "scrapy.crawler.Crawler"){.reference
        .internal} attributes are available in the
        [[`start_requests()`{.xref .py .py-meth .docutils .literal
        .notranslate}]{.pre}](#scrapy.Spider.start_requests "scrapy.Spider.start_requests"){.reference
        .internal} method, handlers of the [[`engine_started`{.xref .std
        .std-signal .docutils .literal
        .notranslate}]{.pre}](index.html#std-signal-engine_started){.hoverxref
        .tooltip .reference .internal} signal and later.
        :::

        Parameters

        :   -   **crawler** ([[`Crawler`{.xref .py .py-class .docutils
                .literal
                .notranslate}]{.pre}](index.html#scrapy.crawler.Crawler "scrapy.crawler.Crawler"){.reference
                .internal} instance) -- crawler to which the spider will
                be bound

            -   **args**
                ([*list*](https://docs.python.org/3/library/stdtypes.html#list "(in Python v3.12)"){.reference
                .external}) -- arguments passed to the
                [[`__init__()`{.xref .py .py-meth .docutils .literal
                .notranslate}]{.pre}](index.html#init__ "__init__"){.reference
                .internal} method

            -   **kwargs**
                ([*dict*](https://docs.python.org/3/library/stdtypes.html#dict "(in Python v3.12)"){.reference
                .external}) -- keyword arguments passed to the
                [[`__init__()`{.xref .py .py-meth .docutils .literal
                .notranslate}]{.pre}](index.html#init__ "__init__"){.reference
                .internal} method

    *[classmethod]{.pre}[ ]{.w}*[[update_settings]{.pre}]{.sig-name .descname}[(]{.sig-paren}*[[settings]{.pre}]{.n}*[)]{.sig-paren}[¶](#scrapy.Spider.update_settings "Permalink to this definition"){.headerlink}

    :   The [`update_settings()`{.docutils .literal .notranslate}]{.pre}
        method is used to modify the spider's settings and is called
        during initialization of a spider instance.

        It takes a [[`Settings`{.xref .py .py-class .docutils .literal
        .notranslate}]{.pre}](index.html#scrapy.settings.Settings "scrapy.settings.Settings"){.reference
        .internal} object as a parameter and can add or update the
        spider's configuration values. This method is a class method,
        meaning that it is called on the [[`Spider`{.xref .py .py-class
        .docutils .literal
        .notranslate}]{.pre}](#scrapy.Spider "scrapy.Spider"){.reference
        .internal} class and allows all instances of the spider to share
        the same configuration.

        While per-spider settings can be set in
        [[`custom_settings`{.xref .py .py-attr .docutils .literal
        .notranslate}]{.pre}](#scrapy.Spider.custom_settings "scrapy.Spider.custom_settings"){.reference
        .internal}, using [`update_settings()`{.docutils .literal
        .notranslate}]{.pre} allows you to dynamically add, remove or
        change settings based on other settings, spider attributes or
        other factors and use setting priorities other than
        [`'spider'`{.docutils .literal .notranslate}]{.pre}. Also, it's
        easy to extend [`update_settings()`{.docutils .literal
        .notranslate}]{.pre} in a subclass by overriding it, while doing
        the same with [[`custom_settings`{.xref .py .py-attr .docutils
        .literal
        .notranslate}]{.pre}](#scrapy.Spider.custom_settings "scrapy.Spider.custom_settings"){.reference
        .internal} can be hard.

        For example, suppose a spider needs to modify [[`FEEDS`{.xref
        .std .std-setting .docutils .literal
        .notranslate}]{.pre}](index.html#std-setting-FEEDS){.hoverxref
        .tooltip .reference .internal}:

        ::: {.highlight-python .notranslate}
        ::: highlight
            import scrapy


            class MySpider(scrapy.Spider):
                name = "myspider"
                custom_feed = {
                    "/home/user/documents/items.json": {
                        "format": "json",
                        "indent": 4,
                    }
                }

                @classmethod
                def update_settings(cls, settings):
                    super().update_settings(settings)
                    settings.setdefault("FEEDS", {}).update(cls.custom_feed)
        :::
        :::

    [[start_requests]{.pre}]{.sig-name .descname}[(]{.sig-paren}[)]{.sig-paren}[¶](#scrapy.Spider.start_requests "Permalink to this definition"){.headerlink}

    :   This method must return an iterable with the first Requests to
        crawl for this spider. It is called by Scrapy when the spider is
        opened for scraping. Scrapy calls it only once, so it is safe to
        implement [[`start_requests()`{.xref .py .py-meth .docutils
        .literal
        .notranslate}]{.pre}](#scrapy.Spider.start_requests "scrapy.Spider.start_requests"){.reference
        .internal} as a generator.

        The default implementation generates [`Request(url,`{.docutils
        .literal .notranslate}]{.pre}` `{.docutils .literal
        .notranslate}[`dont_filter=True)`{.docutils .literal
        .notranslate}]{.pre} for each url in [[`start_urls`{.xref .py
        .py-attr .docutils .literal
        .notranslate}]{.pre}](#scrapy.Spider.start_urls "scrapy.Spider.start_urls"){.reference
        .internal}.

        If you want to change the Requests used to start scraping a
        domain, this is the method to override. For example, if you need
        to start by logging in using a POST request, you could do:

        ::: {.highlight-python .notranslate}
        ::: highlight
            import scrapy


            class MySpider(scrapy.Spider):
                name = "myspider"

                def start_requests(self):
                    return [
                        scrapy.FormRequest(
                            "http://www.example.com/login",
                            formdata={"user": "john", "pass": "secret"},
                            callback=self.logged_in,
                        )
                    ]

                def logged_in(self, response):
                    # here you would extract links to follow and return Requests for
                    # each of them, with another callback
                    pass
        :::
        :::

    [[parse]{.pre}]{.sig-name .descname}[(]{.sig-paren}*[[response]{.pre}]{.n}*[)]{.sig-paren}[¶](#scrapy.Spider.parse "Permalink to this definition"){.headerlink}

    :   This is the default callback used by Scrapy to process
        downloaded responses, when their requests don't specify a
        callback.

        The [`parse`{.docutils .literal .notranslate}]{.pre} method is
        in charge of processing the response and returning scraped data
        and/or more URLs to follow. Other Requests callbacks have the
        same requirements as the [`Spider`{.xref .py .py-class .docutils
        .literal .notranslate}]{.pre} class.

        This method, as well as any other Request callback, must return
        a [`Request`{.xref .py .py-class .docutils .literal
        .notranslate}]{.pre} object, an [[item object]{.std
        .std-ref}](index.html#topics-items){.hoverxref .tooltip
        .reference .internal}, an iterable of [`Request`{.xref .py
        .py-class .docutils .literal .notranslate}]{.pre} objects and/or
        [[item objects]{.std
        .std-ref}](index.html#topics-items){.hoverxref .tooltip
        .reference .internal}, or [`None`{.docutils .literal
        .notranslate}]{.pre}.

        Parameters

        :   **response** ([[`Response`{.xref .py .py-class .docutils
            .literal
            .notranslate}]{.pre}](index.html#scrapy.http.Response "scrapy.http.Response"){.reference
            .internal}) -- the response to parse

    [[log]{.pre}]{.sig-name .descname}[(]{.sig-paren}*[[message]{.pre}]{.n}*[\[]{.optional}, *[[level]{.pre}]{.n}*, *[[component]{.pre}]{.n}*[\]]{.optional}[)]{.sig-paren}[¶](#scrapy.Spider.log "Permalink to this definition"){.headerlink}

    :   Wrapper that sends a log message through the Spider's
        [[`logger`{.xref .py .py-attr .docutils .literal
        .notranslate}]{.pre}](#scrapy.Spider.logger "scrapy.Spider.logger"){.reference
        .internal}, kept for backward compatibility. For more
        information see [[Logging from Spiders]{.std
        .std-ref}](index.html#topics-logging-from-spiders){.hoverxref
        .tooltip .reference .internal}.

    [[closed]{.pre}]{.sig-name .descname}[(]{.sig-paren}*[[reason]{.pre}]{.n}*[)]{.sig-paren}[¶](#scrapy.Spider.closed "Permalink to this definition"){.headerlink}

    :   Called when the spider closes. This method provides a shortcut
        to signals.connect() for the [[`spider_closed`{.xref .std
        .std-signal .docutils .literal
        .notranslate}]{.pre}](index.html#std-signal-spider_closed){.hoverxref
        .tooltip .reference .internal} signal.

Let's see an example:

::: {.highlight-python .notranslate}
::: highlight
    import scrapy


    class MySpider(scrapy.Spider):
        name = "example.com"
        allowed_domains = ["example.com"]
        start_urls = [
            "http://www.example.com/1.html",
            "http://www.example.com/2.html",
            "http://www.example.com/3.html",
        ]

        def parse(self, response):
            self.logger.info("A response from %s just arrived!", response.url)
:::
:::

Return multiple Requests and items from a single callback:

::: {.highlight-python .notranslate}
::: highlight
    import scrapy


    class MySpider(scrapy.Spider):
        name = "example.com"
        allowed_domains = ["example.com"]
        start_urls = [
            "http://www.example.com/1.html",
            "http://www.example.com/2.html",
            "http://www.example.com/3.html",
        ]

        def parse(self, response):
            for h3 in response.xpath("//h3").getall():
                yield {"title": h3}

            for href in response.xpath("//a/@href").getall():
                yield scrapy.Request(response.urljoin(href), self.parse)
:::
:::

Instead of [[`start_urls`{.xref .py .py-attr .docutils .literal
.notranslate}]{.pre}](#scrapy.Spider.start_urls "scrapy.Spider.start_urls"){.reference
.internal} you can use [[`start_requests()`{.xref .py .py-meth .docutils
.literal
.notranslate}]{.pre}](#scrapy.Spider.start_requests "scrapy.Spider.start_requests"){.reference
.internal} directly; to give data more structure you can use
[`Item`{.xref .py .py-class .docutils .literal .notranslate}]{.pre}
objects:

::: {.highlight-python .notranslate}
::: highlight
    import scrapy
    from myproject.items import MyItem


    class MySpider(scrapy.Spider):
        name = "example.com"
        allowed_domains = ["example.com"]

        def start_requests(self):
            yield scrapy.Request("http://www.example.com/1.html", self.parse)
            yield scrapy.Request("http://www.example.com/2.html", self.parse)
            yield scrapy.Request("http://www.example.com/3.html", self.parse)

        def parse(self, response):
            for h3 in response.xpath("//h3").getall():
                yield MyItem(title=h3)

            for href in response.xpath("//a/@href").getall():
                yield scrapy.Request(response.urljoin(href), self.parse)
:::
:::
:::

::: {#spider-arguments .section}
[]{#spiderargs}

#### Spider arguments[¶](#spider-arguments "Permalink to this heading"){.headerlink}

Spiders can receive arguments that modify their behaviour. Some common
uses for spider arguments are to define the start URLs or to restrict
the crawl to certain sections of the site, but they can be used to
configure any functionality of the spider.

Spider arguments are passed through the [[`crawl`{.xref .std
.std-command .docutils .literal
.notranslate}]{.pre}](index.html#std-command-crawl){.hoverxref .tooltip
.reference .internal} command using the [`-a`{.docutils .literal
.notranslate}]{.pre} option. For example:

::: {.highlight-default .notranslate}
::: highlight
    scrapy crawl myspider -a category=electronics
:::
:::

Spiders can access arguments in their \_\_init\_\_ methods:

::: {.highlight-python .notranslate}
::: highlight
    import scrapy


    class MySpider(scrapy.Spider):
        name = "myspider"

        def __init__(self, category=None, *args, **kwargs):
            super(MySpider, self).__init__(*args, **kwargs)
            self.start_urls = [f"http://www.example.com/categories/{category}"]
            # ...
:::
:::

The default \_\_init\_\_ method will take any spider arguments and copy
them to the spider as attributes. The above example can also be written
as follows:

::: {.highlight-python .notranslate}
::: highlight
    import scrapy


    class MySpider(scrapy.Spider):
        name = "myspider"

        def start_requests(self):
            yield scrapy.Request(f"http://www.example.com/categories/{self.category}")
:::
:::

If you are [[running Scrapy from a script]{.std
.std-ref}](index.html#run-from-script){.hoverxref .tooltip .reference
.internal}, you can specify spider arguments when calling
[[`CrawlerProcess.crawl`{.xref .py .py-class .docutils .literal
.notranslate}]{.pre}](index.html#scrapy.crawler.CrawlerProcess.crawl "scrapy.crawler.CrawlerProcess.crawl"){.reference
.internal} or [[`CrawlerRunner.crawl`{.xref .py .py-class .docutils
.literal
.notranslate}]{.pre}](index.html#scrapy.crawler.CrawlerRunner.crawl "scrapy.crawler.CrawlerRunner.crawl"){.reference
.internal}:

::: {.highlight-python .notranslate}
::: highlight
    process = CrawlerProcess()
    process.crawl(MySpider, category="electronics")
:::
:::

Keep in mind that spider arguments are only strings. The spider will not
do any parsing on its own. If you were to set the
[`start_urls`{.docutils .literal .notranslate}]{.pre} attribute from the
command line, you would have to parse it on your own into a list using
something like [[`ast.literal_eval()`{.xref .py .py-func .docutils
.literal
.notranslate}]{.pre}](https://docs.python.org/3/library/ast.html#ast.literal_eval "(in Python v3.12)"){.reference
.external} or [[`json.loads()`{.xref .py .py-func .docutils .literal
.notranslate}]{.pre}](https://docs.python.org/3/library/json.html#json.loads "(in Python v3.12)"){.reference
.external} and then set it as an attribute. Otherwise, you would cause
iteration over a [`start_urls`{.docutils .literal .notranslate}]{.pre}
string (a very common python pitfall) resulting in each character being
seen as a separate url.

A valid use case is to set the http auth credentials used by
[[`HttpAuthMiddleware`{.xref .py .py-class .docutils .literal
.notranslate}]{.pre}](index.html#scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware "scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware"){.reference
.internal} or the user agent used by [[`UserAgentMiddleware`{.xref .py
.py-class .docutils .literal
.notranslate}]{.pre}](index.html#scrapy.downloadermiddlewares.useragent.UserAgentMiddleware "scrapy.downloadermiddlewares.useragent.UserAgentMiddleware"){.reference
.internal}:

::: {.highlight-default .notranslate}
::: highlight
    scrapy crawl myspider -a http_user=myuser -a http_pass=mypassword -a user_agent=mybot
:::
:::

Spider arguments can also be passed through the Scrapyd
[`schedule.json`{.docutils .literal .notranslate}]{.pre} API. See
[Scrapyd
documentation](https://scrapyd.readthedocs.io/en/latest/){.reference
.external}.
:::

::: {#generic-spiders .section}
[]{#builtin-spiders}

#### Generic Spiders[¶](#generic-spiders "Permalink to this heading"){.headerlink}

Scrapy comes with some useful generic spiders that you can use to
subclass your spiders from. Their aim is to provide convenient
functionality for a few common scraping cases, like following all links
on a site based on certain rules, crawling from
[Sitemaps](https://www.sitemaps.org/index.html){.reference .external},
or parsing an XML/CSV feed.

For the examples used in the following spiders, we'll assume you have a
project with a [`TestItem`{.docutils .literal .notranslate}]{.pre}
declared in a [`myproject.items`{.docutils .literal .notranslate}]{.pre}
module:

::: {.highlight-python .notranslate}
::: highlight
    import scrapy


    class TestItem(scrapy.Item):
        id = scrapy.Field()
        name = scrapy.Field()
        description = scrapy.Field()
:::
:::

::: {#crawlspider .section}
##### CrawlSpider[¶](#crawlspider "Permalink to this heading"){.headerlink}

*[class]{.pre}[ ]{.w}*[[scrapy.spiders.]{.pre}]{.sig-prename .descclassname}[[CrawlSpider]{.pre}]{.sig-name .descname}[[[\[source\]]{.pre}]{.viewcode-link}](_modules/scrapy/spiders/crawl.html#CrawlSpider){.reference .internal}[¶](#scrapy.spiders.CrawlSpider "Permalink to this definition"){.headerlink}

:   This is the most commonly used spider for crawling regular websites,
    as it provides a convenient mechanism for following links by
    defining a set of rules. It may not be the best suited for your
    particular web sites or project, but it's generic enough for several
    cases, so you can start from it and override it as needed for more
    custom functionality, or just implement your own spider.

    Apart from the attributes inherited from Spider (that you must
    specify), this class supports a new attribute:

    [[rules]{.pre}]{.sig-name .descname}[¶](#scrapy.spiders.CrawlSpider.rules "Permalink to this definition"){.headerlink}

    :   Which is a list of one (or more) [[`Rule`{.xref .py .py-class
        .docutils .literal
        .notranslate}]{.pre}](#scrapy.spiders.Rule "scrapy.spiders.Rule"){.reference
        .internal} objects. Each [[`Rule`{.xref .py .py-class .docutils
        .literal
        .notranslate}]{.pre}](#scrapy.spiders.Rule "scrapy.spiders.Rule"){.reference
        .internal} defines a certain behaviour for crawling the site.
        Rules objects are described below. If multiple rules match the
        same link, the first one will be used, according to the order
        they're defined in this attribute.

    This spider also exposes an overridable method:

    [[parse_start_url]{.pre}]{.sig-name .descname}[(]{.sig-paren}*[[response]{.pre}]{.n}*, *[[\*\*]{.pre}]{.o}[[kwargs]{.pre}]{.n}*[)]{.sig-paren}[[[\[source\]]{.pre}]{.viewcode-link}](_modules/scrapy/spiders/crawl.html#CrawlSpider.parse_start_url){.reference .internal}[¶](#scrapy.spiders.CrawlSpider.parse_start_url "Permalink to this definition"){.headerlink}

    :   This method is called for each response produced for the URLs in
        the spider's [`start_urls`{.docutils .literal
        .notranslate}]{.pre} attribute. It allows to parse the initial
        responses and must return either an [[item object]{.std
        .std-ref}](index.html#topics-items){.hoverxref .tooltip
        .reference .internal}, a [`Request`{.xref .py .py-class
        .docutils .literal .notranslate}]{.pre} object, or an iterable
        containing any of them.

::: {#crawling-rules .section}
###### Crawling rules[¶](#crawling-rules "Permalink to this heading"){.headerlink}

*[class]{.pre}[ ]{.w}*[[scrapy.spiders.]{.pre}]{.sig-prename .descclassname}[[Rule]{.pre}]{.sig-name .descname}[(]{.sig-paren}*[[link_extractor]{.pre}]{.n}[[=]{.pre}]{.o}[[None]{.pre}]{.default_value}*, *[[callback]{.pre}]{.n}[[=]{.pre}]{.o}[[None]{.pre}]{.default_value}*, *[[cb_kwargs]{.pre}]{.n}[[=]{.pre}]{.o}[[None]{.pre}]{.default_value}*, *[[follow]{.pre}]{.n}[[=]{.pre}]{.o}[[None]{.pre}]{.default_value}*, *[[process_links]{.pre}]{.n}[[=]{.pre}]{.o}[[None]{.pre}]{.default_value}*, *[[process_request]{.pre}]{.n}[[=]{.pre}]{.o}[[None]{.pre}]{.default_value}*, *[[errback]{.pre}]{.n}[[=]{.pre}]{.o}[[None]{.pre}]{.default_value}*[)]{.sig-paren}[[[\[source\]]{.pre}]{.viewcode-link}](_modules/scrapy/spiders/crawl.html#Rule){.reference .internal}[¶](#scrapy.spiders.Rule "Permalink to this definition"){.headerlink}

:   [`link_extractor`{.docutils .literal .notranslate}]{.pre} is a
    [[Link Extractor]{.std
    .std-ref}](index.html#topics-link-extractors){.hoverxref .tooltip
    .reference .internal} object which defines how links will be
    extracted from each crawled page. Each produced link will be used to
    generate a [`Request`{.xref .py .py-class .docutils .literal
    .notranslate}]{.pre} object, which will contain the link's text in
    its [`meta`{.docutils .literal .notranslate}]{.pre} dictionary
    (under the [`link_text`{.docutils .literal .notranslate}]{.pre}
    key). If omitted, a default link extractor created with no arguments
    will be used, resulting in all links being extracted.

    [`callback`{.docutils .literal .notranslate}]{.pre} is a callable or
    a string (in which case a method from the spider object with that
    name will be used) to be called for each link extracted with the
    specified link extractor. This callback receives a
    [[`Response`{.xref .py .py-class .docutils .literal
    .notranslate}]{.pre}](index.html#scrapy.http.Response "scrapy.http.Response"){.reference
    .internal} as its first argument and must return either a single
    instance or an iterable of [[item objects]{.std
    .std-ref}](index.html#topics-items){.hoverxref .tooltip .reference
    .internal} and/or [`Request`{.xref .py .py-class .docutils .literal
    .notranslate}]{.pre} objects (or any subclass of them). As mentioned
    above, the received [[`Response`{.xref .py .py-class .docutils
    .literal
    .notranslate}]{.pre}](index.html#scrapy.http.Response "scrapy.http.Response"){.reference
    .internal} object will contain the text of the link that produced
    the [`Request`{.xref .py .py-class .docutils .literal
    .notranslate}]{.pre} in its [`meta`{.docutils .literal
    .notranslate}]{.pre} dictionary (under the [`link_text`{.docutils
    .literal .notranslate}]{.pre} key)

    [`cb_kwargs`{.docutils .literal .notranslate}]{.pre} is a dict
    containing the keyword arguments to be passed to the callback
    function.

    [`follow`{.docutils .literal .notranslate}]{.pre} is a boolean which
    specifies if links should be followed from each response extracted
    with this rule. If [`callback`{.docutils .literal
    .notranslate}]{.pre} is None [`follow`{.docutils .literal
    .notranslate}]{.pre} defaults to [`True`{.docutils .literal
    .notranslate}]{.pre}, otherwise it defaults to [`False`{.docutils
    .literal .notranslate}]{.pre}.

    [`process_links`{.docutils .literal .notranslate}]{.pre} is a
    callable, or a string (in which case a method from the spider object
    with that name will be used) which will be called for each list of
    links extracted from each response using the specified
    [`link_extractor`{.docutils .literal .notranslate}]{.pre}. This is
    mainly used for filtering purposes.

    [`process_request`{.docutils .literal .notranslate}]{.pre} is a
    callable (or a string, in which case a method from the spider object
    with that name will be used) which will be called for every
    [`Request`{.xref .py .py-class .docutils .literal
    .notranslate}]{.pre} extracted by this rule. This callable should
    take said request as first argument and the [[`Response`{.xref .py
    .py-class .docutils .literal
    .notranslate}]{.pre}](index.html#scrapy.http.Response "scrapy.http.Response"){.reference
    .internal} from which the request originated as second argument. It
    must return a [`Request`{.docutils .literal .notranslate}]{.pre}
    object or [`None`{.docutils .literal .notranslate}]{.pre} (to filter
    out the request).

    [`errback`{.docutils .literal .notranslate}]{.pre} is a callable or
    a string (in which case a method from the spider object with that
    name will be used) to be called if any exception is raised while
    processing a request generated by the rule. It receives a
    [[`Twisted`{.xref .py .py-class .docutils .literal
    .notranslate}]{.pre}` `{.xref .py .py-class .docutils .literal
    .notranslate}[`Failure`{.xref .py .py-class .docutils .literal
    .notranslate}]{.pre}](https://docs.twisted.org/en/stable/api/twisted.python.failure.Failure.html "(in Twisted)"){.reference
    .external} instance as first parameter.

    ::: {.admonition .warning}
    Warning

    Because of its internal implementation, you must explicitly set
    callbacks for new requests when writing [[`CrawlSpider`{.xref .py
    .py-class .docutils .literal
    .notranslate}]{.pre}](#scrapy.spiders.CrawlSpider "scrapy.spiders.CrawlSpider"){.reference
    .internal}-based spiders; unexpected behaviour can occur otherwise.
    :::

    ::: versionadded
    [New in version 2.0: ]{.versionmodified .added}The *errback*
    parameter.
    :::
:::

::: {#crawlspider-example .section}
###### CrawlSpider example[¶](#crawlspider-example "Permalink to this heading"){.headerlink}

Let's now take a look at an example CrawlSpider with rules:

::: {.highlight-python .notranslate}
::: highlight
    import scrapy
    from scrapy.spiders import CrawlSpider, Rule
    from scrapy.linkextractors import LinkExtractor


    class MySpider(CrawlSpider):
        name = "example.com"
        allowed_domains = ["example.com"]
        start_urls = ["http://www.example.com"]

        rules = (
            # Extract links matching 'category.php' (but not matching 'subsection.php')
            # and follow links from them (since no callback means follow=True by default).
            Rule(LinkExtractor(allow=(r"category\.php",), deny=(r"subsection\.php",))),
            # Extract links matching 'item.php' and parse them with the spider's method parse_item
            Rule(LinkExtractor(allow=(r"item\.php",)), callback="parse_item"),
        )

        def parse_item(self, response):
            self.logger.info("Hi, this is an item page! %s", response.url)
            item = scrapy.Item()
            item["id"] = response.xpath('//td[@id="item_id"]/text()').re(r"ID: (\d+)")
            item["name"] = response.xpath('//td[@id="item_name"]/text()').get()
            item["description"] = response.xpath(
                '//td[@id="item_description"]/text()'
            ).get()
            item["link_text"] = response.meta["link_text"]
            url = response.xpath('//td[@id="additional_data"]/@href').get()
            return response.follow(
                url, self.parse_additional_page, cb_kwargs=dict(item=item)
            )

        def parse_additional_page(self, response, item):
            item["additional_data"] = response.xpath(
                '//p[@id="additional_data"]/text()'
            ).get()
            return item
:::
:::

This spider would start crawling example.com's home page, collecting
category links, and item links, parsing the latter with the
[`parse_item`{.docutils .literal .notranslate}]{.pre} method. For each
item response, some data will be extracted from the HTML using XPath,
and an [`Item`{.xref .py .py-class .docutils .literal
.notranslate}]{.pre} will be filled with it.
:::
:::

::: {#xmlfeedspider .section}
##### XMLFeedSpider[¶](#xmlfeedspider "Permalink to this heading"){.headerlink}

*[class]{.pre}[ ]{.w}*[[scrapy.spiders.]{.pre}]{.sig-prename .descclassname}[[XMLFeedSpider]{.pre}]{.sig-name .descname}[[[\[source\]]{.pre}]{.viewcode-link}](_modules/scrapy/spiders/feed.html#XMLFeedSpider){.reference .internal}[¶](#scrapy.spiders.XMLFeedSpider "Permalink to this definition"){.headerlink}

:   XMLFeedSpider is designed for parsing XML feeds by iterating through
    them by a certain node name. The iterator can be chosen from:
    [`iternodes`{.docutils .literal .notranslate}]{.pre},
    [`xml`{.docutils .literal .notranslate}]{.pre}, and
    [`html`{.docutils .literal .notranslate}]{.pre}. It's recommended to
    use the [`iternodes`{.docutils .literal .notranslate}]{.pre}
    iterator for performance reasons, since the [`xml`{.docutils
    .literal .notranslate}]{.pre} and [`html`{.docutils .literal
    .notranslate}]{.pre} iterators generate the whole DOM at once in
    order to parse it. However, using [`html`{.docutils .literal
    .notranslate}]{.pre} as the iterator may be useful when parsing XML
    with bad markup.

    To set the iterator and the tag name, you must define the following
    class attributes:

    [[iterator]{.pre}]{.sig-name .descname}[¶](#scrapy.spiders.XMLFeedSpider.iterator "Permalink to this definition"){.headerlink}

    :   A string which defines the iterator to use. It can be either:

        > <div>
        >
        > -   [`'iternodes'`{.docutils .literal .notranslate}]{.pre} - a
        >     fast iterator based on regular expressions
        >
        > -   [`'html'`{.docutils .literal .notranslate}]{.pre} - an
        >     iterator which uses [`Selector`{.xref .py .py-class
        >     .docutils .literal .notranslate}]{.pre}. Keep in mind this
        >     uses DOM parsing and must load all DOM in memory which
        >     could be a problem for big feeds
        >
        > -   [`'xml'`{.docutils .literal .notranslate}]{.pre} - an
        >     iterator which uses [`Selector`{.xref .py .py-class
        >     .docutils .literal .notranslate}]{.pre}. Keep in mind this
        >     uses DOM parsing and must load all DOM in memory which
        >     could be a problem for big feeds
        >
        > </div>

        It defaults to: [`'iternodes'`{.docutils .literal
        .notranslate}]{.pre}.

    [[itertag]{.pre}]{.sig-name .descname}[¶](#scrapy.spiders.XMLFeedSpider.itertag "Permalink to this definition"){.headerlink}

    :   A string with the name of the node (or element) to iterate in.
        Example:

        ::: {.highlight-default .notranslate}
        ::: highlight
            itertag = 'product'
        :::
        :::

    [[namespaces]{.pre}]{.sig-name .descname}[¶](#scrapy.spiders.XMLFeedSpider.namespaces "Permalink to this definition"){.headerlink}

    :   A list of [`(prefix,`{.docutils .literal
        .notranslate}]{.pre}` `{.docutils .literal
        .notranslate}[`uri)`{.docutils .literal .notranslate}]{.pre}
        tuples which define the namespaces available in that document
        that will be processed with this spider. The [`prefix`{.docutils
        .literal .notranslate}]{.pre} and [`uri`{.docutils .literal
        .notranslate}]{.pre} will be used to automatically register
        namespaces using the [`register_namespace()`{.xref .py .py-meth
        .docutils .literal .notranslate}]{.pre} method.

        You can then specify nodes with namespaces in the
        [[`itertag`{.xref .py .py-attr .docutils .literal
        .notranslate}]{.pre}](#scrapy.spiders.XMLFeedSpider.itertag "scrapy.spiders.XMLFeedSpider.itertag"){.reference
        .internal} attribute.

        Example:

        ::: {.highlight-default .notranslate}
        ::: highlight
            class YourSpider(XMLFeedSpider):

                namespaces = [('n', 'http://www.sitemaps.org/schemas/sitemap/0.9')]
                itertag = 'n:url'
                # ...
        :::
        :::

    Apart from these new attributes, this spider has the following
    overridable methods too:

    [[adapt_response]{.pre}]{.sig-name .descname}[(]{.sig-paren}*[[response]{.pre}]{.n}*[)]{.sig-paren}[[[\[source\]]{.pre}]{.viewcode-link}](_modules/scrapy/spiders/feed.html#XMLFeedSpider.adapt_response){.reference .internal}[¶](#scrapy.spiders.XMLFeedSpider.adapt_response "Permalink to this definition"){.headerlink}

    :   A method that receives the response as soon as it arrives from
        the spider middleware, before the spider starts parsing it. It
        can be used to modify the response body before parsing it. This
        method receives a response and also returns a response (it could
        be the same or another one).

    [[parse_node]{.pre}]{.sig-name .descname}[(]{.sig-paren}*[[response]{.pre}]{.n}*, *[[selector]{.pre}]{.n}*[)]{.sig-paren}[[[\[source\]]{.pre}]{.viewcode-link}](_modules/scrapy/spiders/feed.html#XMLFeedSpider.parse_node){.reference .internal}[¶](#scrapy.spiders.XMLFeedSpider.parse_node "Permalink to this definition"){.headerlink}

    :   This method is called for the nodes matching the provided tag
        name ([`itertag`{.docutils .literal .notranslate}]{.pre}).
        Receives the response and an [`Selector`{.xref .py .py-class
        .docutils .literal .notranslate}]{.pre} for each node.
        Overriding this method is mandatory. Otherwise, you spider won't
        work. This method must return an [[item object]{.std
        .std-ref}](index.html#topics-items){.hoverxref .tooltip
        .reference .internal}, a [`Request`{.xref .py .py-class
        .docutils .literal .notranslate}]{.pre} object, or an iterable
        containing any of them.

    [[process_results]{.pre}]{.sig-name .descname}[(]{.sig-paren}*[[response]{.pre}]{.n}*, *[[results]{.pre}]{.n}*[)]{.sig-paren}[[[\[source\]]{.pre}]{.viewcode-link}](_modules/scrapy/spiders/feed.html#XMLFeedSpider.process_results){.reference .internal}[¶](#scrapy.spiders.XMLFeedSpider.process_results "Permalink to this definition"){.headerlink}

    :   This method is called for each result (item or request) returned
        by the spider, and it's intended to perform any last time
        processing required before returning the results to the
        framework core, for example setting the item IDs. It receives a
        list of results and the response which originated those results.
        It must return a list of results (items or requests).

    ::: {.admonition .warning}
    Warning

    Because of its internal implementation, you must explicitly set
    callbacks for new requests when writing [[`XMLFeedSpider`{.xref .py
    .py-class .docutils .literal
    .notranslate}]{.pre}](#scrapy.spiders.XMLFeedSpider "scrapy.spiders.XMLFeedSpider"){.reference
    .internal}-based spiders; unexpected behaviour can occur otherwise.
    :::

::: {#xmlfeedspider-example .section}
###### XMLFeedSpider example[¶](#xmlfeedspider-example "Permalink to this heading"){.headerlink}

These spiders are pretty easy to use, let's have a look at one example:

::: {.highlight-python .notranslate}
::: highlight
    from scrapy.spiders import XMLFeedSpider
    from myproject.items import TestItem


    class MySpider(XMLFeedSpider):
        name = "example.com"
        allowed_domains = ["example.com"]
        start_urls = ["http://www.example.com/feed.xml"]
        iterator = "iternodes"  # This is actually unnecessary, since it's the default value
        itertag = "item"

        def parse_node(self, response, node):
            self.logger.info(
                "Hi, this is a <%s> node!: %s", self.itertag, "".join(node.getall())
            )

            item = TestItem()
            item["id"] = node.xpath("@id").get()
            item["name"] = node.xpath("name").get()
            item["description"] = node.xpath("description").get()
            return item
:::
:::

Basically what we did up there was to create a spider that downloads a
feed from the given [`start_urls`{.docutils .literal
.notranslate}]{.pre}, and then iterates through each of its
[`item`{.docutils .literal .notranslate}]{.pre} tags, prints them out,
and stores some random data in an [`Item`{.xref .py .py-class .docutils
.literal .notranslate}]{.pre}.
:::
:::

::: {#csvfeedspider .section}
##### CSVFeedSpider[¶](#csvfeedspider "Permalink to this heading"){.headerlink}

*[class]{.pre}[ ]{.w}*[[scrapy.spiders.]{.pre}]{.sig-prename .descclassname}[[CSVFeedSpider]{.pre}]{.sig-name .descname}[[[\[source\]]{.pre}]{.viewcode-link}](_modules/scrapy/spiders/feed.html#CSVFeedSpider){.reference .internal}[¶](#scrapy.spiders.CSVFeedSpider "Permalink to this definition"){.headerlink}

:   This spider is very similar to the XMLFeedSpider, except that it
    iterates over rows, instead of nodes. The method that gets called in
    each iteration is [[`parse_row()`{.xref .py .py-meth .docutils
    .literal
    .notranslate}]{.pre}](#scrapy.spiders.CSVFeedSpider.parse_row "scrapy.spiders.CSVFeedSpider.parse_row"){.reference
    .internal}.

    [[delimiter]{.pre}]{.sig-name .descname}[¶](#scrapy.spiders.CSVFeedSpider.delimiter "Permalink to this definition"){.headerlink}

    :   A string with the separator character for each field in the CSV
        file Defaults to [`','`{.docutils .literal .notranslate}]{.pre}
        (comma).

    [[quotechar]{.pre}]{.sig-name .descname}[¶](#scrapy.spiders.CSVFeedSpider.quotechar "Permalink to this definition"){.headerlink}

    :   A string with the enclosure character for each field in the CSV
        file Defaults to [`'"'`{.docutils .literal .notranslate}]{.pre}
        (quotation mark).

    [[headers]{.pre}]{.sig-name .descname}[¶](#scrapy.spiders.CSVFeedSpider.headers "Permalink to this definition"){.headerlink}

    :   A list of the column names in the CSV file.

    [[parse_row]{.pre}]{.sig-name .descname}[(]{.sig-paren}*[[response]{.pre}]{.n}*, *[[row]{.pre}]{.n}*[)]{.sig-paren}[[[\[source\]]{.pre}]{.viewcode-link}](_modules/scrapy/spiders/feed.html#CSVFeedSpider.parse_row){.reference .internal}[¶](#scrapy.spiders.CSVFeedSpider.parse_row "Permalink to this definition"){.headerlink}

    :   Receives a response and a dict (representing each row) with a
        key for each provided (or detected) header of the CSV file. This
        spider also gives the opportunity to override
        [`adapt_response`{.docutils .literal .notranslate}]{.pre} and
        [`process_results`{.docutils .literal .notranslate}]{.pre}
        methods for pre- and post-processing purposes.

::: {#csvfeedspider-example .section}
###### CSVFeedSpider example[¶](#csvfeedspider-example "Permalink to this heading"){.headerlink}

Let's see an example similar to the previous one, but using a
[[`CSVFeedSpider`{.xref .py .py-class .docutils .literal
.notranslate}]{.pre}](#scrapy.spiders.CSVFeedSpider "scrapy.spiders.CSVFeedSpider"){.reference
.internal}:

::: {.highlight-python .notranslate}
::: highlight
    from scrapy.spiders import CSVFeedSpider
    from myproject.items import TestItem


    class MySpider(CSVFeedSpider):
        name = "example.com"
        allowed_domains = ["example.com"]
        start_urls = ["http://www.example.com/feed.csv"]
        delimiter = ";"
        quotechar = "'"
        headers = ["id", "name", "description"]

        def parse_row(self, response, row):
            self.logger.info("Hi, this is a row!: %r", row)

            item = TestItem()
            item["id"] = row["id"]
            item["name"] = row["name"]
            item["description"] = row["description"]
            return item
:::
:::
:::
:::

::: {#sitemapspider .section}
##### SitemapSpider[¶](#sitemapspider "Permalink to this heading"){.headerlink}

*[class]{.pre}[ ]{.w}*[[scrapy.spiders.]{.pre}]{.sig-prename .descclassname}[[SitemapSpider]{.pre}]{.sig-name .descname}[[[\[source\]]{.pre}]{.viewcode-link}](_modules/scrapy/spiders/sitemap.html#SitemapSpider){.reference .internal}[¶](#scrapy.spiders.SitemapSpider "Permalink to this definition"){.headerlink}

:   SitemapSpider allows you to crawl a site by discovering the URLs
    using [Sitemaps](https://www.sitemaps.org/index.html){.reference
    .external}.

    It supports nested sitemaps and discovering sitemap urls from
    [robots.txt](https://www.robotstxt.org/){.reference .external}.

    [[sitemap_urls]{.pre}]{.sig-name .descname}[¶](#scrapy.spiders.SitemapSpider.sitemap_urls "Permalink to this definition"){.headerlink}

    :   A list of urls pointing to the sitemaps whose urls you want to
        crawl.

        You can also point to a
        [robots.txt](https://www.robotstxt.org/){.reference .external}
        and it will be parsed to extract sitemap urls from it.

    [[sitemap_rules]{.pre}]{.sig-name .descname}[¶](#scrapy.spiders.SitemapSpider.sitemap_rules "Permalink to this definition"){.headerlink}

    :   A list of tuples [`(regex,`{.docutils .literal
        .notranslate}]{.pre}` `{.docutils .literal
        .notranslate}[`callback)`{.docutils .literal
        .notranslate}]{.pre} where:

        -   [`regex`{.docutils .literal .notranslate}]{.pre} is a
            regular expression to match urls extracted from sitemaps.
            [`regex`{.docutils .literal .notranslate}]{.pre} can be
            either a str or a compiled regex object.

        -   callback is the callback to use for processing the urls that
            match the regular expression. [`callback`{.docutils .literal
            .notranslate}]{.pre} can be a string (indicating the name of
            a spider method) or a callable.

        For example:

        ::: {.highlight-default .notranslate}
        ::: highlight
            sitemap_rules = [('/product/', 'parse_product')]
        :::
        :::

        Rules are applied in order, and only the first one that matches
        will be used.

        If you omit this attribute, all urls found in sitemaps will be
        processed with the [`parse`{.docutils .literal
        .notranslate}]{.pre} callback.

    [[sitemap_follow]{.pre}]{.sig-name .descname}[¶](#scrapy.spiders.SitemapSpider.sitemap_follow "Permalink to this definition"){.headerlink}

    :   A list of regexes of sitemap that should be followed. This is
        only for sites that use [Sitemap index
        files](https://www.sitemaps.org/protocol.html#index){.reference
        .external} that point to other sitemap files.

        By default, all sitemaps are followed.

    [[sitemap_alternate_links]{.pre}]{.sig-name .descname}[¶](#scrapy.spiders.SitemapSpider.sitemap_alternate_links "Permalink to this definition"){.headerlink}

    :   Specifies if alternate links for one [`url`{.docutils .literal
        .notranslate}]{.pre} should be followed. These are links for the
        same website in another language passed within the same
        [`url`{.docutils .literal .notranslate}]{.pre} block.

        For example:

        ::: {.highlight-default .notranslate}
        ::: highlight
            <url>
                <loc>http://example.com/</loc>
                <xhtml:link rel="alternate" hreflang="de" href="http://example.com/de"/>
            </url>
        :::
        :::

        With [`sitemap_alternate_links`{.docutils .literal
        .notranslate}]{.pre} set, this would retrieve both URLs. With
        [`sitemap_alternate_links`{.docutils .literal
        .notranslate}]{.pre} disabled, only
        [`http://example.com/`{.docutils .literal .notranslate}]{.pre}
        would be retrieved.

        Default is [`sitemap_alternate_links`{.docutils .literal
        .notranslate}]{.pre} disabled.

    [[sitemap_filter]{.pre}]{.sig-name .descname}[(]{.sig-paren}*[[entries]{.pre}]{.n}*[)]{.sig-paren}[[[\[source\]]{.pre}]{.viewcode-link}](_modules/scrapy/spiders/sitemap.html#SitemapSpider.sitemap_filter){.reference .internal}[¶](#scrapy.spiders.SitemapSpider.sitemap_filter "Permalink to this definition"){.headerlink}

    :   This is a filter function that could be overridden to select
        sitemap entries based on their attributes.

        For example:

        ::: {.highlight-default .notranslate}
        ::: highlight
            <url>
                <loc>http://example.com/</loc>
                <lastmod>2005-01-01</lastmod>
            </url>
        :::
        :::

        We can define a [`sitemap_filter`{.docutils .literal
        .notranslate}]{.pre} function to filter [`entries`{.docutils
        .literal .notranslate}]{.pre} by date:

        ::: {.highlight-python .notranslate}
        ::: highlight
            from datetime import datetime
            from scrapy.spiders import SitemapSpider


            class FilteredSitemapSpider(SitemapSpider):
                name = "filtered_sitemap_spider"
                allowed_domains = ["example.com"]
                sitemap_urls = ["http://example.com/sitemap.xml"]

                def sitemap_filter(self, entries):
                    for entry in entries:
                        date_time = datetime.strptime(entry["lastmod"], "%Y-%m-%d")
                        if date_time.year >= 2005:
                            yield entry
        :::
        :::

        This would retrieve only [`entries`{.docutils .literal
        .notranslate}]{.pre} modified on 2005 and the following years.

        Entries are dict objects extracted from the sitemap document.
        Usually, the key is the tag name and the value is the text
        inside it.

        It's important to notice that:

        -   as the loc attribute is required, entries without this tag
            are discarded

        -   alternate links are stored in a list with the key
            [`alternate`{.docutils .literal .notranslate}]{.pre} (see
            [`sitemap_alternate_links`{.docutils .literal
            .notranslate}]{.pre})

        -   namespaces are removed, so lxml tags named as
            [`{namespace}tagname`{.docutils .literal
            .notranslate}]{.pre} become only [`tagname`{.docutils
            .literal .notranslate}]{.pre}

        If you omit this method, all entries found in sitemaps will be
        processed, observing other attributes and their settings.

::: {#sitemapspider-examples .section}
###### SitemapSpider examples[¶](#sitemapspider-examples "Permalink to this heading"){.headerlink}

Simplest example: process all urls discovered through sitemaps using the
[`parse`{.docutils .literal .notranslate}]{.pre} callback:

::: {.highlight-python .notranslate}
::: highlight
    from scrapy.spiders import SitemapSpider


    class MySpider(SitemapSpider):
        sitemap_urls = ["http://www.example.com/sitemap.xml"]

        def parse(self, response):
            pass  # ... scrape item here ...
:::
:::

Process some urls with certain callback and other urls with a different
callback:

::: {.highlight-python .notranslate}
::: highlight
    from scrapy.spiders import SitemapSpider


    class MySpider(SitemapSpider):
        sitemap_urls = ["http://www.example.com/sitemap.xml"]
        sitemap_rules = [
            ("/product/", "parse_product"),
            ("/category/", "parse_category"),
        ]

        def parse_product(self, response):
            pass  # ... scrape product ...

        def parse_category(self, response):
            pass  # ... scrape category ...
:::
:::

Follow sitemaps defined in the
[robots.txt](https://www.robotstxt.org/){.reference .external} file and
only follow sitemaps whose url contains [`/sitemap_shop`{.docutils
.literal .notranslate}]{.pre}:

::: {.highlight-python .notranslate}
::: highlight
    from scrapy.spiders import SitemapSpider


    class MySpider(SitemapSpider):
        sitemap_urls = ["http://www.example.com/robots.txt"]
        sitemap_rules = [
            ("/shop/", "parse_shop"),
        ]
        sitemap_follow = ["/sitemap_shops"]

        def parse_shop(self, response):
            pass  # ... scrape shop here ...
:::
:::

Combine SitemapSpider with other sources of urls:

::: {.highlight-python .notranslate}
::: highlight
    from scrapy.spiders import SitemapSpider


    class MySpider(SitemapSpider):
        sitemap_urls = ["http://www.example.com/robots.txt"]
        sitemap_rules = [
            ("/shop/", "parse_shop"),
        ]

        other_urls = ["http://www.example.com/about"]

        def start_requests(self):
            requests = list(super(MySpider, self).start_requests())
            requests += [scrapy.Request(x, self.parse_other) for x in self.other_urls]
            return requests

        def parse_shop(self, response):
            pass  # ... scrape shop here ...

        def parse_other(self, response):
            pass  # ... scrape other here ...
:::
:::
:::
:::
:::
:::

[]{#document-topics/selectors}

::: {#selectors .section}
[]{#topics-selectors}

### Selectors[¶](#selectors "Permalink to this heading"){.headerlink}

When you're scraping web pages, the most common task you need to perform
is to extract data from the HTML source. There are several libraries
available to achieve this, such as:

-   [BeautifulSoup](https://www.crummy.com/software/BeautifulSoup/){.reference
    .external} is a very popular web scraping library among Python
    programmers which constructs a Python object based on the structure
    of the HTML code and also deals with bad markup reasonably well, but
    it has one drawback: it's slow.

-   [lxml](https://lxml.de/){.reference .external} is an XML parsing
    library (which also parses HTML) with a pythonic API based on
    [[`ElementTree`{.xref .py .py-mod .docutils .literal
    .notranslate}]{.pre}](https://docs.python.org/3/library/xml.etree.elementtree.html#module-xml.etree.ElementTree "(in Python v3.12)"){.reference
    .external}. (lxml is not part of the Python standard library.)

Scrapy comes with its own mechanism for extracting data. They're called
selectors because they "select" certain parts of the HTML document
specified either by [XPath](https://www.w3.org/TR/xpath/all/){.reference
.external} or [CSS](https://www.w3.org/TR/selectors){.reference
.external} expressions.

[XPath](https://www.w3.org/TR/xpath/all/){.reference .external} is a
language for selecting nodes in XML documents, which can also be used
with HTML. [CSS](https://www.w3.org/TR/selectors){.reference .external}
is a language for applying styles to HTML documents. It defines
selectors to associate those styles with specific HTML elements.

::: {.admonition .note}
Note

Scrapy Selectors is a thin wrapper around
[parsel](https://parsel.readthedocs.io/en/latest/){.reference .external}
library; the purpose of this wrapper is to provide better integration
with Scrapy Response objects.

[parsel](https://parsel.readthedocs.io/en/latest/){.reference .external}
is a stand-alone web scraping library which can be used without Scrapy.
It uses [lxml](https://lxml.de/){.reference .external} library under the
hood, and implements an easy API on top of lxml API. It means Scrapy
selectors are very similar in speed and parsing accuracy to lxml.
:::

::: {#using-selectors .section}
#### Using selectors[¶](#using-selectors "Permalink to this heading"){.headerlink}

::: {#constructing-selectors .section}
##### Constructing selectors[¶](#constructing-selectors "Permalink to this heading"){.headerlink}

Response objects expose a [`Selector`{.xref .py .py-class .docutils
.literal .notranslate}]{.pre} instance on [`.selector`{.docutils
.literal .notranslate}]{.pre} attribute:

::: {.highlight-pycon .notranslate}
::: highlight
    >>> response.selector.xpath("//span/text()").get()
    'good'
:::
:::

Querying responses using XPath and CSS is so common that responses
include two more shortcuts: [`response.xpath()`{.docutils .literal
.notranslate}]{.pre} and [`response.css()`{.docutils .literal
.notranslate}]{.pre}:

::: {.highlight-pycon .notranslate}
::: highlight
    >>> response.xpath("//span/text()").get()
    'good'
    >>> response.css("span::text").get()
    'good'
:::
:::

Scrapy selectors are instances of [`Selector`{.xref .py .py-class
.docutils .literal .notranslate}]{.pre} class constructed by passing
either [[`TextResponse`{.xref .py .py-class .docutils .literal
.notranslate}]{.pre}](index.html#scrapy.http.TextResponse "scrapy.http.TextResponse"){.reference
.internal} object or markup as a string (in [`text`{.docutils .literal
.notranslate}]{.pre} argument).

Usually there is no need to construct Scrapy selectors manually:
[`response`{.docutils .literal .notranslate}]{.pre} object is available
in Spider callbacks, so in most cases it is more convenient to use
[`response.css()`{.docutils .literal .notranslate}]{.pre} and
[`response.xpath()`{.docutils .literal .notranslate}]{.pre} shortcuts.
By using [`response.selector`{.docutils .literal .notranslate}]{.pre} or
one of these shortcuts you can also ensure the response body is parsed
only once.

But if required, it is possible to use [`Selector`{.docutils .literal
.notranslate}]{.pre} directly. Constructing from text:

::: {.highlight-pycon .notranslate}
::: highlight
    >>> from scrapy.selector import Selector
    >>> body = "<html><body><span>good</span></body></html>"
    >>> Selector(text=body).xpath("//span/text()").get()
    'good'
:::
:::

Constructing from response - [[`HtmlResponse`{.xref .py .py-class
.docutils .literal
.notranslate}]{.pre}](index.html#scrapy.http.HtmlResponse "scrapy.http.HtmlResponse"){.reference
.internal} is one of [[`TextResponse`{.xref .py .py-class .docutils
.literal
.notranslate}]{.pre}](index.html#scrapy.http.TextResponse "scrapy.http.TextResponse"){.reference
.internal} subclasses:

::: {.highlight-pycon .notranslate}
::: highlight
    >>> from scrapy.selector import Selector
    >>> from scrapy.http import HtmlResponse
    >>> response = HtmlResponse(url="http://example.com", body=body, encoding="utf-8")
    >>> Selector(response=response).xpath("//span/text()").get()
    'good'
:::
:::

[`Selector`{.docutils .literal .notranslate}]{.pre} automatically
chooses the best parsing rules (XML vs HTML) based on input type.
:::

::: {#id1 .section}
##### Using selectors[¶](#id1 "Permalink to this heading"){.headerlink}

To explain how to use the selectors we'll use the [`Scrapy`{.docutils
.literal .notranslate}]{.pre}` `{.docutils .literal
.notranslate}[`shell`{.docutils .literal .notranslate}]{.pre} (which
provides interactive testing) and an example page located in the Scrapy
documentation server:

> <div>
>
> [https://docs.scrapy.org/en/latest/\_static/selectors-sample1.html](https://docs.scrapy.org/en/latest/_static/selectors-sample1.html){.reference
> .external}
>
> </div>

For the sake of completeness, here's its full HTML code:

::: {.highlight-html .notranslate}
::: highlight
    <!DOCTYPE html>

    <html>
      <head>
        <base href='http://example.com/' />
        <title>Example website</title>
      </head>
      <body>
        <div id='images'>
          <a href='image1.html'>Name: My image 1 <br /><img src='image1_thumb.jpg' alt='image1'/></a>
          <a href='image2.html'>Name: My image 2 <br /><img src='image2_thumb.jpg' alt='image2'/></a>
          <a href='image3.html'>Name: My image 3 <br /><img src='image3_thumb.jpg' alt='image3'/></a>
          <a href='image4.html'>Name: My image 4 <br /><img src='image4_thumb.jpg' alt='image4'/></a>
          <a href='image5.html'>Name: My image 5 <br /><img src='image5_thumb.jpg' alt='image5'/></a>
        </div>
      </body>
    </html>
:::
:::

First, let's open the shell:

::: {.highlight-sh .notranslate}
::: highlight
    scrapy shell https://docs.scrapy.org/en/latest/_static/selectors-sample1.html
:::
:::

Then, after the shell loads, you'll have the response available as
[`response`{.docutils .literal .notranslate}]{.pre} shell variable, and
its attached selector in [`response.selector`{.docutils .literal
.notranslate}]{.pre} attribute.

Since we're dealing with HTML, the selector will automatically use an
HTML parser.

So, by looking at the [[HTML code]{.std
.std-ref}](#topics-selectors-htmlcode){.hoverxref .tooltip .reference
.internal} of that page, let's construct an XPath for selecting the text
inside the title tag:

::: {.highlight-pycon .notranslate}
::: highlight
    >>> response.xpath("//title/text()")
    [<Selector query='//title/text()' data='Example website'>]
:::
:::

To actually extract the textual data, you must call the selector
[`.get()`{.docutils .literal .notranslate}]{.pre} or
[`.getall()`{.docutils .literal .notranslate}]{.pre} methods, as
follows:

::: {.highlight-pycon .notranslate}
::: highlight
    >>> response.xpath("//title/text()").getall()
    ['Example website']
    >>> response.xpath("//title/text()").get()
    'Example website'
:::
:::

[`.get()`{.docutils .literal .notranslate}]{.pre} always returns a
single result; if there are several matches, content of a first match is
returned; if there are no matches, None is returned.
[`.getall()`{.docutils .literal .notranslate}]{.pre} returns a list with
all results.

Notice that CSS selectors can select text or attribute nodes using CSS3
pseudo-elements:

::: {.highlight-pycon .notranslate}
::: highlight
    >>> response.css("title::text").get()
    'Example website'
:::
:::

As you can see, [`.xpath()`{.docutils .literal .notranslate}]{.pre} and
[`.css()`{.docutils .literal .notranslate}]{.pre} methods return a
[[`SelectorList`{.xref .py .py-class .docutils .literal
.notranslate}]{.pre}](#scrapy.selector.SelectorList "scrapy.selector.SelectorList"){.reference
.internal} instance, which is a list of new selectors. This API can be
used for quickly selecting nested data:

::: {.highlight-pycon .notranslate}
::: highlight
    >>> response.css("img").xpath("@src").getall()
    ['image1_thumb.jpg',
    'image2_thumb.jpg',
    'image3_thumb.jpg',
    'image4_thumb.jpg',
    'image5_thumb.jpg']
:::
:::

If you want to extract only the first matched element, you can call the
selector [`.get()`{.docutils .literal .notranslate}]{.pre} (or its alias
[`.extract_first()`{.docutils .literal .notranslate}]{.pre} commonly
used in previous Scrapy versions):

::: {.highlight-pycon .notranslate}
::: highlight
    >>> response.xpath('//div[@id="images"]/a/text()').get()
    'Name: My image 1 '
:::
:::

It returns [`None`{.docutils .literal .notranslate}]{.pre} if no element
was found:

::: {.highlight-pycon .notranslate}
::: highlight
    >>> response.xpath('//div[@id="not-exists"]/text()').get() is None
    True
:::
:::

A default return value can be provided as an argument, to be used
instead of [`None`{.docutils .literal .notranslate}]{.pre}:

::: {.highlight-pycon .notranslate}
::: highlight
    >>> response.xpath('//div[@id="not-exists"]/text()').get(default="not-found")
    'not-found'
:::
:::

Instead of using e.g. [`'@src'`{.docutils .literal .notranslate}]{.pre}
XPath it is possible to query for attributes using [`.attrib`{.docutils
.literal .notranslate}]{.pre} property of a [`Selector`{.xref .py
.py-class .docutils .literal .notranslate}]{.pre}:

::: {.highlight-pycon .notranslate}
::: highlight
    >>> [img.attrib["src"] for img in response.css("img")]
    ['image1_thumb.jpg',
    'image2_thumb.jpg',
    'image3_thumb.jpg',
    'image4_thumb.jpg',
    'image5_thumb.jpg']
:::
:::

As a shortcut, [`.attrib`{.docutils .literal .notranslate}]{.pre} is
also available on SelectorList directly; it returns attributes for the
first matching element:

::: {.highlight-pycon .notranslate}
::: highlight
    >>> response.css("img").attrib["src"]
    'image1_thumb.jpg'
:::
:::

This is most useful when only a single result is expected, e.g. when
selecting by id, or selecting unique elements on a web page:

::: {.highlight-pycon .notranslate}
::: highlight
    >>> response.css("base").attrib["href"]
    'http://example.com/'
:::
:::

Now we're going to get the base URL and some image links:

::: {.highlight-pycon .notranslate}
::: highlight
    >>> response.xpath("//base/@href").get()
    'http://example.com/'

    >>> response.css("base::attr(href)").get()
    'http://example.com/'

    >>> response.css("base").attrib["href"]
    'http://example.com/'

    >>> response.xpath('//a[contains(@href, "image")]/@href').getall()
    ['image1.html',
    'image2.html',
    'image3.html',
    'image4.html',
    'image5.html']

    >>> response.css("a[href*=image]::attr(href)").getall()
    ['image1.html',
    'image2.html',
    'image3.html',
    'image4.html',
    'image5.html']

    >>> response.xpath('//a[contains(@href, "image")]/img/@src').getall()
    ['image1_thumb.jpg',
    'image2_thumb.jpg',
    'image3_thumb.jpg',
    'image4_thumb.jpg',
    'image5_thumb.jpg']

    >>> response.css("a[href*=image] img::attr(src)").getall()
    ['image1_thumb.jpg',
    'image2_thumb.jpg',
    'image3_thumb.jpg',
    'image4_thumb.jpg',
    'image5_thumb.jpg']
:::
:::
:::

::: {#extensions-to-css-selectors .section}
[]{#topics-selectors-css-extensions}

##### Extensions to CSS Selectors[¶](#extensions-to-css-selectors "Permalink to this heading"){.headerlink}

Per W3C standards, [CSS
selectors](https://www.w3.org/TR/selectors-3/#selectors){.reference
.external} do not support selecting text nodes or attribute values. But
selecting these is so essential in a web scraping context that Scrapy
(parsel) implements a couple of **non-standard pseudo-elements**:

-   to select text nodes, use [`::text`{.docutils .literal
    .notranslate}]{.pre}

-   to select attribute values, use [`::attr(name)`{.docutils .literal
    .notranslate}]{.pre} where *name* is the name of the attribute that
    you want the value of

::: {.admonition .warning}
Warning

These pseudo-elements are Scrapy-/Parsel-specific. They will most
probably not work with other libraries like
[lxml](https://lxml.de/){.reference .external} or
[PyQuery](https://pypi.org/project/pyquery/){.reference .external}.
:::

Examples:

-   [`title::text`{.docutils .literal .notranslate}]{.pre} selects
    children text nodes of a descendant [`<title>`{.docutils .literal
    .notranslate}]{.pre} element:

::: {.highlight-pycon .notranslate}
::: highlight
    >>> response.css("title::text").get()
    'Example website'
:::
:::

-   [`*::text`{.docutils .literal .notranslate}]{.pre} selects all
    descendant text nodes of the current selector context:

::: {.highlight-pycon .notranslate}
::: highlight
    >>> response.css("#images *::text").getall()
    ['\n   ',
    'Name: My image 1 ',
    '\n   ',
    'Name: My image 2 ',
    '\n   ',
    'Name: My image 3 ',
    '\n   ',
    'Name: My image 4 ',
    '\n   ',
    'Name: My image 5 ',
    '\n  ']
:::
:::

-   [`foo::text`{.docutils .literal .notranslate}]{.pre} returns no
    results if [`foo`{.docutils .literal .notranslate}]{.pre} element
    exists, but contains no text (i.e. text is empty):

::: {.highlight-pycon .notranslate}
::: highlight
    >>> response.css("img::text").getall()
    []

    This means ``.css('foo::text').get()`` could return None even if an element
    exists. Use ``default=''`` if you always want a string:
:::
:::

::: {.highlight-pycon .notranslate}
::: highlight
    >>> response.css("img::text").get()
    >>> response.css("img::text").get(default="")
    ''
:::
:::

-   [`a::attr(href)`{.docutils .literal .notranslate}]{.pre} selects the
    *href* attribute value of descendant links:

::: {.highlight-pycon .notranslate}
::: highlight
    >>> response.css("a::attr(href)").getall()
    ['image1.html',
    'image2.html',
    'image3.html',
    'image4.html',
    'image5.html']
:::
:::

::: {.admonition .note}
Note

See also: [[Selecting element attributes]{.std
.std-ref}](#selecting-attributes){.hoverxref .tooltip .reference
.internal}.
:::

::: {.admonition .note}
Note

You cannot chain these pseudo-elements. But in practice it would not
make much sense: text nodes do not have attributes, and attribute values
are string values already and do not have children nodes.
:::
:::

::: {#nesting-selectors .section}
[]{#topics-selectors-nesting-selectors}

##### Nesting selectors[¶](#nesting-selectors "Permalink to this heading"){.headerlink}

The selection methods ([`.xpath()`{.docutils .literal
.notranslate}]{.pre} or [`.css()`{.docutils .literal
.notranslate}]{.pre}) return a list of selectors of the same type, so
you can call the selection methods for those selectors too. Here's an
example:

::: {.highlight-pycon .notranslate}
::: highlight
    >>> links = response.xpath('//a[contains(@href, "image")]')
    >>> links.getall()
    ['<a href="image1.html">Name: My image 1 <br><img src="image1_thumb.jpg" alt="image1"></a>',
    '<a href="image2.html">Name: My image 2 <br><img src="image2_thumb.jpg" alt="image2"></a>',
    '<a href="image3.html">Name: My image 3 <br><img src="image3_thumb.jpg" alt="image3"></a>',
    '<a href="image4.html">Name: My image 4 <br><img src="image4_thumb.jpg" alt="image4"></a>',
    '<a href="image5.html">Name: My image 5 <br><img src="image5_thumb.jpg" alt="image5"></a>']

    >>> for index, link in enumerate(links):
    ...     href_xpath = link.xpath("@href").get()
    ...     img_xpath = link.xpath("img/@src").get()
    ...     print(f"Link number {index} points to url {href_xpath!r} and image {img_xpath!r}")
    ...
    Link number 0 points to url 'image1.html' and image 'image1_thumb.jpg'
    Link number 1 points to url 'image2.html' and image 'image2_thumb.jpg'
    Link number 2 points to url 'image3.html' and image 'image3_thumb.jpg'
    Link number 3 points to url 'image4.html' and image 'image4_thumb.jpg'
    Link number 4 points to url 'image5.html' and image 'image5_thumb.jpg'
:::
:::
:::

::: {#selecting-element-attributes .section}
[]{#selecting-attributes}

##### Selecting element attributes[¶](#selecting-element-attributes "Permalink to this heading"){.headerlink}

There are several ways to get a value of an attribute. First, one can
use XPath syntax:

::: {.highlight-pycon .notranslate}
::: highlight
    >>> response.xpath("//a/@href").getall()
    ['image1.html', 'image2.html', 'image3.html', 'image4.html', 'image5.html']
:::
:::

XPath syntax has a few advantages: it is a standard XPath feature, and
[`@attributes`{.docutils .literal .notranslate}]{.pre} can be used in
other parts of an XPath expression - e.g. it is possible to filter by
attribute value.

Scrapy also provides an extension to CSS selectors
([`::attr(...)`{.docutils .literal .notranslate}]{.pre}) which allows to
get attribute values:

::: {.highlight-pycon .notranslate}
::: highlight
    >>> response.css("a::attr(href)").getall()
    ['image1.html', 'image2.html', 'image3.html', 'image4.html', 'image5.html']
:::
:::

In addition to that, there is a [`.attrib`{.docutils .literal
.notranslate}]{.pre} property of Selector. You can use it if you prefer
to lookup attributes in Python code, without using XPaths or CSS
extensions:

::: {.highlight-pycon .notranslate}
::: highlight
    >>> [a.attrib["href"] for a in response.css("a")]
    ['image1.html', 'image2.html', 'image3.html', 'image4.html', 'image5.html']
:::
:::

This property is also available on SelectorList; it returns a dictionary
with attributes of a first matching element. It is convenient to use
when a selector is expected to give a single result (e.g. when selecting
by element ID, or when selecting an unique element on a page):

::: {.highlight-pycon .notranslate}
::: highlight
    >>> response.css("base").attrib
    {'href': 'http://example.com/'}
    >>> response.css("base").attrib["href"]
    'http://example.com/'
:::
:::

[`.attrib`{.docutils .literal .notranslate}]{.pre} property of an empty
SelectorList is empty:

::: {.highlight-pycon .notranslate}
::: highlight
    >>> response.css("foo").attrib
    {}
:::
:::
:::

::: {#using-selectors-with-regular-expressions .section}
##### Using selectors with regular expressions[¶](#using-selectors-with-regular-expressions "Permalink to this heading"){.headerlink}

[`Selector`{.xref .py .py-class .docutils .literal .notranslate}]{.pre}
also has a [`.re()`{.docutils .literal .notranslate}]{.pre} method for
extracting data using regular expressions. However, unlike using
[`.xpath()`{.docutils .literal .notranslate}]{.pre} or
[`.css()`{.docutils .literal .notranslate}]{.pre} methods,
[`.re()`{.docutils .literal .notranslate}]{.pre} returns a list of
strings. So you can't construct nested [`.re()`{.docutils .literal
.notranslate}]{.pre} calls.

Here's an example used to extract image names from the [[HTML code]{.std
.std-ref}](#topics-selectors-htmlcode){.hoverxref .tooltip .reference
.internal} above:

::: {.highlight-pycon .notranslate}
::: highlight
    >>> response.xpath('//a[contains(@href, "image")]/text()').re(r"Name:\s*(.*)")
    ['My image 1 ',
    'My image 2 ',
    'My image 3 ',
    'My image 4 ',
    'My image 5 ']
:::
:::

There's an additional helper reciprocating [`.get()`{.docutils .literal
.notranslate}]{.pre} (and its alias [`.extract_first()`{.docutils
.literal .notranslate}]{.pre}) for [`.re()`{.docutils .literal
.notranslate}]{.pre}, named [`.re_first()`{.docutils .literal
.notranslate}]{.pre}. Use it to extract just the first matching string:

::: {.highlight-pycon .notranslate}
::: highlight
    >>> response.xpath('//a[contains(@href, "image")]/text()').re_first(r"Name:\s*(.*)")
    'My image 1 '
:::
:::
:::

::: {#extract-and-extract-first .section}
[]{#old-extraction-api}

##### extract() and extract_first()[¶](#extract-and-extract-first "Permalink to this heading"){.headerlink}

If you're a long-time Scrapy user, you're probably familiar with
[`.extract()`{.docutils .literal .notranslate}]{.pre} and
[`.extract_first()`{.docutils .literal .notranslate}]{.pre} selector
methods. Many blog posts and tutorials are using them as well. These
methods are still supported by Scrapy, there are **no plans** to
deprecate them.

However, Scrapy usage docs are now written using [`.get()`{.docutils
.literal .notranslate}]{.pre} and [`.getall()`{.docutils .literal
.notranslate}]{.pre} methods. We feel that these new methods result in a
more concise and readable code.

The following examples show how these methods map to each other.

1.  [`SelectorList.get()`{.docutils .literal .notranslate}]{.pre} is the
    same as [`SelectorList.extract_first()`{.docutils .literal
    .notranslate}]{.pre}:

::: {.highlight-pycon .notranslate}
::: highlight
    >>> response.css("a::attr(href)").get()
    'image1.html'
    >>> response.css("a::attr(href)").extract_first()
    'image1.html'
:::
:::

2.  [`SelectorList.getall()`{.docutils .literal .notranslate}]{.pre} is
    the same as [`SelectorList.extract()`{.docutils .literal
    .notranslate}]{.pre}:

::: {.highlight-pycon .notranslate}
::: highlight
    >>> response.css("a::attr(href)").getall()
    ['image1.html', 'image2.html', 'image3.html', 'image4.html', 'image5.html']
    >>> response.css("a::attr(href)").extract()
    ['image1.html', 'image2.html', 'image3.html', 'image4.html', 'image5.html']
:::
:::

3.  [`Selector.get()`{.docutils .literal .notranslate}]{.pre} is the
    same as [`Selector.extract()`{.docutils .literal
    .notranslate}]{.pre}:

::: {.highlight-pycon .notranslate}
::: highlight
    >>> response.css("a::attr(href)")[0].get()
    'image1.html'
    >>> response.css("a::attr(href)")[0].extract()
    'image1.html'
:::
:::

4.  For consistency, there is also [`Selector.getall()`{.docutils
    .literal .notranslate}]{.pre}, which returns a list:

::: {.highlight-pycon .notranslate}
::: highlight
    >>> response.css("a::attr(href)")[0].getall()
    ['image1.html']
:::
:::

So, the main difference is that output of [`.get()`{.docutils .literal
.notranslate}]{.pre} and [`.getall()`{.docutils .literal
.notranslate}]{.pre} methods is more predictable: [`.get()`{.docutils
.literal .notranslate}]{.pre} always returns a single result,
[`.getall()`{.docutils .literal .notranslate}]{.pre} always returns a
list of all extracted results. With [`.extract()`{.docutils .literal
.notranslate}]{.pre} method it was not always obvious if a result is a
list or not; to get a single result either [`.extract()`{.docutils
.literal .notranslate}]{.pre} or [`.extract_first()`{.docutils .literal
.notranslate}]{.pre} should be called.
:::
:::

::: {#working-with-xpaths .section}
[]{#topics-selectors-xpaths}

#### Working with XPaths[¶](#working-with-xpaths "Permalink to this heading"){.headerlink}

Here are some tips which may help you to use XPath with Scrapy selectors
effectively. If you are not much familiar with XPath yet, you may want
to take a look first at this [XPath
tutorial](http://www.zvon.org/comp/r/tut-XPath_1.html){.reference
.external}.

::: {.admonition .note}
Note

Some of the tips are based on [this post from Zyte's
blog](https://www.zyte.com/blog/xpath-tips-from-the-web-scraping-trenches/){.reference
.external}.
:::

::: {#working-with-relative-xpaths .section}
[]{#topics-selectors-relative-xpaths}

##### Working with relative XPaths[¶](#working-with-relative-xpaths "Permalink to this heading"){.headerlink}

Keep in mind that if you are nesting selectors and use an XPath that
starts with [`/`{.docutils .literal .notranslate}]{.pre}, that XPath
will be absolute to the document and not relative to the
[`Selector`{.docutils .literal .notranslate}]{.pre} you're calling it
from.

For example, suppose you want to extract all [`<p>`{.docutils .literal
.notranslate}]{.pre} elements inside [`<div>`{.docutils .literal
.notranslate}]{.pre} elements. First, you would get all
[`<div>`{.docutils .literal .notranslate}]{.pre} elements:

::: {.highlight-pycon .notranslate}
::: highlight
    >>> divs = response.xpath("//div")
:::
:::

At first, you may be tempted to use the following approach, which is
wrong, as it actually extracts all [`<p>`{.docutils .literal
.notranslate}]{.pre} elements from the document, not only those inside
[`<div>`{.docutils .literal .notranslate}]{.pre} elements:

::: {.highlight-pycon .notranslate}
::: highlight
    >>> for p in divs.xpath("//p"):  # this is wrong - gets all <p> from the whole document
    ...     print(p.get())
    ...
:::
:::

This is the proper way to do it (note the dot prefixing the
[`.//p`{.docutils .literal .notranslate}]{.pre} XPath):

::: {.highlight-pycon .notranslate}
::: highlight
    >>> for p in divs.xpath(".//p"):  # extracts all <p> inside
    ...     print(p.get())
    ...
:::
:::

Another common case would be to extract all direct [`<p>`{.docutils
.literal .notranslate}]{.pre} children:

::: {.highlight-pycon .notranslate}
::: highlight
    >>> for p in divs.xpath("p"):
    ...     print(p.get())
    ...
:::
:::

For more details about relative XPaths see the [Location
Paths](https://www.w3.org/TR/xpath/all/#location-paths){.reference
.external} section in the XPath specification.
:::

::: {#when-querying-by-class-consider-using-css .section}
##### When querying by class, consider using CSS[¶](#when-querying-by-class-consider-using-css "Permalink to this heading"){.headerlink}

Because an element can contain multiple CSS classes, the XPath way to
select elements by class is the rather verbose:

::: {.highlight-python .notranslate}
::: highlight
    *[contains(concat(' ', normalize-space(@class), ' '), ' someclass ')]
:::
:::

If you use [`@class='someclass'`{.docutils .literal .notranslate}]{.pre}
you may end up missing elements that have other classes, and if you just
use [`contains(@class,`{.docutils .literal
.notranslate}]{.pre}` `{.docutils .literal
.notranslate}[`'someclass')`{.docutils .literal .notranslate}]{.pre} to
make up for that you may end up with more elements that you want, if
they have a different class name that shares the string
[`someclass`{.docutils .literal .notranslate}]{.pre}.

As it turns out, Scrapy selectors allow you to chain selectors, so most
of the time you can just select by class using CSS and then switch to
XPath when needed:

::: {.highlight-pycon .notranslate}
::: highlight
    >>> from scrapy import Selector
    >>> sel = Selector(
    ...     text='<div class="hero shout"><time datetime="2014-07-23 19:00">Special date</time></div>'
    ... )
    >>> sel.css(".shout").xpath("./time/@datetime").getall()
    ['2014-07-23 19:00']
:::
:::

This is cleaner than using the verbose XPath trick shown above. Just
remember to use the [`.`{.docutils .literal .notranslate}]{.pre} in the
XPath expressions that will follow.
:::

::: {#beware-of-the-difference-between-node-1-and-node-1 .section}
##### Beware of the difference between //node\[1\] and (//node)\[1\][¶](#beware-of-the-difference-between-node-1-and-node-1 "Permalink to this heading"){.headerlink}

[`//node[1]`{.docutils .literal .notranslate}]{.pre} selects all the
nodes occurring first under their respective parents.

[`(//node)[1]`{.docutils .literal .notranslate}]{.pre} selects all the
nodes in the document, and then gets only the first of them.

Example:

::: {.highlight-pycon .notranslate}
::: highlight
    >>> from scrapy import Selector
    >>> sel = Selector(
    ...     text="""
    ...     <ul class="list">
    ...         <li>1</li>
    ...         <li>2</li>
    ...         <li>3</li>
    ...     </ul>
    ...     <ul class="list">
    ...         <li>4</li>
    ...         <li>5</li>
    ...         <li>6</li>
    ...     </ul>"""
    ... )
    >>> xp = lambda x: sel.xpath(x).getall()
:::
:::

This gets all first [`<li>`{.docutils .literal .notranslate}]{.pre}
elements under whatever it is its parent:

::: {.highlight-pycon .notranslate}
::: highlight
    >>> xp("//li[1]")
    ['<li>1</li>', '<li>4</li>']
:::
:::

And this gets the first [`<li>`{.docutils .literal .notranslate}]{.pre}
element in the whole document:

::: {.highlight-pycon .notranslate}
::: highlight
    >>> xp("(//li)[1]")
    ['<li>1</li>']
:::
:::

This gets all first [`<li>`{.docutils .literal .notranslate}]{.pre}
elements under an [`<ul>`{.docutils .literal .notranslate}]{.pre}
parent:

::: {.highlight-pycon .notranslate}
::: highlight
    >>> xp("//ul/li[1]")
    ['<li>1</li>', '<li>4</li>']
:::
:::

And this gets the first [`<li>`{.docutils .literal .notranslate}]{.pre}
element under an [`<ul>`{.docutils .literal .notranslate}]{.pre} parent
in the whole document:

::: {.highlight-pycon .notranslate}
::: highlight
    >>> xp("(//ul/li)[1]")
    ['<li>1</li>']
:::
:::
:::

::: {#using-text-nodes-in-a-condition .section}
##### Using text nodes in a condition[¶](#using-text-nodes-in-a-condition "Permalink to this heading"){.headerlink}

When you need to use the text content as argument to an [XPath string
function](https://www.w3.org/TR/xpath/all/#section-String-Functions){.reference
.external}, avoid using [`.//text()`{.docutils .literal
.notranslate}]{.pre} and use just [`.`{.docutils .literal
.notranslate}]{.pre} instead.

This is because the expression [`.//text()`{.docutils .literal
.notranslate}]{.pre} yields a collection of text elements -- a
*node-set*. And when a node-set is converted to a string, which happens
when it is passed as argument to a string function like
[`contains()`{.docutils .literal .notranslate}]{.pre} or
[`starts-with()`{.docutils .literal .notranslate}]{.pre}, it results in
the text for the first element only.

Example:

::: {.highlight-pycon .notranslate}
::: highlight
    >>> from scrapy import Selector
    >>> sel = Selector(
    ...     text='<a href="#">Click here to go to the <strong>Next Page</strong></a>'
    ... )
:::
:::

Converting a *node-set* to string:

::: {.highlight-pycon .notranslate}
::: highlight
    >>> sel.xpath("//a//text()").getall()  # take a peek at the node-set
    ['Click here to go to the ', 'Next Page']
    >>> sel.xpath("string(//a[1]//text())").getall()  # convert it to string
    ['Click here to go to the ']
:::
:::

A *node* converted to a string, however, puts together the text of
itself plus of all its descendants:

::: {.highlight-pycon .notranslate}
::: highlight
    >>> sel.xpath("//a[1]").getall()  # select the first node
    ['<a href="#">Click here to go to the <strong>Next Page</strong></a>']
    >>> sel.xpath("string(//a[1])").getall()  # convert it to string
    ['Click here to go to the Next Page']
:::
:::

So, using the [`.//text()`{.docutils .literal .notranslate}]{.pre}
node-set won't select anything in this case:

::: {.highlight-pycon .notranslate}
::: highlight
    >>> sel.xpath("//a[contains(.//text(), 'Next Page')]").getall()
    []
:::
:::

But using the [`.`{.docutils .literal .notranslate}]{.pre} to mean the
node, works:

::: {.highlight-pycon .notranslate}
::: highlight
    >>> sel.xpath("//a[contains(., 'Next Page')]").getall()
    ['<a href="#">Click here to go to the <strong>Next Page</strong></a>']
:::
:::
:::

::: {#variables-in-xpath-expressions .section}
[]{#topics-selectors-xpath-variables}

##### Variables in XPath expressions[¶](#variables-in-xpath-expressions "Permalink to this heading"){.headerlink}

XPath allows you to reference variables in your XPath expressions, using
the [`$somevariable`{.docutils .literal .notranslate}]{.pre} syntax.
This is somewhat similar to parameterized queries or prepared statements
in the SQL world where you replace some arguments in your queries with
placeholders like [`?`{.docutils .literal .notranslate}]{.pre}, which
are then substituted with values passed with the query.

Here's an example to match an element based on its "id" attribute value,
without hard-coding it (that was shown previously):

::: {.highlight-pycon .notranslate}
::: highlight
    >>> # `$val` used in the expression, a `val` argument needs to be passed
    >>> response.xpath("//div[@id=$val]/a/text()", val="images").get()
    'Name: My image 1 '
:::
:::

Here's another example, to find the "id" attribute of a
[`<div>`{.docutils .literal .notranslate}]{.pre} tag containing five
[`<a>`{.docutils .literal .notranslate}]{.pre} children (here we pass
the value [`5`{.docutils .literal .notranslate}]{.pre} as an integer):

::: {.highlight-pycon .notranslate}
::: highlight
    >>> response.xpath("//div[count(a)=$cnt]/@id", cnt=5).get()
    'images'
:::
:::

All variable references must have a binding value when calling
[`.xpath()`{.docutils .literal .notranslate}]{.pre} (otherwise you'll
get a [`ValueError:`{.docutils .literal
.notranslate}]{.pre}` `{.docutils .literal
.notranslate}[`XPath`{.docutils .literal
.notranslate}]{.pre}` `{.docutils .literal
.notranslate}[`error:`{.docutils .literal .notranslate}]{.pre}
exception). This is done by passing as many named arguments as
necessary.

[parsel](https://parsel.readthedocs.io/en/latest/){.reference
.external}, the library powering Scrapy selectors, has more details and
examples on [XPath
variables](https://parsel.readthedocs.io/en/latest/usage.html#variables-in-xpath-expressions){.reference
.external}.
:::

::: {#removing-namespaces .section}
[]{#id2}

##### Removing namespaces[¶](#removing-namespaces "Permalink to this heading"){.headerlink}

When dealing with scraping projects, it is often quite convenient to get
rid of namespaces altogether and just work with element names, to write
more simple/convenient XPaths. You can use the
[`Selector.remove_namespaces()`{.xref .py .py-meth .docutils .literal
.notranslate}]{.pre} method for that.

Let's show an example that illustrates this with the Python Insider blog
atom feed.

First, we open the shell with the url we want to scrape:

::: {.highlight-sh .notranslate}
::: highlight
    $ scrapy shell https://feeds.feedburner.com/PythonInsider
:::
:::

This is how the file starts:

::: {.highlight-sh .notranslate}
::: highlight
    <?xml version="1.0" encoding="UTF-8"?>
    <?xml-stylesheet ...
    <feed xmlns="http://www.w3.org/2005/Atom"
          xmlns:openSearch="http://a9.com/-/spec/opensearchrss/1.0/"
          xmlns:blogger="http://schemas.google.com/blogger/2008"
          xmlns:georss="http://www.georss.org/georss"
          xmlns:gd="http://schemas.google.com/g/2005"
          xmlns:thr="http://purl.org/syndication/thread/1.0"
          xmlns:feedburner="http://rssnamespace.org/feedburner/ext/1.0">
      ...
:::
:::

You can see several namespace declarations including a default
"[http://www.w3.org/2005/Atom](http://www.w3.org/2005/Atom){.reference
.external}" and another one using the "gd:" prefix for
"[http://schemas.google.com/g/2005](http://schemas.google.com/g/2005){.reference
.external}".

Once in the shell we can try selecting all [`<link>`{.docutils .literal
.notranslate}]{.pre} objects and see that it doesn't work (because the
Atom XML namespace is obfuscating those nodes):

::: {.highlight-pycon .notranslate}
::: highlight
    >>> response.xpath("//link")
    []
:::
:::

But once we call the [`Selector.remove_namespaces()`{.xref .py .py-meth
.docutils .literal .notranslate}]{.pre} method, all nodes can be
accessed directly by their names:

::: {.highlight-pycon .notranslate}
::: highlight
    >>> response.selector.remove_namespaces()
    >>> response.xpath("//link")
    [<Selector query='//link' data='<link rel="alternate" type="text/html" h'>,
        <Selector query='//link' data='<link rel="next" type="application/atom+'>,
        ...
:::
:::

If you wonder why the namespace removal procedure isn't always called by
default instead of having to call it manually, this is because of two
reasons, which, in order of relevance, are:

1.  Removing namespaces requires to iterate and modify all nodes in the
    document, which is a reasonably expensive operation to perform by
    default for all documents crawled by Scrapy

2.  There could be some cases where using namespaces is actually
    required, in case some element names clash between namespaces. These
    cases are very rare though.
:::

::: {#using-exslt-extensions .section}
##### Using EXSLT extensions[¶](#using-exslt-extensions "Permalink to this heading"){.headerlink}

Being built atop [lxml](https://lxml.de/){.reference .external}, Scrapy
selectors support some [EXSLT](http://exslt.org/){.reference .external}
extensions and come with these pre-registered namespaces to use in XPath
expressions:

+-----+---------------------------------------+------------------------+
| pre | namespace                             | usage                  |
| fix |                                       |                        |
+=====+=======================================+========================+
| re  | http://exslt.org/regular-expressions  | [regular               |
|     |                                       | expressions](ht        |
|     |                                       | tp://exslt.org/regexp/ |
|     |                                       | index.html){.reference |
|     |                                       | .external}             |
+-----+---------------------------------------+------------------------+
| set | http://exslt.org/sets                 | [set                   |
|     |                                       | manipulation]          |
|     |                                       | (http://exslt.org/set/ |
|     |                                       | index.html){.reference |
|     |                                       | .external}             |
+-----+---------------------------------------+------------------------+

::: {#regular-expressions .section}
###### Regular expressions[¶](#regular-expressions "Permalink to this heading"){.headerlink}

The [`test()`{.docutils .literal .notranslate}]{.pre} function, for
example, can prove quite useful when XPath's [`starts-with()`{.docutils
.literal .notranslate}]{.pre} or [`contains()`{.docutils .literal
.notranslate}]{.pre} are not sufficient.

Example selecting links in list item with a "class" attribute ending
with a digit:

::: {.highlight-pycon .notranslate}
::: highlight
    >>> from scrapy import Selector
    >>> doc = """
    ... <div>
    ...     <ul>
    ...         <li class="item-0"><a href="link1.html">first item</a></li>
    ...         <li class="item-1"><a href="link2.html">second item</a></li>
    ...         <li class="item-inactive"><a href="link3.html">third item</a></li>
    ...         <li class="item-1"><a href="link4.html">fourth item</a></li>
    ...         <li class="item-0"><a href="link5.html">fifth item</a></li>
    ...     </ul>
    ... </div>
    ... """
    >>> sel = Selector(text=doc, type="html")
    >>> sel.xpath("//li//@href").getall()
    ['link1.html', 'link2.html', 'link3.html', 'link4.html', 'link5.html']
    >>> sel.xpath('//li[re:test(@class, "item-\d$")]//@href').getall()
    ['link1.html', 'link2.html', 'link4.html', 'link5.html']
:::
:::

::: {.admonition .warning}
Warning

C library [`libxslt`{.docutils .literal .notranslate}]{.pre} doesn't
natively support EXSLT regular expressions so
[lxml](https://lxml.de/){.reference .external}'s implementation uses
hooks to Python's [`re`{.docutils .literal .notranslate}]{.pre} module.
Thus, using regexp functions in your XPath expressions may add a small
performance penalty.
:::
:::

::: {#set-operations .section}
###### Set operations[¶](#set-operations "Permalink to this heading"){.headerlink}

These can be handy for excluding parts of a document tree before
extracting text elements for example.

Example extracting microdata (sample content taken from
[https://schema.org/Product](https://schema.org/Product){.reference
.external}) with groups of itemscopes and corresponding itemprops:

::: {.highlight-pycon .notranslate}
::: highlight
    >>> doc = """
    ... <div itemscope itemtype="http://schema.org/Product">
    ...   <span itemprop="name">Kenmore White 17" Microwave</span>
    ...   <img src="kenmore-microwave-17in.jpg" alt='Kenmore 17" Microwave' />
    ...   <div itemprop="aggregateRating"
    ...     itemscope itemtype="http://schema.org/AggregateRating">
    ...    Rated <span itemprop="ratingValue">3.5</span>/5
    ...    based on <span itemprop="reviewCount">11</span> customer reviews
    ...   </div>
    ...   <div itemprop="offers" itemscope itemtype="http://schema.org/Offer">
    ...     <span itemprop="price">$55.00</span>
    ...     <link itemprop="availability" href="http://schema.org/InStock" />In stock
    ...   </div>
    ...   Product description:
    ...   <span itemprop="description">0.7 cubic feet countertop microwave.
    ...   Has six preset cooking categories and convenience features like
    ...   Add-A-Minute and Child Lock.</span>
    ...   Customer reviews:
    ...   <div itemprop="review" itemscope itemtype="http://schema.org/Review">
    ...     <span itemprop="name">Not a happy camper</span> -
    ...     by <span itemprop="author">Ellie</span>,
    ...     <meta itemprop="datePublished" content="2011-04-01">April 1, 2011
    ...     <div itemprop="reviewRating" itemscope itemtype="http://schema.org/Rating">
    ...       <meta itemprop="worstRating" content = "1">
    ...       <span itemprop="ratingValue">1</span>/
    ...       <span itemprop="bestRating">5</span>stars
    ...     </div>
    ...     <span itemprop="description">The lamp burned out and now I have to replace
    ...     it. </span>
    ...   </div>
    ...   <div itemprop="review" itemscope itemtype="http://schema.org/Review">
    ...     <span itemprop="name">Value purchase</span> -
    ...     by <span itemprop="author">Lucas</span>,
    ...     <meta itemprop="datePublished" content="2011-03-25">March 25, 2011
    ...     <div itemprop="reviewRating" itemscope itemtype="http://schema.org/Rating">
    ...       <meta itemprop="worstRating" content = "1"/>
    ...       <span itemprop="ratingValue">4</span>/
    ...       <span itemprop="bestRating">5</span>stars
    ...     </div>
    ...     <span itemprop="description">Great microwave for the price. It is small and
    ...     fits in my apartment.</span>
    ...   </div>
    ...   ...
    ... </div>
    ... """
    >>> sel = Selector(text=doc, type="html")
    >>> for scope in sel.xpath("//div[@itemscope]"):
    ...     print("current scope:", scope.xpath("@itemtype").getall())
    ...     props = scope.xpath(
    ...         """
    ...                 set:difference(./descendant::*/@itemprop,
    ...                                .//*[@itemscope]/*/@itemprop)"""
    ...     )
    ...     print(f"    properties: {props.getall()}")
    ...     print("")
    ...

    current scope: ['http://schema.org/Product']
        properties: ['name', 'aggregateRating', 'offers', 'description', 'review', 'review']

    current scope: ['http://schema.org/AggregateRating']
        properties: ['ratingValue', 'reviewCount']

    current scope: ['http://schema.org/Offer']
        properties: ['price', 'availability']

    current scope: ['http://schema.org/Review']
        properties: ['name', 'author', 'datePublished', 'reviewRating', 'description']

    current scope: ['http://schema.org/Rating']
        properties: ['worstRating', 'ratingValue', 'bestRating']

    current scope: ['http://schema.org/Review']
        properties: ['name', 'author', 'datePublished', 'reviewRating', 'description']

    current scope: ['http://schema.org/Rating']
        properties: ['worstRating', 'ratingValue', 'bestRating']
:::
:::

Here we first iterate over [`itemscope`{.docutils .literal
.notranslate}]{.pre} elements, and for each one, we look for all
[`itemprops`{.docutils .literal .notranslate}]{.pre} elements and
exclude those that are themselves inside another [`itemscope`{.docutils
.literal .notranslate}]{.pre}.
:::
:::

::: {#other-xpath-extensions .section}
##### Other XPath extensions[¶](#other-xpath-extensions "Permalink to this heading"){.headerlink}

Scrapy selectors also provide a sorely missed XPath extension function
[`has-class`{.docutils .literal .notranslate}]{.pre} that returns
[`True`{.docutils .literal .notranslate}]{.pre} for nodes that have all
of the specified HTML classes.

For the following HTML:

::: {.highlight-pycon .notranslate}
::: highlight
    >>> from scrapy.http import HtmlResponse
    >>> response = HtmlResponse(
    ...     url="http://example.com",
    ...     body="""
    ... <html>
    ...     <body>
    ...         <p class="foo bar-baz">First</p>
    ...         <p class="foo">Second</p>
    ...         <p class="bar">Third</p>
    ...         <p>Fourth</p>
    ...     </body>
    ... </html>
    ... """,
    ...     encoding="utf-8",
    ... )
:::
:::

You can use it like this:

::: {.highlight-pycon .notranslate}
::: highlight
    >>> response.xpath('//p[has-class("foo")]')
    [<Selector query='//p[has-class("foo")]' data='<p class="foo bar-baz">First</p>'>,
    <Selector query='//p[has-class("foo")]' data='<p class="foo">Second</p>'>]
    >>> response.xpath('//p[has-class("foo", "bar-baz")]')
    [<Selector query='//p[has-class("foo", "bar-baz")]' data='<p class="foo bar-baz">First</p>'>]
    >>> response.xpath('//p[has-class("foo", "bar")]')
    []
:::
:::

So XPath [`//p[has-class("foo",`{.docutils .literal
.notranslate}]{.pre}` `{.docutils .literal
.notranslate}[`"bar-baz")]`{.docutils .literal .notranslate}]{.pre} is
roughly equivalent to CSS [`p.foo.bar-baz`{.docutils .literal
.notranslate}]{.pre}. Please note, that it is slower in most of the
cases, because it's a pure-Python function that's invoked for every node
in question whereas the CSS lookup is translated into XPath and thus
runs more efficiently, so performance-wise its uses are limited to
situations that are not easily described with CSS selectors.

Parsel also simplifies adding your own XPath extensions.

[[parsel.xpathfuncs.]{.pre}]{.sig-prename .descclassname}[[set_xpathfunc]{.pre}]{.sig-name .descname}[(]{.sig-paren}*[[fname]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[[[str]{.pre}](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.12)"){.reference .external}]{.n}*, *[[func]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[[[Optional]{.pre}](https://docs.python.org/3/library/typing.html#typing.Optional "(in Python v3.12)"){.reference .external}[[\[]{.pre}]{.p}[[Callable]{.pre}](https://docs.python.org/3/library/typing.html#typing.Callable "(in Python v3.12)"){.reference .external}[[\]]{.pre}]{.p}]{.n}*[)]{.sig-paren} [[→]{.sig-return-icon} [[[None]{.pre}](https://docs.python.org/3/library/constants.html#None "(in Python v3.12)"){.reference .external}]{.sig-return-typehint}]{.sig-return}[[[\[source\]]{.pre}]{.viewcode-link}](_modules/parsel/xpathfuncs.html#set_xpathfunc){.reference .internal}[¶](#parsel.xpathfuncs.set_xpathfunc "Permalink to this definition"){.headerlink}

:   Register a custom extension function to use in XPath expressions.

    The function [`func`{.docutils .literal .notranslate}]{.pre}
    registered under [`fname`{.docutils .literal .notranslate}]{.pre}
    identifier will be called for every matching node, being passed a
    [`context`{.docutils .literal .notranslate}]{.pre} parameter as well
    as any parameters passed from the corresponding XPath expression.

    If [`func`{.docutils .literal .notranslate}]{.pre} is
    [`None`{.docutils .literal .notranslate}]{.pre}, the extension
    function will be removed.

    See more [in lxml
    documentation](https://lxml.de/extensions.html#xpath-extension-functions){.reference
    .external}.
:::
:::

::: {#module-scrapy.selector .section}
[]{#built-in-selectors-reference}[]{#topics-selectors-ref}

#### Built-in Selectors reference[¶](#module-scrapy.selector "Permalink to this heading"){.headerlink}

::: {#selector-objects .section}
##### Selector objects[¶](#selector-objects "Permalink to this heading"){.headerlink}

*[class]{.pre}[ ]{.w}*[[scrapy.selector.]{.pre}]{.sig-prename .descclassname}[[Selector]{.pre}]{.sig-name .descname}[(]{.sig-paren}*[[\*]{.pre}]{.o}[[args]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[[[Any]{.pre}](https://docs.python.org/3/library/typing.html#typing.Any "(in Python v3.12)"){.reference .external}]{.n}*, *[[\*\*]{.pre}]{.o}[[kwargs]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[[[Any]{.pre}](https://docs.python.org/3/library/typing.html#typing.Any "(in Python v3.12)"){.reference .external}]{.n}*[)]{.sig-paren}[[[\[source\]]{.pre}]{.viewcode-link}](_modules/scrapy/selector/unified.html#Selector){.reference .internal}[¶](#scrapy.selector.Selector "Permalink to this definition"){.headerlink}

:   An instance of [[`Selector`{.xref .py .py-class .docutils .literal
    .notranslate}]{.pre}](#scrapy.selector.Selector "scrapy.selector.Selector"){.reference
    .internal} is a wrapper over response to select certain parts of its
    content.

    [`response`{.docutils .literal .notranslate}]{.pre} is an
    [[`HtmlResponse`{.xref .py .py-class .docutils .literal
    .notranslate}]{.pre}](index.html#scrapy.http.HtmlResponse "scrapy.http.HtmlResponse"){.reference
    .internal} or an [[`XmlResponse`{.xref .py .py-class .docutils
    .literal
    .notranslate}]{.pre}](index.html#scrapy.http.XmlResponse "scrapy.http.XmlResponse"){.reference
    .internal} object that will be used for selecting and extracting
    data.

    [`text`{.docutils .literal .notranslate}]{.pre} is a unicode string
    or utf-8 encoded text for cases when a [`response`{.docutils
    .literal .notranslate}]{.pre} isn't available. Using
    [`text`{.docutils .literal .notranslate}]{.pre} and
    [`response`{.docutils .literal .notranslate}]{.pre} together is
    undefined behavior.

    [`type`{.docutils .literal .notranslate}]{.pre} defines the selector
    type, it can be [`"html"`{.docutils .literal .notranslate}]{.pre},
    [`"xml"`{.docutils .literal .notranslate}]{.pre} or
    [`None`{.docutils .literal .notranslate}]{.pre} (default).

    If [`type`{.docutils .literal .notranslate}]{.pre} is
    [`None`{.docutils .literal .notranslate}]{.pre}, the selector
    automatically chooses the best type based on [`response`{.docutils
    .literal .notranslate}]{.pre} type (see below), or defaults to
    [`"html"`{.docutils .literal .notranslate}]{.pre} in case it is used
    together with [`text`{.docutils .literal .notranslate}]{.pre}.

    If [`type`{.docutils .literal .notranslate}]{.pre} is
    [`None`{.docutils .literal .notranslate}]{.pre} and a
    [`response`{.docutils .literal .notranslate}]{.pre} is passed, the
    selector type is inferred from the response type as follows:

    -   [`"html"`{.docutils .literal .notranslate}]{.pre} for
        [[`HtmlResponse`{.xref .py .py-class .docutils .literal
        .notranslate}]{.pre}](index.html#scrapy.http.HtmlResponse "scrapy.http.HtmlResponse"){.reference
        .internal} type

    -   [`"xml"`{.docutils .literal .notranslate}]{.pre} for
        [[`XmlResponse`{.xref .py .py-class .docutils .literal
        .notranslate}]{.pre}](index.html#scrapy.http.XmlResponse "scrapy.http.XmlResponse"){.reference
        .internal} type

    -   [`"html"`{.docutils .literal .notranslate}]{.pre} for anything
        else

    Otherwise, if [`type`{.docutils .literal .notranslate}]{.pre} is
    set, the selector type will be forced and no detection will occur.

    [[xpath]{.pre}]{.sig-name .descname}[(]{.sig-paren}*[[query]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[[[str]{.pre}](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.12)"){.reference .external}]{.n}*, *[[namespaces]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[[[Optional]{.pre}](https://docs.python.org/3/library/typing.html#typing.Optional "(in Python v3.12)"){.reference .external}[[\[]{.pre}]{.p}[[Mapping]{.pre}](https://docs.python.org/3/library/typing.html#typing.Mapping "(in Python v3.12)"){.reference .external}[[\[]{.pre}]{.p}[[str]{.pre}](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.12)"){.reference .external}[[,]{.pre}]{.p}[ ]{.w}[[str]{.pre}](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.12)"){.reference .external}[[\]]{.pre}]{.p}[[\]]{.pre}]{.p}]{.n}[ ]{.w}[[=]{.pre}]{.o}[ ]{.w}[[None]{.pre}]{.default_value}*, *[[\*\*]{.pre}]{.o}[[kwargs]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[[[Any]{.pre}](https://docs.python.org/3/library/typing.html#typing.Any "(in Python v3.12)"){.reference .external}]{.n}*[)]{.sig-paren} [[→]{.sig-return-icon} [[SelectorList]{.pre}[[\[]{.pre}]{.p}[\_SelectorType]{.pre}[[\]]{.pre}]{.p}]{.sig-return-typehint}]{.sig-return}[[[\[source\]]{.pre}]{.viewcode-link}](_modules/parsel/selector.html#Selector.xpath){.reference .internal}[¶](#scrapy.selector.Selector.xpath "Permalink to this definition"){.headerlink}

    :   Find nodes matching the xpath [`query`{.docutils .literal
        .notranslate}]{.pre} and return the result as a
        [[`SelectorList`{.xref .py .py-class .docutils .literal
        .notranslate}]{.pre}](#scrapy.selector.SelectorList "scrapy.selector.SelectorList"){.reference
        .internal} instance with all elements flattened. List elements
        implement [[`Selector`{.xref .py .py-class .docutils .literal
        .notranslate}]{.pre}](#scrapy.selector.Selector "scrapy.selector.Selector"){.reference
        .internal} interface too.

        [`query`{.docutils .literal .notranslate}]{.pre} is a string
        containing the XPATH query to apply.

        [`namespaces`{.docutils .literal .notranslate}]{.pre} is an
        optional [`prefix:`{.docutils .literal
        .notranslate}]{.pre}` `{.docutils .literal
        .notranslate}[`namespace-uri`{.docutils .literal
        .notranslate}]{.pre} mapping (dict) for additional prefixes to
        those registered with [`register_namespace(prefix,`{.docutils
        .literal .notranslate}]{.pre}` `{.docutils .literal
        .notranslate}[`uri)`{.docutils .literal .notranslate}]{.pre}.
        Contrary to [`register_namespace()`{.docutils .literal
        .notranslate}]{.pre}, these prefixes are not saved for future
        calls.

        Any additional named arguments can be used to pass values for
        XPath variables in the XPath expression, e.g.:

        ::: {.highlight-python .notranslate}
        ::: highlight
            selector.xpath('//a[href=$url]', url="http://www.example.com")
        :::
        :::

        ::: {.admonition .note}
        Note

        For convenience, this method can be called as
        [`response.xpath()`{.docutils .literal .notranslate}]{.pre}
        :::

    [[css]{.pre}]{.sig-name .descname}[(]{.sig-paren}*[[query]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[[[str]{.pre}](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.12)"){.reference .external}]{.n}*[)]{.sig-paren} [[→]{.sig-return-icon} [[SelectorList]{.pre}[[\[]{.pre}]{.p}[\_SelectorType]{.pre}[[\]]{.pre}]{.p}]{.sig-return-typehint}]{.sig-return}[[[\[source\]]{.pre}]{.viewcode-link}](_modules/parsel/selector.html#Selector.css){.reference .internal}[¶](#scrapy.selector.Selector.css "Permalink to this definition"){.headerlink}

    :   Apply the given CSS selector and return a [[`SelectorList`{.xref
        .py .py-class .docutils .literal
        .notranslate}]{.pre}](#scrapy.selector.SelectorList "scrapy.selector.SelectorList"){.reference
        .internal} instance.

        [`query`{.docutils .literal .notranslate}]{.pre} is a string
        containing the CSS selector to apply.

        In the background, CSS queries are translated into XPath queries
        using
        [cssselect](https://pypi.python.org/pypi/cssselect/){.reference
        .external} library and run [`.xpath()`{.docutils .literal
        .notranslate}]{.pre} method.

        ::: {.admonition .note}
        Note

        For convenience, this method can be called as
        [`response.css()`{.docutils .literal .notranslate}]{.pre}
        :::

    [[get]{.pre}]{.sig-name .descname}[(]{.sig-paren}[)]{.sig-paren} [[→]{.sig-return-icon} [[[Any]{.pre}](https://docs.python.org/3/library/typing.html#typing.Any "(in Python v3.12)"){.reference .external}]{.sig-return-typehint}]{.sig-return}[[[\[source\]]{.pre}]{.viewcode-link}](_modules/parsel/selector.html#Selector.get){.reference .internal}[¶](#scrapy.selector.Selector.get "Permalink to this definition"){.headerlink}

    :   Serialize and return the matched nodes in a single string.
        Percent encoded content is unquoted.

        See also: [[extract() and extract_first()]{.std
        .std-ref}](#old-extraction-api){.hoverxref .tooltip .reference
        .internal}

    [[attrib]{.pre}]{.sig-name .descname}[¶](#scrapy.selector.Selector.attrib "Permalink to this definition"){.headerlink}

    :   Return the attributes dictionary for underlying element.

        See also: [[Selecting element attributes]{.std
        .std-ref}](#selecting-attributes){.hoverxref .tooltip .reference
        .internal}.

    [[re]{.pre}]{.sig-name .descname}[(]{.sig-paren}*[[regex]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[[[Union]{.pre}](https://docs.python.org/3/library/typing.html#typing.Union "(in Python v3.12)"){.reference .external}[[\[]{.pre}]{.p}[[str]{.pre}](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.12)"){.reference .external}[[,]{.pre}]{.p}[ ]{.w}[[Pattern]{.pre}](https://docs.python.org/3/library/typing.html#typing.Pattern "(in Python v3.12)"){.reference .external}[[\[]{.pre}]{.p}[[str]{.pre}](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.12)"){.reference .external}[[\]]{.pre}]{.p}[[\]]{.pre}]{.p}]{.n}*, *[[replace_entities]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[[[bool]{.pre}](https://docs.python.org/3/library/functions.html#bool "(in Python v3.12)"){.reference .external}]{.n}[ ]{.w}[[=]{.pre}]{.o}[ ]{.w}[[True]{.pre}]{.default_value}*[)]{.sig-paren} [[→]{.sig-return-icon} [[[List]{.pre}](https://docs.python.org/3/library/typing.html#typing.List "(in Python v3.12)"){.reference .external}[[\[]{.pre}]{.p}[[str]{.pre}](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.12)"){.reference .external}[[\]]{.pre}]{.p}]{.sig-return-typehint}]{.sig-return}[[[\[source\]]{.pre}]{.viewcode-link}](_modules/parsel/selector.html#Selector.re){.reference .internal}[¶](#scrapy.selector.Selector.re "Permalink to this definition"){.headerlink}

    :   Apply the given regex and return a list of strings with the
        matches.

        [`regex`{.docutils .literal .notranslate}]{.pre} can be either a
        compiled regular expression or a string which will be compiled
        to a regular expression using [`re.compile(regex)`{.docutils
        .literal .notranslate}]{.pre}.

        By default, character entity references are replaced by their
        corresponding character (except for [`&amp;`{.docutils .literal
        .notranslate}]{.pre} and [`&lt;`{.docutils .literal
        .notranslate}]{.pre}). Passing [`replace_entities`{.docutils
        .literal .notranslate}]{.pre} as [`False`{.docutils .literal
        .notranslate}]{.pre} switches off these replacements.

    [[re_first]{.pre}]{.sig-name .descname}[(]{.sig-paren}*[[regex]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[[[Union]{.pre}](https://docs.python.org/3/library/typing.html#typing.Union "(in Python v3.12)"){.reference .external}[[\[]{.pre}]{.p}[[str]{.pre}](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.12)"){.reference .external}[[,]{.pre}]{.p}[ ]{.w}[[Pattern]{.pre}](https://docs.python.org/3/library/typing.html#typing.Pattern "(in Python v3.12)"){.reference .external}[[\[]{.pre}]{.p}[[str]{.pre}](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.12)"){.reference .external}[[\]]{.pre}]{.p}[[\]]{.pre}]{.p}]{.n}*, *[[default]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[[[None]{.pre}](https://docs.python.org/3/library/constants.html#None "(in Python v3.12)"){.reference .external}]{.n}[ ]{.w}[[=]{.pre}]{.o}[ ]{.w}[[None]{.pre}]{.default_value}*, *[[replace_entities]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[[[bool]{.pre}](https://docs.python.org/3/library/functions.html#bool "(in Python v3.12)"){.reference .external}]{.n}[ ]{.w}[[=]{.pre}]{.o}[ ]{.w}[[True]{.pre}]{.default_value}*[)]{.sig-paren} [[→]{.sig-return-icon} [[[Optional]{.pre}](https://docs.python.org/3/library/typing.html#typing.Optional "(in Python v3.12)"){.reference .external}[[\[]{.pre}]{.p}[[str]{.pre}](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.12)"){.reference .external}[[\]]{.pre}]{.p}]{.sig-return-typehint}]{.sig-return}[[[\[source\]]{.pre}]{.viewcode-link}](_modules/parsel/selector.html#Selector.re_first){.reference .internal}[¶](#scrapy.selector.Selector.re_first "Permalink to this definition"){.headerlink}\
    [[re_first]{.pre}]{.sig-name .descname}[(]{.sig-paren}*[[regex]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[[[Union]{.pre}](https://docs.python.org/3/library/typing.html#typing.Union "(in Python v3.12)"){.reference .external}[[\[]{.pre}]{.p}[[str]{.pre}](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.12)"){.reference .external}[[,]{.pre}]{.p}[ ]{.w}[[Pattern]{.pre}](https://docs.python.org/3/library/typing.html#typing.Pattern "(in Python v3.12)"){.reference .external}[[\[]{.pre}]{.p}[[str]{.pre}](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.12)"){.reference .external}[[\]]{.pre}]{.p}[[\]]{.pre}]{.p}]{.n}*, *[[default]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[[[str]{.pre}](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.12)"){.reference .external}]{.n}*, *[[replace_entities]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[[[bool]{.pre}](https://docs.python.org/3/library/functions.html#bool "(in Python v3.12)"){.reference .external}]{.n}[ ]{.w}[[=]{.pre}]{.o}[ ]{.w}[[True]{.pre}]{.default_value}*[)]{.sig-paren} [[→]{.sig-return-icon} [[[str]{.pre}](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.12)"){.reference .external}]{.sig-return-typehint}]{.sig-return}

    :   Apply the given regex and return the first string which matches.
        If there is no match, return the default value
        ([`None`{.docutils .literal .notranslate}]{.pre} if the argument
        is not provided).

        By default, character entity references are replaced by their
        corresponding character (except for [`&amp;`{.docutils .literal
        .notranslate}]{.pre} and [`&lt;`{.docutils .literal
        .notranslate}]{.pre}). Passing [`replace_entities`{.docutils
        .literal .notranslate}]{.pre} as [`False`{.docutils .literal
        .notranslate}]{.pre} switches off these replacements.

    [[register_namespace]{.pre}]{.sig-name .descname}[(]{.sig-paren}*[[prefix]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[[[str]{.pre}](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.12)"){.reference .external}]{.n}*, *[[uri]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[[[str]{.pre}](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.12)"){.reference .external}]{.n}*[)]{.sig-paren} [[→]{.sig-return-icon} [[[None]{.pre}](https://docs.python.org/3/library/constants.html#None "(in Python v3.12)"){.reference .external}]{.sig-return-typehint}]{.sig-return}[[[\[source\]]{.pre}]{.viewcode-link}](_modules/parsel/selector.html#Selector.register_namespace){.reference .internal}[¶](#scrapy.selector.Selector.register_namespace "Permalink to this definition"){.headerlink}

    :   Register the given namespace to be used in this
        [[`Selector`{.xref .py .py-class .docutils .literal
        .notranslate}]{.pre}](#scrapy.selector.Selector "scrapy.selector.Selector"){.reference
        .internal}. Without registering namespaces you can't select or
        extract data from non-standard namespaces. See [[Selector
        examples on XML response]{.std
        .std-ref}](#selector-examples-xml){.hoverxref .tooltip
        .reference .internal}.

    [[remove_namespaces]{.pre}]{.sig-name .descname}[(]{.sig-paren}[)]{.sig-paren} [[→]{.sig-return-icon} [[[None]{.pre}](https://docs.python.org/3/library/constants.html#None "(in Python v3.12)"){.reference .external}]{.sig-return-typehint}]{.sig-return}[[[\[source\]]{.pre}]{.viewcode-link}](_modules/parsel/selector.html#Selector.remove_namespaces){.reference .internal}[¶](#scrapy.selector.Selector.remove_namespaces "Permalink to this definition"){.headerlink}

    :   Remove all namespaces, allowing to traverse the document using
        namespace-less xpaths. See [[Removing namespaces]{.std
        .std-ref}](#removing-namespaces){.hoverxref .tooltip .reference
        .internal}.

    [[\_\_bool\_\_]{.pre}]{.sig-name .descname}[(]{.sig-paren}[)]{.sig-paren} [[→]{.sig-return-icon} [[[bool]{.pre}](https://docs.python.org/3/library/functions.html#bool "(in Python v3.12)"){.reference .external}]{.sig-return-typehint}]{.sig-return}[[[\[source\]]{.pre}]{.viewcode-link}](_modules/parsel/selector.html#Selector.__bool__){.reference .internal}[¶](#scrapy.selector.Selector.__bool__ "Permalink to this definition"){.headerlink}

    :   Return [`True`{.docutils .literal .notranslate}]{.pre} if there
        is any real content selected or [`False`{.docutils .literal
        .notranslate}]{.pre} otherwise. In other words, the boolean
        value of a [[`Selector`{.xref .py .py-class .docutils .literal
        .notranslate}]{.pre}](#scrapy.selector.Selector "scrapy.selector.Selector"){.reference
        .internal} is given by the contents it selects.

    [[getall]{.pre}]{.sig-name .descname}[(]{.sig-paren}[)]{.sig-paren} [[→]{.sig-return-icon} [[[List]{.pre}](https://docs.python.org/3/library/typing.html#typing.List "(in Python v3.12)"){.reference .external}[[\[]{.pre}]{.p}[[str]{.pre}](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.12)"){.reference .external}[[\]]{.pre}]{.p}]{.sig-return-typehint}]{.sig-return}[[[\[source\]]{.pre}]{.viewcode-link}](_modules/parsel/selector.html#Selector.getall){.reference .internal}[¶](#scrapy.selector.Selector.getall "Permalink to this definition"){.headerlink}

    :   Serialize and return the matched node in a 1-element list of
        strings.

        This method is added to Selector for consistency; it is more
        useful with SelectorList. See also: [[extract() and
        extract_first()]{.std .std-ref}](#old-extraction-api){.hoverxref
        .tooltip .reference .internal}
:::

::: {#selectorlist-objects .section}
##### SelectorList objects[¶](#selectorlist-objects "Permalink to this heading"){.headerlink}

*[class]{.pre}[ ]{.w}*[[scrapy.selector.]{.pre}]{.sig-prename .descclassname}[[SelectorList]{.pre}]{.sig-name .descname}[(]{.sig-paren}*[[iterable]{.pre}]{.n}[[=]{.pre}]{.o}[[()]{.pre}]{.default_value}*, *[[/]{.pre}]{.o}*[)]{.sig-paren}[[[\[source\]]{.pre}]{.viewcode-link}](_modules/scrapy/selector/unified.html#SelectorList){.reference .internal}[¶](#scrapy.selector.SelectorList "Permalink to this definition"){.headerlink}

:   The [[`SelectorList`{.xref .py .py-class .docutils .literal
    .notranslate}]{.pre}](#scrapy.selector.SelectorList "scrapy.selector.SelectorList"){.reference
    .internal} class is a subclass of the builtin [`list`{.docutils
    .literal .notranslate}]{.pre} class, which provides a few additional
    methods.

    [[xpath]{.pre}]{.sig-name .descname}[(]{.sig-paren}*[[xpath]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[[[str]{.pre}](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.12)"){.reference .external}]{.n}*, *[[namespaces]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[[[Optional]{.pre}](https://docs.python.org/3/library/typing.html#typing.Optional "(in Python v3.12)"){.reference .external}[[\[]{.pre}]{.p}[[Mapping]{.pre}](https://docs.python.org/3/library/typing.html#typing.Mapping "(in Python v3.12)"){.reference .external}[[\[]{.pre}]{.p}[[str]{.pre}](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.12)"){.reference .external}[[,]{.pre}]{.p}[ ]{.w}[[str]{.pre}](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.12)"){.reference .external}[[\]]{.pre}]{.p}[[\]]{.pre}]{.p}]{.n}[ ]{.w}[[=]{.pre}]{.o}[ ]{.w}[[None]{.pre}]{.default_value}*, *[[\*\*]{.pre}]{.o}[[kwargs]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[[[Any]{.pre}](https://docs.python.org/3/library/typing.html#typing.Any "(in Python v3.12)"){.reference .external}]{.n}*[)]{.sig-paren} [[→]{.sig-return-icon} [[SelectorList]{.pre}[[\[]{.pre}]{.p}[\_SelectorType]{.pre}[[\]]{.pre}]{.p}]{.sig-return-typehint}]{.sig-return}[[[\[source\]]{.pre}]{.viewcode-link}](_modules/parsel/selector.html#SelectorList.xpath){.reference .internal}[¶](#scrapy.selector.SelectorList.xpath "Permalink to this definition"){.headerlink}

    :   Call the [`.xpath()`{.docutils .literal .notranslate}]{.pre}
        method for each element in this list and return their results
        flattened as another [[`SelectorList`{.xref .py .py-class
        .docutils .literal
        .notranslate}]{.pre}](#scrapy.selector.SelectorList "scrapy.selector.SelectorList"){.reference
        .internal}.

        [`xpath`{.docutils .literal .notranslate}]{.pre} is the same
        argument as the one in [[`Selector.xpath()`{.xref .py .py-meth
        .docutils .literal
        .notranslate}]{.pre}](#scrapy.selector.Selector.xpath "scrapy.selector.Selector.xpath"){.reference
        .internal}

        [`namespaces`{.docutils .literal .notranslate}]{.pre} is an
        optional [`prefix:`{.docutils .literal
        .notranslate}]{.pre}` `{.docutils .literal
        .notranslate}[`namespace-uri`{.docutils .literal
        .notranslate}]{.pre} mapping (dict) for additional prefixes to
        those registered with [`register_namespace(prefix,`{.docutils
        .literal .notranslate}]{.pre}` `{.docutils .literal
        .notranslate}[`uri)`{.docutils .literal .notranslate}]{.pre}.
        Contrary to [`register_namespace()`{.docutils .literal
        .notranslate}]{.pre}, these prefixes are not saved for future
        calls.

        Any additional named arguments can be used to pass values for
        XPath variables in the XPath expression, e.g.:

        ::: {.highlight-python .notranslate}
        ::: highlight
            selector.xpath('//a[href=$url]', url="http://www.example.com")
        :::
        :::

    [[css]{.pre}]{.sig-name .descname}[(]{.sig-paren}*[[query]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[[[str]{.pre}](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.12)"){.reference .external}]{.n}*[)]{.sig-paren} [[→]{.sig-return-icon} [[SelectorList]{.pre}[[\[]{.pre}]{.p}[\_SelectorType]{.pre}[[\]]{.pre}]{.p}]{.sig-return-typehint}]{.sig-return}[[[\[source\]]{.pre}]{.viewcode-link}](_modules/parsel/selector.html#SelectorList.css){.reference .internal}[¶](#scrapy.selector.SelectorList.css "Permalink to this definition"){.headerlink}

    :   Call the [`.css()`{.docutils .literal .notranslate}]{.pre}
        method for each element in this list and return their results
        flattened as another [[`SelectorList`{.xref .py .py-class
        .docutils .literal
        .notranslate}]{.pre}](#scrapy.selector.SelectorList "scrapy.selector.SelectorList"){.reference
        .internal}.

        [`query`{.docutils .literal .notranslate}]{.pre} is the same
        argument as the one in [[`Selector.css()`{.xref .py .py-meth
        .docutils .literal
        .notranslate}]{.pre}](#scrapy.selector.Selector.css "scrapy.selector.Selector.css"){.reference
        .internal}

    [[getall]{.pre}]{.sig-name .descname}[(]{.sig-paren}[)]{.sig-paren} [[→]{.sig-return-icon} [[[List]{.pre}](https://docs.python.org/3/library/typing.html#typing.List "(in Python v3.12)"){.reference .external}[[\[]{.pre}]{.p}[[str]{.pre}](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.12)"){.reference .external}[[\]]{.pre}]{.p}]{.sig-return-typehint}]{.sig-return}[[[\[source\]]{.pre}]{.viewcode-link}](_modules/parsel/selector.html#SelectorList.getall){.reference .internal}[¶](#scrapy.selector.SelectorList.getall "Permalink to this definition"){.headerlink}

    :   Call the [`.get()`{.docutils .literal .notranslate}]{.pre}
        method for each element is this list and return their results
        flattened, as a list of strings.

        See also: [[extract() and extract_first()]{.std
        .std-ref}](#old-extraction-api){.hoverxref .tooltip .reference
        .internal}

    [[get]{.pre}]{.sig-name .descname}[(]{.sig-paren}*[[default]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[[[None]{.pre}](https://docs.python.org/3/library/constants.html#None "(in Python v3.12)"){.reference .external}]{.n}[ ]{.w}[[=]{.pre}]{.o}[ ]{.w}[[None]{.pre}]{.default_value}*[)]{.sig-paren} [[→]{.sig-return-icon} [[[Optional]{.pre}](https://docs.python.org/3/library/typing.html#typing.Optional "(in Python v3.12)"){.reference .external}[[\[]{.pre}]{.p}[[str]{.pre}](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.12)"){.reference .external}[[\]]{.pre}]{.p}]{.sig-return-typehint}]{.sig-return}[[[\[source\]]{.pre}]{.viewcode-link}](_modules/parsel/selector.html#SelectorList.get){.reference .internal}[¶](#scrapy.selector.SelectorList.get "Permalink to this definition"){.headerlink}\
    [[get]{.pre}]{.sig-name .descname}[(]{.sig-paren}*[[default]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[[[str]{.pre}](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.12)"){.reference .external}]{.n}*[)]{.sig-paren} [[→]{.sig-return-icon} [[[str]{.pre}](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.12)"){.reference .external}]{.sig-return-typehint}]{.sig-return}

    :   Return the result of [`.get()`{.docutils .literal
        .notranslate}]{.pre} for the first element in this list. If the
        list is empty, return the default value.

        See also: [[extract() and extract_first()]{.std
        .std-ref}](#old-extraction-api){.hoverxref .tooltip .reference
        .internal}

    [[re]{.pre}]{.sig-name .descname}[(]{.sig-paren}*[[regex]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[[[Union]{.pre}](https://docs.python.org/3/library/typing.html#typing.Union "(in Python v3.12)"){.reference .external}[[\[]{.pre}]{.p}[[str]{.pre}](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.12)"){.reference .external}[[,]{.pre}]{.p}[ ]{.w}[[Pattern]{.pre}](https://docs.python.org/3/library/typing.html#typing.Pattern "(in Python v3.12)"){.reference .external}[[\[]{.pre}]{.p}[[str]{.pre}](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.12)"){.reference .external}[[\]]{.pre}]{.p}[[\]]{.pre}]{.p}]{.n}*, *[[replace_entities]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[[[bool]{.pre}](https://docs.python.org/3/library/functions.html#bool "(in Python v3.12)"){.reference .external}]{.n}[ ]{.w}[[=]{.pre}]{.o}[ ]{.w}[[True]{.pre}]{.default_value}*[)]{.sig-paren} [[→]{.sig-return-icon} [[[List]{.pre}](https://docs.python.org/3/library/typing.html#typing.List "(in Python v3.12)"){.reference .external}[[\[]{.pre}]{.p}[[str]{.pre}](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.12)"){.reference .external}[[\]]{.pre}]{.p}]{.sig-return-typehint}]{.sig-return}[[[\[source\]]{.pre}]{.viewcode-link}](_modules/parsel/selector.html#SelectorList.re){.reference .internal}[¶](#scrapy.selector.SelectorList.re "Permalink to this definition"){.headerlink}

    :   Call the [`.re()`{.docutils .literal .notranslate}]{.pre} method
        for each element in this list and return their results
        flattened, as a list of strings.

        By default, character entity references are replaced by their
        corresponding character (except for [`&amp;`{.docutils .literal
        .notranslate}]{.pre} and [`&lt;`{.docutils .literal
        .notranslate}]{.pre}. Passing [`replace_entities`{.docutils
        .literal .notranslate}]{.pre} as [`False`{.docutils .literal
        .notranslate}]{.pre} switches off these replacements.

    [[re_first]{.pre}]{.sig-name .descname}[(]{.sig-paren}*[[regex]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[[[Union]{.pre}](https://docs.python.org/3/library/typing.html#typing.Union "(in Python v3.12)"){.reference .external}[[\[]{.pre}]{.p}[[str]{.pre}](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.12)"){.reference .external}[[,]{.pre}]{.p}[ ]{.w}[[Pattern]{.pre}](https://docs.python.org/3/library/typing.html#typing.Pattern "(in Python v3.12)"){.reference .external}[[\[]{.pre}]{.p}[[str]{.pre}](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.12)"){.reference .external}[[\]]{.pre}]{.p}[[\]]{.pre}]{.p}]{.n}*, *[[default]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[[[None]{.pre}](https://docs.python.org/3/library/constants.html#None "(in Python v3.12)"){.reference .external}]{.n}[ ]{.w}[[=]{.pre}]{.o}[ ]{.w}[[None]{.pre}]{.default_value}*, *[[replace_entities]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[[[bool]{.pre}](https://docs.python.org/3/library/functions.html#bool "(in Python v3.12)"){.reference .external}]{.n}[ ]{.w}[[=]{.pre}]{.o}[ ]{.w}[[True]{.pre}]{.default_value}*[)]{.sig-paren} [[→]{.sig-return-icon} [[[Optional]{.pre}](https://docs.python.org/3/library/typing.html#typing.Optional "(in Python v3.12)"){.reference .external}[[\[]{.pre}]{.p}[[str]{.pre}](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.12)"){.reference .external}[[\]]{.pre}]{.p}]{.sig-return-typehint}]{.sig-return}[[[\[source\]]{.pre}]{.viewcode-link}](_modules/parsel/selector.html#SelectorList.re_first){.reference .internal}[¶](#scrapy.selector.SelectorList.re_first "Permalink to this definition"){.headerlink}\
    [[re_first]{.pre}]{.sig-name .descname}[(]{.sig-paren}*[[regex]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[[[Union]{.pre}](https://docs.python.org/3/library/typing.html#typing.Union "(in Python v3.12)"){.reference .external}[[\[]{.pre}]{.p}[[str]{.pre}](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.12)"){.reference .external}[[,]{.pre}]{.p}[ ]{.w}[[Pattern]{.pre}](https://docs.python.org/3/library/typing.html#typing.Pattern "(in Python v3.12)"){.reference .external}[[\[]{.pre}]{.p}[[str]{.pre}](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.12)"){.reference .external}[[\]]{.pre}]{.p}[[\]]{.pre}]{.p}]{.n}*, *[[default]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[[[str]{.pre}](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.12)"){.reference .external}]{.n}*, *[[replace_entities]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[[[bool]{.pre}](https://docs.python.org/3/library/functions.html#bool "(in Python v3.12)"){.reference .external}]{.n}[ ]{.w}[[=]{.pre}]{.o}[ ]{.w}[[True]{.pre}]{.default_value}*[)]{.sig-paren} [[→]{.sig-return-icon} [[[str]{.pre}](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.12)"){.reference .external}]{.sig-return-typehint}]{.sig-return}

    :   Call the [`.re()`{.docutils .literal .notranslate}]{.pre} method
        for the first element in this list and return the result in an
        string. If the list is empty or the regex doesn't match
        anything, return the default value ([`None`{.docutils .literal
        .notranslate}]{.pre} if the argument is not provided).

        By default, character entity references are replaced by their
        corresponding character (except for [`&amp;`{.docutils .literal
        .notranslate}]{.pre} and [`&lt;`{.docutils .literal
        .notranslate}]{.pre}. Passing [`replace_entities`{.docutils
        .literal .notranslate}]{.pre} as [`False`{.docutils .literal
        .notranslate}]{.pre} switches off these replacements.

    [[attrib]{.pre}]{.sig-name .descname}[¶](#scrapy.selector.SelectorList.attrib "Permalink to this definition"){.headerlink}

    :   Return the attributes dictionary for the first element. If the
        list is empty, return an empty dict.

        See also: [[Selecting element attributes]{.std
        .std-ref}](#selecting-attributes){.hoverxref .tooltip .reference
        .internal}.
:::
:::

::: {#examples .section}
[]{#selector-examples}

#### Examples[¶](#examples "Permalink to this heading"){.headerlink}

::: {#selector-examples-on-html-response .section}
[]{#selector-examples-html}

##### Selector examples on HTML response[¶](#selector-examples-on-html-response "Permalink to this heading"){.headerlink}

Here are some [[`Selector`{.xref .py .py-class .docutils .literal
.notranslate}]{.pre}](#scrapy.selector.Selector "scrapy.selector.Selector"){.reference
.internal} examples to illustrate several concepts. In all cases, we
assume there is already a [[`Selector`{.xref .py .py-class .docutils
.literal
.notranslate}]{.pre}](#scrapy.selector.Selector "scrapy.selector.Selector"){.reference
.internal} instantiated with a [[`HtmlResponse`{.xref .py .py-class
.docutils .literal
.notranslate}]{.pre}](index.html#scrapy.http.HtmlResponse "scrapy.http.HtmlResponse"){.reference
.internal} object like this:

::: {.highlight-python .notranslate}
::: highlight
    sel = Selector(html_response)
:::
:::

1.  Select all [`<h1>`{.docutils .literal .notranslate}]{.pre} elements
    from an HTML response body, returning a list of [[`Selector`{.xref
    .py .py-class .docutils .literal
    .notranslate}]{.pre}](#scrapy.selector.Selector "scrapy.selector.Selector"){.reference
    .internal} objects (i.e. a [[`SelectorList`{.xref .py .py-class
    .docutils .literal
    .notranslate}]{.pre}](#scrapy.selector.SelectorList "scrapy.selector.SelectorList"){.reference
    .internal} object):

    ::: {.highlight-python .notranslate}
    ::: highlight
        sel.xpath("//h1")
    :::
    :::

2.  Extract the text of all [`<h1>`{.docutils .literal
    .notranslate}]{.pre} elements from an HTML response body, returning
    a list of strings:

    ::: {.highlight-python .notranslate}
    ::: highlight
        sel.xpath("//h1").getall()  # this includes the h1 tag
        sel.xpath("//h1/text()").getall()  # this excludes the h1 tag
    :::
    :::

3.  Iterate over all [`<p>`{.docutils .literal .notranslate}]{.pre} tags
    and print their class attribute:

    ::: {.highlight-python .notranslate}
    ::: highlight
        for node in sel.xpath("//p"):
            print(node.attrib["class"])
    :::
    :::
:::

::: {#selector-examples-on-xml-response .section}
[]{#selector-examples-xml}

##### Selector examples on XML response[¶](#selector-examples-on-xml-response "Permalink to this heading"){.headerlink}

Here are some examples to illustrate concepts for [[`Selector`{.xref .py
.py-class .docutils .literal
.notranslate}]{.pre}](#scrapy.selector.Selector "scrapy.selector.Selector"){.reference
.internal} objects instantiated with an [[`XmlResponse`{.xref .py
.py-class .docutils .literal
.notranslate}]{.pre}](index.html#scrapy.http.XmlResponse "scrapy.http.XmlResponse"){.reference
.internal} object:

::: {.highlight-python .notranslate}
::: highlight
    sel = Selector(xml_response)
:::
:::

1.  Select all [`<product>`{.docutils .literal .notranslate}]{.pre}
    elements from an XML response body, returning a list of
    [[`Selector`{.xref .py .py-class .docutils .literal
    .notranslate}]{.pre}](#scrapy.selector.Selector "scrapy.selector.Selector"){.reference
    .internal} objects (i.e. a [[`SelectorList`{.xref .py .py-class
    .docutils .literal
    .notranslate}]{.pre}](#scrapy.selector.SelectorList "scrapy.selector.SelectorList"){.reference
    .internal} object):

    ::: {.highlight-python .notranslate}
    ::: highlight
        sel.xpath("//product")
    :::
    :::

2.  Extract all prices from a [Google Base XML
    feed](https://support.google.com/merchants/answer/160589?hl=en&ref_topic=2473799){.reference
    .external} which requires registering a namespace:

    ::: {.highlight-python .notranslate}
    ::: highlight
        sel.register_namespace("g", "http://base.google.com/ns/1.0")
        sel.xpath("//g:price").getall()
    :::
    :::
:::
:::
:::

[]{#document-topics/items}

::: {#module-scrapy.item .section}
[]{#items}[]{#topics-items}

### Items[¶](#module-scrapy.item "Permalink to this heading"){.headerlink}

The main goal in scraping is to extract structured data from
unstructured sources, typically, web pages. [[Spiders]{.std
.std-ref}](index.html#topics-spiders){.hoverxref .tooltip .reference
.internal} may return the extracted data as items, Python objects that
define key-value pairs.

Scrapy supports [[multiple types of items]{.std
.std-ref}](#item-types){.hoverxref .tooltip .reference .internal}. When
you create an item, you may use whichever type of item you want. When
you write code that receives an item, your code should [[work for any
item type]{.std .std-ref}](#supporting-item-types){.hoverxref .tooltip
.reference .internal}.

::: {#item-types .section}
[]{#id1}

#### Item Types[¶](#item-types "Permalink to this heading"){.headerlink}

Scrapy supports the following types of items, via the
[itemadapter](https://github.com/scrapy/itemadapter){.reference
.external} library: [[dictionaries]{.std
.std-ref}](#dict-items){.hoverxref .tooltip .reference .internal},
[[Item objects]{.std .std-ref}](#item-objects){.hoverxref .tooltip
.reference .internal}, [[dataclass objects]{.std
.std-ref}](#dataclass-items){.hoverxref .tooltip .reference .internal},
and [[attrs objects]{.std .std-ref}](#attrs-items){.hoverxref .tooltip
.reference .internal}.

::: {#dictionaries .section}
[]{#dict-items}

##### Dictionaries[¶](#dictionaries "Permalink to this heading"){.headerlink}

As an item type, [[`dict`{.xref .py .py-class .docutils .literal
.notranslate}]{.pre}](https://docs.python.org/3/library/stdtypes.html#dict "(in Python v3.12)"){.reference
.external} is convenient and familiar.
:::

::: {#item-objects .section}
[]{#id2}

##### Item objects[¶](#item-objects "Permalink to this heading"){.headerlink}

[`Item`{.xref .py .py-class .docutils .literal .notranslate}]{.pre}
provides a [[`dict`{.xref .py .py-class .docutils .literal
.notranslate}]{.pre}](https://docs.python.org/3/library/stdtypes.html#dict "(in Python v3.12)"){.reference
.external}-like API plus additional features that make it the most
feature-complete item type:

*[class]{.pre}[ ]{.w}*[[scrapy.item.]{.pre}]{.sig-prename .descclassname}[[Item]{.pre}]{.sig-name .descname}[(]{.sig-paren}[\[]{.optional}*[[arg]{.pre}]{.n}*[\]]{.optional}[)]{.sig-paren}[¶](#scrapy.item.scrapy.item.Item "Permalink to this definition"){.headerlink}

:   

```{=html}
<!-- -->
```

*[class]{.pre}[ ]{.w}*[[scrapy.]{.pre}]{.sig-prename .descclassname}[[Item]{.pre}]{.sig-name .descname}[(]{.sig-paren}[\[]{.optional}*[[arg]{.pre}]{.n}*[\]]{.optional}[)]{.sig-paren}[¶](#scrapy.item.scrapy.Item "Permalink to this definition"){.headerlink}

:   [`Item`{.xref .py .py-class .docutils .literal .notranslate}]{.pre}
    objects replicate the standard [[`dict`{.xref .py .py-class
    .docutils .literal
    .notranslate}]{.pre}](https://docs.python.org/3/library/stdtypes.html#dict "(in Python v3.12)"){.reference
    .external} API, including its [`__init__`{.docutils .literal
    .notranslate}]{.pre} method.

    [`Item`{.xref .py .py-class .docutils .literal .notranslate}]{.pre}
    allows defining field names, so that:

    -   [[`KeyError`{.xref .py .py-class .docutils .literal
        .notranslate}]{.pre}](https://docs.python.org/3/library/exceptions.html#KeyError "(in Python v3.12)"){.reference
        .external} is raised when using undefined field names (i.e.
        prevents typos going unnoticed)

    -   [[Item exporters]{.std
        .std-ref}](index.html#topics-exporters){.hoverxref .tooltip
        .reference .internal} can export all fields by default even if
        the first scraped object does not have values for all of them

    [`Item`{.xref .py .py-class .docutils .literal .notranslate}]{.pre}
    also allows defining field metadata, which can be used to
    [[customize serialization]{.std
    .std-ref}](index.html#topics-exporters-field-serialization){.hoverxref
    .tooltip .reference .internal}.

    [`trackref`{.xref .py .py-mod .docutils .literal
    .notranslate}]{.pre} tracks [`Item`{.xref .py .py-class .docutils
    .literal .notranslate}]{.pre} objects to help find memory leaks (see
    [[Debugging memory leaks with trackref]{.std
    .std-ref}](index.html#topics-leaks-trackrefs){.hoverxref .tooltip
    .reference .internal}).

    [`Item`{.xref .py .py-class .docutils .literal .notranslate}]{.pre}
    objects also provide the following additional API members:

    [[Item.]{.pre}]{.sig-prename .descclassname}[[copy]{.pre}]{.sig-name .descname}[(]{.sig-paren}[)]{.sig-paren}[¶](#scrapy.scrapy.Item.Item.copy "Permalink to this definition"){.headerlink}

    :   

    [[Item.]{.pre}]{.sig-prename .descclassname}[[deepcopy]{.pre}]{.sig-name .descname}[(]{.sig-paren}[)]{.sig-paren}[¶](#scrapy.scrapy.Item.Item.deepcopy "Permalink to this definition"){.headerlink}

    :   Return a [[`deepcopy()`{.xref .py .py-func .docutils .literal
        .notranslate}]{.pre}](https://docs.python.org/3/library/copy.html#copy.deepcopy "(in Python v3.12)"){.reference
        .external} of this item.

    [[fields]{.pre}]{.sig-name .descname}[¶](#scrapy.item.scrapy.Item.fields "Permalink to this definition"){.headerlink}

    :   A dictionary containing *all declared fields* for this Item, not
        only those populated. The keys are the field names and the
        values are the [`Field`{.xref .py .py-class .docutils .literal
        .notranslate}]{.pre} objects used in the [[Item
        declaration]{.std .std-ref}](#topics-items-declaring){.hoverxref
        .tooltip .reference .internal}.

Example:

::: {.highlight-python .notranslate}
::: highlight
    from scrapy.item import Item, Field


    class CustomItem(Item):
        one_field = Field()
        another_field = Field()
:::
:::
:::

::: {#dataclass-objects .section}
[]{#dataclass-items}

##### Dataclass objects[¶](#dataclass-objects "Permalink to this heading"){.headerlink}

::: versionadded
[New in version 2.2.]{.versionmodified .added}
:::

[[`dataclass()`{.xref .py .py-func .docutils .literal
.notranslate}]{.pre}](https://docs.python.org/3/library/dataclasses.html#dataclasses.dataclass "(in Python v3.12)"){.reference
.external} allows defining item classes with field names, so that [[item
exporters]{.std .std-ref}](index.html#topics-exporters){.hoverxref
.tooltip .reference .internal} can export all fields by default even if
the first scraped object does not have values for all of them.

Additionally, [`dataclass`{.docutils .literal .notranslate}]{.pre} items
also allow to:

-   define the type and default value of each defined field.

-   define custom field metadata through [[`dataclasses.field()`{.xref
    .py .py-func .docutils .literal
    .notranslate}]{.pre}](https://docs.python.org/3/library/dataclasses.html#dataclasses.field "(in Python v3.12)"){.reference
    .external}, which can be used to [[customize serialization]{.std
    .std-ref}](index.html#topics-exporters-field-serialization){.hoverxref
    .tooltip .reference .internal}.

Example:

::: {.highlight-python .notranslate}
::: highlight
    from dataclasses import dataclass


    @dataclass
    class CustomItem:
        one_field: str
        another_field: int
:::
:::

::: {.admonition .note}
Note

Field types are not enforced at run time.
:::
:::

::: {#attr-s-objects .section}
[]{#attrs-items}

##### attr.s objects[¶](#attr-s-objects "Permalink to this heading"){.headerlink}

::: versionadded
[New in version 2.2.]{.versionmodified .added}
:::

[[`attr.s()`{.xref .py .py-func .docutils .literal
.notranslate}]{.pre}](https://www.attrs.org/en/stable/api-attr.html#attr.s "(in attrs v23.1)"){.reference
.external} allows defining item classes with field names, so that [[item
exporters]{.std .std-ref}](index.html#topics-exporters){.hoverxref
.tooltip .reference .internal} can export all fields by default even if
the first scraped object does not have values for all of them.

Additionally, [`attr.s`{.docutils .literal .notranslate}]{.pre} items
also allow to:

-   define the type and default value of each defined field.

-   define custom field [[metadata]{.xref .std
    .std-ref}](https://www.attrs.org/en/stable/examples.html#metadata "(in attrs v23.1)"){.reference
    .external}, which can be used to [[customize serialization]{.std
    .std-ref}](index.html#topics-exporters-field-serialization){.hoverxref
    .tooltip .reference .internal}.

In order to use this type, the [[attrs package]{.xref .std
.std-doc}](https://www.attrs.org/en/stable/index.html "(in attrs v23.1)"){.reference
.external} needs to be installed.

Example:

::: {.highlight-python .notranslate}
::: highlight
    import attr


    @attr.s
    class CustomItem:
        one_field = attr.ib()
        another_field = attr.ib()
:::
:::
:::
:::

::: {#working-with-item-objects .section}
#### Working with Item objects[¶](#working-with-item-objects "Permalink to this heading"){.headerlink}

::: {#declaring-item-subclasses .section}
[]{#topics-items-declaring}

##### Declaring Item subclasses[¶](#declaring-item-subclasses "Permalink to this heading"){.headerlink}

Item subclasses are declared using a simple class definition syntax and
[`Field`{.xref .py .py-class .docutils .literal .notranslate}]{.pre}
objects. Here is an example:

::: {.highlight-python .notranslate}
::: highlight
    import scrapy


    class Product(scrapy.Item):
        name = scrapy.Field()
        price = scrapy.Field()
        stock = scrapy.Field()
        tags = scrapy.Field()
        last_updated = scrapy.Field(serializer=str)
:::
:::

::: {.admonition .note}
Note

Those familiar with [Django](https://www.djangoproject.com/){.reference
.external} will notice that Scrapy Items are declared similar to [Django
Models](https://docs.djangoproject.com/en/dev/topics/db/models/){.reference
.external}, except that Scrapy Items are much simpler as there is no
concept of different field types.
:::
:::

::: {#declaring-fields .section}
[]{#topics-items-fields}

##### Declaring fields[¶](#declaring-fields "Permalink to this heading"){.headerlink}

[`Field`{.xref .py .py-class .docutils .literal .notranslate}]{.pre}
objects are used to specify metadata for each field. For example, the
serializer function for the [`last_updated`{.docutils .literal
.notranslate}]{.pre} field illustrated in the example above.

You can specify any kind of metadata for each field. There is no
restriction on the values accepted by [`Field`{.xref .py .py-class
.docutils .literal .notranslate}]{.pre} objects. For this same reason,
there is no reference list of all available metadata keys. Each key
defined in [`Field`{.xref .py .py-class .docutils .literal
.notranslate}]{.pre} objects could be used by a different component, and
only those components know about it. You can also define and use any
other [`Field`{.xref .py .py-class .docutils .literal
.notranslate}]{.pre} key in your project too, for your own needs. The
main goal of [`Field`{.xref .py .py-class .docutils .literal
.notranslate}]{.pre} objects is to provide a way to define all field
metadata in one place. Typically, those components whose behaviour
depends on each field use certain field keys to configure that
behaviour. You must refer to their documentation to see which metadata
keys are used by each component.

It's important to note that the [`Field`{.xref .py .py-class .docutils
.literal .notranslate}]{.pre} objects used to declare the item do not
stay assigned as class attributes. Instead, they can be accessed through
the [`Item.fields`{.xref .py .py-attr .docutils .literal
.notranslate}]{.pre} attribute.

*[class]{.pre}[ ]{.w}*[[scrapy.item.]{.pre}]{.sig-prename .descclassname}[[Field]{.pre}]{.sig-name .descname}[(]{.sig-paren}[\[]{.optional}*[[arg]{.pre}]{.n}*[\]]{.optional}[)]{.sig-paren}[¶](#scrapy.item.scrapy.item.Field "Permalink to this definition"){.headerlink}

:   

```{=html}
<!-- -->
```

*[class]{.pre}[ ]{.w}*[[scrapy.]{.pre}]{.sig-prename .descclassname}[[Field]{.pre}]{.sig-name .descname}[(]{.sig-paren}[\[]{.optional}*[[arg]{.pre}]{.n}*[\]]{.optional}[)]{.sig-paren}[¶](#scrapy.item.scrapy.Field "Permalink to this definition"){.headerlink}

:   The [`Field`{.xref .py .py-class .docutils .literal
    .notranslate}]{.pre} class is just an alias to the built-in
    [[`dict`{.xref .py .py-class .docutils .literal
    .notranslate}]{.pre}](https://docs.python.org/3/library/stdtypes.html#dict "(in Python v3.12)"){.reference
    .external} class and doesn't provide any extra functionality or
    attributes. In other words, [`Field`{.xref .py .py-class .docutils
    .literal .notranslate}]{.pre} objects are plain-old Python dicts. A
    separate class is used to support the [[item declaration
    syntax]{.std .std-ref}](#topics-items-declaring){.hoverxref .tooltip
    .reference .internal} based on class attributes.

::: {.admonition .note}
Note

Field metadata can also be declared for [`dataclass`{.docutils .literal
.notranslate}]{.pre} and [`attrs`{.docutils .literal
.notranslate}]{.pre} items. Please refer to the documentation for
[dataclasses.field](https://docs.python.org/3/library/dataclasses.html#dataclasses.field){.reference
.external} and
[attr.ib](https://www.attrs.org/en/stable/api.html#attr.ib){.reference
.external} for additional information.
:::
:::

::: {#id3 .section}
##### Working with Item objects[¶](#id3 "Permalink to this heading"){.headerlink}

Here are some examples of common tasks performed with items, using the
[`Product`{.docutils .literal .notranslate}]{.pre} item [[declared
above]{.std .std-ref}](#topics-items-declaring){.hoverxref .tooltip
.reference .internal}. You will notice the API is very similar to the
[[`dict`{.xref .py .py-class .docutils .literal
.notranslate}]{.pre}](https://docs.python.org/3/library/stdtypes.html#dict "(in Python v3.12)"){.reference
.external} API.

::: {#creating-items .section}
###### Creating items[¶](#creating-items "Permalink to this heading"){.headerlink}

::: {.highlight-pycon .notranslate}
::: highlight
    >>> product = Product(name="Desktop PC", price=1000)
    >>> print(product)
    Product(name='Desktop PC', price=1000)
:::
:::
:::

::: {#getting-field-values .section}
###### Getting field values[¶](#getting-field-values "Permalink to this heading"){.headerlink}

::: {.highlight-pycon .notranslate}
::: highlight
    >>> product["name"]
    Desktop PC
    >>> product.get("name")
    Desktop PC

    >>> product["price"]
    1000

    >>> product["last_updated"]
    Traceback (most recent call last):
        ...
    KeyError: 'last_updated'

    >>> product.get("last_updated", "not set")
    not set

    >>> product["lala"]  # getting unknown field
    Traceback (most recent call last):
        ...
    KeyError: 'lala'

    >>> product.get("lala", "unknown field")
    'unknown field'

    >>> "name" in product  # is name field populated?
    True

    >>> "last_updated" in product  # is last_updated populated?
    False

    >>> "last_updated" in product.fields  # is last_updated a declared field?
    True

    >>> "lala" in product.fields  # is lala a declared field?
    False
:::
:::
:::

::: {#setting-field-values .section}
###### Setting field values[¶](#setting-field-values "Permalink to this heading"){.headerlink}

::: {.highlight-pycon .notranslate}
::: highlight
    >>> product["last_updated"] = "today"
    >>> product["last_updated"]
    today

    >>> product["lala"] = "test"  # setting unknown field
    Traceback (most recent call last):
        ...
    KeyError: 'Product does not support field: lala'
:::
:::
:::

::: {#accessing-all-populated-values .section}
###### Accessing all populated values[¶](#accessing-all-populated-values "Permalink to this heading"){.headerlink}

To access all populated values, just use the typical [[`dict`{.xref .py
.py-class .docutils .literal
.notranslate}]{.pre}](https://docs.python.org/3/library/stdtypes.html#dict "(in Python v3.12)"){.reference
.external} API:

::: {.highlight-pycon .notranslate}
::: highlight
    >>> product.keys()
    ['price', 'name']

    >>> product.items()
    [('price', 1000), ('name', 'Desktop PC')]
:::
:::
:::

::: {#copying-items .section}
[]{#id4}

###### Copying items[¶](#copying-items "Permalink to this heading"){.headerlink}

To copy an item, you must first decide whether you want a shallow copy
or a deep copy.

If your item contains [[mutable]{.xref .std
.std-term}](https://docs.python.org/3/glossary.html#term-mutable "(in Python v3.12)"){.reference
.external} values like lists or dictionaries, a shallow copy will keep
references to the same mutable values across all different copies.

For example, if you have an item with a list of tags, and you create a
shallow copy of that item, both the original item and the copy have the
same list of tags. Adding a tag to the list of one of the items will add
the tag to the other item as well.

If that is not the desired behavior, use a deep copy instead.

See [[`copy`{.xref .py .py-mod .docutils .literal
.notranslate}]{.pre}](https://docs.python.org/3/library/copy.html#module-copy "(in Python v3.12)"){.reference
.external} for more information.

To create a shallow copy of an item, you can either call [`copy()`{.xref
.py .py-meth .docutils .literal .notranslate}]{.pre} on an existing item
([`product2`{.docutils .literal .notranslate}]{.pre}` `{.docutils
.literal .notranslate}[`=`{.docutils .literal
.notranslate}]{.pre}` `{.docutils .literal
.notranslate}[`product.copy()`{.docutils .literal .notranslate}]{.pre})
or instantiate your item class from an existing item
([`product2`{.docutils .literal .notranslate}]{.pre}` `{.docutils
.literal .notranslate}[`=`{.docutils .literal
.notranslate}]{.pre}` `{.docutils .literal
.notranslate}[`Product(product)`{.docutils .literal
.notranslate}]{.pre}).

To create a deep copy, call [`deepcopy()`{.xref .py .py-meth .docutils
.literal .notranslate}]{.pre} instead ([`product2`{.docutils .literal
.notranslate}]{.pre}` `{.docutils .literal .notranslate}[`=`{.docutils
.literal .notranslate}]{.pre}` `{.docutils .literal
.notranslate}[`product.deepcopy()`{.docutils .literal
.notranslate}]{.pre}).
:::

::: {#other-common-tasks .section}
###### Other common tasks[¶](#other-common-tasks "Permalink to this heading"){.headerlink}

Creating dicts from items:

::: {.highlight-pycon .notranslate}
::: highlight
    >>> dict(product)  # create a dict from all populated values
    {'price': 1000, 'name': 'Desktop PC'}

    Creating items from dicts:

    >>> Product({"name": "Laptop PC", "price": 1500})
    Product(price=1500, name='Laptop PC')

    >>> Product({"name": "Laptop PC", "lala": 1500})  # warning: unknown field in dict
    Traceback (most recent call last):
        ...
    KeyError: 'Product does not support field: lala'
:::
:::
:::
:::

::: {#extending-item-subclasses .section}
##### Extending Item subclasses[¶](#extending-item-subclasses "Permalink to this heading"){.headerlink}

You can extend Items (to add more fields or to change some metadata for
some fields) by declaring a subclass of your original Item.

For example:

::: {.highlight-python .notranslate}
::: highlight
    class DiscountedProduct(Product):
        discount_percent = scrapy.Field(serializer=str)
        discount_expiration_date = scrapy.Field()
:::
:::

You can also extend field metadata by using the previous field metadata
and appending more values, or changing existing values, like this:

::: {.highlight-python .notranslate}
::: highlight
    class SpecificProduct(Product):
        name = scrapy.Field(Product.fields["name"], serializer=my_serializer)
:::
:::

That adds (or replaces) the [`serializer`{.docutils .literal
.notranslate}]{.pre} metadata key for the [`name`{.docutils .literal
.notranslate}]{.pre} field, keeping all the previously existing metadata
values.
:::
:::

::: {#supporting-all-item-types .section}
[]{#supporting-item-types}

#### Supporting All Item Types[¶](#supporting-all-item-types "Permalink to this heading"){.headerlink}

In code that receives an item, such as methods of [[item pipelines]{.std
.std-ref}](index.html#topics-item-pipeline){.hoverxref .tooltip
.reference .internal} or [[spider middlewares]{.std
.std-ref}](index.html#topics-spider-middleware){.hoverxref .tooltip
.reference .internal}, it is a good practice to use the
[[`ItemAdapter`{.xref .py .py-class .docutils .literal
.notranslate}]{.pre}](#itemadapter.ItemAdapter "itemadapter.ItemAdapter"){.reference
.internal} class and the [[`is_item()`{.xref .py .py-func .docutils
.literal
.notranslate}]{.pre}](#itemadapter.is_item "itemadapter.is_item"){.reference
.internal} function to write code that works for any [[supported item
type]{.std .std-ref}](#item-types){.hoverxref .tooltip .reference
.internal}:

*[class]{.pre}[ ]{.w}*[[itemadapter.]{.pre}]{.sig-prename .descclassname}[[ItemAdapter]{.pre}]{.sig-name .descname}[(]{.sig-paren}*[[item]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[[[Any]{.pre}](https://docs.python.org/3/library/typing.html#typing.Any "(in Python v3.12)"){.reference .external}]{.n}*[)]{.sig-paren}[[[\[source\]]{.pre}]{.viewcode-link}](_modules/itemadapter/adapter.html#ItemAdapter){.reference .internal}[¶](#itemadapter.ItemAdapter "Permalink to this definition"){.headerlink}

:   Wrapper class to interact with data container objects. It provides a
    common interface to extract and set data without having to take the
    object's type into account.

```{=html}
<!-- -->
```

[[itemadapter.]{.pre}]{.sig-prename .descclassname}[[is_item]{.pre}]{.sig-name .descname}[(]{.sig-paren}*[[obj]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[[[Any]{.pre}](https://docs.python.org/3/library/typing.html#typing.Any "(in Python v3.12)"){.reference .external}]{.n}*[)]{.sig-paren} [[→]{.sig-return-icon} [[[bool]{.pre}](https://docs.python.org/3/library/functions.html#bool "(in Python v3.12)"){.reference .external}]{.sig-return-typehint}]{.sig-return}[[[\[source\]]{.pre}]{.viewcode-link}](_modules/itemadapter/utils.html#is_item){.reference .internal}[¶](#itemadapter.is_item "Permalink to this definition"){.headerlink}

:   Return True if the given object belongs to one of the supported
    types, False otherwise.

    Alias for ItemAdapter.is_item
:::

::: {#other-classes-related-to-items .section}
#### Other classes related to items[¶](#other-classes-related-to-items "Permalink to this heading"){.headerlink}

*[class]{.pre}[ ]{.w}*[[scrapy.item.]{.pre}]{.sig-prename .descclassname}[[ItemMeta]{.pre}]{.sig-name .descname}[(]{.sig-paren}*[[class_name]{.pre}]{.n}*, *[[bases]{.pre}]{.n}*, *[[attrs]{.pre}]{.n}*[)]{.sig-paren}[[[\[source\]]{.pre}]{.viewcode-link}](_modules/scrapy/item.html#ItemMeta){.reference .internal}[¶](#scrapy.item.ItemMeta "Permalink to this definition"){.headerlink}

:   [Metaclass](https://realpython.com/python-metaclasses){.reference
    .external} of [`Item`{.xref .py .py-class .docutils .literal
    .notranslate}]{.pre} that handles field definitions.
:::
:::

[]{#document-topics/loaders}

::: {#module-scrapy.loader .section}
[]{#item-loaders}[]{#topics-loaders}

### Item Loaders[¶](#module-scrapy.loader "Permalink to this heading"){.headerlink}

Item Loaders provide a convenient mechanism for populating scraped
[[items]{.std .std-ref}](index.html#topics-items){.hoverxref .tooltip
.reference .internal}. Even though items can be populated directly, Item
Loaders provide a much more convenient API for populating them from a
scraping process, by automating some common tasks like parsing the raw
extracted data before assigning it.

In other words, [[items]{.std
.std-ref}](index.html#topics-items){.hoverxref .tooltip .reference
.internal} provide the *container* of scraped data, while Item Loaders
provide the mechanism for *populating* that container.

Item Loaders are designed to provide a flexible, efficient and easy
mechanism for extending and overriding different field parsing rules,
either by spider, or by source format (HTML, XML, etc) without becoming
a nightmare to maintain.

::: {.admonition .note}
Note

Item Loaders are an extension of the
[itemloaders](https://itemloaders.readthedocs.io/en/latest/){.reference
.external} library that make it easier to work with Scrapy by adding
support for [[responses]{.std
.std-ref}](index.html#topics-request-response){.hoverxref .tooltip
.reference .internal}.
:::

::: {#using-item-loaders-to-populate-items .section}
#### Using Item Loaders to populate items[¶](#using-item-loaders-to-populate-items "Permalink to this heading"){.headerlink}

To use an Item Loader, you must first instantiate it. You can either
instantiate it with an [[item object]{.std
.std-ref}](index.html#topics-items){.hoverxref .tooltip .reference
.internal} or without one, in which case an [[item object]{.std
.std-ref}](index.html#topics-items){.hoverxref .tooltip .reference
.internal} is automatically created in the Item Loader
[`__init__`{.docutils .literal .notranslate}]{.pre} method using the
[[item]{.std .std-ref}](index.html#topics-items){.hoverxref .tooltip
.reference .internal} class specified in the
[[`ItemLoader.default_item_class`{.xref .py .py-attr .docutils .literal
.notranslate}]{.pre}](#scrapy.loader.ItemLoader.default_item_class "scrapy.loader.ItemLoader.default_item_class"){.reference
.internal} attribute.

Then, you start collecting values into the Item Loader, typically using
[[Selectors]{.std .std-ref}](index.html#topics-selectors){.hoverxref
.tooltip .reference .internal}. You can add more than one value to the
same item field; the Item Loader will know how to "join" those values
later using a proper processing function.

::: {.admonition .note}
Note

Collected data is internally stored as lists, allowing to add several
values to the same field. If an [`item`{.docutils .literal
.notranslate}]{.pre} argument is passed when creating a loader, each of
the item's values will be stored as-is if it's already an iterable, or
wrapped with a list if it's a single value.
:::

Here is a typical Item Loader usage in a [[Spider]{.std
.std-ref}](index.html#topics-spiders){.hoverxref .tooltip .reference
.internal}, using the [[Product item]{.std
.std-ref}](index.html#topics-items-declaring){.hoverxref .tooltip
.reference .internal} declared in the [[Items chapter]{.std
.std-ref}](index.html#topics-items){.hoverxref .tooltip .reference
.internal}:

::: {.highlight-python .notranslate}
::: highlight
    from scrapy.loader import ItemLoader
    from myproject.items import Product


    def parse(self, response):
        l = ItemLoader(item=Product(), response=response)
        l.add_xpath("name", '//div[@class="product_name"]')
        l.add_xpath("name", '//div[@class="product_title"]')
        l.add_xpath("price", '//p[@id="price"]')
        l.add_css("stock", "p#stock")
        l.add_value("last_updated", "today")  # you can also use literal values
        return l.load_item()
:::
:::

By quickly looking at that code, we can see the [`name`{.docutils
.literal .notranslate}]{.pre} field is being extracted from two
different XPath locations in the page:

1.  [`//div[@class="product_name"]`{.docutils .literal
    .notranslate}]{.pre}

2.  [`//div[@class="product_title"]`{.docutils .literal
    .notranslate}]{.pre}

In other words, data is being collected by extracting it from two XPath
locations, using the [[`add_xpath()`{.xref .py .py-meth .docutils
.literal
.notranslate}]{.pre}](#scrapy.loader.ItemLoader.add_xpath "scrapy.loader.ItemLoader.add_xpath"){.reference
.internal} method. This is the data that will be assigned to the
[`name`{.docutils .literal .notranslate}]{.pre} field later.

Afterwards, similar calls are used for [`price`{.docutils .literal
.notranslate}]{.pre} and [`stock`{.docutils .literal
.notranslate}]{.pre} fields (the latter using a CSS selector with the
[[`add_css()`{.xref .py .py-meth .docutils .literal
.notranslate}]{.pre}](#scrapy.loader.ItemLoader.add_css "scrapy.loader.ItemLoader.add_css"){.reference
.internal} method), and finally the [`last_update`{.docutils .literal
.notranslate}]{.pre} field is populated directly with a literal value
([`today`{.docutils .literal .notranslate}]{.pre}) using a different
method: [[`add_value()`{.xref .py .py-meth .docutils .literal
.notranslate}]{.pre}](#scrapy.loader.ItemLoader.add_value "scrapy.loader.ItemLoader.add_value"){.reference
.internal}.

Finally, when all data is collected, the
[[`ItemLoader.load_item()`{.xref .py .py-meth .docutils .literal
.notranslate}]{.pre}](#scrapy.loader.ItemLoader.load_item "scrapy.loader.ItemLoader.load_item"){.reference
.internal} method is called which actually returns the item populated
with the data previously extracted and collected with the
[[`add_xpath()`{.xref .py .py-meth .docutils .literal
.notranslate}]{.pre}](#scrapy.loader.ItemLoader.add_xpath "scrapy.loader.ItemLoader.add_xpath"){.reference
.internal}, [[`add_css()`{.xref .py .py-meth .docutils .literal
.notranslate}]{.pre}](#scrapy.loader.ItemLoader.add_css "scrapy.loader.ItemLoader.add_css"){.reference
.internal}, and [[`add_value()`{.xref .py .py-meth .docutils .literal
.notranslate}]{.pre}](#scrapy.loader.ItemLoader.add_value "scrapy.loader.ItemLoader.add_value"){.reference
.internal} calls.
:::

::: {#working-with-dataclass-items .section}
[]{#topics-loaders-dataclass}

#### Working with dataclass items[¶](#working-with-dataclass-items "Permalink to this heading"){.headerlink}

By default, [[dataclass items]{.std
.std-ref}](index.html#dataclass-items){.hoverxref .tooltip .reference
.internal} require all fields to be passed when created. This could be
an issue when using dataclass items with item loaders: unless a
pre-populated item is passed to the loader, fields will be populated
incrementally using the loader's [[`add_xpath()`{.xref .py .py-meth
.docutils .literal
.notranslate}]{.pre}](#scrapy.loader.ItemLoader.add_xpath "scrapy.loader.ItemLoader.add_xpath"){.reference
.internal}, [[`add_css()`{.xref .py .py-meth .docutils .literal
.notranslate}]{.pre}](#scrapy.loader.ItemLoader.add_css "scrapy.loader.ItemLoader.add_css"){.reference
.internal} and [[`add_value()`{.xref .py .py-meth .docutils .literal
.notranslate}]{.pre}](#scrapy.loader.ItemLoader.add_value "scrapy.loader.ItemLoader.add_value"){.reference
.internal} methods.

One approach to overcome this is to define items using the
[[`field()`{.xref .py .py-func .docutils .literal
.notranslate}]{.pre}](https://docs.python.org/3/library/dataclasses.html#dataclasses.field "(in Python v3.12)"){.reference
.external} function, with a [`default`{.docutils .literal
.notranslate}]{.pre} argument:

::: {.highlight-python .notranslate}
::: highlight
    from dataclasses import dataclass, field
    from typing import Optional


    @dataclass
    class InventoryItem:
        name: Optional[str] = field(default=None)
        price: Optional[float] = field(default=None)
        stock: Optional[int] = field(default=None)
:::
:::
:::

::: {#input-and-output-processors .section}
[]{#topics-loaders-processors}

#### Input and Output processors[¶](#input-and-output-processors "Permalink to this heading"){.headerlink}

An Item Loader contains one input processor and one output processor for
each (item) field. The input processor processes the extracted data as
soon as it's received (through the [[`add_xpath()`{.xref .py .py-meth
.docutils .literal
.notranslate}]{.pre}](#scrapy.loader.ItemLoader.add_xpath "scrapy.loader.ItemLoader.add_xpath"){.reference
.internal}, [[`add_css()`{.xref .py .py-meth .docutils .literal
.notranslate}]{.pre}](#scrapy.loader.ItemLoader.add_css "scrapy.loader.ItemLoader.add_css"){.reference
.internal} or [[`add_value()`{.xref .py .py-meth .docutils .literal
.notranslate}]{.pre}](#scrapy.loader.ItemLoader.add_value "scrapy.loader.ItemLoader.add_value"){.reference
.internal} methods) and the result of the input processor is collected
and kept inside the ItemLoader. After collecting all data, the
[[`ItemLoader.load_item()`{.xref .py .py-meth .docutils .literal
.notranslate}]{.pre}](#scrapy.loader.ItemLoader.load_item "scrapy.loader.ItemLoader.load_item"){.reference
.internal} method is called to populate and get the populated [[item
object]{.std .std-ref}](index.html#topics-items){.hoverxref .tooltip
.reference .internal}. That's when the output processor is called with
the data previously collected (and processed using the input processor).
The result of the output processor is the final value that gets assigned
to the item.

Let's see an example to illustrate how the input and output processors
are called for a particular field (the same applies for any other
field):

::: {.highlight-python .notranslate}
::: highlight
    l = ItemLoader(Product(), some_selector)
    l.add_xpath("name", xpath1)  # (1)
    l.add_xpath("name", xpath2)  # (2)
    l.add_css("name", css)  # (3)
    l.add_value("name", "test")  # (4)
    return l.load_item()  # (5)
:::
:::

So what happens is:

1.  Data from [`xpath1`{.docutils .literal .notranslate}]{.pre} is
    extracted, and passed through the *input processor* of the
    [`name`{.docutils .literal .notranslate}]{.pre} field. The result of
    the input processor is collected and kept in the Item Loader (but
    not yet assigned to the item).

2.  Data from [`xpath2`{.docutils .literal .notranslate}]{.pre} is
    extracted, and passed through the same *input processor* used in
    (1). The result of the input processor is appended to the data
    collected in (1) (if any).

3.  This case is similar to the previous ones, except that the data is
    extracted from the [`css`{.docutils .literal .notranslate}]{.pre}
    CSS selector, and passed through the same *input processor* used
    in (1) and (2). The result of the input processor is appended to the
    data collected in (1) and (2) (if any).

4.  This case is also similar to the previous ones, except that the
    value to be collected is assigned directly, instead of being
    extracted from a XPath expression or a CSS selector. However, the
    value is still passed through the input processors. In this case,
    since the value is not iterable it is converted to an iterable of a
    single element before passing it to the input processor, because
    input processor always receive iterables.

5.  The data collected in steps (1), (2), (3) and (4) is passed through
    the *output processor* of the [`name`{.docutils .literal
    .notranslate}]{.pre} field. The result of the output processor is
    the value assigned to the [`name`{.docutils .literal
    .notranslate}]{.pre} field in the item.

It's worth noticing that processors are just callable objects, which are
called with the data to be parsed, and return a parsed value. So you can
use any function as input or output processor. The only requirement is
that they must accept one (and only one) positional argument, which will
be an iterable.

::: versionchanged
[Changed in version 2.0: ]{.versionmodified .changed}Processors no
longer need to be methods.
:::

::: {.admonition .note}
Note

Both input and output processors must receive an iterable as their first
argument. The output of those functions can be anything. The result of
input processors will be appended to an internal list (in the Loader)
containing the collected values (for that field). The result of the
output processors is the value that will be finally assigned to the
item.
:::

The other thing you need to keep in mind is that the values returned by
input processors are collected internally (in lists) and then passed to
output processors to populate the fields.

Last, but not least,
[itemloaders](https://itemloaders.readthedocs.io/en/latest/){.reference
.external} comes with some [[commonly used processors]{.xref .std
.std-ref}](https://itemloaders.readthedocs.io/en/latest/built-in-processors.html#built-in-processors "(in itemloaders)"){.reference
.external} built-in for convenience.
:::

::: {#declaring-item-loaders .section}
#### Declaring Item Loaders[¶](#declaring-item-loaders "Permalink to this heading"){.headerlink}

Item Loaders are declared using a class definition syntax. Here is an
example:

::: {.highlight-python .notranslate}
::: highlight
    from itemloaders.processors import TakeFirst, MapCompose, Join
    from scrapy.loader import ItemLoader


    class ProductLoader(ItemLoader):
        default_output_processor = TakeFirst()

        name_in = MapCompose(str.title)
        name_out = Join()

        price_in = MapCompose(str.strip)

        # ...
:::
:::

As you can see, input processors are declared using the [`_in`{.docutils
.literal .notranslate}]{.pre} suffix while output processors are
declared using the [`_out`{.docutils .literal .notranslate}]{.pre}
suffix. And you can also declare a default input/output processors using
the [[`ItemLoader.default_input_processor`{.xref .py .py-attr .docutils
.literal
.notranslate}]{.pre}](#scrapy.loader.ItemLoader.default_input_processor "scrapy.loader.ItemLoader.default_input_processor"){.reference
.internal} and [[`ItemLoader.default_output_processor`{.xref .py
.py-attr .docutils .literal
.notranslate}]{.pre}](#scrapy.loader.ItemLoader.default_output_processor "scrapy.loader.ItemLoader.default_output_processor"){.reference
.internal} attributes.
:::

::: {#declaring-input-and-output-processors .section}
[]{#topics-loaders-processors-declaring}

#### Declaring Input and Output Processors[¶](#declaring-input-and-output-processors "Permalink to this heading"){.headerlink}

As seen in the previous section, input and output processors can be
declared in the Item Loader definition, and it's very common to declare
input processors this way. However, there is one more place where you
can specify the input and output processors to use: in the [[Item
Field]{.std .std-ref}](index.html#topics-items-fields){.hoverxref
.tooltip .reference .internal} metadata. Here is an example:

::: {.highlight-python .notranslate}
::: highlight
    import scrapy
    from itemloaders.processors import Join, MapCompose, TakeFirst
    from w3lib.html import remove_tags


    def filter_price(value):
        if value.isdigit():
            return value


    class Product(scrapy.Item):
        name = scrapy.Field(
            input_processor=MapCompose(remove_tags),
            output_processor=Join(),
        )
        price = scrapy.Field(
            input_processor=MapCompose(remove_tags, filter_price),
            output_processor=TakeFirst(),
        )
:::
:::

::: {.highlight-pycon .notranslate}
::: highlight
    >>> from scrapy.loader import ItemLoader
    >>> il = ItemLoader(item=Product())
    >>> il.add_value("name", ["Welcome to my", "<strong>website</strong>"])
    >>> il.add_value("price", ["&euro;", "<span>1000</span>"])
    >>> il.load_item()
    {'name': 'Welcome to my website', 'price': '1000'}
:::
:::

The precedence order, for both input and output processors, is as
follows:

1.  Item Loader field-specific attributes: [`field_in`{.docutils
    .literal .notranslate}]{.pre} and [`field_out`{.docutils .literal
    .notranslate}]{.pre} (most precedence)

2.  Field metadata ([`input_processor`{.docutils .literal
    .notranslate}]{.pre} and [`output_processor`{.docutils .literal
    .notranslate}]{.pre} key)

3.  Item Loader defaults: [[`ItemLoader.default_input_processor()`{.xref
    .py .py-meth .docutils .literal
    .notranslate}]{.pre}](#scrapy.loader.ItemLoader.default_input_processor "scrapy.loader.ItemLoader.default_input_processor"){.reference
    .internal} and [[`ItemLoader.default_output_processor()`{.xref .py
    .py-meth .docutils .literal
    .notranslate}]{.pre}](#scrapy.loader.ItemLoader.default_output_processor "scrapy.loader.ItemLoader.default_output_processor"){.reference
    .internal} (least precedence)

See also: [[Reusing and extending Item Loaders]{.std
.std-ref}](#topics-loaders-extending){.hoverxref .tooltip .reference
.internal}.
:::

::: {#item-loader-context .section}
[]{#topics-loaders-context}

#### Item Loader Context[¶](#item-loader-context "Permalink to this heading"){.headerlink}

The Item Loader Context is a dict of arbitrary key/values which is
shared among all input and output processors in the Item Loader. It can
be passed when declaring, instantiating or using Item Loader. They are
used to modify the behaviour of the input/output processors.

For example, suppose you have a function [`parse_length`{.docutils
.literal .notranslate}]{.pre} which receives a text value and extracts a
length from it:

::: {.highlight-python .notranslate}
::: highlight
    def parse_length(text, loader_context):
        unit = loader_context.get("unit", "m")
        # ... length parsing code goes here ...
        return parsed_length
:::
:::

By accepting a [`loader_context`{.docutils .literal .notranslate}]{.pre}
argument the function is explicitly telling the Item Loader that it's
able to receive an Item Loader context, so the Item Loader passes the
currently active context when calling it, and the processor function
([`parse_length`{.docutils .literal .notranslate}]{.pre} in this case)
can thus use them.

There are several ways to modify Item Loader context values:

1.  By modifying the currently active Item Loader context
    ([[`context`{.xref .py .py-attr .docutils .literal
    .notranslate}]{.pre}](#scrapy.loader.ItemLoader.context "scrapy.loader.ItemLoader.context"){.reference
    .internal} attribute):

    ::: {.highlight-python .notranslate}
    ::: highlight
        loader = ItemLoader(product)
        loader.context["unit"] = "cm"
    :::
    :::

2.  On Item Loader instantiation (the keyword arguments of Item Loader
    [`__init__`{.docutils .literal .notranslate}]{.pre} method are
    stored in the Item Loader context):

    ::: {.highlight-python .notranslate}
    ::: highlight
        loader = ItemLoader(product, unit="cm")
    :::
    :::

3.  On Item Loader declaration, for those input/output processors that
    support instantiating them with an Item Loader context.
    [`MapCompose`{.xref .py .py-class .docutils .literal
    .notranslate}]{.pre} is one of them:

    ::: {.highlight-python .notranslate}
    ::: highlight
        class ProductLoader(ItemLoader):
            length_out = MapCompose(parse_length, unit="cm")
    :::
    :::
:::

::: {#itemloader-objects .section}
#### ItemLoader objects[¶](#itemloader-objects "Permalink to this heading"){.headerlink}

*[class]{.pre}[ ]{.w}*[[scrapy.loader.]{.pre}]{.sig-prename .descclassname}[[ItemLoader]{.pre}]{.sig-name .descname}[(]{.sig-paren}*[[item]{.pre}]{.n}[[=]{.pre}]{.o}[[None]{.pre}]{.default_value}*, *[[selector]{.pre}]{.n}[[=]{.pre}]{.o}[[None]{.pre}]{.default_value}*, *[[response]{.pre}]{.n}[[=]{.pre}]{.o}[[None]{.pre}]{.default_value}*, *[[parent]{.pre}]{.n}[[=]{.pre}]{.o}[[None]{.pre}]{.default_value}*, *[[\*\*]{.pre}]{.o}[[context]{.pre}]{.n}*[)]{.sig-paren}[[[\[source\]]{.pre}]{.viewcode-link}](_modules/scrapy/loader.html#ItemLoader){.reference .internal}[¶](#scrapy.loader.ItemLoader "Permalink to this definition"){.headerlink}

:   A user-friendly abstraction to populate an [[item]{.std
    .std-ref}](index.html#topics-items){.hoverxref .tooltip .reference
    .internal} with data by applying [[field processors]{.std
    .std-ref}](#topics-loaders-processors){.hoverxref .tooltip
    .reference .internal} to scraped data. When instantiated with a
    [`selector`{.docutils .literal .notranslate}]{.pre} or a
    [`response`{.docutils .literal .notranslate}]{.pre} it supports data
    extraction from web pages using [[selectors]{.std
    .std-ref}](index.html#topics-selectors){.hoverxref .tooltip
    .reference .internal}.

    Parameters

    :   -   **item**
            ([*scrapy.item.Item*](index.html#scrapy.item.scrapy.item.Item "scrapy.item.scrapy.item.Item"){.reference
            .internal}) -- The item instance to populate using
            subsequent calls to [[`add_xpath()`{.xref .py .py-meth
            .docutils .literal
            .notranslate}]{.pre}](#scrapy.loader.ItemLoader.add_xpath "scrapy.loader.ItemLoader.add_xpath"){.reference
            .internal}, [[`add_css()`{.xref .py .py-meth .docutils
            .literal
            .notranslate}]{.pre}](#scrapy.loader.ItemLoader.add_css "scrapy.loader.ItemLoader.add_css"){.reference
            .internal}, or [[`add_value()`{.xref .py .py-meth .docutils
            .literal
            .notranslate}]{.pre}](#scrapy.loader.ItemLoader.add_value "scrapy.loader.ItemLoader.add_value"){.reference
            .internal}.

        -   **selector** ([[`Selector`{.xref .py .py-class .docutils
            .literal
            .notranslate}]{.pre}](index.html#scrapy.selector.Selector "scrapy.selector.Selector"){.reference
            .internal} object) -- The selector to extract data from,
            when using the [[`add_xpath()`{.xref .py .py-meth .docutils
            .literal
            .notranslate}]{.pre}](#scrapy.loader.ItemLoader.add_xpath "scrapy.loader.ItemLoader.add_xpath"){.reference
            .internal}, [[`add_css()`{.xref .py .py-meth .docutils
            .literal
            .notranslate}]{.pre}](#scrapy.loader.ItemLoader.add_css "scrapy.loader.ItemLoader.add_css"){.reference
            .internal}, [[`replace_xpath()`{.xref .py .py-meth .docutils
            .literal
            .notranslate}]{.pre}](#scrapy.loader.ItemLoader.replace_xpath "scrapy.loader.ItemLoader.replace_xpath"){.reference
            .internal}, or [[`replace_css()`{.xref .py .py-meth
            .docutils .literal
            .notranslate}]{.pre}](#scrapy.loader.ItemLoader.replace_css "scrapy.loader.ItemLoader.replace_css"){.reference
            .internal} method.

        -   **response** ([[`Response`{.xref .py .py-class .docutils
            .literal
            .notranslate}]{.pre}](index.html#scrapy.http.Response "scrapy.http.Response"){.reference
            .internal} object) -- The response used to construct the
            selector using the [[`default_selector_class`{.xref .py
            .py-attr .docutils .literal
            .notranslate}]{.pre}](#scrapy.loader.ItemLoader.default_selector_class "scrapy.loader.ItemLoader.default_selector_class"){.reference
            .internal}, unless the selector argument is given, in which
            case this argument is ignored.

    If no item is given, one is instantiated automatically using the
    class in [[`default_item_class`{.xref .py .py-attr .docutils
    .literal
    .notranslate}]{.pre}](#scrapy.loader.ItemLoader.default_item_class "scrapy.loader.ItemLoader.default_item_class"){.reference
    .internal}.

    The item, selector, response and remaining keyword arguments are
    assigned to the Loader context (accessible through the
    [[`context`{.xref .py .py-attr .docutils .literal
    .notranslate}]{.pre}](#scrapy.loader.ItemLoader.context "scrapy.loader.ItemLoader.context"){.reference
    .internal} attribute).

    [[item]{.pre}]{.sig-name .descname}[¶](#scrapy.loader.ItemLoader.item "Permalink to this definition"){.headerlink}

    :   The item object being parsed by this Item Loader. This is mostly
        used as a property so, when attempting to override this value,
        you may want to check out [[`default_item_class`{.xref .py
        .py-attr .docutils .literal
        .notranslate}]{.pre}](#scrapy.loader.ItemLoader.default_item_class "scrapy.loader.ItemLoader.default_item_class"){.reference
        .internal} first.

    [[context]{.pre}]{.sig-name .descname}[¶](#scrapy.loader.ItemLoader.context "Permalink to this definition"){.headerlink}

    :   The currently active [[Context]{.xref .std
        .std-ref}](https://itemloaders.readthedocs.io/en/latest/loaders-context.html#loaders-context "(in itemloaders)"){.reference
        .external} of this Item Loader.

    [[default_item_class]{.pre}]{.sig-name .descname}[¶](#scrapy.loader.ItemLoader.default_item_class "Permalink to this definition"){.headerlink}

    :   An [[item]{.std .std-ref}](index.html#topics-items){.hoverxref
        .tooltip .reference .internal} class (or factory), used to
        instantiate items when not given in the [`__init__`{.docutils
        .literal .notranslate}]{.pre} method.

    [[default_input_processor]{.pre}]{.sig-name .descname}[¶](#scrapy.loader.ItemLoader.default_input_processor "Permalink to this definition"){.headerlink}

    :   The default input processor to use for those fields which don't
        specify one.

    [[default_output_processor]{.pre}]{.sig-name .descname}[¶](#scrapy.loader.ItemLoader.default_output_processor "Permalink to this definition"){.headerlink}

    :   The default output processor to use for those fields which don't
        specify one.

    [[default_selector_class]{.pre}]{.sig-name .descname}[¶](#scrapy.loader.ItemLoader.default_selector_class "Permalink to this definition"){.headerlink}

    :   The class used to construct the [[`selector`{.xref .py .py-attr
        .docutils .literal
        .notranslate}]{.pre}](#scrapy.loader.ItemLoader.selector "scrapy.loader.ItemLoader.selector"){.reference
        .internal} of this [[`ItemLoader`{.xref .py .py-class .docutils
        .literal
        .notranslate}]{.pre}](#scrapy.loader.ItemLoader "scrapy.loader.ItemLoader"){.reference
        .internal}, if only a response is given in the
        [`__init__`{.docutils .literal .notranslate}]{.pre} method. If a
        selector is given in the [`__init__`{.docutils .literal
        .notranslate}]{.pre} method this attribute is ignored. This
        attribute is sometimes overridden in subclasses.

    [[selector]{.pre}]{.sig-name .descname}[¶](#scrapy.loader.ItemLoader.selector "Permalink to this definition"){.headerlink}

    :   The [[`Selector`{.xref .py .py-class .docutils .literal
        .notranslate}]{.pre}](index.html#scrapy.selector.Selector "scrapy.selector.Selector"){.reference
        .internal} object to extract data from. It's either the selector
        given in the [`__init__`{.docutils .literal .notranslate}]{.pre}
        method or one created from the response given in the
        [`__init__`{.docutils .literal .notranslate}]{.pre} method using
        the [[`default_selector_class`{.xref .py .py-attr .docutils
        .literal
        .notranslate}]{.pre}](#scrapy.loader.ItemLoader.default_selector_class "scrapy.loader.ItemLoader.default_selector_class"){.reference
        .internal}. This attribute is meant to be read-only.

    [[add_css]{.pre}]{.sig-name .descname}[(]{.sig-paren}*[[field_name]{.pre}]{.n}*, *[[css]{.pre}]{.n}*, *[[\*]{.pre}]{.o}[[processors]{.pre}]{.n}*, *[[re]{.pre}]{.n}[[=]{.pre}]{.o}[[None]{.pre}]{.default_value}*, *[[\*\*]{.pre}]{.o}[[kw]{.pre}]{.n}*[)]{.sig-paren}[[[\[source\]]{.pre}]{.viewcode-link}](_modules/itemloaders.html#ItemLoader.add_css){.reference .internal}[¶](#scrapy.loader.ItemLoader.add_css "Permalink to this definition"){.headerlink}

    :   Similar to [[`ItemLoader.add_value()`{.xref .py .py-meth
        .docutils .literal
        .notranslate}]{.pre}](#scrapy.loader.ItemLoader.add_value "scrapy.loader.ItemLoader.add_value"){.reference
        .internal} but receives a CSS selector instead of a value, which
        is used to extract a list of unicode strings from the selector
        associated with this [[`ItemLoader`{.xref .py .py-class
        .docutils .literal
        .notranslate}]{.pre}](#scrapy.loader.ItemLoader "scrapy.loader.ItemLoader"){.reference
        .internal}.

        See [[`get_css()`{.xref .py .py-meth .docutils .literal
        .notranslate}]{.pre}](#scrapy.loader.ItemLoader.get_css "scrapy.loader.ItemLoader.get_css"){.reference
        .internal} for [`kwargs`{.docutils .literal
        .notranslate}]{.pre}.

        Parameters

        :   **css**
            ([*str*](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.12)"){.reference
            .external}) -- the CSS selector to extract data from

        Examples:

        ::: {.highlight-default .notranslate}
        ::: highlight
            # HTML snippet: <p class="product-name">Color TV</p>
            loader.add_css('name', 'p.product-name')
            # HTML snippet: <p id="price">the price is $1200</p>
            loader.add_css('price', 'p#price', re='the price is (.*)')
        :::
        :::

    [[add_jmes]{.pre}]{.sig-name .descname}[(]{.sig-paren}*[[field_name]{.pre}]{.n}*, *[[jmes]{.pre}]{.n}*, *[[\*]{.pre}]{.o}[[processors]{.pre}]{.n}*, *[[re]{.pre}]{.n}[[=]{.pre}]{.o}[[None]{.pre}]{.default_value}*, *[[\*\*]{.pre}]{.o}[[kw]{.pre}]{.n}*[)]{.sig-paren}[[[\[source\]]{.pre}]{.viewcode-link}](_modules/itemloaders.html#ItemLoader.add_jmes){.reference .internal}[¶](#scrapy.loader.ItemLoader.add_jmes "Permalink to this definition"){.headerlink}

    :   Similar to [[`ItemLoader.add_value()`{.xref .py .py-meth
        .docutils .literal
        .notranslate}]{.pre}](#scrapy.loader.ItemLoader.add_value "scrapy.loader.ItemLoader.add_value"){.reference
        .internal} but receives a JMESPath selector instead of a value,
        which is used to extract a list of unicode strings from the
        selector associated with this [[`ItemLoader`{.xref .py .py-class
        .docutils .literal
        .notranslate}]{.pre}](#scrapy.loader.ItemLoader "scrapy.loader.ItemLoader"){.reference
        .internal}.

        See [[`get_jmes()`{.xref .py .py-meth .docutils .literal
        .notranslate}]{.pre}](#scrapy.loader.ItemLoader.get_jmes "scrapy.loader.ItemLoader.get_jmes"){.reference
        .internal} for [`kwargs`{.docutils .literal
        .notranslate}]{.pre}.

        Parameters

        :   **jmes**
            ([*str*](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.12)"){.reference
            .external}) -- the JMESPath selector to extract data from

        Examples:

        ::: {.highlight-default .notranslate}
        ::: highlight
            # HTML snippet: {"name": "Color TV"}
            loader.add_jmes('name')
            # HTML snippet: {"price": the price is $1200"}
            loader.add_jmes('price', TakeFirst(), re='the price is (.*)')
        :::
        :::

    [[add_value]{.pre}]{.sig-name .descname}[(]{.sig-paren}*[[field_name]{.pre}]{.n}*, *[[value]{.pre}]{.n}*, *[[\*]{.pre}]{.o}[[processors]{.pre}]{.n}*, *[[re]{.pre}]{.n}[[=]{.pre}]{.o}[[None]{.pre}]{.default_value}*, *[[\*\*]{.pre}]{.o}[[kw]{.pre}]{.n}*[)]{.sig-paren}[[[\[source\]]{.pre}]{.viewcode-link}](_modules/itemloaders.html#ItemLoader.add_value){.reference .internal}[¶](#scrapy.loader.ItemLoader.add_value "Permalink to this definition"){.headerlink}

    :   Process and then add the given [`value`{.docutils .literal
        .notranslate}]{.pre} for the given field.

        The value is first passed through [[`get_value()`{.xref .py
        .py-meth .docutils .literal
        .notranslate}]{.pre}](#scrapy.loader.ItemLoader.get_value "scrapy.loader.ItemLoader.get_value"){.reference
        .internal} by giving the [`processors`{.docutils .literal
        .notranslate}]{.pre} and [`kwargs`{.docutils .literal
        .notranslate}]{.pre}, and then passed through the [[field input
        processor]{.xref .std
        .std-ref}](https://itemloaders.readthedocs.io/en/latest/processors.html#processors "(in itemloaders)"){.reference
        .external} and its result appended to the data collected for
        that field. If the field already contains collected data, the
        new data is added.

        The given [`field_name`{.docutils .literal .notranslate}]{.pre}
        can be [`None`{.docutils .literal .notranslate}]{.pre}, in which
        case values for multiple fields may be added. And the processed
        value should be a dict with field_name mapped to values.

        Examples:

        ::: {.highlight-default .notranslate}
        ::: highlight
            loader.add_value('name', 'Color TV')
            loader.add_value('colours', ['white', 'blue'])
            loader.add_value('length', '100')
            loader.add_value('name', 'name: foo', TakeFirst(), re='name: (.+)')
            loader.add_value(None, {'name': 'foo', 'sex': 'male'})
        :::
        :::

    [[add_xpath]{.pre}]{.sig-name .descname}[(]{.sig-paren}*[[field_name]{.pre}]{.n}*, *[[xpath]{.pre}]{.n}*, *[[\*]{.pre}]{.o}[[processors]{.pre}]{.n}*, *[[re]{.pre}]{.n}[[=]{.pre}]{.o}[[None]{.pre}]{.default_value}*, *[[\*\*]{.pre}]{.o}[[kw]{.pre}]{.n}*[)]{.sig-paren}[[[\[source\]]{.pre}]{.viewcode-link}](_modules/itemloaders.html#ItemLoader.add_xpath){.reference .internal}[¶](#scrapy.loader.ItemLoader.add_xpath "Permalink to this definition"){.headerlink}

    :   Similar to [[`ItemLoader.add_value()`{.xref .py .py-meth
        .docutils .literal
        .notranslate}]{.pre}](#scrapy.loader.ItemLoader.add_value "scrapy.loader.ItemLoader.add_value"){.reference
        .internal} but receives an XPath instead of a value, which is
        used to extract a list of strings from the selector associated
        with this [[`ItemLoader`{.xref .py .py-class .docutils .literal
        .notranslate}]{.pre}](#scrapy.loader.ItemLoader "scrapy.loader.ItemLoader"){.reference
        .internal}.

        See [[`get_xpath()`{.xref .py .py-meth .docutils .literal
        .notranslate}]{.pre}](#scrapy.loader.ItemLoader.get_xpath "scrapy.loader.ItemLoader.get_xpath"){.reference
        .internal} for [`kwargs`{.docutils .literal
        .notranslate}]{.pre}.

        Parameters

        :   **xpath**
            ([*str*](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.12)"){.reference
            .external}) -- the XPath to extract data from

        Examples:

        ::: {.highlight-default .notranslate}
        ::: highlight
            # HTML snippet: <p class="product-name">Color TV</p>
            loader.add_xpath('name', '//p[@class="product-name"]')
            # HTML snippet: <p id="price">the price is $1200</p>
            loader.add_xpath('price', '//p[@id="price"]', re='the price is (.*)')
        :::
        :::

    [[get_collected_values]{.pre}]{.sig-name .descname}[(]{.sig-paren}*[[field_name]{.pre}]{.n}*[)]{.sig-paren}[[[\[source\]]{.pre}]{.viewcode-link}](_modules/itemloaders.html#ItemLoader.get_collected_values){.reference .internal}[¶](#scrapy.loader.ItemLoader.get_collected_values "Permalink to this definition"){.headerlink}

    :   Return the collected values for the given field.

    [[get_css]{.pre}]{.sig-name .descname}[(]{.sig-paren}*[[css]{.pre}]{.n}*, *[[\*]{.pre}]{.o}[[processors]{.pre}]{.n}*, *[[re]{.pre}]{.n}[[=]{.pre}]{.o}[[None]{.pre}]{.default_value}*, *[[\*\*]{.pre}]{.o}[[kw]{.pre}]{.n}*[)]{.sig-paren}[[[\[source\]]{.pre}]{.viewcode-link}](_modules/itemloaders.html#ItemLoader.get_css){.reference .internal}[¶](#scrapy.loader.ItemLoader.get_css "Permalink to this definition"){.headerlink}

    :   Similar to [[`ItemLoader.get_value()`{.xref .py .py-meth
        .docutils .literal
        .notranslate}]{.pre}](#scrapy.loader.ItemLoader.get_value "scrapy.loader.ItemLoader.get_value"){.reference
        .internal} but receives a CSS selector instead of a value, which
        is used to extract a list of unicode strings from the selector
        associated with this [[`ItemLoader`{.xref .py .py-class
        .docutils .literal
        .notranslate}]{.pre}](#scrapy.loader.ItemLoader "scrapy.loader.ItemLoader"){.reference
        .internal}.

        Parameters

        :   -   **css**
                ([*str*](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.12)"){.reference
                .external}) -- the CSS selector to extract data from

            -   **re**
                ([*str*](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.12)"){.reference
                .external} *or*
                [*Pattern*](https://docs.python.org/3/library/typing.html#typing.Pattern "(in Python v3.12)"){.reference
                .external}) -- a regular expression to use for
                extracting data from the selected CSS region

        Examples:

        ::: {.highlight-default .notranslate}
        ::: highlight
            # HTML snippet: <p class="product-name">Color TV</p>
            loader.get_css('p.product-name')
            # HTML snippet: <p id="price">the price is $1200</p>
            loader.get_css('p#price', TakeFirst(), re='the price is (.*)')
        :::
        :::

    [[get_jmes]{.pre}]{.sig-name .descname}[(]{.sig-paren}*[[jmes]{.pre}]{.n}*, *[[\*]{.pre}]{.o}[[processors]{.pre}]{.n}*, *[[re]{.pre}]{.n}[[=]{.pre}]{.o}[[None]{.pre}]{.default_value}*, *[[\*\*]{.pre}]{.o}[[kw]{.pre}]{.n}*[)]{.sig-paren}[[[\[source\]]{.pre}]{.viewcode-link}](_modules/itemloaders.html#ItemLoader.get_jmes){.reference .internal}[¶](#scrapy.loader.ItemLoader.get_jmes "Permalink to this definition"){.headerlink}

    :   Similar to [[`ItemLoader.get_value()`{.xref .py .py-meth
        .docutils .literal
        .notranslate}]{.pre}](#scrapy.loader.ItemLoader.get_value "scrapy.loader.ItemLoader.get_value"){.reference
        .internal} but receives a JMESPath selector instead of a value,
        which is used to extract a list of unicode strings from the
        selector associated with this [[`ItemLoader`{.xref .py .py-class
        .docutils .literal
        .notranslate}]{.pre}](#scrapy.loader.ItemLoader "scrapy.loader.ItemLoader"){.reference
        .internal}.

        Parameters

        :   -   **jmes**
                ([*str*](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.12)"){.reference
                .external}) -- the JMESPath selector to extract data
                from

            -   **re**
                ([*str*](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.12)"){.reference
                .external} *or*
                [*Pattern*](https://docs.python.org/3/library/typing.html#typing.Pattern "(in Python v3.12)"){.reference
                .external}) -- a regular expression to use for
                extracting data from the selected JMESPath

        Examples:

        ::: {.highlight-default .notranslate}
        ::: highlight
            # HTML snippet: {"name": "Color TV"}
            loader.get_jmes('name')
            # HTML snippet: {"price": the price is $1200"}
            loader.get_jmes('price', TakeFirst(), re='the price is (.*)')
        :::
        :::

    [[get_output_value]{.pre}]{.sig-name .descname}[(]{.sig-paren}*[[field_name]{.pre}]{.n}*[)]{.sig-paren}[[[\[source\]]{.pre}]{.viewcode-link}](_modules/itemloaders.html#ItemLoader.get_output_value){.reference .internal}[¶](#scrapy.loader.ItemLoader.get_output_value "Permalink to this definition"){.headerlink}

    :   Return the collected values parsed using the output processor,
        for the given field. This method doesn't populate or modify the
        item at all.

    [[get_value]{.pre}]{.sig-name .descname}[(]{.sig-paren}*[[value]{.pre}]{.n}*, *[[\*]{.pre}]{.o}[[processors]{.pre}]{.n}*, *[[re]{.pre}]{.n}[[=]{.pre}]{.o}[[None]{.pre}]{.default_value}*, *[[\*\*]{.pre}]{.o}[[kw]{.pre}]{.n}*[)]{.sig-paren}[[[\[source\]]{.pre}]{.viewcode-link}](_modules/itemloaders.html#ItemLoader.get_value){.reference .internal}[¶](#scrapy.loader.ItemLoader.get_value "Permalink to this definition"){.headerlink}

    :   Process the given [`value`{.docutils .literal
        .notranslate}]{.pre} by the given [`processors`{.docutils
        .literal .notranslate}]{.pre} and keyword arguments.

        Available keyword arguments:

        Parameters

        :   **re**
            ([*str*](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.12)"){.reference
            .external} *or*
            [*Pattern*](https://docs.python.org/3/library/typing.html#typing.Pattern "(in Python v3.12)"){.reference
            .external}) -- a regular expression to use for extracting
            data from the given value using [`extract_regex()`{.xref .py
            .py-func .docutils .literal .notranslate}]{.pre} method,
            applied before processors

        Examples:

        ::: {.doctest .highlight-default .notranslate}
        ::: highlight
            >>> from itemloaders import ItemLoader
            >>> from itemloaders.processors import TakeFirst
            >>> loader = ItemLoader()
            >>> loader.get_value('name: foo', TakeFirst(), str.upper, re='name: (.+)')
            'FOO'
        :::
        :::

    [[get_xpath]{.pre}]{.sig-name .descname}[(]{.sig-paren}*[[xpath]{.pre}]{.n}*, *[[\*]{.pre}]{.o}[[processors]{.pre}]{.n}*, *[[re]{.pre}]{.n}[[=]{.pre}]{.o}[[None]{.pre}]{.default_value}*, *[[\*\*]{.pre}]{.o}[[kw]{.pre}]{.n}*[)]{.sig-paren}[[[\[source\]]{.pre}]{.viewcode-link}](_modules/itemloaders.html#ItemLoader.get_xpath){.reference .internal}[¶](#scrapy.loader.ItemLoader.get_xpath "Permalink to this definition"){.headerlink}

    :   Similar to [[`ItemLoader.get_value()`{.xref .py .py-meth
        .docutils .literal
        .notranslate}]{.pre}](#scrapy.loader.ItemLoader.get_value "scrapy.loader.ItemLoader.get_value"){.reference
        .internal} but receives an XPath instead of a value, which is
        used to extract a list of unicode strings from the selector
        associated with this [[`ItemLoader`{.xref .py .py-class
        .docutils .literal
        .notranslate}]{.pre}](#scrapy.loader.ItemLoader "scrapy.loader.ItemLoader"){.reference
        .internal}.

        Parameters

        :   -   **xpath**
                ([*str*](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.12)"){.reference
                .external}) -- the XPath to extract data from

            -   **re**
                ([*str*](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.12)"){.reference
                .external} *or*
                [*Pattern*](https://docs.python.org/3/library/typing.html#typing.Pattern "(in Python v3.12)"){.reference
                .external}) -- a regular expression to use for
                extracting data from the selected XPath region

        Examples:

        ::: {.highlight-default .notranslate}
        ::: highlight
            # HTML snippet: <p class="product-name">Color TV</p>
            loader.get_xpath('//p[@class="product-name"]')
            # HTML snippet: <p id="price">the price is $1200</p>
            loader.get_xpath('//p[@id="price"]', TakeFirst(), re='the price is (.*)')
        :::
        :::

    [[load_item]{.pre}]{.sig-name .descname}[(]{.sig-paren}[)]{.sig-paren}[[[\[source\]]{.pre}]{.viewcode-link}](_modules/itemloaders.html#ItemLoader.load_item){.reference .internal}[¶](#scrapy.loader.ItemLoader.load_item "Permalink to this definition"){.headerlink}

    :   Populate the item with the data collected so far, and return it.
        The data collected is first passed through the [[output
        processors]{.xref .std
        .std-ref}](https://itemloaders.readthedocs.io/en/latest/processors.html#processors "(in itemloaders)"){.reference
        .external} to get the final value to assign to each item field.

    [[nested_css]{.pre}]{.sig-name .descname}[(]{.sig-paren}*[[css]{.pre}]{.n}*, *[[\*\*]{.pre}]{.o}[[context]{.pre}]{.n}*[)]{.sig-paren}[[[\[source\]]{.pre}]{.viewcode-link}](_modules/itemloaders.html#ItemLoader.nested_css){.reference .internal}[¶](#scrapy.loader.ItemLoader.nested_css "Permalink to this definition"){.headerlink}

    :   Create a nested loader with a css selector. The supplied
        selector is applied relative to selector associated with this
        [[`ItemLoader`{.xref .py .py-class .docutils .literal
        .notranslate}]{.pre}](#scrapy.loader.ItemLoader "scrapy.loader.ItemLoader"){.reference
        .internal}. The nested loader shares the item with the parent
        [[`ItemLoader`{.xref .py .py-class .docutils .literal
        .notranslate}]{.pre}](#scrapy.loader.ItemLoader "scrapy.loader.ItemLoader"){.reference
        .internal} so calls to [[`add_xpath()`{.xref .py .py-meth
        .docutils .literal
        .notranslate}]{.pre}](#scrapy.loader.ItemLoader.add_xpath "scrapy.loader.ItemLoader.add_xpath"){.reference
        .internal}, [[`add_value()`{.xref .py .py-meth .docutils
        .literal
        .notranslate}]{.pre}](#scrapy.loader.ItemLoader.add_value "scrapy.loader.ItemLoader.add_value"){.reference
        .internal}, [[`replace_value()`{.xref .py .py-meth .docutils
        .literal
        .notranslate}]{.pre}](#scrapy.loader.ItemLoader.replace_value "scrapy.loader.ItemLoader.replace_value"){.reference
        .internal}, etc. will behave as expected.

    [[nested_xpath]{.pre}]{.sig-name .descname}[(]{.sig-paren}*[[xpath]{.pre}]{.n}*, *[[\*\*]{.pre}]{.o}[[context]{.pre}]{.n}*[)]{.sig-paren}[[[\[source\]]{.pre}]{.viewcode-link}](_modules/itemloaders.html#ItemLoader.nested_xpath){.reference .internal}[¶](#scrapy.loader.ItemLoader.nested_xpath "Permalink to this definition"){.headerlink}

    :   Create a nested loader with an xpath selector. The supplied
        selector is applied relative to selector associated with this
        [[`ItemLoader`{.xref .py .py-class .docutils .literal
        .notranslate}]{.pre}](#scrapy.loader.ItemLoader "scrapy.loader.ItemLoader"){.reference
        .internal}. The nested loader shares the item with the parent
        [[`ItemLoader`{.xref .py .py-class .docutils .literal
        .notranslate}]{.pre}](#scrapy.loader.ItemLoader "scrapy.loader.ItemLoader"){.reference
        .internal} so calls to [[`add_xpath()`{.xref .py .py-meth
        .docutils .literal
        .notranslate}]{.pre}](#scrapy.loader.ItemLoader.add_xpath "scrapy.loader.ItemLoader.add_xpath"){.reference
        .internal}, [[`add_value()`{.xref .py .py-meth .docutils
        .literal
        .notranslate}]{.pre}](#scrapy.loader.ItemLoader.add_value "scrapy.loader.ItemLoader.add_value"){.reference
        .internal}, [[`replace_value()`{.xref .py .py-meth .docutils
        .literal
        .notranslate}]{.pre}](#scrapy.loader.ItemLoader.replace_value "scrapy.loader.ItemLoader.replace_value"){.reference
        .internal}, etc. will behave as expected.

    [[replace_css]{.pre}]{.sig-name .descname}[(]{.sig-paren}*[[field_name]{.pre}]{.n}*, *[[css]{.pre}]{.n}*, *[[\*]{.pre}]{.o}[[processors]{.pre}]{.n}*, *[[re]{.pre}]{.n}[[=]{.pre}]{.o}[[None]{.pre}]{.default_value}*, *[[\*\*]{.pre}]{.o}[[kw]{.pre}]{.n}*[)]{.sig-paren}[[[\[source\]]{.pre}]{.viewcode-link}](_modules/itemloaders.html#ItemLoader.replace_css){.reference .internal}[¶](#scrapy.loader.ItemLoader.replace_css "Permalink to this definition"){.headerlink}

    :   Similar to [[`add_css()`{.xref .py .py-meth .docutils .literal
        .notranslate}]{.pre}](#scrapy.loader.ItemLoader.add_css "scrapy.loader.ItemLoader.add_css"){.reference
        .internal} but replaces collected data instead of adding it.

    [[replace_jmes]{.pre}]{.sig-name .descname}[(]{.sig-paren}*[[field_name]{.pre}]{.n}*, *[[jmes]{.pre}]{.n}*, *[[\*]{.pre}]{.o}[[processors]{.pre}]{.n}*, *[[re]{.pre}]{.n}[[=]{.pre}]{.o}[[None]{.pre}]{.default_value}*, *[[\*\*]{.pre}]{.o}[[kw]{.pre}]{.n}*[)]{.sig-paren}[[[\[source\]]{.pre}]{.viewcode-link}](_modules/itemloaders.html#ItemLoader.replace_jmes){.reference .internal}[¶](#scrapy.loader.ItemLoader.replace_jmes "Permalink to this definition"){.headerlink}

    :   Similar to [[`add_jmes()`{.xref .py .py-meth .docutils .literal
        .notranslate}]{.pre}](#scrapy.loader.ItemLoader.add_jmes "scrapy.loader.ItemLoader.add_jmes"){.reference
        .internal} but replaces collected data instead of adding it.

    [[replace_value]{.pre}]{.sig-name .descname}[(]{.sig-paren}*[[field_name]{.pre}]{.n}*, *[[value]{.pre}]{.n}*, *[[\*]{.pre}]{.o}[[processors]{.pre}]{.n}*, *[[re]{.pre}]{.n}[[=]{.pre}]{.o}[[None]{.pre}]{.default_value}*, *[[\*\*]{.pre}]{.o}[[kw]{.pre}]{.n}*[)]{.sig-paren}[[[\[source\]]{.pre}]{.viewcode-link}](_modules/itemloaders.html#ItemLoader.replace_value){.reference .internal}[¶](#scrapy.loader.ItemLoader.replace_value "Permalink to this definition"){.headerlink}

    :   Similar to [[`add_value()`{.xref .py .py-meth .docutils .literal
        .notranslate}]{.pre}](#scrapy.loader.ItemLoader.add_value "scrapy.loader.ItemLoader.add_value"){.reference
        .internal} but replaces the collected data with the new value
        instead of adding it.

    [[replace_xpath]{.pre}]{.sig-name .descname}[(]{.sig-paren}*[[field_name]{.pre}]{.n}*, *[[xpath]{.pre}]{.n}*, *[[\*]{.pre}]{.o}[[processors]{.pre}]{.n}*, *[[re]{.pre}]{.n}[[=]{.pre}]{.o}[[None]{.pre}]{.default_value}*, *[[\*\*]{.pre}]{.o}[[kw]{.pre}]{.n}*[)]{.sig-paren}[[[\[source\]]{.pre}]{.viewcode-link}](_modules/itemloaders.html#ItemLoader.replace_xpath){.reference .internal}[¶](#scrapy.loader.ItemLoader.replace_xpath "Permalink to this definition"){.headerlink}

    :   Similar to [[`add_xpath()`{.xref .py .py-meth .docutils .literal
        .notranslate}]{.pre}](#scrapy.loader.ItemLoader.add_xpath "scrapy.loader.ItemLoader.add_xpath"){.reference
        .internal} but replaces collected data instead of adding it.
:::

::: {#nested-loaders .section}
[]{#topics-loaders-nested}

#### Nested Loaders[¶](#nested-loaders "Permalink to this heading"){.headerlink}

When parsing related values from a subsection of a document, it can be
useful to create nested loaders. Imagine you're extracting details from
a footer of a page that looks something like:

Example:

::: {.highlight-default .notranslate}
::: highlight
    <footer>
        <a class="social" href="https://facebook.com/whatever">Like Us</a>
        <a class="social" href="https://twitter.com/whatever">Follow Us</a>
        <a class="email" href="mailto:whatever@example.com">Email Us</a>
    </footer>
:::
:::

Without nested loaders, you need to specify the full xpath (or css) for
each value that you wish to extract.

Example:

::: {.highlight-python .notranslate}
::: highlight
    loader = ItemLoader(item=Item())
    # load stuff not in the footer
    loader.add_xpath("social", '//footer/a[@class = "social"]/@href')
    loader.add_xpath("email", '//footer/a[@class = "email"]/@href')
    loader.load_item()
:::
:::

Instead, you can create a nested loader with the footer selector and add
values relative to the footer. The functionality is the same but you
avoid repeating the footer selector.

Example:

::: {.highlight-python .notranslate}
::: highlight
    loader = ItemLoader(item=Item())
    # load stuff not in the footer
    footer_loader = loader.nested_xpath("//footer")
    footer_loader.add_xpath("social", 'a[@class = "social"]/@href')
    footer_loader.add_xpath("email", 'a[@class = "email"]/@href')
    # no need to call footer_loader.load_item()
    loader.load_item()
:::
:::

You can nest loaders arbitrarily and they work with either xpath or css
selectors. As a general guideline, use nested loaders when they make
your code simpler but do not go overboard with nesting or your parser
can become difficult to read.
:::

::: {#reusing-and-extending-item-loaders .section}
[]{#topics-loaders-extending}

#### Reusing and extending Item Loaders[¶](#reusing-and-extending-item-loaders "Permalink to this heading"){.headerlink}

As your project grows bigger and acquires more and more spiders,
maintenance becomes a fundamental problem, especially when you have to
deal with many different parsing rules for each spider, having a lot of
exceptions, but also wanting to reuse the common processors.

Item Loaders are designed to ease the maintenance burden of parsing
rules, without losing flexibility and, at the same time, providing a
convenient mechanism for extending and overriding them. For this reason
Item Loaders support traditional Python class inheritance for dealing
with differences of specific spiders (or groups of spiders).

Suppose, for example, that some particular site encloses their product
names in three dashes (e.g. [`---Plasma`{.docutils .literal
.notranslate}]{.pre}` `{.docutils .literal
.notranslate}[`TV---`{.docutils .literal .notranslate}]{.pre}) and you
don't want to end up scraping those dashes in the final product names.

Here's how you can remove those dashes by reusing and extending the
default Product Item Loader ([`ProductLoader`{.docutils .literal
.notranslate}]{.pre}):

::: {.highlight-python .notranslate}
::: highlight
    from itemloaders.processors import MapCompose
    from myproject.ItemLoaders import ProductLoader


    def strip_dashes(x):
        return x.strip("-")


    class SiteSpecificLoader(ProductLoader):
        name_in = MapCompose(strip_dashes, ProductLoader.name_in)
:::
:::

Another case where extending Item Loaders can be very helpful is when
you have multiple source formats, for example XML and HTML. In the XML
version you may want to remove [`CDATA`{.docutils .literal
.notranslate}]{.pre} occurrences. Here's an example of how to do it:

::: {.highlight-python .notranslate}
::: highlight
    from itemloaders.processors import MapCompose
    from myproject.ItemLoaders import ProductLoader
    from myproject.utils.xml import remove_cdata


    class XmlProductLoader(ProductLoader):
        name_in = MapCompose(remove_cdata, ProductLoader.name_in)
:::
:::

And that's how you typically extend input processors.

As for output processors, it is more common to declare them in the field
metadata, as they usually depend only on the field and not on each
specific site parsing rule (as input processors do). See also:
[[Declaring Input and Output Processors]{.std
.std-ref}](#topics-loaders-processors-declaring){.hoverxref .tooltip
.reference .internal}.

There are many other possible ways to extend, inherit and override your
Item Loaders, and different Item Loaders hierarchies may fit better for
different projects. Scrapy only provides the mechanism; it doesn't
impose any specific organization of your Loaders collection - that's up
to you and your project's needs.
:::
:::

[]{#document-topics/shell}

::: {#scrapy-shell .section}
[]{#topics-shell}

### Scrapy shell[¶](#scrapy-shell "Permalink to this heading"){.headerlink}

The Scrapy shell is an interactive shell where you can try and debug
your scraping code very quickly, without having to run the spider. It's
meant to be used for testing data extraction code, but you can actually
use it for testing any kind of code as it is also a regular Python
shell.

The shell is used for testing XPath or CSS expressions and see how they
work and what data they extract from the web pages you're trying to
scrape. It allows you to interactively test your expressions while
you're writing your spider, without having to run the spider to test
every change.

Once you get familiarized with the Scrapy shell, you'll see that it's an
invaluable tool for developing and debugging your spiders.

::: {#configuring-the-shell .section}
#### Configuring the shell[¶](#configuring-the-shell "Permalink to this heading"){.headerlink}

If you have [IPython](https://ipython.org/){.reference .external}
installed, the Scrapy shell will use it (instead of the standard Python
console). The [IPython](https://ipython.org/){.reference .external}
console is much more powerful and provides smart auto-completion and
colorized output, among other things.

We highly recommend you install
[IPython](https://ipython.org/){.reference .external}, specially if
you're working on Unix systems (where
[IPython](https://ipython.org/){.reference .external} excels). See the
[IPython installation
guide](https://ipython.org/install.html){.reference .external} for more
info.

Scrapy also has support for
[bpython](https://bpython-interpreter.org/){.reference .external}, and
will try to use it where [IPython](https://ipython.org/){.reference
.external} is unavailable.

Through Scrapy's settings you can configure it to use any one of
[`ipython`{.docutils .literal .notranslate}]{.pre}, [`bpython`{.docutils
.literal .notranslate}]{.pre} or the standard [`python`{.docutils
.literal .notranslate}]{.pre} shell, regardless of which are installed.
This is done by setting the [`SCRAPY_PYTHON_SHELL`{.docutils .literal
.notranslate}]{.pre} environment variable; or by defining it in your
[[scrapy.cfg]{.std
.std-ref}](index.html#topics-config-settings){.hoverxref .tooltip
.reference .internal}:

::: {.highlight-default .notranslate}
::: highlight
    [settings]
    shell = bpython
:::
:::
:::

::: {#launch-the-shell .section}
#### Launch the shell[¶](#launch-the-shell "Permalink to this heading"){.headerlink}

To launch the Scrapy shell you can use the [[`shell`{.xref .std
.std-command .docutils .literal
.notranslate}]{.pre}](index.html#std-command-shell){.hoverxref .tooltip
.reference .internal} command like this:

::: {.highlight-default .notranslate}
::: highlight
    scrapy shell <url>
:::
:::

Where the [`<url>`{.docutils .literal .notranslate}]{.pre} is the URL
you want to scrape.

[[`shell`{.xref .std .std-command .docutils .literal
.notranslate}]{.pre}](index.html#std-command-shell){.hoverxref .tooltip
.reference .internal} also works for local files. This can be handy if
you want to play around with a local copy of a web page. [[`shell`{.xref
.std .std-command .docutils .literal
.notranslate}]{.pre}](index.html#std-command-shell){.hoverxref .tooltip
.reference .internal} understands the following syntaxes for local
files:

::: {.highlight-default .notranslate}
::: highlight
    # UNIX-style
    scrapy shell ./path/to/file.html
    scrapy shell ../other/path/to/file.html
    scrapy shell /absolute/path/to/file.html

    # File URI
    scrapy shell file:///absolute/path/to/file.html
:::
:::

::: {.admonition .note}
Note

When using relative file paths, be explicit and prepend them with
[`./`{.docutils .literal .notranslate}]{.pre} (or [`../`{.docutils
.literal .notranslate}]{.pre} when relevant). [`scrapy`{.docutils
.literal .notranslate}]{.pre}` `{.docutils .literal
.notranslate}[`shell`{.docutils .literal
.notranslate}]{.pre}` `{.docutils .literal
.notranslate}[`index.html`{.docutils .literal .notranslate}]{.pre} will
not work as one might expect (and this is by design, not a bug).

Because [[`shell`{.xref .std .std-command .docutils .literal
.notranslate}]{.pre}](index.html#std-command-shell){.hoverxref .tooltip
.reference .internal} favors HTTP URLs over File URIs, and
[`index.html`{.docutils .literal .notranslate}]{.pre} being
syntactically similar to [`example.com`{.docutils .literal
.notranslate}]{.pre}, [[`shell`{.xref .std .std-command .docutils
.literal .notranslate}]{.pre}](index.html#std-command-shell){.hoverxref
.tooltip .reference .internal} will treat [`index.html`{.docutils
.literal .notranslate}]{.pre} as a domain name and trigger a DNS lookup
error:

::: {.highlight-default .notranslate}
::: highlight
    $ scrapy shell index.html
    [ ... scrapy shell starts ... ]
    [ ... traceback ... ]
    twisted.internet.error.DNSLookupError: DNS lookup failed:
    address 'index.html' not found: [Errno -5] No address associated with hostname.
:::
:::

[[`shell`{.xref .std .std-command .docutils .literal
.notranslate}]{.pre}](index.html#std-command-shell){.hoverxref .tooltip
.reference .internal} will not test beforehand if a file called
[`index.html`{.docutils .literal .notranslate}]{.pre} exists in the
current directory. Again, be explicit.
:::
:::

::: {#using-the-shell .section}
#### Using the shell[¶](#using-the-shell "Permalink to this heading"){.headerlink}

The Scrapy shell is just a regular Python console (or
[IPython](https://ipython.org/){.reference .external} console if you
have it available) which provides some additional shortcut functions for
convenience.

::: {#available-shortcuts .section}
##### Available Shortcuts[¶](#available-shortcuts "Permalink to this heading"){.headerlink}

-   [`shelp()`{.docutils .literal .notranslate}]{.pre} - print a help
    with the list of available objects and shortcuts

-   [`fetch(url[,`{.docutils .literal .notranslate}]{.pre}` `{.docutils
    .literal .notranslate}[`redirect=True])`{.docutils .literal
    .notranslate}]{.pre} - fetch a new response from the given URL and
    update all related objects accordingly. You can optionally ask for
    HTTP 3xx redirections to not be followed by passing
    [`redirect=False`{.docutils .literal .notranslate}]{.pre}

-   [`fetch(request)`{.docutils .literal .notranslate}]{.pre} - fetch a
    new response from the given request and update all related objects
    accordingly.

-   [`view(response)`{.docutils .literal .notranslate}]{.pre} - open the
    given response in your local web browser, for inspection. This will
    add a [\<base\>
    tag](https://developer.mozilla.org/en-US/docs/Web/HTML/Element/base){.reference
    .external} to the response body in order for external links (such as
    images and style sheets) to display properly. Note, however, that
    this will create a temporary file in your computer, which won't be
    removed automatically.
:::

::: {#available-scrapy-objects .section}
##### Available Scrapy objects[¶](#available-scrapy-objects "Permalink to this heading"){.headerlink}

The Scrapy shell automatically creates some convenient objects from the
downloaded page, like the [[`Response`{.xref .py .py-class .docutils
.literal
.notranslate}]{.pre}](index.html#scrapy.http.Response "scrapy.http.Response"){.reference
.internal} object and the [`Selector`{.xref .py .py-class .docutils
.literal .notranslate}]{.pre} objects (for both HTML and XML content).

Those objects are:

-   [`crawler`{.docutils .literal .notranslate}]{.pre} - the current
    [[`Crawler`{.xref .py .py-class .docutils .literal
    .notranslate}]{.pre}](index.html#scrapy.crawler.Crawler "scrapy.crawler.Crawler"){.reference
    .internal} object.

-   [`spider`{.docutils .literal .notranslate}]{.pre} - the Spider which
    is known to handle the URL, or a [[`Spider`{.xref .py .py-class
    .docutils .literal
    .notranslate}]{.pre}](index.html#scrapy.Spider "scrapy.Spider"){.reference
    .internal} object if there is no spider found for the current URL

-   [`request`{.docutils .literal .notranslate}]{.pre} - a
    [`Request`{.xref .py .py-class .docutils .literal
    .notranslate}]{.pre} object of the last fetched page. You can modify
    this request using [`replace()`{.xref .py .py-meth .docutils
    .literal .notranslate}]{.pre} or fetch a new request (without
    leaving the shell) using the [`fetch`{.docutils .literal
    .notranslate}]{.pre} shortcut.

-   [`response`{.docutils .literal .notranslate}]{.pre} - a
    [[`Response`{.xref .py .py-class .docutils .literal
    .notranslate}]{.pre}](index.html#scrapy.http.Response "scrapy.http.Response"){.reference
    .internal} object containing the last fetched page

-   [`settings`{.docutils .literal .notranslate}]{.pre} - the current
    [[Scrapy settings]{.std
    .std-ref}](index.html#topics-settings){.hoverxref .tooltip
    .reference .internal}
:::
:::

::: {#example-of-shell-session .section}
#### Example of shell session[¶](#example-of-shell-session "Permalink to this heading"){.headerlink}

Here's an example of a typical shell session where we start by scraping
the [https://scrapy.org](https://scrapy.org){.reference .external} page,
and then proceed to scrape the
[https://old.reddit.com/](https://old.reddit.com/){.reference .external}
page. Finally, we modify the (Reddit) request method to POST and
re-fetch it getting an error. We end the session by typing Ctrl-D (in
Unix systems) or Ctrl-Z in Windows.

Keep in mind that the data extracted here may not be the same when you
try it, as those pages are not static and could have changed by the time
you test this. The only purpose of this example is to get you
familiarized with how the Scrapy shell works.

First, we launch the shell:

::: {.highlight-default .notranslate}
::: highlight
    scrapy shell 'https://scrapy.org' --nolog
:::
:::

::: {.admonition .note}
Note

Remember to always enclose URLs in quotes when running the Scrapy shell
from the command line, otherwise URLs containing arguments (i.e. the
[`&`{.docutils .literal .notranslate}]{.pre} character) will not work.

On Windows, use double quotes instead:

::: {.highlight-default .notranslate}
::: highlight
    scrapy shell "https://scrapy.org" --nolog
:::
:::
:::

Then, the shell fetches the URL (using the Scrapy downloader) and prints
the list of available objects and useful shortcuts (you'll notice that
these lines all start with the [`[s]`{.docutils .literal
.notranslate}]{.pre} prefix):

::: {.highlight-default .notranslate}
::: highlight
    [s] Available Scrapy objects:
    [s]   scrapy     scrapy module (contains scrapy.Request, scrapy.Selector, etc)
    [s]   crawler    <scrapy.crawler.Crawler object at 0x7f07395dd690>
    [s]   item       {}
    [s]   request    <GET https://scrapy.org>
    [s]   response   <200 https://scrapy.org/>
    [s]   settings   <scrapy.settings.Settings object at 0x7f07395dd710>
    [s]   spider     <DefaultSpider 'default' at 0x7f0735891690>
    [s] Useful shortcuts:
    [s]   fetch(url[, redirect=True]) Fetch URL and update local objects (by default, redirects are followed)
    [s]   fetch(req)                  Fetch a scrapy.Request and update local objects
    [s]   shelp()           Shell help (print this help)
    [s]   view(response)    View response in a browser

    >>>
:::
:::

After that, we can start playing with the objects:

::: {.highlight-pycon .notranslate}
::: highlight
    >>> response.xpath("//title/text()").get()
    'Scrapy | A Fast and Powerful Scraping and Web Crawling Framework'

    >>> fetch("https://old.reddit.com/")

    >>> response.xpath("//title/text()").get()
    'reddit: the front page of the internet'

    >>> request = request.replace(method="POST")

    >>> fetch(request)

    >>> response.status
    404

    >>> from pprint import pprint

    >>> pprint(response.headers)
    {'Accept-Ranges': ['bytes'],
    'Cache-Control': ['max-age=0, must-revalidate'],
    'Content-Type': ['text/html; charset=UTF-8'],
    'Date': ['Thu, 08 Dec 2016 16:21:19 GMT'],
    'Server': ['snooserv'],
    'Set-Cookie': ['loid=KqNLou0V9SKMX4qb4n; Domain=reddit.com; Max-Age=63071999; Path=/; expires=Sat, 08-Dec-2018 16:21:19 GMT; secure',
                    'loidcreated=2016-12-08T16%3A21%3A19.445Z; Domain=reddit.com; Max-Age=63071999; Path=/; expires=Sat, 08-Dec-2018 16:21:19 GMT; secure',
                    'loid=vi0ZVe4NkxNWdlH7r7; Domain=reddit.com; Max-Age=63071999; Path=/; expires=Sat, 08-Dec-2018 16:21:19 GMT; secure',
                    'loidcreated=2016-12-08T16%3A21%3A19.459Z; Domain=reddit.com; Max-Age=63071999; Path=/; expires=Sat, 08-Dec-2018 16:21:19 GMT; secure'],
    'Vary': ['accept-encoding'],
    'Via': ['1.1 varnish'],
    'X-Cache': ['MISS'],
    'X-Cache-Hits': ['0'],
    'X-Content-Type-Options': ['nosniff'],
    'X-Frame-Options': ['SAMEORIGIN'],
    'X-Moose': ['majestic'],
    'X-Served-By': ['cache-cdg8730-CDG'],
    'X-Timer': ['S1481214079.394283,VS0,VE159'],
    'X-Ua-Compatible': ['IE=edge'],
    'X-Xss-Protection': ['1; mode=block']}
:::
:::
:::

::: {#invoking-the-shell-from-spiders-to-inspect-responses .section}
[]{#topics-shell-inspect-response}

#### Invoking the shell from spiders to inspect responses[¶](#invoking-the-shell-from-spiders-to-inspect-responses "Permalink to this heading"){.headerlink}

Sometimes you want to inspect the responses that are being processed in
a certain point of your spider, if only to check that response you
expect is getting there.

This can be achieved by using the
[`scrapy.shell.inspect_response`{.docutils .literal .notranslate}]{.pre}
function.

Here's an example of how you would call it from your spider:

::: {.highlight-python .notranslate}
::: highlight
    import scrapy


    class MySpider(scrapy.Spider):
        name = "myspider"
        start_urls = [
            "http://example.com",
            "http://example.org",
            "http://example.net",
        ]

        def parse(self, response):
            # We want to inspect one specific response.
            if ".org" in response.url:
                from scrapy.shell import inspect_response

                inspect_response(response, self)

            # Rest of parsing code.
:::
:::

When you run the spider, you will get something similar to this:

::: {.highlight-default .notranslate}
::: highlight
    2014-01-23 17:48:31-0400 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://example.com> (referer: None)
    2014-01-23 17:48:31-0400 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://example.org> (referer: None)
    [s] Available Scrapy objects:
    [s]   crawler    <scrapy.crawler.Crawler object at 0x1e16b50>
    ...

    >>> response.url
    'http://example.org'
:::
:::

Then, you can check if the extraction code is working:

::: {.highlight-pycon .notranslate}
::: highlight
    >>> response.xpath('//h1[@class="fn"]')
    []
:::
:::

Nope, it doesn't. So you can open the response in your web browser and
see if it's the response you were expecting:

::: {.highlight-pycon .notranslate}
::: highlight
    >>> view(response)
    True
:::
:::

Finally you hit Ctrl-D (or Ctrl-Z in Windows) to exit the shell and
resume the crawling:

::: {.highlight-default .notranslate}
::: highlight
    >>> ^D
    2014-01-23 17:50:03-0400 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://example.net> (referer: None)
    ...
:::
:::

Note that you can't use the [`fetch`{.docutils .literal
.notranslate}]{.pre} shortcut here since the Scrapy engine is blocked by
the shell. However, after you leave the shell, the spider will continue
crawling where it stopped, as shown above.
:::
:::

[]{#document-topics/item-pipeline}

::: {#item-pipeline .section}
[]{#topics-item-pipeline}

### Item Pipeline[¶](#item-pipeline "Permalink to this heading"){.headerlink}

After an item has been scraped by a spider, it is sent to the Item
Pipeline which processes it through several components that are executed
sequentially.

Each item pipeline component (sometimes referred as just "Item
Pipeline") is a Python class that implements a simple method. They
receive an item and perform an action over it, also deciding if the item
should continue through the pipeline or be dropped and no longer
processed.

Typical uses of item pipelines are:

-   cleansing HTML data

-   validating scraped data (checking that the items contain certain
    fields)

-   checking for duplicates (and dropping them)

-   storing the scraped item in a database

::: {#writing-your-own-item-pipeline .section}
#### Writing your own item pipeline[¶](#writing-your-own-item-pipeline "Permalink to this heading"){.headerlink}

Each item pipeline component is a Python class that must implement the
following method:

[[process_item]{.pre}]{.sig-name .descname}[(]{.sig-paren}*[[self]{.pre}]{.n}*, *[[item]{.pre}]{.n}*, *[[spider]{.pre}]{.n}*[)]{.sig-paren}[¶](#process_item "Permalink to this definition"){.headerlink}

:   This method is called for every item pipeline component.

    item is an [[item object]{.std
    .std-ref}](index.html#item-types){.hoverxref .tooltip .reference
    .internal}, see [[Supporting All Item Types]{.std
    .std-ref}](index.html#supporting-item-types){.hoverxref .tooltip
    .reference .internal}.

    [[`process_item()`{.xref .py .py-meth .docutils .literal
    .notranslate}]{.pre}](#process_item "process_item"){.reference
    .internal} must either: return an [[item object]{.std
    .std-ref}](index.html#item-types){.hoverxref .tooltip .reference
    .internal}, return a [[`Deferred`{.xref .py .py-class .docutils
    .literal
    .notranslate}]{.pre}](https://docs.twisted.org/en/stable/api/twisted.internet.defer.Deferred.html "(in Twisted)"){.reference
    .external} or raise a [[`DropItem`{.xref .py .py-exc .docutils
    .literal
    .notranslate}]{.pre}](index.html#scrapy.exceptions.DropItem "scrapy.exceptions.DropItem"){.reference
    .internal} exception.

    Dropped items are no longer processed by further pipeline
    components.

    Parameters

    :   -   **item** ([[item object]{.std
            .std-ref}](index.html#item-types){.hoverxref .tooltip
            .reference .internal}) -- the scraped item

        -   **spider** ([[`Spider`{.xref .py .py-class .docutils
            .literal
            .notranslate}]{.pre}](index.html#scrapy.Spider "scrapy.Spider"){.reference
            .internal} object) -- the spider which scraped the item

Additionally, they may also implement the following methods:

[[open_spider]{.pre}]{.sig-name .descname}[(]{.sig-paren}*[[self]{.pre}]{.n}*, *[[spider]{.pre}]{.n}*[)]{.sig-paren}[¶](#open_spider "Permalink to this definition"){.headerlink}

:   This method is called when the spider is opened.

    Parameters

    :   **spider** ([[`Spider`{.xref .py .py-class .docutils .literal
        .notranslate}]{.pre}](index.html#scrapy.Spider "scrapy.Spider"){.reference
        .internal} object) -- the spider which was opened

```{=html}
<!-- -->
```

[[close_spider]{.pre}]{.sig-name .descname}[(]{.sig-paren}*[[self]{.pre}]{.n}*, *[[spider]{.pre}]{.n}*[)]{.sig-paren}[¶](#close_spider "Permalink to this definition"){.headerlink}

:   This method is called when the spider is closed.

    Parameters

    :   **spider** ([[`Spider`{.xref .py .py-class .docutils .literal
        .notranslate}]{.pre}](index.html#scrapy.Spider "scrapy.Spider"){.reference
        .internal} object) -- the spider which was closed

```{=html}
<!-- -->
```

*[classmethod]{.pre}[ ]{.w}*[[from_crawler]{.pre}]{.sig-name .descname}[(]{.sig-paren}*[[cls]{.pre}]{.n}*, *[[crawler]{.pre}]{.n}*[)]{.sig-paren}[¶](#from_crawler "Permalink to this definition"){.headerlink}

:   If present, this class method is called to create a pipeline
    instance from a [[`Crawler`{.xref .py .py-class .docutils .literal
    .notranslate}]{.pre}](index.html#scrapy.crawler.Crawler "scrapy.crawler.Crawler"){.reference
    .internal}. It must return a new instance of the pipeline. Crawler
    object provides access to all Scrapy core components like settings
    and signals; it is a way for pipeline to access them and hook its
    functionality into Scrapy.

    Parameters

    :   **crawler** ([[`Crawler`{.xref .py .py-class .docutils .literal
        .notranslate}]{.pre}](index.html#scrapy.crawler.Crawler "scrapy.crawler.Crawler"){.reference
        .internal} object) -- crawler that uses this pipeline
:::

::: {#item-pipeline-example .section}
#### Item pipeline example[¶](#item-pipeline-example "Permalink to this heading"){.headerlink}

::: {#price-validation-and-dropping-items-with-no-prices .section}
##### Price validation and dropping items with no prices[¶](#price-validation-and-dropping-items-with-no-prices "Permalink to this heading"){.headerlink}

Let's take a look at the following hypothetical pipeline that adjusts
the [`price`{.docutils .literal .notranslate}]{.pre} attribute for those
items that do not include VAT ([`price_excludes_vat`{.docutils .literal
.notranslate}]{.pre} attribute), and drops those items which don't
contain a price:

::: {.highlight-python .notranslate}
::: highlight
    from itemadapter import ItemAdapter
    from scrapy.exceptions import DropItem


    class PricePipeline:
        vat_factor = 1.15

        def process_item(self, item, spider):
            adapter = ItemAdapter(item)
            if adapter.get("price"):
                if adapter.get("price_excludes_vat"):
                    adapter["price"] = adapter["price"] * self.vat_factor
                return item
            else:
                raise DropItem(f"Missing price in {item}")
:::
:::
:::

::: {#write-items-to-a-json-lines-file .section}
##### Write items to a JSON lines file[¶](#write-items-to-a-json-lines-file "Permalink to this heading"){.headerlink}

The following pipeline stores all scraped items (from all spiders) into
a single [`items.jsonl`{.docutils .literal .notranslate}]{.pre} file,
containing one item per line serialized in JSON format:

::: {.highlight-python .notranslate}
::: highlight
    import json

    from itemadapter import ItemAdapter


    class JsonWriterPipeline:
        def open_spider(self, spider):
            self.file = open("items.jsonl", "w")

        def close_spider(self, spider):
            self.file.close()

        def process_item(self, item, spider):
            line = json.dumps(ItemAdapter(item).asdict()) + "\n"
            self.file.write(line)
            return item
:::
:::

::: {.admonition .note}
Note

The purpose of JsonWriterPipeline is just to introduce how to write item
pipelines. If you really want to store all scraped items into a JSON
file you should use the [[Feed exports]{.std
.std-ref}](index.html#topics-feed-exports){.hoverxref .tooltip
.reference .internal}.
:::
:::

::: {#write-items-to-mongodb .section}
##### Write items to MongoDB[¶](#write-items-to-mongodb "Permalink to this heading"){.headerlink}

In this example we'll write items to
[MongoDB](https://www.mongodb.com/){.reference .external} using
[pymongo](https://api.mongodb.com/python/current/){.reference
.external}. MongoDB address and database name are specified in Scrapy
settings; MongoDB collection is named after item class.

The main point of this example is to show how to use
[[`from_crawler()`{.xref .py .py-meth .docutils .literal
.notranslate}]{.pre}](#from_crawler "from_crawler"){.reference
.internal} method and how to clean up the resources properly.

::: {.highlight-python .notranslate}
::: highlight
    import pymongo
    from itemadapter import ItemAdapter


    class MongoPipeline:
        collection_name = "scrapy_items"

        def __init__(self, mongo_uri, mongo_db):
            self.mongo_uri = mongo_uri
            self.mongo_db = mongo_db

        @classmethod
        def from_crawler(cls, crawler):
            return cls(
                mongo_uri=crawler.settings.get("MONGO_URI"),
                mongo_db=crawler.settings.get("MONGO_DATABASE", "items"),
            )

        def open_spider(self, spider):
            self.client = pymongo.MongoClient(self.mongo_uri)
            self.db = self.client[self.mongo_db]

        def close_spider(self, spider):
            self.client.close()

        def process_item(self, item, spider):
            self.db[self.collection_name].insert_one(ItemAdapter(item).asdict())
            return item
:::
:::
:::

::: {#take-screenshot-of-item .section}
[]{#screenshotpipeline}

##### Take screenshot of item[¶](#take-screenshot-of-item "Permalink to this heading"){.headerlink}

This example demonstrates how to use [[coroutine
syntax]{.doc}](index.html#document-topics/coroutines){.reference
.internal} in the [[`process_item()`{.xref .py .py-meth .docutils
.literal .notranslate}]{.pre}](#process_item "process_item"){.reference
.internal} method.

This item pipeline makes a request to a locally-running instance of
[Splash](https://splash.readthedocs.io/en/stable/){.reference .external}
to render a screenshot of the item URL. After the request response is
downloaded, the item pipeline saves the screenshot to a file and adds
the filename to the item.

::: {.highlight-python .notranslate}
::: highlight
    import hashlib
    from pathlib import Path
    from urllib.parse import quote

    import scrapy
    from itemadapter import ItemAdapter
    from scrapy.http.request import NO_CALLBACK
    from scrapy.utils.defer import maybe_deferred_to_future


    class ScreenshotPipeline:
        """Pipeline that uses Splash to render screenshot of
        every Scrapy item."""

        SPLASH_URL = "http://localhost:8050/render.png?url={}"

        async def process_item(self, item, spider):
            adapter = ItemAdapter(item)
            encoded_item_url = quote(adapter["url"])
            screenshot_url = self.SPLASH_URL.format(encoded_item_url)
            request = scrapy.Request(screenshot_url, callback=NO_CALLBACK)
            response = await maybe_deferred_to_future(
                spider.crawler.engine.download(request)
            )

            if response.status != 200:
                # Error happened, return item.
                return item

            # Save screenshot to file, filename will be hash of url.
            url = adapter["url"]
            url_hash = hashlib.md5(url.encode("utf8")).hexdigest()
            filename = f"{url_hash}.png"
            Path(filename).write_bytes(response.body)

            # Store filename in item.
            adapter["screenshot_filename"] = filename
            return item
:::
:::
:::

::: {#duplicates-filter .section}
##### Duplicates filter[¶](#duplicates-filter "Permalink to this heading"){.headerlink}

A filter that looks for duplicate items, and drops those items that were
already processed. Let's say that our items have a unique id, but our
spider returns multiples items with the same id:

::: {.highlight-python .notranslate}
::: highlight
    from itemadapter import ItemAdapter
    from scrapy.exceptions import DropItem


    class DuplicatesPipeline:
        def __init__(self):
            self.ids_seen = set()

        def process_item(self, item, spider):
            adapter = ItemAdapter(item)
            if adapter["id"] in self.ids_seen:
                raise DropItem(f"Duplicate item found: {item!r}")
            else:
                self.ids_seen.add(adapter["id"])
                return item
:::
:::
:::
:::

::: {#activating-an-item-pipeline-component .section}
#### Activating an Item Pipeline component[¶](#activating-an-item-pipeline-component "Permalink to this heading"){.headerlink}

To activate an Item Pipeline component you must add its class to the
[[`ITEM_PIPELINES`{.xref .std .std-setting .docutils .literal
.notranslate}]{.pre}](index.html#std-setting-ITEM_PIPELINES){.hoverxref
.tooltip .reference .internal} setting, like in the following example:

::: {.highlight-python .notranslate}
::: highlight
    ITEM_PIPELINES = {
        "myproject.pipelines.PricePipeline": 300,
        "myproject.pipelines.JsonWriterPipeline": 800,
    }
:::
:::

The integer values you assign to classes in this setting determine the
order in which they run: items go through from lower valued to higher
valued classes. It's customary to define these numbers in the 0-1000
range.
:::
:::

[]{#document-topics/feed-exports}

::: {#feed-exports .section}
[]{#topics-feed-exports}

### Feed exports[¶](#feed-exports "Permalink to this heading"){.headerlink}

One of the most frequently required features when implementing scrapers
is being able to store the scraped data properly and, quite often, that
means generating an "export file" with the scraped data (commonly called
"export feed") to be consumed by other systems.

Scrapy provides this functionality out of the box with the Feed Exports,
which allows you to generate feeds with the scraped items, using
multiple serialization formats and storage backends.

::: {#serialization-formats .section}
[]{#topics-feed-format}

#### Serialization formats[¶](#serialization-formats "Permalink to this heading"){.headerlink}

For serializing the scraped data, the feed exports use the [[Item
exporters]{.std .std-ref}](index.html#topics-exporters){.hoverxref
.tooltip .reference .internal}. These formats are supported out of the
box:

-   [[JSON]{.std .std-ref}](#topics-feed-format-json){.hoverxref
    .tooltip .reference .internal}

-   [[JSON lines]{.std
    .std-ref}](#topics-feed-format-jsonlines){.hoverxref .tooltip
    .reference .internal}

-   [[CSV]{.std .std-ref}](#topics-feed-format-csv){.hoverxref .tooltip
    .reference .internal}

-   [[XML]{.std .std-ref}](#topics-feed-format-xml){.hoverxref .tooltip
    .reference .internal}

But you can also extend the supported format through the
[[`FEED_EXPORTERS`{.xref .std .std-setting .docutils .literal
.notranslate}]{.pre}](#std-setting-FEED_EXPORTERS){.hoverxref .tooltip
.reference .internal} setting.

::: {#json .section}
[]{#topics-feed-format-json}

##### JSON[¶](#json "Permalink to this heading"){.headerlink}

-   Value for the [`format`{.docutils .literal .notranslate}]{.pre} key
    in the [[`FEEDS`{.xref .std .std-setting .docutils .literal
    .notranslate}]{.pre}](#std-setting-FEEDS){.hoverxref .tooltip
    .reference .internal} setting: [`json`{.docutils .literal
    .notranslate}]{.pre}

-   Exporter used: [[`JsonItemExporter`{.xref .py .py-class .docutils
    .literal
    .notranslate}]{.pre}](index.html#scrapy.exporters.JsonItemExporter "scrapy.exporters.JsonItemExporter"){.reference
    .internal}

-   See [[this warning]{.std
    .std-ref}](index.html#json-with-large-data){.hoverxref .tooltip
    .reference .internal} if you're using JSON with large feeds.
:::

::: {#json-lines .section}
[]{#topics-feed-format-jsonlines}

##### JSON lines[¶](#json-lines "Permalink to this heading"){.headerlink}

-   Value for the [`format`{.docutils .literal .notranslate}]{.pre} key
    in the [[`FEEDS`{.xref .std .std-setting .docutils .literal
    .notranslate}]{.pre}](#std-setting-FEEDS){.hoverxref .tooltip
    .reference .internal} setting: [`jsonlines`{.docutils .literal
    .notranslate}]{.pre}

-   Exporter used: [[`JsonLinesItemExporter`{.xref .py .py-class
    .docutils .literal
    .notranslate}]{.pre}](index.html#scrapy.exporters.JsonLinesItemExporter "scrapy.exporters.JsonLinesItemExporter"){.reference
    .internal}
:::

::: {#csv .section}
[]{#topics-feed-format-csv}

##### CSV[¶](#csv "Permalink to this heading"){.headerlink}

-   Value for the [`format`{.docutils .literal .notranslate}]{.pre} key
    in the [[`FEEDS`{.xref .std .std-setting .docutils .literal
    .notranslate}]{.pre}](#std-setting-FEEDS){.hoverxref .tooltip
    .reference .internal} setting: [`csv`{.docutils .literal
    .notranslate}]{.pre}

-   Exporter used: [[`CsvItemExporter`{.xref .py .py-class .docutils
    .literal
    .notranslate}]{.pre}](index.html#scrapy.exporters.CsvItemExporter "scrapy.exporters.CsvItemExporter"){.reference
    .internal}

-   To specify columns to export, their order and their column names,
    use [[`FEED_EXPORT_FIELDS`{.xref .std .std-setting .docutils
    .literal
    .notranslate}]{.pre}](#std-setting-FEED_EXPORT_FIELDS){.hoverxref
    .tooltip .reference .internal}. Other feed exporters can also use
    this option, but it is important for CSV because unlike many other
    export formats CSV uses a fixed header.
:::

::: {#xml .section}
[]{#topics-feed-format-xml}

##### XML[¶](#xml "Permalink to this heading"){.headerlink}

-   Value for the [`format`{.docutils .literal .notranslate}]{.pre} key
    in the [[`FEEDS`{.xref .std .std-setting .docutils .literal
    .notranslate}]{.pre}](#std-setting-FEEDS){.hoverxref .tooltip
    .reference .internal} setting: [`xml`{.docutils .literal
    .notranslate}]{.pre}

-   Exporter used: [[`XmlItemExporter`{.xref .py .py-class .docutils
    .literal
    .notranslate}]{.pre}](index.html#scrapy.exporters.XmlItemExporter "scrapy.exporters.XmlItemExporter"){.reference
    .internal}
:::

::: {#pickle .section}
[]{#topics-feed-format-pickle}

##### Pickle[¶](#pickle "Permalink to this heading"){.headerlink}

-   Value for the [`format`{.docutils .literal .notranslate}]{.pre} key
    in the [[`FEEDS`{.xref .std .std-setting .docutils .literal
    .notranslate}]{.pre}](#std-setting-FEEDS){.hoverxref .tooltip
    .reference .internal} setting: [`pickle`{.docutils .literal
    .notranslate}]{.pre}

-   Exporter used: [[`PickleItemExporter`{.xref .py .py-class .docutils
    .literal
    .notranslate}]{.pre}](index.html#scrapy.exporters.PickleItemExporter "scrapy.exporters.PickleItemExporter"){.reference
    .internal}
:::

::: {#marshal .section}
[]{#topics-feed-format-marshal}

##### Marshal[¶](#marshal "Permalink to this heading"){.headerlink}

-   Value for the [`format`{.docutils .literal .notranslate}]{.pre} key
    in the [[`FEEDS`{.xref .std .std-setting .docutils .literal
    .notranslate}]{.pre}](#std-setting-FEEDS){.hoverxref .tooltip
    .reference .internal} setting: [`marshal`{.docutils .literal
    .notranslate}]{.pre}

-   Exporter used: [[`MarshalItemExporter`{.xref .py .py-class .docutils
    .literal
    .notranslate}]{.pre}](index.html#scrapy.exporters.MarshalItemExporter "scrapy.exporters.MarshalItemExporter"){.reference
    .internal}
:::
:::

::: {#storages .section}
[]{#topics-feed-storage}

#### Storages[¶](#storages "Permalink to this heading"){.headerlink}

When using the feed exports you define where to store the feed using one
or multiple
[URIs](https://en.wikipedia.org/wiki/Uniform_Resource_Identifier){.reference
.external} (through the [[`FEEDS`{.xref .std .std-setting .docutils
.literal .notranslate}]{.pre}](#std-setting-FEEDS){.hoverxref .tooltip
.reference .internal} setting). The feed exports supports multiple
storage backend types which are defined by the URI scheme.

The storages backends supported out of the box are:

-   [[Local filesystem]{.std
    .std-ref}](#topics-feed-storage-fs){.hoverxref .tooltip .reference
    .internal}

-   [[FTP]{.std .std-ref}](#topics-feed-storage-ftp){.hoverxref .tooltip
    .reference .internal}

-   [[S3]{.std .std-ref}](#topics-feed-storage-s3){.hoverxref .tooltip
    .reference .internal} (requires
    [boto3](https://github.com/boto/boto3){.reference .external})

-   [[Google Cloud Storage (GCS)]{.std
    .std-ref}](#topics-feed-storage-gcs){.hoverxref .tooltip .reference
    .internal} (requires
    [google-cloud-storage](https://cloud.google.com/storage/docs/reference/libraries#client-libraries-install-python){.reference
    .external})

-   [[Standard output]{.std
    .std-ref}](#topics-feed-storage-stdout){.hoverxref .tooltip
    .reference .internal}

Some storage backends may be unavailable if the required external
libraries are not available. For example, the S3 backend is only
available if the [boto3](https://github.com/boto/boto3){.reference
.external} library is installed.
:::

::: {#storage-uri-parameters .section}
[]{#topics-feed-uri-params}

#### Storage URI parameters[¶](#storage-uri-parameters "Permalink to this heading"){.headerlink}

The storage URI can also contain parameters that get replaced when the
feed is being created. These parameters are:

-   [`%(time)s`{.docutils .literal .notranslate}]{.pre} - gets replaced
    by a timestamp when the feed is being created

-   [`%(name)s`{.docutils .literal .notranslate}]{.pre} - gets replaced
    by the spider name

Any other named parameter gets replaced by the spider attribute of the
same name. For example, [`%(site_id)s`{.docutils .literal
.notranslate}]{.pre} would get replaced by the
[`spider.site_id`{.docutils .literal .notranslate}]{.pre} attribute the
moment the feed is being created.

Here are some examples to illustrate:

-   Store in FTP using one directory per spider:

    -   [`ftp://user:password@ftp.example.com/scraping/feeds/%(name)s/%(time)s.json`{.docutils
        .literal .notranslate}]{.pre}

-   Store in S3 using one directory per spider:

    -   [`s3://mybucket/scraping/feeds/%(name)s/%(time)s.json`{.docutils
        .literal .notranslate}]{.pre}

::: {.admonition .note}
Note

[[Spider arguments]{.std .std-ref}](index.html#spiderargs){.hoverxref
.tooltip .reference .internal} become spider attributes, hence they can
also be used as storage URI parameters.
:::
:::

::: {#storage-backends .section}
[]{#topics-feed-storage-backends}

#### Storage backends[¶](#storage-backends "Permalink to this heading"){.headerlink}

::: {#local-filesystem .section}
[]{#topics-feed-storage-fs}

##### Local filesystem[¶](#local-filesystem "Permalink to this heading"){.headerlink}

The feeds are stored in the local filesystem.

-   URI scheme: [`file`{.docutils .literal .notranslate}]{.pre}

-   Example URI: [`file:///tmp/export.csv`{.docutils .literal
    .notranslate}]{.pre}

-   Required external libraries: none

Note that for the local filesystem storage (only) you can omit the
scheme if you specify an absolute path like [`/tmp/export.csv`{.docutils
.literal .notranslate}]{.pre} (Unix systems only). Alternatively you can
also use a [[`pathlib.Path`{.xref .py .py-class .docutils .literal
.notranslate}]{.pre}](https://docs.python.org/3/library/pathlib.html#pathlib.Path "(in Python v3.12)"){.reference
.external} object.
:::

::: {#ftp .section}
[]{#topics-feed-storage-ftp}

##### FTP[¶](#ftp "Permalink to this heading"){.headerlink}

The feeds are stored in a FTP server.

-   URI scheme: [`ftp`{.docutils .literal .notranslate}]{.pre}

-   Example URI:
    [`ftp://user:pass@ftp.example.com/path/to/export.csv`{.docutils
    .literal .notranslate}]{.pre}

-   Required external libraries: none

FTP supports two different connection modes: [active or
passive](https://stackoverflow.com/a/1699163){.reference .external}.
Scrapy uses the passive connection mode by default. To use the active
connection mode instead, set the [[`FEED_STORAGE_FTP_ACTIVE`{.xref .std
.std-setting .docutils .literal
.notranslate}]{.pre}](#std-setting-FEED_STORAGE_FTP_ACTIVE){.hoverxref
.tooltip .reference .internal} setting to [`True`{.docutils .literal
.notranslate}]{.pre}.

The default value for the [`overwrite`{.docutils .literal
.notranslate}]{.pre} key in the [[`FEEDS`{.xref .std .std-setting
.docutils .literal .notranslate}]{.pre}](#std-setting-FEEDS){.hoverxref
.tooltip .reference .internal} for this storage backend is:
[`True`{.docutils .literal .notranslate}]{.pre}.

::: {.admonition .caution}
Caution

The value [`True`{.docutils .literal .notranslate}]{.pre} in
[`overwrite`{.docutils .literal .notranslate}]{.pre} will cause you to
lose the previous version of your data.
:::

This storage backend uses [[delayed file delivery]{.std
.std-ref}](#delayed-file-delivery){.hoverxref .tooltip .reference
.internal}.
:::

::: {#s3 .section}
[]{#topics-feed-storage-s3}

##### S3[¶](#s3 "Permalink to this heading"){.headerlink}

The feeds are stored on [Amazon
S3](https://aws.amazon.com/s3/){.reference .external}.

-   URI scheme: [`s3`{.docutils .literal .notranslate}]{.pre}

-   Example URIs:

    -   [`s3://mybucket/path/to/export.csv`{.docutils .literal
        .notranslate}]{.pre}

    -   [`s3://aws_key:aws_secret@mybucket/path/to/export.csv`{.docutils
        .literal .notranslate}]{.pre}

-   Required external libraries:
    [boto3](https://github.com/boto/boto3){.reference .external} \>=
    1.20.0

The AWS credentials can be passed as user/password in the URI, or they
can be passed through the following settings:

-   [[`AWS_ACCESS_KEY_ID`{.xref .std .std-setting .docutils .literal
    .notranslate}]{.pre}](index.html#std-setting-AWS_ACCESS_KEY_ID){.hoverxref
    .tooltip .reference .internal}

-   [[`AWS_SECRET_ACCESS_KEY`{.xref .std .std-setting .docutils .literal
    .notranslate}]{.pre}](index.html#std-setting-AWS_SECRET_ACCESS_KEY){.hoverxref
    .tooltip .reference .internal}

-   [[`AWS_SESSION_TOKEN`{.xref .std .std-setting .docutils .literal
    .notranslate}]{.pre}](index.html#std-setting-AWS_SESSION_TOKEN){.hoverxref
    .tooltip .reference .internal} (only needed for [temporary security
    credentials](https://docs.aws.amazon.com/general/latest/gr/aws-sec-cred-types.html#temporary-access-keys){.reference
    .external})

You can also define a custom ACL, custom endpoint, and region name for
exported feeds using these settings:

-   [[`FEED_STORAGE_S3_ACL`{.xref .std .std-setting .docutils .literal
    .notranslate}]{.pre}](#std-setting-FEED_STORAGE_S3_ACL){.hoverxref
    .tooltip .reference .internal}

-   [[`AWS_ENDPOINT_URL`{.xref .std .std-setting .docutils .literal
    .notranslate}]{.pre}](index.html#std-setting-AWS_ENDPOINT_URL){.hoverxref
    .tooltip .reference .internal}

-   [[`AWS_REGION_NAME`{.xref .std .std-setting .docutils .literal
    .notranslate}]{.pre}](index.html#std-setting-AWS_REGION_NAME){.hoverxref
    .tooltip .reference .internal}

The default value for the [`overwrite`{.docutils .literal
.notranslate}]{.pre} key in the [[`FEEDS`{.xref .std .std-setting
.docutils .literal .notranslate}]{.pre}](#std-setting-FEEDS){.hoverxref
.tooltip .reference .internal} for this storage backend is:
[`True`{.docutils .literal .notranslate}]{.pre}.

::: {.admonition .caution}
Caution

The value [`True`{.docutils .literal .notranslate}]{.pre} in
[`overwrite`{.docutils .literal .notranslate}]{.pre} will cause you to
lose the previous version of your data.
:::

This storage backend uses [[delayed file delivery]{.std
.std-ref}](#delayed-file-delivery){.hoverxref .tooltip .reference
.internal}.
:::

::: {#google-cloud-storage-gcs .section}
[]{#topics-feed-storage-gcs}

##### Google Cloud Storage (GCS)[¶](#google-cloud-storage-gcs "Permalink to this heading"){.headerlink}

::: versionadded
[New in version 2.3.]{.versionmodified .added}
:::

The feeds are stored on [Google Cloud
Storage](https://cloud.google.com/storage/){.reference .external}.

-   URI scheme: [`gs`{.docutils .literal .notranslate}]{.pre}

-   Example URIs:

    -   [`gs://mybucket/path/to/export.csv`{.docutils .literal
        .notranslate}]{.pre}

-   Required external libraries:
    [google-cloud-storage](https://cloud.google.com/storage/docs/reference/libraries#client-libraries-install-python){.reference
    .external}.

For more information about authentication, please refer to [Google Cloud
documentation](https://cloud.google.com/docs/authentication/production){.reference
.external}.

You can set a *Project ID* and *Access Control List (ACL)* through the
following settings:

-   [[`FEED_STORAGE_GCS_ACL`{.xref .std .std-setting .docutils .literal
    .notranslate}]{.pre}](index.html#std-setting-FEED_STORAGE_GCS_ACL){.hoverxref
    .tooltip .reference .internal}

-   [[`GCS_PROJECT_ID`{.xref .std .std-setting .docutils .literal
    .notranslate}]{.pre}](index.html#std-setting-GCS_PROJECT_ID){.hoverxref
    .tooltip .reference .internal}

The default value for the [`overwrite`{.docutils .literal
.notranslate}]{.pre} key in the [[`FEEDS`{.xref .std .std-setting
.docutils .literal .notranslate}]{.pre}](#std-setting-FEEDS){.hoverxref
.tooltip .reference .internal} for this storage backend is:
[`True`{.docutils .literal .notranslate}]{.pre}.

::: {.admonition .caution}
Caution

The value [`True`{.docutils .literal .notranslate}]{.pre} in
[`overwrite`{.docutils .literal .notranslate}]{.pre} will cause you to
lose the previous version of your data.
:::

This storage backend uses [[delayed file delivery]{.std
.std-ref}](#delayed-file-delivery){.hoverxref .tooltip .reference
.internal}.
:::

::: {#standard-output .section}
[]{#topics-feed-storage-stdout}

##### Standard output[¶](#standard-output "Permalink to this heading"){.headerlink}

The feeds are written to the standard output of the Scrapy process.

-   URI scheme: [`stdout`{.docutils .literal .notranslate}]{.pre}

-   Example URI: [`stdout:`{.docutils .literal .notranslate}]{.pre}

-   Required external libraries: none
:::

::: {#delayed-file-delivery .section}
[]{#id1}

##### Delayed file delivery[¶](#delayed-file-delivery "Permalink to this heading"){.headerlink}

As indicated above, some of the described storage backends use delayed
file delivery.

These storage backends do not upload items to the feed URI as those
items are scraped. Instead, Scrapy writes items into a temporary local
file, and only once all the file contents have been written (i.e. at the
end of the crawl) is that file uploaded to the feed URI.

If you want item delivery to start earlier when using one of these
storage backends, use [[`FEED_EXPORT_BATCH_ITEM_COUNT`{.xref .std
.std-setting .docutils .literal
.notranslate}]{.pre}](#std-setting-FEED_EXPORT_BATCH_ITEM_COUNT){.hoverxref
.tooltip .reference .internal} to split the output items in multiple
files, with the specified maximum item count per file. That way, as soon
as a file reaches the maximum item count, that file is delivered to the
feed URI, allowing item delivery to start way before the end of the
crawl.
:::
:::

::: {#item-filtering .section}
[]{#item-filter}

#### Item filtering[¶](#item-filtering "Permalink to this heading"){.headerlink}

::: versionadded
[New in version 2.6.0.]{.versionmodified .added}
:::

You can filter items that you want to allow for a particular feed by
using the [`item_classes`{.docutils .literal .notranslate}]{.pre} option
in [[feeds options]{.std .std-ref}](#feed-options){.hoverxref .tooltip
.reference .internal}. Only items of the specified types will be added
to the feed.

The [`item_classes`{.docutils .literal .notranslate}]{.pre} option is
implemented by the [[`ItemFilter`{.xref .py .py-class .docutils .literal
.notranslate}]{.pre}](#scrapy.extensions.feedexport.ItemFilter "scrapy.extensions.feedexport.ItemFilter"){.reference
.internal} class, which is the default value of the
[`item_filter`{.docutils .literal .notranslate}]{.pre} [[feed
option]{.std .std-ref}](#feed-options){.hoverxref .tooltip .reference
.internal}.

You can create your own custom filtering class by implementing
[[`ItemFilter`{.xref .py .py-class .docutils .literal
.notranslate}]{.pre}](#scrapy.extensions.feedexport.ItemFilter "scrapy.extensions.feedexport.ItemFilter"){.reference
.internal}'s method [`accepts`{.docutils .literal .notranslate}]{.pre}
and taking [`feed_options`{.docutils .literal .notranslate}]{.pre} as an
argument.

For instance:

::: {.highlight-python .notranslate}
::: highlight
    class MyCustomFilter:
        def __init__(self, feed_options):
            self.feed_options = feed_options

        def accepts(self, item):
            if "field1" in item and item["field1"] == "expected_data":
                return True
            return False
:::
:::

You can assign your custom filtering class to the
[`item_filter`{.docutils .literal .notranslate}]{.pre} [[option of a
feed]{.std .std-ref}](#feed-options){.hoverxref .tooltip .reference
.internal}. See [[`FEEDS`{.xref .std .std-setting .docutils .literal
.notranslate}]{.pre}](#std-setting-FEEDS){.hoverxref .tooltip .reference
.internal} for examples.

::: {#itemfilter .section}
##### ItemFilter[¶](#itemfilter "Permalink to this heading"){.headerlink}

*[class]{.pre}[ ]{.w}*[[scrapy.extensions.feedexport.]{.pre}]{.sig-prename .descclassname}[[ItemFilter]{.pre}]{.sig-name .descname}[(]{.sig-paren}*[[feed_options]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[[[Optional]{.pre}](https://docs.python.org/3/library/typing.html#typing.Optional "(in Python v3.12)"){.reference .external}[[\[]{.pre}]{.p}[[dict]{.pre}](https://docs.python.org/3/library/stdtypes.html#dict "(in Python v3.12)"){.reference .external}[[\]]{.pre}]{.p}]{.n}*[)]{.sig-paren}[[[\[source\]]{.pre}]{.viewcode-link}](_modules/scrapy/extensions/feedexport.html#ItemFilter){.reference .internal}[¶](#scrapy.extensions.feedexport.ItemFilter "Permalink to this definition"){.headerlink}

:   This will be used by FeedExporter to decide if an item should be
    allowed to be exported to a particular feed.

    Parameters

    :   **feed_options**
        ([*dict*](https://docs.python.org/3/library/stdtypes.html#dict "(in Python v3.12)"){.reference
        .external}) -- feed specific options passed from FeedExporter

    [[accepts]{.pre}]{.sig-name .descname}[(]{.sig-paren}*[[item]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[[[Any]{.pre}](https://docs.python.org/3/library/typing.html#typing.Any "(in Python v3.12)"){.reference .external}]{.n}*[)]{.sig-paren} [[→]{.sig-return-icon} [[[bool]{.pre}](https://docs.python.org/3/library/functions.html#bool "(in Python v3.12)"){.reference .external}]{.sig-return-typehint}]{.sig-return}[[[\[source\]]{.pre}]{.viewcode-link}](_modules/scrapy/extensions/feedexport.html#ItemFilter.accepts){.reference .internal}[¶](#scrapy.extensions.feedexport.ItemFilter.accepts "Permalink to this definition"){.headerlink}

    :   Return [`True`{.docutils .literal .notranslate}]{.pre} if item
        should be exported or [`False`{.docutils .literal
        .notranslate}]{.pre} otherwise.

        Parameters

        :   **item** ([[Scrapy items]{.std
            .std-ref}](index.html#topics-items){.hoverxref .tooltip
            .reference .internal}) -- scraped item which user wants to
            check if is acceptable

        Returns

        :   True if accepted, False otherwise

        Return type

        :   [bool](https://docs.python.org/3/library/functions.html#bool "(in Python v3.12)"){.reference
            .external}
:::
:::

::: {#post-processing .section}
[]{#id2}

#### Post-Processing[¶](#post-processing "Permalink to this heading"){.headerlink}

::: versionadded
[New in version 2.6.0.]{.versionmodified .added}
:::

Scrapy provides an option to activate plugins to post-process feeds
before they are exported to feed storages. In addition to using
[[builtin plugins]{.std .std-ref}](#builtin-plugins){.hoverxref .tooltip
.reference .internal}, you can create your own [[plugins]{.std
.std-ref}](#custom-plugins){.hoverxref .tooltip .reference .internal}.

These plugins can be activated through the [`postprocessing`{.docutils
.literal .notranslate}]{.pre} option of a feed. The option must be
passed a list of post-processing plugins in the order you want the feed
to be processed. These plugins can be declared either as an import
string or with the imported class of the plugin. Parameters to plugins
can be passed through the feed options. See [[feed options]{.std
.std-ref}](#feed-options){.hoverxref .tooltip .reference .internal} for
examples.

::: {#built-in-plugins .section}
[]{#builtin-plugins}

##### Built-in Plugins[¶](#built-in-plugins "Permalink to this heading"){.headerlink}

*[class]{.pre}[ ]{.w}*[[scrapy.extensions.postprocessing.]{.pre}]{.sig-prename .descclassname}[[GzipPlugin]{.pre}]{.sig-name .descname}[(]{.sig-paren}*[[file]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[[[BinaryIO]{.pre}](https://docs.python.org/3/library/typing.html#typing.BinaryIO "(in Python v3.12)"){.reference .external}]{.n}*, *[[feed_options]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[[[Dict]{.pre}](https://docs.python.org/3/library/typing.html#typing.Dict "(in Python v3.12)"){.reference .external}[[\[]{.pre}]{.p}[[str]{.pre}](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.12)"){.reference .external}[[,]{.pre}]{.p}[ ]{.w}[[Any]{.pre}](https://docs.python.org/3/library/typing.html#typing.Any "(in Python v3.12)"){.reference .external}[[\]]{.pre}]{.p}]{.n}*[)]{.sig-paren}[[[\[source\]]{.pre}]{.viewcode-link}](_modules/scrapy/extensions/postprocessing.html#GzipPlugin){.reference .internal}[¶](#scrapy.extensions.postprocessing.GzipPlugin "Permalink to this definition"){.headerlink}

:   Compresses received data using
    [gzip](https://en.wikipedia.org/wiki/Gzip){.reference .external}.

    Accepted [`feed_options`{.docutils .literal .notranslate}]{.pre}
    parameters:

    -   gzip_compresslevel

    -   gzip_mtime

    -   gzip_filename

    See [[`gzip.GzipFile`{.xref .py .py-class .docutils .literal
    .notranslate}]{.pre}](https://docs.python.org/3/library/gzip.html#gzip.GzipFile "(in Python v3.12)"){.reference
    .external} for more info about parameters.

```{=html}
<!-- -->
```

*[class]{.pre}[ ]{.w}*[[scrapy.extensions.postprocessing.]{.pre}]{.sig-prename .descclassname}[[LZMAPlugin]{.pre}]{.sig-name .descname}[(]{.sig-paren}*[[file]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[[[BinaryIO]{.pre}](https://docs.python.org/3/library/typing.html#typing.BinaryIO "(in Python v3.12)"){.reference .external}]{.n}*, *[[feed_options]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[[[Dict]{.pre}](https://docs.python.org/3/library/typing.html#typing.Dict "(in Python v3.12)"){.reference .external}[[\[]{.pre}]{.p}[[str]{.pre}](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.12)"){.reference .external}[[,]{.pre}]{.p}[ ]{.w}[[Any]{.pre}](https://docs.python.org/3/library/typing.html#typing.Any "(in Python v3.12)"){.reference .external}[[\]]{.pre}]{.p}]{.n}*[)]{.sig-paren}[[[\[source\]]{.pre}]{.viewcode-link}](_modules/scrapy/extensions/postprocessing.html#LZMAPlugin){.reference .internal}[¶](#scrapy.extensions.postprocessing.LZMAPlugin "Permalink to this definition"){.headerlink}

:   Compresses received data using
    [lzma](https://en.wikipedia.org/wiki/Lempel–Ziv–Markov_chain_algorithm){.reference
    .external}.

    Accepted [`feed_options`{.docutils .literal .notranslate}]{.pre}
    parameters:

    -   lzma_format

    -   lzma_check

    -   lzma_preset

    -   lzma_filters

    ::: {.admonition .note}
    Note

    [`lzma_filters`{.docutils .literal .notranslate}]{.pre} cannot be
    used in pypy version 7.3.1 and older.
    :::

    See [[`lzma.LZMAFile`{.xref .py .py-class .docutils .literal
    .notranslate}]{.pre}](https://docs.python.org/3/library/lzma.html#lzma.LZMAFile "(in Python v3.12)"){.reference
    .external} for more info about parameters.

```{=html}
<!-- -->
```

*[class]{.pre}[ ]{.w}*[[scrapy.extensions.postprocessing.]{.pre}]{.sig-prename .descclassname}[[Bz2Plugin]{.pre}]{.sig-name .descname}[(]{.sig-paren}*[[file]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[[[BinaryIO]{.pre}](https://docs.python.org/3/library/typing.html#typing.BinaryIO "(in Python v3.12)"){.reference .external}]{.n}*, *[[feed_options]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[[[Dict]{.pre}](https://docs.python.org/3/library/typing.html#typing.Dict "(in Python v3.12)"){.reference .external}[[\[]{.pre}]{.p}[[str]{.pre}](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.12)"){.reference .external}[[,]{.pre}]{.p}[ ]{.w}[[Any]{.pre}](https://docs.python.org/3/library/typing.html#typing.Any "(in Python v3.12)"){.reference .external}[[\]]{.pre}]{.p}]{.n}*[)]{.sig-paren}[[[\[source\]]{.pre}]{.viewcode-link}](_modules/scrapy/extensions/postprocessing.html#Bz2Plugin){.reference .internal}[¶](#scrapy.extensions.postprocessing.Bz2Plugin "Permalink to this definition"){.headerlink}

:   Compresses received data using
    [bz2](https://en.wikipedia.org/wiki/Bzip2){.reference .external}.

    Accepted [`feed_options`{.docutils .literal .notranslate}]{.pre}
    parameters:

    -   bz2_compresslevel

    See [[`bz2.BZ2File`{.xref .py .py-class .docutils .literal
    .notranslate}]{.pre}](https://docs.python.org/3/library/bz2.html#bz2.BZ2File "(in Python v3.12)"){.reference
    .external} for more info about parameters.
:::

::: {#custom-plugins .section}
[]{#id3}

##### Custom Plugins[¶](#custom-plugins "Permalink to this heading"){.headerlink}

Each plugin is a class that must implement the following methods:

[[\_\_init\_\_]{.pre}]{.sig-name .descname}[(]{.sig-paren}*[[self]{.pre}]{.n}*, *[[file]{.pre}]{.n}*, *[[feed_options]{.pre}]{.n}*[)]{.sig-paren}[¶](#init__ "Permalink to this definition"){.headerlink}

:   Initialize the plugin.

    Parameters

    :   -   **file** -- file-like object having at least the write, tell
            and close methods implemented

        -   **feed_options** ([[`dict`{.xref .py .py-class .docutils
            .literal
            .notranslate}]{.pre}](https://docs.python.org/3/library/stdtypes.html#dict "(in Python v3.12)"){.reference
            .external}) -- feed-specific [[options]{.std
            .std-ref}](#feed-options){.hoverxref .tooltip .reference
            .internal}

```{=html}
<!-- -->
```

[[write]{.pre}]{.sig-name .descname}[(]{.sig-paren}*[[self]{.pre}]{.n}*, *[[data]{.pre}]{.n}*[)]{.sig-paren}[¶](#write "Permalink to this definition"){.headerlink}

:   Process and write data ([[`bytes`{.xref .py .py-class .docutils
    .literal
    .notranslate}]{.pre}](https://docs.python.org/3/library/stdtypes.html#bytes "(in Python v3.12)"){.reference
    .external} or [[`memoryview`{.xref .py .py-class .docutils .literal
    .notranslate}]{.pre}](https://docs.python.org/3/library/stdtypes.html#memoryview "(in Python v3.12)"){.reference
    .external}) into the plugin's target file. It must return number of
    bytes written.

```{=html}
<!-- -->
```

[[close]{.pre}]{.sig-name .descname}[(]{.sig-paren}*[[self]{.pre}]{.n}*[)]{.sig-paren}[¶](#close "Permalink to this definition"){.headerlink}

:   Close the target file object.

To pass a parameter to your plugin, use [[feed options]{.std
.std-ref}](#feed-options){.hoverxref .tooltip .reference .internal}. You
can then access those parameters from the [`__init__`{.docutils .literal
.notranslate}]{.pre} method of your plugin.
:::
:::

::: {#settings .section}
#### Settings[¶](#settings "Permalink to this heading"){.headerlink}

These are the settings used for configuring the feed exports:

-   [[`FEEDS`{.xref .std .std-setting .docutils .literal
    .notranslate}]{.pre}](#std-setting-FEEDS){.hoverxref .tooltip
    .reference .internal} (mandatory)

-   [[`FEED_EXPORT_ENCODING`{.xref .std .std-setting .docutils .literal
    .notranslate}]{.pre}](#std-setting-FEED_EXPORT_ENCODING){.hoverxref
    .tooltip .reference .internal}

-   [[`FEED_STORE_EMPTY`{.xref .std .std-setting .docutils .literal
    .notranslate}]{.pre}](#std-setting-FEED_STORE_EMPTY){.hoverxref
    .tooltip .reference .internal}

-   [[`FEED_EXPORT_FIELDS`{.xref .std .std-setting .docutils .literal
    .notranslate}]{.pre}](#std-setting-FEED_EXPORT_FIELDS){.hoverxref
    .tooltip .reference .internal}

-   [[`FEED_EXPORT_INDENT`{.xref .std .std-setting .docutils .literal
    .notranslate}]{.pre}](#std-setting-FEED_EXPORT_INDENT){.hoverxref
    .tooltip .reference .internal}

-   [[`FEED_STORAGES`{.xref .std .std-setting .docutils .literal
    .notranslate}]{.pre}](#std-setting-FEED_STORAGES){.hoverxref
    .tooltip .reference .internal}

-   [[`FEED_STORAGE_FTP_ACTIVE`{.xref .std .std-setting .docutils
    .literal
    .notranslate}]{.pre}](#std-setting-FEED_STORAGE_FTP_ACTIVE){.hoverxref
    .tooltip .reference .internal}

-   [[`FEED_STORAGE_S3_ACL`{.xref .std .std-setting .docutils .literal
    .notranslate}]{.pre}](#std-setting-FEED_STORAGE_S3_ACL){.hoverxref
    .tooltip .reference .internal}

-   [[`FEED_EXPORTERS`{.xref .std .std-setting .docutils .literal
    .notranslate}]{.pre}](#std-setting-FEED_EXPORTERS){.hoverxref
    .tooltip .reference .internal}

-   [[`FEED_EXPORT_BATCH_ITEM_COUNT`{.xref .std .std-setting .docutils
    .literal
    .notranslate}]{.pre}](#std-setting-FEED_EXPORT_BATCH_ITEM_COUNT){.hoverxref
    .tooltip .reference .internal}

::: {#feeds .section}
[]{#std-setting-FEEDS}

##### FEEDS[¶](#feeds "Permalink to this heading"){.headerlink}

::: versionadded
[New in version 2.1.]{.versionmodified .added}
:::

Default: [`{}`{.docutils .literal .notranslate}]{.pre}

A dictionary in which every key is a feed URI (or a
[[`pathlib.Path`{.xref .py .py-class .docutils .literal
.notranslate}]{.pre}](https://docs.python.org/3/library/pathlib.html#pathlib.Path "(in Python v3.12)"){.reference
.external} object) and each value is a nested dictionary containing
configuration parameters for the specific feed.

This setting is required for enabling the feed export feature.

See [[Storage backends]{.std
.std-ref}](#topics-feed-storage-backends){.hoverxref .tooltip .reference
.internal} for supported URI schemes.

For instance:

::: {.highlight-default .notranslate}
::: highlight
    {
        'items.json': {
            'format': 'json',
            'encoding': 'utf8',
            'store_empty': False,
            'item_classes': [MyItemClass1, 'myproject.items.MyItemClass2'],
            'fields': None,
            'indent': 4,
            'item_export_kwargs': {
               'export_empty_fields': True,
            },
        },
        '/home/user/documents/items.xml': {
            'format': 'xml',
            'fields': ['name', 'price'],
            'item_filter': MyCustomFilter1,
            'encoding': 'latin1',
            'indent': 8,
        },
        pathlib.Path('items.csv.gz'): {
            'format': 'csv',
            'fields': ['price', 'name'],
            'item_filter': 'myproject.filters.MyCustomFilter2',
            'postprocessing': [MyPlugin1, 'scrapy.extensions.postprocessing.GzipPlugin'],
            'gzip_compresslevel': 5,
        },
    }
:::
:::

The following is a list of the accepted keys and the setting that is
used as a fallback value if that key is not provided for a specific feed
definition:

-   [`format`{.docutils .literal .notranslate}]{.pre}: the
    [[serialization format]{.std
    .std-ref}](#topics-feed-format){.hoverxref .tooltip .reference
    .internal}.

    This setting is mandatory, there is no fallback value.

-   [`batch_item_count`{.docutils .literal .notranslate}]{.pre}: falls
    back to [[`FEED_EXPORT_BATCH_ITEM_COUNT`{.xref .std .std-setting
    .docutils .literal
    .notranslate}]{.pre}](#std-setting-FEED_EXPORT_BATCH_ITEM_COUNT){.hoverxref
    .tooltip .reference .internal}.

    ::: versionadded
    [New in version 2.3.0.]{.versionmodified .added}
    :::

-   [`encoding`{.docutils .literal .notranslate}]{.pre}: falls back to
    [[`FEED_EXPORT_ENCODING`{.xref .std .std-setting .docutils .literal
    .notranslate}]{.pre}](#std-setting-FEED_EXPORT_ENCODING){.hoverxref
    .tooltip .reference .internal}.

-   [`fields`{.docutils .literal .notranslate}]{.pre}: falls back to
    [[`FEED_EXPORT_FIELDS`{.xref .std .std-setting .docutils .literal
    .notranslate}]{.pre}](#std-setting-FEED_EXPORT_FIELDS){.hoverxref
    .tooltip .reference .internal}.

-   [`item_classes`{.docutils .literal .notranslate}]{.pre}: list of
    [[item classes]{.std .std-ref}](index.html#topics-items){.hoverxref
    .tooltip .reference .internal} to export.

    If undefined or empty, all items are exported.

    ::: versionadded
    [New in version 2.6.0.]{.versionmodified .added}
    :::

-   [`item_filter`{.docutils .literal .notranslate}]{.pre}: a [[filter
    class]{.std .std-ref}](#item-filter){.hoverxref .tooltip .reference
    .internal} to filter items to export.

    [[`ItemFilter`{.xref .py .py-class .docutils .literal
    .notranslate}]{.pre}](#scrapy.extensions.feedexport.ItemFilter "scrapy.extensions.feedexport.ItemFilter"){.reference
    .internal} is used be default.

    ::: versionadded
    [New in version 2.6.0.]{.versionmodified .added}
    :::

-   [`indent`{.docutils .literal .notranslate}]{.pre}: falls back to
    [[`FEED_EXPORT_INDENT`{.xref .std .std-setting .docutils .literal
    .notranslate}]{.pre}](#std-setting-FEED_EXPORT_INDENT){.hoverxref
    .tooltip .reference .internal}.

-   [`item_export_kwargs`{.docutils .literal .notranslate}]{.pre}:
    [[`dict`{.xref .py .py-class .docutils .literal
    .notranslate}]{.pre}](https://docs.python.org/3/library/stdtypes.html#dict "(in Python v3.12)"){.reference
    .external} with keyword arguments for the corresponding [[item
    exporter class]{.std
    .std-ref}](index.html#topics-exporters){.hoverxref .tooltip
    .reference .internal}.

    ::: versionadded
    [New in version 2.4.0.]{.versionmodified .added}
    :::

-   [`overwrite`{.docutils .literal .notranslate}]{.pre}: whether to
    overwrite the file if it already exists ([`True`{.docutils .literal
    .notranslate}]{.pre}) or append to its content ([`False`{.docutils
    .literal .notranslate}]{.pre}).

    The default value depends on the [[storage backend]{.std
    .std-ref}](#topics-feed-storage-backends){.hoverxref .tooltip
    .reference .internal}:

    -   [[Local filesystem]{.std
        .std-ref}](#topics-feed-storage-fs){.hoverxref .tooltip
        .reference .internal}: [`False`{.docutils .literal
        .notranslate}]{.pre}

    -   [[FTP]{.std .std-ref}](#topics-feed-storage-ftp){.hoverxref
        .tooltip .reference .internal}: [`True`{.docutils .literal
        .notranslate}]{.pre}

        ::: {.admonition .note}
        Note

        Some FTP servers may not support appending to files (the
        [`APPE`{.docutils .literal .notranslate}]{.pre} FTP command).
        :::

    -   [[S3]{.std .std-ref}](#topics-feed-storage-s3){.hoverxref
        .tooltip .reference .internal}: [`True`{.docutils .literal
        .notranslate}]{.pre} (appending [is not
        supported](https://forums.aws.amazon.com/message.jspa?messageID=540395){.reference
        .external})

    -   [[Google Cloud Storage (GCS)]{.std
        .std-ref}](#topics-feed-storage-gcs){.hoverxref .tooltip
        .reference .internal}: [`True`{.docutils .literal
        .notranslate}]{.pre} (appending is not supported)

    -   [[Standard output]{.std
        .std-ref}](#topics-feed-storage-stdout){.hoverxref .tooltip
        .reference .internal}: [`False`{.docutils .literal
        .notranslate}]{.pre} (overwriting is not supported)

    ::: versionadded
    [New in version 2.4.0.]{.versionmodified .added}
    :::

-   [`store_empty`{.docutils .literal .notranslate}]{.pre}: falls back
    to [[`FEED_STORE_EMPTY`{.xref .std .std-setting .docutils .literal
    .notranslate}]{.pre}](#std-setting-FEED_STORE_EMPTY){.hoverxref
    .tooltip .reference .internal}.

-   [`uri_params`{.docutils .literal .notranslate}]{.pre}: falls back to
    [[`FEED_URI_PARAMS`{.xref .std .std-setting .docutils .literal
    .notranslate}]{.pre}](#std-setting-FEED_URI_PARAMS){.hoverxref
    .tooltip .reference .internal}.

-   [`postprocessing`{.docutils .literal .notranslate}]{.pre}: list of
    [[plugins]{.std .std-ref}](#post-processing){.hoverxref .tooltip
    .reference .internal} to use for post-processing.

    The plugins will be used in the order of the list passed.

    ::: versionadded
    [New in version 2.6.0.]{.versionmodified .added}
    :::
:::

::: {#feed-export-encoding .section}
[]{#std-setting-FEED_EXPORT_ENCODING}

##### FEED_EXPORT_ENCODING[¶](#feed-export-encoding "Permalink to this heading"){.headerlink}

Default: [`None`{.docutils .literal .notranslate}]{.pre}

The encoding to be used for the feed.

If unset or set to [`None`{.docutils .literal .notranslate}]{.pre}
(default) it uses UTF-8 for everything except JSON output, which uses
safe numeric encoding ([`\uXXXX`{.docutils .literal .notranslate}]{.pre}
sequences) for historic reasons.

Use [`utf-8`{.docutils .literal .notranslate}]{.pre} if you want UTF-8
for JSON too.

::: versionchanged
[Changed in version 2.8: ]{.versionmodified .changed}The
[[`startproject`{.xref .std .std-command .docutils .literal
.notranslate}]{.pre}](index.html#std-command-startproject){.hoverxref
.tooltip .reference .internal} command now sets this setting to
[`utf-8`{.docutils .literal .notranslate}]{.pre} in the generated
[`settings.py`{.docutils .literal .notranslate}]{.pre} file.
:::
:::

::: {#feed-export-fields .section}
[]{#std-setting-FEED_EXPORT_FIELDS}

##### FEED_EXPORT_FIELDS[¶](#feed-export-fields "Permalink to this heading"){.headerlink}

Default: [`None`{.docutils .literal .notranslate}]{.pre}

Use the [`FEED_EXPORT_FIELDS`{.docutils .literal .notranslate}]{.pre}
setting to define the fields to export, their order and their output
names. See [[`BaseItemExporter.fields_to_export`{.xref .py .py-attr
.docutils .literal
.notranslate}]{.pre}](index.html#scrapy.exporters.BaseItemExporter.fields_to_export "scrapy.exporters.BaseItemExporter.fields_to_export"){.reference
.internal} for more information.
:::

::: {#feed-export-indent .section}
[]{#std-setting-FEED_EXPORT_INDENT}

##### FEED_EXPORT_INDENT[¶](#feed-export-indent "Permalink to this heading"){.headerlink}

Default: [`0`{.docutils .literal .notranslate}]{.pre}

Amount of spaces used to indent the output on each level. If
[`FEED_EXPORT_INDENT`{.docutils .literal .notranslate}]{.pre} is a
non-negative integer, then array elements and object members will be
pretty-printed with that indent level. An indent level of [`0`{.docutils
.literal .notranslate}]{.pre} (the default), or negative, will put each
item on a new line. [`None`{.docutils .literal .notranslate}]{.pre}
selects the most compact representation.

Currently implemented only by [[`JsonItemExporter`{.xref .py .py-class
.docutils .literal
.notranslate}]{.pre}](index.html#scrapy.exporters.JsonItemExporter "scrapy.exporters.JsonItemExporter"){.reference
.internal} and [[`XmlItemExporter`{.xref .py .py-class .docutils
.literal
.notranslate}]{.pre}](index.html#scrapy.exporters.XmlItemExporter "scrapy.exporters.XmlItemExporter"){.reference
.internal}, i.e. when you are exporting to [`.json`{.docutils .literal
.notranslate}]{.pre} or [`.xml`{.docutils .literal .notranslate}]{.pre}.
:::

::: {#feed-store-empty .section}
[]{#std-setting-FEED_STORE_EMPTY}

##### FEED_STORE_EMPTY[¶](#feed-store-empty "Permalink to this heading"){.headerlink}

Default: [`True`{.docutils .literal .notranslate}]{.pre}

Whether to export empty feeds (i.e. feeds with no items). If
[`False`{.docutils .literal .notranslate}]{.pre}, and there are no items
to export, no new files are created and existing files are not modified,
even if the [[overwrite feed option]{.std
.std-ref}](#feed-options){.hoverxref .tooltip .reference .internal} is
enabled.
:::

::: {#feed-storages .section}
[]{#std-setting-FEED_STORAGES}

##### FEED_STORAGES[¶](#feed-storages "Permalink to this heading"){.headerlink}

Default: [`{}`{.docutils .literal .notranslate}]{.pre}

A dict containing additional feed storage backends supported by your
project. The keys are URI schemes and the values are paths to storage
classes.
:::

::: {#feed-storage-ftp-active .section}
[]{#std-setting-FEED_STORAGE_FTP_ACTIVE}

##### FEED_STORAGE_FTP_ACTIVE[¶](#feed-storage-ftp-active "Permalink to this heading"){.headerlink}

Default: [`False`{.docutils .literal .notranslate}]{.pre}

Whether to use the active connection mode when exporting feeds to an FTP
server ([`True`{.docutils .literal .notranslate}]{.pre}) or use the
passive connection mode instead ([`False`{.docutils .literal
.notranslate}]{.pre}, default).

For information about FTP connection modes, see [What is the difference
between active and passive
FTP?](https://stackoverflow.com/a/1699163){.reference .external}.
:::

::: {#feed-storage-s3-acl .section}
[]{#std-setting-FEED_STORAGE_S3_ACL}

##### FEED_STORAGE_S3_ACL[¶](#feed-storage-s3-acl "Permalink to this heading"){.headerlink}

Default: [`''`{.docutils .literal .notranslate}]{.pre} (empty string)

A string containing a custom ACL for feeds exported to Amazon S3 by your
project.

For a complete list of available values, access the [Canned
ACL](https://docs.aws.amazon.com/AmazonS3/latest/dev/acl-overview.html#canned-acl){.reference
.external} section on Amazon S3 docs.
:::

::: {#feed-storages-base .section}
[]{#std-setting-FEED_STORAGES_BASE}

##### FEED_STORAGES_BASE[¶](#feed-storages-base "Permalink to this heading"){.headerlink}

Default:

::: {.highlight-python .notranslate}
::: highlight
    {
        "": "scrapy.extensions.feedexport.FileFeedStorage",
        "file": "scrapy.extensions.feedexport.FileFeedStorage",
        "stdout": "scrapy.extensions.feedexport.StdoutFeedStorage",
        "s3": "scrapy.extensions.feedexport.S3FeedStorage",
        "ftp": "scrapy.extensions.feedexport.FTPFeedStorage",
    }
:::
:::

A dict containing the built-in feed storage backends supported by
Scrapy. You can disable any of these backends by assigning
[`None`{.docutils .literal .notranslate}]{.pre} to their URI scheme in
[[`FEED_STORAGES`{.xref .std .std-setting .docutils .literal
.notranslate}]{.pre}](#std-setting-FEED_STORAGES){.hoverxref .tooltip
.reference .internal}. E.g., to disable the built-in FTP storage backend
(without replacement), place this in your [`settings.py`{.docutils
.literal .notranslate}]{.pre}:

::: {.highlight-python .notranslate}
::: highlight
    FEED_STORAGES = {
        "ftp": None,
    }
:::
:::
:::

::: {#feed-exporters .section}
[]{#std-setting-FEED_EXPORTERS}

##### FEED_EXPORTERS[¶](#feed-exporters "Permalink to this heading"){.headerlink}

Default: [`{}`{.docutils .literal .notranslate}]{.pre}

A dict containing additional exporters supported by your project. The
keys are serialization formats and the values are paths to [[Item
exporter]{.std .std-ref}](index.html#topics-exporters){.hoverxref
.tooltip .reference .internal} classes.
:::

::: {#feed-exporters-base .section}
[]{#std-setting-FEED_EXPORTERS_BASE}

##### FEED_EXPORTERS_BASE[¶](#feed-exporters-base "Permalink to this heading"){.headerlink}

Default:

::: {.highlight-python .notranslate}
::: highlight
    {
        "json": "scrapy.exporters.JsonItemExporter",
        "jsonlines": "scrapy.exporters.JsonLinesItemExporter",
        "jsonl": "scrapy.exporters.JsonLinesItemExporter",
        "jl": "scrapy.exporters.JsonLinesItemExporter",
        "csv": "scrapy.exporters.CsvItemExporter",
        "xml": "scrapy.exporters.XmlItemExporter",
        "marshal": "scrapy.exporters.MarshalItemExporter",
        "pickle": "scrapy.exporters.PickleItemExporter",
    }
:::
:::

A dict containing the built-in feed exporters supported by Scrapy. You
can disable any of these exporters by assigning [`None`{.docutils
.literal .notranslate}]{.pre} to their serialization format in
[[`FEED_EXPORTERS`{.xref .std .std-setting .docutils .literal
.notranslate}]{.pre}](#std-setting-FEED_EXPORTERS){.hoverxref .tooltip
.reference .internal}. E.g., to disable the built-in CSV exporter
(without replacement), place this in your [`settings.py`{.docutils
.literal .notranslate}]{.pre}:

::: {.highlight-python .notranslate}
::: highlight
    FEED_EXPORTERS = {
        "csv": None,
    }
:::
:::
:::

::: {#feed-export-batch-item-count .section}
[]{#std-setting-FEED_EXPORT_BATCH_ITEM_COUNT}

##### FEED_EXPORT_BATCH_ITEM_COUNT[¶](#feed-export-batch-item-count "Permalink to this heading"){.headerlink}

::: versionadded
[New in version 2.3.0.]{.versionmodified .added}
:::

Default: [`0`{.docutils .literal .notranslate}]{.pre}

If assigned an integer number higher than [`0`{.docutils .literal
.notranslate}]{.pre}, Scrapy generates multiple output files storing up
to the specified number of items in each output file.

When generating multiple output files, you must use at least one of the
following placeholders in the feed URI to indicate how the different
output file names are generated:

-   [`%(batch_time)s`{.docutils .literal .notranslate}]{.pre} - gets
    replaced by a timestamp when the feed is being created (e.g.
    [`2020-03-28T14-45-08.237134`{.docutils .literal
    .notranslate}]{.pre})

-   [`%(batch_id)d`{.docutils .literal .notranslate}]{.pre} - gets
    replaced by the 1-based sequence number of the batch.

    Use [[printf-style string formatting]{.xref .std
    .std-ref}](https://docs.python.org/3/library/stdtypes.html#old-string-formatting "(in Python v3.12)"){.reference
    .external} to alter the number format. For example, to make the
    batch ID a 5-digit number by introducing leading zeroes as needed,
    use [`%(batch_id)05d`{.docutils .literal .notranslate}]{.pre} (e.g.
    [`3`{.docutils .literal .notranslate}]{.pre} becomes
    [`00003`{.docutils .literal .notranslate}]{.pre}, [`123`{.docutils
    .literal .notranslate}]{.pre} becomes [`00123`{.docutils .literal
    .notranslate}]{.pre}).

For instance, if your settings include:

::: {.highlight-python .notranslate}
::: highlight
    FEED_EXPORT_BATCH_ITEM_COUNT = 100
:::
:::

And your [[`crawl`{.xref .std .std-command .docutils .literal
.notranslate}]{.pre}](index.html#std-command-crawl){.hoverxref .tooltip
.reference .internal} command line is:

::: {.highlight-default .notranslate}
::: highlight
    scrapy crawl spidername -o "dirname/%(batch_id)d-filename%(batch_time)s.json"
:::
:::

The command line above can generate a directory tree like:

::: {.highlight-default .notranslate}
::: highlight
    ->projectname
    -->dirname
    --->1-filename2020-03-28T14-45-08.237134.json
    --->2-filename2020-03-28T14-45-09.148903.json
    --->3-filename2020-03-28T14-45-10.046092.json
:::
:::

Where the first and second files contain exactly 100 items. The last one
contains 100 items or fewer.
:::

::: {#feed-uri-params .section}
[]{#std-setting-FEED_URI_PARAMS}

##### FEED_URI_PARAMS[¶](#feed-uri-params "Permalink to this heading"){.headerlink}

Default: [`None`{.docutils .literal .notranslate}]{.pre}

A string with the import path of a function to set the parameters to
apply with [[printf-style string formatting]{.xref .std
.std-ref}](https://docs.python.org/3/library/stdtypes.html#old-string-formatting "(in Python v3.12)"){.reference
.external} to the feed URI.

The function signature should be as follows:

[[scrapy.extensions.feedexport.]{.pre}]{.sig-prename .descclassname}[[uri_params]{.pre}]{.sig-name .descname}[(]{.sig-paren}*[[params]{.pre}]{.n}*, *[[spider]{.pre}]{.n}*[)]{.sig-paren}[¶](#scrapy.extensions.feedexport.uri_params "Permalink to this definition"){.headerlink}

:   Return a [[`dict`{.xref .py .py-class .docutils .literal
    .notranslate}]{.pre}](https://docs.python.org/3/library/stdtypes.html#dict "(in Python v3.12)"){.reference
    .external} of key-value pairs to apply to the feed URI using
    [[printf-style string formatting]{.xref .std
    .std-ref}](https://docs.python.org/3/library/stdtypes.html#old-string-formatting "(in Python v3.12)"){.reference
    .external}.

    Parameters

    :   -   **params**
            ([*dict*](https://docs.python.org/3/library/stdtypes.html#dict "(in Python v3.12)"){.reference
            .external}) --

            default key-value pairs

            Specifically:

            -   [`batch_id`{.docutils .literal .notranslate}]{.pre}: ID
                of the file batch. See
                [[`FEED_EXPORT_BATCH_ITEM_COUNT`{.xref .std .std-setting
                .docutils .literal
                .notranslate}]{.pre}](#std-setting-FEED_EXPORT_BATCH_ITEM_COUNT){.hoverxref
                .tooltip .reference .internal}.

                If [[`FEED_EXPORT_BATCH_ITEM_COUNT`{.xref .std
                .std-setting .docutils .literal
                .notranslate}]{.pre}](#std-setting-FEED_EXPORT_BATCH_ITEM_COUNT){.hoverxref
                .tooltip .reference .internal} is [`0`{.docutils
                .literal .notranslate}]{.pre}, [`batch_id`{.docutils
                .literal .notranslate}]{.pre} is always [`1`{.docutils
                .literal .notranslate}]{.pre}.

                ::: versionadded
                [New in version 2.3.0.]{.versionmodified .added}
                :::

            -   [`batch_time`{.docutils .literal .notranslate}]{.pre}:
                UTC date and time, in ISO format with [`:`{.docutils
                .literal .notranslate}]{.pre} replaced with
                [`-`{.docutils .literal .notranslate}]{.pre}.

                See [[`FEED_EXPORT_BATCH_ITEM_COUNT`{.xref .std
                .std-setting .docutils .literal
                .notranslate}]{.pre}](#std-setting-FEED_EXPORT_BATCH_ITEM_COUNT){.hoverxref
                .tooltip .reference .internal}.

                ::: versionadded
                [New in version 2.3.0.]{.versionmodified .added}
                :::

            -   [`time`{.docutils .literal .notranslate}]{.pre}:
                [`batch_time`{.docutils .literal .notranslate}]{.pre},
                with microseconds set to [`0`{.docutils .literal
                .notranslate}]{.pre}.

        -   **spider**
            ([*scrapy.Spider*](index.html#scrapy.Spider "scrapy.Spider"){.reference
            .internal}) -- source spider of the feed items

    ::: {.admonition .caution}
    Caution

    The function should return a new dictionary, modifying the received
    [`params`{.docutils .literal .notranslate}]{.pre} in-place is
    deprecated.
    :::

For example, to include the [[`name`{.xref .py .py-attr .docutils
.literal
.notranslate}]{.pre}](index.html#scrapy.Spider.name "scrapy.Spider.name"){.reference
.internal} of the source spider in the feed URI:

1.  Define the following function somewhere in your project:

    ::: {.highlight-python .notranslate}
    ::: highlight
        # myproject/utils.py
        def uri_params(params, spider):
            return {**params, "spider_name": spider.name}
    :::
    :::

2.  Point [[`FEED_URI_PARAMS`{.xref .std .std-setting .docutils .literal
    .notranslate}]{.pre}](#std-setting-FEED_URI_PARAMS){.hoverxref
    .tooltip .reference .internal} to that function in your settings:

    ::: {.highlight-python .notranslate}
    ::: highlight
        # myproject/settings.py
        FEED_URI_PARAMS = "myproject.utils.uri_params"
    :::
    :::

3.  Use [`%(spider_name)s`{.docutils .literal .notranslate}]{.pre} in
    your feed URI:

    ::: {.highlight-default .notranslate}
    ::: highlight
        scrapy crawl <spider_name> -o "%(spider_name)s.jsonl"
    :::
    :::
:::
:::
:::

[]{#document-topics/request-response}

::: {#module-scrapy.http .section}
[]{#requests-and-responses}[]{#topics-request-response}

### Requests and Responses[¶](#module-scrapy.http "Permalink to this heading"){.headerlink}

Scrapy uses [[`Request`{.xref .py .py-class .docutils .literal
.notranslate}]{.pre}](#scrapy.http.Request "scrapy.http.Request"){.reference
.internal} and [[`Response`{.xref .py .py-class .docutils .literal
.notranslate}]{.pre}](#scrapy.http.Response "scrapy.http.Response"){.reference
.internal} objects for crawling web sites.

Typically, [[`Request`{.xref .py .py-class .docutils .literal
.notranslate}]{.pre}](#scrapy.http.Request "scrapy.http.Request"){.reference
.internal} objects are generated in the spiders and pass across the
system until they reach the Downloader, which executes the request and
returns a [[`Response`{.xref .py .py-class .docutils .literal
.notranslate}]{.pre}](#scrapy.http.Response "scrapy.http.Response"){.reference
.internal} object which travels back to the spider that issued the
request.

Both [[`Request`{.xref .py .py-class .docutils .literal
.notranslate}]{.pre}](#scrapy.http.Request "scrapy.http.Request"){.reference
.internal} and [[`Response`{.xref .py .py-class .docutils .literal
.notranslate}]{.pre}](#scrapy.http.Response "scrapy.http.Response"){.reference
.internal} classes have subclasses which add functionality not required
in the base classes. These are described below in [[Request
subclasses]{.std
.std-ref}](#topics-request-response-ref-request-subclasses){.hoverxref
.tooltip .reference .internal} and [[Response subclasses]{.std
.std-ref}](#topics-request-response-ref-response-subclasses){.hoverxref
.tooltip .reference .internal}.

::: {#request-objects .section}
#### Request objects[¶](#request-objects "Permalink to this heading"){.headerlink}

*[class]{.pre}[ ]{.w}*[[scrapy.http.]{.pre}]{.sig-prename .descclassname}[[Request]{.pre}]{.sig-name .descname}[(]{.sig-paren}*[[\*]{.pre}]{.o}[[args]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[[[Any]{.pre}](https://docs.python.org/3/library/typing.html#typing.Any "(in Python v3.12)"){.reference .external}]{.n}*, *[[\*\*]{.pre}]{.o}[[kwargs]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[[[Any]{.pre}](https://docs.python.org/3/library/typing.html#typing.Any "(in Python v3.12)"){.reference .external}]{.n}*[)]{.sig-paren}[[[\[source\]]{.pre}]{.viewcode-link}](_modules/scrapy/http/request.html#Request){.reference .internal}[¶](#scrapy.http.Request "Permalink to this definition"){.headerlink}

:   Represents an HTTP request, which is usually generated in a Spider
    and executed by the Downloader, thus generating a [[`Response`{.xref
    .py .py-class .docutils .literal
    .notranslate}]{.pre}](#scrapy.http.Response "scrapy.http.Response"){.reference
    .internal}.

    Parameters

    :   -   **url**
            ([*str*](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.12)"){.reference
            .external}) --

            the URL of this request

            If the URL is invalid, a [[`ValueError`{.xref .py .py-exc
            .docutils .literal
            .notranslate}]{.pre}](https://docs.python.org/3/library/exceptions.html#ValueError "(in Python v3.12)"){.reference
            .external} exception is raised.

        -   **callback**
            ([*collections.abc.Callable*](https://docs.python.org/3/library/collections.abc.html#collections.abc.Callable "(in Python v3.12)"){.reference
            .external}) --

            the function that will be called with the response of this
            request (once it's downloaded) as its first parameter.

            In addition to a function, the following values are
            supported:

            -   [`None`{.docutils .literal .notranslate}]{.pre}
                (default), which indicates that the spider's
                [[`parse()`{.xref .py .py-meth .docutils .literal
                .notranslate}]{.pre}](index.html#scrapy.Spider.parse "scrapy.Spider.parse"){.reference
                .internal} method must be used.

            -   [[`NO_CALLBACK()`{.xref .py .py-func .docutils .literal
                .notranslate}]{.pre}](#scrapy.http.request.NO_CALLBACK "scrapy.http.request.NO_CALLBACK"){.reference
                .internal}

            For more information, see [[Passing additional data to
            callback functions]{.std
            .std-ref}](#topics-request-response-ref-request-callback-arguments){.hoverxref
            .tooltip .reference .internal}.

            ::: {.admonition .note}
            Note

            If exceptions are raised during processing,
            [`errback`{.docutils .literal .notranslate}]{.pre} is called
            instead.
            :::

        -   **method**
            ([*str*](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.12)"){.reference
            .external}) -- the HTTP method of this request. Defaults to
            [`'GET'`{.docutils .literal .notranslate}]{.pre}.

        -   **meta**
            ([*dict*](https://docs.python.org/3/library/stdtypes.html#dict "(in Python v3.12)"){.reference
            .external}) -- the initial values for the
            [[`Request.meta`{.xref .py .py-attr .docutils .literal
            .notranslate}]{.pre}](#scrapy.http.Request.meta "scrapy.http.Request.meta"){.reference
            .internal} attribute. If given, the dict passed in this
            parameter will be shallow copied.

        -   **body**
            ([*bytes*](https://docs.python.org/3/library/stdtypes.html#bytes "(in Python v3.12)"){.reference
            .external} *or*
            [*str*](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.12)"){.reference
            .external}) -- the request body. If a string is passed, then
            it's encoded as bytes using the [`encoding`{.docutils
            .literal .notranslate}]{.pre} passed (which defaults to
            [`utf-8`{.docutils .literal .notranslate}]{.pre}). If
            [`body`{.docutils .literal .notranslate}]{.pre} is not
            given, an empty bytes object is stored. Regardless of the
            type of this argument, the final value stored will be a
            bytes object (never a string or [`None`{.docutils .literal
            .notranslate}]{.pre}).

        -   **headers**
            ([*dict*](https://docs.python.org/3/library/stdtypes.html#dict "(in Python v3.12)"){.reference
            .external}) --

            the headers of this request. The dict values can be strings
            (for single valued headers) or lists (for multi-valued
            headers). If [`None`{.docutils .literal .notranslate}]{.pre}
            is passed as value, the HTTP header will not be sent at all.

            > <div>
            >
            > ::: {.admonition .caution}
            > Caution
            >
            > Cookies set via the [`Cookie`{.docutils .literal
            > .notranslate}]{.pre} header are not considered by the
            > [[CookiesMiddleware]{.std
            > .std-ref}](index.html#cookies-mw){.hoverxref .tooltip
            > .reference .internal}. If you need to set cookies for a
            > request, use the [`Request.cookies`{.xref .py .py-class
            > .docutils .literal .notranslate}]{.pre} parameter. This is
            > a known current limitation that is being worked on.
            > :::
            >
            > </div>

        -   **cookies**
            ([*dict*](https://docs.python.org/3/library/stdtypes.html#dict "(in Python v3.12)"){.reference
            .external} *or*
            [*list*](https://docs.python.org/3/library/stdtypes.html#list "(in Python v3.12)"){.reference
            .external}) --

            the request cookies. These can be sent in two forms.

            1.  Using a dict:

            ::: {.highlight-python .notranslate}
            ::: highlight
                request_with_cookies = Request(
                    url="http://www.example.com",
                    cookies={"currency": "USD", "country": "UY"},
                )
            :::
            :::

            2.  Using a list of dicts:

            ::: {.highlight-python .notranslate}
            ::: highlight
                request_with_cookies = Request(
                    url="http://www.example.com",
                    cookies=[
                        {
                            "name": "currency",
                            "value": "USD",
                            "domain": "example.com",
                            "path": "/currency",
                        },
                    ],
                )
            :::
            :::

            The latter form allows for customizing the
            [`domain`{.docutils .literal .notranslate}]{.pre} and
            [`path`{.docutils .literal .notranslate}]{.pre} attributes
            of the cookie. This is only useful if the cookies are saved
            for later requests.

            []{#std-reqmeta-dont_merge_cookies .target}

            When some site returns cookies (in a response) those are
            stored in the cookies for that domain and will be sent again
            in future requests. That's the typical behaviour of any
            regular web browser.

            Note that setting the [[`dont_merge_cookies`{.xref .std
            .std-reqmeta .docutils .literal
            .notranslate}]{.pre}](#std-reqmeta-dont_merge_cookies){.hoverxref
            .tooltip .reference .internal} key to [`True`{.docutils
            .literal .notranslate}]{.pre} in [`request.meta`{.xref .py
            .py-attr .docutils .literal .notranslate}]{.pre} causes
            custom cookies to be ignored.

            For more info see [[CookiesMiddleware]{.std
            .std-ref}](index.html#cookies-mw){.hoverxref .tooltip
            .reference .internal}.

            ::: {.admonition .caution}
            Caution

            Cookies set via the [`Cookie`{.docutils .literal
            .notranslate}]{.pre} header are not considered by the
            [[CookiesMiddleware]{.std
            .std-ref}](index.html#cookies-mw){.hoverxref .tooltip
            .reference .internal}. If you need to set cookies for a
            request, use the [`Request.cookies`{.xref .py .py-class
            .docutils .literal .notranslate}]{.pre} parameter. This is a
            known current limitation that is being worked on.
            :::

            ::: versionadded
            [New in version 2.6.0: ]{.versionmodified .added}Cookie
            values that are [[`bool`{.xref .py .py-class .docutils
            .literal
            .notranslate}]{.pre}](https://docs.python.org/3/library/functions.html#bool "(in Python v3.12)"){.reference
            .external}, [[`float`{.xref .py .py-class .docutils .literal
            .notranslate}]{.pre}](https://docs.python.org/3/library/functions.html#float "(in Python v3.12)"){.reference
            .external} or [[`int`{.xref .py .py-class .docutils .literal
            .notranslate}]{.pre}](https://docs.python.org/3/library/functions.html#int "(in Python v3.12)"){.reference
            .external} are casted to [[`str`{.xref .py .py-class
            .docutils .literal
            .notranslate}]{.pre}](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.12)"){.reference
            .external}.
            :::

        -   **encoding**
            ([*str*](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.12)"){.reference
            .external}) -- the encoding of this request (defaults to
            [`'utf-8'`{.docutils .literal .notranslate}]{.pre}). This
            encoding will be used to percent-encode the URL and to
            convert the body to bytes (if given as a string).

        -   **priority**
            ([*int*](https://docs.python.org/3/library/functions.html#int "(in Python v3.12)"){.reference
            .external}) -- the priority of this request (defaults to
            [`0`{.docutils .literal .notranslate}]{.pre}). The priority
            is used by the scheduler to define the order used to process
            requests. Requests with a higher priority value will execute
            earlier. Negative values are allowed in order to indicate
            relatively low-priority.

        -   **dont_filter**
            ([*bool*](https://docs.python.org/3/library/functions.html#bool "(in Python v3.12)"){.reference
            .external}) -- indicates that this request should not be
            filtered by the scheduler. This is used when you want to
            perform an identical request multiple times, to ignore the
            duplicates filter. Use it with care, or you will get into
            crawling loops. Default to [`False`{.docutils .literal
            .notranslate}]{.pre}.

        -   **errback**
            ([*collections.abc.Callable*](https://docs.python.org/3/library/collections.abc.html#collections.abc.Callable "(in Python v3.12)"){.reference
            .external}) --

            a function that will be called if any exception was raised
            while processing the request. This includes pages that
            failed with 404 HTTP errors and such. It receives a
            [[`Failure`{.xref .py .py-exc .docutils .literal
            .notranslate}]{.pre}](https://docs.twisted.org/en/stable/api/twisted.python.failure.Failure.html "(in Twisted)"){.reference
            .external} as first parameter. For more information, see
            [[Using errbacks to catch exceptions in request
            processing]{.std
            .std-ref}](#topics-request-response-ref-errbacks){.hoverxref
            .tooltip .reference .internal} below.

            ::: versionchanged
            [Changed in version 2.0: ]{.versionmodified .changed}The
            *callback* parameter is no longer required when the
            *errback* parameter is specified.
            :::

        -   **flags**
            ([*list*](https://docs.python.org/3/library/stdtypes.html#list "(in Python v3.12)"){.reference
            .external}) -- Flags sent to the request, can be used for
            logging or similar purposes.

        -   **cb_kwargs**
            ([*dict*](https://docs.python.org/3/library/stdtypes.html#dict "(in Python v3.12)"){.reference
            .external}) -- A dict with arbitrary data that will be
            passed as keyword arguments to the Request's callback.

    [[url]{.pre}]{.sig-name .descname}[¶](#scrapy.http.Request.url "Permalink to this definition"){.headerlink}

    :   A string containing the URL of this request. Keep in mind that
        this attribute contains the escaped URL, so it can differ from
        the URL passed in the [`__init__`{.docutils .literal
        .notranslate}]{.pre} method.

        This attribute is read-only. To change the URL of a Request use
        [[`replace()`{.xref .py .py-meth .docutils .literal
        .notranslate}]{.pre}](#scrapy.http.Request.replace "scrapy.http.Request.replace"){.reference
        .internal}.

    [[method]{.pre}]{.sig-name .descname}[¶](#scrapy.http.Request.method "Permalink to this definition"){.headerlink}

    :   A string representing the HTTP method in the request. This is
        guaranteed to be uppercase. Example: [`"GET"`{.docutils .literal
        .notranslate}]{.pre}, [`"POST"`{.docutils .literal
        .notranslate}]{.pre}, [`"PUT"`{.docutils .literal
        .notranslate}]{.pre}, etc

    [[headers]{.pre}]{.sig-name .descname}[¶](#scrapy.http.Request.headers "Permalink to this definition"){.headerlink}

    :   A dictionary-like object which contains the request headers.

    [[body]{.pre}]{.sig-name .descname}[¶](#scrapy.http.Request.body "Permalink to this definition"){.headerlink}

    :   The request body as bytes.

        This attribute is read-only. To change the body of a Request use
        [[`replace()`{.xref .py .py-meth .docutils .literal
        .notranslate}]{.pre}](#scrapy.http.Request.replace "scrapy.http.Request.replace"){.reference
        .internal}.

    [[meta]{.pre}]{.sig-name .descname}*[ ]{.w}[[=]{.pre}]{.p}[ ]{.w}[{}]{.pre}*[¶](#scrapy.http.Request.meta "Permalink to this definition"){.headerlink}

    :   <div>
        >
        > A dictionary of arbitrary metadata for the request.
        >
        > You may extend request metadata as you see fit.
        >
        > Request metadata can also be accessed through the
        > [[`meta`{.xref .py .py-attr .docutils .literal
        > .notranslate}]{.pre}](#scrapy.http.Response.meta "scrapy.http.Response.meta"){.reference
        > .internal} attribute of a response.
        >
        > To pass data from one spider callback to another, consider
        > using [[`cb_kwargs`{.xref .py .py-attr .docutils .literal
        > .notranslate}]{.pre}](#scrapy.http.Request.cb_kwargs "scrapy.http.Request.cb_kwargs"){.reference
        > .internal} instead. However, request metadata may be the right
        > choice in certain scenarios, such as to maintain some
        > debugging data across all follow-up requests (e.g. the source
        > URL).
        >
        > A common use of request metadata is to define request-specific
        > parameters for Scrapy components (extensions, middlewares,
        > etc.). For example, if you set [`dont_retry`{.docutils
        > .literal .notranslate}]{.pre} to [`True`{.docutils .literal
        > .notranslate}]{.pre}, [[`RetryMiddleware`{.xref .py .py-class
        > .docutils .literal
        > .notranslate}]{.pre}](index.html#scrapy.downloadermiddlewares.retry.RetryMiddleware "scrapy.downloadermiddlewares.retry.RetryMiddleware"){.reference
        > .internal} will never retry that request, even if it fails.
        > See [[Request.meta special keys]{.std
        > .std-ref}](#topics-request-meta){.hoverxref .tooltip
        > .reference .internal}.
        >
        > You may also use request metadata in your custom Scrapy
        > components, for example, to keep request state information
        > relevant to your component. For example,
        > [[`RetryMiddleware`{.xref .py .py-class .docutils .literal
        > .notranslate}]{.pre}](index.html#scrapy.downloadermiddlewares.retry.RetryMiddleware "scrapy.downloadermiddlewares.retry.RetryMiddleware"){.reference
        > .internal} uses the [`retry_times`{.docutils .literal
        > .notranslate}]{.pre} metadata key to keep track of how many
        > times a request has been retried so far.
        >
        > Copying all the metadata of a previous request into a new,
        > follow-up request in a spider callback is a bad practice,
        > because request metadata may include metadata set by Scrapy
        > components that is not meant to be copied into other requests.
        > For example, copying the [`retry_times`{.docutils .literal
        > .notranslate}]{.pre} metadata key into follow-up requests can
        > lower the amount of retries allowed for those follow-up
        > requests.
        >
        > You should only copy all request metadata from one request to
        > another if the new request is meant to replace the old
        > request, as is often the case when returning a request from a
        > [[downloader middleware]{.std
        > .std-ref}](index.html#topics-downloader-middleware){.hoverxref
        > .tooltip .reference .internal} method.
        >
        > Also mind that the [[`copy()`{.xref .py .py-meth .docutils
        > .literal
        > .notranslate}]{.pre}](#scrapy.http.Request.copy "scrapy.http.Request.copy"){.reference
        > .internal} and [[`replace()`{.xref .py .py-meth .docutils
        > .literal
        > .notranslate}]{.pre}](#scrapy.http.Request.replace "scrapy.http.Request.replace"){.reference
        > .internal} request methods [[shallow-copy]{.xref .std
        > .std-doc}](https://docs.python.org/3/library/copy.html "(in Python v3.12)"){.reference
        > .external} request metadata.
        >
        > </div>

    [[cb_kwargs]{.pre}]{.sig-name .descname}[¶](#scrapy.http.Request.cb_kwargs "Permalink to this definition"){.headerlink}

    :   A dictionary that contains arbitrary metadata for this request.
        Its contents will be passed to the Request's callback as keyword
        arguments. It is empty for new Requests, which means by default
        callbacks only get a [[`Response`{.xref .py .py-class .docutils
        .literal
        .notranslate}]{.pre}](#scrapy.http.Response "scrapy.http.Response"){.reference
        .internal} object as argument.

        This dict is [[shallow copied]{.xref .std
        .std-doc}](https://docs.python.org/3/library/copy.html "(in Python v3.12)"){.reference
        .external} when the request is cloned using the
        [`copy()`{.docutils .literal .notranslate}]{.pre} or
        [`replace()`{.docutils .literal .notranslate}]{.pre} methods,
        and can also be accessed, in your spider, from the
        [`response.cb_kwargs`{.docutils .literal .notranslate}]{.pre}
        attribute.

        In case of a failure to process the request, this dict can be
        accessed as [`failure.request.cb_kwargs`{.docutils .literal
        .notranslate}]{.pre} in the request's errback. For more
        information, see [[Accessing additional data in errback
        functions]{.std .std-ref}](#errback-cb-kwargs){.hoverxref
        .tooltip .reference .internal}.

    [[attributes]{.pre}]{.sig-name .descname}*[[:]{.pre}]{.p}[ ]{.w}[[Tuple]{.pre}](https://docs.python.org/3/library/typing.html#typing.Tuple "(in Python v3.12)"){.reference .external}[[\[]{.pre}]{.p}[[str]{.pre}](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.12)"){.reference .external}[[,]{.pre}]{.p}[ ]{.w}[[\...]{.pre}]{.p}[[\]]{.pre}]{.p}[ ]{.w}[[=]{.pre}]{.p}[ ]{.w}[(\'url\',]{.pre} [\'callback\',]{.pre} [\'method\',]{.pre} [\'headers\',]{.pre} [\'body\',]{.pre} [\'cookies\',]{.pre} [\'meta\',]{.pre} [\'encoding\',]{.pre} [\'priority\',]{.pre} [\'dont_filter\',]{.pre} [\'errback\',]{.pre} [\'flags\',]{.pre} [\'cb_kwargs\')]{.pre}*[¶](#scrapy.http.Request.attributes "Permalink to this definition"){.headerlink}

    :   A tuple of [[`str`{.xref .py .py-class .docutils .literal
        .notranslate}]{.pre}](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.12)"){.reference
        .external} objects containing the name of all public attributes
        of the class that are also keyword parameters of the
        [`__init__`{.docutils .literal .notranslate}]{.pre} method.

        Currently used by [[`Request.replace()`{.xref .py .py-meth
        .docutils .literal
        .notranslate}]{.pre}](#scrapy.http.Request.replace "scrapy.http.Request.replace"){.reference
        .internal}, [[`Request.to_dict()`{.xref .py .py-meth .docutils
        .literal
        .notranslate}]{.pre}](#scrapy.http.Request.to_dict "scrapy.http.Request.to_dict"){.reference
        .internal} and [[`request_from_dict()`{.xref .py .py-func
        .docutils .literal
        .notranslate}]{.pre}](#scrapy.utils.request.request_from_dict "scrapy.utils.request.request_from_dict"){.reference
        .internal}.

    [[copy]{.pre}]{.sig-name .descname}[(]{.sig-paren}[)]{.sig-paren}[[[\[source\]]{.pre}]{.viewcode-link}](_modules/scrapy/http/request.html#Request.copy){.reference .internal}[¶](#scrapy.http.Request.copy "Permalink to this definition"){.headerlink}

    :   Return a new Request which is a copy of this Request. See also:
        [[Passing additional data to callback functions]{.std
        .std-ref}](#topics-request-response-ref-request-callback-arguments){.hoverxref
        .tooltip .reference .internal}.

    [[replace]{.pre}]{.sig-name .descname}[(]{.sig-paren}[\[]{.optional}*[[url]{.pre}]{.n}*, *[[method]{.pre}]{.n}*, *[[headers]{.pre}]{.n}*, *[[body]{.pre}]{.n}*, *[[cookies]{.pre}]{.n}*, *[[meta]{.pre}]{.n}*, *[[flags]{.pre}]{.n}*, *[[encoding]{.pre}]{.n}*, *[[priority]{.pre}]{.n}*, *[[dont_filter]{.pre}]{.n}*, *[[callback]{.pre}]{.n}*, *[[errback]{.pre}]{.n}*, *[[cb_kwargs]{.pre}]{.n}*[\]]{.optional}[)]{.sig-paren}[[[\[source\]]{.pre}]{.viewcode-link}](_modules/scrapy/http/request.html#Request.replace){.reference .internal}[¶](#scrapy.http.Request.replace "Permalink to this definition"){.headerlink}

    :   Return a Request object with the same members, except for those
        members given new values by whichever keyword arguments are
        specified. The [[`Request.cb_kwargs`{.xref .py .py-attr
        .docutils .literal
        .notranslate}]{.pre}](#scrapy.http.Request.cb_kwargs "scrapy.http.Request.cb_kwargs"){.reference
        .internal} and [[`Request.meta`{.xref .py .py-attr .docutils
        .literal
        .notranslate}]{.pre}](#scrapy.http.Request.meta "scrapy.http.Request.meta"){.reference
        .internal} attributes are shallow copied by default (unless new
        values are given as arguments). See also [[Passing additional
        data to callback functions]{.std
        .std-ref}](#topics-request-response-ref-request-callback-arguments){.hoverxref
        .tooltip .reference .internal}.

    *[classmethod]{.pre}[ ]{.w}*[[from_curl]{.pre}]{.sig-name .descname}[(]{.sig-paren}*[[curl_command]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[[[str]{.pre}](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.12)"){.reference .external}]{.n}*, *[[ignore_unknown_options]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[[[bool]{.pre}](https://docs.python.org/3/library/functions.html#bool "(in Python v3.12)"){.reference .external}]{.n}[ ]{.w}[[=]{.pre}]{.o}[ ]{.w}[[True]{.pre}]{.default_value}*, *[[\*\*]{.pre}]{.o}[[kwargs]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[[[Any]{.pre}](https://docs.python.org/3/library/typing.html#typing.Any "(in Python v3.12)"){.reference .external}]{.n}*[)]{.sig-paren} [[→]{.sig-return-icon} [[RequestTypeVar]{.pre}]{.sig-return-typehint}]{.sig-return}[[[\[source\]]{.pre}]{.viewcode-link}](_modules/scrapy/http/request.html#Request.from_curl){.reference .internal}[¶](#scrapy.http.Request.from_curl "Permalink to this definition"){.headerlink}

    :   Create a Request object from a string containing a
        [cURL](https://curl.haxx.se/){.reference .external} command. It
        populates the HTTP method, the URL, the headers, the cookies and
        the body. It accepts the same arguments as the [[`Request`{.xref
        .py .py-class .docutils .literal
        .notranslate}]{.pre}](#scrapy.http.Request "scrapy.http.Request"){.reference
        .internal} class, taking preference and overriding the values of
        the same arguments contained in the cURL command.

        Unrecognized options are ignored by default. To raise an error
        when finding unknown options call this method by passing
        [`ignore_unknown_options=False`{.docutils .literal
        .notranslate}]{.pre}.

        ::: {.admonition .caution}
        Caution

        Using [[`from_curl()`{.xref .py .py-meth .docutils .literal
        .notranslate}]{.pre}](#scrapy.http.Request.from_curl "scrapy.http.Request.from_curl"){.reference
        .internal} from [[`Request`{.xref .py .py-class .docutils
        .literal
        .notranslate}]{.pre}](#scrapy.http.Request "scrapy.http.Request"){.reference
        .internal} subclasses, such as [[`JsonRequest`{.xref .py
        .py-class .docutils .literal
        .notranslate}]{.pre}](#scrapy.http.JsonRequest "scrapy.http.JsonRequest"){.reference
        .internal}, or [`XmlRpcRequest`{.xref .py .py-class .docutils
        .literal .notranslate}]{.pre}, as well as having [[downloader
        middlewares]{.std
        .std-ref}](index.html#topics-downloader-middleware){.hoverxref
        .tooltip .reference .internal} and [[spider middlewares]{.std
        .std-ref}](index.html#topics-spider-middleware){.hoverxref
        .tooltip .reference .internal} enabled, such as
        [[`DefaultHeadersMiddleware`{.xref .py .py-class .docutils
        .literal
        .notranslate}]{.pre}](index.html#scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware "scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware"){.reference
        .internal}, [[`UserAgentMiddleware`{.xref .py .py-class
        .docutils .literal
        .notranslate}]{.pre}](index.html#scrapy.downloadermiddlewares.useragent.UserAgentMiddleware "scrapy.downloadermiddlewares.useragent.UserAgentMiddleware"){.reference
        .internal}, or [[`HttpCompressionMiddleware`{.xref .py .py-class
        .docutils .literal
        .notranslate}]{.pre}](index.html#scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware "scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware"){.reference
        .internal}, may modify the [[`Request`{.xref .py .py-class
        .docutils .literal
        .notranslate}]{.pre}](#scrapy.http.Request "scrapy.http.Request"){.reference
        .internal} object.
        :::

        To translate a cURL command into a Scrapy request, you may use
        [curl2scrapy](https://michael-shub.github.io/curl2scrapy/){.reference
        .external}.

    [[to_dict]{.pre}]{.sig-name .descname}[(]{.sig-paren}*[[\*]{.pre}]{.o}*, *[[spider]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[[[Optional]{.pre}](https://docs.python.org/3/library/typing.html#typing.Optional "(in Python v3.12)"){.reference .external}[[\[]{.pre}]{.p}[[Spider]{.pre}](index.html#scrapy.spiders.Spider "scrapy.spiders.Spider"){.reference .internal}[[\]]{.pre}]{.p}]{.n}[ ]{.w}[[=]{.pre}]{.o}[ ]{.w}[[None]{.pre}]{.default_value}*[)]{.sig-paren} [[→]{.sig-return-icon} [[[Dict]{.pre}](https://docs.python.org/3/library/typing.html#typing.Dict "(in Python v3.12)"){.reference .external}[[\[]{.pre}]{.p}[[str]{.pre}](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.12)"){.reference .external}[[,]{.pre}]{.p}[ ]{.w}[[Any]{.pre}](https://docs.python.org/3/library/typing.html#typing.Any "(in Python v3.12)"){.reference .external}[[\]]{.pre}]{.p}]{.sig-return-typehint}]{.sig-return}[[[\[source\]]{.pre}]{.viewcode-link}](_modules/scrapy/http/request.html#Request.to_dict){.reference .internal}[¶](#scrapy.http.Request.to_dict "Permalink to this definition"){.headerlink}

    :   Return a dictionary containing the Request's data.

        Use [[`request_from_dict()`{.xref .py .py-func .docutils
        .literal
        .notranslate}]{.pre}](#scrapy.utils.request.request_from_dict "scrapy.utils.request.request_from_dict"){.reference
        .internal} to convert back into a [`Request`{.xref .py .py-class
        .docutils .literal .notranslate}]{.pre} object.

        If a spider is given, this method will try to find out the name
        of the spider methods used as callback and errback and include
        them in the output dict, raising an exception if they cannot be
        found.

::: {#other-functions-related-to-requests .section}
##### Other functions related to requests[¶](#other-functions-related-to-requests "Permalink to this heading"){.headerlink}

[[scrapy.http.request.]{.pre}]{.sig-prename .descclassname}[[NO_CALLBACK]{.pre}]{.sig-name .descname}[(]{.sig-paren}*[[\*]{.pre}]{.o}[[args]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[[[Any]{.pre}](https://docs.python.org/3/library/typing.html#typing.Any "(in Python v3.12)"){.reference .external}]{.n}*, *[[\*\*]{.pre}]{.o}[[kwargs]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[[[Any]{.pre}](https://docs.python.org/3/library/typing.html#typing.Any "(in Python v3.12)"){.reference .external}]{.n}*[)]{.sig-paren} [[→]{.sig-return-icon} [[[NoReturn]{.pre}](https://docs.python.org/3/library/typing.html#typing.NoReturn "(in Python v3.12)"){.reference .external}]{.sig-return-typehint}]{.sig-return}[[[\[source\]]{.pre}]{.viewcode-link}](_modules/scrapy/http/request.html#NO_CALLBACK){.reference .internal}[¶](#scrapy.http.request.NO_CALLBACK "Permalink to this definition"){.headerlink}

:   When assigned to the [`callback`{.docutils .literal
    .notranslate}]{.pre} parameter of [[`Request`{.xref .py .py-class
    .docutils .literal
    .notranslate}]{.pre}](#scrapy.http.Request "scrapy.http.Request"){.reference
    .internal}, it indicates that the request is not meant to have a
    spider callback at all.

    For example:

    ::: {.highlight-python .notranslate}
    ::: highlight
        Request("https://example.com", callback=NO_CALLBACK)
    :::
    :::

    This value should be used by [[components]{.std
    .std-ref}](index.html#topics-components){.hoverxref .tooltip
    .reference .internal} that create and handle their own requests,
    e.g. through [`scrapy.core.engine.ExecutionEngine.download()`{.xref
    .py .py-meth .docutils .literal .notranslate}]{.pre}, so that
    downloader middlewares handling such requests can treat them
    differently from requests intended for the [[`parse()`{.xref .py
    .py-meth .docutils .literal
    .notranslate}]{.pre}](index.html#scrapy.Spider.parse "scrapy.Spider.parse"){.reference
    .internal} callback.

```{=html}
<!-- -->
```

[[scrapy.utils.request.]{.pre}]{.sig-prename .descclassname}[[request_from_dict]{.pre}]{.sig-name .descname}[(]{.sig-paren}*[[d]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[[[dict]{.pre}](https://docs.python.org/3/library/stdtypes.html#dict "(in Python v3.12)"){.reference .external}]{.n}*, *[[\*]{.pre}]{.o}*, *[[spider]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[[[Optional]{.pre}](https://docs.python.org/3/library/typing.html#typing.Optional "(in Python v3.12)"){.reference .external}[[\[]{.pre}]{.p}[[Spider]{.pre}](index.html#scrapy.spiders.Spider "scrapy.spiders.Spider"){.reference .internal}[[\]]{.pre}]{.p}]{.n}[ ]{.w}[[=]{.pre}]{.o}[ ]{.w}[[None]{.pre}]{.default_value}*[)]{.sig-paren} [[→]{.sig-return-icon} [[[Request]{.pre}](index.html#scrapy.http.Request "scrapy.http.request.Request"){.reference .internal}]{.sig-return-typehint}]{.sig-return}[[[\[source\]]{.pre}]{.viewcode-link}](_modules/scrapy/utils/request.html#request_from_dict){.reference .internal}[¶](#scrapy.utils.request.request_from_dict "Permalink to this definition"){.headerlink}

:   Create a [`Request`{.xref .py .py-class .docutils .literal
    .notranslate}]{.pre} object from a dict.

    If a spider is given, it will try to resolve the callbacks looking
    at the spider for methods with the same name.
:::

::: {#passing-additional-data-to-callback-functions .section}
[]{#topics-request-response-ref-request-callback-arguments}

##### Passing additional data to callback functions[¶](#passing-additional-data-to-callback-functions "Permalink to this heading"){.headerlink}

The callback of a request is a function that will be called when the
response of that request is downloaded. The callback function will be
called with the downloaded [[`Response`{.xref .py .py-class .docutils
.literal
.notranslate}]{.pre}](#scrapy.http.Response "scrapy.http.Response"){.reference
.internal} object as its first argument.

Example:

::: {.highlight-python .notranslate}
::: highlight
    def parse_page1(self, response):
        return scrapy.Request(
            "http://www.example.com/some_page.html", callback=self.parse_page2
        )


    def parse_page2(self, response):
        # this would log http://www.example.com/some_page.html
        self.logger.info("Visited %s", response.url)
:::
:::

In some cases you may be interested in passing arguments to those
callback functions so you can receive the arguments later, in the second
callback. The following example shows how to achieve this by using the
[[`Request.cb_kwargs`{.xref .py .py-attr .docutils .literal
.notranslate}]{.pre}](#scrapy.http.Request.cb_kwargs "scrapy.http.Request.cb_kwargs"){.reference
.internal} attribute:

::: {.highlight-python .notranslate}
::: highlight
    def parse(self, response):
        request = scrapy.Request(
            "http://www.example.com/index.html",
            callback=self.parse_page2,
            cb_kwargs=dict(main_url=response.url),
        )
        request.cb_kwargs["foo"] = "bar"  # add more arguments for the callback
        yield request


    def parse_page2(self, response, main_url, foo):
        yield dict(
            main_url=main_url,
            other_url=response.url,
            foo=foo,
        )
:::
:::

::: {.admonition .caution}
Caution

[[`Request.cb_kwargs`{.xref .py .py-attr .docutils .literal
.notranslate}]{.pre}](#scrapy.http.Request.cb_kwargs "scrapy.http.Request.cb_kwargs"){.reference
.internal} was introduced in version [`1.7`{.docutils .literal
.notranslate}]{.pre}. Prior to that, using [[`Request.meta`{.xref .py
.py-attr .docutils .literal
.notranslate}]{.pre}](#scrapy.http.Request.meta "scrapy.http.Request.meta"){.reference
.internal} was recommended for passing information around callbacks.
After [`1.7`{.docutils .literal .notranslate}]{.pre},
[[`Request.cb_kwargs`{.xref .py .py-attr .docutils .literal
.notranslate}]{.pre}](#scrapy.http.Request.cb_kwargs "scrapy.http.Request.cb_kwargs"){.reference
.internal} became the preferred way for handling user information,
leaving [[`Request.meta`{.xref .py .py-attr .docutils .literal
.notranslate}]{.pre}](#scrapy.http.Request.meta "scrapy.http.Request.meta"){.reference
.internal} for communication with components like middlewares and
extensions.
:::
:::

::: {#using-errbacks-to-catch-exceptions-in-request-processing .section}
[]{#topics-request-response-ref-errbacks}

##### Using errbacks to catch exceptions in request processing[¶](#using-errbacks-to-catch-exceptions-in-request-processing "Permalink to this heading"){.headerlink}

The errback of a request is a function that will be called when an
exception is raise while processing it.

It receives a [[`Failure`{.xref .py .py-exc .docutils .literal
.notranslate}]{.pre}](https://docs.twisted.org/en/stable/api/twisted.python.failure.Failure.html "(in Twisted)"){.reference
.external} as first parameter and can be used to track connection
establishment timeouts, DNS errors etc.

Here's an example spider logging all errors and catching some specific
errors if needed:

::: {.highlight-python .notranslate}
::: highlight
    import scrapy

    from scrapy.spidermiddlewares.httperror import HttpError
    from twisted.internet.error import DNSLookupError
    from twisted.internet.error import TimeoutError, TCPTimedOutError


    class ErrbackSpider(scrapy.Spider):
        name = "errback_example"
        start_urls = [
            "http://www.httpbin.org/",  # HTTP 200 expected
            "http://www.httpbin.org/status/404",  # Not found error
            "http://www.httpbin.org/status/500",  # server issue
            "http://www.httpbin.org:12345/",  # non-responding host, timeout expected
            "https://example.invalid/",  # DNS error expected
        ]

        def start_requests(self):
            for u in self.start_urls:
                yield scrapy.Request(
                    u,
                    callback=self.parse_httpbin,
                    errback=self.errback_httpbin,
                    dont_filter=True,
                )

        def parse_httpbin(self, response):
            self.logger.info("Got successful response from {}".format(response.url))
            # do something useful here...

        def errback_httpbin(self, failure):
            # log all failures
            self.logger.error(repr(failure))

            # in case you want to do something special for some errors,
            # you may need the failure's type:

            if failure.check(HttpError):
                # these exceptions come from HttpError spider middleware
                # you can get the non-200 response
                response = failure.value.response
                self.logger.error("HttpError on %s", response.url)

            elif failure.check(DNSLookupError):
                # this is the original request
                request = failure.request
                self.logger.error("DNSLookupError on %s", request.url)

            elif failure.check(TimeoutError, TCPTimedOutError):
                request = failure.request
                self.logger.error("TimeoutError on %s", request.url)
:::
:::
:::

::: {#accessing-additional-data-in-errback-functions .section}
[]{#errback-cb-kwargs}

##### Accessing additional data in errback functions[¶](#accessing-additional-data-in-errback-functions "Permalink to this heading"){.headerlink}

In case of a failure to process the request, you may be interested in
accessing arguments to the callback functions so you can process further
based on the arguments in the errback. The following example shows how
to achieve this by using [`Failure.request.cb_kwargs`{.docutils .literal
.notranslate}]{.pre}:

::: {.highlight-python .notranslate}
::: highlight
    def parse(self, response):
        request = scrapy.Request(
            "http://www.example.com/index.html",
            callback=self.parse_page2,
            errback=self.errback_page2,
            cb_kwargs=dict(main_url=response.url),
        )
        yield request


    def parse_page2(self, response, main_url):
        pass


    def errback_page2(self, failure):
        yield dict(
            main_url=failure.request.cb_kwargs["main_url"],
        )
:::
:::
:::

::: {#request-fingerprints .section}
[]{#id1}

##### Request fingerprints[¶](#request-fingerprints "Permalink to this heading"){.headerlink}

There are some aspects of scraping, such as filtering out duplicate
requests (see [[`DUPEFILTER_CLASS`{.xref .std .std-setting .docutils
.literal
.notranslate}]{.pre}](index.html#std-setting-DUPEFILTER_CLASS){.hoverxref
.tooltip .reference .internal}) or caching responses (see
[[`HTTPCACHE_POLICY`{.xref .std .std-setting .docutils .literal
.notranslate}]{.pre}](index.html#std-setting-HTTPCACHE_POLICY){.hoverxref
.tooltip .reference .internal}), where you need the ability to generate
a short, unique identifier from a [[`Request`{.xref .py .py-class
.docutils .literal
.notranslate}]{.pre}](#scrapy.http.Request "scrapy.http.Request"){.reference
.internal} object: a request fingerprint.

You often do not need to worry about request fingerprints, the default
request fingerprinter works for most projects.

However, there is no universal way to generate a unique identifier from
a request, because different situations require comparing requests
differently. For example, sometimes you may need to compare URLs
case-insensitively, include URL fragments, exclude certain URL query
parameters, include some or all headers, etc.

To change how request fingerprints are built for your requests, use the
[[`REQUEST_FINGERPRINTER_CLASS`{.xref .std .std-setting .docutils
.literal
.notranslate}]{.pre}](#std-setting-REQUEST_FINGERPRINTER_CLASS){.hoverxref
.tooltip .reference .internal} setting.

::: {#request-fingerprinter-class .section}
[]{#std-setting-REQUEST_FINGERPRINTER_CLASS}

###### REQUEST_FINGERPRINTER_CLASS[¶](#request-fingerprinter-class "Permalink to this heading"){.headerlink}

::: versionadded
[New in version 2.7.]{.versionmodified .added}
:::

Default: [[`scrapy.utils.request.RequestFingerprinter`{.xref .py
.py-class .docutils .literal
.notranslate}]{.pre}](#scrapy.utils.request.RequestFingerprinter "scrapy.utils.request.RequestFingerprinter"){.reference
.internal}

A [[request fingerprinter class]{.std
.std-ref}](#custom-request-fingerprinter){.hoverxref .tooltip .reference
.internal} or its import path.

*[class]{.pre}[ ]{.w}*[[scrapy.utils.request.]{.pre}]{.sig-prename .descclassname}[[RequestFingerprinter]{.pre}]{.sig-name .descname}[(]{.sig-paren}*[[crawler]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[[[Optional]{.pre}](https://docs.python.org/3/library/typing.html#typing.Optional "(in Python v3.12)"){.reference .external}[[\[]{.pre}]{.p}[[Crawler]{.pre}](index.html#scrapy.crawler.Crawler "scrapy.crawler.Crawler"){.reference .internal}[[\]]{.pre}]{.p}]{.n}[ ]{.w}[[=]{.pre}]{.o}[ ]{.w}[[None]{.pre}]{.default_value}*[)]{.sig-paren}[[[\[source\]]{.pre}]{.viewcode-link}](_modules/scrapy/utils/request.html#RequestFingerprinter){.reference .internal}[¶](#scrapy.utils.request.RequestFingerprinter "Permalink to this definition"){.headerlink}

:   Default fingerprinter.

    It takes into account a canonical version
    ([[`w3lib.url.canonicalize_url()`{.xref .py .py-func .docutils
    .literal
    .notranslate}]{.pre}](https://w3lib.readthedocs.io/en/latest/w3lib.html#w3lib.url.canonicalize_url "(in w3lib v2.1)"){.reference
    .external}) of [[`request.url`{.xref .py .py-attr .docutils .literal
    .notranslate}]{.pre}](#scrapy.http.Request.url "scrapy.http.Request.url"){.reference
    .internal} and the values of [[`request.method`{.xref .py .py-attr
    .docutils .literal
    .notranslate}]{.pre}](#scrapy.http.Request.method "scrapy.http.Request.method"){.reference
    .internal} and [[`request.body`{.xref .py .py-attr .docutils
    .literal
    .notranslate}]{.pre}](#scrapy.http.Request.body "scrapy.http.Request.body"){.reference
    .internal}. It then generates an
    [SHA1](https://en.wikipedia.org/wiki/SHA-1){.reference .external}
    hash.

    ::: {.admonition .seealso}
    See also

    [[`REQUEST_FINGERPRINTER_IMPLEMENTATION`{.xref .std .std-setting
    .docutils .literal
    .notranslate}]{.pre}](#std-setting-REQUEST_FINGERPRINTER_IMPLEMENTATION){.hoverxref
    .tooltip .reference .internal}.
    :::
:::

::: {#request-fingerprinter-implementation .section}
[]{#std-setting-REQUEST_FINGERPRINTER_IMPLEMENTATION}

###### REQUEST_FINGERPRINTER_IMPLEMENTATION[¶](#request-fingerprinter-implementation "Permalink to this heading"){.headerlink}

::: versionadded
[New in version 2.7.]{.versionmodified .added}
:::

Default: [`'2.6'`{.docutils .literal .notranslate}]{.pre}

Determines which request fingerprinting algorithm is used by the default
request fingerprinter class (see [[`REQUEST_FINGERPRINTER_CLASS`{.xref
.std .std-setting .docutils .literal
.notranslate}]{.pre}](#std-setting-REQUEST_FINGERPRINTER_CLASS){.hoverxref
.tooltip .reference .internal}).

Possible values are:

-   [`'2.6'`{.docutils .literal .notranslate}]{.pre} (default)

    This implementation uses the same request fingerprinting algorithm
    as Scrapy 2.6 and earlier versions.

    Even though this is the default value for backward compatibility
    reasons, it is a deprecated value.

-   [`'2.7'`{.docutils .literal .notranslate}]{.pre}

    This implementation was introduced in Scrapy 2.7 to fix an issue of
    the previous implementation.

    New projects should use this value. The [[`startproject`{.xref .std
    .std-command .docutils .literal
    .notranslate}]{.pre}](index.html#std-command-startproject){.hoverxref
    .tooltip .reference .internal} command sets this value in the
    generated [`settings.py`{.docutils .literal .notranslate}]{.pre}
    file.

If you are using the default value ([`'2.6'`{.docutils .literal
.notranslate}]{.pre}) for this setting, and you are using Scrapy
components where changing the request fingerprinting algorithm would
cause undesired results, you need to carefully decide when to change the
value of this setting, or switch the
[[`REQUEST_FINGERPRINTER_CLASS`{.xref .std .std-setting .docutils
.literal
.notranslate}]{.pre}](#std-setting-REQUEST_FINGERPRINTER_CLASS){.hoverxref
.tooltip .reference .internal} setting to a custom request fingerprinter
class that implements the 2.6 request fingerprinting algorithm and does
not log this warning ( [[Writing your own request fingerprinter]{.std
.std-ref}](#request-fingerprinter){.hoverxref .tooltip .reference
.internal} includes an example implementation of such a class).

Scenarios where changing the request fingerprinting algorithm may cause
undesired results include, for example, using the HTTP cache middleware
(see [[`HttpCacheMiddleware`{.xref .py .py-class .docutils .literal
.notranslate}]{.pre}](index.html#scrapy.downloadermiddlewares.httpcache.HttpCacheMiddleware "scrapy.downloadermiddlewares.httpcache.HttpCacheMiddleware"){.reference
.internal}). Changing the request fingerprinting algorithm would
invalidate the current cache, requiring you to redownload all requests
again.

Otherwise, set [[`REQUEST_FINGERPRINTER_IMPLEMENTATION`{.xref .std
.std-setting .docutils .literal
.notranslate}]{.pre}](#std-setting-REQUEST_FINGERPRINTER_IMPLEMENTATION){.hoverxref
.tooltip .reference .internal} to [`'2.7'`{.docutils .literal
.notranslate}]{.pre} in your settings to switch already to the request
fingerprinting implementation that will be the only request
fingerprinting implementation available in a future version of Scrapy,
and remove the deprecation warning triggered by using the default value
([`'2.6'`{.docutils .literal .notranslate}]{.pre}).
:::

::: {#writing-your-own-request-fingerprinter .section}
[]{#custom-request-fingerprinter}[]{#request-fingerprinter}

###### Writing your own request fingerprinter[¶](#writing-your-own-request-fingerprinter "Permalink to this heading"){.headerlink}

A request fingerprinter is a class that must implement the following
method:

[[fingerprint]{.pre}]{.sig-name .descname}[(]{.sig-paren}*[[self]{.pre}]{.n}*, *[[request]{.pre}]{.n}*[)]{.sig-paren}[¶](#fingerprint "Permalink to this definition"){.headerlink}

:   Return a [[`bytes`{.xref .py .py-class .docutils .literal
    .notranslate}]{.pre}](https://docs.python.org/3/library/stdtypes.html#bytes "(in Python v3.12)"){.reference
    .external} object that uniquely identifies *request*.

    See also [[Request fingerprint restrictions]{.std
    .std-ref}](#request-fingerprint-restrictions){.hoverxref .tooltip
    .reference .internal}.

    Parameters

    :   **request**
        ([*scrapy.http.Request*](index.html#scrapy.http.Request "scrapy.http.Request"){.reference
        .internal}) -- request to fingerprint

Additionally, it may also implement the following methods:

*[classmethod]{.pre}[ ]{.w}*[[from_crawler]{.pre}]{.sig-name .descname}[(]{.sig-paren}*[[cls]{.pre}]{.n}*, *[[crawler]{.pre}]{.n}*[)]{.sig-paren}

:   If present, this class method is called to create a request
    fingerprinter instance from a [[`Crawler`{.xref .py .py-class
    .docutils .literal
    .notranslate}]{.pre}](index.html#scrapy.crawler.Crawler "scrapy.crawler.Crawler"){.reference
    .internal} object. It must return a new instance of the request
    fingerprinter.

    *crawler* provides access to all Scrapy core components like
    settings and signals; it is a way for the request fingerprinter to
    access them and hook its functionality into Scrapy.

    Parameters

    :   **crawler** ([[`Crawler`{.xref .py .py-class .docutils .literal
        .notranslate}]{.pre}](index.html#scrapy.crawler.Crawler "scrapy.crawler.Crawler"){.reference
        .internal} object) -- crawler that uses this request
        fingerprinter

```{=html}
<!-- -->
```

*[classmethod]{.pre}[ ]{.w}*[[from_settings]{.pre}]{.sig-name .descname}[(]{.sig-paren}*[[cls]{.pre}]{.n}*, *[[settings]{.pre}]{.n}*[)]{.sig-paren}[¶](#from_settings "Permalink to this definition"){.headerlink}

:   If present, and [`from_crawler`{.docutils .literal
    .notranslate}]{.pre} is not defined, this class method is called to
    create a request fingerprinter instance from a [[`Settings`{.xref
    .py .py-class .docutils .literal
    .notranslate}]{.pre}](index.html#scrapy.settings.Settings "scrapy.settings.Settings"){.reference
    .internal} object. It must return a new instance of the request
    fingerprinter.

The [[`fingerprint()`{.xref .py .py-meth .docutils .literal
.notranslate}]{.pre}](#fingerprint "fingerprint"){.reference .internal}
method of the default request fingerprinter,
[[`scrapy.utils.request.RequestFingerprinter`{.xref .py .py-class
.docutils .literal
.notranslate}]{.pre}](#scrapy.utils.request.RequestFingerprinter "scrapy.utils.request.RequestFingerprinter"){.reference
.internal}, uses [[`scrapy.utils.request.fingerprint()`{.xref .py
.py-func .docutils .literal
.notranslate}]{.pre}](#scrapy.utils.request.fingerprint "scrapy.utils.request.fingerprint"){.reference
.internal} with its default parameters. For some common use cases you
can use [[`scrapy.utils.request.fingerprint()`{.xref .py .py-func
.docutils .literal
.notranslate}]{.pre}](#scrapy.utils.request.fingerprint "scrapy.utils.request.fingerprint"){.reference
.internal} as well in your [[`fingerprint()`{.xref .py .py-meth
.docutils .literal
.notranslate}]{.pre}](#fingerprint "fingerprint"){.reference .internal}
method implementation:

[[scrapy.utils.request.]{.pre}]{.sig-prename .descclassname}[[fingerprint]{.pre}]{.sig-name .descname}[(]{.sig-paren}*[[request]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[[[Request]{.pre}](index.html#scrapy.http.Request "scrapy.http.request.Request"){.reference .internal}]{.n}*, *[[\*]{.pre}]{.o}*, *[[include_headers]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[[[Optional]{.pre}](https://docs.python.org/3/library/typing.html#typing.Optional "(in Python v3.12)"){.reference .external}[[\[]{.pre}]{.p}[[Iterable]{.pre}](https://docs.python.org/3/library/typing.html#typing.Iterable "(in Python v3.12)"){.reference .external}[[\[]{.pre}]{.p}[[Union]{.pre}](https://docs.python.org/3/library/typing.html#typing.Union "(in Python v3.12)"){.reference .external}[[\[]{.pre}]{.p}[[bytes]{.pre}](https://docs.python.org/3/library/stdtypes.html#bytes "(in Python v3.12)"){.reference .external}[[,]{.pre}]{.p}[ ]{.w}[[str]{.pre}](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.12)"){.reference .external}[[\]]{.pre}]{.p}[[\]]{.pre}]{.p}[[\]]{.pre}]{.p}]{.n}[ ]{.w}[[=]{.pre}]{.o}[ ]{.w}[[None]{.pre}]{.default_value}*, *[[keep_fragments]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[[[bool]{.pre}](https://docs.python.org/3/library/functions.html#bool "(in Python v3.12)"){.reference .external}]{.n}[ ]{.w}[[=]{.pre}]{.o}[ ]{.w}[[False]{.pre}]{.default_value}*[)]{.sig-paren} [[→]{.sig-return-icon} [[[bytes]{.pre}](https://docs.python.org/3/library/stdtypes.html#bytes "(in Python v3.12)"){.reference .external}]{.sig-return-typehint}]{.sig-return}[[[\[source\]]{.pre}]{.viewcode-link}](_modules/scrapy/utils/request.html#fingerprint){.reference .internal}[¶](#scrapy.utils.request.fingerprint "Permalink to this definition"){.headerlink}

:   Return the request fingerprint.

    The request fingerprint is a hash that uniquely identifies the
    resource the request points to. For example, take the following two
    urls:

    [http://www.example.com/query?id=111&cat=222](http://www.example.com/query?id=111&cat=222){.reference
    .external}
    [http://www.example.com/query?cat=222&id=111](http://www.example.com/query?cat=222&id=111){.reference
    .external}

    Even though those are two different URLs both point to the same
    resource and are equivalent (i.e. they should return the same
    response).

    Another example are cookies used to store session ids. Suppose the
    following page is only accessible to authenticated users:

    [http://www.example.com/members/offers.html](http://www.example.com/members/offers.html){.reference
    .external}

    Lots of sites use a cookie to store the session id, which adds a
    random component to the HTTP Request and thus should be ignored when
    calculating the fingerprint.

    For this reason, request headers are ignored by default when
    calculating the fingerprint. If you want to include specific headers
    use the include_headers argument, which is a list of Request headers
    to include.

    Also, servers usually ignore fragments in urls when handling
    requests, so they are also ignored by default when calculating the
    fingerprint. If you want to include them, set the keep_fragments
    argument to True (for instance when handling requests with a
    headless browser).

For example, to take the value of a request header named
[`X-ID`{.docutils .literal .notranslate}]{.pre} into account:

::: {.highlight-python .notranslate}
::: highlight
    # my_project/settings.py
    REQUEST_FINGERPRINTER_CLASS = "my_project.utils.RequestFingerprinter"

    # my_project/utils.py
    from scrapy.utils.request import fingerprint


    class RequestFingerprinter:
        def fingerprint(self, request):
            return fingerprint(request, include_headers=["X-ID"])
:::
:::

You can also write your own fingerprinting logic from scratch.

However, if you do not use [[`scrapy.utils.request.fingerprint()`{.xref
.py .py-func .docutils .literal
.notranslate}]{.pre}](#scrapy.utils.request.fingerprint "scrapy.utils.request.fingerprint"){.reference
.internal}, make sure you use [[`WeakKeyDictionary`{.xref .py .py-class
.docutils .literal
.notranslate}]{.pre}](https://docs.python.org/3/library/weakref.html#weakref.WeakKeyDictionary "(in Python v3.12)"){.reference
.external} to cache request fingerprints:

-   Caching saves CPU by ensuring that fingerprints are calculated only
    once per request, and not once per Scrapy component that needs the
    fingerprint of a request.

-   Using [[`WeakKeyDictionary`{.xref .py .py-class .docutils .literal
    .notranslate}]{.pre}](https://docs.python.org/3/library/weakref.html#weakref.WeakKeyDictionary "(in Python v3.12)"){.reference
    .external} saves memory by ensuring that request objects do not stay
    in memory forever just because you have references to them in your
    cache dictionary.

For example, to take into account only the URL of a request, without any
prior URL canonicalization or taking the request method or body into
account:

::: {.highlight-python .notranslate}
::: highlight
    from hashlib import sha1
    from weakref import WeakKeyDictionary

    from scrapy.utils.python import to_bytes


    class RequestFingerprinter:
        cache = WeakKeyDictionary()

        def fingerprint(self, request):
            if request not in self.cache:
                fp = sha1()
                fp.update(to_bytes(request.url))
                self.cache[request] = fp.digest()
            return self.cache[request]
:::
:::

If you need to be able to override the request fingerprinting for
arbitrary requests from your spider callbacks, you may implement a
request fingerprinter that reads fingerprints from
[[`request.meta`{.xref .py .py-attr .docutils .literal
.notranslate}]{.pre}](#scrapy.http.Request.meta "scrapy.http.Request.meta"){.reference
.internal} when available, and then falls back to
[[`scrapy.utils.request.fingerprint()`{.xref .py .py-func .docutils
.literal
.notranslate}]{.pre}](#scrapy.utils.request.fingerprint "scrapy.utils.request.fingerprint"){.reference
.internal}. For example:

::: {.highlight-python .notranslate}
::: highlight
    from scrapy.utils.request import fingerprint


    class RequestFingerprinter:
        def fingerprint(self, request):
            if "fingerprint" in request.meta:
                return request.meta["fingerprint"]
            return fingerprint(request)
:::
:::

If you need to reproduce the same fingerprinting algorithm as Scrapy 2.6
without using the deprecated [`'2.6'`{.docutils .literal
.notranslate}]{.pre} value of the
[[`REQUEST_FINGERPRINTER_IMPLEMENTATION`{.xref .std .std-setting
.docutils .literal
.notranslate}]{.pre}](#std-setting-REQUEST_FINGERPRINTER_IMPLEMENTATION){.hoverxref
.tooltip .reference .internal} setting, use the following request
fingerprinter:

::: {.highlight-python .notranslate}
::: highlight
    from hashlib import sha1
    from weakref import WeakKeyDictionary

    from scrapy.utils.python import to_bytes
    from w3lib.url import canonicalize_url


    class RequestFingerprinter:
        cache = WeakKeyDictionary()

        def fingerprint(self, request):
            if request not in self.cache:
                fp = sha1()
                fp.update(to_bytes(request.method))
                fp.update(to_bytes(canonicalize_url(request.url)))
                fp.update(request.body or b"")
                self.cache[request] = fp.digest()
            return self.cache[request]
:::
:::
:::

::: {#request-fingerprint-restrictions .section}
[]{#id2}

###### Request fingerprint restrictions[¶](#request-fingerprint-restrictions "Permalink to this heading"){.headerlink}

Scrapy components that use request fingerprints may impose additional
restrictions on the format of the fingerprints that your [[request
fingerprinter]{.std .std-ref}](#custom-request-fingerprinter){.hoverxref
.tooltip .reference .internal} generates.

The following built-in Scrapy components have such restrictions:

-   [[`scrapy.extensions.httpcache.FilesystemCacheStorage`{.xref .py
    .py-class .docutils .literal
    .notranslate}]{.pre}](index.html#scrapy.extensions.httpcache.FilesystemCacheStorage "scrapy.extensions.httpcache.FilesystemCacheStorage"){.reference
    .internal} (default value of [[`HTTPCACHE_STORAGE`{.xref .std
    .std-setting .docutils .literal
    .notranslate}]{.pre}](index.html#std-setting-HTTPCACHE_STORAGE){.hoverxref
    .tooltip .reference .internal})

    Request fingerprints must be at least 1 byte long.

    Path and filename length limits of the file system of
    [[`HTTPCACHE_DIR`{.xref .std .std-setting .docutils .literal
    .notranslate}]{.pre}](index.html#std-setting-HTTPCACHE_DIR){.hoverxref
    .tooltip .reference .internal} also apply. Inside
    [[`HTTPCACHE_DIR`{.xref .std .std-setting .docutils .literal
    .notranslate}]{.pre}](index.html#std-setting-HTTPCACHE_DIR){.hoverxref
    .tooltip .reference .internal}, the following directory structure is
    created:

    -   [`Spider.name`{.xref .py .py-attr .docutils .literal
        .notranslate}]{.pre}

        -   first byte of a request fingerprint as hexadecimal

            -   fingerprint as hexadecimal

                -   filenames up to 16 characters long

    For example, if a request fingerprint is made of 20 bytes (default),
    [[`HTTPCACHE_DIR`{.xref .std .std-setting .docutils .literal
    .notranslate}]{.pre}](index.html#std-setting-HTTPCACHE_DIR){.hoverxref
    .tooltip .reference .internal} is
    [`'/home/user/project/.scrapy/httpcache'`{.docutils .literal
    .notranslate}]{.pre}, and the name of your spider is
    [`'my_spider'`{.docutils .literal .notranslate}]{.pre} your file
    system must support a file path like:

    ::: {.highlight-default .notranslate}
    ::: highlight
        /home/user/project/.scrapy/httpcache/my_spider/01/0123456789abcdef0123456789abcdef01234567/response_headers
    :::
    :::

-   [[`scrapy.extensions.httpcache.DbmCacheStorage`{.xref .py .py-class
    .docutils .literal
    .notranslate}]{.pre}](index.html#scrapy.extensions.httpcache.DbmCacheStorage "scrapy.extensions.httpcache.DbmCacheStorage"){.reference
    .internal}

    The underlying DBM implementation must support keys as long as twice
    the number of bytes of a request fingerprint, plus 5. For example,
    if a request fingerprint is made of 20 bytes (default),
    45-character-long keys must be supported.
:::
:::
:::

::: {#request-meta-special-keys .section}
[]{#topics-request-meta}

#### Request.meta special keys[¶](#request-meta-special-keys "Permalink to this heading"){.headerlink}

The [[`Request.meta`{.xref .py .py-attr .docutils .literal
.notranslate}]{.pre}](#scrapy.http.Request.meta "scrapy.http.Request.meta"){.reference
.internal} attribute can contain any arbitrary data, but there are some
special keys recognized by Scrapy and its built-in extensions.

Those are:

-   [[`bindaddress`{.xref .std .std-reqmeta .docutils .literal
    .notranslate}]{.pre}](#std-reqmeta-bindaddress){.hoverxref .tooltip
    .reference .internal}

-   [[`cookiejar`{.xref .std .std-reqmeta .docutils .literal
    .notranslate}]{.pre}](index.html#std-reqmeta-cookiejar){.hoverxref
    .tooltip .reference .internal}

-   [[`dont_cache`{.xref .std .std-reqmeta .docutils .literal
    .notranslate}]{.pre}](index.html#std-reqmeta-dont_cache){.hoverxref
    .tooltip .reference .internal}

-   [[`dont_merge_cookies`{.xref .std .std-reqmeta .docutils .literal
    .notranslate}]{.pre}](#std-reqmeta-dont_merge_cookies){.hoverxref
    .tooltip .reference .internal}

-   [[`dont_obey_robotstxt`{.xref .std .std-reqmeta .docutils .literal
    .notranslate}]{.pre}](index.html#std-reqmeta-dont_obey_robotstxt){.hoverxref
    .tooltip .reference .internal}

-   [[`dont_redirect`{.xref .std .std-reqmeta .docutils .literal
    .notranslate}]{.pre}](index.html#std-reqmeta-dont_redirect){.hoverxref
    .tooltip .reference .internal}

-   [[`dont_retry`{.xref .std .std-reqmeta .docutils .literal
    .notranslate}]{.pre}](index.html#std-reqmeta-dont_retry){.hoverxref
    .tooltip .reference .internal}

-   [[`download_fail_on_dataloss`{.xref .std .std-reqmeta .docutils
    .literal
    .notranslate}]{.pre}](#std-reqmeta-download_fail_on_dataloss){.hoverxref
    .tooltip .reference .internal}

-   [[`download_latency`{.xref .std .std-reqmeta .docutils .literal
    .notranslate}]{.pre}](#std-reqmeta-download_latency){.hoverxref
    .tooltip .reference .internal}

-   [[`download_maxsize`{.xref .std .std-reqmeta .docutils .literal
    .notranslate}]{.pre}](index.html#std-reqmeta-download_maxsize){.hoverxref
    .tooltip .reference .internal}

-   [[`download_timeout`{.xref .std .std-reqmeta .docutils .literal
    .notranslate}]{.pre}](#std-reqmeta-download_timeout){.hoverxref
    .tooltip .reference .internal}

-   [`ftp_password`{.docutils .literal .notranslate}]{.pre} (See
    [[`FTP_PASSWORD`{.xref .std .std-setting .docutils .literal
    .notranslate}]{.pre}](index.html#std-setting-FTP_PASSWORD){.hoverxref
    .tooltip .reference .internal} for more info)

-   [`ftp_user`{.docutils .literal .notranslate}]{.pre} (See
    [[`FTP_USER`{.xref .std .std-setting .docutils .literal
    .notranslate}]{.pre}](index.html#std-setting-FTP_USER){.hoverxref
    .tooltip .reference .internal} for more info)

-   [[`handle_httpstatus_all`{.xref .std .std-reqmeta .docutils .literal
    .notranslate}]{.pre}](index.html#std-reqmeta-handle_httpstatus_all){.hoverxref
    .tooltip .reference .internal}

-   [[`handle_httpstatus_list`{.xref .std .std-reqmeta .docutils
    .literal
    .notranslate}]{.pre}](index.html#std-reqmeta-handle_httpstatus_list){.hoverxref
    .tooltip .reference .internal}

-   [[`max_retry_times`{.xref .std .std-reqmeta .docutils .literal
    .notranslate}]{.pre}](#std-reqmeta-max_retry_times){.hoverxref
    .tooltip .reference .internal}

-   [[`proxy`{.xref .std .std-reqmeta .docutils .literal
    .notranslate}]{.pre}](index.html#std-reqmeta-proxy){.hoverxref
    .tooltip .reference .internal}

-   [[`redirect_reasons`{.xref .std .std-reqmeta .docutils .literal
    .notranslate}]{.pre}](index.html#std-reqmeta-redirect_reasons){.hoverxref
    .tooltip .reference .internal}

-   [[`redirect_urls`{.xref .std .std-reqmeta .docutils .literal
    .notranslate}]{.pre}](index.html#std-reqmeta-redirect_urls){.hoverxref
    .tooltip .reference .internal}

-   [[`referrer_policy`{.xref .std .std-reqmeta .docutils .literal
    .notranslate}]{.pre}](index.html#std-reqmeta-referrer_policy){.hoverxref
    .tooltip .reference .internal}

::: {#bindaddress .section}
[]{#std-reqmeta-bindaddress}

##### bindaddress[¶](#bindaddress "Permalink to this heading"){.headerlink}

The IP of the outgoing IP address to use for the performing the request.
:::

::: {#download-timeout .section}
[]{#std-reqmeta-download_timeout}

##### download_timeout[¶](#download-timeout "Permalink to this heading"){.headerlink}

The amount of time (in secs) that the downloader will wait before timing
out. See also: [[`DOWNLOAD_TIMEOUT`{.xref .std .std-setting .docutils
.literal
.notranslate}]{.pre}](index.html#std-setting-DOWNLOAD_TIMEOUT){.hoverxref
.tooltip .reference .internal}.
:::

::: {#download-latency .section}
[]{#std-reqmeta-download_latency}

##### download_latency[¶](#download-latency "Permalink to this heading"){.headerlink}

The amount of time spent to fetch the response, since the request has
been started, i.e. HTTP message sent over the network. This meta key
only becomes available when the response has been downloaded. While most
other meta keys are used to control Scrapy behavior, this one is
supposed to be read-only.
:::

::: {#download-fail-on-dataloss .section}
[]{#std-reqmeta-download_fail_on_dataloss}

##### download_fail_on_dataloss[¶](#download-fail-on-dataloss "Permalink to this heading"){.headerlink}

Whether or not to fail on broken responses. See:
[[`DOWNLOAD_FAIL_ON_DATALOSS`{.xref .std .std-setting .docutils .literal
.notranslate}]{.pre}](index.html#std-setting-DOWNLOAD_FAIL_ON_DATALOSS){.hoverxref
.tooltip .reference .internal}.
:::

::: {#max-retry-times .section}
[]{#std-reqmeta-max_retry_times}

##### max_retry_times[¶](#max-retry-times "Permalink to this heading"){.headerlink}

The meta key is used set retry times per request. When initialized, the
[[`max_retry_times`{.xref .std .std-reqmeta .docutils .literal
.notranslate}]{.pre}](#std-reqmeta-max_retry_times){.hoverxref .tooltip
.reference .internal} meta key takes higher precedence over the
[[`RETRY_TIMES`{.xref .std .std-setting .docutils .literal
.notranslate}]{.pre}](index.html#std-setting-RETRY_TIMES){.hoverxref
.tooltip .reference .internal} setting.
:::
:::

::: {#stopping-the-download-of-a-response .section}
[]{#topics-stop-response-download}

#### Stopping the download of a Response[¶](#stopping-the-download-of-a-response "Permalink to this heading"){.headerlink}

Raising a [[`StopDownload`{.xref .py .py-exc .docutils .literal
.notranslate}]{.pre}](index.html#scrapy.exceptions.StopDownload "scrapy.exceptions.StopDownload"){.reference
.internal} exception from a handler for the [[`bytes_received`{.xref .py
.py-class .docutils .literal
.notranslate}]{.pre}](index.html#scrapy.signals.bytes_received "scrapy.signals.bytes_received"){.reference
.internal} or [[`headers_received`{.xref .py .py-class .docutils
.literal
.notranslate}]{.pre}](index.html#scrapy.signals.headers_received "scrapy.signals.headers_received"){.reference
.internal} signals will stop the download of a given response. See the
following example:

::: {.highlight-python .notranslate}
::: highlight
    import scrapy


    class StopSpider(scrapy.Spider):
        name = "stop"
        start_urls = ["https://docs.scrapy.org/en/latest/"]

        @classmethod
        def from_crawler(cls, crawler):
            spider = super().from_crawler(crawler)
            crawler.signals.connect(
                spider.on_bytes_received, signal=scrapy.signals.bytes_received
            )
            return spider

        def parse(self, response):
            # 'last_chars' show that the full response was not downloaded
            yield {"len": len(response.text), "last_chars": response.text[-40:]}

        def on_bytes_received(self, data, request, spider):
            raise scrapy.exceptions.StopDownload(fail=False)
:::
:::

which produces the following output:

::: {.highlight-default .notranslate}
::: highlight
    2020-05-19 17:26:12 [scrapy.core.engine] INFO: Spider opened
    2020-05-19 17:26:12 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
    2020-05-19 17:26:13 [scrapy.core.downloader.handlers.http11] DEBUG: Download stopped for <GET https://docs.scrapy.org/en/latest/> from signal handler StopSpider.on_bytes_received
    2020-05-19 17:26:13 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://docs.scrapy.org/en/latest/> (referer: None) ['download_stopped']
    2020-05-19 17:26:13 [scrapy.core.scraper] DEBUG: Scraped from <200 https://docs.scrapy.org/en/latest/>
    {'len': 279, 'last_chars': 'dth, initial-scale=1.0">\n  \n  <title>Scr'}
    2020-05-19 17:26:13 [scrapy.core.engine] INFO: Closing spider (finished)
:::
:::

By default, resulting responses are handled by their corresponding
errbacks. To call their callback instead, like in this example, pass
[`fail=False`{.docutils .literal .notranslate}]{.pre} to the
[[`StopDownload`{.xref .py .py-exc .docutils .literal
.notranslate}]{.pre}](index.html#scrapy.exceptions.StopDownload "scrapy.exceptions.StopDownload"){.reference
.internal} exception.
:::

::: {#request-subclasses .section}
[]{#topics-request-response-ref-request-subclasses}

#### Request subclasses[¶](#request-subclasses "Permalink to this heading"){.headerlink}

Here is the list of built-in [[`Request`{.xref .py .py-class .docutils
.literal
.notranslate}]{.pre}](#scrapy.http.Request "scrapy.http.Request"){.reference
.internal} subclasses. You can also subclass it to implement your own
custom functionality.

::: {#formrequest-objects .section}
##### FormRequest objects[¶](#formrequest-objects "Permalink to this heading"){.headerlink}

The FormRequest class extends the base [[`Request`{.xref .py .py-class
.docutils .literal
.notranslate}]{.pre}](#scrapy.http.Request "scrapy.http.Request"){.reference
.internal} with functionality for dealing with HTML forms. It uses
[lxml.html forms](https://lxml.de/lxmlhtml.html#forms){.reference
.external} to pre-populate form fields with form data from
[[`Response`{.xref .py .py-class .docutils .literal
.notranslate}]{.pre}](#scrapy.http.Response "scrapy.http.Response"){.reference
.internal} objects.

*[class]{.pre}[ ]{.w}*[[scrapy.http.request.form.]{.pre}]{.sig-prename .descclassname}[[FormRequest]{.pre}]{.sig-name .descname}[¶](#scrapy.http.scrapy.http.request.form.FormRequest "Permalink to this definition"){.headerlink}

:   

```{=html}
<!-- -->
```

*[class]{.pre}[ ]{.w}*[[scrapy.http.]{.pre}]{.sig-prename .descclassname}[[FormRequest]{.pre}]{.sig-name .descname}[¶](#scrapy.http.scrapy.http.FormRequest "Permalink to this definition"){.headerlink}

:   

```{=html}
<!-- -->
```

*[class]{.pre}[ ]{.w}*[[scrapy.]{.pre}]{.sig-prename .descclassname}[[FormRequest]{.pre}]{.sig-name .descname}[(]{.sig-paren}*[[url]{.pre}]{.n}*[\[]{.optional}, *[[formdata]{.pre}]{.n}*, *[[\...]{.pre}]{.n}*[\]]{.optional}[)]{.sig-paren}[¶](#scrapy.http.scrapy.FormRequest "Permalink to this definition"){.headerlink}

:   The [`FormRequest`{.xref .py .py-class .docutils .literal
    .notranslate}]{.pre} class adds a new keyword parameter to the
    [`__init__`{.docutils .literal .notranslate}]{.pre} method. The
    remaining arguments are the same as for the [[`Request`{.xref .py
    .py-class .docutils .literal
    .notranslate}]{.pre}](#scrapy.http.Request "scrapy.http.Request"){.reference
    .internal} class and are not documented here.

    Parameters

    :   **formdata**
        ([*dict*](https://docs.python.org/3/library/stdtypes.html#dict "(in Python v3.12)"){.reference
        .external} *or*
        [*collections.abc.Iterable*](https://docs.python.org/3/library/collections.abc.html#collections.abc.Iterable "(in Python v3.12)"){.reference
        .external}) -- is a dictionary (or iterable of (key, value)
        tuples) containing HTML Form data which will be url-encoded and
        assigned to the body of the request.

    The [`FormRequest`{.xref .py .py-class .docutils .literal
    .notranslate}]{.pre} objects support the following class method in
    addition to the standard [[`Request`{.xref .py .py-class .docutils
    .literal
    .notranslate}]{.pre}](#scrapy.http.Request "scrapy.http.Request"){.reference
    .internal} methods:

    *[classmethod]{.pre}[ ]{.w}*[[FormRequest.]{.pre}]{.sig-prename .descclassname}[[from_response]{.pre}]{.sig-name .descname}[(]{.sig-paren}*[[response]{.pre}]{.n}*[\[]{.optional}, *[[formname=None]{.pre}]{.n}*, *[[formid=None]{.pre}]{.n}*, *[[formnumber=0]{.pre}]{.n}*, *[[formdata=None]{.pre}]{.n}*, *[[formxpath=None]{.pre}]{.n}*, *[[formcss=None]{.pre}]{.n}*, *[[clickdata=None]{.pre}]{.n}*, *[[dont_click=False]{.pre}]{.n}*, *[[\...]{.pre}]{.n}*[\]]{.optional}[)]{.sig-paren}[¶](#scrapy.http.scrapy.FormRequest.FormRequest.from_response "Permalink to this definition"){.headerlink}

    :   Returns a new [`FormRequest`{.xref .py .py-class .docutils
        .literal .notranslate}]{.pre} object with its form field values
        pre-populated with those found in the HTML [`<form>`{.docutils
        .literal .notranslate}]{.pre} element contained in the given
        response. For an example see [[Using FormRequest.from_response()
        to simulate a user login]{.std
        .std-ref}](#topics-request-response-ref-request-userlogin){.hoverxref
        .tooltip .reference .internal}.

        The policy is to automatically simulate a click, by default, on
        any form control that looks clickable, like a
        [`<input`{.docutils .literal .notranslate}]{.pre}` `{.docutils
        .literal .notranslate}[`type="submit">`{.docutils .literal
        .notranslate}]{.pre}. Even though this is quite convenient, and
        often the desired behaviour, sometimes it can cause problems
        which could be hard to debug. For example, when working with
        forms that are filled and/or submitted using javascript, the
        default [`from_response()`{.xref .py .py-meth .docutils .literal
        .notranslate}]{.pre} behaviour may not be the most appropriate.
        To disable this behaviour you can set the
        [`dont_click`{.docutils .literal .notranslate}]{.pre} argument
        to [`True`{.docutils .literal .notranslate}]{.pre}. Also, if you
        want to change the control clicked (instead of disabling it) you
        can also use the [`clickdata`{.docutils .literal
        .notranslate}]{.pre} argument.

        ::: {.admonition .caution}
        Caution

        Using this method with select elements which have leading or
        trailing whitespace in the option values will not work due to a
        [bug in
        lxml](https://bugs.launchpad.net/lxml/+bug/1665241){.reference
        .external}, which should be fixed in lxml 3.8 and above.
        :::

        Parameters

        :   -   **response** ([[`Response`{.xref .py .py-class .docutils
                .literal
                .notranslate}]{.pre}](#scrapy.http.Response "scrapy.http.Response"){.reference
                .internal} object) -- the response containing a HTML
                form which will be used to pre-populate the form fields

            -   **formname**
                ([*str*](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.12)"){.reference
                .external}) -- if given, the form with name attribute
                set to this value will be used.

            -   **formid**
                ([*str*](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.12)"){.reference
                .external}) -- if given, the form with id attribute set
                to this value will be used.

            -   **formxpath**
                ([*str*](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.12)"){.reference
                .external}) -- if given, the first form that matches the
                xpath will be used.

            -   **formcss**
                ([*str*](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.12)"){.reference
                .external}) -- if given, the first form that matches the
                css selector will be used.

            -   **formnumber**
                ([*int*](https://docs.python.org/3/library/functions.html#int "(in Python v3.12)"){.reference
                .external}) -- the number of form to use, when the
                response contains multiple forms. The first one (and
                also the default) is [`0`{.docutils .literal
                .notranslate}]{.pre}.

            -   **formdata**
                ([*dict*](https://docs.python.org/3/library/stdtypes.html#dict "(in Python v3.12)"){.reference
                .external}) -- fields to override in the form data. If a
                field was already present in the response
                [`<form>`{.docutils .literal .notranslate}]{.pre}
                element, its value is overridden by the one passed in
                this parameter. If a value passed in this parameter is
                [`None`{.docutils .literal .notranslate}]{.pre}, the
                field will not be included in the request, even if it
                was present in the response [`<form>`{.docutils .literal
                .notranslate}]{.pre} element.

            -   **clickdata**
                ([*dict*](https://docs.python.org/3/library/stdtypes.html#dict "(in Python v3.12)"){.reference
                .external}) -- attributes to lookup the control clicked.
                If it's not given, the form data will be submitted
                simulating a click on the first clickable element. In
                addition to html attributes, the control can be
                identified by its zero-based index relative to other
                submittable inputs inside the form, via the
                [`nr`{.docutils .literal .notranslate}]{.pre} attribute.

            -   **dont_click**
                ([*bool*](https://docs.python.org/3/library/functions.html#bool "(in Python v3.12)"){.reference
                .external}) -- If True, the form data will be submitted
                without clicking in any element.

        The other parameters of this class method are passed directly to
        the [`FormRequest`{.xref .py .py-class .docutils .literal
        .notranslate}]{.pre} [`__init__`{.docutils .literal
        .notranslate}]{.pre} method.
:::

::: {#request-usage-examples .section}
##### Request usage examples[¶](#request-usage-examples "Permalink to this heading"){.headerlink}

::: {#using-formrequest-to-send-data-via-http-post .section}
###### Using FormRequest to send data via HTTP POST[¶](#using-formrequest-to-send-data-via-http-post "Permalink to this heading"){.headerlink}

If you want to simulate a HTML Form POST in your spider and send a
couple of key-value fields, you can return a [`FormRequest`{.xref .py
.py-class .docutils .literal .notranslate}]{.pre} object (from your
spider) like this:

::: {.highlight-python .notranslate}
::: highlight
    return [
        FormRequest(
            url="http://www.example.com/post/action",
            formdata={"name": "John Doe", "age": "27"},
            callback=self.after_post,
        )
    ]
:::
:::
:::

::: {#using-formrequest-from-response-to-simulate-a-user-login .section}
[]{#topics-request-response-ref-request-userlogin}

###### Using FormRequest.from_response() to simulate a user login[¶](#using-formrequest-from-response-to-simulate-a-user-login "Permalink to this heading"){.headerlink}

It is usual for web sites to provide pre-populated form fields through
[`<input`{.docutils .literal .notranslate}]{.pre}` `{.docutils .literal
.notranslate}[`type="hidden">`{.docutils .literal .notranslate}]{.pre}
elements, such as session related data or authentication tokens (for
login pages). When scraping, you'll want these fields to be
automatically pre-populated and only override a couple of them, such as
the user name and password. You can use the
[`FormRequest.from_response()`{.xref .py .py-meth .docutils .literal
.notranslate}]{.pre} method for this job. Here's an example spider which
uses it:

::: {.highlight-python .notranslate}
::: highlight
    import scrapy


    def authentication_failed(response):
        # TODO: Check the contents of the response and return True if it failed
        # or False if it succeeded.
        pass


    class LoginSpider(scrapy.Spider):
        name = "example.com"
        start_urls = ["http://www.example.com/users/login.php"]

        def parse(self, response):
            return scrapy.FormRequest.from_response(
                response,
                formdata={"username": "john", "password": "secret"},
                callback=self.after_login,
            )

        def after_login(self, response):
            if authentication_failed(response):
                self.logger.error("Login failed")
                return

            # continue scraping with authenticated session...
:::
:::
:::
:::

::: {#jsonrequest .section}
##### JsonRequest[¶](#jsonrequest "Permalink to this heading"){.headerlink}

The JsonRequest class extends the base [[`Request`{.xref .py .py-class
.docutils .literal
.notranslate}]{.pre}](#scrapy.http.Request "scrapy.http.Request"){.reference
.internal} class with functionality for dealing with JSON requests.

*[class]{.pre}[ ]{.w}*[[scrapy.http.]{.pre}]{.sig-prename .descclassname}[[JsonRequest]{.pre}]{.sig-name .descname}[(]{.sig-paren}*[[url]{.pre}]{.n}*[\[]{.optional}, *[[\...]{.pre} [data]{.pre}]{.n}*, *[[dumps_kwargs]{.pre}]{.n}*[\]]{.optional}[)]{.sig-paren}[[[\[source\]]{.pre}]{.viewcode-link}](_modules/scrapy/http/request/json_request.html#JsonRequest){.reference .internal}[¶](#scrapy.http.JsonRequest "Permalink to this definition"){.headerlink}

:   The [[`JsonRequest`{.xref .py .py-class .docutils .literal
    .notranslate}]{.pre}](#scrapy.http.JsonRequest "scrapy.http.JsonRequest"){.reference
    .internal} class adds two new keyword parameters to the
    [`__init__`{.docutils .literal .notranslate}]{.pre} method. The
    remaining arguments are the same as for the [[`Request`{.xref .py
    .py-class .docutils .literal
    .notranslate}]{.pre}](#scrapy.http.Request "scrapy.http.Request"){.reference
    .internal} class and are not documented here.

    Using the [[`JsonRequest`{.xref .py .py-class .docutils .literal
    .notranslate}]{.pre}](#scrapy.http.JsonRequest "scrapy.http.JsonRequest"){.reference
    .internal} will set the [`Content-Type`{.docutils .literal
    .notranslate}]{.pre} header to [`application/json`{.docutils
    .literal .notranslate}]{.pre} and [`Accept`{.docutils .literal
    .notranslate}]{.pre} header to [`application/json,`{.docutils
    .literal .notranslate}]{.pre}` `{.docutils .literal
    .notranslate}[`text/javascript,`{.docutils .literal
    .notranslate}]{.pre}` `{.docutils .literal
    .notranslate}[`*/*;`{.docutils .literal
    .notranslate}]{.pre}` `{.docutils .literal
    .notranslate}[`q=0.01`{.docutils .literal .notranslate}]{.pre}

    Parameters

    :   -   **data**
            ([*object*](https://docs.python.org/3/library/functions.html#object "(in Python v3.12)"){.reference
            .external}) -- is any JSON serializable object that needs to
            be JSON encoded and assigned to body. if
            [[`Request.body`{.xref .py .py-attr .docutils .literal
            .notranslate}]{.pre}](#scrapy.http.Request.body "scrapy.http.Request.body"){.reference
            .internal} argument is provided this parameter will be
            ignored. if [[`Request.body`{.xref .py .py-attr .docutils
            .literal
            .notranslate}]{.pre}](#scrapy.http.Request.body "scrapy.http.Request.body"){.reference
            .internal} argument is not provided and data argument is
            provided [[`Request.method`{.xref .py .py-attr .docutils
            .literal
            .notranslate}]{.pre}](#scrapy.http.Request.method "scrapy.http.Request.method"){.reference
            .internal} will be set to [`'POST'`{.docutils .literal
            .notranslate}]{.pre} automatically.

        -   **dumps_kwargs**
            ([*dict*](https://docs.python.org/3/library/stdtypes.html#dict "(in Python v3.12)"){.reference
            .external}) -- Parameters that will be passed to underlying
            [[`json.dumps()`{.xref .py .py-func .docutils .literal
            .notranslate}]{.pre}](https://docs.python.org/3/library/json.html#json.dumps "(in Python v3.12)"){.reference
            .external} method which is used to serialize data into JSON
            format.

    [[attributes]{.pre}]{.sig-name .descname}*[[:]{.pre}]{.p}[ ]{.w}[[Tuple]{.pre}](https://docs.python.org/3/library/typing.html#typing.Tuple "(in Python v3.12)"){.reference .external}[[\[]{.pre}]{.p}[[str]{.pre}](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.12)"){.reference .external}[[,]{.pre}]{.p}[ ]{.w}[[\...]{.pre}]{.p}[[\]]{.pre}]{.p}[ ]{.w}[[=]{.pre}]{.p}[ ]{.w}[(\'url\',]{.pre} [\'callback\',]{.pre} [\'method\',]{.pre} [\'headers\',]{.pre} [\'body\',]{.pre} [\'cookies\',]{.pre} [\'meta\',]{.pre} [\'encoding\',]{.pre} [\'priority\',]{.pre} [\'dont_filter\',]{.pre} [\'errback\',]{.pre} [\'flags\',]{.pre} [\'cb_kwargs\',]{.pre} [\'dumps_kwargs\')]{.pre}*[¶](#scrapy.http.JsonRequest.attributes "Permalink to this definition"){.headerlink}

    :   A tuple of [[`str`{.xref .py .py-class .docutils .literal
        .notranslate}]{.pre}](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.12)"){.reference
        .external} objects containing the name of all public attributes
        of the class that are also keyword parameters of the
        [`__init__`{.docutils .literal .notranslate}]{.pre} method.

        Currently used by [[`Request.replace()`{.xref .py .py-meth
        .docutils .literal
        .notranslate}]{.pre}](#scrapy.http.Request.replace "scrapy.http.Request.replace"){.reference
        .internal}, [[`Request.to_dict()`{.xref .py .py-meth .docutils
        .literal
        .notranslate}]{.pre}](#scrapy.http.Request.to_dict "scrapy.http.Request.to_dict"){.reference
        .internal} and [[`request_from_dict()`{.xref .py .py-func
        .docutils .literal
        .notranslate}]{.pre}](#scrapy.utils.request.request_from_dict "scrapy.utils.request.request_from_dict"){.reference
        .internal}.
:::

::: {#jsonrequest-usage-example .section}
##### JsonRequest usage example[¶](#jsonrequest-usage-example "Permalink to this heading"){.headerlink}

Sending a JSON POST request with a JSON payload:

::: {.highlight-python .notranslate}
::: highlight
    data = {
        "name1": "value1",
        "name2": "value2",
    }
    yield JsonRequest(url="http://www.example.com/post/action", data=data)
:::
:::
:::
:::

::: {#response-objects .section}
#### Response objects[¶](#response-objects "Permalink to this heading"){.headerlink}

*[class]{.pre}[ ]{.w}*[[scrapy.http.]{.pre}]{.sig-prename .descclassname}[[Response]{.pre}]{.sig-name .descname}[(]{.sig-paren}*[[\*]{.pre}]{.o}[[args]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[[[Any]{.pre}](https://docs.python.org/3/library/typing.html#typing.Any "(in Python v3.12)"){.reference .external}]{.n}*, *[[\*\*]{.pre}]{.o}[[kwargs]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[[[Any]{.pre}](https://docs.python.org/3/library/typing.html#typing.Any "(in Python v3.12)"){.reference .external}]{.n}*[)]{.sig-paren}[[[\[source\]]{.pre}]{.viewcode-link}](_modules/scrapy/http/response.html#Response){.reference .internal}[¶](#scrapy.http.Response "Permalink to this definition"){.headerlink}

:   An object that represents an HTTP response, which is usually
    downloaded (by the Downloader) and fed to the Spiders for
    processing.

    Parameters

    :   -   **url**
            ([*str*](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.12)"){.reference
            .external}) -- the URL of this response

        -   **status**
            ([*int*](https://docs.python.org/3/library/functions.html#int "(in Python v3.12)"){.reference
            .external}) -- the HTTP status of the response. Defaults to
            [`200`{.docutils .literal .notranslate}]{.pre}.

        -   **headers**
            ([*dict*](https://docs.python.org/3/library/stdtypes.html#dict "(in Python v3.12)"){.reference
            .external}) -- the headers of this response. The dict values
            can be strings (for single valued headers) or lists (for
            multi-valued headers).

        -   **body**
            ([*bytes*](https://docs.python.org/3/library/stdtypes.html#bytes "(in Python v3.12)"){.reference
            .external}) -- the response body. To access the decoded text
            as a string, use [`response.text`{.docutils .literal
            .notranslate}]{.pre} from an encoding-aware [[Response
            subclass]{.std
            .std-ref}](#topics-request-response-ref-response-subclasses){.hoverxref
            .tooltip .reference .internal}, such as
            [[`TextResponse`{.xref .py .py-class .docutils .literal
            .notranslate}]{.pre}](#scrapy.http.TextResponse "scrapy.http.TextResponse"){.reference
            .internal}.

        -   **flags**
            ([*list*](https://docs.python.org/3/library/stdtypes.html#list "(in Python v3.12)"){.reference
            .external}) -- is a list containing the initial values for
            the [[`Response.flags`{.xref .py .py-attr .docutils .literal
            .notranslate}]{.pre}](#scrapy.http.Response.flags "scrapy.http.Response.flags"){.reference
            .internal} attribute. If given, the list will be shallow
            copied.

        -   **request** (*scrapy.Request*) -- the initial value of the
            [[`Response.request`{.xref .py .py-attr .docutils .literal
            .notranslate}]{.pre}](#scrapy.http.Response.request "scrapy.http.Response.request"){.reference
            .internal} attribute. This represents the [[`Request`{.xref
            .py .py-class .docutils .literal
            .notranslate}]{.pre}](#scrapy.http.Request "scrapy.http.Request"){.reference
            .internal} that generated this response.

        -   **certificate**
            ([*twisted.internet.ssl.Certificate*](https://docs.twisted.org/en/stable/api/twisted.internet.ssl.Certificate.html "(in Twisted)"){.reference
            .external}) -- an object representing the server's SSL
            certificate.

        -   **ip_address** ([[`ipaddress.IPv4Address`{.xref .py
            .py-class .docutils .literal
            .notranslate}]{.pre}](https://docs.python.org/3/library/ipaddress.html#ipaddress.IPv4Address "(in Python v3.12)"){.reference
            .external} or [[`ipaddress.IPv6Address`{.xref .py .py-class
            .docutils .literal
            .notranslate}]{.pre}](https://docs.python.org/3/library/ipaddress.html#ipaddress.IPv6Address "(in Python v3.12)"){.reference
            .external}) -- The IP address of the server from which the
            Response originated.

        -   **protocol** ([[`str`{.xref .py .py-class .docutils .literal
            .notranslate}]{.pre}](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.12)"){.reference
            .external}) -- The protocol that was used to download the
            response. For instance: "HTTP/1.0", "HTTP/1.1", "h2"

    ::: versionadded
    [New in version 2.0.0: ]{.versionmodified .added}The
    [`certificate`{.docutils .literal .notranslate}]{.pre} parameter.
    :::

    ::: versionadded
    [New in version 2.1.0: ]{.versionmodified .added}The
    [`ip_address`{.docutils .literal .notranslate}]{.pre} parameter.
    :::

    ::: versionadded
    [New in version 2.5.0: ]{.versionmodified .added}The
    [`protocol`{.docutils .literal .notranslate}]{.pre} parameter.
    :::

    [[url]{.pre}]{.sig-name .descname}[¶](#scrapy.http.Response.url "Permalink to this definition"){.headerlink}

    :   A string containing the URL of the response.

        This attribute is read-only. To change the URL of a Response use
        [[`replace()`{.xref .py .py-meth .docutils .literal
        .notranslate}]{.pre}](#scrapy.http.Response.replace "scrapy.http.Response.replace"){.reference
        .internal}.

    [[status]{.pre}]{.sig-name .descname}[¶](#scrapy.http.Response.status "Permalink to this definition"){.headerlink}

    :   An integer representing the HTTP status of the response.
        Example: [`200`{.docutils .literal .notranslate}]{.pre},
        [`404`{.docutils .literal .notranslate}]{.pre}.

    [[headers]{.pre}]{.sig-name .descname}[¶](#scrapy.http.Response.headers "Permalink to this definition"){.headerlink}

    :   A dictionary-like object which contains the response headers.
        Values can be accessed using [`get()`{.xref .py .py-meth
        .docutils .literal .notranslate}]{.pre} to return the first
        header value with the specified name or [`getlist()`{.xref .py
        .py-meth .docutils .literal .notranslate}]{.pre} to return all
        header values with the specified name. For example, this call
        will give you all cookies in the headers:

        ::: {.highlight-default .notranslate}
        ::: highlight
            response.headers.getlist('Set-Cookie')
        :::
        :::

    [[body]{.pre}]{.sig-name .descname}[¶](#scrapy.http.Response.body "Permalink to this definition"){.headerlink}

    :   The response body as bytes.

        If you want the body as a string, use
        [[`TextResponse.text`{.xref .py .py-attr .docutils .literal
        .notranslate}]{.pre}](#scrapy.http.TextResponse.text "scrapy.http.TextResponse.text"){.reference
        .internal} (only available in [[`TextResponse`{.xref .py
        .py-class .docutils .literal
        .notranslate}]{.pre}](#scrapy.http.TextResponse "scrapy.http.TextResponse"){.reference
        .internal} and subclasses).

        This attribute is read-only. To change the body of a Response
        use [[`replace()`{.xref .py .py-meth .docutils .literal
        .notranslate}]{.pre}](#scrapy.http.Response.replace "scrapy.http.Response.replace"){.reference
        .internal}.

    [[request]{.pre}]{.sig-name .descname}[¶](#scrapy.http.Response.request "Permalink to this definition"){.headerlink}

    :   The [[`Request`{.xref .py .py-class .docutils .literal
        .notranslate}]{.pre}](#scrapy.http.Request "scrapy.http.Request"){.reference
        .internal} object that generated this response. This attribute
        is assigned in the Scrapy engine, after the response and the
        request have passed through all [[Downloader Middlewares]{.std
        .std-ref}](index.html#topics-downloader-middleware){.hoverxref
        .tooltip .reference .internal}. In particular, this means that:

        -   HTTP redirections will create a new request from the request
            before redirection. It has the majority of the same metadata
            and original request attributes and gets assigned to the
            redirected response instead of the propagation of the
            original request.

        -   Response.request.url doesn't always equal Response.url

        -   This attribute is only available in the spider code, and in
            the [[Spider Middlewares]{.std
            .std-ref}](index.html#topics-spider-middleware){.hoverxref
            .tooltip .reference .internal}, but not in Downloader
            Middlewares (although you have the Request available there
            by other means) and handlers of the
            [[`response_downloaded`{.xref .std .std-signal .docutils
            .literal
            .notranslate}]{.pre}](index.html#std-signal-response_downloaded){.hoverxref
            .tooltip .reference .internal} signal.

    [[meta]{.pre}]{.sig-name .descname}[¶](#scrapy.http.Response.meta "Permalink to this definition"){.headerlink}

    :   A shortcut to the [[`Request.meta`{.xref .py .py-attr .docutils
        .literal
        .notranslate}]{.pre}](#scrapy.http.Request.meta "scrapy.http.Request.meta"){.reference
        .internal} attribute of the [[`Response.request`{.xref .py
        .py-attr .docutils .literal
        .notranslate}]{.pre}](#scrapy.http.Response.request "scrapy.http.Response.request"){.reference
        .internal} object (i.e. [`self.request.meta`{.docutils .literal
        .notranslate}]{.pre}).

        Unlike the [[`Response.request`{.xref .py .py-attr .docutils
        .literal
        .notranslate}]{.pre}](#scrapy.http.Response.request "scrapy.http.Response.request"){.reference
        .internal} attribute, the [[`Response.meta`{.xref .py .py-attr
        .docutils .literal
        .notranslate}]{.pre}](#scrapy.http.Response.meta "scrapy.http.Response.meta"){.reference
        .internal} attribute is propagated along redirects and retries,
        so you will get the original [[`Request.meta`{.xref .py .py-attr
        .docutils .literal
        .notranslate}]{.pre}](#scrapy.http.Request.meta "scrapy.http.Request.meta"){.reference
        .internal} sent from your spider.

        ::: {.admonition .seealso}
        See also

        [[`Request.meta`{.xref .py .py-attr .docutils .literal
        .notranslate}]{.pre}](#scrapy.http.Request.meta "scrapy.http.Request.meta"){.reference
        .internal} attribute
        :::

    [[cb_kwargs]{.pre}]{.sig-name .descname}[¶](#scrapy.http.Response.cb_kwargs "Permalink to this definition"){.headerlink}

    :   ::: versionadded
        [New in version 2.0.]{.versionmodified .added}
        :::

        A shortcut to the [[`Request.cb_kwargs`{.xref .py .py-attr
        .docutils .literal
        .notranslate}]{.pre}](#scrapy.http.Request.cb_kwargs "scrapy.http.Request.cb_kwargs"){.reference
        .internal} attribute of the [[`Response.request`{.xref .py
        .py-attr .docutils .literal
        .notranslate}]{.pre}](#scrapy.http.Response.request "scrapy.http.Response.request"){.reference
        .internal} object (i.e. [`self.request.cb_kwargs`{.docutils
        .literal .notranslate}]{.pre}).

        Unlike the [[`Response.request`{.xref .py .py-attr .docutils
        .literal
        .notranslate}]{.pre}](#scrapy.http.Response.request "scrapy.http.Response.request"){.reference
        .internal} attribute, the [[`Response.cb_kwargs`{.xref .py
        .py-attr .docutils .literal
        .notranslate}]{.pre}](#scrapy.http.Response.cb_kwargs "scrapy.http.Response.cb_kwargs"){.reference
        .internal} attribute is propagated along redirects and retries,
        so you will get the original [[`Request.cb_kwargs`{.xref .py
        .py-attr .docutils .literal
        .notranslate}]{.pre}](#scrapy.http.Request.cb_kwargs "scrapy.http.Request.cb_kwargs"){.reference
        .internal} sent from your spider.

        ::: {.admonition .seealso}
        See also

        [[`Request.cb_kwargs`{.xref .py .py-attr .docutils .literal
        .notranslate}]{.pre}](#scrapy.http.Request.cb_kwargs "scrapy.http.Request.cb_kwargs"){.reference
        .internal} attribute
        :::

    [[flags]{.pre}]{.sig-name .descname}[¶](#scrapy.http.Response.flags "Permalink to this definition"){.headerlink}

    :   A list that contains flags for this response. Flags are labels
        used for tagging Responses. For example: [`'cached'`{.docutils
        .literal .notranslate}]{.pre}, [`'redirected`{.docutils .literal
        .notranslate}]{.pre}', etc. And they're shown on the string
        representation of the Response (\_\_str\_\_ method) which is
        used by the engine for logging.

    [[certificate]{.pre}]{.sig-name .descname}[¶](#scrapy.http.Response.certificate "Permalink to this definition"){.headerlink}

    :   ::: versionadded
        [New in version 2.0.0.]{.versionmodified .added}
        :::

        A [[`twisted.internet.ssl.Certificate`{.xref .py .py-class
        .docutils .literal
        .notranslate}]{.pre}](https://docs.twisted.org/en/stable/api/twisted.internet.ssl.Certificate.html "(in Twisted)"){.reference
        .external} object representing the server's SSL certificate.

        Only populated for [`https`{.docutils .literal
        .notranslate}]{.pre} responses, [`None`{.docutils .literal
        .notranslate}]{.pre} otherwise.

    [[ip_address]{.pre}]{.sig-name .descname}[¶](#scrapy.http.Response.ip_address "Permalink to this definition"){.headerlink}

    :   ::: versionadded
        [New in version 2.1.0.]{.versionmodified .added}
        :::

        The IP address of the server from which the Response originated.

        This attribute is currently only populated by the HTTP 1.1
        download handler, i.e. for [`http(s)`{.docutils .literal
        .notranslate}]{.pre} responses. For other handlers,
        [[`ip_address`{.xref .py .py-attr .docutils .literal
        .notranslate}]{.pre}](#scrapy.http.Response.ip_address "scrapy.http.Response.ip_address"){.reference
        .internal} is always [`None`{.docutils .literal
        .notranslate}]{.pre}.

    [[protocol]{.pre}]{.sig-name .descname}[¶](#scrapy.http.Response.protocol "Permalink to this definition"){.headerlink}

    :   ::: versionadded
        [New in version 2.5.0.]{.versionmodified .added}
        :::

        The protocol that was used to download the response. For
        instance: "HTTP/1.0", "HTTP/1.1"

        This attribute is currently only populated by the HTTP download
        handlers, i.e. for [`http(s)`{.docutils .literal
        .notranslate}]{.pre} responses. For other handlers,
        [[`protocol`{.xref .py .py-attr .docutils .literal
        .notranslate}]{.pre}](#scrapy.http.Response.protocol "scrapy.http.Response.protocol"){.reference
        .internal} is always [`None`{.docutils .literal
        .notranslate}]{.pre}.

    [[attributes]{.pre}]{.sig-name .descname}*[[:]{.pre}]{.p}[ ]{.w}[[Tuple]{.pre}](https://docs.python.org/3/library/typing.html#typing.Tuple "(in Python v3.12)"){.reference .external}[[\[]{.pre}]{.p}[[str]{.pre}](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.12)"){.reference .external}[[,]{.pre}]{.p}[ ]{.w}[[\...]{.pre}]{.p}[[\]]{.pre}]{.p}[ ]{.w}[[=]{.pre}]{.p}[ ]{.w}[(\'url\',]{.pre} [\'status\',]{.pre} [\'headers\',]{.pre} [\'body\',]{.pre} [\'flags\',]{.pre} [\'request\',]{.pre} [\'certificate\',]{.pre} [\'ip_address\',]{.pre} [\'protocol\')]{.pre}*[¶](#scrapy.http.Response.attributes "Permalink to this definition"){.headerlink}

    :   A tuple of [[`str`{.xref .py .py-class .docutils .literal
        .notranslate}]{.pre}](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.12)"){.reference
        .external} objects containing the name of all public attributes
        of the class that are also keyword parameters of the
        [`__init__`{.docutils .literal .notranslate}]{.pre} method.

        Currently used by [[`Response.replace()`{.xref .py .py-meth
        .docutils .literal
        .notranslate}]{.pre}](#scrapy.http.Response.replace "scrapy.http.Response.replace"){.reference
        .internal}.

    [[copy]{.pre}]{.sig-name .descname}[(]{.sig-paren}[)]{.sig-paren}[[[\[source\]]{.pre}]{.viewcode-link}](_modules/scrapy/http/response.html#Response.copy){.reference .internal}[¶](#scrapy.http.Response.copy "Permalink to this definition"){.headerlink}

    :   Returns a new Response which is a copy of this Response.

    [[replace]{.pre}]{.sig-name .descname}[(]{.sig-paren}[\[]{.optional}*[[url]{.pre}]{.n}*, *[[status]{.pre}]{.n}*, *[[headers]{.pre}]{.n}*, *[[body]{.pre}]{.n}*, *[[request]{.pre}]{.n}*, *[[flags]{.pre}]{.n}*, *[[cls]{.pre}]{.n}*[\]]{.optional}[)]{.sig-paren}[[[\[source\]]{.pre}]{.viewcode-link}](_modules/scrapy/http/response.html#Response.replace){.reference .internal}[¶](#scrapy.http.Response.replace "Permalink to this definition"){.headerlink}

    :   Returns a Response object with the same members, except for
        those members given new values by whichever keyword arguments
        are specified. The attribute [[`Response.meta`{.xref .py
        .py-attr .docutils .literal
        .notranslate}]{.pre}](#scrapy.http.Response.meta "scrapy.http.Response.meta"){.reference
        .internal} is copied by default.

    [[urljoin]{.pre}]{.sig-name .descname}[(]{.sig-paren}*[[url]{.pre}]{.n}*[)]{.sig-paren}[[[\[source\]]{.pre}]{.viewcode-link}](_modules/scrapy/http/response.html#Response.urljoin){.reference .internal}[¶](#scrapy.http.Response.urljoin "Permalink to this definition"){.headerlink}

    :   Constructs an absolute url by combining the Response's
        [[`url`{.xref .py .py-attr .docutils .literal
        .notranslate}]{.pre}](#scrapy.http.Response.url "scrapy.http.Response.url"){.reference
        .internal} with a possible relative url.

        This is a wrapper over [[`urljoin()`{.xref .py .py-func
        .docutils .literal
        .notranslate}]{.pre}](https://docs.python.org/3/library/urllib.parse.html#urllib.parse.urljoin "(in Python v3.12)"){.reference
        .external}, it's merely an alias for making this call:

        ::: {.highlight-default .notranslate}
        ::: highlight
            urllib.parse.urljoin(response.url, url)
        :::
        :::

    [[follow]{.pre}]{.sig-name .descname}[(]{.sig-paren}*[[url]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[[[Union]{.pre}](https://docs.python.org/3/library/typing.html#typing.Union "(in Python v3.12)"){.reference .external}[[\[]{.pre}]{.p}[[str]{.pre}](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.12)"){.reference .external}[[,]{.pre}]{.p}[ ]{.w}[[Link]{.pre}](index.html#scrapy.link.Link "scrapy.link.Link"){.reference .internal}[[\]]{.pre}]{.p}]{.n}*, *[[callback]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[[[Optional]{.pre}](https://docs.python.org/3/library/typing.html#typing.Optional "(in Python v3.12)"){.reference .external}[[\[]{.pre}]{.p}[[Callable]{.pre}](https://docs.python.org/3/library/typing.html#typing.Callable "(in Python v3.12)"){.reference .external}[[\]]{.pre}]{.p}]{.n}[ ]{.w}[[=]{.pre}]{.o}[ ]{.w}[[None]{.pre}]{.default_value}*, *[[method]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[[[str]{.pre}](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.12)"){.reference .external}]{.n}[ ]{.w}[[=]{.pre}]{.o}[ ]{.w}[[\'GET\']{.pre}]{.default_value}*, *[[headers]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[[[Optional]{.pre}](https://docs.python.org/3/library/typing.html#typing.Optional "(in Python v3.12)"){.reference .external}[[\[]{.pre}]{.p}[[Union]{.pre}](https://docs.python.org/3/library/typing.html#typing.Union "(in Python v3.12)"){.reference .external}[[\[]{.pre}]{.p}[[Mapping]{.pre}](https://docs.python.org/3/library/typing.html#typing.Mapping "(in Python v3.12)"){.reference .external}[[\[]{.pre}]{.p}[AnyStr]{.pre}[[,]{.pre}]{.p}[ ]{.w}[[Any]{.pre}](https://docs.python.org/3/library/typing.html#typing.Any "(in Python v3.12)"){.reference .external}[[\]]{.pre}]{.p}[[,]{.pre}]{.p}[ ]{.w}[[Iterable]{.pre}](https://docs.python.org/3/library/typing.html#typing.Iterable "(in Python v3.12)"){.reference .external}[[\[]{.pre}]{.p}[[Tuple]{.pre}](https://docs.python.org/3/library/typing.html#typing.Tuple "(in Python v3.12)"){.reference .external}[[\[]{.pre}]{.p}[AnyStr]{.pre}[[,]{.pre}]{.p}[ ]{.w}[[Any]{.pre}](https://docs.python.org/3/library/typing.html#typing.Any "(in Python v3.12)"){.reference .external}[[\]]{.pre}]{.p}[[\]]{.pre}]{.p}[[\]]{.pre}]{.p}[[\]]{.pre}]{.p}]{.n}[ ]{.w}[[=]{.pre}]{.o}[ ]{.w}[[None]{.pre}]{.default_value}*, *[[body]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[[[Optional]{.pre}](https://docs.python.org/3/library/typing.html#typing.Optional "(in Python v3.12)"){.reference .external}[[\[]{.pre}]{.p}[[Union]{.pre}](https://docs.python.org/3/library/typing.html#typing.Union "(in Python v3.12)"){.reference .external}[[\[]{.pre}]{.p}[[bytes]{.pre}](https://docs.python.org/3/library/stdtypes.html#bytes "(in Python v3.12)"){.reference .external}[[,]{.pre}]{.p}[ ]{.w}[[str]{.pre}](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.12)"){.reference .external}[[\]]{.pre}]{.p}[[\]]{.pre}]{.p}]{.n}[ ]{.w}[[=]{.pre}]{.o}[ ]{.w}[[None]{.pre}]{.default_value}*, *[[cookies]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[[[Optional]{.pre}](https://docs.python.org/3/library/typing.html#typing.Optional "(in Python v3.12)"){.reference .external}[[\[]{.pre}]{.p}[[Union]{.pre}](https://docs.python.org/3/library/typing.html#typing.Union "(in Python v3.12)"){.reference .external}[[\[]{.pre}]{.p}[[dict]{.pre}](https://docs.python.org/3/library/stdtypes.html#dict "(in Python v3.12)"){.reference .external}[[,]{.pre}]{.p}[ ]{.w}[[List]{.pre}](https://docs.python.org/3/library/typing.html#typing.List "(in Python v3.12)"){.reference .external}[[\[]{.pre}]{.p}[[dict]{.pre}](https://docs.python.org/3/library/stdtypes.html#dict "(in Python v3.12)"){.reference .external}[[\]]{.pre}]{.p}[[\]]{.pre}]{.p}[[\]]{.pre}]{.p}]{.n}[ ]{.w}[[=]{.pre}]{.o}[ ]{.w}[[None]{.pre}]{.default_value}*, *[[meta]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[[[Optional]{.pre}](https://docs.python.org/3/library/typing.html#typing.Optional "(in Python v3.12)"){.reference .external}[[\[]{.pre}]{.p}[[Dict]{.pre}](https://docs.python.org/3/library/typing.html#typing.Dict "(in Python v3.12)"){.reference .external}[[\[]{.pre}]{.p}[[str]{.pre}](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.12)"){.reference .external}[[,]{.pre}]{.p}[ ]{.w}[[Any]{.pre}](https://docs.python.org/3/library/typing.html#typing.Any "(in Python v3.12)"){.reference .external}[[\]]{.pre}]{.p}[[\]]{.pre}]{.p}]{.n}[ ]{.w}[[=]{.pre}]{.o}[ ]{.w}[[None]{.pre}]{.default_value}*, *[[encoding]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[[[Optional]{.pre}](https://docs.python.org/3/library/typing.html#typing.Optional "(in Python v3.12)"){.reference .external}[[\[]{.pre}]{.p}[[str]{.pre}](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.12)"){.reference .external}[[\]]{.pre}]{.p}]{.n}[ ]{.w}[[=]{.pre}]{.o}[ ]{.w}[[\'utf-8\']{.pre}]{.default_value}*, *[[priority]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[[[int]{.pre}](https://docs.python.org/3/library/functions.html#int "(in Python v3.12)"){.reference .external}]{.n}[ ]{.w}[[=]{.pre}]{.o}[ ]{.w}[[0]{.pre}]{.default_value}*, *[[dont_filter]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[[[bool]{.pre}](https://docs.python.org/3/library/functions.html#bool "(in Python v3.12)"){.reference .external}]{.n}[ ]{.w}[[=]{.pre}]{.o}[ ]{.w}[[False]{.pre}]{.default_value}*, *[[errback]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[[[Optional]{.pre}](https://docs.python.org/3/library/typing.html#typing.Optional "(in Python v3.12)"){.reference .external}[[\[]{.pre}]{.p}[[Callable]{.pre}](https://docs.python.org/3/library/typing.html#typing.Callable "(in Python v3.12)"){.reference .external}[[\]]{.pre}]{.p}]{.n}[ ]{.w}[[=]{.pre}]{.o}[ ]{.w}[[None]{.pre}]{.default_value}*, *[[cb_kwargs]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[[[Optional]{.pre}](https://docs.python.org/3/library/typing.html#typing.Optional "(in Python v3.12)"){.reference .external}[[\[]{.pre}]{.p}[[Dict]{.pre}](https://docs.python.org/3/library/typing.html#typing.Dict "(in Python v3.12)"){.reference .external}[[\[]{.pre}]{.p}[[str]{.pre}](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.12)"){.reference .external}[[,]{.pre}]{.p}[ ]{.w}[[Any]{.pre}](https://docs.python.org/3/library/typing.html#typing.Any "(in Python v3.12)"){.reference .external}[[\]]{.pre}]{.p}[[\]]{.pre}]{.p}]{.n}[ ]{.w}[[=]{.pre}]{.o}[ ]{.w}[[None]{.pre}]{.default_value}*, *[[flags]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[[[Optional]{.pre}](https://docs.python.org/3/library/typing.html#typing.Optional "(in Python v3.12)"){.reference .external}[[\[]{.pre}]{.p}[[List]{.pre}](https://docs.python.org/3/library/typing.html#typing.List "(in Python v3.12)"){.reference .external}[[\[]{.pre}]{.p}[[str]{.pre}](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.12)"){.reference .external}[[\]]{.pre}]{.p}[[\]]{.pre}]{.p}]{.n}[ ]{.w}[[=]{.pre}]{.o}[ ]{.w}[[None]{.pre}]{.default_value}*[)]{.sig-paren} [[→]{.sig-return-icon} [[[Request]{.pre}](index.html#scrapy.http.Request "scrapy.http.request.Request"){.reference .internal}]{.sig-return-typehint}]{.sig-return}[[[\[source\]]{.pre}]{.viewcode-link}](_modules/scrapy/http/response.html#Response.follow){.reference .internal}[¶](#scrapy.http.Response.follow "Permalink to this definition"){.headerlink}

    :   Return a [[`Request`{.xref .py .py-class .docutils .literal
        .notranslate}]{.pre}](#scrapy.http.Request "scrapy.http.Request"){.reference
        .internal} instance to follow a link [`url`{.docutils .literal
        .notranslate}]{.pre}. It accepts the same arguments as
        [`Request.__init__`{.docutils .literal .notranslate}]{.pre}
        method, but [`url`{.docutils .literal .notranslate}]{.pre} can
        be a relative URL or a [`scrapy.link.Link`{.docutils .literal
        .notranslate}]{.pre} object, not only an absolute URL.

        [[`TextResponse`{.xref .py .py-class .docutils .literal
        .notranslate}]{.pre}](#scrapy.http.TextResponse "scrapy.http.TextResponse"){.reference
        .internal} provides a [[`follow()`{.xref .py .py-meth .docutils
        .literal
        .notranslate}]{.pre}](#scrapy.http.TextResponse.follow "scrapy.http.TextResponse.follow"){.reference
        .internal} method which supports selectors in addition to
        absolute/relative URLs and Link objects.

        ::: versionadded
        [New in version 2.0: ]{.versionmodified .added}The *flags*
        parameter.
        :::

    [[follow_all]{.pre}]{.sig-name .descname}[(]{.sig-paren}*[[urls]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[[[Iterable]{.pre}](https://docs.python.org/3/library/typing.html#typing.Iterable "(in Python v3.12)"){.reference .external}[[\[]{.pre}]{.p}[[Union]{.pre}](https://docs.python.org/3/library/typing.html#typing.Union "(in Python v3.12)"){.reference .external}[[\[]{.pre}]{.p}[[str]{.pre}](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.12)"){.reference .external}[[,]{.pre}]{.p}[ ]{.w}[[Link]{.pre}](index.html#scrapy.link.Link "scrapy.link.Link"){.reference .internal}[[\]]{.pre}]{.p}[[\]]{.pre}]{.p}]{.n}*, *[[callback]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[[[Optional]{.pre}](https://docs.python.org/3/library/typing.html#typing.Optional "(in Python v3.12)"){.reference .external}[[\[]{.pre}]{.p}[[Callable]{.pre}](https://docs.python.org/3/library/typing.html#typing.Callable "(in Python v3.12)"){.reference .external}[[\]]{.pre}]{.p}]{.n}[ ]{.w}[[=]{.pre}]{.o}[ ]{.w}[[None]{.pre}]{.default_value}*, *[[method]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[[[str]{.pre}](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.12)"){.reference .external}]{.n}[ ]{.w}[[=]{.pre}]{.o}[ ]{.w}[[\'GET\']{.pre}]{.default_value}*, *[[headers]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[[[Optional]{.pre}](https://docs.python.org/3/library/typing.html#typing.Optional "(in Python v3.12)"){.reference .external}[[\[]{.pre}]{.p}[[Union]{.pre}](https://docs.python.org/3/library/typing.html#typing.Union "(in Python v3.12)"){.reference .external}[[\[]{.pre}]{.p}[[Mapping]{.pre}](https://docs.python.org/3/library/typing.html#typing.Mapping "(in Python v3.12)"){.reference .external}[[\[]{.pre}]{.p}[AnyStr]{.pre}[[,]{.pre}]{.p}[ ]{.w}[[Any]{.pre}](https://docs.python.org/3/library/typing.html#typing.Any "(in Python v3.12)"){.reference .external}[[\]]{.pre}]{.p}[[,]{.pre}]{.p}[ ]{.w}[[Iterable]{.pre}](https://docs.python.org/3/library/typing.html#typing.Iterable "(in Python v3.12)"){.reference .external}[[\[]{.pre}]{.p}[[Tuple]{.pre}](https://docs.python.org/3/library/typing.html#typing.Tuple "(in Python v3.12)"){.reference .external}[[\[]{.pre}]{.p}[AnyStr]{.pre}[[,]{.pre}]{.p}[ ]{.w}[[Any]{.pre}](https://docs.python.org/3/library/typing.html#typing.Any "(in Python v3.12)"){.reference .external}[[\]]{.pre}]{.p}[[\]]{.pre}]{.p}[[\]]{.pre}]{.p}[[\]]{.pre}]{.p}]{.n}[ ]{.w}[[=]{.pre}]{.o}[ ]{.w}[[None]{.pre}]{.default_value}*, *[[body]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[[[Optional]{.pre}](https://docs.python.org/3/library/typing.html#typing.Optional "(in Python v3.12)"){.reference .external}[[\[]{.pre}]{.p}[[Union]{.pre}](https://docs.python.org/3/library/typing.html#typing.Union "(in Python v3.12)"){.reference .external}[[\[]{.pre}]{.p}[[bytes]{.pre}](https://docs.python.org/3/library/stdtypes.html#bytes "(in Python v3.12)"){.reference .external}[[,]{.pre}]{.p}[ ]{.w}[[str]{.pre}](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.12)"){.reference .external}[[\]]{.pre}]{.p}[[\]]{.pre}]{.p}]{.n}[ ]{.w}[[=]{.pre}]{.o}[ ]{.w}[[None]{.pre}]{.default_value}*, *[[cookies]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[[[Optional]{.pre}](https://docs.python.org/3/library/typing.html#typing.Optional "(in Python v3.12)"){.reference .external}[[\[]{.pre}]{.p}[[Union]{.pre}](https://docs.python.org/3/library/typing.html#typing.Union "(in Python v3.12)"){.reference .external}[[\[]{.pre}]{.p}[[dict]{.pre}](https://docs.python.org/3/library/stdtypes.html#dict "(in Python v3.12)"){.reference .external}[[,]{.pre}]{.p}[ ]{.w}[[List]{.pre}](https://docs.python.org/3/library/typing.html#typing.List "(in Python v3.12)"){.reference .external}[[\[]{.pre}]{.p}[[dict]{.pre}](https://docs.python.org/3/library/stdtypes.html#dict "(in Python v3.12)"){.reference .external}[[\]]{.pre}]{.p}[[\]]{.pre}]{.p}[[\]]{.pre}]{.p}]{.n}[ ]{.w}[[=]{.pre}]{.o}[ ]{.w}[[None]{.pre}]{.default_value}*, *[[meta]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[[[Optional]{.pre}](https://docs.python.org/3/library/typing.html#typing.Optional "(in Python v3.12)"){.reference .external}[[\[]{.pre}]{.p}[[Dict]{.pre}](https://docs.python.org/3/library/typing.html#typing.Dict "(in Python v3.12)"){.reference .external}[[\[]{.pre}]{.p}[[str]{.pre}](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.12)"){.reference .external}[[,]{.pre}]{.p}[ ]{.w}[[Any]{.pre}](https://docs.python.org/3/library/typing.html#typing.Any "(in Python v3.12)"){.reference .external}[[\]]{.pre}]{.p}[[\]]{.pre}]{.p}]{.n}[ ]{.w}[[=]{.pre}]{.o}[ ]{.w}[[None]{.pre}]{.default_value}*, *[[encoding]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[[[Optional]{.pre}](https://docs.python.org/3/library/typing.html#typing.Optional "(in Python v3.12)"){.reference .external}[[\[]{.pre}]{.p}[[str]{.pre}](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.12)"){.reference .external}[[\]]{.pre}]{.p}]{.n}[ ]{.w}[[=]{.pre}]{.o}[ ]{.w}[[\'utf-8\']{.pre}]{.default_value}*, *[[priority]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[[[int]{.pre}](https://docs.python.org/3/library/functions.html#int "(in Python v3.12)"){.reference .external}]{.n}[ ]{.w}[[=]{.pre}]{.o}[ ]{.w}[[0]{.pre}]{.default_value}*, *[[dont_filter]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[[[bool]{.pre}](https://docs.python.org/3/library/functions.html#bool "(in Python v3.12)"){.reference .external}]{.n}[ ]{.w}[[=]{.pre}]{.o}[ ]{.w}[[False]{.pre}]{.default_value}*, *[[errback]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[[[Optional]{.pre}](https://docs.python.org/3/library/typing.html#typing.Optional "(in Python v3.12)"){.reference .external}[[\[]{.pre}]{.p}[[Callable]{.pre}](https://docs.python.org/3/library/typing.html#typing.Callable "(in Python v3.12)"){.reference .external}[[\]]{.pre}]{.p}]{.n}[ ]{.w}[[=]{.pre}]{.o}[ ]{.w}[[None]{.pre}]{.default_value}*, *[[cb_kwargs]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[[[Optional]{.pre}](https://docs.python.org/3/library/typing.html#typing.Optional "(in Python v3.12)"){.reference .external}[[\[]{.pre}]{.p}[[Dict]{.pre}](https://docs.python.org/3/library/typing.html#typing.Dict "(in Python v3.12)"){.reference .external}[[\[]{.pre}]{.p}[[str]{.pre}](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.12)"){.reference .external}[[,]{.pre}]{.p}[ ]{.w}[[Any]{.pre}](https://docs.python.org/3/library/typing.html#typing.Any "(in Python v3.12)"){.reference .external}[[\]]{.pre}]{.p}[[\]]{.pre}]{.p}]{.n}[ ]{.w}[[=]{.pre}]{.o}[ ]{.w}[[None]{.pre}]{.default_value}*, *[[flags]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[[[Optional]{.pre}](https://docs.python.org/3/library/typing.html#typing.Optional "(in Python v3.12)"){.reference .external}[[\[]{.pre}]{.p}[[List]{.pre}](https://docs.python.org/3/library/typing.html#typing.List "(in Python v3.12)"){.reference .external}[[\[]{.pre}]{.p}[[str]{.pre}](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.12)"){.reference .external}[[\]]{.pre}]{.p}[[\]]{.pre}]{.p}]{.n}[ ]{.w}[[=]{.pre}]{.o}[ ]{.w}[[None]{.pre}]{.default_value}*[)]{.sig-paren} [[→]{.sig-return-icon} [[[Generator]{.pre}](https://docs.python.org/3/library/typing.html#typing.Generator "(in Python v3.12)"){.reference .external}[[\[]{.pre}]{.p}[[Request]{.pre}](index.html#scrapy.http.Request "scrapy.http.request.Request"){.reference .internal}[[,]{.pre}]{.p}[ ]{.w}[[None]{.pre}](https://docs.python.org/3/library/constants.html#None "(in Python v3.12)"){.reference .external}[[,]{.pre}]{.p}[ ]{.w}[[None]{.pre}](https://docs.python.org/3/library/constants.html#None "(in Python v3.12)"){.reference .external}[[\]]{.pre}]{.p}]{.sig-return-typehint}]{.sig-return}[[[\[source\]]{.pre}]{.viewcode-link}](_modules/scrapy/http/response.html#Response.follow_all){.reference .internal}[¶](#scrapy.http.Response.follow_all "Permalink to this definition"){.headerlink}

    :   ::: versionadded
        [New in version 2.0.]{.versionmodified .added}
        :::

        Return an iterable of [[`Request`{.xref .py .py-class .docutils
        .literal
        .notranslate}]{.pre}](#scrapy.http.Request "scrapy.http.Request"){.reference
        .internal} instances to follow all links in [`urls`{.docutils
        .literal .notranslate}]{.pre}. It accepts the same arguments as
        [`Request.__init__`{.docutils .literal .notranslate}]{.pre}
        method, but elements of [`urls`{.docutils .literal
        .notranslate}]{.pre} can be relative URLs or [[`Link`{.xref .py
        .py-class .docutils .literal
        .notranslate}]{.pre}](index.html#scrapy.link.Link "scrapy.link.Link"){.reference
        .internal} objects, not only absolute URLs.

        [[`TextResponse`{.xref .py .py-class .docutils .literal
        .notranslate}]{.pre}](#scrapy.http.TextResponse "scrapy.http.TextResponse"){.reference
        .internal} provides a [[`follow_all()`{.xref .py .py-meth
        .docutils .literal
        .notranslate}]{.pre}](#scrapy.http.TextResponse.follow_all "scrapy.http.TextResponse.follow_all"){.reference
        .internal} method which supports selectors in addition to
        absolute/relative URLs and Link objects.
:::

::: {#response-subclasses .section}
[]{#topics-request-response-ref-response-subclasses}

#### Response subclasses[¶](#response-subclasses "Permalink to this heading"){.headerlink}

Here is the list of available built-in Response subclasses. You can also
subclass the Response class to implement your own functionality.

::: {#textresponse-objects .section}
##### TextResponse objects[¶](#textresponse-objects "Permalink to this heading"){.headerlink}

*[class]{.pre}[ ]{.w}*[[scrapy.http.]{.pre}]{.sig-prename .descclassname}[[TextResponse]{.pre}]{.sig-name .descname}[(]{.sig-paren}*[[url]{.pre}]{.n}*[\[]{.optional}, *[[encoding]{.pre}]{.n}*[\[]{.optional}, *[[\...]{.pre}]{.n}*[\]]{.optional}[\]]{.optional}[)]{.sig-paren}[[[\[source\]]{.pre}]{.viewcode-link}](_modules/scrapy/http/response/text.html#TextResponse){.reference .internal}[¶](#scrapy.http.TextResponse "Permalink to this definition"){.headerlink}

:   [[`TextResponse`{.xref .py .py-class .docutils .literal
    .notranslate}]{.pre}](#scrapy.http.TextResponse "scrapy.http.TextResponse"){.reference
    .internal} objects adds encoding capabilities to the base
    [[`Response`{.xref .py .py-class .docutils .literal
    .notranslate}]{.pre}](#scrapy.http.Response "scrapy.http.Response"){.reference
    .internal} class, which is meant to be used only for binary data,
    such as images, sounds or any media file.

    [[`TextResponse`{.xref .py .py-class .docutils .literal
    .notranslate}]{.pre}](#scrapy.http.TextResponse "scrapy.http.TextResponse"){.reference
    .internal} objects support a new [`__init__`{.docutils .literal
    .notranslate}]{.pre} method argument, in addition to the base
    [[`Response`{.xref .py .py-class .docutils .literal
    .notranslate}]{.pre}](#scrapy.http.Response "scrapy.http.Response"){.reference
    .internal} objects. The remaining functionality is the same as for
    the [[`Response`{.xref .py .py-class .docutils .literal
    .notranslate}]{.pre}](#scrapy.http.Response "scrapy.http.Response"){.reference
    .internal} class and is not documented here.

    Parameters

    :   **encoding**
        ([*str*](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.12)"){.reference
        .external}) -- is a string which contains the encoding to use
        for this response. If you create a [[`TextResponse`{.xref .py
        .py-class .docutils .literal
        .notranslate}]{.pre}](#scrapy.http.TextResponse "scrapy.http.TextResponse"){.reference
        .internal} object with a string as body, it will be converted to
        bytes encoded using this encoding. If *encoding* is
        [`None`{.docutils .literal .notranslate}]{.pre} (default), the
        encoding will be looked up in the response headers and body
        instead.

    [[`TextResponse`{.xref .py .py-class .docutils .literal
    .notranslate}]{.pre}](#scrapy.http.TextResponse "scrapy.http.TextResponse"){.reference
    .internal} objects support the following attributes in addition to
    the standard [[`Response`{.xref .py .py-class .docutils .literal
    .notranslate}]{.pre}](#scrapy.http.Response "scrapy.http.Response"){.reference
    .internal} ones:

    [[text]{.pre}]{.sig-name .descname}[¶](#scrapy.http.TextResponse.text "Permalink to this definition"){.headerlink}

    :   Response body, as a string.

        The same as [`response.body.decode(response.encoding)`{.docutils
        .literal .notranslate}]{.pre}, but the result is cached after
        the first call, so you can access [`response.text`{.docutils
        .literal .notranslate}]{.pre} multiple times without extra
        overhead.

        ::: {.admonition .note}
        Note

        [`str(response.body)`{.docutils .literal .notranslate}]{.pre} is
        not a correct way to convert the response body into a string:

        ::: {.highlight-pycon .notranslate}
        ::: highlight
            >>> str(b"body")
            "b'body'"
        :::
        :::
        :::

    [[encoding]{.pre}]{.sig-name .descname}[¶](#scrapy.http.TextResponse.encoding "Permalink to this definition"){.headerlink}

    :   A string with the encoding of this response. The encoding is
        resolved by trying the following mechanisms, in order:

        1.  the encoding passed in the [`__init__`{.docutils .literal
            .notranslate}]{.pre} method [`encoding`{.docutils .literal
            .notranslate}]{.pre} argument

        2.  the encoding declared in the Content-Type HTTP header. If
            this encoding is not valid (i.e. unknown), it is ignored and
            the next resolution mechanism is tried.

        3.  the encoding declared in the response body. The TextResponse
            class doesn't provide any special functionality for this.
            However, the [[`HtmlResponse`{.xref .py .py-class .docutils
            .literal
            .notranslate}]{.pre}](#scrapy.http.HtmlResponse "scrapy.http.HtmlResponse"){.reference
            .internal} and [[`XmlResponse`{.xref .py .py-class .docutils
            .literal
            .notranslate}]{.pre}](#scrapy.http.XmlResponse "scrapy.http.XmlResponse"){.reference
            .internal} classes do.

        4.  the encoding inferred by looking at the response body. This
            is the more fragile method but also the last one tried.

    [[selector]{.pre}]{.sig-name .descname}[¶](#scrapy.http.TextResponse.selector "Permalink to this definition"){.headerlink}

    :   A [`Selector`{.xref .py .py-class .docutils .literal
        .notranslate}]{.pre} instance using the response as target. The
        selector is lazily instantiated on first access.

    [[attributes]{.pre}]{.sig-name .descname}*[[:]{.pre}]{.p}[ ]{.w}[Tuple]{.pre}[[\[]{.pre}]{.p}[[str]{.pre}](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.12)"){.reference .external}[[,]{.pre}]{.p}[ ]{.w}[[\...]{.pre}]{.p}[[\]]{.pre}]{.p}[ ]{.w}[[=]{.pre}]{.p}[ ]{.w}[(\'url\',]{.pre} [\'status\',]{.pre} [\'headers\',]{.pre} [\'body\',]{.pre} [\'flags\',]{.pre} [\'request\',]{.pre} [\'certificate\',]{.pre} [\'ip_address\',]{.pre} [\'protocol\',]{.pre} [\'encoding\')]{.pre}*[¶](#scrapy.http.TextResponse.attributes "Permalink to this definition"){.headerlink}

    :   A tuple of [[`str`{.xref .py .py-class .docutils .literal
        .notranslate}]{.pre}](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.12)"){.reference
        .external} objects containing the name of all public attributes
        of the class that are also keyword parameters of the
        [`__init__`{.docutils .literal .notranslate}]{.pre} method.

        Currently used by [[`Response.replace()`{.xref .py .py-meth
        .docutils .literal
        .notranslate}]{.pre}](#scrapy.http.Response.replace "scrapy.http.Response.replace"){.reference
        .internal}.

    [[`TextResponse`{.xref .py .py-class .docutils .literal
    .notranslate}]{.pre}](#scrapy.http.TextResponse "scrapy.http.TextResponse"){.reference
    .internal} objects support the following methods in addition to the
    standard [[`Response`{.xref .py .py-class .docutils .literal
    .notranslate}]{.pre}](#scrapy.http.Response "scrapy.http.Response"){.reference
    .internal} ones:

    [[jmespath]{.pre}]{.sig-name .descname}[(]{.sig-paren}*[[query]{.pre}]{.n}*[)]{.sig-paren}[[[\[source\]]{.pre}]{.viewcode-link}](_modules/scrapy/http/response/text.html#TextResponse.jmespath){.reference .internal}[¶](#scrapy.http.TextResponse.jmespath "Permalink to this definition"){.headerlink}

    :   A shortcut to [`TextResponse.selector.jmespath(query)`{.docutils
        .literal .notranslate}]{.pre}:

        ::: {.highlight-default .notranslate}
        ::: highlight
            response.jmespath('object.[*]')
        :::
        :::

    [[xpath]{.pre}]{.sig-name .descname}[(]{.sig-paren}*[[query]{.pre}]{.n}*[)]{.sig-paren}[[[\[source\]]{.pre}]{.viewcode-link}](_modules/scrapy/http/response/text.html#TextResponse.xpath){.reference .internal}[¶](#scrapy.http.TextResponse.xpath "Permalink to this definition"){.headerlink}

    :   A shortcut to [`TextResponse.selector.xpath(query)`{.docutils
        .literal .notranslate}]{.pre}:

        ::: {.highlight-default .notranslate}
        ::: highlight
            response.xpath('//p')
        :::
        :::

    [[css]{.pre}]{.sig-name .descname}[(]{.sig-paren}*[[query]{.pre}]{.n}*[)]{.sig-paren}[[[\[source\]]{.pre}]{.viewcode-link}](_modules/scrapy/http/response/text.html#TextResponse.css){.reference .internal}[¶](#scrapy.http.TextResponse.css "Permalink to this definition"){.headerlink}

    :   A shortcut to [`TextResponse.selector.css(query)`{.docutils
        .literal .notranslate}]{.pre}:

        ::: {.highlight-default .notranslate}
        ::: highlight
            response.css('p')
        :::
        :::

    [[follow]{.pre}]{.sig-name .descname}[(]{.sig-paren}*[[url]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[[[Union]{.pre}](https://docs.python.org/3/library/typing.html#typing.Union "(in Python v3.12)"){.reference .external}[[\[]{.pre}]{.p}[[str]{.pre}](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.12)"){.reference .external}[[,]{.pre}]{.p}[ ]{.w}[[Link]{.pre}](index.html#scrapy.link.Link "scrapy.link.Link"){.reference .internal}[[,]{.pre}]{.p}[ ]{.w}[Selector]{.pre}[[\]]{.pre}]{.p}]{.n}*, *[[callback]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[[[Optional]{.pre}](https://docs.python.org/3/library/typing.html#typing.Optional "(in Python v3.12)"){.reference .external}[[\[]{.pre}]{.p}[[Callable]{.pre}](https://docs.python.org/3/library/typing.html#typing.Callable "(in Python v3.12)"){.reference .external}[[\]]{.pre}]{.p}]{.n}[ ]{.w}[[=]{.pre}]{.o}[ ]{.w}[[None]{.pre}]{.default_value}*, *[[method]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[[[str]{.pre}](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.12)"){.reference .external}]{.n}[ ]{.w}[[=]{.pre}]{.o}[ ]{.w}[[\'GET\']{.pre}]{.default_value}*, *[[headers]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[[[Optional]{.pre}](https://docs.python.org/3/library/typing.html#typing.Optional "(in Python v3.12)"){.reference .external}[[\[]{.pre}]{.p}[[Union]{.pre}](https://docs.python.org/3/library/typing.html#typing.Union "(in Python v3.12)"){.reference .external}[[\[]{.pre}]{.p}[[Mapping]{.pre}](https://docs.python.org/3/library/typing.html#typing.Mapping "(in Python v3.12)"){.reference .external}[[\[]{.pre}]{.p}[AnyStr]{.pre}[[,]{.pre}]{.p}[ ]{.w}[[Any]{.pre}](https://docs.python.org/3/library/typing.html#typing.Any "(in Python v3.12)"){.reference .external}[[\]]{.pre}]{.p}[[,]{.pre}]{.p}[ ]{.w}[[Iterable]{.pre}](https://docs.python.org/3/library/typing.html#typing.Iterable "(in Python v3.12)"){.reference .external}[[\[]{.pre}]{.p}[[Tuple]{.pre}](https://docs.python.org/3/library/typing.html#typing.Tuple "(in Python v3.12)"){.reference .external}[[\[]{.pre}]{.p}[AnyStr]{.pre}[[,]{.pre}]{.p}[ ]{.w}[[Any]{.pre}](https://docs.python.org/3/library/typing.html#typing.Any "(in Python v3.12)"){.reference .external}[[\]]{.pre}]{.p}[[\]]{.pre}]{.p}[[\]]{.pre}]{.p}[[\]]{.pre}]{.p}]{.n}[ ]{.w}[[=]{.pre}]{.o}[ ]{.w}[[None]{.pre}]{.default_value}*, *[[body]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[[[Optional]{.pre}](https://docs.python.org/3/library/typing.html#typing.Optional "(in Python v3.12)"){.reference .external}[[\[]{.pre}]{.p}[[Union]{.pre}](https://docs.python.org/3/library/typing.html#typing.Union "(in Python v3.12)"){.reference .external}[[\[]{.pre}]{.p}[[bytes]{.pre}](https://docs.python.org/3/library/stdtypes.html#bytes "(in Python v3.12)"){.reference .external}[[,]{.pre}]{.p}[ ]{.w}[[str]{.pre}](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.12)"){.reference .external}[[\]]{.pre}]{.p}[[\]]{.pre}]{.p}]{.n}[ ]{.w}[[=]{.pre}]{.o}[ ]{.w}[[None]{.pre}]{.default_value}*, *[[cookies]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[[[Optional]{.pre}](https://docs.python.org/3/library/typing.html#typing.Optional "(in Python v3.12)"){.reference .external}[[\[]{.pre}]{.p}[[Union]{.pre}](https://docs.python.org/3/library/typing.html#typing.Union "(in Python v3.12)"){.reference .external}[[\[]{.pre}]{.p}[[dict]{.pre}](https://docs.python.org/3/library/stdtypes.html#dict "(in Python v3.12)"){.reference .external}[[,]{.pre}]{.p}[ ]{.w}[[List]{.pre}](https://docs.python.org/3/library/typing.html#typing.List "(in Python v3.12)"){.reference .external}[[\[]{.pre}]{.p}[[dict]{.pre}](https://docs.python.org/3/library/stdtypes.html#dict "(in Python v3.12)"){.reference .external}[[\]]{.pre}]{.p}[[\]]{.pre}]{.p}[[\]]{.pre}]{.p}]{.n}[ ]{.w}[[=]{.pre}]{.o}[ ]{.w}[[None]{.pre}]{.default_value}*, *[[meta]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[[[Optional]{.pre}](https://docs.python.org/3/library/typing.html#typing.Optional "(in Python v3.12)"){.reference .external}[[\[]{.pre}]{.p}[[Dict]{.pre}](https://docs.python.org/3/library/typing.html#typing.Dict "(in Python v3.12)"){.reference .external}[[\[]{.pre}]{.p}[[str]{.pre}](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.12)"){.reference .external}[[,]{.pre}]{.p}[ ]{.w}[[Any]{.pre}](https://docs.python.org/3/library/typing.html#typing.Any "(in Python v3.12)"){.reference .external}[[\]]{.pre}]{.p}[[\]]{.pre}]{.p}]{.n}[ ]{.w}[[=]{.pre}]{.o}[ ]{.w}[[None]{.pre}]{.default_value}*, *[[encoding]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[[[Optional]{.pre}](https://docs.python.org/3/library/typing.html#typing.Optional "(in Python v3.12)"){.reference .external}[[\[]{.pre}]{.p}[[str]{.pre}](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.12)"){.reference .external}[[\]]{.pre}]{.p}]{.n}[ ]{.w}[[=]{.pre}]{.o}[ ]{.w}[[None]{.pre}]{.default_value}*, *[[priority]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[[[int]{.pre}](https://docs.python.org/3/library/functions.html#int "(in Python v3.12)"){.reference .external}]{.n}[ ]{.w}[[=]{.pre}]{.o}[ ]{.w}[[0]{.pre}]{.default_value}*, *[[dont_filter]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[[[bool]{.pre}](https://docs.python.org/3/library/functions.html#bool "(in Python v3.12)"){.reference .external}]{.n}[ ]{.w}[[=]{.pre}]{.o}[ ]{.w}[[False]{.pre}]{.default_value}*, *[[errback]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[[[Optional]{.pre}](https://docs.python.org/3/library/typing.html#typing.Optional "(in Python v3.12)"){.reference .external}[[\[]{.pre}]{.p}[[Callable]{.pre}](https://docs.python.org/3/library/typing.html#typing.Callable "(in Python v3.12)"){.reference .external}[[\]]{.pre}]{.p}]{.n}[ ]{.w}[[=]{.pre}]{.o}[ ]{.w}[[None]{.pre}]{.default_value}*, *[[cb_kwargs]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[[[Optional]{.pre}](https://docs.python.org/3/library/typing.html#typing.Optional "(in Python v3.12)"){.reference .external}[[\[]{.pre}]{.p}[[Dict]{.pre}](https://docs.python.org/3/library/typing.html#typing.Dict "(in Python v3.12)"){.reference .external}[[\[]{.pre}]{.p}[[str]{.pre}](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.12)"){.reference .external}[[,]{.pre}]{.p}[ ]{.w}[[Any]{.pre}](https://docs.python.org/3/library/typing.html#typing.Any "(in Python v3.12)"){.reference .external}[[\]]{.pre}]{.p}[[\]]{.pre}]{.p}]{.n}[ ]{.w}[[=]{.pre}]{.o}[ ]{.w}[[None]{.pre}]{.default_value}*, *[[flags]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[[[Optional]{.pre}](https://docs.python.org/3/library/typing.html#typing.Optional "(in Python v3.12)"){.reference .external}[[\[]{.pre}]{.p}[[List]{.pre}](https://docs.python.org/3/library/typing.html#typing.List "(in Python v3.12)"){.reference .external}[[\[]{.pre}]{.p}[[str]{.pre}](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.12)"){.reference .external}[[\]]{.pre}]{.p}[[\]]{.pre}]{.p}]{.n}[ ]{.w}[[=]{.pre}]{.o}[ ]{.w}[[None]{.pre}]{.default_value}*[)]{.sig-paren} [[→]{.sig-return-icon} [[[Request]{.pre}](index.html#scrapy.http.Request "scrapy.http.request.Request"){.reference .internal}]{.sig-return-typehint}]{.sig-return}[[[\[source\]]{.pre}]{.viewcode-link}](_modules/scrapy/http/response/text.html#TextResponse.follow){.reference .internal}[¶](#scrapy.http.TextResponse.follow "Permalink to this definition"){.headerlink}

    :   Return a [[`Request`{.xref .py .py-class .docutils .literal
        .notranslate}]{.pre}](#scrapy.http.Request "scrapy.http.Request"){.reference
        .internal} instance to follow a link [`url`{.docutils .literal
        .notranslate}]{.pre}. It accepts the same arguments as
        [`Request.__init__`{.docutils .literal .notranslate}]{.pre}
        method, but [`url`{.docutils .literal .notranslate}]{.pre} can
        be not only an absolute URL, but also

        -   a relative URL

        -   a [[`Link`{.xref .py .py-class .docutils .literal
            .notranslate}]{.pre}](index.html#scrapy.link.Link "scrapy.link.Link"){.reference
            .internal} object, e.g. the result of [[Link
            Extractors]{.std
            .std-ref}](index.html#topics-link-extractors){.hoverxref
            .tooltip .reference .internal}

        -   a [[`Selector`{.xref .py .py-class .docutils .literal
            .notranslate}]{.pre}](index.html#scrapy.selector.Selector "scrapy.selector.Selector"){.reference
            .internal} object for a [`<link>`{.docutils .literal
            .notranslate}]{.pre} or [`<a>`{.docutils .literal
            .notranslate}]{.pre} element, e.g.
            [`response.css('a.my_link')[0]`{.docutils .literal
            .notranslate}]{.pre}

        -   an attribute [[`Selector`{.xref .py .py-class .docutils
            .literal
            .notranslate}]{.pre}](index.html#scrapy.selector.Selector "scrapy.selector.Selector"){.reference
            .internal} (not SelectorList), e.g.
            [`response.css('a::attr(href)')[0]`{.docutils .literal
            .notranslate}]{.pre} or
            [`response.xpath('//img/@src')[0]`{.docutils .literal
            .notranslate}]{.pre}

        See [[A shortcut for creating Requests]{.std
        .std-ref}](index.html#response-follow-example){.hoverxref
        .tooltip .reference .internal} for usage examples.

    [[follow_all]{.pre}]{.sig-name .descname}[(]{.sig-paren}*[[urls]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[[[Optional]{.pre}](https://docs.python.org/3/library/typing.html#typing.Optional "(in Python v3.12)"){.reference .external}[[\[]{.pre}]{.p}[[Union]{.pre}](https://docs.python.org/3/library/typing.html#typing.Union "(in Python v3.12)"){.reference .external}[[\[]{.pre}]{.p}[[Iterable]{.pre}](https://docs.python.org/3/library/typing.html#typing.Iterable "(in Python v3.12)"){.reference .external}[[\[]{.pre}]{.p}[[Union]{.pre}](https://docs.python.org/3/library/typing.html#typing.Union "(in Python v3.12)"){.reference .external}[[\[]{.pre}]{.p}[[str]{.pre}](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.12)"){.reference .external}[[,]{.pre}]{.p}[ ]{.w}[[Link]{.pre}](index.html#scrapy.link.Link "scrapy.link.Link"){.reference .internal}[[\]]{.pre}]{.p}[[\]]{.pre}]{.p}[[,]{.pre}]{.p}[ ]{.w}[SelectorList]{.pre}[[\]]{.pre}]{.p}[[\]]{.pre}]{.p}]{.n}[ ]{.w}[[=]{.pre}]{.o}[ ]{.w}[[None]{.pre}]{.default_value}*, *[[callback]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[[[Optional]{.pre}](https://docs.python.org/3/library/typing.html#typing.Optional "(in Python v3.12)"){.reference .external}[[\[]{.pre}]{.p}[[Callable]{.pre}](https://docs.python.org/3/library/typing.html#typing.Callable "(in Python v3.12)"){.reference .external}[[\]]{.pre}]{.p}]{.n}[ ]{.w}[[=]{.pre}]{.o}[ ]{.w}[[None]{.pre}]{.default_value}*, *[[method]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[[[str]{.pre}](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.12)"){.reference .external}]{.n}[ ]{.w}[[=]{.pre}]{.o}[ ]{.w}[[\'GET\']{.pre}]{.default_value}*, *[[headers]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[[[Optional]{.pre}](https://docs.python.org/3/library/typing.html#typing.Optional "(in Python v3.12)"){.reference .external}[[\[]{.pre}]{.p}[[Union]{.pre}](https://docs.python.org/3/library/typing.html#typing.Union "(in Python v3.12)"){.reference .external}[[\[]{.pre}]{.p}[[Mapping]{.pre}](https://docs.python.org/3/library/typing.html#typing.Mapping "(in Python v3.12)"){.reference .external}[[\[]{.pre}]{.p}[AnyStr]{.pre}[[,]{.pre}]{.p}[ ]{.w}[[Any]{.pre}](https://docs.python.org/3/library/typing.html#typing.Any "(in Python v3.12)"){.reference .external}[[\]]{.pre}]{.p}[[,]{.pre}]{.p}[ ]{.w}[[Iterable]{.pre}](https://docs.python.org/3/library/typing.html#typing.Iterable "(in Python v3.12)"){.reference .external}[[\[]{.pre}]{.p}[[Tuple]{.pre}](https://docs.python.org/3/library/typing.html#typing.Tuple "(in Python v3.12)"){.reference .external}[[\[]{.pre}]{.p}[AnyStr]{.pre}[[,]{.pre}]{.p}[ ]{.w}[[Any]{.pre}](https://docs.python.org/3/library/typing.html#typing.Any "(in Python v3.12)"){.reference .external}[[\]]{.pre}]{.p}[[\]]{.pre}]{.p}[[\]]{.pre}]{.p}[[\]]{.pre}]{.p}]{.n}[ ]{.w}[[=]{.pre}]{.o}[ ]{.w}[[None]{.pre}]{.default_value}*, *[[body]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[[[Optional]{.pre}](https://docs.python.org/3/library/typing.html#typing.Optional "(in Python v3.12)"){.reference .external}[[\[]{.pre}]{.p}[[Union]{.pre}](https://docs.python.org/3/library/typing.html#typing.Union "(in Python v3.12)"){.reference .external}[[\[]{.pre}]{.p}[[bytes]{.pre}](https://docs.python.org/3/library/stdtypes.html#bytes "(in Python v3.12)"){.reference .external}[[,]{.pre}]{.p}[ ]{.w}[[str]{.pre}](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.12)"){.reference .external}[[\]]{.pre}]{.p}[[\]]{.pre}]{.p}]{.n}[ ]{.w}[[=]{.pre}]{.o}[ ]{.w}[[None]{.pre}]{.default_value}*, *[[cookies]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[[[Optional]{.pre}](https://docs.python.org/3/library/typing.html#typing.Optional "(in Python v3.12)"){.reference .external}[[\[]{.pre}]{.p}[[Union]{.pre}](https://docs.python.org/3/library/typing.html#typing.Union "(in Python v3.12)"){.reference .external}[[\[]{.pre}]{.p}[[dict]{.pre}](https://docs.python.org/3/library/stdtypes.html#dict "(in Python v3.12)"){.reference .external}[[,]{.pre}]{.p}[ ]{.w}[[List]{.pre}](https://docs.python.org/3/library/typing.html#typing.List "(in Python v3.12)"){.reference .external}[[\[]{.pre}]{.p}[[dict]{.pre}](https://docs.python.org/3/library/stdtypes.html#dict "(in Python v3.12)"){.reference .external}[[\]]{.pre}]{.p}[[\]]{.pre}]{.p}[[\]]{.pre}]{.p}]{.n}[ ]{.w}[[=]{.pre}]{.o}[ ]{.w}[[None]{.pre}]{.default_value}*, *[[meta]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[[[Optional]{.pre}](https://docs.python.org/3/library/typing.html#typing.Optional "(in Python v3.12)"){.reference .external}[[\[]{.pre}]{.p}[[Dict]{.pre}](https://docs.python.org/3/library/typing.html#typing.Dict "(in Python v3.12)"){.reference .external}[[\[]{.pre}]{.p}[[str]{.pre}](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.12)"){.reference .external}[[,]{.pre}]{.p}[ ]{.w}[[Any]{.pre}](https://docs.python.org/3/library/typing.html#typing.Any "(in Python v3.12)"){.reference .external}[[\]]{.pre}]{.p}[[\]]{.pre}]{.p}]{.n}[ ]{.w}[[=]{.pre}]{.o}[ ]{.w}[[None]{.pre}]{.default_value}*, *[[encoding]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[[[Optional]{.pre}](https://docs.python.org/3/library/typing.html#typing.Optional "(in Python v3.12)"){.reference .external}[[\[]{.pre}]{.p}[[str]{.pre}](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.12)"){.reference .external}[[\]]{.pre}]{.p}]{.n}[ ]{.w}[[=]{.pre}]{.o}[ ]{.w}[[None]{.pre}]{.default_value}*, *[[priority]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[[[int]{.pre}](https://docs.python.org/3/library/functions.html#int "(in Python v3.12)"){.reference .external}]{.n}[ ]{.w}[[=]{.pre}]{.o}[ ]{.w}[[0]{.pre}]{.default_value}*, *[[dont_filter]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[[[bool]{.pre}](https://docs.python.org/3/library/functions.html#bool "(in Python v3.12)"){.reference .external}]{.n}[ ]{.w}[[=]{.pre}]{.o}[ ]{.w}[[False]{.pre}]{.default_value}*, *[[errback]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[[[Optional]{.pre}](https://docs.python.org/3/library/typing.html#typing.Optional "(in Python v3.12)"){.reference .external}[[\[]{.pre}]{.p}[[Callable]{.pre}](https://docs.python.org/3/library/typing.html#typing.Callable "(in Python v3.12)"){.reference .external}[[\]]{.pre}]{.p}]{.n}[ ]{.w}[[=]{.pre}]{.o}[ ]{.w}[[None]{.pre}]{.default_value}*, *[[cb_kwargs]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[[[Optional]{.pre}](https://docs.python.org/3/library/typing.html#typing.Optional "(in Python v3.12)"){.reference .external}[[\[]{.pre}]{.p}[[Dict]{.pre}](https://docs.python.org/3/library/typing.html#typing.Dict "(in Python v3.12)"){.reference .external}[[\[]{.pre}]{.p}[[str]{.pre}](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.12)"){.reference .external}[[,]{.pre}]{.p}[ ]{.w}[[Any]{.pre}](https://docs.python.org/3/library/typing.html#typing.Any "(in Python v3.12)"){.reference .external}[[\]]{.pre}]{.p}[[\]]{.pre}]{.p}]{.n}[ ]{.w}[[=]{.pre}]{.o}[ ]{.w}[[None]{.pre}]{.default_value}*, *[[flags]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[[[Optional]{.pre}](https://docs.python.org/3/library/typing.html#typing.Optional "(in Python v3.12)"){.reference .external}[[\[]{.pre}]{.p}[[List]{.pre}](https://docs.python.org/3/library/typing.html#typing.List "(in Python v3.12)"){.reference .external}[[\[]{.pre}]{.p}[[str]{.pre}](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.12)"){.reference .external}[[\]]{.pre}]{.p}[[\]]{.pre}]{.p}]{.n}[ ]{.w}[[=]{.pre}]{.o}[ ]{.w}[[None]{.pre}]{.default_value}*, *[[css]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[[[Optional]{.pre}](https://docs.python.org/3/library/typing.html#typing.Optional "(in Python v3.12)"){.reference .external}[[\[]{.pre}]{.p}[[str]{.pre}](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.12)"){.reference .external}[[\]]{.pre}]{.p}]{.n}[ ]{.w}[[=]{.pre}]{.o}[ ]{.w}[[None]{.pre}]{.default_value}*, *[[xpath]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[[[Optional]{.pre}](https://docs.python.org/3/library/typing.html#typing.Optional "(in Python v3.12)"){.reference .external}[[\[]{.pre}]{.p}[[str]{.pre}](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.12)"){.reference .external}[[\]]{.pre}]{.p}]{.n}[ ]{.w}[[=]{.pre}]{.o}[ ]{.w}[[None]{.pre}]{.default_value}*[)]{.sig-paren} [[→]{.sig-return-icon} [[[Generator]{.pre}](https://docs.python.org/3/library/typing.html#typing.Generator "(in Python v3.12)"){.reference .external}[[\[]{.pre}]{.p}[[Request]{.pre}](index.html#scrapy.http.Request "scrapy.http.request.Request"){.reference .internal}[[,]{.pre}]{.p}[ ]{.w}[[None]{.pre}](https://docs.python.org/3/library/constants.html#None "(in Python v3.12)"){.reference .external}[[,]{.pre}]{.p}[ ]{.w}[[None]{.pre}](https://docs.python.org/3/library/constants.html#None "(in Python v3.12)"){.reference .external}[[\]]{.pre}]{.p}]{.sig-return-typehint}]{.sig-return}[[[\[source\]]{.pre}]{.viewcode-link}](_modules/scrapy/http/response/text.html#TextResponse.follow_all){.reference .internal}[¶](#scrapy.http.TextResponse.follow_all "Permalink to this definition"){.headerlink}

    :   A generator that produces [[`Request`{.xref .py .py-class
        .docutils .literal
        .notranslate}]{.pre}](#scrapy.http.Request "scrapy.http.Request"){.reference
        .internal} instances to follow all links in [`urls`{.docutils
        .literal .notranslate}]{.pre}. It accepts the same arguments as
        the [[`Request`{.xref .py .py-class .docutils .literal
        .notranslate}]{.pre}](#scrapy.http.Request "scrapy.http.Request"){.reference
        .internal}'s [`__init__`{.docutils .literal .notranslate}]{.pre}
        method, except that each [`urls`{.docutils .literal
        .notranslate}]{.pre} element does not need to be an absolute
        URL, it can be any of the following:

        -   a relative URL

        -   a [[`Link`{.xref .py .py-class .docutils .literal
            .notranslate}]{.pre}](index.html#scrapy.link.Link "scrapy.link.Link"){.reference
            .internal} object, e.g. the result of [[Link
            Extractors]{.std
            .std-ref}](index.html#topics-link-extractors){.hoverxref
            .tooltip .reference .internal}

        -   a [[`Selector`{.xref .py .py-class .docutils .literal
            .notranslate}]{.pre}](index.html#scrapy.selector.Selector "scrapy.selector.Selector"){.reference
            .internal} object for a [`<link>`{.docutils .literal
            .notranslate}]{.pre} or [`<a>`{.docutils .literal
            .notranslate}]{.pre} element, e.g.
            [`response.css('a.my_link')[0]`{.docutils .literal
            .notranslate}]{.pre}

        -   an attribute [[`Selector`{.xref .py .py-class .docutils
            .literal
            .notranslate}]{.pre}](index.html#scrapy.selector.Selector "scrapy.selector.Selector"){.reference
            .internal} (not SelectorList), e.g.
            [`response.css('a::attr(href)')[0]`{.docutils .literal
            .notranslate}]{.pre} or
            [`response.xpath('//img/@src')[0]`{.docutils .literal
            .notranslate}]{.pre}

        In addition, [`css`{.docutils .literal .notranslate}]{.pre} and
        [`xpath`{.docutils .literal .notranslate}]{.pre} arguments are
        accepted to perform the link extraction within the
        [`follow_all`{.docutils .literal .notranslate}]{.pre} method
        (only one of [`urls`{.docutils .literal .notranslate}]{.pre},
        [`css`{.docutils .literal .notranslate}]{.pre} and
        [`xpath`{.docutils .literal .notranslate}]{.pre} is accepted).

        Note that when passing a [`SelectorList`{.docutils .literal
        .notranslate}]{.pre} as argument for the [`urls`{.docutils
        .literal .notranslate}]{.pre} parameter or using the
        [`css`{.docutils .literal .notranslate}]{.pre} or
        [`xpath`{.docutils .literal .notranslate}]{.pre} parameters,
        this method will not produce requests for selectors from which
        links cannot be obtained (for instance, anchor tags without an
        [`href`{.docutils .literal .notranslate}]{.pre} attribute)

    [[json]{.pre}]{.sig-name .descname}[(]{.sig-paren}[)]{.sig-paren}[[[\[source\]]{.pre}]{.viewcode-link}](_modules/scrapy/http/response/text.html#TextResponse.json){.reference .internal}[¶](#scrapy.http.TextResponse.json "Permalink to this definition"){.headerlink}

    :   ::: versionadded
        [New in version 2.2.]{.versionmodified .added}
        :::

        Deserialize a JSON document to a Python object.

        Returns a Python object from deserialized JSON document. The
        result is cached after the first call.

    [[urljoin]{.pre}]{.sig-name .descname}[(]{.sig-paren}*[[url]{.pre}]{.n}*[)]{.sig-paren}[[[\[source\]]{.pre}]{.viewcode-link}](_modules/scrapy/http/response/text.html#TextResponse.urljoin){.reference .internal}[¶](#scrapy.http.TextResponse.urljoin "Permalink to this definition"){.headerlink}

    :   Constructs an absolute url by combining the Response's base url
        with a possible relative url. The base url shall be extracted
        from the [`<base>`{.docutils .literal .notranslate}]{.pre} tag,
        or just the Response's [`url`{.xref .py .py-attr .docutils
        .literal .notranslate}]{.pre} if there is no such tag.
:::

::: {#htmlresponse-objects .section}
##### HtmlResponse objects[¶](#htmlresponse-objects "Permalink to this heading"){.headerlink}

*[class]{.pre}[ ]{.w}*[[scrapy.http.]{.pre}]{.sig-prename .descclassname}[[HtmlResponse]{.pre}]{.sig-name .descname}[(]{.sig-paren}*[[url]{.pre}]{.n}*[\[]{.optional}, *[[\...]{.pre}]{.n}*[\]]{.optional}[)]{.sig-paren}[[[\[source\]]{.pre}]{.viewcode-link}](_modules/scrapy/http/response/html.html#HtmlResponse){.reference .internal}[¶](#scrapy.http.HtmlResponse "Permalink to this definition"){.headerlink}

:   The [[`HtmlResponse`{.xref .py .py-class .docutils .literal
    .notranslate}]{.pre}](#scrapy.http.HtmlResponse "scrapy.http.HtmlResponse"){.reference
    .internal} class is a subclass of [[`TextResponse`{.xref .py
    .py-class .docutils .literal
    .notranslate}]{.pre}](#scrapy.http.TextResponse "scrapy.http.TextResponse"){.reference
    .internal} which adds encoding auto-discovering support by looking
    into the HTML [meta
    http-equiv](https://www.w3schools.com/TAGS/att_meta_http_equiv.asp){.reference
    .external} attribute. See [[`TextResponse.encoding`{.xref .py
    .py-attr .docutils .literal
    .notranslate}]{.pre}](#scrapy.http.TextResponse.encoding "scrapy.http.TextResponse.encoding"){.reference
    .internal}.
:::

::: {#xmlresponse-objects .section}
##### XmlResponse objects[¶](#xmlresponse-objects "Permalink to this heading"){.headerlink}

*[class]{.pre}[ ]{.w}*[[scrapy.http.]{.pre}]{.sig-prename .descclassname}[[XmlResponse]{.pre}]{.sig-name .descname}[(]{.sig-paren}*[[url]{.pre}]{.n}*[\[]{.optional}, *[[\...]{.pre}]{.n}*[\]]{.optional}[)]{.sig-paren}[[[\[source\]]{.pre}]{.viewcode-link}](_modules/scrapy/http/response/xml.html#XmlResponse){.reference .internal}[¶](#scrapy.http.XmlResponse "Permalink to this definition"){.headerlink}

:   The [[`XmlResponse`{.xref .py .py-class .docutils .literal
    .notranslate}]{.pre}](#scrapy.http.XmlResponse "scrapy.http.XmlResponse"){.reference
    .internal} class is a subclass of [[`TextResponse`{.xref .py
    .py-class .docutils .literal
    .notranslate}]{.pre}](#scrapy.http.TextResponse "scrapy.http.TextResponse"){.reference
    .internal} which adds encoding auto-discovering support by looking
    into the XML declaration line. See [[`TextResponse.encoding`{.xref
    .py .py-attr .docutils .literal
    .notranslate}]{.pre}](#scrapy.http.TextResponse.encoding "scrapy.http.TextResponse.encoding"){.reference
    .internal}.
:::
:::
:::

[]{#document-topics/link-extractors}

::: {#link-extractors .section}
[]{#topics-link-extractors}

### Link Extractors[¶](#link-extractors "Permalink to this heading"){.headerlink}

A link extractor is an object that extracts links from responses.

The [`__init__`{.docutils .literal .notranslate}]{.pre} method of
[[`LxmlLinkExtractor`{.xref .py .py-class .docutils .literal
.notranslate}]{.pre}](#scrapy.linkextractors.lxmlhtml.LxmlLinkExtractor "scrapy.linkextractors.lxmlhtml.LxmlLinkExtractor"){.reference
.internal} takes settings that determine which links may be extracted.
[[`LxmlLinkExtractor.extract_links`{.xref .py .py-class .docutils
.literal
.notranslate}]{.pre}](#scrapy.linkextractors.lxmlhtml.LxmlLinkExtractor.extract_links "scrapy.linkextractors.lxmlhtml.LxmlLinkExtractor.extract_links"){.reference
.internal} returns a list of matching [[`Link`{.xref .py .py-class
.docutils .literal
.notranslate}]{.pre}](#scrapy.link.Link "scrapy.link.Link"){.reference
.internal} objects from a [[`Response`{.xref .py .py-class .docutils
.literal
.notranslate}]{.pre}](index.html#scrapy.http.Response "scrapy.http.Response"){.reference
.internal} object.

Link extractors are used in [[`CrawlSpider`{.xref .py .py-class
.docutils .literal
.notranslate}]{.pre}](index.html#scrapy.spiders.CrawlSpider "scrapy.spiders.CrawlSpider"){.reference
.internal} spiders through a set of [[`Rule`{.xref .py .py-class
.docutils .literal
.notranslate}]{.pre}](index.html#scrapy.spiders.Rule "scrapy.spiders.Rule"){.reference
.internal} objects.

You can also use link extractors in regular spiders. For example, you
can instantiate [[`LinkExtractor`{.xref .py .py-class .docutils .literal
.notranslate}]{.pre}](#scrapy.linkextractors.lxmlhtml.LxmlLinkExtractor "scrapy.linkextractors.lxmlhtml.LxmlLinkExtractor"){.reference
.internal} into a class variable in your spider, and use it from your
spider callbacks:

::: {.highlight-python .notranslate}
::: highlight
    def parse(self, response):
        for link in self.link_extractor.extract_links(response):
            yield Request(link.url, callback=self.parse)
:::
:::

::: {#module-scrapy.linkextractors .section}
[]{#link-extractor-reference}[]{#topics-link-extractors-ref}

#### Link extractor reference[¶](#module-scrapy.linkextractors "Permalink to this heading"){.headerlink}

The link extractor class is
[[`scrapy.linkextractors.lxmlhtml.LxmlLinkExtractor`{.xref .py .py-class
.docutils .literal
.notranslate}]{.pre}](#scrapy.linkextractors.lxmlhtml.LxmlLinkExtractor "scrapy.linkextractors.lxmlhtml.LxmlLinkExtractor"){.reference
.internal}. For convenience it can also be imported as
[`scrapy.linkextractors.LinkExtractor`{.docutils .literal
.notranslate}]{.pre}:

::: {.highlight-default .notranslate}
::: highlight
    from scrapy.linkextractors import LinkExtractor
:::
:::

::: {#module-scrapy.linkextractors.lxmlhtml .section}
[]{#lxmllinkextractor}

##### LxmlLinkExtractor[¶](#module-scrapy.linkextractors.lxmlhtml "Permalink to this heading"){.headerlink}

*[class]{.pre}[ ]{.w}*[[scrapy.linkextractors.lxmlhtml.]{.pre}]{.sig-prename .descclassname}[[LxmlLinkExtractor]{.pre}]{.sig-name .descname}[(]{.sig-paren}*[[allow]{.pre}]{.n}[[=]{.pre}]{.o}[[()]{.pre}]{.default_value}*, *[[deny]{.pre}]{.n}[[=]{.pre}]{.o}[[()]{.pre}]{.default_value}*, *[[allow_domains]{.pre}]{.n}[[=]{.pre}]{.o}[[()]{.pre}]{.default_value}*, *[[deny_domains]{.pre}]{.n}[[=]{.pre}]{.o}[[()]{.pre}]{.default_value}*, *[[deny_extensions]{.pre}]{.n}[[=]{.pre}]{.o}[[None]{.pre}]{.default_value}*, *[[restrict_xpaths]{.pre}]{.n}[[=]{.pre}]{.o}[[()]{.pre}]{.default_value}*, *[[restrict_css]{.pre}]{.n}[[=]{.pre}]{.o}[[()]{.pre}]{.default_value}*, *[[tags]{.pre}]{.n}[[=]{.pre}]{.o}[[(\'a\',]{.pre} [\'area\')]{.pre}]{.default_value}*, *[[attrs]{.pre}]{.n}[[=]{.pre}]{.o}[[(\'href\',)]{.pre}]{.default_value}*, *[[canonicalize]{.pre}]{.n}[[=]{.pre}]{.o}[[False]{.pre}]{.default_value}*, *[[unique]{.pre}]{.n}[[=]{.pre}]{.o}[[True]{.pre}]{.default_value}*, *[[process_value]{.pre}]{.n}[[=]{.pre}]{.o}[[None]{.pre}]{.default_value}*, *[[strip]{.pre}]{.n}[[=]{.pre}]{.o}[[True]{.pre}]{.default_value}*[)]{.sig-paren}[[[\[source\]]{.pre}]{.viewcode-link}](_modules/scrapy/linkextractors/lxmlhtml.html#LxmlLinkExtractor){.reference .internal}[¶](#scrapy.linkextractors.lxmlhtml.LxmlLinkExtractor "Permalink to this definition"){.headerlink}

:   LxmlLinkExtractor is the recommended link extractor with handy
    filtering options. It is implemented using lxml's robust HTMLParser.

    Parameters

    :   -   **allow**
            ([*str*](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.12)"){.reference
            .external} *or*
            [*list*](https://docs.python.org/3/library/stdtypes.html#list "(in Python v3.12)"){.reference
            .external}) -- a single regular expression (or list of
            regular expressions) that the (absolute) urls must match in
            order to be extracted. If not given (or empty), it will
            match all links.

        -   **deny**
            ([*str*](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.12)"){.reference
            .external} *or*
            [*list*](https://docs.python.org/3/library/stdtypes.html#list "(in Python v3.12)"){.reference
            .external}) -- a single regular expression (or list of
            regular expressions) that the (absolute) urls must match in
            order to be excluded (i.e. not extracted). It has precedence
            over the [`allow`{.docutils .literal .notranslate}]{.pre}
            parameter. If not given (or empty) it won't exclude any
            links.

        -   **allow_domains**
            ([*str*](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.12)"){.reference
            .external} *or*
            [*list*](https://docs.python.org/3/library/stdtypes.html#list "(in Python v3.12)"){.reference
            .external}) -- a single value or a list of string containing
            domains which will be considered for extracting the links

        -   **deny_domains**
            ([*str*](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.12)"){.reference
            .external} *or*
            [*list*](https://docs.python.org/3/library/stdtypes.html#list "(in Python v3.12)"){.reference
            .external}) -- a single value or a list of strings
            containing domains which won't be considered for extracting
            the links

        -   **deny_extensions**
            ([*list*](https://docs.python.org/3/library/stdtypes.html#list "(in Python v3.12)"){.reference
            .external}) --

            a single value or list of strings containing extensions that
            should be ignored when extracting links. If not given, it
            will default to
            [`scrapy.linkextractors.IGNORED_EXTENSIONS`{.xref .py
            .py-data .docutils .literal .notranslate}]{.pre}.

            ::: versionchanged
            [Changed in version 2.0: ]{.versionmodified
            .changed}[`IGNORED_EXTENSIONS`{.xref .py .py-data .docutils
            .literal .notranslate}]{.pre} now includes [`7z`{.docutils
            .literal .notranslate}]{.pre}, [`7zip`{.docutils .literal
            .notranslate}]{.pre}, [`apk`{.docutils .literal
            .notranslate}]{.pre}, [`bz2`{.docutils .literal
            .notranslate}]{.pre}, [`cdr`{.docutils .literal
            .notranslate}]{.pre}, [`dmg`{.docutils .literal
            .notranslate}]{.pre}, [`ico`{.docutils .literal
            .notranslate}]{.pre}, [`iso`{.docutils .literal
            .notranslate}]{.pre}, [`tar`{.docutils .literal
            .notranslate}]{.pre}, [`tar.gz`{.docutils .literal
            .notranslate}]{.pre}, [`webm`{.docutils .literal
            .notranslate}]{.pre}, and [`xz`{.docutils .literal
            .notranslate}]{.pre}.
            :::

        -   **restrict_xpaths**
            ([*str*](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.12)"){.reference
            .external} *or*
            [*list*](https://docs.python.org/3/library/stdtypes.html#list "(in Python v3.12)"){.reference
            .external}) -- is an XPath (or list of XPath's) which
            defines regions inside the response where links should be
            extracted from. If given, only the text selected by those
            XPath will be scanned for links. See examples below.

        -   **restrict_css**
            ([*str*](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.12)"){.reference
            .external} *or*
            [*list*](https://docs.python.org/3/library/stdtypes.html#list "(in Python v3.12)"){.reference
            .external}) -- a CSS selector (or list of selectors) which
            defines regions inside the response where links should be
            extracted from. Has the same behaviour as
            [`restrict_xpaths`{.docutils .literal .notranslate}]{.pre}.

        -   **restrict_text**
            ([*str*](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.12)"){.reference
            .external} *or*
            [*list*](https://docs.python.org/3/library/stdtypes.html#list "(in Python v3.12)"){.reference
            .external}) -- a single regular expression (or list of
            regular expressions) that the link's text must match in
            order to be extracted. If not given (or empty), it will
            match all links. If a list of regular expressions is given,
            the link will be extracted if it matches at least one.

        -   **tags**
            ([*str*](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.12)"){.reference
            .external} *or*
            [*list*](https://docs.python.org/3/library/stdtypes.html#list "(in Python v3.12)"){.reference
            .external}) -- a tag or a list of tags to consider when
            extracting links. Defaults to [`('a',`{.docutils .literal
            .notranslate}]{.pre}` `{.docutils .literal
            .notranslate}[`'area')`{.docutils .literal
            .notranslate}]{.pre}.

        -   **attrs**
            ([*list*](https://docs.python.org/3/library/stdtypes.html#list "(in Python v3.12)"){.reference
            .external}) -- an attribute or list of attributes which
            should be considered when looking for links to extract (only
            for those tags specified in the [`tags`{.docutils .literal
            .notranslate}]{.pre} parameter). Defaults to
            [`('href',)`{.docutils .literal .notranslate}]{.pre}

        -   **canonicalize**
            ([*bool*](https://docs.python.org/3/library/functions.html#bool "(in Python v3.12)"){.reference
            .external}) -- canonicalize each extracted url (using
            w3lib.url.canonicalize_url). Defaults to [`False`{.docutils
            .literal .notranslate}]{.pre}. Note that canonicalize_url is
            meant for duplicate checking; it can change the URL visible
            at server side, so the response can be different for
            requests with canonicalized and raw URLs. If you're using
            LinkExtractor to follow links it is more robust to keep the
            default [`canonicalize=False`{.docutils .literal
            .notranslate}]{.pre}.

        -   **unique**
            ([*bool*](https://docs.python.org/3/library/functions.html#bool "(in Python v3.12)"){.reference
            .external}) -- whether duplicate filtering should be applied
            to extracted links.

        -   **process_value**
            ([*collections.abc.Callable*](https://docs.python.org/3/library/collections.abc.html#collections.abc.Callable "(in Python v3.12)"){.reference
            .external}) --

            a function which receives each value extracted from the tag
            and attributes scanned and can modify the value and return a
            new one, or return [`None`{.docutils .literal
            .notranslate}]{.pre} to ignore the link altogether. If not
            given, [`process_value`{.docutils .literal
            .notranslate}]{.pre} defaults to [`lambda`{.docutils
            .literal .notranslate}]{.pre}` `{.docutils .literal
            .notranslate}[`x:`{.docutils .literal
            .notranslate}]{.pre}` `{.docutils .literal
            .notranslate}[`x`{.docutils .literal .notranslate}]{.pre}.

            For example, to extract links from this code:

            ::: {.highlight-html .notranslate}
            ::: highlight
                <a href="javascript:goToPage('../other/page.html'); return false">Link text</a>
            :::
            :::

            You can use the following function in
            [`process_value`{.docutils .literal .notranslate}]{.pre}:

            ::: {.highlight-python .notranslate}
            ::: highlight
                def process_value(value):
                    m = re.search(r"javascript:goToPage\('(.*?)'", value)
                    if m:
                        return m.group(1)
            :::
            :::

        -   **strip**
            ([*bool*](https://docs.python.org/3/library/functions.html#bool "(in Python v3.12)"){.reference
            .external}) -- whether to strip whitespaces from extracted
            attributes. According to HTML5 standard, leading and
            trailing whitespaces must be stripped from [`href`{.docutils
            .literal .notranslate}]{.pre} attributes of [`<a>`{.docutils
            .literal .notranslate}]{.pre}, [`<area>`{.docutils .literal
            .notranslate}]{.pre} and many other elements,
            [`src`{.docutils .literal .notranslate}]{.pre} attribute of
            [`<img>`{.docutils .literal .notranslate}]{.pre},
            [`<iframe>`{.docutils .literal .notranslate}]{.pre}
            elements, etc., so LinkExtractor strips space chars by
            default. Set [`strip=False`{.docutils .literal
            .notranslate}]{.pre} to turn it off (e.g. if you're
            extracting urls from elements or attributes which allow
            leading/trailing whitespaces).

    [[extract_links]{.pre}]{.sig-name .descname}[(]{.sig-paren}*[[response]{.pre}]{.n}*[)]{.sig-paren}[[[\[source\]]{.pre}]{.viewcode-link}](_modules/scrapy/linkextractors/lxmlhtml.html#LxmlLinkExtractor.extract_links){.reference .internal}[¶](#scrapy.linkextractors.lxmlhtml.LxmlLinkExtractor.extract_links "Permalink to this definition"){.headerlink}

    :   Returns a list of [[`Link`{.xref .py .py-class .docutils
        .literal
        .notranslate}]{.pre}](#scrapy.link.Link "scrapy.link.Link"){.reference
        .internal} objects from the specified [[`response`{.xref .py
        .py-class .docutils .literal
        .notranslate}]{.pre}](index.html#scrapy.http.Response "scrapy.http.Response"){.reference
        .internal}.

        Only links that match the settings passed to the
        [`__init__`{.docutils .literal .notranslate}]{.pre} method of
        the link extractor are returned.

        Duplicate links are omitted if the [`unique`{.docutils .literal
        .notranslate}]{.pre} attribute is set to [`True`{.docutils
        .literal .notranslate}]{.pre}, otherwise they are returned.
:::

::: {#module-scrapy.link .section}
[]{#link}

##### Link[¶](#module-scrapy.link "Permalink to this heading"){.headerlink}

*[class]{.pre}[ ]{.w}*[[scrapy.link.]{.pre}]{.sig-prename .descclassname}[[Link]{.pre}]{.sig-name .descname}[(]{.sig-paren}*[[url]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[[[str]{.pre}](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.12)"){.reference .external}]{.n}*, *[[text]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[[[str]{.pre}](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.12)"){.reference .external}]{.n}[ ]{.w}[[=]{.pre}]{.o}[ ]{.w}[[\'\']{.pre}]{.default_value}*, *[[fragment]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[[[str]{.pre}](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.12)"){.reference .external}]{.n}[ ]{.w}[[=]{.pre}]{.o}[ ]{.w}[[\'\']{.pre}]{.default_value}*, *[[nofollow]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[[[bool]{.pre}](https://docs.python.org/3/library/functions.html#bool "(in Python v3.12)"){.reference .external}]{.n}[ ]{.w}[[=]{.pre}]{.o}[ ]{.w}[[False]{.pre}]{.default_value}*[)]{.sig-paren}[[[\[source\]]{.pre}]{.viewcode-link}](_modules/scrapy/link.html#Link){.reference .internal}[¶](#scrapy.link.Link "Permalink to this definition"){.headerlink}

:   Link objects represent an extracted link by the LinkExtractor.

    Using the anchor tag sample below to illustrate the parameters:

    ::: {.highlight-python .notranslate}
    ::: highlight
        <a href="https://example.com/nofollow.html#foo" rel="nofollow">Dont follow this one</a>
    :::
    :::

    Parameters

    :   -   **url** -- the absolute url being linked to in the anchor
            tag. From the sample, this is
            [`https://example.com/nofollow.html`{.docutils .literal
            .notranslate}]{.pre}.

        -   **text** -- the text in the anchor tag. From the sample,
            this is [`Dont`{.docutils .literal
            .notranslate}]{.pre}` `{.docutils .literal
            .notranslate}[`follow`{.docutils .literal
            .notranslate}]{.pre}` `{.docutils .literal
            .notranslate}[`this`{.docutils .literal
            .notranslate}]{.pre}` `{.docutils .literal
            .notranslate}[`one`{.docutils .literal .notranslate}]{.pre}.

        -   **fragment** -- the part of the url after the hash symbol.
            From the sample, this is [`foo`{.docutils .literal
            .notranslate}]{.pre}.

        -   **nofollow** -- an indication of the presence or absence of
            a nofollow value in the [`rel`{.docutils .literal
            .notranslate}]{.pre} attribute of the anchor tag.
:::
:::
:::

[]{#document-topics/settings}

::: {#settings .section}
[]{#topics-settings}

### Settings[¶](#settings "Permalink to this heading"){.headerlink}

The Scrapy settings allows you to customize the behaviour of all Scrapy
components, including the core, extensions, pipelines and spiders
themselves.

The infrastructure of the settings provides a global namespace of
key-value mappings that the code can use to pull configuration values
from. The settings can be populated through different mechanisms, which
are described below.

The settings are also the mechanism for selecting the currently active
Scrapy project (in case you have many).

For a list of available built-in settings see: [[Built-in settings
reference]{.std .std-ref}](#topics-settings-ref){.hoverxref .tooltip
.reference .internal}.

::: {#designating-the-settings .section}
[]{#topics-settings-module-envvar}

#### Designating the settings[¶](#designating-the-settings "Permalink to this heading"){.headerlink}

When you use Scrapy, you have to tell it which settings you're using.
You can do this by using an environment variable,
[`SCRAPY_SETTINGS_MODULE`{.docutils .literal .notranslate}]{.pre}.

The value of [`SCRAPY_SETTINGS_MODULE`{.docutils .literal
.notranslate}]{.pre} should be in Python path syntax, e.g.
[`myproject.settings`{.docutils .literal .notranslate}]{.pre}. Note that
the settings module should be on the Python [[import search path]{.xref
.std
.std-ref}](https://docs.python.org/3/tutorial/modules.html#tut-searchpath "(in Python v3.12)"){.reference
.external}.
:::

::: {#populating-the-settings .section}
[]{#populating-settings}

#### Populating the settings[¶](#populating-the-settings "Permalink to this heading"){.headerlink}

Settings can be populated using different mechanisms, each of which
having a different precedence. Here is the list of them in decreasing
order of precedence:

> <div>
>
> 1.  Command line options (most precedence)
>
> 2.  Settings per-spider
>
> 3.  Project settings module
>
> 4.  Settings set by add-ons
>
> 5.  Default settings per-command
>
> 6.  Default global settings (less precedence)
>
> </div>

The population of these settings sources is taken care of internally,
but a manual handling is possible using API calls. See the [[Settings
API]{.std .std-ref}](index.html#topics-api-settings){.hoverxref .tooltip
.reference .internal} topic for reference.

These mechanisms are described in more detail below.

::: {#command-line-options .section}
##### 1. Command line options[¶](#command-line-options "Permalink to this heading"){.headerlink}

Arguments provided by the command line are the ones that take most
precedence, overriding any other options. You can explicitly override
one (or more) settings using the [`-s`{.docutils .literal
.notranslate}]{.pre} (or [`--set`{.docutils .literal
.notranslate}]{.pre}) command line option.

Example:

::: {.highlight-sh .notranslate}
::: highlight
    scrapy crawl myspider -s LOG_FILE=scrapy.log
:::
:::
:::

::: {#settings-per-spider .section}
##### 2. Settings per-spider[¶](#settings-per-spider "Permalink to this heading"){.headerlink}

Spiders (See the [[Spiders]{.std
.std-ref}](index.html#topics-spiders){.hoverxref .tooltip .reference
.internal} chapter for reference) can define their own settings that
will take precedence and override the project ones. One way to do so is
by setting their [[`custom_settings`{.xref .py .py-attr .docutils
.literal
.notranslate}]{.pre}](index.html#scrapy.Spider.custom_settings "scrapy.Spider.custom_settings"){.reference
.internal} attribute:

::: {.highlight-python .notranslate}
::: highlight
    import scrapy


    class MySpider(scrapy.Spider):
        name = "myspider"

        custom_settings = {
            "SOME_SETTING": "some value",
        }
:::
:::

It's often better to implement [[`update_settings()`{.xref .py .py-meth
.docutils .literal
.notranslate}]{.pre}](index.html#scrapy.Spider.update_settings "scrapy.Spider.update_settings"){.reference
.internal} instead, and settings set there should use the "spider"
priority explicitly:

::: {.highlight-python .notranslate}
::: highlight
    import scrapy


    class MySpider(scrapy.Spider):
        name = "myspider"

        @classmethod
        def update_settings(cls, settings):
            super().update_settings(settings)
            settings.set("SOME_SETTING", "some value", priority="spider")
:::
:::

::: versionadded
[New in version 2.11.]{.versionmodified .added}
:::

It's also possible to modify the settings in the
[[`from_crawler()`{.xref .py .py-meth .docutils .literal
.notranslate}]{.pre}](index.html#scrapy.Spider.from_crawler "scrapy.Spider.from_crawler"){.reference
.internal} method, e.g. based on [[spider arguments]{.std
.std-ref}](index.html#spiderargs){.hoverxref .tooltip .reference
.internal} or other logic:

::: {.highlight-python .notranslate}
::: highlight
    import scrapy


    class MySpider(scrapy.Spider):
        name = "myspider"

        @classmethod
        def from_crawler(cls, crawler, *args, **kwargs):
            spider = super().from_crawler(crawler, *args, **kwargs)
            if "some_argument" in kwargs:
                spider.settings.set(
                    "SOME_SETTING", kwargs["some_argument"], priority="spider"
                )
            return spider
:::
:::
:::

::: {#project-settings-module .section}
##### 3. Project settings module[¶](#project-settings-module "Permalink to this heading"){.headerlink}

The project settings module is the standard configuration file for your
Scrapy project, it's where most of your custom settings will be
populated. For a standard Scrapy project, this means you'll be adding or
changing the settings in the [`settings.py`{.docutils .literal
.notranslate}]{.pre} file created for your project.
:::

::: {#settings-set-by-add-ons .section}
##### 4. Settings set by add-ons[¶](#settings-set-by-add-ons "Permalink to this heading"){.headerlink}

[[Add-ons]{.std .std-ref}](index.html#topics-addons){.hoverxref .tooltip
.reference .internal} can modify settings. They should do this with this
priority, though this is not enforced.
:::

::: {#default-settings-per-command .section}
##### 5. Default settings per-command[¶](#default-settings-per-command "Permalink to this heading"){.headerlink}

Each [[Scrapy
tool]{.doc}](index.html#document-topics/commands){.reference .internal}
command can have its own default settings, which override the global
default settings. Those custom command settings are specified in the
[`default_settings`{.docutils .literal .notranslate}]{.pre} attribute of
the command class.
:::

::: {#default-global-settings .section}
##### 6. Default global settings[¶](#default-global-settings "Permalink to this heading"){.headerlink}

The global defaults are located in the
[`scrapy.settings.default_settings`{.docutils .literal
.notranslate}]{.pre} module and documented in the [[Built-in settings
reference]{.std .std-ref}](#topics-settings-ref){.hoverxref .tooltip
.reference .internal} section.
:::
:::

::: {#compatibility-with-pickle .section}
#### Compatibility with pickle[¶](#compatibility-with-pickle "Permalink to this heading"){.headerlink}

Setting values must be [[picklable]{.xref .std
.std-ref}](https://docs.python.org/3/library/pickle.html#pickle-picklable "(in Python v3.12)"){.reference
.external}.
:::

::: {#import-paths-and-classes .section}
#### Import paths and classes[¶](#import-paths-and-classes "Permalink to this heading"){.headerlink}

::: versionadded
[New in version 2.4.0.]{.versionmodified .added}
:::

When a setting references a callable object to be imported by Scrapy,
such as a class or a function, there are two different ways you can
specify that object:

-   As a string containing the import path of that object

-   As the object itself

For example:

::: {.highlight-python .notranslate}
::: highlight
    from mybot.pipelines.validate import ValidateMyItem

    ITEM_PIPELINES = {
        # passing the classname...
        ValidateMyItem: 300,
        # ...equals passing the class path
        "mybot.pipelines.validate.ValidateMyItem": 300,
    }
:::
:::

::: {.admonition .note}
Note

Passing non-callable objects is not supported.
:::
:::

::: {#how-to-access-settings .section}
#### How to access settings[¶](#how-to-access-settings "Permalink to this heading"){.headerlink}

In a spider, the settings are available through
[`self.settings`{.docutils .literal .notranslate}]{.pre}:

::: {.highlight-python .notranslate}
::: highlight
    class MySpider(scrapy.Spider):
        name = "myspider"
        start_urls = ["http://example.com"]

        def parse(self, response):
            print(f"Existing settings: {self.settings.attributes.keys()}")
:::
:::

::: {.admonition .note}
Note

The [`settings`{.docutils .literal .notranslate}]{.pre} attribute is set
in the base Spider class after the spider is initialized. If you want to
use the settings before the initialization (e.g., in your spider's
[`__init__()`{.docutils .literal .notranslate}]{.pre} method), you'll
need to override the [[`from_crawler()`{.xref .py .py-meth .docutils
.literal
.notranslate}]{.pre}](index.html#scrapy.Spider.from_crawler "scrapy.Spider.from_crawler"){.reference
.internal} method.
:::

Settings can be accessed through the
[[`scrapy.crawler.Crawler.settings`{.xref .py .py-attr .docutils
.literal
.notranslate}]{.pre}](index.html#scrapy.crawler.Crawler.settings "scrapy.crawler.Crawler.settings"){.reference
.internal} attribute of the Crawler that is passed to
[`from_crawler`{.docutils .literal .notranslate}]{.pre} method in
extensions, middlewares and item pipelines:

::: {.highlight-python .notranslate}
::: highlight
    class MyExtension:
        def __init__(self, log_is_enabled=False):
            if log_is_enabled:
                print("log is enabled!")

        @classmethod
        def from_crawler(cls, crawler):
            settings = crawler.settings
            return cls(settings.getbool("LOG_ENABLED"))
:::
:::

The settings object can be used like a dict (e.g.,
[`settings['LOG_ENABLED']`{.docutils .literal .notranslate}]{.pre}), but
it's usually preferred to extract the setting in the format you need it
to avoid type errors, using one of the methods provided by the
[[`Settings`{.xref .py .py-class .docutils .literal
.notranslate}]{.pre}](index.html#scrapy.settings.Settings "scrapy.settings.Settings"){.reference
.internal} API.
:::

::: {#rationale-for-setting-names .section}
#### Rationale for setting names[¶](#rationale-for-setting-names "Permalink to this heading"){.headerlink}

Setting names are usually prefixed with the component that they
configure. For example, proper setting names for a fictional robots.txt
extension would be [`ROBOTSTXT_ENABLED`{.docutils .literal
.notranslate}]{.pre}, [`ROBOTSTXT_OBEY`{.docutils .literal
.notranslate}]{.pre}, [`ROBOTSTXT_CACHEDIR`{.docutils .literal
.notranslate}]{.pre}, etc.
:::

::: {#built-in-settings-reference .section}
[]{#topics-settings-ref}

#### Built-in settings reference[¶](#built-in-settings-reference "Permalink to this heading"){.headerlink}

Here's a list of all available Scrapy settings, in alphabetical order,
along with their default values and the scope where they apply.

The scope, where available, shows where the setting is being used, if
it's tied to any particular component. In that case the module of that
component will be shown, typically an extension, middleware or pipeline.
It also means that the component must be enabled in order for the
setting to have any effect.

::: {#addons .section}
[]{#std-setting-ADDONS}

##### ADDONS[¶](#addons "Permalink to this heading"){.headerlink}

Default: [`{}`{.docutils .literal .notranslate}]{.pre}

A dict containing paths to the add-ons enabled in your project and their
priorities. For more information, see [[Add-ons]{.std
.std-ref}](index.html#topics-addons){.hoverxref .tooltip .reference
.internal}.
:::

::: {#aws-access-key-id .section}
[]{#std-setting-AWS_ACCESS_KEY_ID}

##### AWS_ACCESS_KEY_ID[¶](#aws-access-key-id "Permalink to this heading"){.headerlink}

Default: [`None`{.docutils .literal .notranslate}]{.pre}

The AWS access key used by code that requires access to [Amazon Web
services](https://aws.amazon.com/){.reference .external}, such as the
[[S3 feed storage backend]{.std
.std-ref}](index.html#topics-feed-storage-s3){.hoverxref .tooltip
.reference .internal}.
:::

::: {#aws-secret-access-key .section}
[]{#std-setting-AWS_SECRET_ACCESS_KEY}

##### AWS_SECRET_ACCESS_KEY[¶](#aws-secret-access-key "Permalink to this heading"){.headerlink}

Default: [`None`{.docutils .literal .notranslate}]{.pre}

The AWS secret key used by code that requires access to [Amazon Web
services](https://aws.amazon.com/){.reference .external}, such as the
[[S3 feed storage backend]{.std
.std-ref}](index.html#topics-feed-storage-s3){.hoverxref .tooltip
.reference .internal}.
:::

::: {#aws-session-token .section}
[]{#std-setting-AWS_SESSION_TOKEN}

##### AWS_SESSION_TOKEN[¶](#aws-session-token "Permalink to this heading"){.headerlink}

Default: [`None`{.docutils .literal .notranslate}]{.pre}

The AWS security token used by code that requires access to [Amazon Web
services](https://aws.amazon.com/){.reference .external}, such as the
[[S3 feed storage backend]{.std
.std-ref}](index.html#topics-feed-storage-s3){.hoverxref .tooltip
.reference .internal}, when using [temporary security
credentials](https://docs.aws.amazon.com/general/latest/gr/aws-sec-cred-types.html#temporary-access-keys){.reference
.external}.
:::

::: {#aws-endpoint-url .section}
[]{#std-setting-AWS_ENDPOINT_URL}

##### AWS_ENDPOINT_URL[¶](#aws-endpoint-url "Permalink to this heading"){.headerlink}

Default: [`None`{.docutils .literal .notranslate}]{.pre}

Endpoint URL used for S3-like storage, for example Minio or s3.scality.
:::

::: {#aws-use-ssl .section}
[]{#std-setting-AWS_USE_SSL}

##### AWS_USE_SSL[¶](#aws-use-ssl "Permalink to this heading"){.headerlink}

Default: [`None`{.docutils .literal .notranslate}]{.pre}

Use this option if you want to disable SSL connection for communication
with S3 or S3-like storage. By default SSL will be used.
:::

::: {#aws-verify .section}
[]{#std-setting-AWS_VERIFY}

##### AWS_VERIFY[¶](#aws-verify "Permalink to this heading"){.headerlink}

Default: [`None`{.docutils .literal .notranslate}]{.pre}

Verify SSL connection between Scrapy and S3 or S3-like storage. By
default SSL verification will occur.
:::

::: {#aws-region-name .section}
[]{#std-setting-AWS_REGION_NAME}

##### AWS_REGION_NAME[¶](#aws-region-name "Permalink to this heading"){.headerlink}

Default: [`None`{.docutils .literal .notranslate}]{.pre}

The name of the region associated with the AWS client.
:::

::: {#asyncio-event-loop .section}
[]{#std-setting-ASYNCIO_EVENT_LOOP}

##### ASYNCIO_EVENT_LOOP[¶](#asyncio-event-loop "Permalink to this heading"){.headerlink}

Default: [`None`{.docutils .literal .notranslate}]{.pre}

Import path of a given [`asyncio`{.docutils .literal
.notranslate}]{.pre} event loop class.

If the asyncio reactor is enabled (see [[`TWISTED_REACTOR`{.xref .std
.std-setting .docutils .literal
.notranslate}]{.pre}](#std-setting-TWISTED_REACTOR){.hoverxref .tooltip
.reference .internal}) this setting can be used to specify the asyncio
event loop to be used with it. Set the setting to the import path of the
desired asyncio event loop class. If the setting is set to
[`None`{.docutils .literal .notranslate}]{.pre} the default asyncio
event loop will be used.

If you are installing the asyncio reactor manually using the
[[`install_reactor()`{.xref .py .py-func .docutils .literal
.notranslate}]{.pre}](#scrapy.utils.reactor.install_reactor "scrapy.utils.reactor.install_reactor"){.reference
.internal} function, you can use the [`event_loop_path`{.docutils
.literal .notranslate}]{.pre} parameter to indicate the import path of
the event loop class to be used.

Note that the event loop class must inherit from
[[`asyncio.AbstractEventLoop`{.xref .py .py-class .docutils .literal
.notranslate}]{.pre}](https://docs.python.org/3/library/asyncio-eventloop.html#asyncio.AbstractEventLoop "(in Python v3.12)"){.reference
.external}.

::: {.admonition .caution}
Caution

Please be aware that, when using a non-default event loop (either
defined via [[`ASYNCIO_EVENT_LOOP`{.xref .std .std-setting .docutils
.literal
.notranslate}]{.pre}](#std-setting-ASYNCIO_EVENT_LOOP){.hoverxref
.tooltip .reference .internal} or installed with
[[`install_reactor()`{.xref .py .py-func .docutils .literal
.notranslate}]{.pre}](#scrapy.utils.reactor.install_reactor "scrapy.utils.reactor.install_reactor"){.reference
.internal}), Scrapy will call [[`asyncio.set_event_loop()`{.xref .py
.py-func .docutils .literal
.notranslate}]{.pre}](https://docs.python.org/3/library/asyncio-eventloop.html#asyncio.set_event_loop "(in Python v3.12)"){.reference
.external}, which will set the specified event loop as the current loop
for the current OS thread.
:::
:::

::: {#bot-name .section}
[]{#std-setting-BOT_NAME}

##### BOT_NAME[¶](#bot-name "Permalink to this heading"){.headerlink}

Default: [`'scrapybot'`{.docutils .literal .notranslate}]{.pre}

The name of the bot implemented by this Scrapy project (also known as
the project name). This name will be used for the logging too.

It's automatically populated with your project name when you create your
project with the [[`startproject`{.xref .std .std-command .docutils
.literal
.notranslate}]{.pre}](index.html#std-command-startproject){.hoverxref
.tooltip .reference .internal} command.
:::

::: {#concurrent-items .section}
[]{#std-setting-CONCURRENT_ITEMS}

##### CONCURRENT_ITEMS[¶](#concurrent-items "Permalink to this heading"){.headerlink}

Default: [`100`{.docutils .literal .notranslate}]{.pre}

Maximum number of concurrent items (per response) to process in parallel
in [[item pipelines]{.std
.std-ref}](index.html#topics-item-pipeline){.hoverxref .tooltip
.reference .internal}.
:::

::: {#concurrent-requests .section}
[]{#std-setting-CONCURRENT_REQUESTS}

##### CONCURRENT_REQUESTS[¶](#concurrent-requests "Permalink to this heading"){.headerlink}

Default: [`16`{.docutils .literal .notranslate}]{.pre}

The maximum number of concurrent (i.e. simultaneous) requests that will
be performed by the Scrapy downloader.
:::

::: {#concurrent-requests-per-domain .section}
[]{#std-setting-CONCURRENT_REQUESTS_PER_DOMAIN}

##### CONCURRENT_REQUESTS_PER_DOMAIN[¶](#concurrent-requests-per-domain "Permalink to this heading"){.headerlink}

Default: [`8`{.docutils .literal .notranslate}]{.pre}

The maximum number of concurrent (i.e. simultaneous) requests that will
be performed to any single domain.

See also: [[AutoThrottle extension]{.std
.std-ref}](index.html#topics-autothrottle){.hoverxref .tooltip
.reference .internal} and its [[`AUTOTHROTTLE_TARGET_CONCURRENCY`{.xref
.std .std-setting .docutils .literal
.notranslate}]{.pre}](index.html#std-setting-AUTOTHROTTLE_TARGET_CONCURRENCY){.hoverxref
.tooltip .reference .internal} option.
:::

::: {#concurrent-requests-per-ip .section}
[]{#std-setting-CONCURRENT_REQUESTS_PER_IP}

##### CONCURRENT_REQUESTS_PER_IP[¶](#concurrent-requests-per-ip "Permalink to this heading"){.headerlink}

Default: [`0`{.docutils .literal .notranslate}]{.pre}

The maximum number of concurrent (i.e. simultaneous) requests that will
be performed to any single IP. If non-zero, the
[[`CONCURRENT_REQUESTS_PER_DOMAIN`{.xref .std .std-setting .docutils
.literal
.notranslate}]{.pre}](#std-setting-CONCURRENT_REQUESTS_PER_DOMAIN){.hoverxref
.tooltip .reference .internal} setting is ignored, and this one is used
instead. In other words, concurrency limits will be applied per IP, not
per domain.

This setting also affects [[`DOWNLOAD_DELAY`{.xref .std .std-setting
.docutils .literal
.notranslate}]{.pre}](#std-setting-DOWNLOAD_DELAY){.hoverxref .tooltip
.reference .internal} and [[AutoThrottle extension]{.std
.std-ref}](index.html#topics-autothrottle){.hoverxref .tooltip
.reference .internal}: if [[`CONCURRENT_REQUESTS_PER_IP`{.xref .std
.std-setting .docutils .literal
.notranslate}]{.pre}](#std-setting-CONCURRENT_REQUESTS_PER_IP){.hoverxref
.tooltip .reference .internal} is non-zero, download delay is enforced
per IP, not per domain.
:::

::: {#default-item-class .section}
[]{#std-setting-DEFAULT_ITEM_CLASS}

##### DEFAULT_ITEM_CLASS[¶](#default-item-class "Permalink to this heading"){.headerlink}

Default: [`'scrapy.Item'`{.docutils .literal .notranslate}]{.pre}

The default class that will be used for instantiating items in the [[the
Scrapy shell]{.std .std-ref}](index.html#topics-shell){.hoverxref
.tooltip .reference .internal}.
:::

::: {#default-request-headers .section}
[]{#std-setting-DEFAULT_REQUEST_HEADERS}

##### DEFAULT_REQUEST_HEADERS[¶](#default-request-headers "Permalink to this heading"){.headerlink}

Default:

::: {.highlight-python .notranslate}
::: highlight
    {
        "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8",
        "Accept-Language": "en",
    }
:::
:::

The default headers used for Scrapy HTTP Requests. They're populated in
the [[`DefaultHeadersMiddleware`{.xref .py .py-class .docutils .literal
.notranslate}]{.pre}](index.html#scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware "scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware"){.reference
.internal}.

::: {.admonition .caution}
Caution

Cookies set via the [`Cookie`{.docutils .literal .notranslate}]{.pre}
header are not considered by the [[CookiesMiddleware]{.std
.std-ref}](index.html#cookies-mw){.hoverxref .tooltip .reference
.internal}. If you need to set cookies for a request, use the
[`Request.cookies`{.xref .py .py-class .docutils .literal
.notranslate}]{.pre} parameter. This is a known current limitation that
is being worked on.
:::
:::

::: {#depth-limit .section}
[]{#std-setting-DEPTH_LIMIT}

##### DEPTH_LIMIT[¶](#depth-limit "Permalink to this heading"){.headerlink}

Default: [`0`{.docutils .literal .notranslate}]{.pre}

Scope: [`scrapy.spidermiddlewares.depth.DepthMiddleware`{.docutils
.literal .notranslate}]{.pre}

The maximum depth that will be allowed to crawl for any site. If zero,
no limit will be imposed.
:::

::: {#depth-priority .section}
[]{#std-setting-DEPTH_PRIORITY}

##### DEPTH_PRIORITY[¶](#depth-priority "Permalink to this heading"){.headerlink}

Default: [`0`{.docutils .literal .notranslate}]{.pre}

Scope: [`scrapy.spidermiddlewares.depth.DepthMiddleware`{.docutils
.literal .notranslate}]{.pre}

An integer that is used to adjust the [`priority`{.xref .py .py-attr
.docutils .literal .notranslate}]{.pre} of a [`Request`{.xref .py
.py-class .docutils .literal .notranslate}]{.pre} based on its depth.

The priority of a request is adjusted as follows:

::: {.highlight-python .notranslate}
::: highlight
    request.priority = request.priority - (depth * DEPTH_PRIORITY)
:::
:::

As depth increases, positive values of [`DEPTH_PRIORITY`{.docutils
.literal .notranslate}]{.pre} decrease request priority (BFO), while
negative values increase request priority (DFO). See also [[Does Scrapy
crawl in breadth-first or depth-first order?]{.std
.std-ref}](index.html#faq-bfo-dfo){.hoverxref .tooltip .reference
.internal}.

::: {.admonition .note}
Note

This setting adjusts priority **in the opposite way** compared to other
priority settings [[`REDIRECT_PRIORITY_ADJUST`{.xref .std .std-setting
.docutils .literal
.notranslate}]{.pre}](#std-setting-REDIRECT_PRIORITY_ADJUST){.hoverxref
.tooltip .reference .internal} and [[`RETRY_PRIORITY_ADJUST`{.xref .std
.std-setting .docutils .literal
.notranslate}]{.pre}](index.html#std-setting-RETRY_PRIORITY_ADJUST){.hoverxref
.tooltip .reference .internal}.
:::
:::

::: {#depth-stats-verbose .section}
[]{#std-setting-DEPTH_STATS_VERBOSE}

##### DEPTH_STATS_VERBOSE[¶](#depth-stats-verbose "Permalink to this heading"){.headerlink}

Default: [`False`{.docutils .literal .notranslate}]{.pre}

Scope: [`scrapy.spidermiddlewares.depth.DepthMiddleware`{.docutils
.literal .notranslate}]{.pre}

Whether to collect verbose depth stats. If this is enabled, the number
of requests for each depth is collected in the stats.
:::

::: {#dnscache-enabled .section}
[]{#std-setting-DNSCACHE_ENABLED}

##### DNSCACHE_ENABLED[¶](#dnscache-enabled "Permalink to this heading"){.headerlink}

Default: [`True`{.docutils .literal .notranslate}]{.pre}

Whether to enable DNS in-memory cache.
:::

::: {#dnscache-size .section}
[]{#std-setting-DNSCACHE_SIZE}

##### DNSCACHE_SIZE[¶](#dnscache-size "Permalink to this heading"){.headerlink}

Default: [`10000`{.docutils .literal .notranslate}]{.pre}

DNS in-memory cache size.
:::

::: {#dns-resolver .section}
[]{#std-setting-DNS_RESOLVER}

##### DNS_RESOLVER[¶](#dns-resolver "Permalink to this heading"){.headerlink}

::: versionadded
[New in version 2.0.]{.versionmodified .added}
:::

Default: [`'scrapy.resolver.CachingThreadedResolver'`{.docutils .literal
.notranslate}]{.pre}

The class to be used to resolve DNS names. The default
[`scrapy.resolver.CachingThreadedResolver`{.docutils .literal
.notranslate}]{.pre} supports specifying a timeout for DNS requests via
the [[`DNS_TIMEOUT`{.xref .std .std-setting .docutils .literal
.notranslate}]{.pre}](#std-setting-DNS_TIMEOUT){.hoverxref .tooltip
.reference .internal} setting, but works only with IPv4 addresses.
Scrapy provides an alternative resolver,
[`scrapy.resolver.CachingHostnameResolver`{.docutils .literal
.notranslate}]{.pre}, which supports IPv4/IPv6 addresses but does not
take the [[`DNS_TIMEOUT`{.xref .std .std-setting .docutils .literal
.notranslate}]{.pre}](#std-setting-DNS_TIMEOUT){.hoverxref .tooltip
.reference .internal} setting into account.
:::

::: {#dns-timeout .section}
[]{#std-setting-DNS_TIMEOUT}

##### DNS_TIMEOUT[¶](#dns-timeout "Permalink to this heading"){.headerlink}

Default: [`60`{.docutils .literal .notranslate}]{.pre}

Timeout for processing of DNS queries in seconds. Float is supported.
:::

::: {#downloader .section}
[]{#std-setting-DOWNLOADER}

##### DOWNLOADER[¶](#downloader "Permalink to this heading"){.headerlink}

Default: [`'scrapy.core.downloader.Downloader'`{.docutils .literal
.notranslate}]{.pre}

The downloader to use for crawling.
:::

::: {#downloader-httpclientfactory .section}
[]{#std-setting-DOWNLOADER_HTTPCLIENTFACTORY}

##### DOWNLOADER_HTTPCLIENTFACTORY[¶](#downloader-httpclientfactory "Permalink to this heading"){.headerlink}

Default:
[`'scrapy.core.downloader.webclient.ScrapyHTTPClientFactory'`{.docutils
.literal .notranslate}]{.pre}

Defines a Twisted [`protocol.ClientFactory`{.docutils .literal
.notranslate}]{.pre} class to use for HTTP/1.0 connections (for
[`HTTP10DownloadHandler`{.docutils .literal .notranslate}]{.pre}).

::: {.admonition .note}
Note

HTTP/1.0 is rarely used nowadays so you can safely ignore this setting,
unless you really want to use HTTP/1.0 and override
[[`DOWNLOAD_HANDLERS`{.xref .std .std-setting .docutils .literal
.notranslate}]{.pre}](#std-setting-DOWNLOAD_HANDLERS){.hoverxref
.tooltip .reference .internal} for [`http(s)`{.docutils .literal
.notranslate}]{.pre} scheme accordingly, i.e. to
[`'scrapy.core.downloader.handlers.http.HTTP10DownloadHandler'`{.docutils
.literal .notranslate}]{.pre}.
:::
:::

::: {#downloader-clientcontextfactory .section}
[]{#std-setting-DOWNLOADER_CLIENTCONTEXTFACTORY}

##### DOWNLOADER_CLIENTCONTEXTFACTORY[¶](#downloader-clientcontextfactory "Permalink to this heading"){.headerlink}

Default:
[`'scrapy.core.downloader.contextfactory.ScrapyClientContextFactory'`{.docutils
.literal .notranslate}]{.pre}

Represents the classpath to the ContextFactory to use.

Here, "ContextFactory" is a Twisted term for SSL/TLS contexts, defining
the TLS/SSL protocol version to use, whether to do certificate
verification, or even enable client-side authentication (and various
other things).

::: {.admonition .note}
Note

Scrapy default context factory **does NOT perform remote server
certificate verification**. This is usually fine for web scraping.

If you do need remote server certificate verification enabled, Scrapy
also has another context factory class that you can set,
[`'scrapy.core.downloader.contextfactory.BrowserLikeContextFactory'`{.docutils
.literal .notranslate}]{.pre}, which uses the platform's certificates to
validate remote endpoints.
:::

If you do use a custom ContextFactory, make sure its
[`__init__`{.docutils .literal .notranslate}]{.pre} method accepts a
[`method`{.docutils .literal .notranslate}]{.pre} parameter (this is the
[`OpenSSL.SSL`{.docutils .literal .notranslate}]{.pre} method mapping
[[`DOWNLOADER_CLIENT_TLS_METHOD`{.xref .std .std-setting .docutils
.literal
.notranslate}]{.pre}](#std-setting-DOWNLOADER_CLIENT_TLS_METHOD){.hoverxref
.tooltip .reference .internal}), a [`tls_verbose_logging`{.docutils
.literal .notranslate}]{.pre} parameter ([`bool`{.docutils .literal
.notranslate}]{.pre}) and a [`tls_ciphers`{.docutils .literal
.notranslate}]{.pre} parameter (see
[[`DOWNLOADER_CLIENT_TLS_CIPHERS`{.xref .std .std-setting .docutils
.literal
.notranslate}]{.pre}](#std-setting-DOWNLOADER_CLIENT_TLS_CIPHERS){.hoverxref
.tooltip .reference .internal}).
:::

::: {#downloader-client-tls-ciphers .section}
[]{#std-setting-DOWNLOADER_CLIENT_TLS_CIPHERS}

##### DOWNLOADER_CLIENT_TLS_CIPHERS[¶](#downloader-client-tls-ciphers "Permalink to this heading"){.headerlink}

Default: [`'DEFAULT'`{.docutils .literal .notranslate}]{.pre}

Use this setting to customize the TLS/SSL ciphers used by the default
HTTP/1.1 downloader.

The setting should contain a string in the [OpenSSL cipher list
format](https://www.openssl.org/docs/manmaster/man1/openssl-ciphers.html#CIPHER-LIST-FORMAT){.reference
.external}, these ciphers will be used as client ciphers. Changing this
setting may be necessary to access certain HTTPS websites: for example,
you may need to use [`'DEFAULT:!DH'`{.docutils .literal
.notranslate}]{.pre} for a website with weak DH parameters or enable a
specific cipher that is not included in [`DEFAULT`{.docutils .literal
.notranslate}]{.pre} if a website requires it.
:::

::: {#downloader-client-tls-method .section}
[]{#std-setting-DOWNLOADER_CLIENT_TLS_METHOD}

##### DOWNLOADER_CLIENT_TLS_METHOD[¶](#downloader-client-tls-method "Permalink to this heading"){.headerlink}

Default: [`'TLS'`{.docutils .literal .notranslate}]{.pre}

Use this setting to customize the TLS/SSL method used by the default
HTTP/1.1 downloader.

This setting must be one of these string values:

-   [`'TLS'`{.docutils .literal .notranslate}]{.pre}: maps to OpenSSL's
    [`TLS_method()`{.docutils .literal .notranslate}]{.pre} (a.k.a
    [`SSLv23_method()`{.docutils .literal .notranslate}]{.pre}), which
    allows protocol negotiation, starting from the highest supported by
    the platform; **default, recommended**

-   [`'TLSv1.0'`{.docutils .literal .notranslate}]{.pre}: this value
    forces HTTPS connections to use TLS version 1.0 ; set this if you
    want the behavior of Scrapy\<1.1

-   [`'TLSv1.1'`{.docutils .literal .notranslate}]{.pre}: forces TLS
    version 1.1

-   [`'TLSv1.2'`{.docutils .literal .notranslate}]{.pre}: forces TLS
    version 1.2
:::

::: {#downloader-client-tls-verbose-logging .section}
[]{#std-setting-DOWNLOADER_CLIENT_TLS_VERBOSE_LOGGING}

##### DOWNLOADER_CLIENT_TLS_VERBOSE_LOGGING[¶](#downloader-client-tls-verbose-logging "Permalink to this heading"){.headerlink}

Default: [`False`{.docutils .literal .notranslate}]{.pre}

Setting this to [`True`{.docutils .literal .notranslate}]{.pre} will
enable DEBUG level messages about TLS connection parameters after
establishing HTTPS connections. The kind of information logged depends
on the versions of OpenSSL and pyOpenSSL.

This setting is only used for the default
[[`DOWNLOADER_CLIENTCONTEXTFACTORY`{.xref .std .std-setting .docutils
.literal
.notranslate}]{.pre}](#std-setting-DOWNLOADER_CLIENTCONTEXTFACTORY){.hoverxref
.tooltip .reference .internal}.
:::

::: {#downloader-middlewares .section}
[]{#std-setting-DOWNLOADER_MIDDLEWARES}

##### DOWNLOADER_MIDDLEWARES[¶](#downloader-middlewares "Permalink to this heading"){.headerlink}

Default:: [`{}`{.docutils .literal .notranslate}]{.pre}

A dict containing the downloader middlewares enabled in your project,
and their orders. For more info see [[Activating a downloader
middleware]{.std
.std-ref}](index.html#topics-downloader-middleware-setting){.hoverxref
.tooltip .reference .internal}.
:::

::: {#downloader-middlewares-base .section}
[]{#std-setting-DOWNLOADER_MIDDLEWARES_BASE}

##### DOWNLOADER_MIDDLEWARES_BASE[¶](#downloader-middlewares-base "Permalink to this heading"){.headerlink}

Default:

::: {.highlight-python .notranslate}
::: highlight
    {
        "scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware": 100,
        "scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware": 300,
        "scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware": 350,
        "scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware": 400,
        "scrapy.downloadermiddlewares.useragent.UserAgentMiddleware": 500,
        "scrapy.downloadermiddlewares.retry.RetryMiddleware": 550,
        "scrapy.downloadermiddlewares.ajaxcrawl.AjaxCrawlMiddleware": 560,
        "scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware": 580,
        "scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware": 590,
        "scrapy.downloadermiddlewares.redirect.RedirectMiddleware": 600,
        "scrapy.downloadermiddlewares.cookies.CookiesMiddleware": 700,
        "scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware": 750,
        "scrapy.downloadermiddlewares.stats.DownloaderStats": 850,
        "scrapy.downloadermiddlewares.httpcache.HttpCacheMiddleware": 900,
    }
:::
:::

A dict containing the downloader middlewares enabled by default in
Scrapy. Low orders are closer to the engine, high orders are closer to
the downloader. You should never modify this setting in your project,
modify [[`DOWNLOADER_MIDDLEWARES`{.xref .std .std-setting .docutils
.literal
.notranslate}]{.pre}](#std-setting-DOWNLOADER_MIDDLEWARES){.hoverxref
.tooltip .reference .internal} instead. For more info see [[Activating a
downloader middleware]{.std
.std-ref}](index.html#topics-downloader-middleware-setting){.hoverxref
.tooltip .reference .internal}.
:::

::: {#downloader-stats .section}
[]{#std-setting-DOWNLOADER_STATS}

##### DOWNLOADER_STATS[¶](#downloader-stats "Permalink to this heading"){.headerlink}

Default: [`True`{.docutils .literal .notranslate}]{.pre}

Whether to enable downloader stats collection.
:::

::: {#download-delay .section}
[]{#std-setting-DOWNLOAD_DELAY}

##### DOWNLOAD_DELAY[¶](#download-delay "Permalink to this heading"){.headerlink}

Default: [`0`{.docutils .literal .notranslate}]{.pre}

Minimum seconds to wait between 2 consecutive requests to the same
domain.

Use [[`DOWNLOAD_DELAY`{.xref .std .std-setting .docutils .literal
.notranslate}]{.pre}](#std-setting-DOWNLOAD_DELAY){.hoverxref .tooltip
.reference .internal} to throttle your crawling speed, to avoid hitting
servers too hard.

Decimal numbers are supported. For example, to send a maximum of 4
requests every 10 seconds:

::: {.highlight-python .notranslate}
::: highlight
    DOWNLOAD_DELAY = 2.5
:::
:::

This setting is also affected by the [[`RANDOMIZE_DOWNLOAD_DELAY`{.xref
.std .std-setting .docutils .literal
.notranslate}]{.pre}](#std-setting-RANDOMIZE_DOWNLOAD_DELAY){.hoverxref
.tooltip .reference .internal} setting, which is enabled by default.

When [[`CONCURRENT_REQUESTS_PER_IP`{.xref .std .std-setting .docutils
.literal
.notranslate}]{.pre}](#std-setting-CONCURRENT_REQUESTS_PER_IP){.hoverxref
.tooltip .reference .internal} is non-zero, delays are enforced per IP
address instead of per domain.

Note that [[`DOWNLOAD_DELAY`{.xref .std .std-setting .docutils .literal
.notranslate}]{.pre}](#std-setting-DOWNLOAD_DELAY){.hoverxref .tooltip
.reference .internal} can lower the effective per-domain concurrency
below [[`CONCURRENT_REQUESTS_PER_DOMAIN`{.xref .std .std-setting
.docutils .literal
.notranslate}]{.pre}](#std-setting-CONCURRENT_REQUESTS_PER_DOMAIN){.hoverxref
.tooltip .reference .internal}. If the response time of a domain is
lower than [[`DOWNLOAD_DELAY`{.xref .std .std-setting .docutils .literal
.notranslate}]{.pre}](#std-setting-DOWNLOAD_DELAY){.hoverxref .tooltip
.reference .internal}, the effective concurrency for that domain is 1.
When testing throttling configurations, it usually makes sense to lower
[[`CONCURRENT_REQUESTS_PER_DOMAIN`{.xref .std .std-setting .docutils
.literal
.notranslate}]{.pre}](#std-setting-CONCURRENT_REQUESTS_PER_DOMAIN){.hoverxref
.tooltip .reference .internal} first, and only increase
[[`DOWNLOAD_DELAY`{.xref .std .std-setting .docutils .literal
.notranslate}]{.pre}](#std-setting-DOWNLOAD_DELAY){.hoverxref .tooltip
.reference .internal} once [[`CONCURRENT_REQUESTS_PER_DOMAIN`{.xref .std
.std-setting .docutils .literal
.notranslate}]{.pre}](#std-setting-CONCURRENT_REQUESTS_PER_DOMAIN){.hoverxref
.tooltip .reference .internal} is 1 but a higher throttling is desired.

::: {#spider-download-delay-attribute .admonition .note}
Note

This delay can be set per spider using [`download_delay`{.xref .py
.py-attr .docutils .literal .notranslate}]{.pre} spider attribute.
:::

It is also possible to change this setting per domain, although it
requires non-trivial code. See the implementation of the
[[AutoThrottle]{.std
.std-ref}](index.html#topics-autothrottle){.hoverxref .tooltip
.reference .internal} extension for an example.
:::

::: {#download-handlers .section}
[]{#std-setting-DOWNLOAD_HANDLERS}

##### DOWNLOAD_HANDLERS[¶](#download-handlers "Permalink to this heading"){.headerlink}

Default: [`{}`{.docutils .literal .notranslate}]{.pre}

A dict containing the request downloader handlers enabled in your
project. See [[`DOWNLOAD_HANDLERS_BASE`{.xref .std .std-setting
.docutils .literal
.notranslate}]{.pre}](#std-setting-DOWNLOAD_HANDLERS_BASE){.hoverxref
.tooltip .reference .internal} for example format.
:::

::: {#download-handlers-base .section}
[]{#std-setting-DOWNLOAD_HANDLERS_BASE}

##### DOWNLOAD_HANDLERS_BASE[¶](#download-handlers-base "Permalink to this heading"){.headerlink}

Default:

::: {.highlight-python .notranslate}
::: highlight
    {
        "data": "scrapy.core.downloader.handlers.datauri.DataURIDownloadHandler",
        "file": "scrapy.core.downloader.handlers.file.FileDownloadHandler",
        "http": "scrapy.core.downloader.handlers.http.HTTPDownloadHandler",
        "https": "scrapy.core.downloader.handlers.http.HTTPDownloadHandler",
        "s3": "scrapy.core.downloader.handlers.s3.S3DownloadHandler",
        "ftp": "scrapy.core.downloader.handlers.ftp.FTPDownloadHandler",
    }
:::
:::

A dict containing the request download handlers enabled by default in
Scrapy. You should never modify this setting in your project, modify
[[`DOWNLOAD_HANDLERS`{.xref .std .std-setting .docutils .literal
.notranslate}]{.pre}](#std-setting-DOWNLOAD_HANDLERS){.hoverxref
.tooltip .reference .internal} instead.

You can disable any of these download handlers by assigning
[`None`{.docutils .literal .notranslate}]{.pre} to their URI scheme in
[[`DOWNLOAD_HANDLERS`{.xref .std .std-setting .docutils .literal
.notranslate}]{.pre}](#std-setting-DOWNLOAD_HANDLERS){.hoverxref
.tooltip .reference .internal}. E.g., to disable the built-in FTP
handler (without replacement), place this in your
[`settings.py`{.docutils .literal .notranslate}]{.pre}:

::: {.highlight-python .notranslate}
::: highlight
    DOWNLOAD_HANDLERS = {
        "ftp": None,
    }
:::
:::

The default HTTPS handler uses HTTP/1.1. To use HTTP/2:

1.  Install [`Twisted[http2]>=17.9.0`{.docutils .literal
    .notranslate}]{.pre} to install the packages required to enable
    HTTP/2 support in Twisted.

2.  Update [[`DOWNLOAD_HANDLERS`{.xref .std .std-setting .docutils
    .literal
    .notranslate}]{.pre}](#std-setting-DOWNLOAD_HANDLERS){.hoverxref
    .tooltip .reference .internal} as follows:

    ::: {.highlight-python .notranslate}
    ::: highlight
        DOWNLOAD_HANDLERS = {
            "https": "scrapy.core.downloader.handlers.http2.H2DownloadHandler",
        }
    :::
    :::

::: {.admonition .warning}
Warning

HTTP/2 support in Scrapy is experimental, and not yet recommended for
production environments. Future Scrapy versions may introduce related
changes without a deprecation period or warning.
:::

::: {.admonition .note}
Note

Known limitations of the current HTTP/2 implementation of Scrapy
include:

-   No support for HTTP/2 Cleartext (h2c), since no major browser
    supports HTTP/2 unencrypted (refer [http2
    faq](https://http2.github.io/faq/#does-http2-require-encryption){.reference
    .external}).

-   No setting to specify a maximum [frame
    size](https://tools.ietf.org/html/rfc7540#section-4.2){.reference
    .external} larger than the default value, 16384. Connections to
    servers that send a larger frame will fail.

-   No support for [server
    pushes](https://tools.ietf.org/html/rfc7540#section-8.2){.reference
    .external}, which are ignored.

-   No support for the [[`bytes_received`{.xref .std .std-signal
    .docutils .literal
    .notranslate}]{.pre}](index.html#std-signal-bytes_received){.hoverxref
    .tooltip .reference .internal} and [[`headers_received`{.xref .std
    .std-signal .docutils .literal
    .notranslate}]{.pre}](index.html#std-signal-headers_received){.hoverxref
    .tooltip .reference .internal} signals.
:::
:::

::: {#download-slots .section}
[]{#std-setting-DOWNLOAD_SLOTS}

##### DOWNLOAD_SLOTS[¶](#download-slots "Permalink to this heading"){.headerlink}

Default: [`{}`{.docutils .literal .notranslate}]{.pre}

Allows to define concurrency/delay parameters on per slot (domain)
basis:

> <div>
>
> ::: {.highlight-python .notranslate}
> ::: highlight
>     DOWNLOAD_SLOTS = {
>         "quotes.toscrape.com": {"concurrency": 1, "delay": 2, "randomize_delay": False},
>         "books.toscrape.com": {"delay": 3, "randomize_delay": False},
>     }
> :::
> :::
>
> </div>

::: {.admonition .note}
Note

For other downloader slots default settings values will be used:

-   [[`DOWNLOAD_DELAY`{.xref .std .std-setting .docutils .literal
    .notranslate}]{.pre}](#std-setting-DOWNLOAD_DELAY){.hoverxref
    .tooltip .reference .internal}: [`delay`{.docutils .literal
    .notranslate}]{.pre}

-   [[`CONCURRENT_REQUESTS_PER_DOMAIN`{.xref .std .std-setting .docutils
    .literal
    .notranslate}]{.pre}](#std-setting-CONCURRENT_REQUESTS_PER_DOMAIN){.hoverxref
    .tooltip .reference .internal}: [`concurrency`{.docutils .literal
    .notranslate}]{.pre}

-   [[`RANDOMIZE_DOWNLOAD_DELAY`{.xref .std .std-setting .docutils
    .literal
    .notranslate}]{.pre}](#std-setting-RANDOMIZE_DOWNLOAD_DELAY){.hoverxref
    .tooltip .reference .internal}: [`randomize_delay`{.docutils
    .literal .notranslate}]{.pre}
:::
:::

::: {#download-timeout .section}
[]{#std-setting-DOWNLOAD_TIMEOUT}

##### DOWNLOAD_TIMEOUT[¶](#download-timeout "Permalink to this heading"){.headerlink}

Default: [`180`{.docutils .literal .notranslate}]{.pre}

The amount of time (in secs) that the downloader will wait before timing
out.

::: {.admonition .note}
Note

This timeout can be set per spider using [`download_timeout`{.xref .py
.py-attr .docutils .literal .notranslate}]{.pre} spider attribute and
per-request using [[`download_timeout`{.xref .std .std-reqmeta .docutils
.literal
.notranslate}]{.pre}](index.html#std-reqmeta-download_timeout){.hoverxref
.tooltip .reference .internal} Request.meta key.
:::
:::

::: {#download-maxsize .section}
[]{#std-setting-DOWNLOAD_MAXSIZE}

##### DOWNLOAD_MAXSIZE[¶](#download-maxsize "Permalink to this heading"){.headerlink}

Default: [`1073741824`{.docutils .literal .notranslate}]{.pre} (1024MB)

The maximum response size (in bytes) that downloader will download.

If you want to disable it set to 0.

::: {#std-reqmeta-download_maxsize .admonition .note}
Note

This size can be set per spider using [`download_maxsize`{.xref .py
.py-attr .docutils .literal .notranslate}]{.pre} spider attribute and
per-request using [[`download_maxsize`{.xref .std .std-reqmeta .docutils
.literal .notranslate}]{.pre}](#std-reqmeta-download_maxsize){.hoverxref
.tooltip .reference .internal} Request.meta key.
:::
:::

::: {#download-warnsize .section}
[]{#std-setting-DOWNLOAD_WARNSIZE}

##### DOWNLOAD_WARNSIZE[¶](#download-warnsize "Permalink to this heading"){.headerlink}

Default: [`33554432`{.docutils .literal .notranslate}]{.pre} (32MB)

The response size (in bytes) that downloader will start to warn.

If you want to disable it set to 0.

::: {.admonition .note}
Note

This size can be set per spider using [`download_warnsize`{.xref .py
.py-attr .docutils .literal .notranslate}]{.pre} spider attribute and
per-request using [`download_warnsize`{.xref .std .std-reqmeta .docutils
.literal .notranslate}]{.pre} Request.meta key.
:::
:::

::: {#download-fail-on-dataloss .section}
[]{#std-setting-DOWNLOAD_FAIL_ON_DATALOSS}

##### DOWNLOAD_FAIL_ON_DATALOSS[¶](#download-fail-on-dataloss "Permalink to this heading"){.headerlink}

Default: [`True`{.docutils .literal .notranslate}]{.pre}

Whether or not to fail on broken responses, that is, declared
[`Content-Length`{.docutils .literal .notranslate}]{.pre} does not match
content sent by the server or chunked response was not properly finish.
If [`True`{.docutils .literal .notranslate}]{.pre}, these responses
raise a [`ResponseFailed([_DataLoss])`{.docutils .literal
.notranslate}]{.pre} error. If [`False`{.docutils .literal
.notranslate}]{.pre}, these responses are passed through and the flag
[`dataloss`{.docutils .literal .notranslate}]{.pre} is added to the
response, i.e.: [`'dataloss'`{.docutils .literal
.notranslate}]{.pre}` `{.docutils .literal .notranslate}[`in`{.docutils
.literal .notranslate}]{.pre}` `{.docutils .literal
.notranslate}[`response.flags`{.docutils .literal .notranslate}]{.pre}
is [`True`{.docutils .literal .notranslate}]{.pre}.

Optionally, this can be set per-request basis by using the
[[`download_fail_on_dataloss`{.xref .std .std-reqmeta .docutils .literal
.notranslate}]{.pre}](index.html#std-reqmeta-download_fail_on_dataloss){.hoverxref
.tooltip .reference .internal} Request.meta key to [`False`{.docutils
.literal .notranslate}]{.pre}.

::: {.admonition .note}
Note

A broken response, or data loss error, may happen under several
circumstances, from server misconfiguration to network errors to data
corruption. It is up to the user to decide if it makes sense to process
broken responses considering they may contain partial or incomplete
content. If [[`RETRY_ENABLED`{.xref .std .std-setting .docutils .literal
.notranslate}]{.pre}](index.html#std-setting-RETRY_ENABLED){.hoverxref
.tooltip .reference .internal} is [`True`{.docutils .literal
.notranslate}]{.pre} and this setting is set to [`True`{.docutils
.literal .notranslate}]{.pre}, the
[`ResponseFailed([_DataLoss])`{.docutils .literal .notranslate}]{.pre}
failure will be retried as usual.
:::

::: {.admonition .warning}
Warning

This setting is ignored by the [`H2DownloadHandler`{.xref .py .py-class
.docutils .literal .notranslate}]{.pre} download handler (see
[[`DOWNLOAD_HANDLERS`{.xref .std .std-setting .docutils .literal
.notranslate}]{.pre}](#std-setting-DOWNLOAD_HANDLERS){.hoverxref
.tooltip .reference .internal}). In case of a data loss error, the
corresponding HTTP/2 connection may be corrupted, affecting other
requests that use the same connection; hence, a
[`ResponseFailed([InvalidBodyLengthError])`{.docutils .literal
.notranslate}]{.pre} failure is always raised for every request that was
using that connection.
:::
:::

::: {#dupefilter-class .section}
[]{#std-setting-DUPEFILTER_CLASS}

##### DUPEFILTER_CLASS[¶](#dupefilter-class "Permalink to this heading"){.headerlink}

Default: [`'scrapy.dupefilters.RFPDupeFilter'`{.docutils .literal
.notranslate}]{.pre}

The class used to detect and filter duplicate requests.

The default ([`RFPDupeFilter`{.docutils .literal .notranslate}]{.pre})
filters based on the [[`REQUEST_FINGERPRINTER_CLASS`{.xref .std
.std-setting .docutils .literal
.notranslate}]{.pre}](index.html#std-setting-REQUEST_FINGERPRINTER_CLASS){.hoverxref
.tooltip .reference .internal} setting.

You can disable filtering of duplicate requests by setting
[[`DUPEFILTER_CLASS`{.xref .std .std-setting .docutils .literal
.notranslate}]{.pre}](#std-setting-DUPEFILTER_CLASS){.hoverxref .tooltip
.reference .internal} to
[`'scrapy.dupefilters.BaseDupeFilter'`{.docutils .literal
.notranslate}]{.pre}. Be very careful about this however, because you
can get into crawling loops. It's usually a better idea to set the
[`dont_filter`{.docutils .literal .notranslate}]{.pre} parameter to
[`True`{.docutils .literal .notranslate}]{.pre} on the specific
[`Request`{.xref .py .py-class .docutils .literal .notranslate}]{.pre}
that should not be filtered.
:::

::: {#dupefilter-debug .section}
[]{#std-setting-DUPEFILTER_DEBUG}

##### DUPEFILTER_DEBUG[¶](#dupefilter-debug "Permalink to this heading"){.headerlink}

Default: [`False`{.docutils .literal .notranslate}]{.pre}

By default, [`RFPDupeFilter`{.docutils .literal .notranslate}]{.pre}
only logs the first duplicate request. Setting
[[`DUPEFILTER_DEBUG`{.xref .std .std-setting .docutils .literal
.notranslate}]{.pre}](#std-setting-DUPEFILTER_DEBUG){.hoverxref .tooltip
.reference .internal} to [`True`{.docutils .literal .notranslate}]{.pre}
will make it log all duplicate requests.
:::

::: {#editor .section}
[]{#std-setting-EDITOR}

##### EDITOR[¶](#editor "Permalink to this heading"){.headerlink}

Default: [`vi`{.docutils .literal .notranslate}]{.pre} (on Unix systems)
or the IDLE editor (on Windows)

The editor to use for editing spiders with the [[`edit`{.xref .std
.std-command .docutils .literal
.notranslate}]{.pre}](index.html#std-command-edit){.hoverxref .tooltip
.reference .internal} command. Additionally, if the [`EDITOR`{.docutils
.literal .notranslate}]{.pre} environment variable is set, the
[[`edit`{.xref .std .std-command .docutils .literal
.notranslate}]{.pre}](index.html#std-command-edit){.hoverxref .tooltip
.reference .internal} command will prefer it over the default setting.
:::

::: {#extensions .section}
[]{#std-setting-EXTENSIONS}

##### EXTENSIONS[¶](#extensions "Permalink to this heading"){.headerlink}

Default:: [`{}`{.docutils .literal .notranslate}]{.pre}

A dict containing the extensions enabled in your project, and their
orders.
:::

::: {#extensions-base .section}
[]{#std-setting-EXTENSIONS_BASE}

##### EXTENSIONS_BASE[¶](#extensions-base "Permalink to this heading"){.headerlink}

Default:

::: {.highlight-python .notranslate}
::: highlight
    {
        "scrapy.extensions.corestats.CoreStats": 0,
        "scrapy.extensions.telnet.TelnetConsole": 0,
        "scrapy.extensions.memusage.MemoryUsage": 0,
        "scrapy.extensions.memdebug.MemoryDebugger": 0,
        "scrapy.extensions.closespider.CloseSpider": 0,
        "scrapy.extensions.feedexport.FeedExporter": 0,
        "scrapy.extensions.logstats.LogStats": 0,
        "scrapy.extensions.spiderstate.SpiderState": 0,
        "scrapy.extensions.throttle.AutoThrottle": 0,
    }
:::
:::

A dict containing the extensions available by default in Scrapy, and
their orders. This setting contains all stable built-in extensions. Keep
in mind that some of them need to be enabled through a setting.

For more information See the [[extensions user guide]{.std
.std-ref}](index.html#topics-extensions){.hoverxref .tooltip .reference
.internal} and the [[list of available extensions]{.std
.std-ref}](index.html#topics-extensions-ref){.hoverxref .tooltip
.reference .internal}.
:::

::: {#feed-tempdir .section}
[]{#std-setting-FEED_TEMPDIR}

##### FEED_TEMPDIR[¶](#feed-tempdir "Permalink to this heading"){.headerlink}

The Feed Temp dir allows you to set a custom folder to save crawler
temporary files before uploading with [[FTP feed storage]{.std
.std-ref}](index.html#topics-feed-storage-ftp){.hoverxref .tooltip
.reference .internal} and [[Amazon S3]{.std
.std-ref}](index.html#topics-feed-storage-s3){.hoverxref .tooltip
.reference .internal}.
:::

::: {#feed-storage-gcs-acl .section}
[]{#std-setting-FEED_STORAGE_GCS_ACL}

##### FEED_STORAGE_GCS_ACL[¶](#feed-storage-gcs-acl "Permalink to this heading"){.headerlink}

The Access Control List (ACL) used when storing items to [[Google Cloud
Storage]{.std .std-ref}](index.html#topics-feed-storage-gcs){.hoverxref
.tooltip .reference .internal}. For more information on how to set this
value, please refer to the column *JSON API* in [Google Cloud
documentation](https://cloud.google.com/storage/docs/access-control/lists){.reference
.external}.
:::

::: {#ftp-passive-mode .section}
[]{#std-setting-FTP_PASSIVE_MODE}

##### FTP_PASSIVE_MODE[¶](#ftp-passive-mode "Permalink to this heading"){.headerlink}

Default: [`True`{.docutils .literal .notranslate}]{.pre}

Whether or not to use passive mode when initiating FTP transfers.

[]{#std-reqmeta-ftp_password .target}
:::

::: {#ftp-password .section}
[]{#std-setting-FTP_PASSWORD}

##### FTP_PASSWORD[¶](#ftp-password "Permalink to this heading"){.headerlink}

Default: [`"guest"`{.docutils .literal .notranslate}]{.pre}

The password to use for FTP connections when there is no
[`"ftp_password"`{.docutils .literal .notranslate}]{.pre} in
[`Request`{.docutils .literal .notranslate}]{.pre} meta.

::: {.admonition .note}
Note

Paraphrasing [RFC 1635](https://tools.ietf.org/html/rfc1635){.reference
.external}, although it is common to use either the password "guest" or
one's e-mail address for anonymous FTP, some FTP servers explicitly ask
for the user's e-mail address and will not allow login with the "guest"
password.
:::

[]{#std-reqmeta-ftp_user .target}
:::

::: {#ftp-user .section}
[]{#std-setting-FTP_USER}

##### FTP_USER[¶](#ftp-user "Permalink to this heading"){.headerlink}

Default: [`"anonymous"`{.docutils .literal .notranslate}]{.pre}

The username to use for FTP connections when there is no
[`"ftp_user"`{.docutils .literal .notranslate}]{.pre} in
[`Request`{.docutils .literal .notranslate}]{.pre} meta.
:::

::: {#gcs-project-id .section}
[]{#std-setting-GCS_PROJECT_ID}

##### GCS_PROJECT_ID[¶](#gcs-project-id "Permalink to this heading"){.headerlink}

Default: [`None`{.docutils .literal .notranslate}]{.pre}

The Project ID that will be used when storing data on [Google Cloud
Storage](https://cloud.google.com/storage/){.reference .external}.
:::

::: {#item-pipelines .section}
[]{#std-setting-ITEM_PIPELINES}

##### ITEM_PIPELINES[¶](#item-pipelines "Permalink to this heading"){.headerlink}

Default: [`{}`{.docutils .literal .notranslate}]{.pre}

A dict containing the item pipelines to use, and their orders. Order
values are arbitrary, but it is customary to define them in the 0-1000
range. Lower orders process before higher orders.

Example:

::: {.highlight-python .notranslate}
::: highlight
    ITEM_PIPELINES = {
        "mybot.pipelines.validate.ValidateMyItem": 300,
        "mybot.pipelines.validate.StoreMyItem": 800,
    }
:::
:::
:::

::: {#item-pipelines-base .section}
[]{#std-setting-ITEM_PIPELINES_BASE}

##### ITEM_PIPELINES_BASE[¶](#item-pipelines-base "Permalink to this heading"){.headerlink}

Default: [`{}`{.docutils .literal .notranslate}]{.pre}

A dict containing the pipelines enabled by default in Scrapy. You should
never modify this setting in your project, modify
[[`ITEM_PIPELINES`{.xref .std .std-setting .docutils .literal
.notranslate}]{.pre}](#std-setting-ITEM_PIPELINES){.hoverxref .tooltip
.reference .internal} instead.
:::

::: {#jobdir .section}
[]{#std-setting-JOBDIR}

##### JOBDIR[¶](#jobdir "Permalink to this heading"){.headerlink}

Default: [`None`{.docutils .literal .notranslate}]{.pre}

A string indicating the directory for storing the state of a crawl when
[[pausing and resuming crawls]{.std
.std-ref}](index.html#topics-jobs){.hoverxref .tooltip .reference
.internal}.
:::

::: {#log-enabled .section}
[]{#std-setting-LOG_ENABLED}

##### LOG_ENABLED[¶](#log-enabled "Permalink to this heading"){.headerlink}

Default: [`True`{.docutils .literal .notranslate}]{.pre}

Whether to enable logging.
:::

::: {#log-encoding .section}
[]{#std-setting-LOG_ENCODING}

##### LOG_ENCODING[¶](#log-encoding "Permalink to this heading"){.headerlink}

Default: [`'utf-8'`{.docutils .literal .notranslate}]{.pre}

The encoding to use for logging.
:::

::: {#log-file .section}
[]{#std-setting-LOG_FILE}

##### LOG_FILE[¶](#log-file "Permalink to this heading"){.headerlink}

Default: [`None`{.docutils .literal .notranslate}]{.pre}

File name to use for logging output. If [`None`{.docutils .literal
.notranslate}]{.pre}, standard error will be used.
:::

::: {#log-file-append .section}
[]{#std-setting-LOG_FILE_APPEND}

##### LOG_FILE_APPEND[¶](#log-file-append "Permalink to this heading"){.headerlink}

Default: [`True`{.docutils .literal .notranslate}]{.pre}

If [`False`{.docutils .literal .notranslate}]{.pre}, the log file
specified with [[`LOG_FILE`{.xref .std .std-setting .docutils .literal
.notranslate}]{.pre}](#std-setting-LOG_FILE){.hoverxref .tooltip
.reference .internal} will be overwritten (discarding the output from
previous runs, if any).
:::

::: {#log-format .section}
[]{#std-setting-LOG_FORMAT}

##### LOG_FORMAT[¶](#log-format "Permalink to this heading"){.headerlink}

Default: [`'%(asctime)s`{.docutils .literal
.notranslate}]{.pre}` `{.docutils .literal
.notranslate}[`[%(name)s]`{.docutils .literal
.notranslate}]{.pre}` `{.docutils .literal
.notranslate}[`%(levelname)s:`{.docutils .literal
.notranslate}]{.pre}` `{.docutils .literal
.notranslate}[`%(message)s'`{.docutils .literal .notranslate}]{.pre}

String for formatting log messages. Refer to the [[Python logging
documentation]{.xref .std
.std-ref}](https://docs.python.org/3/library/logging.html#logrecord-attributes "(in Python v3.12)"){.reference
.external} for the whole list of available placeholders.
:::

::: {#log-dateformat .section}
[]{#std-setting-LOG_DATEFORMAT}

##### LOG_DATEFORMAT[¶](#log-dateformat "Permalink to this heading"){.headerlink}

Default: [`'%Y-%m-%d`{.docutils .literal
.notranslate}]{.pre}` `{.docutils .literal
.notranslate}[`%H:%M:%S'`{.docutils .literal .notranslate}]{.pre}

String for formatting date/time, expansion of the
[`%(asctime)s`{.docutils .literal .notranslate}]{.pre} placeholder in
[[`LOG_FORMAT`{.xref .std .std-setting .docutils .literal
.notranslate}]{.pre}](#std-setting-LOG_FORMAT){.hoverxref .tooltip
.reference .internal}. Refer to the [[Python datetime
documentation]{.xref .std
.std-ref}](https://docs.python.org/3/library/datetime.html#strftime-strptime-behavior "(in Python v3.12)"){.reference
.external} for the whole list of available directives.
:::

::: {#log-formatter .section}
[]{#std-setting-LOG_FORMATTER}

##### LOG_FORMATTER[¶](#log-formatter "Permalink to this heading"){.headerlink}

Default: [[`scrapy.logformatter.LogFormatter`{.xref .py .py-class
.docutils .literal
.notranslate}]{.pre}](index.html#scrapy.logformatter.LogFormatter "scrapy.logformatter.LogFormatter"){.reference
.internal}

The class to use for [[formatting log messages]{.std
.std-ref}](index.html#custom-log-formats){.hoverxref .tooltip .reference
.internal} for different actions.
:::

::: {#log-level .section}
[]{#std-setting-LOG_LEVEL}

##### LOG_LEVEL[¶](#log-level "Permalink to this heading"){.headerlink}

Default: [`'DEBUG'`{.docutils .literal .notranslate}]{.pre}

Minimum level to log. Available levels are: CRITICAL, ERROR, WARNING,
INFO, DEBUG. For more info see [[Logging]{.std
.std-ref}](index.html#topics-logging){.hoverxref .tooltip .reference
.internal}.
:::

::: {#log-stdout .section}
[]{#std-setting-LOG_STDOUT}

##### LOG_STDOUT[¶](#log-stdout "Permalink to this heading"){.headerlink}

Default: [`False`{.docutils .literal .notranslate}]{.pre}

If [`True`{.docutils .literal .notranslate}]{.pre}, all standard output
(and error) of your process will be redirected to the log. For example
if you [`print('hello')`{.docutils .literal .notranslate}]{.pre} it will
appear in the Scrapy log.
:::

::: {#log-short-names .section}
[]{#std-setting-LOG_SHORT_NAMES}

##### LOG_SHORT_NAMES[¶](#log-short-names "Permalink to this heading"){.headerlink}

Default: [`False`{.docutils .literal .notranslate}]{.pre}

If [`True`{.docutils .literal .notranslate}]{.pre}, the logs will just
contain the root path. If it is set to [`False`{.docutils .literal
.notranslate}]{.pre} then it displays the component responsible for the
log output
:::

::: {#logstats-interval .section}
[]{#std-setting-LOGSTATS_INTERVAL}

##### LOGSTATS_INTERVAL[¶](#logstats-interval "Permalink to this heading"){.headerlink}

Default: [`60.0`{.docutils .literal .notranslate}]{.pre}

The interval (in seconds) between each logging printout of the stats by
[[`LogStats`{.xref .py .py-class .docutils .literal
.notranslate}]{.pre}](index.html#scrapy.extensions.logstats.LogStats "scrapy.extensions.logstats.LogStats"){.reference
.internal}.
:::

::: {#memdebug-enabled .section}
[]{#std-setting-MEMDEBUG_ENABLED}

##### MEMDEBUG_ENABLED[¶](#memdebug-enabled "Permalink to this heading"){.headerlink}

Default: [`False`{.docutils .literal .notranslate}]{.pre}

Whether to enable memory debugging.
:::

::: {#memdebug-notify .section}
[]{#std-setting-MEMDEBUG_NOTIFY}

##### MEMDEBUG_NOTIFY[¶](#memdebug-notify "Permalink to this heading"){.headerlink}

Default: [`[]`{.docutils .literal .notranslate}]{.pre}

When memory debugging is enabled a memory report will be sent to the
specified addresses if this setting is not empty, otherwise the report
will be written to the log.

Example:

::: {.highlight-python .notranslate}
::: highlight
    MEMDEBUG_NOTIFY = ['user@example.com']
:::
:::
:::

::: {#memusage-enabled .section}
[]{#std-setting-MEMUSAGE_ENABLED}

##### MEMUSAGE_ENABLED[¶](#memusage-enabled "Permalink to this heading"){.headerlink}

Default: [`True`{.docutils .literal .notranslate}]{.pre}

Scope: [`scrapy.extensions.memusage`{.docutils .literal
.notranslate}]{.pre}

Whether to enable the memory usage extension. This extension keeps track
of a peak memory used by the process (it writes it to stats). It can
also optionally shutdown the Scrapy process when it exceeds a memory
limit (see [[`MEMUSAGE_LIMIT_MB`{.xref .std .std-setting .docutils
.literal
.notranslate}]{.pre}](#std-setting-MEMUSAGE_LIMIT_MB){.hoverxref
.tooltip .reference .internal}), and notify by email when that happened
(see [[`MEMUSAGE_NOTIFY_MAIL`{.xref .std .std-setting .docutils .literal
.notranslate}]{.pre}](#std-setting-MEMUSAGE_NOTIFY_MAIL){.hoverxref
.tooltip .reference .internal}).

See [[Memory usage extension]{.std
.std-ref}](index.html#topics-extensions-ref-memusage){.hoverxref
.tooltip .reference .internal}.
:::

::: {#memusage-limit-mb .section}
[]{#std-setting-MEMUSAGE_LIMIT_MB}

##### MEMUSAGE_LIMIT_MB[¶](#memusage-limit-mb "Permalink to this heading"){.headerlink}

Default: [`0`{.docutils .literal .notranslate}]{.pre}

Scope: [`scrapy.extensions.memusage`{.docutils .literal
.notranslate}]{.pre}

The maximum amount of memory to allow (in megabytes) before shutting
down Scrapy (if MEMUSAGE_ENABLED is True). If zero, no check will be
performed.

See [[Memory usage extension]{.std
.std-ref}](index.html#topics-extensions-ref-memusage){.hoverxref
.tooltip .reference .internal}.
:::

::: {#memusage-check-interval-seconds .section}
[]{#std-setting-MEMUSAGE_CHECK_INTERVAL_SECONDS}

##### MEMUSAGE_CHECK_INTERVAL_SECONDS[¶](#memusage-check-interval-seconds "Permalink to this heading"){.headerlink}

Default: [`60.0`{.docutils .literal .notranslate}]{.pre}

Scope: [`scrapy.extensions.memusage`{.docutils .literal
.notranslate}]{.pre}

The [[Memory usage extension]{.std
.std-ref}](index.html#topics-extensions-ref-memusage){.hoverxref
.tooltip .reference .internal} checks the current memory usage, versus
the limits set by [[`MEMUSAGE_LIMIT_MB`{.xref .std .std-setting
.docutils .literal
.notranslate}]{.pre}](#std-setting-MEMUSAGE_LIMIT_MB){.hoverxref
.tooltip .reference .internal} and [[`MEMUSAGE_WARNING_MB`{.xref .std
.std-setting .docutils .literal
.notranslate}]{.pre}](#std-setting-MEMUSAGE_WARNING_MB){.hoverxref
.tooltip .reference .internal}, at fixed time intervals.

This sets the length of these intervals, in seconds.

See [[Memory usage extension]{.std
.std-ref}](index.html#topics-extensions-ref-memusage){.hoverxref
.tooltip .reference .internal}.
:::

::: {#memusage-notify-mail .section}
[]{#std-setting-MEMUSAGE_NOTIFY_MAIL}

##### MEMUSAGE_NOTIFY_MAIL[¶](#memusage-notify-mail "Permalink to this heading"){.headerlink}

Default: [`False`{.docutils .literal .notranslate}]{.pre}

Scope: [`scrapy.extensions.memusage`{.docutils .literal
.notranslate}]{.pre}

A list of emails to notify if the memory limit has been reached.

Example:

::: {.highlight-python .notranslate}
::: highlight
    MEMUSAGE_NOTIFY_MAIL = ['user@example.com']
:::
:::

See [[Memory usage extension]{.std
.std-ref}](index.html#topics-extensions-ref-memusage){.hoverxref
.tooltip .reference .internal}.
:::

::: {#memusage-warning-mb .section}
[]{#std-setting-MEMUSAGE_WARNING_MB}

##### MEMUSAGE_WARNING_MB[¶](#memusage-warning-mb "Permalink to this heading"){.headerlink}

Default: [`0`{.docutils .literal .notranslate}]{.pre}

Scope: [`scrapy.extensions.memusage`{.docutils .literal
.notranslate}]{.pre}

The maximum amount of memory to allow (in megabytes) before sending a
warning email notifying about it. If zero, no warning will be produced.
:::

::: {#newspider-module .section}
[]{#std-setting-NEWSPIDER_MODULE}

##### NEWSPIDER_MODULE[¶](#newspider-module "Permalink to this heading"){.headerlink}

Default: [`''`{.docutils .literal .notranslate}]{.pre}

Module where to create new spiders using the [[`genspider`{.xref .std
.std-command .docutils .literal
.notranslate}]{.pre}](index.html#std-command-genspider){.hoverxref
.tooltip .reference .internal} command.

Example:

::: {.highlight-python .notranslate}
::: highlight
    NEWSPIDER_MODULE = 'mybot.spiders_dev'
:::
:::
:::

::: {#randomize-download-delay .section}
[]{#std-setting-RANDOMIZE_DOWNLOAD_DELAY}

##### RANDOMIZE_DOWNLOAD_DELAY[¶](#randomize-download-delay "Permalink to this heading"){.headerlink}

Default: [`True`{.docutils .literal .notranslate}]{.pre}

If enabled, Scrapy will wait a random amount of time (between 0.5 \*
[[`DOWNLOAD_DELAY`{.xref .std .std-setting .docutils .literal
.notranslate}]{.pre}](#std-setting-DOWNLOAD_DELAY){.hoverxref .tooltip
.reference .internal} and 1.5 \* [[`DOWNLOAD_DELAY`{.xref .std
.std-setting .docutils .literal
.notranslate}]{.pre}](#std-setting-DOWNLOAD_DELAY){.hoverxref .tooltip
.reference .internal}) while fetching requests from the same website.

This randomization decreases the chance of the crawler being detected
(and subsequently blocked) by sites which analyze requests looking for
statistically significant similarities in the time between their
requests.

The randomization policy is the same used by
[wget](https://www.gnu.org/software/wget/manual/wget.html){.reference
.external} [`--random-wait`{.docutils .literal .notranslate}]{.pre}
option.

If [[`DOWNLOAD_DELAY`{.xref .std .std-setting .docutils .literal
.notranslate}]{.pre}](#std-setting-DOWNLOAD_DELAY){.hoverxref .tooltip
.reference .internal} is zero (default) this option has no effect.
:::

::: {#reactor-threadpool-maxsize .section}
[]{#std-setting-REACTOR_THREADPOOL_MAXSIZE}

##### REACTOR_THREADPOOL_MAXSIZE[¶](#reactor-threadpool-maxsize "Permalink to this heading"){.headerlink}

Default: [`10`{.docutils .literal .notranslate}]{.pre}

The maximum limit for Twisted Reactor thread pool size. This is common
multi-purpose thread pool used by various Scrapy components. Threaded
DNS Resolver, BlockingFeedStorage, S3FilesStore just to name a few.
Increase this value if you're experiencing problems with insufficient
blocking IO.
:::

::: {#redirect-priority-adjust .section}
[]{#std-setting-REDIRECT_PRIORITY_ADJUST}

##### REDIRECT_PRIORITY_ADJUST[¶](#redirect-priority-adjust "Permalink to this heading"){.headerlink}

Default: [`+2`{.docutils .literal .notranslate}]{.pre}

Scope:
[`scrapy.downloadermiddlewares.redirect.RedirectMiddleware`{.docutils
.literal .notranslate}]{.pre}

Adjust redirect request priority relative to original request:

-   **a positive priority adjust (default) means higher priority.**

-   a negative priority adjust means lower priority.
:::

::: {#robotstxt-obey .section}
[]{#std-setting-ROBOTSTXT_OBEY}

##### ROBOTSTXT_OBEY[¶](#robotstxt-obey "Permalink to this heading"){.headerlink}

Default: [`False`{.docutils .literal .notranslate}]{.pre}

Scope: [`scrapy.downloadermiddlewares.robotstxt`{.docutils .literal
.notranslate}]{.pre}

If enabled, Scrapy will respect robots.txt policies. For more
information see [[RobotsTxtMiddleware]{.std
.std-ref}](index.html#topics-dlmw-robots){.hoverxref .tooltip .reference
.internal}.

::: {.admonition .note}
Note

While the default value is [`False`{.docutils .literal
.notranslate}]{.pre} for historical reasons, this option is enabled by
default in settings.py file generated by [`scrapy`{.docutils .literal
.notranslate}]{.pre}` `{.docutils .literal
.notranslate}[`startproject`{.docutils .literal .notranslate}]{.pre}
command.
:::
:::

::: {#robotstxt-parser .section}
[]{#std-setting-ROBOTSTXT_PARSER}

##### ROBOTSTXT_PARSER[¶](#robotstxt-parser "Permalink to this heading"){.headerlink}

Default: [`'scrapy.robotstxt.ProtegoRobotParser'`{.docutils .literal
.notranslate}]{.pre}

The parser backend to use for parsing [`robots.txt`{.docutils .literal
.notranslate}]{.pre} files. For more information see
[[RobotsTxtMiddleware]{.std
.std-ref}](index.html#topics-dlmw-robots){.hoverxref .tooltip .reference
.internal}.

::: {#robotstxt-user-agent .section}
[]{#std-setting-ROBOTSTXT_USER_AGENT}

###### ROBOTSTXT_USER_AGENT[¶](#robotstxt-user-agent "Permalink to this heading"){.headerlink}

Default: [`None`{.docutils .literal .notranslate}]{.pre}

The user agent string to use for matching in the robots.txt file. If
[`None`{.docutils .literal .notranslate}]{.pre}, the User-Agent header
you are sending with the request or the [[`USER_AGENT`{.xref .std
.std-setting .docutils .literal
.notranslate}]{.pre}](#std-setting-USER_AGENT){.hoverxref .tooltip
.reference .internal} setting (in that order) will be used for
determining the user agent to use in the robots.txt file.
:::
:::

::: {#scheduler .section}
[]{#std-setting-SCHEDULER}

##### SCHEDULER[¶](#scheduler "Permalink to this heading"){.headerlink}

Default: [`'scrapy.core.scheduler.Scheduler'`{.docutils .literal
.notranslate}]{.pre}

The scheduler class to be used for crawling. See the [[Scheduler]{.std
.std-ref}](index.html#topics-scheduler){.hoverxref .tooltip .reference
.internal} topic for details.
:::

::: {#scheduler-debug .section}
[]{#std-setting-SCHEDULER_DEBUG}

##### SCHEDULER_DEBUG[¶](#scheduler-debug "Permalink to this heading"){.headerlink}

Default: [`False`{.docutils .literal .notranslate}]{.pre}

Setting to [`True`{.docutils .literal .notranslate}]{.pre} will log
debug information about the requests scheduler. This currently logs
(only once) if the requests cannot be serialized to disk. Stats counter
([`scheduler/unserializable`{.docutils .literal .notranslate}]{.pre})
tracks the number of times this happens.

Example entry in logs:

::: {.highlight-python .notranslate}
::: highlight
    1956-01-31 00:00:00+0800 [scrapy.core.scheduler] ERROR: Unable to serialize request:
    <GET http://example.com> - reason: cannot serialize <Request at 0x9a7c7ec>
    (type Request)> - no more unserializable requests will be logged
    (see 'scheduler/unserializable' stats counter)
:::
:::
:::

::: {#scheduler-disk-queue .section}
[]{#std-setting-SCHEDULER_DISK_QUEUE}

##### SCHEDULER_DISK_QUEUE[¶](#scheduler-disk-queue "Permalink to this heading"){.headerlink}

Default: [`'scrapy.squeues.PickleLifoDiskQueue'`{.docutils .literal
.notranslate}]{.pre}

Type of disk queue that will be used by scheduler. Other available types
are [`scrapy.squeues.PickleFifoDiskQueue`{.docutils .literal
.notranslate}]{.pre}, [`scrapy.squeues.MarshalFifoDiskQueue`{.docutils
.literal .notranslate}]{.pre},
[`scrapy.squeues.MarshalLifoDiskQueue`{.docutils .literal
.notranslate}]{.pre}.
:::

::: {#scheduler-memory-queue .section}
[]{#std-setting-SCHEDULER_MEMORY_QUEUE}

##### SCHEDULER_MEMORY_QUEUE[¶](#scheduler-memory-queue "Permalink to this heading"){.headerlink}

Default: [`'scrapy.squeues.LifoMemoryQueue'`{.docutils .literal
.notranslate}]{.pre}

Type of in-memory queue used by scheduler. Other available type is:
[`scrapy.squeues.FifoMemoryQueue`{.docutils .literal
.notranslate}]{.pre}.
:::

::: {#scheduler-priority-queue .section}
[]{#std-setting-SCHEDULER_PRIORITY_QUEUE}

##### SCHEDULER_PRIORITY_QUEUE[¶](#scheduler-priority-queue "Permalink to this heading"){.headerlink}

Default: [`'scrapy.pqueues.ScrapyPriorityQueue'`{.docutils .literal
.notranslate}]{.pre}

Type of priority queue used by the scheduler. Another available type is
[`scrapy.pqueues.DownloaderAwarePriorityQueue`{.docutils .literal
.notranslate}]{.pre}.
[`scrapy.pqueues.DownloaderAwarePriorityQueue`{.docutils .literal
.notranslate}]{.pre} works better than
[`scrapy.pqueues.ScrapyPriorityQueue`{.docutils .literal
.notranslate}]{.pre} when you crawl many different domains in parallel.
But currently [`scrapy.pqueues.DownloaderAwarePriorityQueue`{.docutils
.literal .notranslate}]{.pre} does not work together with
[[`CONCURRENT_REQUESTS_PER_IP`{.xref .std .std-setting .docutils
.literal
.notranslate}]{.pre}](#std-setting-CONCURRENT_REQUESTS_PER_IP){.hoverxref
.tooltip .reference .internal}.
:::

::: {#scraper-slot-max-active-size .section}
[]{#std-setting-SCRAPER_SLOT_MAX_ACTIVE_SIZE}

##### SCRAPER_SLOT_MAX_ACTIVE_SIZE[¶](#scraper-slot-max-active-size "Permalink to this heading"){.headerlink}

::: versionadded
[New in version 2.0.]{.versionmodified .added}
:::

Default: [`5_000_000`{.docutils .literal .notranslate}]{.pre}

Soft limit (in bytes) for response data being processed.

While the sum of the sizes of all responses being processed is above
this value, Scrapy does not process new requests.
:::

::: {#spider-contracts .section}
[]{#std-setting-SPIDER_CONTRACTS}

##### SPIDER_CONTRACTS[¶](#spider-contracts "Permalink to this heading"){.headerlink}

Default:: [`{}`{.docutils .literal .notranslate}]{.pre}

A dict containing the spider contracts enabled in your project, used for
testing spiders. For more info see [[Spiders Contracts]{.std
.std-ref}](index.html#topics-contracts){.hoverxref .tooltip .reference
.internal}.
:::

::: {#spider-contracts-base .section}
[]{#std-setting-SPIDER_CONTRACTS_BASE}

##### SPIDER_CONTRACTS_BASE[¶](#spider-contracts-base "Permalink to this heading"){.headerlink}

Default:

::: {.highlight-python .notranslate}
::: highlight
    {
        "scrapy.contracts.default.UrlContract": 1,
        "scrapy.contracts.default.ReturnsContract": 2,
        "scrapy.contracts.default.ScrapesContract": 3,
    }
:::
:::

A dict containing the Scrapy contracts enabled by default in Scrapy. You
should never modify this setting in your project, modify
[[`SPIDER_CONTRACTS`{.xref .std .std-setting .docutils .literal
.notranslate}]{.pre}](#std-setting-SPIDER_CONTRACTS){.hoverxref .tooltip
.reference .internal} instead. For more info see [[Spiders
Contracts]{.std .std-ref}](index.html#topics-contracts){.hoverxref
.tooltip .reference .internal}.

You can disable any of these contracts by assigning [`None`{.docutils
.literal .notranslate}]{.pre} to their class path in
[[`SPIDER_CONTRACTS`{.xref .std .std-setting .docutils .literal
.notranslate}]{.pre}](#std-setting-SPIDER_CONTRACTS){.hoverxref .tooltip
.reference .internal}. E.g., to disable the built-in
[`ScrapesContract`{.docutils .literal .notranslate}]{.pre}, place this
in your [`settings.py`{.docutils .literal .notranslate}]{.pre}:

::: {.highlight-python .notranslate}
::: highlight
    SPIDER_CONTRACTS = {
        "scrapy.contracts.default.ScrapesContract": None,
    }
:::
:::
:::

::: {#spider-loader-class .section}
[]{#std-setting-SPIDER_LOADER_CLASS}

##### SPIDER_LOADER_CLASS[¶](#spider-loader-class "Permalink to this heading"){.headerlink}

Default: [`'scrapy.spiderloader.SpiderLoader'`{.docutils .literal
.notranslate}]{.pre}

The class that will be used for loading spiders, which must implement
the [[SpiderLoader API]{.std
.std-ref}](index.html#topics-api-spiderloader){.hoverxref .tooltip
.reference .internal}.
:::

::: {#spider-loader-warn-only .section}
[]{#std-setting-SPIDER_LOADER_WARN_ONLY}

##### SPIDER_LOADER_WARN_ONLY[¶](#spider-loader-warn-only "Permalink to this heading"){.headerlink}

Default: [`False`{.docutils .literal .notranslate}]{.pre}

By default, when Scrapy tries to import spider classes from
[[`SPIDER_MODULES`{.xref .std .std-setting .docutils .literal
.notranslate}]{.pre}](#std-setting-SPIDER_MODULES){.hoverxref .tooltip
.reference .internal}, it will fail loudly if there is any
[`ImportError`{.docutils .literal .notranslate}]{.pre} exception. But
you can choose to silence this exception and turn it into a simple
warning by setting [`SPIDER_LOADER_WARN_ONLY`{.docutils .literal
.notranslate}]{.pre}` `{.docutils .literal .notranslate}[`=`{.docutils
.literal .notranslate}]{.pre}` `{.docutils .literal
.notranslate}[`True`{.docutils .literal .notranslate}]{.pre}.

::: {.admonition .note}
Note

Some [[scrapy commands]{.std
.std-ref}](index.html#topics-commands){.hoverxref .tooltip .reference
.internal} run with this setting to [`True`{.docutils .literal
.notranslate}]{.pre} already (i.e. they will only issue a warning and
will not fail) since they do not actually need to load spider classes to
work: [[`scrapy`{.xref .std .std-command .docutils .literal
.notranslate}]{.pre}` `{.xref .std .std-command .docutils .literal
.notranslate}[`runspider`{.xref .std .std-command .docutils .literal
.notranslate}]{.pre}](index.html#std-command-runspider){.hoverxref
.tooltip .reference .internal}, [[`scrapy`{.xref .std .std-command
.docutils .literal .notranslate}]{.pre}` `{.xref .std .std-command
.docutils .literal .notranslate}[`settings`{.xref .std .std-command
.docutils .literal
.notranslate}]{.pre}](index.html#std-command-settings){.hoverxref
.tooltip .reference .internal}, [[`scrapy`{.xref .std .std-command
.docutils .literal .notranslate}]{.pre}` `{.xref .std .std-command
.docutils .literal .notranslate}[`startproject`{.xref .std .std-command
.docutils .literal
.notranslate}]{.pre}](index.html#std-command-startproject){.hoverxref
.tooltip .reference .internal}, [[`scrapy`{.xref .std .std-command
.docutils .literal .notranslate}]{.pre}` `{.xref .std .std-command
.docutils .literal .notranslate}[`version`{.xref .std .std-command
.docutils .literal
.notranslate}]{.pre}](index.html#std-command-version){.hoverxref
.tooltip .reference .internal}.
:::
:::

::: {#spider-middlewares .section}
[]{#std-setting-SPIDER_MIDDLEWARES}

##### SPIDER_MIDDLEWARES[¶](#spider-middlewares "Permalink to this heading"){.headerlink}

Default:: [`{}`{.docutils .literal .notranslate}]{.pre}

A dict containing the spider middlewares enabled in your project, and
their orders. For more info see [[Activating a spider middleware]{.std
.std-ref}](index.html#topics-spider-middleware-setting){.hoverxref
.tooltip .reference .internal}.
:::

::: {#spider-middlewares-base .section}
[]{#std-setting-SPIDER_MIDDLEWARES_BASE}

##### SPIDER_MIDDLEWARES_BASE[¶](#spider-middlewares-base "Permalink to this heading"){.headerlink}

Default:

::: {.highlight-python .notranslate}
::: highlight
    {
        "scrapy.spidermiddlewares.httperror.HttpErrorMiddleware": 50,
        "scrapy.spidermiddlewares.offsite.OffsiteMiddleware": 500,
        "scrapy.spidermiddlewares.referer.RefererMiddleware": 700,
        "scrapy.spidermiddlewares.urllength.UrlLengthMiddleware": 800,
        "scrapy.spidermiddlewares.depth.DepthMiddleware": 900,
    }
:::
:::

A dict containing the spider middlewares enabled by default in Scrapy,
and their orders. Low orders are closer to the engine, high orders are
closer to the spider. For more info see [[Activating a spider
middleware]{.std
.std-ref}](index.html#topics-spider-middleware-setting){.hoverxref
.tooltip .reference .internal}.
:::

::: {#spider-modules .section}
[]{#std-setting-SPIDER_MODULES}

##### SPIDER_MODULES[¶](#spider-modules "Permalink to this heading"){.headerlink}

Default: [`[]`{.docutils .literal .notranslate}]{.pre}

A list of modules where Scrapy will look for spiders.

Example:

::: {.highlight-python .notranslate}
::: highlight
    SPIDER_MODULES = ["mybot.spiders_prod", "mybot.spiders_dev"]
:::
:::
:::

::: {#stats-class .section}
[]{#std-setting-STATS_CLASS}

##### STATS_CLASS[¶](#stats-class "Permalink to this heading"){.headerlink}

Default: [`'scrapy.statscollectors.MemoryStatsCollector'`{.docutils
.literal .notranslate}]{.pre}

The class to use for collecting stats, who must implement the [[Stats
Collector API]{.std .std-ref}](index.html#topics-api-stats){.hoverxref
.tooltip .reference .internal}.
:::

::: {#stats-dump .section}
[]{#std-setting-STATS_DUMP}

##### STATS_DUMP[¶](#stats-dump "Permalink to this heading"){.headerlink}

Default: [`True`{.docutils .literal .notranslate}]{.pre}

Dump the [[Scrapy stats]{.std
.std-ref}](index.html#topics-stats){.hoverxref .tooltip .reference
.internal} (to the Scrapy log) once the spider finishes.

For more info see: [[Stats Collection]{.std
.std-ref}](index.html#topics-stats){.hoverxref .tooltip .reference
.internal}.
:::

::: {#statsmailer-rcpts .section}
[]{#std-setting-STATSMAILER_RCPTS}

##### STATSMAILER_RCPTS[¶](#statsmailer-rcpts "Permalink to this heading"){.headerlink}

Default: [`[]`{.docutils .literal .notranslate}]{.pre} (empty list)

Send Scrapy stats after spiders finish scraping. See
[[`StatsMailer`{.xref .py .py-class .docutils .literal
.notranslate}]{.pre}](index.html#scrapy.extensions.statsmailer.StatsMailer "scrapy.extensions.statsmailer.StatsMailer"){.reference
.internal} for more info.
:::

::: {#telnetconsole-enabled .section}
[]{#std-setting-TELNETCONSOLE_ENABLED}

##### TELNETCONSOLE_ENABLED[¶](#telnetconsole-enabled "Permalink to this heading"){.headerlink}

Default: [`True`{.docutils .literal .notranslate}]{.pre}

A boolean which specifies if the [[telnet console]{.std
.std-ref}](index.html#topics-telnetconsole){.hoverxref .tooltip
.reference .internal} will be enabled (provided its extension is also
enabled).
:::

::: {#templates-dir .section}
[]{#std-setting-TEMPLATES_DIR}

##### TEMPLATES_DIR[¶](#templates-dir "Permalink to this heading"){.headerlink}

Default: [`templates`{.docutils .literal .notranslate}]{.pre} dir inside
scrapy module

The directory where to look for templates when creating new projects
with [[`startproject`{.xref .std .std-command .docutils .literal
.notranslate}]{.pre}](index.html#std-command-startproject){.hoverxref
.tooltip .reference .internal} command and new spiders with
[[`genspider`{.xref .std .std-command .docutils .literal
.notranslate}]{.pre}](index.html#std-command-genspider){.hoverxref
.tooltip .reference .internal} command.

The project name must not conflict with the name of custom files or
directories in the [`project`{.docutils .literal .notranslate}]{.pre}
subdirectory.
:::

::: {#twisted-reactor .section}
[]{#std-setting-TWISTED_REACTOR}

##### TWISTED_REACTOR[¶](#twisted-reactor "Permalink to this heading"){.headerlink}

::: versionadded
[New in version 2.0.]{.versionmodified .added}
:::

Default: [`None`{.docutils .literal .notranslate}]{.pre}

Import path of a given [[`reactor`{.xref .py .py-mod .docutils .literal
.notranslate}]{.pre}](https://docs.twisted.org/en/stable/api/twisted.internet.reactor.html "(in Twisted)"){.reference
.external}.

Scrapy will install this reactor if no other reactor is installed yet,
such as when the [`scrapy`{.docutils .literal .notranslate}]{.pre} CLI
program is invoked or when using the [[`CrawlerProcess`{.xref .py
.py-class .docutils .literal
.notranslate}]{.pre}](index.html#scrapy.crawler.CrawlerProcess "scrapy.crawler.CrawlerProcess"){.reference
.internal} class.

If you are using the [[`CrawlerRunner`{.xref .py .py-class .docutils
.literal
.notranslate}]{.pre}](index.html#scrapy.crawler.CrawlerRunner "scrapy.crawler.CrawlerRunner"){.reference
.internal} class, you also need to install the correct reactor manually.
You can do that using [[`install_reactor()`{.xref .py .py-func .docutils
.literal
.notranslate}]{.pre}](#scrapy.utils.reactor.install_reactor "scrapy.utils.reactor.install_reactor"){.reference
.internal}:

[[scrapy.utils.reactor.]{.pre}]{.sig-prename .descclassname}[[install_reactor]{.pre}]{.sig-name .descname}[(]{.sig-paren}*[[reactor_path]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[[[str]{.pre}](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.12)"){.reference .external}]{.n}*, *[[event_loop_path]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[[[Optional]{.pre}](https://docs.python.org/3/library/typing.html#typing.Optional "(in Python v3.12)"){.reference .external}[[\[]{.pre}]{.p}[[str]{.pre}](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.12)"){.reference .external}[[\]]{.pre}]{.p}]{.n}[ ]{.w}[[=]{.pre}]{.o}[ ]{.w}[[None]{.pre}]{.default_value}*[)]{.sig-paren} [[→]{.sig-return-icon} [[[None]{.pre}](https://docs.python.org/3/library/constants.html#None "(in Python v3.12)"){.reference .external}]{.sig-return-typehint}]{.sig-return}[[[\[source\]]{.pre}]{.viewcode-link}](_modules/scrapy/utils/reactor.html#install_reactor){.reference .internal}[¶](#scrapy.utils.reactor.install_reactor "Permalink to this definition"){.headerlink}

:   Installs the [[`reactor`{.xref .py .py-mod .docutils .literal
    .notranslate}]{.pre}](https://docs.twisted.org/en/stable/api/twisted.internet.reactor.html "(in Twisted)"){.reference
    .external} with the specified import path. Also installs the asyncio
    event loop with the specified import path if the asyncio reactor is
    enabled

If a reactor is already installed, [[`install_reactor()`{.xref .py
.py-func .docutils .literal
.notranslate}]{.pre}](#scrapy.utils.reactor.install_reactor "scrapy.utils.reactor.install_reactor"){.reference
.internal} has no effect.

[`CrawlerRunner.__init__`{.xref .py .py-meth .docutils .literal
.notranslate}]{.pre} raises [[`Exception`{.xref .py .py-exc .docutils
.literal
.notranslate}]{.pre}](https://docs.python.org/3/library/exceptions.html#Exception "(in Python v3.12)"){.reference
.external} if the installed reactor does not match the
[[`TWISTED_REACTOR`{.xref .std .std-setting .docutils .literal
.notranslate}]{.pre}](#std-setting-TWISTED_REACTOR){.hoverxref .tooltip
.reference .internal} setting; therefore, having top-level
[[`reactor`{.xref .py .py-mod .docutils .literal
.notranslate}]{.pre}](https://docs.twisted.org/en/stable/api/twisted.internet.reactor.html "(in Twisted)"){.reference
.external} imports in project files and imported third-party libraries
will make Scrapy raise [[`Exception`{.xref .py .py-exc .docutils
.literal
.notranslate}]{.pre}](https://docs.python.org/3/library/exceptions.html#Exception "(in Python v3.12)"){.reference
.external} when it checks which reactor is installed.

In order to use the reactor installed by Scrapy:

::: {.highlight-python .notranslate}
::: highlight
    import scrapy
    from twisted.internet import reactor


    class QuotesSpider(scrapy.Spider):
        name = "quotes"

        def __init__(self, *args, **kwargs):
            self.timeout = int(kwargs.pop("timeout", "60"))
            super(QuotesSpider, self).__init__(*args, **kwargs)

        def start_requests(self):
            reactor.callLater(self.timeout, self.stop)

            urls = ["https://quotes.toscrape.com/page/1"]
            for url in urls:
                yield scrapy.Request(url=url, callback=self.parse)

        def parse(self, response):
            for quote in response.css("div.quote"):
                yield {"text": quote.css("span.text::text").get()}

        def stop(self):
            self.crawler.engine.close_spider(self, "timeout")
:::
:::

which raises [[`Exception`{.xref .py .py-exc .docutils .literal
.notranslate}]{.pre}](https://docs.python.org/3/library/exceptions.html#Exception "(in Python v3.12)"){.reference
.external}, becomes:

::: {.highlight-python .notranslate}
::: highlight
    import scrapy


    class QuotesSpider(scrapy.Spider):
        name = "quotes"

        def __init__(self, *args, **kwargs):
            self.timeout = int(kwargs.pop("timeout", "60"))
            super(QuotesSpider, self).__init__(*args, **kwargs)

        def start_requests(self):
            from twisted.internet import reactor

            reactor.callLater(self.timeout, self.stop)

            urls = ["https://quotes.toscrape.com/page/1"]
            for url in urls:
                yield scrapy.Request(url=url, callback=self.parse)

        def parse(self, response):
            for quote in response.css("div.quote"):
                yield {"text": quote.css("span.text::text").get()}

        def stop(self):
            self.crawler.engine.close_spider(self, "timeout")
:::
:::

The default value of the [[`TWISTED_REACTOR`{.xref .std .std-setting
.docutils .literal
.notranslate}]{.pre}](#std-setting-TWISTED_REACTOR){.hoverxref .tooltip
.reference .internal} setting is [`None`{.docutils .literal
.notranslate}]{.pre}, which means that Scrapy will use the existing
reactor if one is already installed, or install the default reactor
defined by Twisted for the current platform. This is to maintain
backward compatibility and avoid possible problems caused by using a
non-default reactor.

::: versionchanged
[Changed in version 2.7: ]{.versionmodified .changed}The
[[`startproject`{.xref .std .std-command .docutils .literal
.notranslate}]{.pre}](index.html#std-command-startproject){.hoverxref
.tooltip .reference .internal} command now sets this setting to
[`twisted.internet.asyncioreactor.AsyncioSelectorReactor`{.docutils
.literal .notranslate}]{.pre} in the generated [`settings.py`{.docutils
.literal .notranslate}]{.pre} file.
:::

For additional information, see [Choosing a Reactor and GUI Toolkit
Integration](https://docs.twisted.org/en/stable/core/howto/choosing-reactor.html "(in Twisted v23.10)"){.reference
.external}.
:::

::: {#urllength-limit .section}
[]{#std-setting-URLLENGTH_LIMIT}

##### URLLENGTH_LIMIT[¶](#urllength-limit "Permalink to this heading"){.headerlink}

Default: [`2083`{.docutils .literal .notranslate}]{.pre}

Scope: [`spidermiddlewares.urllength`{.docutils .literal
.notranslate}]{.pre}

The maximum URL length to allow for crawled URLs.

This setting can act as a stopping condition in case of URLs of
ever-increasing length, which may be caused for example by a programming
error either in the target server or in your code. See also
[[`REDIRECT_MAX_TIMES`{.xref .std .std-setting .docutils .literal
.notranslate}]{.pre}](index.html#std-setting-REDIRECT_MAX_TIMES){.hoverxref
.tooltip .reference .internal} and [[`DEPTH_LIMIT`{.xref .std
.std-setting .docutils .literal
.notranslate}]{.pre}](#std-setting-DEPTH_LIMIT){.hoverxref .tooltip
.reference .internal}.

Use [`0`{.docutils .literal .notranslate}]{.pre} to allow URLs of any
length.

The default value is copied from the [Microsoft Internet Explorer
maximum URL
length](https://support.microsoft.com/en-us/topic/maximum-url-length-is-2-083-characters-in-internet-explorer-174e7c8a-6666-f4e0-6fd6-908b53c12246){.reference
.external}, even though this setting exists for different reasons.
:::

::: {#user-agent .section}
[]{#std-setting-USER_AGENT}

##### USER_AGENT[¶](#user-agent "Permalink to this heading"){.headerlink}

Default: [`"Scrapy/VERSION`{.docutils .literal
.notranslate}]{.pre}` `{.docutils .literal
.notranslate}[`(+https://scrapy.org)"`{.docutils .literal
.notranslate}]{.pre}

The default User-Agent to use when crawling, unless overridden. This
user agent is also used by [[`RobotsTxtMiddleware`{.xref .py .py-class
.docutils .literal
.notranslate}]{.pre}](index.html#scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware "scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware"){.reference
.internal} if [[`ROBOTSTXT_USER_AGENT`{.xref .std .std-setting .docutils
.literal
.notranslate}]{.pre}](#std-setting-ROBOTSTXT_USER_AGENT){.hoverxref
.tooltip .reference .internal} setting is [`None`{.docutils .literal
.notranslate}]{.pre} and there is no overriding User-Agent header
specified for the request.
:::

::: {#settings-documented-elsewhere .section}
##### Settings documented elsewhere:[¶](#settings-documented-elsewhere "Permalink to this heading"){.headerlink}

The following settings are documented elsewhere, please check each
specific case to see how to enable and use them.

-   [ADDONS](index.html#std-setting-ADDONS){.reference .internal}

-   [AJAXCRAWL_ENABLED](index.html#std-setting-AJAXCRAWL_ENABLED){.reference
    .internal}

-   [ASYNCIO_EVENT_LOOP](index.html#std-setting-ASYNCIO_EVENT_LOOP){.reference
    .internal}

-   [AUTOTHROTTLE_DEBUG](index.html#std-setting-AUTOTHROTTLE_DEBUG){.reference
    .internal}

-   [AUTOTHROTTLE_ENABLED](index.html#std-setting-AUTOTHROTTLE_ENABLED){.reference
    .internal}

-   [AUTOTHROTTLE_MAX_DELAY](index.html#std-setting-AUTOTHROTTLE_MAX_DELAY){.reference
    .internal}

-   [AUTOTHROTTLE_START_DELAY](index.html#std-setting-AUTOTHROTTLE_START_DELAY){.reference
    .internal}

-   [AUTOTHROTTLE_TARGET_CONCURRENCY](index.html#std-setting-AUTOTHROTTLE_TARGET_CONCURRENCY){.reference
    .internal}

-   [AWS_ACCESS_KEY_ID](index.html#std-setting-AWS_ACCESS_KEY_ID){.reference
    .internal}

-   [AWS_ENDPOINT_URL](index.html#std-setting-AWS_ENDPOINT_URL){.reference
    .internal}

-   [AWS_REGION_NAME](index.html#std-setting-AWS_REGION_NAME){.reference
    .internal}

-   [AWS_SECRET_ACCESS_KEY](index.html#std-setting-AWS_SECRET_ACCESS_KEY){.reference
    .internal}

-   [AWS_SESSION_TOKEN](index.html#std-setting-AWS_SESSION_TOKEN){.reference
    .internal}

-   [AWS_USE_SSL](index.html#std-setting-AWS_USE_SSL){.reference
    .internal}

-   [AWS_VERIFY](index.html#std-setting-AWS_VERIFY){.reference
    .internal}

-   [BOT_NAME](index.html#std-setting-BOT_NAME){.reference .internal}

-   [CLOSESPIDER_ERRORCOUNT](index.html#std-setting-CLOSESPIDER_ERRORCOUNT){.reference
    .internal}

-   [CLOSESPIDER_ITEMCOUNT](index.html#std-setting-CLOSESPIDER_ITEMCOUNT){.reference
    .internal}

-   [CLOSESPIDER_PAGECOUNT](index.html#std-setting-CLOSESPIDER_PAGECOUNT){.reference
    .internal}

-   [CLOSESPIDER_TIMEOUT](index.html#std-setting-CLOSESPIDER_TIMEOUT){.reference
    .internal}

-   [CLOSESPIDER_TIMEOUT_NO_ITEM](index.html#std-setting-CLOSESPIDER_TIMEOUT_NO_ITEM){.reference
    .internal}

-   [COMMANDS_MODULE](index.html#std-setting-COMMANDS_MODULE){.reference
    .internal}

-   [COMPRESSION_ENABLED](index.html#std-setting-COMPRESSION_ENABLED){.reference
    .internal}

-   [CONCURRENT_ITEMS](index.html#std-setting-CONCURRENT_ITEMS){.reference
    .internal}

-   [CONCURRENT_REQUESTS](index.html#std-setting-CONCURRENT_REQUESTS){.reference
    .internal}

-   [CONCURRENT_REQUESTS_PER_DOMAIN](index.html#std-setting-CONCURRENT_REQUESTS_PER_DOMAIN){.reference
    .internal}

-   [CONCURRENT_REQUESTS_PER_IP](index.html#std-setting-CONCURRENT_REQUESTS_PER_IP){.reference
    .internal}

-   [COOKIES_DEBUG](index.html#std-setting-COOKIES_DEBUG){.reference
    .internal}

-   [COOKIES_ENABLED](index.html#std-setting-COOKIES_ENABLED){.reference
    .internal}

-   [DEFAULT_ITEM_CLASS](index.html#std-setting-DEFAULT_ITEM_CLASS){.reference
    .internal}

-   [DEFAULT_REQUEST_HEADERS](index.html#std-setting-DEFAULT_REQUEST_HEADERS){.reference
    .internal}

-   [DEPTH_LIMIT](index.html#std-setting-DEPTH_LIMIT){.reference
    .internal}

-   [DEPTH_PRIORITY](index.html#std-setting-DEPTH_PRIORITY){.reference
    .internal}

-   [DEPTH_STATS_VERBOSE](index.html#std-setting-DEPTH_STATS_VERBOSE){.reference
    .internal}

-   [DNSCACHE_ENABLED](index.html#std-setting-DNSCACHE_ENABLED){.reference
    .internal}

-   [DNSCACHE_SIZE](index.html#std-setting-DNSCACHE_SIZE){.reference
    .internal}

-   [DNS_RESOLVER](index.html#std-setting-DNS_RESOLVER){.reference
    .internal}

-   [DNS_TIMEOUT](index.html#std-setting-DNS_TIMEOUT){.reference
    .internal}

-   [DOWNLOADER](index.html#std-setting-DOWNLOADER){.reference
    .internal}

-   [DOWNLOADER_CLIENTCONTEXTFACTORY](index.html#std-setting-DOWNLOADER_CLIENTCONTEXTFACTORY){.reference
    .internal}

-   [DOWNLOADER_CLIENT_TLS_CIPHERS](index.html#std-setting-DOWNLOADER_CLIENT_TLS_CIPHERS){.reference
    .internal}

-   [DOWNLOADER_CLIENT_TLS_METHOD](index.html#std-setting-DOWNLOADER_CLIENT_TLS_METHOD){.reference
    .internal}

-   [DOWNLOADER_CLIENT_TLS_VERBOSE_LOGGING](index.html#std-setting-DOWNLOADER_CLIENT_TLS_VERBOSE_LOGGING){.reference
    .internal}

-   [DOWNLOADER_HTTPCLIENTFACTORY](index.html#std-setting-DOWNLOADER_HTTPCLIENTFACTORY){.reference
    .internal}

-   [DOWNLOADER_MIDDLEWARES](index.html#std-setting-DOWNLOADER_MIDDLEWARES){.reference
    .internal}

-   [DOWNLOADER_MIDDLEWARES_BASE](index.html#std-setting-DOWNLOADER_MIDDLEWARES_BASE){.reference
    .internal}

-   [DOWNLOADER_STATS](index.html#std-setting-DOWNLOADER_STATS){.reference
    .internal}

-   [DOWNLOAD_DELAY](index.html#std-setting-DOWNLOAD_DELAY){.reference
    .internal}

-   [DOWNLOAD_FAIL_ON_DATALOSS](index.html#std-setting-DOWNLOAD_FAIL_ON_DATALOSS){.reference
    .internal}

-   [DOWNLOAD_HANDLERS](index.html#std-setting-DOWNLOAD_HANDLERS){.reference
    .internal}

-   [DOWNLOAD_HANDLERS_BASE](index.html#std-setting-DOWNLOAD_HANDLERS_BASE){.reference
    .internal}

-   [DOWNLOAD_MAXSIZE](index.html#std-setting-DOWNLOAD_MAXSIZE){.reference
    .internal}

-   [DOWNLOAD_SLOTS](index.html#std-setting-DOWNLOAD_SLOTS){.reference
    .internal}

-   [DOWNLOAD_TIMEOUT](index.html#std-setting-DOWNLOAD_TIMEOUT){.reference
    .internal}

-   [DOWNLOAD_WARNSIZE](index.html#std-setting-DOWNLOAD_WARNSIZE){.reference
    .internal}

-   [DUPEFILTER_CLASS](index.html#std-setting-DUPEFILTER_CLASS){.reference
    .internal}

-   [DUPEFILTER_DEBUG](index.html#std-setting-DUPEFILTER_DEBUG){.reference
    .internal}

-   [EDITOR](index.html#std-setting-EDITOR){.reference .internal}

-   [EXTENSIONS](index.html#std-setting-EXTENSIONS){.reference
    .internal}

-   [EXTENSIONS_BASE](index.html#std-setting-EXTENSIONS_BASE){.reference
    .internal}

-   [FEEDS](index.html#std-setting-FEEDS){.reference .internal}

-   [FEED_EXPORTERS](index.html#std-setting-FEED_EXPORTERS){.reference
    .internal}

-   [FEED_EXPORTERS_BASE](index.html#std-setting-FEED_EXPORTERS_BASE){.reference
    .internal}

-   [FEED_EXPORT_BATCH_ITEM_COUNT](index.html#std-setting-FEED_EXPORT_BATCH_ITEM_COUNT){.reference
    .internal}

-   [FEED_EXPORT_ENCODING](index.html#std-setting-FEED_EXPORT_ENCODING){.reference
    .internal}

-   [FEED_EXPORT_FIELDS](index.html#std-setting-FEED_EXPORT_FIELDS){.reference
    .internal}

-   [FEED_EXPORT_INDENT](index.html#std-setting-FEED_EXPORT_INDENT){.reference
    .internal}

-   [FEED_STORAGES](index.html#std-setting-FEED_STORAGES){.reference
    .internal}

-   [FEED_STORAGES_BASE](index.html#std-setting-FEED_STORAGES_BASE){.reference
    .internal}

-   [FEED_STORAGE_FTP_ACTIVE](index.html#std-setting-FEED_STORAGE_FTP_ACTIVE){.reference
    .internal}

-   [FEED_STORAGE_GCS_ACL](index.html#std-setting-FEED_STORAGE_GCS_ACL){.reference
    .internal}

-   [FEED_STORAGE_S3_ACL](index.html#std-setting-FEED_STORAGE_S3_ACL){.reference
    .internal}

-   [FEED_STORE_EMPTY](index.html#std-setting-FEED_STORE_EMPTY){.reference
    .internal}

-   [FEED_TEMPDIR](index.html#std-setting-FEED_TEMPDIR){.reference
    .internal}

-   [FEED_URI_PARAMS](index.html#std-setting-FEED_URI_PARAMS){.reference
    .internal}

-   [FILES_EXPIRES](index.html#std-setting-FILES_EXPIRES){.reference
    .internal}

-   [FILES_RESULT_FIELD](index.html#std-setting-FILES_RESULT_FIELD){.reference
    .internal}

-   [FILES_STORE](index.html#std-setting-FILES_STORE){.reference
    .internal}

-   [FILES_STORE_GCS_ACL](index.html#std-setting-FILES_STORE_GCS_ACL){.reference
    .internal}

-   [FILES_STORE_S3_ACL](index.html#std-setting-FILES_STORE_S3_ACL){.reference
    .internal}

-   [FILES_URLS_FIELD](index.html#std-setting-FILES_URLS_FIELD){.reference
    .internal}

-   [FTP_PASSIVE_MODE](index.html#std-setting-FTP_PASSIVE_MODE){.reference
    .internal}

-   [FTP_PASSWORD](index.html#std-setting-FTP_PASSWORD){.reference
    .internal}

-   [FTP_USER](index.html#std-setting-FTP_USER){.reference .internal}

-   [GCS_PROJECT_ID](index.html#std-setting-GCS_PROJECT_ID){.reference
    .internal}

-   [HTTPCACHE_ALWAYS_STORE](index.html#std-setting-HTTPCACHE_ALWAYS_STORE){.reference
    .internal}

-   [HTTPCACHE_DBM_MODULE](index.html#std-setting-HTTPCACHE_DBM_MODULE){.reference
    .internal}

-   [HTTPCACHE_DIR](index.html#std-setting-HTTPCACHE_DIR){.reference
    .internal}

-   [HTTPCACHE_ENABLED](index.html#std-setting-HTTPCACHE_ENABLED){.reference
    .internal}

-   [HTTPCACHE_EXPIRATION_SECS](index.html#std-setting-HTTPCACHE_EXPIRATION_SECS){.reference
    .internal}

-   [HTTPCACHE_GZIP](index.html#std-setting-HTTPCACHE_GZIP){.reference
    .internal}

-   [HTTPCACHE_IGNORE_HTTP_CODES](index.html#std-setting-HTTPCACHE_IGNORE_HTTP_CODES){.reference
    .internal}

-   [HTTPCACHE_IGNORE_MISSING](index.html#std-setting-HTTPCACHE_IGNORE_MISSING){.reference
    .internal}

-   [HTTPCACHE_IGNORE_RESPONSE_CACHE_CONTROLS](index.html#std-setting-HTTPCACHE_IGNORE_RESPONSE_CACHE_CONTROLS){.reference
    .internal}

-   [HTTPCACHE_IGNORE_SCHEMES](index.html#std-setting-HTTPCACHE_IGNORE_SCHEMES){.reference
    .internal}

-   [HTTPCACHE_POLICY](index.html#std-setting-HTTPCACHE_POLICY){.reference
    .internal}

-   [HTTPCACHE_STORAGE](index.html#std-setting-HTTPCACHE_STORAGE){.reference
    .internal}

-   [HTTPERROR_ALLOWED_CODES](index.html#std-setting-HTTPERROR_ALLOWED_CODES){.reference
    .internal}

-   [HTTPERROR_ALLOW_ALL](index.html#std-setting-HTTPERROR_ALLOW_ALL){.reference
    .internal}

-   [HTTPPROXY_AUTH_ENCODING](index.html#std-setting-HTTPPROXY_AUTH_ENCODING){.reference
    .internal}

-   [HTTPPROXY_ENABLED](index.html#std-setting-HTTPPROXY_ENABLED){.reference
    .internal}

-   [IMAGES_EXPIRES](index.html#std-setting-IMAGES_EXPIRES){.reference
    .internal}

-   [IMAGES_MIN_HEIGHT](index.html#std-setting-IMAGES_MIN_HEIGHT){.reference
    .internal}

-   [IMAGES_MIN_WIDTH](index.html#std-setting-IMAGES_MIN_WIDTH){.reference
    .internal}

-   [IMAGES_RESULT_FIELD](index.html#std-setting-IMAGES_RESULT_FIELD){.reference
    .internal}

-   [IMAGES_STORE](index.html#std-setting-IMAGES_STORE){.reference
    .internal}

-   [IMAGES_STORE_GCS_ACL](index.html#std-setting-IMAGES_STORE_GCS_ACL){.reference
    .internal}

-   [IMAGES_STORE_S3_ACL](index.html#std-setting-IMAGES_STORE_S3_ACL){.reference
    .internal}

-   [IMAGES_THUMBS](index.html#std-setting-IMAGES_THUMBS){.reference
    .internal}

-   [IMAGES_URLS_FIELD](index.html#std-setting-IMAGES_URLS_FIELD){.reference
    .internal}

-   [ITEM_PIPELINES](index.html#std-setting-ITEM_PIPELINES){.reference
    .internal}

-   [ITEM_PIPELINES_BASE](index.html#std-setting-ITEM_PIPELINES_BASE){.reference
    .internal}

-   [JOBDIR](index.html#std-setting-JOBDIR){.reference .internal}

-   [LOGSTATS_INTERVAL](index.html#std-setting-LOGSTATS_INTERVAL){.reference
    .internal}

-   [LOG_DATEFORMAT](index.html#std-setting-LOG_DATEFORMAT){.reference
    .internal}

-   [LOG_ENABLED](index.html#std-setting-LOG_ENABLED){.reference
    .internal}

-   [LOG_ENCODING](index.html#std-setting-LOG_ENCODING){.reference
    .internal}

-   [LOG_FILE](index.html#std-setting-LOG_FILE){.reference .internal}

-   [LOG_FILE_APPEND](index.html#std-setting-LOG_FILE_APPEND){.reference
    .internal}

-   [LOG_FORMAT](index.html#std-setting-LOG_FORMAT){.reference
    .internal}

-   [LOG_FORMATTER](index.html#std-setting-LOG_FORMATTER){.reference
    .internal}

-   [LOG_LEVEL](index.html#std-setting-LOG_LEVEL){.reference .internal}

-   [LOG_SHORT_NAMES](index.html#std-setting-LOG_SHORT_NAMES){.reference
    .internal}

-   [LOG_STDOUT](index.html#std-setting-LOG_STDOUT){.reference
    .internal}

-   [MAIL_FROM](index.html#std-setting-MAIL_FROM){.reference .internal}

-   [MAIL_HOST](index.html#std-setting-MAIL_HOST){.reference .internal}

-   [MAIL_PASS](index.html#std-setting-MAIL_PASS){.reference .internal}

-   [MAIL_PORT](index.html#std-setting-MAIL_PORT){.reference .internal}

-   [MAIL_SSL](index.html#std-setting-MAIL_SSL){.reference .internal}

-   [MAIL_TLS](index.html#std-setting-MAIL_TLS){.reference .internal}

-   [MAIL_USER](index.html#std-setting-MAIL_USER){.reference .internal}

-   [MEDIA_ALLOW_REDIRECTS](index.html#std-setting-MEDIA_ALLOW_REDIRECTS){.reference
    .internal}

-   [MEMDEBUG_ENABLED](index.html#std-setting-MEMDEBUG_ENABLED){.reference
    .internal}

-   [MEMDEBUG_NOTIFY](index.html#std-setting-MEMDEBUG_NOTIFY){.reference
    .internal}

-   [MEMUSAGE_CHECK_INTERVAL_SECONDS](index.html#std-setting-MEMUSAGE_CHECK_INTERVAL_SECONDS){.reference
    .internal}

-   [MEMUSAGE_ENABLED](index.html#std-setting-MEMUSAGE_ENABLED){.reference
    .internal}

-   [MEMUSAGE_LIMIT_MB](index.html#std-setting-MEMUSAGE_LIMIT_MB){.reference
    .internal}

-   [MEMUSAGE_NOTIFY_MAIL](index.html#std-setting-MEMUSAGE_NOTIFY_MAIL){.reference
    .internal}

-   [MEMUSAGE_WARNING_MB](index.html#std-setting-MEMUSAGE_WARNING_MB){.reference
    .internal}

-   [METAREFRESH_ENABLED](index.html#std-setting-METAREFRESH_ENABLED){.reference
    .internal}

-   [METAREFRESH_IGNORE_TAGS](index.html#std-setting-METAREFRESH_IGNORE_TAGS){.reference
    .internal}

-   [METAREFRESH_MAXDELAY](index.html#std-setting-METAREFRESH_MAXDELAY){.reference
    .internal}

-   [NEWSPIDER_MODULE](index.html#std-setting-NEWSPIDER_MODULE){.reference
    .internal}

-   [PERIODIC_LOG_DELTA](index.html#std-setting-PERIODIC_LOG_DELTA){.reference
    .internal}

-   [PERIODIC_LOG_STATS](index.html#std-setting-PERIODIC_LOG_STATS){.reference
    .internal}

-   [PERIODIC_LOG_TIMING_ENABLED](index.html#std-setting-PERIODIC_LOG_TIMING_ENABLED){.reference
    .internal}

-   [RANDOMIZE_DOWNLOAD_DELAY](index.html#std-setting-RANDOMIZE_DOWNLOAD_DELAY){.reference
    .internal}

-   [REACTOR_THREADPOOL_MAXSIZE](index.html#std-setting-REACTOR_THREADPOOL_MAXSIZE){.reference
    .internal}

-   [REDIRECT_ENABLED](index.html#std-setting-REDIRECT_ENABLED){.reference
    .internal}

-   [REDIRECT_MAX_TIMES](index.html#std-setting-REDIRECT_MAX_TIMES){.reference
    .internal}

-   [REDIRECT_PRIORITY_ADJUST](index.html#std-setting-REDIRECT_PRIORITY_ADJUST){.reference
    .internal}

-   [REFERER_ENABLED](index.html#std-setting-REFERER_ENABLED){.reference
    .internal}

-   [REFERRER_POLICY](index.html#std-setting-REFERRER_POLICY){.reference
    .internal}

-   [REQUEST_FINGERPRINTER_CLASS](index.html#std-setting-REQUEST_FINGERPRINTER_CLASS){.reference
    .internal}

-   [REQUEST_FINGERPRINTER_IMPLEMENTATION](index.html#std-setting-REQUEST_FINGERPRINTER_IMPLEMENTATION){.reference
    .internal}

-   [RETRY_ENABLED](index.html#std-setting-RETRY_ENABLED){.reference
    .internal}

-   [RETRY_EXCEPTIONS](index.html#std-setting-RETRY_EXCEPTIONS){.reference
    .internal}

-   [RETRY_HTTP_CODES](index.html#std-setting-RETRY_HTTP_CODES){.reference
    .internal}

-   [RETRY_PRIORITY_ADJUST](index.html#std-setting-RETRY_PRIORITY_ADJUST){.reference
    .internal}

-   [RETRY_TIMES](index.html#std-setting-RETRY_TIMES){.reference
    .internal}

-   [ROBOTSTXT_OBEY](index.html#std-setting-ROBOTSTXT_OBEY){.reference
    .internal}

-   [ROBOTSTXT_PARSER](index.html#std-setting-ROBOTSTXT_PARSER){.reference
    .internal}

-   [ROBOTSTXT_USER_AGENT](index.html#std-setting-ROBOTSTXT_USER_AGENT){.reference
    .internal}

-   [SCHEDULER](index.html#std-setting-SCHEDULER){.reference .internal}

-   [SCHEDULER_DEBUG](index.html#std-setting-SCHEDULER_DEBUG){.reference
    .internal}

-   [SCHEDULER_DISK_QUEUE](index.html#std-setting-SCHEDULER_DISK_QUEUE){.reference
    .internal}

-   [SCHEDULER_MEMORY_QUEUE](index.html#std-setting-SCHEDULER_MEMORY_QUEUE){.reference
    .internal}

-   [SCHEDULER_PRIORITY_QUEUE](index.html#std-setting-SCHEDULER_PRIORITY_QUEUE){.reference
    .internal}

-   [SCRAPER_SLOT_MAX_ACTIVE_SIZE](index.html#std-setting-SCRAPER_SLOT_MAX_ACTIVE_SIZE){.reference
    .internal}

-   [SPIDER_CONTRACTS](index.html#std-setting-SPIDER_CONTRACTS){.reference
    .internal}

-   [SPIDER_CONTRACTS_BASE](index.html#std-setting-SPIDER_CONTRACTS_BASE){.reference
    .internal}

-   [SPIDER_LOADER_CLASS](index.html#std-setting-SPIDER_LOADER_CLASS){.reference
    .internal}

-   [SPIDER_LOADER_WARN_ONLY](index.html#std-setting-SPIDER_LOADER_WARN_ONLY){.reference
    .internal}

-   [SPIDER_MIDDLEWARES](index.html#std-setting-SPIDER_MIDDLEWARES){.reference
    .internal}

-   [SPIDER_MIDDLEWARES_BASE](index.html#std-setting-SPIDER_MIDDLEWARES_BASE){.reference
    .internal}

-   [SPIDER_MODULES](index.html#std-setting-SPIDER_MODULES){.reference
    .internal}

-   [STATSMAILER_RCPTS](index.html#std-setting-STATSMAILER_RCPTS){.reference
    .internal}

-   [STATS_CLASS](index.html#std-setting-STATS_CLASS){.reference
    .internal}

-   [STATS_DUMP](index.html#std-setting-STATS_DUMP){.reference
    .internal}

-   [TELNETCONSOLE_ENABLED](index.html#std-setting-TELNETCONSOLE_ENABLED){.reference
    .internal}

-   [TELNETCONSOLE_HOST](index.html#std-setting-TELNETCONSOLE_HOST){.reference
    .internal}

-   [TELNETCONSOLE_PASSWORD](index.html#std-setting-TELNETCONSOLE_PASSWORD){.reference
    .internal}

-   [TELNETCONSOLE_PORT](index.html#std-setting-TELNETCONSOLE_PORT){.reference
    .internal}

-   [TELNETCONSOLE_USERNAME](index.html#std-setting-TELNETCONSOLE_USERNAME){.reference
    .internal}

-   [TEMPLATES_DIR](index.html#std-setting-TEMPLATES_DIR){.reference
    .internal}

-   [TWISTED_REACTOR](index.html#std-setting-TWISTED_REACTOR){.reference
    .internal}

-   [URLLENGTH_LIMIT](index.html#std-setting-URLLENGTH_LIMIT){.reference
    .internal}

-   [USER_AGENT](index.html#std-setting-USER_AGENT){.reference
    .internal}
:::
:::
:::

[]{#document-topics/exceptions}

::: {#module-scrapy.exceptions .section}
[]{#exceptions}[]{#topics-exceptions}

### Exceptions[¶](#module-scrapy.exceptions "Permalink to this heading"){.headerlink}

::: {#built-in-exceptions-reference .section}
[]{#topics-exceptions-ref}

#### Built-in Exceptions reference[¶](#built-in-exceptions-reference "Permalink to this heading"){.headerlink}

Here's a list of all exceptions included in Scrapy and their usage.

::: {#closespider .section}
##### CloseSpider[¶](#closespider "Permalink to this heading"){.headerlink}

*[exception]{.pre}[ ]{.w}*[[scrapy.exceptions.]{.pre}]{.sig-prename .descclassname}[[CloseSpider]{.pre}]{.sig-name .descname}[(]{.sig-paren}*[[reason]{.pre}]{.n}[[=]{.pre}]{.o}[[\'cancelled\']{.pre}]{.default_value}*[)]{.sig-paren}[[[\[source\]]{.pre}]{.viewcode-link}](_modules/scrapy/exceptions.html#CloseSpider){.reference .internal}[¶](#scrapy.exceptions.CloseSpider "Permalink to this definition"){.headerlink}

:   This exception can be raised from a spider callback to request the
    spider to be closed/stopped. Supported arguments:

    Parameters

    :   **reason**
        ([*str*](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.12)"){.reference
        .external}) -- the reason for closing

For example:

::: {.highlight-python .notranslate}
::: highlight
    def parse_page(self, response):
        if "Bandwidth exceeded" in response.body:
            raise CloseSpider("bandwidth_exceeded")
:::
:::
:::

::: {#dontclosespider .section}
##### DontCloseSpider[¶](#dontclosespider "Permalink to this heading"){.headerlink}

*[exception]{.pre}[ ]{.w}*[[scrapy.exceptions.]{.pre}]{.sig-prename .descclassname}[[DontCloseSpider]{.pre}]{.sig-name .descname}[[[\[source\]]{.pre}]{.viewcode-link}](_modules/scrapy/exceptions.html#DontCloseSpider){.reference .internal}[¶](#scrapy.exceptions.DontCloseSpider "Permalink to this definition"){.headerlink}

:   

This exception can be raised in a [[`spider_idle`{.xref .std .std-signal
.docutils .literal
.notranslate}]{.pre}](index.html#std-signal-spider_idle){.hoverxref
.tooltip .reference .internal} signal handler to prevent the spider from
being closed.
:::

::: {#dropitem .section}
##### DropItem[¶](#dropitem "Permalink to this heading"){.headerlink}

*[exception]{.pre}[ ]{.w}*[[scrapy.exceptions.]{.pre}]{.sig-prename .descclassname}[[DropItem]{.pre}]{.sig-name .descname}[[[\[source\]]{.pre}]{.viewcode-link}](_modules/scrapy/exceptions.html#DropItem){.reference .internal}[¶](#scrapy.exceptions.DropItem "Permalink to this definition"){.headerlink}

:   

The exception that must be raised by item pipeline stages to stop
processing an Item. For more information see [[Item Pipeline]{.std
.std-ref}](index.html#topics-item-pipeline){.hoverxref .tooltip
.reference .internal}.
:::

::: {#ignorerequest .section}
##### IgnoreRequest[¶](#ignorerequest "Permalink to this heading"){.headerlink}

*[exception]{.pre}[ ]{.w}*[[scrapy.exceptions.]{.pre}]{.sig-prename .descclassname}[[IgnoreRequest]{.pre}]{.sig-name .descname}[[[\[source\]]{.pre}]{.viewcode-link}](_modules/scrapy/exceptions.html#IgnoreRequest){.reference .internal}[¶](#scrapy.exceptions.IgnoreRequest "Permalink to this definition"){.headerlink}

:   

This exception can be raised by the Scheduler or any downloader
middleware to indicate that the request should be ignored.
:::

::: {#notconfigured .section}
##### NotConfigured[¶](#notconfigured "Permalink to this heading"){.headerlink}

*[exception]{.pre}[ ]{.w}*[[scrapy.exceptions.]{.pre}]{.sig-prename .descclassname}[[NotConfigured]{.pre}]{.sig-name .descname}[[[\[source\]]{.pre}]{.viewcode-link}](_modules/scrapy/exceptions.html#NotConfigured){.reference .internal}[¶](#scrapy.exceptions.NotConfigured "Permalink to this definition"){.headerlink}

:   

This exception can be raised by some components to indicate that they
will remain disabled. Those components include:

-   Extensions

-   Item pipelines

-   Downloader middlewares

-   Spider middlewares

The exception must be raised in the component's [`__init__`{.docutils
.literal .notranslate}]{.pre} method.
:::

::: {#notsupported .section}
##### NotSupported[¶](#notsupported "Permalink to this heading"){.headerlink}

*[exception]{.pre}[ ]{.w}*[[scrapy.exceptions.]{.pre}]{.sig-prename .descclassname}[[NotSupported]{.pre}]{.sig-name .descname}[[[\[source\]]{.pre}]{.viewcode-link}](_modules/scrapy/exceptions.html#NotSupported){.reference .internal}[¶](#scrapy.exceptions.NotSupported "Permalink to this definition"){.headerlink}

:   

This exception is raised to indicate an unsupported feature.
:::

::: {#stopdownload .section}
##### StopDownload[¶](#stopdownload "Permalink to this heading"){.headerlink}

::: versionadded
[New in version 2.2.]{.versionmodified .added}
:::

*[exception]{.pre}[ ]{.w}*[[scrapy.exceptions.]{.pre}]{.sig-prename .descclassname}[[StopDownload]{.pre}]{.sig-name .descname}[(]{.sig-paren}*[[fail]{.pre}]{.n}[[=]{.pre}]{.o}[[True]{.pre}]{.default_value}*[)]{.sig-paren}[[[\[source\]]{.pre}]{.viewcode-link}](_modules/scrapy/exceptions.html#StopDownload){.reference .internal}[¶](#scrapy.exceptions.StopDownload "Permalink to this definition"){.headerlink}

:   

Raised from a [[`bytes_received`{.xref .py .py-class .docutils .literal
.notranslate}]{.pre}](index.html#scrapy.signals.bytes_received "scrapy.signals.bytes_received"){.reference
.internal} or [[`headers_received`{.xref .py .py-class .docutils
.literal
.notranslate}]{.pre}](index.html#scrapy.signals.headers_received "scrapy.signals.headers_received"){.reference
.internal} signal handler to indicate that no further bytes should be
downloaded for a response.

The [`fail`{.docutils .literal .notranslate}]{.pre} boolean parameter
controls which method will handle the resulting response:

-   If [`fail=True`{.docutils .literal .notranslate}]{.pre} (default),
    the request errback is called. The response object is available as
    the [`response`{.docutils .literal .notranslate}]{.pre} attribute of
    the [`StopDownload`{.docutils .literal .notranslate}]{.pre}
    exception, which is in turn stored as the [`value`{.docutils
    .literal .notranslate}]{.pre} attribute of the received
    [[`Failure`{.xref .py .py-class .docutils .literal
    .notranslate}]{.pre}](https://docs.twisted.org/en/stable/api/twisted.python.failure.Failure.html "(in Twisted)"){.reference
    .external} object. This means that in an errback defined as
    [`def`{.docutils .literal .notranslate}]{.pre}` `{.docutils .literal
    .notranslate}[`errback(self,`{.docutils .literal
    .notranslate}]{.pre}` `{.docutils .literal
    .notranslate}[`failure)`{.docutils .literal .notranslate}]{.pre},
    the response can be accessed though
    [`failure.value.response`{.docutils .literal .notranslate}]{.pre}.

-   If [`fail=False`{.docutils .literal .notranslate}]{.pre}, the
    request callback is called instead.

In both cases, the response could have its body truncated: the body
contains all bytes received up until the exception is raised, including
the bytes received in the signal handler that raises the exception.
Also, the response object is marked with [`"download_stopped"`{.docutils
.literal .notranslate}]{.pre} in its [`Response.flags`{.xref .py
.py-attr .docutils .literal .notranslate}]{.pre} attribute.

::: {.admonition .note}
Note

[`fail`{.docutils .literal .notranslate}]{.pre} is a keyword-only
parameter, i.e. raising [`StopDownload(False)`{.docutils .literal
.notranslate}]{.pre} or [`StopDownload(True)`{.docutils .literal
.notranslate}]{.pre} will raise a [[`TypeError`{.xref .py .py-class
.docutils .literal
.notranslate}]{.pre}](https://docs.python.org/3/library/exceptions.html#TypeError "(in Python v3.12)"){.reference
.external}.
:::

See the documentation for the [[`bytes_received`{.xref .py .py-class
.docutils .literal
.notranslate}]{.pre}](index.html#scrapy.signals.bytes_received "scrapy.signals.bytes_received"){.reference
.internal} and [[`headers_received`{.xref .py .py-class .docutils
.literal
.notranslate}]{.pre}](index.html#scrapy.signals.headers_received "scrapy.signals.headers_received"){.reference
.internal} signals and the [[Stopping the download of a Response]{.std
.std-ref}](index.html#topics-stop-response-download){.hoverxref .tooltip
.reference .internal} topic for additional information and examples.
:::
:::
:::
:::

[[Command line tool]{.doc}](index.html#document-topics/commands){.reference .internal}

:   Learn about the command-line tool used to manage your Scrapy
    project.

[[Spiders]{.doc}](index.html#document-topics/spiders){.reference .internal}

:   Write the rules to crawl your websites.

[[Selectors]{.doc}](index.html#document-topics/selectors){.reference .internal}

:   Extract the data from web pages using XPath.

[[Scrapy shell]{.doc}](index.html#document-topics/shell){.reference .internal}

:   Test your extraction code in an interactive environment.

[[Items]{.doc}](index.html#document-topics/items){.reference .internal}

:   Define the data you want to scrape.

[[Item Loaders]{.doc}](index.html#document-topics/loaders){.reference .internal}

:   Populate your items with the extracted data.

[[Item Pipeline]{.doc}](index.html#document-topics/item-pipeline){.reference .internal}

:   Post-process and store your scraped data.

[[Feed exports]{.doc}](index.html#document-topics/feed-exports){.reference .internal}

:   Output your scraped data using different formats and storages.

[[Requests and Responses]{.doc}](index.html#document-topics/request-response){.reference .internal}

:   Understand the classes used to represent HTTP requests and
    responses.

[[Link Extractors]{.doc}](index.html#document-topics/link-extractors){.reference .internal}

:   Convenient classes to extract links to follow from pages.

[[Settings]{.doc}](index.html#document-topics/settings){.reference .internal}

:   Learn how to configure Scrapy and see all [[available settings]{.std
    .std-ref}](index.html#topics-settings-ref){.hoverxref .tooltip
    .reference .internal}.

[[Exceptions]{.doc}](index.html#document-topics/exceptions){.reference .internal}

:   See all available exceptions and their meaning.
:::

::: {#built-in-services .section}
## Built-in services[¶](#built-in-services "Permalink to this heading"){.headerlink}

::: {.toctree-wrapper .compound}
[]{#document-topics/logging}

::: {#logging .section}
[]{#topics-logging}

### Logging[¶](#logging "Permalink to this heading"){.headerlink}

::: {.admonition .note}
Note

[`scrapy.log`{.xref .py .py-mod .docutils .literal .notranslate}]{.pre}
has been deprecated alongside its functions in favor of explicit calls
to the Python standard logging. Keep reading to learn more about the new
logging system.
:::

Scrapy uses [[`logging`{.xref .py .py-mod .docutils .literal
.notranslate}]{.pre}](https://docs.python.org/3/library/logging.html#module-logging "(in Python v3.12)"){.reference
.external} for event logging. We'll provide some simple examples to get
you started, but for more advanced use-cases it's strongly suggested to
read thoroughly its documentation.

Logging works out of the box, and can be configured to some extent with
the Scrapy settings listed in [[Logging settings]{.std
.std-ref}](#topics-logging-settings){.hoverxref .tooltip .reference
.internal}.

Scrapy calls [[`scrapy.utils.log.configure_logging()`{.xref .py .py-func
.docutils .literal
.notranslate}]{.pre}](#scrapy.utils.log.configure_logging "scrapy.utils.log.configure_logging"){.reference
.internal} to set some reasonable defaults and handle those settings in
[[Logging settings]{.std .std-ref}](#topics-logging-settings){.hoverxref
.tooltip .reference .internal} when running commands, so it's
recommended to manually call it if you're running Scrapy from scripts as
described in [[Run Scrapy from a script]{.std
.std-ref}](index.html#run-from-script){.hoverxref .tooltip .reference
.internal}.

::: {#log-levels .section}
[]{#topics-logging-levels}

#### Log levels[¶](#log-levels "Permalink to this heading"){.headerlink}

Python's builtin logging defines 5 different levels to indicate the
severity of a given log message. Here are the standard ones, listed in
decreasing order:

1.  [`logging.CRITICAL`{.docutils .literal .notranslate}]{.pre} - for
    critical errors (highest severity)

2.  [`logging.ERROR`{.docutils .literal .notranslate}]{.pre} - for
    regular errors

3.  [`logging.WARNING`{.docutils .literal .notranslate}]{.pre} - for
    warning messages

4.  [`logging.INFO`{.docutils .literal .notranslate}]{.pre} - for
    informational messages

5.  [`logging.DEBUG`{.docutils .literal .notranslate}]{.pre} - for
    debugging messages (lowest severity)
:::

::: {#how-to-log-messages .section}
#### How to log messages[¶](#how-to-log-messages "Permalink to this heading"){.headerlink}

Here's a quick example of how to log a message using the
[`logging.WARNING`{.docutils .literal .notranslate}]{.pre} level:

::: {.highlight-python .notranslate}
::: highlight
    import logging

    logging.warning("This is a warning")
:::
:::

There are shortcuts for issuing log messages on any of the standard 5
levels, and there's also a general [`logging.log`{.docutils .literal
.notranslate}]{.pre} method which takes a given level as argument. If
needed, the last example could be rewritten as:

::: {.highlight-python .notranslate}
::: highlight
    import logging

    logging.log(logging.WARNING, "This is a warning")
:::
:::

On top of that, you can create different "loggers" to encapsulate
messages. (For example, a common practice is to create different loggers
for every module). These loggers can be configured independently, and
they allow hierarchical constructions.

The previous examples use the root logger behind the scenes, which is a
top level logger where all messages are propagated to (unless otherwise
specified). Using [`logging`{.docutils .literal .notranslate}]{.pre}
helpers is merely a shortcut for getting the root logger explicitly, so
this is also an equivalent of the last snippets:

::: {.highlight-python .notranslate}
::: highlight
    import logging

    logger = logging.getLogger()
    logger.warning("This is a warning")
:::
:::

You can use a different logger just by getting its name with the
[`logging.getLogger`{.docutils .literal .notranslate}]{.pre} function:

::: {.highlight-python .notranslate}
::: highlight
    import logging

    logger = logging.getLogger("mycustomlogger")
    logger.warning("This is a warning")
:::
:::

Finally, you can ensure having a custom logger for any module you're
working on by using the [`__name__`{.docutils .literal
.notranslate}]{.pre} variable, which is populated with current module's
path:

::: {.highlight-python .notranslate}
::: highlight
    import logging

    logger = logging.getLogger(__name__)
    logger.warning("This is a warning")
:::
:::

::: {.admonition .seealso}
See also

Module logging, [[HowTo]{.xref .std .std-doc}](https://docs.python.org/3/howto/logging.html "(in Python v3.12)"){.reference .external}

:   Basic Logging Tutorial

Module logging, [[Loggers]{.xref .std .std-ref}](https://docs.python.org/3/library/logging.html#logger "(in Python v3.12)"){.reference .external}

:   Further documentation on loggers
:::
:::

::: {#logging-from-spiders .section}
[]{#topics-logging-from-spiders}

#### Logging from Spiders[¶](#logging-from-spiders "Permalink to this heading"){.headerlink}

Scrapy provides a [[`logger`{.xref .py .py-data .docutils .literal
.notranslate}]{.pre}](index.html#scrapy.Spider.logger "scrapy.Spider.logger"){.reference
.internal} within each Spider instance, which can be accessed and used
like this:

::: {.highlight-python .notranslate}
::: highlight
    import scrapy


    class MySpider(scrapy.Spider):
        name = "myspider"
        start_urls = ["https://scrapy.org"]

        def parse(self, response):
            self.logger.info("Parse function called on %s", response.url)
:::
:::

That logger is created using the Spider's name, but you can use any
custom Python logger you want. For example:

::: {.highlight-python .notranslate}
::: highlight
    import logging
    import scrapy

    logger = logging.getLogger("mycustomlogger")


    class MySpider(scrapy.Spider):
        name = "myspider"
        start_urls = ["https://scrapy.org"]

        def parse(self, response):
            logger.info("Parse function called on %s", response.url)
:::
:::
:::

::: {#logging-configuration .section}
[]{#topics-logging-configuration}

#### Logging configuration[¶](#logging-configuration "Permalink to this heading"){.headerlink}

Loggers on their own don't manage how messages sent through them are
displayed. For this task, different "handlers" can be attached to any
logger instance and they will redirect those messages to appropriate
destinations, such as the standard output, files, emails, etc.

By default, Scrapy sets and configures a handler for the root logger,
based on the settings below.

::: {#logging-settings .section}
[]{#topics-logging-settings}

##### Logging settings[¶](#logging-settings "Permalink to this heading"){.headerlink}

These settings can be used to configure the logging:

-   [[`LOG_FILE`{.xref .std .std-setting .docutils .literal
    .notranslate}]{.pre}](index.html#std-setting-LOG_FILE){.hoverxref
    .tooltip .reference .internal}

-   [[`LOG_FILE_APPEND`{.xref .std .std-setting .docutils .literal
    .notranslate}]{.pre}](index.html#std-setting-LOG_FILE_APPEND){.hoverxref
    .tooltip .reference .internal}

-   [[`LOG_ENABLED`{.xref .std .std-setting .docutils .literal
    .notranslate}]{.pre}](index.html#std-setting-LOG_ENABLED){.hoverxref
    .tooltip .reference .internal}

-   [[`LOG_ENCODING`{.xref .std .std-setting .docutils .literal
    .notranslate}]{.pre}](index.html#std-setting-LOG_ENCODING){.hoverxref
    .tooltip .reference .internal}

-   [[`LOG_LEVEL`{.xref .std .std-setting .docutils .literal
    .notranslate}]{.pre}](index.html#std-setting-LOG_LEVEL){.hoverxref
    .tooltip .reference .internal}

-   [[`LOG_FORMAT`{.xref .std .std-setting .docutils .literal
    .notranslate}]{.pre}](index.html#std-setting-LOG_FORMAT){.hoverxref
    .tooltip .reference .internal}

-   [[`LOG_DATEFORMAT`{.xref .std .std-setting .docutils .literal
    .notranslate}]{.pre}](index.html#std-setting-LOG_DATEFORMAT){.hoverxref
    .tooltip .reference .internal}

-   [[`LOG_STDOUT`{.xref .std .std-setting .docutils .literal
    .notranslate}]{.pre}](index.html#std-setting-LOG_STDOUT){.hoverxref
    .tooltip .reference .internal}

-   [[`LOG_SHORT_NAMES`{.xref .std .std-setting .docutils .literal
    .notranslate}]{.pre}](index.html#std-setting-LOG_SHORT_NAMES){.hoverxref
    .tooltip .reference .internal}

The first couple of settings define a destination for log messages. If
[[`LOG_FILE`{.xref .std .std-setting .docutils .literal
.notranslate}]{.pre}](index.html#std-setting-LOG_FILE){.hoverxref
.tooltip .reference .internal} is set, messages sent through the root
logger will be redirected to a file named [[`LOG_FILE`{.xref .std
.std-setting .docutils .literal
.notranslate}]{.pre}](index.html#std-setting-LOG_FILE){.hoverxref
.tooltip .reference .internal} with encoding [[`LOG_ENCODING`{.xref .std
.std-setting .docutils .literal
.notranslate}]{.pre}](index.html#std-setting-LOG_ENCODING){.hoverxref
.tooltip .reference .internal}. If unset and [[`LOG_ENABLED`{.xref .std
.std-setting .docutils .literal
.notranslate}]{.pre}](index.html#std-setting-LOG_ENABLED){.hoverxref
.tooltip .reference .internal} is [`True`{.docutils .literal
.notranslate}]{.pre}, log messages will be displayed on the standard
error. If [[`LOG_FILE`{.xref .std .std-setting .docutils .literal
.notranslate}]{.pre}](index.html#std-setting-LOG_FILE){.hoverxref
.tooltip .reference .internal} is set and [[`LOG_FILE_APPEND`{.xref .std
.std-setting .docutils .literal
.notranslate}]{.pre}](index.html#std-setting-LOG_FILE_APPEND){.hoverxref
.tooltip .reference .internal} is [`False`{.docutils .literal
.notranslate}]{.pre}, the file will be overwritten (discarding the
output from previous runs, if any). Lastly, if [[`LOG_ENABLED`{.xref
.std .std-setting .docutils .literal
.notranslate}]{.pre}](index.html#std-setting-LOG_ENABLED){.hoverxref
.tooltip .reference .internal} is [`False`{.docutils .literal
.notranslate}]{.pre}, there won't be any visible log output.

[[`LOG_LEVEL`{.xref .std .std-setting .docutils .literal
.notranslate}]{.pre}](index.html#std-setting-LOG_LEVEL){.hoverxref
.tooltip .reference .internal} determines the minimum level of severity
to display, those messages with lower severity will be filtered out. It
ranges through the possible levels listed in [[Log levels]{.std
.std-ref}](#topics-logging-levels){.hoverxref .tooltip .reference
.internal}.

[[`LOG_FORMAT`{.xref .std .std-setting .docutils .literal
.notranslate}]{.pre}](index.html#std-setting-LOG_FORMAT){.hoverxref
.tooltip .reference .internal} and [[`LOG_DATEFORMAT`{.xref .std
.std-setting .docutils .literal
.notranslate}]{.pre}](index.html#std-setting-LOG_DATEFORMAT){.hoverxref
.tooltip .reference .internal} specify formatting strings used as
layouts for all messages. Those strings can contain any placeholders
listed in [[logging's logrecord attributes docs]{.xref .std
.std-ref}](https://docs.python.org/3/library/logging.html#logrecord-attributes "(in Python v3.12)"){.reference
.external} and [[datetime's strftime and strptime directives]{.xref .std
.std-ref}](https://docs.python.org/3/library/datetime.html#strftime-strptime-behavior "(in Python v3.12)"){.reference
.external} respectively.

If [[`LOG_SHORT_NAMES`{.xref .std .std-setting .docutils .literal
.notranslate}]{.pre}](index.html#std-setting-LOG_SHORT_NAMES){.hoverxref
.tooltip .reference .internal} is set, then the logs will not display
the Scrapy component that prints the log. It is unset by default, hence
logs contain the Scrapy component responsible for that log output.
:::

::: {#command-line-options .section}
##### Command-line options[¶](#command-line-options "Permalink to this heading"){.headerlink}

There are command-line arguments, available for all commands, that you
can use to override some of the Scrapy settings regarding logging.

-   

    [`--logfile`{.docutils .literal .notranslate}]{.pre}` `{.docutils .literal .notranslate}[`FILE`{.docutils .literal .notranslate}]{.pre}

    :   Overrides [[`LOG_FILE`{.xref .std .std-setting .docutils
        .literal
        .notranslate}]{.pre}](index.html#std-setting-LOG_FILE){.hoverxref
        .tooltip .reference .internal}

-   

    [`--loglevel/-L`{.docutils .literal .notranslate}]{.pre}` `{.docutils .literal .notranslate}[`LEVEL`{.docutils .literal .notranslate}]{.pre}

    :   Overrides [[`LOG_LEVEL`{.xref .std .std-setting .docutils
        .literal
        .notranslate}]{.pre}](index.html#std-setting-LOG_LEVEL){.hoverxref
        .tooltip .reference .internal}

-   

    [`--nolog`{.docutils .literal .notranslate}]{.pre}

    :   Sets [[`LOG_ENABLED`{.xref .std .std-setting .docutils .literal
        .notranslate}]{.pre}](index.html#std-setting-LOG_ENABLED){.hoverxref
        .tooltip .reference .internal} to [`False`{.docutils .literal
        .notranslate}]{.pre}

::: {.admonition .seealso}
See also

Module [[`logging.handlers`{.xref .py .py-mod .docutils .literal .notranslate}]{.pre}](https://docs.python.org/3/library/logging.handlers.html#module-logging.handlers "(in Python v3.12)"){.reference .external}

:   Further documentation on available handlers
:::
:::

::: {#custom-log-formats .section}
[]{#id1}

##### Custom Log Formats[¶](#custom-log-formats "Permalink to this heading"){.headerlink}

A custom log format can be set for different actions by extending
[[`LogFormatter`{.xref .py .py-class .docutils .literal
.notranslate}]{.pre}](#scrapy.logformatter.LogFormatter "scrapy.logformatter.LogFormatter"){.reference
.internal} class and making [[`LOG_FORMATTER`{.xref .std .std-setting
.docutils .literal
.notranslate}]{.pre}](index.html#std-setting-LOG_FORMATTER){.hoverxref
.tooltip .reference .internal} point to your new class.

*[class]{.pre}[ ]{.w}*[[scrapy.logformatter.]{.pre}]{.sig-prename .descclassname}[[LogFormatter]{.pre}]{.sig-name .descname}[[[\[source\]]{.pre}]{.viewcode-link}](_modules/scrapy/logformatter.html#LogFormatter){.reference .internal}[¶](#scrapy.logformatter.LogFormatter "Permalink to this definition"){.headerlink}

:   Class for generating log messages for different actions.

    All methods must return a dictionary listing the parameters
    [`level`{.docutils .literal .notranslate}]{.pre}, [`msg`{.docutils
    .literal .notranslate}]{.pre} and [`args`{.docutils .literal
    .notranslate}]{.pre} which are going to be used for constructing the
    log message when calling [`logging.log`{.docutils .literal
    .notranslate}]{.pre}.

    Dictionary keys for the method outputs:

    -   [`level`{.docutils .literal .notranslate}]{.pre} is the log
        level for that action, you can use those from the [python
        logging
        library](https://docs.python.org/3/library/logging.html){.reference
        .external} : [`logging.DEBUG`{.docutils .literal
        .notranslate}]{.pre}, [`logging.INFO`{.docutils .literal
        .notranslate}]{.pre}, [`logging.WARNING`{.docutils .literal
        .notranslate}]{.pre}, [`logging.ERROR`{.docutils .literal
        .notranslate}]{.pre} and [`logging.CRITICAL`{.docutils .literal
        .notranslate}]{.pre}.

    -   [`msg`{.docutils .literal .notranslate}]{.pre} should be a
        string that can contain different formatting placeholders. This
        string, formatted with the provided [`args`{.docutils .literal
        .notranslate}]{.pre}, is going to be the long message for that
        action.

    -   [`args`{.docutils .literal .notranslate}]{.pre} should be a
        tuple or dict with the formatting placeholders for
        [`msg`{.docutils .literal .notranslate}]{.pre}. The final log
        message is computed as [`msg`{.docutils .literal
        .notranslate}]{.pre}` `{.docutils .literal
        .notranslate}[`%`{.docutils .literal
        .notranslate}]{.pre}` `{.docutils .literal
        .notranslate}[`args`{.docutils .literal .notranslate}]{.pre}.

    Users can define their own [`LogFormatter`{.docutils .literal
    .notranslate}]{.pre} class if they want to customize how each action
    is logged or if they want to omit it entirely. In order to omit
    logging an action the method must return [`None`{.docutils .literal
    .notranslate}]{.pre}.

    Here is an example on how to create a custom log formatter to lower
    the severity level of the log message when an item is dropped from
    the pipeline:

    ::: {.highlight-default .notranslate}
    ::: highlight
        class PoliteLogFormatter(logformatter.LogFormatter):
            def dropped(self, item, exception, response, spider):
                return {
                    'level': logging.INFO, # lowering the level from logging.WARNING
                    'msg': "Dropped: %(exception)s" + os.linesep + "%(item)s",
                    'args': {
                        'exception': exception,
                        'item': item,
                    }
                }
    :::
    :::

    [[crawled]{.pre}]{.sig-name .descname}[(]{.sig-paren}*[[request]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[[[Request]{.pre}](index.html#scrapy.http.Request "scrapy.http.request.Request"){.reference .internal}]{.n}*, *[[response]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[[[Response]{.pre}](index.html#scrapy.http.Response "scrapy.http.response.Response"){.reference .internal}]{.n}*, *[[spider]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[[[Spider]{.pre}](index.html#scrapy.spiders.Spider "scrapy.spiders.Spider"){.reference .internal}]{.n}*[)]{.sig-paren} [[→]{.sig-return-icon} [[[dict]{.pre}](https://docs.python.org/3/library/stdtypes.html#dict "(in Python v3.12)"){.reference .external}]{.sig-return-typehint}]{.sig-return}[[[\[source\]]{.pre}]{.viewcode-link}](_modules/scrapy/logformatter.html#LogFormatter.crawled){.reference .internal}[¶](#scrapy.logformatter.LogFormatter.crawled "Permalink to this definition"){.headerlink}

    :   Logs a message when the crawler finds a webpage.

    [[download_error]{.pre}]{.sig-name .descname}[(]{.sig-paren}*[[failure]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[[[Failure]{.pre}](https://docs.twisted.org/en/stable/api/twisted.python.failure.Failure.html "(in Twisted)"){.reference .external}]{.n}*, *[[request]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[[[Request]{.pre}](index.html#scrapy.http.Request "scrapy.http.request.Request"){.reference .internal}]{.n}*, *[[spider]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[[[Spider]{.pre}](index.html#scrapy.spiders.Spider "scrapy.spiders.Spider"){.reference .internal}]{.n}*, *[[errmsg]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[[[Optional]{.pre}](https://docs.python.org/3/library/typing.html#typing.Optional "(in Python v3.12)"){.reference .external}[[\[]{.pre}]{.p}[[str]{.pre}](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.12)"){.reference .external}[[\]]{.pre}]{.p}]{.n}[ ]{.w}[[=]{.pre}]{.o}[ ]{.w}[[None]{.pre}]{.default_value}*[)]{.sig-paren} [[→]{.sig-return-icon} [[[dict]{.pre}](https://docs.python.org/3/library/stdtypes.html#dict "(in Python v3.12)"){.reference .external}]{.sig-return-typehint}]{.sig-return}[[[\[source\]]{.pre}]{.viewcode-link}](_modules/scrapy/logformatter.html#LogFormatter.download_error){.reference .internal}[¶](#scrapy.logformatter.LogFormatter.download_error "Permalink to this definition"){.headerlink}

    :   Logs a download error message from a spider (typically coming
        from the engine).

        ::: versionadded
        [New in version 2.0.]{.versionmodified .added}
        :::

    [[dropped]{.pre}]{.sig-name .descname}[(]{.sig-paren}*[[item]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[[[Any]{.pre}](https://docs.python.org/3/library/typing.html#typing.Any "(in Python v3.12)"){.reference .external}]{.n}*, *[[exception]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[[[BaseException]{.pre}](https://docs.python.org/3/library/exceptions.html#BaseException "(in Python v3.12)"){.reference .external}]{.n}*, *[[response]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[[[Response]{.pre}](index.html#scrapy.http.Response "scrapy.http.response.Response"){.reference .internal}]{.n}*, *[[spider]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[[[Spider]{.pre}](index.html#scrapy.spiders.Spider "scrapy.spiders.Spider"){.reference .internal}]{.n}*[)]{.sig-paren} [[→]{.sig-return-icon} [[[dict]{.pre}](https://docs.python.org/3/library/stdtypes.html#dict "(in Python v3.12)"){.reference .external}]{.sig-return-typehint}]{.sig-return}[[[\[source\]]{.pre}]{.viewcode-link}](_modules/scrapy/logformatter.html#LogFormatter.dropped){.reference .internal}[¶](#scrapy.logformatter.LogFormatter.dropped "Permalink to this definition"){.headerlink}

    :   Logs a message when an item is dropped while it is passing
        through the item pipeline.

    [[item_error]{.pre}]{.sig-name .descname}[(]{.sig-paren}*[[item]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[[[Any]{.pre}](https://docs.python.org/3/library/typing.html#typing.Any "(in Python v3.12)"){.reference .external}]{.n}*, *[[exception]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[[[BaseException]{.pre}](https://docs.python.org/3/library/exceptions.html#BaseException "(in Python v3.12)"){.reference .external}]{.n}*, *[[response]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[[[Response]{.pre}](index.html#scrapy.http.Response "scrapy.http.response.Response"){.reference .internal}]{.n}*, *[[spider]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[[[Spider]{.pre}](index.html#scrapy.spiders.Spider "scrapy.spiders.Spider"){.reference .internal}]{.n}*[)]{.sig-paren} [[→]{.sig-return-icon} [[[dict]{.pre}](https://docs.python.org/3/library/stdtypes.html#dict "(in Python v3.12)"){.reference .external}]{.sig-return-typehint}]{.sig-return}[[[\[source\]]{.pre}]{.viewcode-link}](_modules/scrapy/logformatter.html#LogFormatter.item_error){.reference .internal}[¶](#scrapy.logformatter.LogFormatter.item_error "Permalink to this definition"){.headerlink}

    :   Logs a message when an item causes an error while it is passing
        through the item pipeline.

        ::: versionadded
        [New in version 2.0.]{.versionmodified .added}
        :::

    [[scraped]{.pre}]{.sig-name .descname}[(]{.sig-paren}*[[item]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[[[Any]{.pre}](https://docs.python.org/3/library/typing.html#typing.Any "(in Python v3.12)"){.reference .external}]{.n}*, *[[response]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[[[Union]{.pre}](https://docs.python.org/3/library/typing.html#typing.Union "(in Python v3.12)"){.reference .external}[[\[]{.pre}]{.p}[[Response]{.pre}](index.html#scrapy.http.Response "scrapy.http.response.Response"){.reference .internal}[[,]{.pre}]{.p}[ ]{.w}[[Failure]{.pre}](https://docs.twisted.org/en/stable/api/twisted.python.failure.Failure.html "(in Twisted)"){.reference .external}[[\]]{.pre}]{.p}]{.n}*, *[[spider]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[[[Spider]{.pre}](index.html#scrapy.spiders.Spider "scrapy.spiders.Spider"){.reference .internal}]{.n}*[)]{.sig-paren} [[→]{.sig-return-icon} [[[dict]{.pre}](https://docs.python.org/3/library/stdtypes.html#dict "(in Python v3.12)"){.reference .external}]{.sig-return-typehint}]{.sig-return}[[[\[source\]]{.pre}]{.viewcode-link}](_modules/scrapy/logformatter.html#LogFormatter.scraped){.reference .internal}[¶](#scrapy.logformatter.LogFormatter.scraped "Permalink to this definition"){.headerlink}

    :   Logs a message when an item is scraped by a spider.

    [[spider_error]{.pre}]{.sig-name .descname}[(]{.sig-paren}*[[failure]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[[[Failure]{.pre}](https://docs.twisted.org/en/stable/api/twisted.python.failure.Failure.html "(in Twisted)"){.reference .external}]{.n}*, *[[request]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[[[Request]{.pre}](index.html#scrapy.http.Request "scrapy.http.request.Request"){.reference .internal}]{.n}*, *[[response]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[[[Union]{.pre}](https://docs.python.org/3/library/typing.html#typing.Union "(in Python v3.12)"){.reference .external}[[\[]{.pre}]{.p}[[Response]{.pre}](index.html#scrapy.http.Response "scrapy.http.response.Response"){.reference .internal}[[,]{.pre}]{.p}[ ]{.w}[[Failure]{.pre}](https://docs.twisted.org/en/stable/api/twisted.python.failure.Failure.html "(in Twisted)"){.reference .external}[[\]]{.pre}]{.p}]{.n}*, *[[spider]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[[[Spider]{.pre}](index.html#scrapy.spiders.Spider "scrapy.spiders.Spider"){.reference .internal}]{.n}*[)]{.sig-paren} [[→]{.sig-return-icon} [[[dict]{.pre}](https://docs.python.org/3/library/stdtypes.html#dict "(in Python v3.12)"){.reference .external}]{.sig-return-typehint}]{.sig-return}[[[\[source\]]{.pre}]{.viewcode-link}](_modules/scrapy/logformatter.html#LogFormatter.spider_error){.reference .internal}[¶](#scrapy.logformatter.LogFormatter.spider_error "Permalink to this definition"){.headerlink}

    :   Logs an error message from a spider.

        ::: versionadded
        [New in version 2.0.]{.versionmodified .added}
        :::
:::

::: {#advanced-customization .section}
[]{#topics-logging-advanced-customization}

##### Advanced customization[¶](#advanced-customization "Permalink to this heading"){.headerlink}

Because Scrapy uses stdlib logging module, you can customize logging
using all features of stdlib logging.

For example, let's say you're scraping a website which returns many HTTP
404 and 500 responses, and you want to hide all messages like this:

::: {.highlight-default .notranslate}
::: highlight
    2016-12-16 22:00:06 [scrapy.spidermiddlewares.httperror] INFO: Ignoring
    response <500 https://quotes.toscrape.com/page/1-34/>: HTTP status code
    is not handled or not allowed
:::
:::

The first thing to note is a logger name - it is in brackets:
[`[scrapy.spidermiddlewares.httperror]`{.docutils .literal
.notranslate}]{.pre}. If you get just [`[scrapy]`{.docutils .literal
.notranslate}]{.pre} then [[`LOG_SHORT_NAMES`{.xref .std .std-setting
.docutils .literal
.notranslate}]{.pre}](index.html#std-setting-LOG_SHORT_NAMES){.hoverxref
.tooltip .reference .internal} is likely set to True; set it to False
and re-run the crawl.

Next, we can see that the message has INFO level. To hide it we should
set logging level for [`scrapy.spidermiddlewares.httperror`{.docutils
.literal .notranslate}]{.pre} higher than INFO; next level after INFO is
WARNING. It could be done e.g. in the spider's [`__init__`{.docutils
.literal .notranslate}]{.pre} method:

::: {.highlight-python .notranslate}
::: highlight
    import logging
    import scrapy


    class MySpider(scrapy.Spider):
        # ...
        def __init__(self, *args, **kwargs):
            logger = logging.getLogger("scrapy.spidermiddlewares.httperror")
            logger.setLevel(logging.WARNING)
            super().__init__(*args, **kwargs)
:::
:::

If you run this spider again then INFO messages from
[`scrapy.spidermiddlewares.httperror`{.docutils .literal
.notranslate}]{.pre} logger will be gone.

You can also filter log records by [[`LogRecord`{.xref .py .py-class
.docutils .literal
.notranslate}]{.pre}](https://docs.python.org/3/library/logging.html#logging.LogRecord "(in Python v3.12)"){.reference
.external} data. For example, you can filter log records by message
content using a substring or a regular expression. Create a
[[`logging.Filter`{.xref .py .py-class .docutils .literal
.notranslate}]{.pre}](https://docs.python.org/3/library/logging.html#logging.Filter "(in Python v3.12)"){.reference
.external} subclass and equip it with a regular expression pattern to
filter out unwanted messages:

::: {.highlight-python .notranslate}
::: highlight
    import logging
    import re


    class ContentFilter(logging.Filter):
        def filter(self, record):
            match = re.search(r"\d{3} [Ee]rror, retrying", record.message)
            if match:
                return False
:::
:::

A project-level filter may be attached to the root handler created by
Scrapy, this is a wieldy way to filter all loggers in different parts of
the project (middlewares, spider, etc.):

::: {.highlight-python .notranslate}
::: highlight
    import logging
    import scrapy


    class MySpider(scrapy.Spider):
        # ...
        def __init__(self, *args, **kwargs):
            for handler in logging.root.handlers:
                handler.addFilter(ContentFilter())
:::
:::

Alternatively, you may choose a specific logger and hide it without
affecting other loggers:

::: {.highlight-python .notranslate}
::: highlight
    import logging
    import scrapy


    class MySpider(scrapy.Spider):
        # ...
        def __init__(self, *args, **kwargs):
            logger = logging.getLogger("my_logger")
            logger.addFilter(ContentFilter())
:::
:::
:::
:::

::: {#module-scrapy.utils.log .section}
[]{#scrapy-utils-log-module}

#### scrapy.utils.log module[¶](#module-scrapy.utils.log "Permalink to this heading"){.headerlink}

[[scrapy.utils.log.]{.pre}]{.sig-prename .descclassname}[[configure_logging]{.pre}]{.sig-name .descname}[(]{.sig-paren}*[[settings]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[[[Optional]{.pre}](https://docs.python.org/3/library/typing.html#typing.Optional "(in Python v3.12)"){.reference .external}[[\[]{.pre}]{.p}[[Union]{.pre}](https://docs.python.org/3/library/typing.html#typing.Union "(in Python v3.12)"){.reference .external}[[\[]{.pre}]{.p}[[Settings]{.pre}](index.html#scrapy.settings.Settings "scrapy.settings.Settings"){.reference .internal}[[,]{.pre}]{.p}[ ]{.w}[[dict]{.pre}](https://docs.python.org/3/library/stdtypes.html#dict "(in Python v3.12)"){.reference .external}[[\]]{.pre}]{.p}[[\]]{.pre}]{.p}]{.n}[ ]{.w}[[=]{.pre}]{.o}[ ]{.w}[[None]{.pre}]{.default_value}*, *[[install_root_handler]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[[[bool]{.pre}](https://docs.python.org/3/library/functions.html#bool "(in Python v3.12)"){.reference .external}]{.n}[ ]{.w}[[=]{.pre}]{.o}[ ]{.w}[[True]{.pre}]{.default_value}*[)]{.sig-paren} [[→]{.sig-return-icon} [[[None]{.pre}](https://docs.python.org/3/library/constants.html#None "(in Python v3.12)"){.reference .external}]{.sig-return-typehint}]{.sig-return}[[[\[source\]]{.pre}]{.viewcode-link}](_modules/scrapy/utils/log.html#configure_logging){.reference .internal}[¶](#scrapy.utils.log.configure_logging "Permalink to this definition"){.headerlink}

:   Initialize logging defaults for Scrapy.

    Parameters

    :   -   **settings** (dict, [[`Settings`{.xref .py .py-class
            .docutils .literal
            .notranslate}]{.pre}](index.html#scrapy.settings.Settings "scrapy.settings.Settings"){.reference
            .internal} object or [`None`{.docutils .literal
            .notranslate}]{.pre}) -- settings used to create and
            configure a handler for the root logger (default: None).

        -   **install_root_handler**
            ([*bool*](https://docs.python.org/3/library/functions.html#bool "(in Python v3.12)"){.reference
            .external}) -- whether to install root logging handler
            (default: True)

    This function does:

    -   Route warnings and twisted logging through Python standard
        logging

    -   Assign DEBUG and ERROR level to Scrapy and Twisted loggers
        respectively

    -   Route stdout to log if LOG_STDOUT setting is True

    When [`install_root_handler`{.docutils .literal .notranslate}]{.pre}
    is True (default), this function also creates a handler for the root
    logger according to given settings (see [[Logging settings]{.std
    .std-ref}](#topics-logging-settings){.hoverxref .tooltip .reference
    .internal}). You can override default options using
    [`settings`{.docutils .literal .notranslate}]{.pre} argument. When
    [`settings`{.docutils .literal .notranslate}]{.pre} is empty or
    None, defaults are used.

    [`configure_logging`{.docutils .literal .notranslate}]{.pre} is
    automatically called when using Scrapy commands or
    [[`CrawlerProcess`{.xref .py .py-class .docutils .literal
    .notranslate}]{.pre}](index.html#scrapy.crawler.CrawlerProcess "scrapy.crawler.CrawlerProcess"){.reference
    .internal}, but needs to be called explicitly when running custom
    scripts using [[`CrawlerRunner`{.xref .py .py-class .docutils
    .literal
    .notranslate}]{.pre}](index.html#scrapy.crawler.CrawlerRunner "scrapy.crawler.CrawlerRunner"){.reference
    .internal}. In that case, its usage is not required but it's
    recommended.

    Another option when running custom scripts is to manually configure
    the logging. To do this you can use [[`logging.basicConfig()`{.xref
    .py .py-func .docutils .literal
    .notranslate}]{.pre}](https://docs.python.org/3/library/logging.html#logging.basicConfig "(in Python v3.12)"){.reference
    .external} to set a basic root handler.

    Note that [[`CrawlerProcess`{.xref .py .py-class .docutils .literal
    .notranslate}]{.pre}](index.html#scrapy.crawler.CrawlerProcess "scrapy.crawler.CrawlerProcess"){.reference
    .internal} automatically calls [`configure_logging`{.docutils
    .literal .notranslate}]{.pre}, so it is recommended to only use
    [[`logging.basicConfig()`{.xref .py .py-func .docutils .literal
    .notranslate}]{.pre}](https://docs.python.org/3/library/logging.html#logging.basicConfig "(in Python v3.12)"){.reference
    .external} together with [[`CrawlerRunner`{.xref .py .py-class
    .docutils .literal
    .notranslate}]{.pre}](index.html#scrapy.crawler.CrawlerRunner "scrapy.crawler.CrawlerRunner"){.reference
    .internal}.

    This is an example on how to redirect [`INFO`{.docutils .literal
    .notranslate}]{.pre} or higher messages to a file:

    ::: {.highlight-python .notranslate}
    ::: highlight
        import logging

        logging.basicConfig(
            filename="log.txt", format="%(levelname)s: %(message)s", level=logging.INFO
        )
    :::
    :::

    Refer to [[Run Scrapy from a script]{.std
    .std-ref}](index.html#run-from-script){.hoverxref .tooltip
    .reference .internal} for more details about using Scrapy this way.
:::
:::

[]{#document-topics/stats}

::: {#stats-collection .section}
[]{#topics-stats}

### Stats Collection[¶](#stats-collection "Permalink to this heading"){.headerlink}

Scrapy provides a convenient facility for collecting stats in the form
of key/values, where values are often counters. The facility is called
the Stats Collector, and can be accessed through the [[`stats`{.xref .py
.py-attr .docutils .literal
.notranslate}]{.pre}](index.html#scrapy.crawler.Crawler.stats "scrapy.crawler.Crawler.stats"){.reference
.internal} attribute of the [[Crawler API]{.std
.std-ref}](index.html#topics-api-crawler){.hoverxref .tooltip .reference
.internal}, as illustrated by the examples in the [[Common Stats
Collector uses]{.std .std-ref}](#topics-stats-usecases){.hoverxref
.tooltip .reference .internal} section below.

However, the Stats Collector is always available, so you can always
import it in your module and use its API (to increment or set new stat
keys), regardless of whether the stats collection is enabled or not. If
it's disabled, the API will still work but it won't collect anything.
This is aimed at simplifying the stats collector usage: you should spend
no more than one line of code for collecting stats in your spider,
Scrapy extension, or whatever code you're using the Stats Collector
from.

Another feature of the Stats Collector is that it's very efficient (when
enabled) and extremely efficient (almost unnoticeable) when disabled.

The Stats Collector keeps a stats table per open spider which is
automatically opened when the spider is opened, and closed when the
spider is closed.

::: {#common-stats-collector-uses .section}
[]{#topics-stats-usecases}

#### Common Stats Collector uses[¶](#common-stats-collector-uses "Permalink to this heading"){.headerlink}

Access the stats collector through the [[`stats`{.xref .py .py-attr
.docutils .literal
.notranslate}]{.pre}](index.html#scrapy.crawler.Crawler.stats "scrapy.crawler.Crawler.stats"){.reference
.internal} attribute. Here is an example of an extension that access
stats:

::: {.highlight-python .notranslate}
::: highlight
    class ExtensionThatAccessStats:
        def __init__(self, stats):
            self.stats = stats

        @classmethod
        def from_crawler(cls, crawler):
            return cls(crawler.stats)
:::
:::

Set stat value:

::: {.highlight-python .notranslate}
::: highlight
    stats.set_value("hostname", socket.gethostname())
:::
:::

Increment stat value:

::: {.highlight-python .notranslate}
::: highlight
    stats.inc_value("custom_count")
:::
:::

Set stat value only if greater than previous:

::: {.highlight-python .notranslate}
::: highlight
    stats.max_value("max_items_scraped", value)
:::
:::

Set stat value only if lower than previous:

::: {.highlight-python .notranslate}
::: highlight
    stats.min_value("min_free_memory_percent", value)
:::
:::

Get stat value:

::: {.highlight-pycon .notranslate}
::: highlight
    >>> stats.get_value("custom_count")
    1
:::
:::

Get all stats:

::: {.highlight-pycon .notranslate}
::: highlight
    >>> stats.get_stats()
    {'custom_count': 1, 'start_time': datetime.datetime(2009, 7, 14, 21, 47, 28, 977139)}
:::
:::
:::

::: {#available-stats-collectors .section}
#### Available Stats Collectors[¶](#available-stats-collectors "Permalink to this heading"){.headerlink}

Besides the basic [`StatsCollector`{.xref .py .py-class .docutils
.literal .notranslate}]{.pre} there are other Stats Collectors available
in Scrapy which extend the basic Stats Collector. You can select which
Stats Collector to use through the [[`STATS_CLASS`{.xref .std
.std-setting .docutils .literal
.notranslate}]{.pre}](index.html#std-setting-STATS_CLASS){.hoverxref
.tooltip .reference .internal} setting. The default Stats Collector used
is the [`MemoryStatsCollector`{.xref .py .py-class .docutils .literal
.notranslate}]{.pre}.

::: {#memorystatscollector .section}
##### MemoryStatsCollector[¶](#memorystatscollector "Permalink to this heading"){.headerlink}

*[class]{.pre}[ ]{.w}*[[scrapy.statscollectors.]{.pre}]{.sig-prename .descclassname}[[MemoryStatsCollector]{.pre}]{.sig-name .descname}[[[\[source\]]{.pre}]{.viewcode-link}](_modules/scrapy/statscollectors.html#MemoryStatsCollector){.reference .internal}[¶](#scrapy.statscollectors.MemoryStatsCollector "Permalink to this definition"){.headerlink}

:   A simple stats collector that keeps the stats of the last scraping
    run (for each spider) in memory, after they're closed. The stats can
    be accessed through the [[`spider_stats`{.xref .py .py-attr
    .docutils .literal
    .notranslate}]{.pre}](#scrapy.statscollectors.MemoryStatsCollector.spider_stats "scrapy.statscollectors.MemoryStatsCollector.spider_stats"){.reference
    .internal} attribute, which is a dict keyed by spider domain name.

    This is the default Stats Collector used in Scrapy.

    [[spider_stats]{.pre}]{.sig-name .descname}[¶](#scrapy.statscollectors.MemoryStatsCollector.spider_stats "Permalink to this definition"){.headerlink}

    :   A dict of dicts (keyed by spider name) containing the stats of
        the last scraping run for each spider.
:::

::: {#dummystatscollector .section}
##### DummyStatsCollector[¶](#dummystatscollector "Permalink to this heading"){.headerlink}

*[class]{.pre}[ ]{.w}*[[scrapy.statscollectors.]{.pre}]{.sig-prename .descclassname}[[DummyStatsCollector]{.pre}]{.sig-name .descname}[[[\[source\]]{.pre}]{.viewcode-link}](_modules/scrapy/statscollectors.html#DummyStatsCollector){.reference .internal}[¶](#scrapy.statscollectors.DummyStatsCollector "Permalink to this definition"){.headerlink}

:   A Stats collector which does nothing but is very efficient (because
    it does nothing). This stats collector can be set via the
    [[`STATS_CLASS`{.xref .std .std-setting .docutils .literal
    .notranslate}]{.pre}](index.html#std-setting-STATS_CLASS){.hoverxref
    .tooltip .reference .internal} setting, to disable stats collect in
    order to improve performance. However, the performance penalty of
    stats collection is usually marginal compared to other Scrapy
    workload like parsing pages.
:::
:::
:::

[]{#document-topics/email}

::: {#module-scrapy.mail .section}
[]{#sending-e-mail}[]{#topics-email}

### Sending e-mail[¶](#module-scrapy.mail "Permalink to this heading"){.headerlink}

Although Python makes sending e-mails relatively easy via the
[[`smtplib`{.xref .py .py-mod .docutils .literal
.notranslate}]{.pre}](https://docs.python.org/3/library/smtplib.html#module-smtplib "(in Python v3.12)"){.reference
.external} library, Scrapy provides its own facility for sending e-mails
which is very easy to use and it's implemented using [[Twisted
non-blocking IO]{.xref .std
.std-doc}](https://docs.twisted.org/en/stable/core/howto/defer-intro.html "(in Twisted v23.10)"){.reference
.external}, to avoid interfering with the non-blocking IO of the
crawler. It also provides a simple API for sending attachments and it's
very easy to configure, with a few [[settings]{.std
.std-ref}](#topics-email-settings){.hoverxref .tooltip .reference
.internal}.

::: {#quick-example .section}
#### Quick example[¶](#quick-example "Permalink to this heading"){.headerlink}

There are two ways to instantiate the mail sender. You can instantiate
it using the standard [`__init__`{.docutils .literal
.notranslate}]{.pre} method:

::: {.highlight-python .notranslate}
::: highlight
    from scrapy.mail import MailSender

    mailer = MailSender()
:::
:::

Or you can instantiate it passing a Scrapy settings object, which will
respect the [[settings]{.std
.std-ref}](#topics-email-settings){.hoverxref .tooltip .reference
.internal}:

::: {.highlight-python .notranslate}
::: highlight
    mailer = MailSender.from_settings(settings)
:::
:::

And here is how to use it to send an e-mail (without attachments):

::: {.highlight-python .notranslate}
::: highlight
    mailer.send(
        to=["someone@example.com"],
        subject="Some subject",
        body="Some body",
        cc=["another@example.com"],
    )
:::
:::
:::

::: {#mailsender-class-reference .section}
#### MailSender class reference[¶](#mailsender-class-reference "Permalink to this heading"){.headerlink}

MailSender is the preferred class to use for sending emails from Scrapy,
as it uses [[Twisted non-blocking IO]{.xref .std
.std-doc}](https://docs.twisted.org/en/stable/core/howto/defer-intro.html "(in Twisted v23.10)"){.reference
.external}, like the rest of the framework.

*[class]{.pre}[ ]{.w}*[[scrapy.mail.]{.pre}]{.sig-prename .descclassname}[[MailSender]{.pre}]{.sig-name .descname}[(]{.sig-paren}*[[smtphost]{.pre}]{.n}[[=]{.pre}]{.o}[[None]{.pre}]{.default_value}*, *[[mailfrom]{.pre}]{.n}[[=]{.pre}]{.o}[[None]{.pre}]{.default_value}*, *[[smtpuser]{.pre}]{.n}[[=]{.pre}]{.o}[[None]{.pre}]{.default_value}*, *[[smtppass]{.pre}]{.n}[[=]{.pre}]{.o}[[None]{.pre}]{.default_value}*, *[[smtpport]{.pre}]{.n}[[=]{.pre}]{.o}[[None]{.pre}]{.default_value}*[)]{.sig-paren}[[[\[source\]]{.pre}]{.viewcode-link}](_modules/scrapy/mail.html#MailSender){.reference .internal}[¶](#scrapy.mail.MailSender "Permalink to this definition"){.headerlink}

:   

    Parameters

    :   -   **smtphost**
            ([*str*](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.12)"){.reference
            .external} *or*
            [*bytes*](https://docs.python.org/3/library/stdtypes.html#bytes "(in Python v3.12)"){.reference
            .external}) -- the SMTP host to use for sending the emails.
            If omitted, the [[`MAIL_HOST`{.xref .std .std-setting
            .docutils .literal
            .notranslate}]{.pre}](#std-setting-MAIL_HOST){.hoverxref
            .tooltip .reference .internal} setting will be used.

        -   **mailfrom**
            ([*str*](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.12)"){.reference
            .external}) -- the address used to send emails (in the
            [`From:`{.docutils .literal .notranslate}]{.pre} header). If
            omitted, the [[`MAIL_FROM`{.xref .std .std-setting .docutils
            .literal
            .notranslate}]{.pre}](#std-setting-MAIL_FROM){.hoverxref
            .tooltip .reference .internal} setting will be used.

        -   **smtpuser** -- the SMTP user. If omitted, the
            [[`MAIL_USER`{.xref .std .std-setting .docutils .literal
            .notranslate}]{.pre}](#std-setting-MAIL_USER){.hoverxref
            .tooltip .reference .internal} setting will be used. If not
            given, no SMTP authentication will be performed.

        -   **smtppass**
            ([*str*](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.12)"){.reference
            .external} *or*
            [*bytes*](https://docs.python.org/3/library/stdtypes.html#bytes "(in Python v3.12)"){.reference
            .external}) -- the SMTP pass for authentication.

        -   **smtpport**
            ([*int*](https://docs.python.org/3/library/functions.html#int "(in Python v3.12)"){.reference
            .external}) -- the SMTP port to connect to

        -   **smtptls**
            ([*bool*](https://docs.python.org/3/library/functions.html#bool "(in Python v3.12)"){.reference
            .external}) -- enforce using SMTP STARTTLS

        -   **smtpssl**
            ([*bool*](https://docs.python.org/3/library/functions.html#bool "(in Python v3.12)"){.reference
            .external}) -- enforce using a secure SSL connection

    *[classmethod]{.pre}[ ]{.w}*[[from_settings]{.pre}]{.sig-name .descname}[(]{.sig-paren}*[[settings]{.pre}]{.n}*[)]{.sig-paren}[[[\[source\]]{.pre}]{.viewcode-link}](_modules/scrapy/mail.html#MailSender.from_settings){.reference .internal}[¶](#scrapy.mail.MailSender.from_settings "Permalink to this definition"){.headerlink}

    :   Instantiate using a Scrapy settings object, which will respect
        [[these Scrapy settings]{.std
        .std-ref}](#topics-email-settings){.hoverxref .tooltip
        .reference .internal}.

        Parameters

        :   **settings** ([[`scrapy.settings.Settings`{.xref .py
            .py-class .docutils .literal
            .notranslate}]{.pre}](index.html#scrapy.settings.Settings "scrapy.settings.Settings"){.reference
            .internal} object) -- the e-mail recipients

    [[send]{.pre}]{.sig-name .descname}[(]{.sig-paren}*[[to]{.pre}]{.n}*, *[[subject]{.pre}]{.n}*, *[[body]{.pre}]{.n}*, *[[cc]{.pre}]{.n}[[=]{.pre}]{.o}[[None]{.pre}]{.default_value}*, *[[attachs]{.pre}]{.n}[[=]{.pre}]{.o}[[()]{.pre}]{.default_value}*, *[[mimetype]{.pre}]{.n}[[=]{.pre}]{.o}[[\'text/plain\']{.pre}]{.default_value}*, *[[charset]{.pre}]{.n}[[=]{.pre}]{.o}[[None]{.pre}]{.default_value}*[)]{.sig-paren}[[[\[source\]]{.pre}]{.viewcode-link}](_modules/scrapy/mail.html#MailSender.send){.reference .internal}[¶](#scrapy.mail.MailSender.send "Permalink to this definition"){.headerlink}

    :   Send email to the given recipients.

        Parameters

        :   -   **to**
                ([*str*](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.12)"){.reference
                .external} *or*
                [*list*](https://docs.python.org/3/library/stdtypes.html#list "(in Python v3.12)"){.reference
                .external}) -- the e-mail recipients as a string or as a
                list of strings

            -   **subject**
                ([*str*](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.12)"){.reference
                .external}) -- the subject of the e-mail

            -   **cc**
                ([*str*](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.12)"){.reference
                .external} *or*
                [*list*](https://docs.python.org/3/library/stdtypes.html#list "(in Python v3.12)"){.reference
                .external}) -- the e-mails to CC as a string or as a
                list of strings

            -   **body**
                ([*str*](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.12)"){.reference
                .external}) -- the e-mail body

            -   **attachs**
                ([*collections.abc.Iterable*](https://docs.python.org/3/library/collections.abc.html#collections.abc.Iterable "(in Python v3.12)"){.reference
                .external}) -- an iterable of tuples
                [`(attach_name,`{.docutils .literal
                .notranslate}]{.pre}` `{.docutils .literal
                .notranslate}[`mimetype,`{.docutils .literal
                .notranslate}]{.pre}` `{.docutils .literal
                .notranslate}[`file_object)`{.docutils .literal
                .notranslate}]{.pre} where [`attach_name`{.docutils
                .literal .notranslate}]{.pre} is a string with the name
                that will appear on the e-mail's attachment,
                [`mimetype`{.docutils .literal .notranslate}]{.pre} is
                the mimetype of the attachment and
                [`file_object`{.docutils .literal .notranslate}]{.pre}
                is a readable file object with the contents of the
                attachment

            -   **mimetype**
                ([*str*](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.12)"){.reference
                .external}) -- the MIME type of the e-mail

            -   **charset**
                ([*str*](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.12)"){.reference
                .external}) -- the character encoding to use for the
                e-mail contents
:::

::: {#mail-settings .section}
[]{#topics-email-settings}

#### Mail settings[¶](#mail-settings "Permalink to this heading"){.headerlink}

These settings define the default [`__init__`{.docutils .literal
.notranslate}]{.pre} method values of the [[`MailSender`{.xref .py
.py-class .docutils .literal
.notranslate}]{.pre}](#scrapy.mail.MailSender "scrapy.mail.MailSender"){.reference
.internal} class, and can be used to configure e-mail notifications in
your project without writing any code (for those extensions and code
that uses [[`MailSender`{.xref .py .py-class .docutils .literal
.notranslate}]{.pre}](#scrapy.mail.MailSender "scrapy.mail.MailSender"){.reference
.internal}).

::: {#mail-from .section}
[]{#std-setting-MAIL_FROM}

##### MAIL_FROM[¶](#mail-from "Permalink to this heading"){.headerlink}

Default: [`'scrapy@localhost'`{.docutils .literal .notranslate}]{.pre}

Sender email to use ([`From:`{.docutils .literal .notranslate}]{.pre}
header) for sending emails.
:::

::: {#mail-host .section}
[]{#std-setting-MAIL_HOST}

##### MAIL_HOST[¶](#mail-host "Permalink to this heading"){.headerlink}

Default: [`'localhost'`{.docutils .literal .notranslate}]{.pre}

SMTP host to use for sending emails.
:::

::: {#mail-port .section}
[]{#std-setting-MAIL_PORT}

##### MAIL_PORT[¶](#mail-port "Permalink to this heading"){.headerlink}

Default: [`25`{.docutils .literal .notranslate}]{.pre}

SMTP port to use for sending emails.
:::

::: {#mail-user .section}
[]{#std-setting-MAIL_USER}

##### MAIL_USER[¶](#mail-user "Permalink to this heading"){.headerlink}

Default: [`None`{.docutils .literal .notranslate}]{.pre}

User to use for SMTP authentication. If disabled no SMTP authentication
will be performed.
:::

::: {#mail-pass .section}
[]{#std-setting-MAIL_PASS}

##### MAIL_PASS[¶](#mail-pass "Permalink to this heading"){.headerlink}

Default: [`None`{.docutils .literal .notranslate}]{.pre}

Password to use for SMTP authentication, along with [[`MAIL_USER`{.xref
.std .std-setting .docutils .literal
.notranslate}]{.pre}](#std-setting-MAIL_USER){.hoverxref .tooltip
.reference .internal}.
:::

::: {#mail-tls .section}
[]{#std-setting-MAIL_TLS}

##### MAIL_TLS[¶](#mail-tls "Permalink to this heading"){.headerlink}

Default: [`False`{.docutils .literal .notranslate}]{.pre}

Enforce using STARTTLS. STARTTLS is a way to take an existing insecure
connection, and upgrade it to a secure connection using SSL/TLS.
:::

::: {#mail-ssl .section}
[]{#std-setting-MAIL_SSL}

##### MAIL_SSL[¶](#mail-ssl "Permalink to this heading"){.headerlink}

Default: [`False`{.docutils .literal .notranslate}]{.pre}

Enforce connecting using an SSL encrypted connection
:::
:::
:::

[]{#document-topics/telnetconsole}

::: {#telnet-console .section}
[]{#topics-telnetconsole}

### Telnet Console[¶](#telnet-console "Permalink to this heading"){.headerlink}

Scrapy comes with a built-in telnet console for inspecting and
controlling a Scrapy running process. The telnet console is just a
regular python shell running inside the Scrapy process, so you can do
literally anything from it.

The telnet console is a [[built-in Scrapy extension]{.std
.std-ref}](index.html#topics-extensions-ref){.hoverxref .tooltip
.reference .internal} which comes enabled by default, but you can also
disable it if you want. For more information about the extension itself
see [[Telnet console extension]{.std
.std-ref}](index.html#topics-extensions-ref-telnetconsole){.hoverxref
.tooltip .reference .internal}.

::: {.admonition .warning}
Warning

It is not secure to use telnet console via public networks, as telnet
doesn't provide any transport-layer security. Having username/password
authentication doesn't change that.

Intended usage is connecting to a running Scrapy spider locally (spider
process and telnet client are on the same machine) or over a secure
connection (VPN, SSH tunnel). Please avoid using telnet console over
insecure connections, or disable it completely using
[[`TELNETCONSOLE_ENABLED`{.xref .std .std-setting .docutils .literal
.notranslate}]{.pre}](index.html#std-setting-TELNETCONSOLE_ENABLED){.hoverxref
.tooltip .reference .internal} option.
:::

::: {#how-to-access-the-telnet-console .section}
#### How to access the telnet console[¶](#how-to-access-the-telnet-console "Permalink to this heading"){.headerlink}

The telnet console listens in the TCP port defined in the
[[`TELNETCONSOLE_PORT`{.xref .std .std-setting .docutils .literal
.notranslate}]{.pre}](#std-setting-TELNETCONSOLE_PORT){.hoverxref
.tooltip .reference .internal} setting, which defaults to
[`6023`{.docutils .literal .notranslate}]{.pre}. To access the console
you need to type:

::: {.highlight-none .notranslate}
::: highlight
    telnet localhost 6023
    Trying localhost...
    Connected to localhost.
    Escape character is '^]'.
    Username:
    Password:
    >>>
:::
:::

By default Username is [`scrapy`{.docutils .literal .notranslate}]{.pre}
and Password is autogenerated. The autogenerated Password can be seen on
Scrapy logs like the example below:

::: {.highlight-none .notranslate}
::: highlight
    2018-10-16 14:35:21 [scrapy.extensions.telnet] INFO: Telnet Password: 16f92501e8a59326
:::
:::

Default Username and Password can be overridden by the settings
[[`TELNETCONSOLE_USERNAME`{.xref .std .std-setting .docutils .literal
.notranslate}]{.pre}](#std-setting-TELNETCONSOLE_USERNAME){.hoverxref
.tooltip .reference .internal} and [[`TELNETCONSOLE_PASSWORD`{.xref .std
.std-setting .docutils .literal
.notranslate}]{.pre}](#std-setting-TELNETCONSOLE_PASSWORD){.hoverxref
.tooltip .reference .internal}.

::: {.admonition .warning}
Warning

Username and password provide only a limited protection, as telnet is
not using secure transport - by default traffic is not encrypted even if
username and password are set.
:::

You need the telnet program which comes installed by default in Windows,
and most Linux distros.
:::

::: {#available-variables-in-the-telnet-console .section}
#### Available variables in the telnet console[¶](#available-variables-in-the-telnet-console "Permalink to this heading"){.headerlink}

The telnet console is like a regular Python shell running inside the
Scrapy process, so you can do anything from it including importing new
modules, etc.

However, the telnet console comes with some default variables defined
for convenience:

+------------+---------------------------------------------------------+
| Shortcut   | Description                                             |
+============+=========================================================+
| [`crawler` | the Scrapy Crawler ([[`scrapy.crawler.Crawler`{.xref    |
| {.docutils | .py .py-class .docutils .literal                        |
| .literal   | .notranslate}]{.pre}](index.html#scra                   |
| .notransla | py.crawler.Crawler "scrapy.crawler.Crawler"){.reference |
| te}]{.pre} | .internal} object)                                      |
+------------+---------------------------------------------------------+
| [`engine`  | Crawler.engine attribute                                |
| {.docutils |                                                         |
| .literal   |                                                         |
| .notransla |                                                         |
| te}]{.pre} |                                                         |
+------------+---------------------------------------------------------+
| [`spider`  | the active spider                                       |
| {.docutils |                                                         |
| .literal   |                                                         |
| .notransla |                                                         |
| te}]{.pre} |                                                         |
+------------+---------------------------------------------------------+
| [`slot`    | the engine slot                                         |
| {.docutils |                                                         |
| .literal   |                                                         |
| .notransla |                                                         |
| te}]{.pre} |                                                         |
+------------+---------------------------------------------------------+
| [`e        | the Extension Manager (Crawler.extensions attribute)    |
| xtensions` |                                                         |
| {.docutils |                                                         |
| .literal   |                                                         |
| .notransla |                                                         |
| te}]{.pre} |                                                         |
+------------+---------------------------------------------------------+
| [`stats`   | the Stats Collector (Crawler.stats attribute)           |
| {.docutils |                                                         |
| .literal   |                                                         |
| .notransla |                                                         |
| te}]{.pre} |                                                         |
+------------+---------------------------------------------------------+
| [          | the Scrapy settings object (Crawler.settings attribute) |
| `settings` |                                                         |
| {.docutils |                                                         |
| .literal   |                                                         |
| .notransla |                                                         |
| te}]{.pre} |                                                         |
+------------+---------------------------------------------------------+
| [`est`     | print a report of the engine status                     |
| {.docutils |                                                         |
| .literal   |                                                         |
| .notransla |                                                         |
| te}]{.pre} |                                                         |
+------------+---------------------------------------------------------+
| [`prefs`   | for memory debugging (see [[Debugging memory            |
| {.docutils | leaks]{.std                                             |
| .literal   | .std-ref}](index.html#topics-leaks){.hoverxref .tooltip |
| .notransla | .reference .internal})                                  |
| te}]{.pre} |                                                         |
+------------+---------------------------------------------------------+
| [`p`       | a shortcut to the [[`pprint.pprint()`{.xref .py         |
| {.docutils | .py-func .docutils .literal                             |
| .literal   | .no                                                     |
| .notransla | translate}]{.pre}](https://docs.python.org/3/library/pp |
| te}]{.pre} | rint.html#pprint.pprint "(in Python v3.12)"){.reference |
|            | .external} function                                     |
+------------+---------------------------------------------------------+
| [`hpy`     | for memory debugging (see [[Debugging memory            |
| {.docutils | leaks]{.std                                             |
| .literal   | .std-ref}](index.html#topics-leaks){.hoverxref .tooltip |
| .notransla | .reference .internal})                                  |
| te}]{.pre} |                                                         |
+------------+---------------------------------------------------------+
:::

::: {#telnet-console-usage-examples .section}
#### Telnet console usage examples[¶](#telnet-console-usage-examples "Permalink to this heading"){.headerlink}

Here are some example tasks you can do with the telnet console:

::: {#view-engine-status .section}
##### View engine status[¶](#view-engine-status "Permalink to this heading"){.headerlink}

You can use the [`est()`{.docutils .literal .notranslate}]{.pre} method
of the Scrapy engine to quickly show its state using the telnet console:

::: {.highlight-none .notranslate}
::: highlight
    telnet localhost 6023
    >>> est()
    Execution engine status

    time()-engine.start_time                        : 8.62972998619
    len(engine.downloader.active)                   : 16
    engine.scraper.is_idle()                        : False
    engine.spider.name                              : followall
    engine.spider_is_idle()                         : False
    engine.slot.closing                             : False
    len(engine.slot.inprogress)                     : 16
    len(engine.slot.scheduler.dqs or [])            : 0
    len(engine.slot.scheduler.mqs)                  : 92
    len(engine.scraper.slot.queue)                  : 0
    len(engine.scraper.slot.active)                 : 0
    engine.scraper.slot.active_size                 : 0
    engine.scraper.slot.itemproc_size               : 0
    engine.scraper.slot.needs_backout()             : False
:::
:::
:::

::: {#pause-resume-and-stop-the-scrapy-engine .section}
##### Pause, resume and stop the Scrapy engine[¶](#pause-resume-and-stop-the-scrapy-engine "Permalink to this heading"){.headerlink}

To pause:

::: {.highlight-none .notranslate}
::: highlight
    telnet localhost 6023
    >>> engine.pause()
    >>>
:::
:::

To resume:

::: {.highlight-none .notranslate}
::: highlight
    telnet localhost 6023
    >>> engine.unpause()
    >>>
:::
:::

To stop:

::: {.highlight-none .notranslate}
::: highlight
    telnet localhost 6023
    >>> engine.stop()
    Connection closed by foreign host.
:::
:::
:::
:::

::: {#telnet-console-signals .section}
#### Telnet Console signals[¶](#telnet-console-signals "Permalink to this heading"){.headerlink}

[]{#std-signal-update_telnet_vars .target}

[[scrapy.extensions.telnet.]{.pre}]{.sig-prename .descclassname}[[update_telnet_vars]{.pre}]{.sig-name .descname}[(]{.sig-paren}*[[telnet_vars]{.pre}]{.n}*[)]{.sig-paren}[¶](#scrapy.extensions.telnet.update_telnet_vars "Permalink to this definition"){.headerlink}

:   Sent just before the telnet console is opened. You can hook up to
    this signal to add, remove or update the variables that will be
    available in the telnet local namespace. In order to do that, you
    need to update the [`telnet_vars`{.docutils .literal
    .notranslate}]{.pre} dict in your handler.

    Parameters

    :   **telnet_vars**
        ([*dict*](https://docs.python.org/3/library/stdtypes.html#dict "(in Python v3.12)"){.reference
        .external}) -- the dict of telnet variables
:::

::: {#telnet-settings .section}
#### Telnet settings[¶](#telnet-settings "Permalink to this heading"){.headerlink}

These are the settings that control the telnet console's behaviour:

::: {#telnetconsole-port .section}
[]{#std-setting-TELNETCONSOLE_PORT}

##### TELNETCONSOLE_PORT[¶](#telnetconsole-port "Permalink to this heading"){.headerlink}

Default: [`[6023,`{.docutils .literal .notranslate}]{.pre}` `{.docutils
.literal .notranslate}[`6073]`{.docutils .literal .notranslate}]{.pre}

The port range to use for the telnet console. If set to
[`None`{.docutils .literal .notranslate}]{.pre} or [`0`{.docutils
.literal .notranslate}]{.pre}, a dynamically assigned port is used.
:::

::: {#telnetconsole-host .section}
[]{#std-setting-TELNETCONSOLE_HOST}

##### TELNETCONSOLE_HOST[¶](#telnetconsole-host "Permalink to this heading"){.headerlink}

Default: [`'127.0.0.1'`{.docutils .literal .notranslate}]{.pre}

The interface the telnet console should listen on
:::

::: {#telnetconsole-username .section}
[]{#std-setting-TELNETCONSOLE_USERNAME}

##### TELNETCONSOLE_USERNAME[¶](#telnetconsole-username "Permalink to this heading"){.headerlink}

Default: [`'scrapy'`{.docutils .literal .notranslate}]{.pre}

The username used for the telnet console
:::

::: {#telnetconsole-password .section}
[]{#std-setting-TELNETCONSOLE_PASSWORD}

##### TELNETCONSOLE_PASSWORD[¶](#telnetconsole-password "Permalink to this heading"){.headerlink}

Default: [`None`{.docutils .literal .notranslate}]{.pre}

The password used for the telnet console, default behaviour is to have
it autogenerated
:::
:::
:::
:::

[[Logging]{.doc}](index.html#document-topics/logging){.reference .internal}

:   Learn how to use Python's builtin logging on Scrapy.

[[Stats Collection]{.doc}](index.html#document-topics/stats){.reference .internal}

:   Collect statistics about your scraping crawler.

[[Sending e-mail]{.doc}](index.html#document-topics/email){.reference .internal}

:   Send email notifications when certain events occur.

[[Telnet Console]{.doc}](index.html#document-topics/telnetconsole){.reference .internal}

:   Inspect a running crawler using a built-in Python console.
:::

::: {#solving-specific-problems .section}
## Solving specific problems[¶](#solving-specific-problems "Permalink to this heading"){.headerlink}

::: {.toctree-wrapper .compound}
[]{#document-faq}

::: {#frequently-asked-questions .section}
[]{#faq}

### Frequently Asked Questions[¶](#frequently-asked-questions "Permalink to this heading"){.headerlink}

::: {#how-does-scrapy-compare-to-beautifulsoup-or-lxml .section}
[]{#faq-scrapy-bs-cmp}

#### How does Scrapy compare to BeautifulSoup or lxml?[¶](#how-does-scrapy-compare-to-beautifulsoup-or-lxml "Permalink to this heading"){.headerlink}

[BeautifulSoup](https://www.crummy.com/software/BeautifulSoup/){.reference
.external} and [lxml](https://lxml.de/){.reference .external} are
libraries for parsing HTML and XML. Scrapy is an application framework
for writing web spiders that crawl web sites and extract data from them.

Scrapy provides a built-in mechanism for extracting data (called
[[selectors]{.std .std-ref}](index.html#topics-selectors){.hoverxref
.tooltip .reference .internal}) but you can easily use
[BeautifulSoup](https://www.crummy.com/software/BeautifulSoup/){.reference
.external} (or [lxml](https://lxml.de/){.reference .external}) instead,
if you feel more comfortable working with them. After all, they're just
parsing libraries which can be imported and used from any Python code.

In other words, comparing
[BeautifulSoup](https://www.crummy.com/software/BeautifulSoup/){.reference
.external} (or [lxml](https://lxml.de/){.reference .external}) to Scrapy
is like comparing
[jinja2](https://palletsprojects.com/p/jinja/){.reference .external} to
[Django](https://www.djangoproject.com/){.reference .external}.
:::

::: {#can-i-use-scrapy-with-beautifulsoup .section}
#### Can I use Scrapy with BeautifulSoup?[¶](#can-i-use-scrapy-with-beautifulsoup "Permalink to this heading"){.headerlink}

Yes, you can. As mentioned [[above]{.std
.std-ref}](#faq-scrapy-bs-cmp){.hoverxref .tooltip .reference
.internal},
[BeautifulSoup](https://www.crummy.com/software/BeautifulSoup/){.reference
.external} can be used for parsing HTML responses in Scrapy callbacks.
You just have to feed the response's body into a
[`BeautifulSoup`{.docutils .literal .notranslate}]{.pre} object and
extract whatever data you need from it.

Here's an example spider using BeautifulSoup API, with [`lxml`{.docutils
.literal .notranslate}]{.pre} as the HTML parser:

::: {.highlight-python .notranslate}
::: highlight
    from bs4 import BeautifulSoup
    import scrapy


    class ExampleSpider(scrapy.Spider):
        name = "example"
        allowed_domains = ["example.com"]
        start_urls = ("http://www.example.com/",)

        def parse(self, response):
            # use lxml to get decent HTML parsing speed
            soup = BeautifulSoup(response.text, "lxml")
            yield {"url": response.url, "title": soup.h1.string}
:::
:::

::: {.admonition .note}
Note

[`BeautifulSoup`{.docutils .literal .notranslate}]{.pre} supports
several HTML/XML parsers. See [BeautifulSoup's official
documentation](https://www.crummy.com/software/BeautifulSoup/bs4/doc/#specifying-the-parser-to-use){.reference
.external} on which ones are available.
:::
:::

::: {#did-scrapy-steal-x-from-django .section}
#### Did Scrapy "steal" X from Django?[¶](#did-scrapy-steal-x-from-django "Permalink to this heading"){.headerlink}

Probably, but we don't like that word. We think
[Django](https://www.djangoproject.com/){.reference .external} is a
great open source project and an example to follow, so we've used it as
an inspiration for Scrapy.

We believe that, if something is already done well, there's no need to
reinvent it. This concept, besides being one of the foundations for open
source and free software, not only applies to software but also to
documentation, procedures, policies, etc. So, instead of going through
each problem ourselves, we choose to copy ideas from those projects that
have already solved them properly, and focus on the real problems we
need to solve.

We'd be proud if Scrapy serves as an inspiration for other projects.
Feel free to steal from us!
:::

::: {#does-scrapy-work-with-http-proxies .section}
#### Does Scrapy work with HTTP proxies?[¶](#does-scrapy-work-with-http-proxies "Permalink to this heading"){.headerlink}

Yes. Support for HTTP proxies is provided (since Scrapy 0.8) through the
HTTP Proxy downloader middleware. See [[`HttpProxyMiddleware`{.xref .py
.py-class .docutils .literal
.notranslate}]{.pre}](index.html#scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware "scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware"){.reference
.internal}.
:::

::: {#how-can-i-scrape-an-item-with-attributes-in-different-pages .section}
#### How can I scrape an item with attributes in different pages?[¶](#how-can-i-scrape-an-item-with-attributes-in-different-pages "Permalink to this heading"){.headerlink}

See [[Passing additional data to callback functions]{.std
.std-ref}](index.html#topics-request-response-ref-request-callback-arguments){.hoverxref
.tooltip .reference .internal}.
:::

::: {#how-can-i-simulate-a-user-login-in-my-spider .section}
#### How can I simulate a user login in my spider?[¶](#how-can-i-simulate-a-user-login-in-my-spider "Permalink to this heading"){.headerlink}

See [[Using FormRequest.from_response() to simulate a user login]{.std
.std-ref}](index.html#topics-request-response-ref-request-userlogin){.hoverxref
.tooltip .reference .internal}.
:::

::: {#does-scrapy-crawl-in-breadth-first-or-depth-first-order .section}
[]{#faq-bfo-dfo}

#### Does Scrapy crawl in breadth-first or depth-first order?[¶](#does-scrapy-crawl-in-breadth-first-or-depth-first-order "Permalink to this heading"){.headerlink}

By default, Scrapy uses a
[LIFO](https://en.wikipedia.org/wiki/Stack_(abstract_data_type)){.reference
.external} queue for storing pending requests, which basically means
that it crawls in [DFO
order](https://en.wikipedia.org/wiki/Depth-first_search){.reference
.external}. This order is more convenient in most cases.

If you do want to crawl in true [BFO
order](https://en.wikipedia.org/wiki/Breadth-first_search){.reference
.external}, you can do it by setting the following settings:

::: {.highlight-python .notranslate}
::: highlight
    DEPTH_PRIORITY = 1
    SCHEDULER_DISK_QUEUE = "scrapy.squeues.PickleFifoDiskQueue"
    SCHEDULER_MEMORY_QUEUE = "scrapy.squeues.FifoMemoryQueue"
:::
:::

While pending requests are below the configured values of
[[`CONCURRENT_REQUESTS`{.xref .std .std-setting .docutils .literal
.notranslate}]{.pre}](index.html#std-setting-CONCURRENT_REQUESTS){.hoverxref
.tooltip .reference .internal}, [[`CONCURRENT_REQUESTS_PER_DOMAIN`{.xref
.std .std-setting .docutils .literal
.notranslate}]{.pre}](index.html#std-setting-CONCURRENT_REQUESTS_PER_DOMAIN){.hoverxref
.tooltip .reference .internal} or [[`CONCURRENT_REQUESTS_PER_IP`{.xref
.std .std-setting .docutils .literal
.notranslate}]{.pre}](index.html#std-setting-CONCURRENT_REQUESTS_PER_IP){.hoverxref
.tooltip .reference .internal}, those requests are sent concurrently. As
a result, the first few requests of a crawl rarely follow the desired
order. Lowering those settings to [`1`{.docutils .literal
.notranslate}]{.pre} enforces the desired order, but it significantly
slows down the crawl as a whole.
:::

::: {#my-scrapy-crawler-has-memory-leaks-what-can-i-do .section}
#### My Scrapy crawler has memory leaks. What can I do?[¶](#my-scrapy-crawler-has-memory-leaks-what-can-i-do "Permalink to this heading"){.headerlink}

See [[Debugging memory leaks]{.std
.std-ref}](index.html#topics-leaks){.hoverxref .tooltip .reference
.internal}.

Also, Python has a builtin memory leak issue which is described in
[[Leaks without leaks]{.std
.std-ref}](index.html#topics-leaks-without-leaks){.hoverxref .tooltip
.reference .internal}.
:::

::: {#how-can-i-make-scrapy-consume-less-memory .section}
#### How can I make Scrapy consume less memory?[¶](#how-can-i-make-scrapy-consume-less-memory "Permalink to this heading"){.headerlink}

See previous question.
:::

::: {#how-can-i-prevent-memory-errors-due-to-many-allowed-domains .section}
#### How can I prevent memory errors due to many allowed domains?[¶](#how-can-i-prevent-memory-errors-due-to-many-allowed-domains "Permalink to this heading"){.headerlink}

If you have a spider with a long list of [[`allowed_domains`{.xref .py
.py-attr .docutils .literal
.notranslate}]{.pre}](index.html#scrapy.Spider.allowed_domains "scrapy.Spider.allowed_domains"){.reference
.internal} (e.g. 50,000+), consider replacing the default
[[`OffsiteMiddleware`{.xref .py .py-class .docutils .literal
.notranslate}]{.pre}](index.html#scrapy.spidermiddlewares.offsite.OffsiteMiddleware "scrapy.spidermiddlewares.offsite.OffsiteMiddleware"){.reference
.internal} spider middleware with a [[custom spider middleware]{.std
.std-ref}](index.html#custom-spider-middleware){.hoverxref .tooltip
.reference .internal} that requires less memory. For example:

-   If your domain names are similar enough, use your own regular
    expression instead joining the strings in [[`allowed_domains`{.xref
    .py .py-attr .docutils .literal
    .notranslate}]{.pre}](index.html#scrapy.Spider.allowed_domains "scrapy.Spider.allowed_domains"){.reference
    .internal} into a complex regular expression.

-   If you can [meet the installation
    requirements](https://github.com/andreasvc/pyre2#installation){.reference
    .external}, use
    [pyre2](https://github.com/andreasvc/pyre2){.reference .external}
    instead of Python's
    [re](https://docs.python.org/library/re.html){.reference .external}
    to compile your URL-filtering regular expression. See [issue
    1908](https://github.com/scrapy/scrapy/issues/1908){.reference
    .external}.

See also other suggestions at
[StackOverflow](https://stackoverflow.com/q/36440681/939364){.reference
.external}.

::: {.admonition .note}
Note

Remember to disable
[[`scrapy.spidermiddlewares.offsite.OffsiteMiddleware`{.xref .py
.py-class .docutils .literal
.notranslate}]{.pre}](index.html#scrapy.spidermiddlewares.offsite.OffsiteMiddleware "scrapy.spidermiddlewares.offsite.OffsiteMiddleware"){.reference
.internal} when you enable your custom implementation:

::: {.highlight-python .notranslate}
::: highlight
    SPIDER_MIDDLEWARES = {
        "scrapy.spidermiddlewares.offsite.OffsiteMiddleware": None,
        "myproject.middlewares.CustomOffsiteMiddleware": 500,
    }
:::
:::
:::
:::

::: {#can-i-use-basic-http-authentication-in-my-spiders .section}
#### Can I use Basic HTTP Authentication in my spiders?[¶](#can-i-use-basic-http-authentication-in-my-spiders "Permalink to this heading"){.headerlink}

Yes, see [[`HttpAuthMiddleware`{.xref .py .py-class .docutils .literal
.notranslate}]{.pre}](index.html#scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware "scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware"){.reference
.internal}.
:::

::: {#why-does-scrapy-download-pages-in-english-instead-of-my-native-language .section}
#### Why does Scrapy download pages in English instead of my native language?[¶](#why-does-scrapy-download-pages-in-english-instead-of-my-native-language "Permalink to this heading"){.headerlink}

Try changing the default
[Accept-Language](https://www.w3.org/Protocols/rfc2616/rfc2616-sec14.html#sec14.4){.reference
.external} request header by overriding the
[[`DEFAULT_REQUEST_HEADERS`{.xref .std .std-setting .docutils .literal
.notranslate}]{.pre}](index.html#std-setting-DEFAULT_REQUEST_HEADERS){.hoverxref
.tooltip .reference .internal} setting.
:::

::: {#where-can-i-find-some-example-scrapy-projects .section}
#### Where can I find some example Scrapy projects?[¶](#where-can-i-find-some-example-scrapy-projects "Permalink to this heading"){.headerlink}

See [[Examples]{.std .std-ref}](index.html#intro-examples){.hoverxref
.tooltip .reference .internal}.
:::

::: {#can-i-run-a-spider-without-creating-a-project .section}
#### Can I run a spider without creating a project?[¶](#can-i-run-a-spider-without-creating-a-project "Permalink to this heading"){.headerlink}

Yes. You can use the [[`runspider`{.xref .std .std-command .docutils
.literal
.notranslate}]{.pre}](index.html#std-command-runspider){.hoverxref
.tooltip .reference .internal} command. For example, if you have a
spider written in a [`my_spider.py`{.docutils .literal
.notranslate}]{.pre} file you can run it with:

::: {.highlight-default .notranslate}
::: highlight
    scrapy runspider my_spider.py
:::
:::

See [[`runspider`{.xref .std .std-command .docutils .literal
.notranslate}]{.pre}](index.html#std-command-runspider){.hoverxref
.tooltip .reference .internal} command for more info.
:::

::: {#i-get-filtered-offsite-request-messages-how-can-i-fix-them .section}
#### I get "Filtered offsite request" messages. How can I fix them?[¶](#i-get-filtered-offsite-request-messages-how-can-i-fix-them "Permalink to this heading"){.headerlink}

Those messages (logged with [`DEBUG`{.docutils .literal
.notranslate}]{.pre} level) don't necessarily mean there is a problem,
so you may not need to fix them.

Those messages are thrown by the Offsite Spider Middleware, which is a
spider middleware (enabled by default) whose purpose is to filter out
requests to domains outside the ones covered by the spider.

For more info see: [[`OffsiteMiddleware`{.xref .py .py-class .docutils
.literal
.notranslate}]{.pre}](index.html#scrapy.spidermiddlewares.offsite.OffsiteMiddleware "scrapy.spidermiddlewares.offsite.OffsiteMiddleware"){.reference
.internal}.
:::

::: {#what-is-the-recommended-way-to-deploy-a-scrapy-crawler-in-production .section}
#### What is the recommended way to deploy a Scrapy crawler in production?[¶](#what-is-the-recommended-way-to-deploy-a-scrapy-crawler-in-production "Permalink to this heading"){.headerlink}

See [[Deploying Spiders]{.std
.std-ref}](index.html#topics-deploy){.hoverxref .tooltip .reference
.internal}.
:::

::: {#can-i-use-json-for-large-exports .section}
#### Can I use JSON for large exports?[¶](#can-i-use-json-for-large-exports "Permalink to this heading"){.headerlink}

It'll depend on how large your output is. See [[this warning]{.std
.std-ref}](index.html#json-with-large-data){.hoverxref .tooltip
.reference .internal} in [[`JsonItemExporter`{.xref .py .py-class
.docutils .literal
.notranslate}]{.pre}](index.html#scrapy.exporters.JsonItemExporter "scrapy.exporters.JsonItemExporter"){.reference
.internal} documentation.
:::

::: {#can-i-return-twisted-deferreds-from-signal-handlers .section}
#### Can I return (Twisted) deferreds from signal handlers?[¶](#can-i-return-twisted-deferreds-from-signal-handlers "Permalink to this heading"){.headerlink}

Some signals support returning deferreds from their handlers, others
don't. See the [[Built-in signals reference]{.std
.std-ref}](index.html#topics-signals-ref){.hoverxref .tooltip .reference
.internal} to know which ones.
:::

::: {#what-does-the-response-status-code-999-mean .section}
#### What does the response status code 999 mean?[¶](#what-does-the-response-status-code-999-mean "Permalink to this heading"){.headerlink}

999 is a custom response status code used by Yahoo sites to throttle
requests. Try slowing down the crawling speed by using a download delay
of [`2`{.docutils .literal .notranslate}]{.pre} (or higher) in your
spider:

::: {.highlight-python .notranslate}
::: highlight
    from scrapy.spiders import CrawlSpider


    class MySpider(CrawlSpider):
        name = "myspider"

        download_delay = 2

        # [ ... rest of the spider code ... ]
:::
:::

Or by setting a global download delay in your project with the
[[`DOWNLOAD_DELAY`{.xref .std .std-setting .docutils .literal
.notranslate}]{.pre}](index.html#std-setting-DOWNLOAD_DELAY){.hoverxref
.tooltip .reference .internal} setting.
:::

::: {#can-i-call-pdb-set-trace-from-my-spiders-to-debug-them .section}
#### Can I call [`pdb.set_trace()`{.docutils .literal .notranslate}]{.pre} from my spiders to debug them?[¶](#can-i-call-pdb-set-trace-from-my-spiders-to-debug-them "Permalink to this heading"){.headerlink}

Yes, but you can also use the Scrapy shell which allows you to quickly
analyze (and even modify) the response being processed by your spider,
which is, quite often, more useful than plain old
[`pdb.set_trace()`{.docutils .literal .notranslate}]{.pre}.

For more info see [[Invoking the shell from spiders to inspect
responses]{.std
.std-ref}](index.html#topics-shell-inspect-response){.hoverxref .tooltip
.reference .internal}.
:::

::: {#simplest-way-to-dump-all-my-scraped-items-into-a-json-csv-xml-file .section}
#### Simplest way to dump all my scraped items into a JSON/CSV/XML file?[¶](#simplest-way-to-dump-all-my-scraped-items-into-a-json-csv-xml-file "Permalink to this heading"){.headerlink}

To dump into a JSON file:

::: {.highlight-default .notranslate}
::: highlight
    scrapy crawl myspider -O items.json
:::
:::

To dump into a CSV file:

::: {.highlight-default .notranslate}
::: highlight
    scrapy crawl myspider -O items.csv
:::
:::

To dump into a XML file:

::: {.highlight-default .notranslate}
::: highlight
    scrapy crawl myspider -O items.xml
:::
:::

For more information see [[Feed exports]{.std
.std-ref}](index.html#topics-feed-exports){.hoverxref .tooltip
.reference .internal}
:::

::: {#what-s-this-huge-cryptic-viewstate-parameter-used-in-some-forms .section}
#### What's this huge cryptic [`__VIEWSTATE`{.docutils .literal .notranslate}]{.pre} parameter used in some forms?[¶](#what-s-this-huge-cryptic-viewstate-parameter-used-in-some-forms "Permalink to this heading"){.headerlink}

The [`__VIEWSTATE`{.docutils .literal .notranslate}]{.pre} parameter is
used in sites built with ASP.NET/VB.NET. For more info on how it works
see [this
page](https://metacpan.org/pod/release/ECARROLL/HTML-TreeBuilderX-ASP_NET-0.09/lib/HTML/TreeBuilderX/ASP_NET.pm){.reference
.external}. Also, here's an [example
spider](https://github.com/AmbientLighter/rpn-fas/blob/master/fas/spiders/rnp.py){.reference
.external} which scrapes one of these sites.
:::

::: {#what-s-the-best-way-to-parse-big-xml-csv-data-feeds .section}
#### What's the best way to parse big XML/CSV data feeds?[¶](#what-s-the-best-way-to-parse-big-xml-csv-data-feeds "Permalink to this heading"){.headerlink}

Parsing big feeds with XPath selectors can be problematic since they
need to build the DOM of the entire feed in memory, and this can be
quite slow and consume a lot of memory.

In order to avoid parsing all the entire feed at once in memory, you can
use the functions [`xmliter`{.docutils .literal .notranslate}]{.pre} and
[`csviter`{.docutils .literal .notranslate}]{.pre} from
[`scrapy.utils.iterators`{.docutils .literal .notranslate}]{.pre}
module. In fact, this is what the feed spiders (see [[Spiders]{.std
.std-ref}](index.html#topics-spiders){.hoverxref .tooltip .reference
.internal}) use under the cover.
:::

::: {#does-scrapy-manage-cookies-automatically .section}
#### Does Scrapy manage cookies automatically?[¶](#does-scrapy-manage-cookies-automatically "Permalink to this heading"){.headerlink}

Yes, Scrapy receives and keeps track of cookies sent by servers, and
sends them back on subsequent requests, like any regular web browser
does.

For more info see [[Requests and Responses]{.std
.std-ref}](index.html#topics-request-response){.hoverxref .tooltip
.reference .internal} and [[CookiesMiddleware]{.std
.std-ref}](index.html#cookies-mw){.hoverxref .tooltip .reference
.internal}.
:::

::: {#how-can-i-see-the-cookies-being-sent-and-received-from-scrapy .section}
#### How can I see the cookies being sent and received from Scrapy?[¶](#how-can-i-see-the-cookies-being-sent-and-received-from-scrapy "Permalink to this heading"){.headerlink}

Enable the [[`COOKIES_DEBUG`{.xref .std .std-setting .docutils .literal
.notranslate}]{.pre}](index.html#std-setting-COOKIES_DEBUG){.hoverxref
.tooltip .reference .internal} setting.
:::

::: {#how-can-i-instruct-a-spider-to-stop-itself .section}
#### How can I instruct a spider to stop itself?[¶](#how-can-i-instruct-a-spider-to-stop-itself "Permalink to this heading"){.headerlink}

Raise the [[`CloseSpider`{.xref .py .py-exc .docutils .literal
.notranslate}]{.pre}](index.html#scrapy.exceptions.CloseSpider "scrapy.exceptions.CloseSpider"){.reference
.internal} exception from a callback. For more info see:
[[`CloseSpider`{.xref .py .py-exc .docutils .literal
.notranslate}]{.pre}](index.html#scrapy.exceptions.CloseSpider "scrapy.exceptions.CloseSpider"){.reference
.internal}.
:::

::: {#how-can-i-prevent-my-scrapy-bot-from-getting-banned .section}
#### How can I prevent my Scrapy bot from getting banned?[¶](#how-can-i-prevent-my-scrapy-bot-from-getting-banned "Permalink to this heading"){.headerlink}

See [[Avoiding getting banned]{.std
.std-ref}](index.html#bans){.hoverxref .tooltip .reference .internal}.
:::

::: {#should-i-use-spider-arguments-or-settings-to-configure-my-spider .section}
#### Should I use spider arguments or settings to configure my spider?[¶](#should-i-use-spider-arguments-or-settings-to-configure-my-spider "Permalink to this heading"){.headerlink}

Both [[spider arguments]{.std
.std-ref}](index.html#spiderargs){.hoverxref .tooltip .reference
.internal} and [[settings]{.std
.std-ref}](index.html#topics-settings){.hoverxref .tooltip .reference
.internal} can be used to configure your spider. There is no strict rule
that mandates to use one or the other, but settings are more suited for
parameters that, once set, don't change much, while spider arguments are
meant to change more often, even on each spider run and sometimes are
required for the spider to run at all (for example, to set the start url
of a spider).

To illustrate with an example, assuming you have a spider that needs to
log into a site to scrape data, and you only want to scrape data from a
certain section of the site (which varies each time). In that case, the
credentials to log in would be settings, while the url of the section to
scrape would be a spider argument.
:::

::: {#i-m-scraping-a-xml-document-and-my-xpath-selector-doesn-t-return-any-items .section}
#### I'm scraping a XML document and my XPath selector doesn't return any items[¶](#i-m-scraping-a-xml-document-and-my-xpath-selector-doesn-t-return-any-items "Permalink to this heading"){.headerlink}

You may need to remove namespaces. See [[Removing namespaces]{.std
.std-ref}](index.html#removing-namespaces){.hoverxref .tooltip
.reference .internal}.
:::

::: {#how-to-split-an-item-into-multiple-items-in-an-item-pipeline .section}
[]{#faq-split-item}

#### How to split an item into multiple items in an item pipeline?[¶](#how-to-split-an-item-into-multiple-items-in-an-item-pipeline "Permalink to this heading"){.headerlink}

[[Item pipelines]{.std
.std-ref}](index.html#topics-item-pipeline){.hoverxref .tooltip
.reference .internal} cannot yield multiple items per input item.
[[Create a spider middleware]{.std
.std-ref}](index.html#custom-spider-middleware){.hoverxref .tooltip
.reference .internal} instead, and use its
[[`process_spider_output()`{.xref .py .py-meth .docutils .literal
.notranslate}]{.pre}](index.html#scrapy.spidermiddlewares.SpiderMiddleware.process_spider_output "scrapy.spidermiddlewares.SpiderMiddleware.process_spider_output"){.reference
.internal} method for this purpose. For example:

::: {.highlight-python .notranslate}
::: highlight
    from copy import deepcopy

    from itemadapter import is_item, ItemAdapter


    class MultiplyItemsMiddleware:
        def process_spider_output(self, response, result, spider):
            for item in result:
                if is_item(item):
                    adapter = ItemAdapter(item)
                    for _ in range(adapter["multiply_by"]):
                        yield deepcopy(item)
:::
:::
:::

::: {#does-scrapy-support-ipv6-addresses .section}
#### Does Scrapy support IPv6 addresses?[¶](#does-scrapy-support-ipv6-addresses "Permalink to this heading"){.headerlink}

Yes, by setting [[`DNS_RESOLVER`{.xref .std .std-setting .docutils
.literal
.notranslate}]{.pre}](index.html#std-setting-DNS_RESOLVER){.hoverxref
.tooltip .reference .internal} to
[`scrapy.resolver.CachingHostnameResolver`{.docutils .literal
.notranslate}]{.pre}. Note that by doing so, you lose the ability to set
a specific timeout for DNS requests (the value of the
[[`DNS_TIMEOUT`{.xref .std .std-setting .docutils .literal
.notranslate}]{.pre}](index.html#std-setting-DNS_TIMEOUT){.hoverxref
.tooltip .reference .internal} setting is ignored).
:::

::: {#how-to-deal-with-class-valueerror-filedescriptor-out-of-range-in-select-exceptions .section}
[]{#faq-specific-reactor}

#### How to deal with [`<class`{.docutils .literal .notranslate}]{.pre}` `{.docutils .literal .notranslate}[`'ValueError'>:`{.docutils .literal .notranslate}]{.pre}` `{.docutils .literal .notranslate}[`filedescriptor`{.docutils .literal .notranslate}]{.pre}` `{.docutils .literal .notranslate}[`out`{.docutils .literal .notranslate}]{.pre}` `{.docutils .literal .notranslate}[`of`{.docutils .literal .notranslate}]{.pre}` `{.docutils .literal .notranslate}[`range`{.docutils .literal .notranslate}]{.pre}` `{.docutils .literal .notranslate}[`in`{.docutils .literal .notranslate}]{.pre}` `{.docutils .literal .notranslate}[`select()`{.docutils .literal .notranslate}]{.pre} exceptions?[¶](#how-to-deal-with-class-valueerror-filedescriptor-out-of-range-in-select-exceptions "Permalink to this heading"){.headerlink}

This issue [has been
reported](https://github.com/scrapy/scrapy/issues/2905){.reference
.external} to appear when running broad crawls in macOS, where the
default Twisted reactor is
[[`twisted.internet.selectreactor.SelectReactor`{.xref .py .py-class
.docutils .literal
.notranslate}]{.pre}](https://docs.twisted.org/en/stable/api/twisted.internet.selectreactor.SelectReactor.html "(in Twisted)"){.reference
.external}. Switching to a different reactor is possible by using the
[[`TWISTED_REACTOR`{.xref .std .std-setting .docutils .literal
.notranslate}]{.pre}](index.html#std-setting-TWISTED_REACTOR){.hoverxref
.tooltip .reference .internal} setting.
:::

::: {#how-can-i-cancel-the-download-of-a-given-response .section}
[]{#faq-stop-response-download}

#### How can I cancel the download of a given response?[¶](#how-can-i-cancel-the-download-of-a-given-response "Permalink to this heading"){.headerlink}

In some situations, it might be useful to stop the download of a certain
response. For instance, sometimes you can determine whether or not you
need the full contents of a response by inspecting its headers or the
first bytes of its body. In that case, you could save resources by
attaching a handler to the [[`bytes_received`{.xref .py .py-class
.docutils .literal
.notranslate}]{.pre}](index.html#scrapy.signals.bytes_received "scrapy.signals.bytes_received"){.reference
.internal} or [[`headers_received`{.xref .py .py-class .docutils
.literal
.notranslate}]{.pre}](index.html#scrapy.signals.headers_received "scrapy.signals.headers_received"){.reference
.internal} signals and raising a [[`StopDownload`{.xref .py .py-exc
.docutils .literal
.notranslate}]{.pre}](index.html#scrapy.exceptions.StopDownload "scrapy.exceptions.StopDownload"){.reference
.internal} exception. Please refer to the [[Stopping the download of a
Response]{.std
.std-ref}](index.html#topics-stop-response-download){.hoverxref .tooltip
.reference .internal} topic for additional information and examples.
:::

::: {#running-runspider-i-get-error-no-spider-found-in-file-filename .section}
#### Running [`runspider`{.docutils .literal .notranslate}]{.pre} I get [`error:`{.docutils .literal .notranslate}]{.pre}` `{.docutils .literal .notranslate}[`No`{.docutils .literal .notranslate}]{.pre}` `{.docutils .literal .notranslate}[`spider`{.docutils .literal .notranslate}]{.pre}` `{.docutils .literal .notranslate}[`found`{.docutils .literal .notranslate}]{.pre}` `{.docutils .literal .notranslate}[`in`{.docutils .literal .notranslate}]{.pre}` `{.docutils .literal .notranslate}[`file:`{.docutils .literal .notranslate}]{.pre}` `{.docutils .literal .notranslate}[`<filename>`{.docutils .literal .notranslate}]{.pre}[¶](#running-runspider-i-get-error-no-spider-found-in-file-filename "Permalink to this heading"){.headerlink}

This may happen if your Scrapy project has a spider module with a name
that conflicts with the name of one of the [Python standard library
modules](https://docs.python.org/py-modindex.html){.reference
.external}, such as [`csv.py`{.docutils .literal .notranslate}]{.pre} or
[`os.py`{.docutils .literal .notranslate}]{.pre}, or any [Python
package](https://pypi.org/){.reference .external} that you have
installed. See [issue
2680](https://github.com/scrapy/scrapy/issues/2680){.reference
.external}.
:::
:::

[]{#document-topics/debug}

::: {#debugging-spiders .section}
[]{#topics-debug}

### Debugging Spiders[¶](#debugging-spiders "Permalink to this heading"){.headerlink}

This document explains the most common techniques for debugging spiders.
Consider the following Scrapy spider below:

::: {.highlight-python .notranslate}
::: highlight
    import scrapy
    from myproject.items import MyItem


    class MySpider(scrapy.Spider):
        name = "myspider"
        start_urls = (
            "http://example.com/page1",
            "http://example.com/page2",
        )

        def parse(self, response):
            # <processing code not shown>
            # collect `item_urls`
            for item_url in item_urls:
                yield scrapy.Request(item_url, self.parse_item)

        def parse_item(self, response):
            # <processing code not shown>
            item = MyItem()
            # populate `item` fields
            # and extract item_details_url
            yield scrapy.Request(
                item_details_url, self.parse_details, cb_kwargs={"item": item}
            )

        def parse_details(self, response, item):
            # populate more `item` fields
            return item
:::
:::

Basically this is a simple spider which parses two pages of items (the
start_urls). Items also have a details page with additional information,
so we use the [`cb_kwargs`{.docutils .literal .notranslate}]{.pre}
functionality of [`Request`{.xref .py .py-class .docutils .literal
.notranslate}]{.pre} to pass a partially populated item.

::: {#parse-command .section}
#### Parse Command[¶](#parse-command "Permalink to this heading"){.headerlink}

The most basic way of checking the output of your spider is to use the
[[`parse`{.xref .std .std-command .docutils .literal
.notranslate}]{.pre}](index.html#std-command-parse){.hoverxref .tooltip
.reference .internal} command. It allows to check the behaviour of
different parts of the spider at the method level. It has the advantage
of being flexible and simple to use, but does not allow debugging code
inside a method.

In order to see the item scraped from a specific url:

::: {.highlight-none .notranslate}
::: highlight
    $ scrapy parse --spider=myspider -c parse_item -d 2 <item_url>
    [ ... scrapy log lines crawling example.com spider ... ]

    >>> STATUS DEPTH LEVEL 2 <<<
    # Scraped Items  ------------------------------------------------------------
    [{'url': <item_url>}]

    # Requests  -----------------------------------------------------------------
    []
:::
:::

Using the [`--verbose`{.docutils .literal .notranslate}]{.pre} or
[`-v`{.docutils .literal .notranslate}]{.pre} option we can see the
status at each depth level:

::: {.highlight-none .notranslate}
::: highlight
    $ scrapy parse --spider=myspider -c parse_item -d 2 -v <item_url>
    [ ... scrapy log lines crawling example.com spider ... ]

    >>> DEPTH LEVEL: 1 <<<
    # Scraped Items  ------------------------------------------------------------
    []

    # Requests  -----------------------------------------------------------------
    [<GET item_details_url>]


    >>> DEPTH LEVEL: 2 <<<
    # Scraped Items  ------------------------------------------------------------
    [{'url': <item_url>}]

    # Requests  -----------------------------------------------------------------
    []
:::
:::

Checking items scraped from a single start_url, can also be easily
achieved using:

::: {.highlight-none .notranslate}
::: highlight
    $ scrapy parse --spider=myspider -d 3 'http://example.com/page1'
:::
:::
:::

::: {#scrapy-shell .section}
#### Scrapy Shell[¶](#scrapy-shell "Permalink to this heading"){.headerlink}

While the [[`parse`{.xref .std .std-command .docutils .literal
.notranslate}]{.pre}](index.html#std-command-parse){.hoverxref .tooltip
.reference .internal} command is very useful for checking behaviour of a
spider, it is of little help to check what happens inside a callback,
besides showing the response received and the output. How to debug the
situation when [`parse_details`{.docutils .literal .notranslate}]{.pre}
sometimes receives no item?

Fortunately, the [[`shell`{.xref .std .std-command .docutils .literal
.notranslate}]{.pre}](index.html#std-command-shell){.hoverxref .tooltip
.reference .internal} is your bread and butter in this case (see
[[Invoking the shell from spiders to inspect responses]{.std
.std-ref}](index.html#topics-shell-inspect-response){.hoverxref .tooltip
.reference .internal}):

::: {.highlight-python .notranslate}
::: highlight
    from scrapy.shell import inspect_response


    def parse_details(self, response, item=None):
        if item:
            # populate more `item` fields
            return item
        else:
            inspect_response(response, self)
:::
:::

See also: [[Invoking the shell from spiders to inspect responses]{.std
.std-ref}](index.html#topics-shell-inspect-response){.hoverxref .tooltip
.reference .internal}.
:::

::: {#open-in-browser .section}
#### Open in browser[¶](#open-in-browser "Permalink to this heading"){.headerlink}

Sometimes you just want to see how a certain response looks in a
browser, you can use the [`open_in_browser`{.docutils .literal
.notranslate}]{.pre} function for that. Here is an example of how you
would use it:

::: {.highlight-python .notranslate}
::: highlight
    from scrapy.utils.response import open_in_browser


    def parse_details(self, response):
        if "item name" not in response.body:
            open_in_browser(response)
:::
:::

[`open_in_browser`{.docutils .literal .notranslate}]{.pre} will open a
browser with the response received by Scrapy at that point, adjusting
the [base tag](https://www.w3schools.com/tags/tag_base.asp){.reference
.external} so that images and styles are displayed properly.
:::

::: {#logging .section}
#### Logging[¶](#logging "Permalink to this heading"){.headerlink}

Logging is another useful option for getting information about your
spider run. Although not as convenient, it comes with the advantage that
the logs will be available in all future runs should they be necessary
again:

::: {.highlight-python .notranslate}
::: highlight
    def parse_details(self, response, item=None):
        if item:
            # populate more `item` fields
            return item
        else:
            self.logger.warning("No item received for %s", response.url)
:::
:::

For more information, check the [[Logging]{.std
.std-ref}](index.html#topics-logging){.hoverxref .tooltip .reference
.internal} section.
:::

::: {#visual-studio-code .section}
[]{#debug-vscode}

#### Visual Studio Code[¶](#visual-studio-code "Permalink to this heading"){.headerlink}

To debug spiders with Visual Studio Code you can use the following
[`launch.json`{.docutils .literal .notranslate}]{.pre}:

::: {.highlight-json .notranslate}
::: highlight
    {
        "version": "0.1.0",
        "configurations": [
            {
                "name": "Python: Launch Scrapy Spider",
                "type": "python",
                "request": "launch",
                "module": "scrapy",
                "args": [
                    "runspider",
                    "${file}"
                ],
                "console": "integratedTerminal"
            }
        ]
    }
:::
:::

Also, make sure you enable "User Uncaught Exceptions", to catch
exceptions in your Scrapy spider.
:::
:::

[]{#document-topics/contracts}

::: {#spiders-contracts .section}
[]{#topics-contracts}

### Spiders Contracts[¶](#spiders-contracts "Permalink to this heading"){.headerlink}

Testing spiders can get particularly annoying and while nothing prevents
you from writing unit tests the task gets cumbersome quickly. Scrapy
offers an integrated way of testing your spiders by the means of
contracts.

This allows you to test each callback of your spider by hardcoding a
sample url and check various constraints for how the callback processes
the response. Each contract is prefixed with an [`@`{.docutils .literal
.notranslate}]{.pre} and included in the docstring. See the following
example:

::: {.highlight-python .notranslate}
::: highlight
    def parse(self, response):
        """
        This function parses a sample response. Some contracts are mingled
        with this docstring.

        @url http://www.amazon.com/s?field-keywords=selfish+gene
        @returns items 1 16
        @returns requests 0 0
        @scrapes Title Author Year Price
        """
:::
:::

This callback is tested using three built-in contracts:

[]{#module-scrapy.contracts.default .target}

*[class]{.pre}[ ]{.w}*[[scrapy.contracts.default.]{.pre}]{.sig-prename .descclassname}[[UrlContract]{.pre}]{.sig-name .descname}[[[\[source\]]{.pre}]{.viewcode-link}](_modules/scrapy/contracts/default.html#UrlContract){.reference .internal}[¶](#scrapy.contracts.default.UrlContract "Permalink to this definition"){.headerlink}

:   This contract ([`@url`{.docutils .literal .notranslate}]{.pre}) sets
    the sample URL used when checking other contract conditions for this
    spider. This contract is mandatory. All callbacks lacking this
    contract are ignored when running the checks:

    ::: {.highlight-default .notranslate}
    ::: highlight
        @url url
    :::
    :::

```{=html}
<!-- -->
```

*[class]{.pre}[ ]{.w}*[[scrapy.contracts.default.]{.pre}]{.sig-prename .descclassname}[[CallbackKeywordArgumentsContract]{.pre}]{.sig-name .descname}[[[\[source\]]{.pre}]{.viewcode-link}](_modules/scrapy/contracts/default.html#CallbackKeywordArgumentsContract){.reference .internal}[¶](#scrapy.contracts.default.CallbackKeywordArgumentsContract "Permalink to this definition"){.headerlink}

:   This contract ([`@cb_kwargs`{.docutils .literal
    .notranslate}]{.pre}) sets the [`cb_kwargs`{.xref .py .py-attr
    .docutils .literal .notranslate}]{.pre} attribute for the sample
    request. It must be a valid JSON dictionary.

    ::: {.highlight-default .notranslate}
    ::: highlight
        @cb_kwargs {"arg1": "value1", "arg2": "value2", ...}
    :::
    :::

```{=html}
<!-- -->
```

*[class]{.pre}[ ]{.w}*[[scrapy.contracts.default.]{.pre}]{.sig-prename .descclassname}[[ReturnsContract]{.pre}]{.sig-name .descname}[[[\[source\]]{.pre}]{.viewcode-link}](_modules/scrapy/contracts/default.html#ReturnsContract){.reference .internal}[¶](#scrapy.contracts.default.ReturnsContract "Permalink to this definition"){.headerlink}

:   This contract ([`@returns`{.docutils .literal .notranslate}]{.pre})
    sets lower and upper bounds for the items and requests returned by
    the spider. The upper bound is optional:

    ::: {.highlight-default .notranslate}
    ::: highlight
        @returns item(s)|request(s) [min [max]]
    :::
    :::

```{=html}
<!-- -->
```

*[class]{.pre}[ ]{.w}*[[scrapy.contracts.default.]{.pre}]{.sig-prename .descclassname}[[ScrapesContract]{.pre}]{.sig-name .descname}[[[\[source\]]{.pre}]{.viewcode-link}](_modules/scrapy/contracts/default.html#ScrapesContract){.reference .internal}[¶](#scrapy.contracts.default.ScrapesContract "Permalink to this definition"){.headerlink}

:   This contract ([`@scrapes`{.docutils .literal .notranslate}]{.pre})
    checks that all the items returned by the callback have the
    specified fields:

    ::: {.highlight-default .notranslate}
    ::: highlight
        @scrapes field_1 field_2 ...
    :::
    :::

Use the [[`check`{.xref .std .std-command .docutils .literal
.notranslate}]{.pre}](index.html#std-command-check){.hoverxref .tooltip
.reference .internal} command to run the contract checks.

::: {#custom-contracts .section}
#### Custom Contracts[¶](#custom-contracts "Permalink to this heading"){.headerlink}

If you find you need more power than the built-in Scrapy contracts you
can create and load your own contracts in the project by using the
[[`SPIDER_CONTRACTS`{.xref .std .std-setting .docutils .literal
.notranslate}]{.pre}](index.html#std-setting-SPIDER_CONTRACTS){.hoverxref
.tooltip .reference .internal} setting:

::: {.highlight-python .notranslate}
::: highlight
    SPIDER_CONTRACTS = {
        "myproject.contracts.ResponseCheck": 10,
        "myproject.contracts.ItemValidate": 10,
    }
:::
:::

Each contract must inherit from [[`Contract`{.xref .py .py-class
.docutils .literal
.notranslate}]{.pre}](#scrapy.contracts.Contract "scrapy.contracts.Contract"){.reference
.internal} and can override three methods:

[]{#module-scrapy.contracts .target}

*[class]{.pre}[ ]{.w}*[[scrapy.contracts.]{.pre}]{.sig-prename .descclassname}[[Contract]{.pre}]{.sig-name .descname}[(]{.sig-paren}*[[method]{.pre}]{.n}*, *[[\*]{.pre}]{.o}[[args]{.pre}]{.n}*[)]{.sig-paren}[[[\[source\]]{.pre}]{.viewcode-link}](_modules/scrapy/contracts.html#Contract){.reference .internal}[¶](#scrapy.contracts.Contract "Permalink to this definition"){.headerlink}

:   

    Parameters

    :   -   **method**
            ([*collections.abc.Callable*](https://docs.python.org/3/library/collections.abc.html#collections.abc.Callable "(in Python v3.12)"){.reference
            .external}) -- callback function to which the contract is
            associated

        -   **args**
            ([*list*](https://docs.python.org/3/library/stdtypes.html#list "(in Python v3.12)"){.reference
            .external}) -- list of arguments passed into the docstring
            (whitespace separated)

    [[adjust_request_args]{.pre}]{.sig-name .descname}[(]{.sig-paren}*[[args]{.pre}]{.n}*[)]{.sig-paren}[[[\[source\]]{.pre}]{.viewcode-link}](_modules/scrapy/contracts.html#Contract.adjust_request_args){.reference .internal}[¶](#scrapy.contracts.Contract.adjust_request_args "Permalink to this definition"){.headerlink}

    :   This receives a [`dict`{.docutils .literal .notranslate}]{.pre}
        as an argument containing default arguments for request object.
        [`Request`{.xref .py .py-class .docutils .literal
        .notranslate}]{.pre} is used by default, but this can be changed
        with the [`request_cls`{.docutils .literal .notranslate}]{.pre}
        attribute. If multiple contracts in chain have this attribute
        defined, the last one is used.

        Must return the same or a modified version of it.

    [[pre_process]{.pre}]{.sig-name .descname}[(]{.sig-paren}*[[response]{.pre}]{.n}*[)]{.sig-paren}[¶](#scrapy.contracts.Contract.pre_process "Permalink to this definition"){.headerlink}

    :   This allows hooking in various checks on the response received
        from the sample request, before it's being passed to the
        callback.

    [[post_process]{.pre}]{.sig-name .descname}[(]{.sig-paren}*[[output]{.pre}]{.n}*[)]{.sig-paren}[¶](#scrapy.contracts.Contract.post_process "Permalink to this definition"){.headerlink}

    :   This allows processing the output of the callback. Iterators are
        converted to lists before being passed to this hook.

Raise [[`ContractFail`{.xref .py .py-class .docutils .literal
.notranslate}]{.pre}](#scrapy.exceptions.ContractFail "scrapy.exceptions.ContractFail"){.reference
.internal} from [[`pre_process`{.xref .py .py-class .docutils .literal
.notranslate}]{.pre}](#scrapy.contracts.Contract.pre_process "scrapy.contracts.Contract.pre_process"){.reference
.internal} or [[`post_process`{.xref .py .py-class .docutils .literal
.notranslate}]{.pre}](#scrapy.contracts.Contract.post_process "scrapy.contracts.Contract.post_process"){.reference
.internal} if expectations are not met:

*[class]{.pre}[ ]{.w}*[[scrapy.exceptions.]{.pre}]{.sig-prename .descclassname}[[ContractFail]{.pre}]{.sig-name .descname}[[[\[source\]]{.pre}]{.viewcode-link}](_modules/scrapy/exceptions.html#ContractFail){.reference .internal}[¶](#scrapy.exceptions.ContractFail "Permalink to this definition"){.headerlink}

:   Error raised in case of a failing contract

Here is a demo contract which checks the presence of a custom header in
the response received:

::: {.highlight-python .notranslate}
::: highlight
    from scrapy.contracts import Contract
    from scrapy.exceptions import ContractFail


    class HasHeaderContract(Contract):
        """
        Demo contract which checks the presence of a custom header
        @has_header X-CustomHeader
        """

        name = "has_header"

        def pre_process(self, response):
            for header in self.args:
                if header not in response.headers:
                    raise ContractFail("X-CustomHeader not present")
:::
:::
:::

::: {#detecting-check-runs .section}
[]{#detecting-contract-check-runs}

#### Detecting check runs[¶](#detecting-check-runs "Permalink to this heading"){.headerlink}

When [`scrapy`{.docutils .literal .notranslate}]{.pre}` `{.docutils
.literal .notranslate}[`check`{.docutils .literal .notranslate}]{.pre}
is running, the [`SCRAPY_CHECK`{.docutils .literal .notranslate}]{.pre}
environment variable is set to the [`true`{.docutils .literal
.notranslate}]{.pre} string. You can use [[`os.environ`{.xref .py
.py-data .docutils .literal
.notranslate}]{.pre}](https://docs.python.org/3/library/os.html#os.environ "(in Python v3.12)"){.reference
.external} to perform any change to your spiders or your settings when
[`scrapy`{.docutils .literal .notranslate}]{.pre}` `{.docutils .literal
.notranslate}[`check`{.docutils .literal .notranslate}]{.pre} is used:

::: {.highlight-python .notranslate}
::: highlight
    import os
    import scrapy


    class ExampleSpider(scrapy.Spider):
        name = "example"

        def __init__(self):
            if os.environ.get("SCRAPY_CHECK"):
                pass  # Do some scraper adjustments when a check is running
:::
:::
:::
:::

[]{#document-topics/practices}

::: {#common-practices .section}
[]{#topics-practices}

### Common Practices[¶](#common-practices "Permalink to this heading"){.headerlink}

This section documents common practices when using Scrapy. These are
things that cover many topics and don't often fall into any other
specific section.

::: {#run-scrapy-from-a-script .section}
[]{#run-from-script}

#### Run Scrapy from a script[¶](#run-scrapy-from-a-script "Permalink to this heading"){.headerlink}

You can use the [[API]{.std .std-ref}](index.html#topics-api){.hoverxref
.tooltip .reference .internal} to run Scrapy from a script, instead of
the typical way of running Scrapy via [`scrapy`{.docutils .literal
.notranslate}]{.pre}` `{.docutils .literal
.notranslate}[`crawl`{.docutils .literal .notranslate}]{.pre}.

Remember that Scrapy is built on top of the Twisted asynchronous
networking library, so you need to run it inside the Twisted reactor.

The first utility you can use to run your spiders is
[[`scrapy.crawler.CrawlerProcess`{.xref .py .py-class .docutils .literal
.notranslate}]{.pre}](index.html#scrapy.crawler.CrawlerProcess "scrapy.crawler.CrawlerProcess"){.reference
.internal}. This class will start a Twisted reactor for you, configuring
the logging and setting shutdown handlers. This class is the one used by
all Scrapy commands.

Here's an example showing how to run a single spider with it.

::: {.highlight-python .notranslate}
::: highlight
    import scrapy
    from scrapy.crawler import CrawlerProcess


    class MySpider(scrapy.Spider):
        # Your spider definition
        ...


    process = CrawlerProcess(
        settings={
            "FEEDS": {
                "items.json": {"format": "json"},
            },
        }
    )

    process.crawl(MySpider)
    process.start()  # the script will block here until the crawling is finished
:::
:::

Define settings within dictionary in CrawlerProcess. Make sure to check
[[`CrawlerProcess`{.xref .py .py-class .docutils .literal
.notranslate}]{.pre}](index.html#scrapy.crawler.CrawlerProcess "scrapy.crawler.CrawlerProcess"){.reference
.internal} documentation to get acquainted with its usage details.

If you are inside a Scrapy project there are some additional helpers you
can use to import those components within the project. You can
automatically import your spiders passing their name to
[[`CrawlerProcess`{.xref .py .py-class .docutils .literal
.notranslate}]{.pre}](index.html#scrapy.crawler.CrawlerProcess "scrapy.crawler.CrawlerProcess"){.reference
.internal}, and use [`get_project_settings`{.docutils .literal
.notranslate}]{.pre} to get a [[`Settings`{.xref .py .py-class .docutils
.literal
.notranslate}]{.pre}](index.html#scrapy.settings.Settings "scrapy.settings.Settings"){.reference
.internal} instance with your project settings.

What follows is a working example of how to do that, using the
[testspiders](https://github.com/scrapinghub/testspiders){.reference
.external} project as example.

::: {.highlight-python .notranslate}
::: highlight
    from scrapy.crawler import CrawlerProcess
    from scrapy.utils.project import get_project_settings

    process = CrawlerProcess(get_project_settings())

    # 'followall' is the name of one of the spiders of the project.
    process.crawl("followall", domain="scrapy.org")
    process.start()  # the script will block here until the crawling is finished
:::
:::

There's another Scrapy utility that provides more control over the
crawling process: [[`scrapy.crawler.CrawlerRunner`{.xref .py .py-class
.docutils .literal
.notranslate}]{.pre}](index.html#scrapy.crawler.CrawlerRunner "scrapy.crawler.CrawlerRunner"){.reference
.internal}. This class is a thin wrapper that encapsulates some simple
helpers to run multiple crawlers, but it won't start or interfere with
existing reactors in any way.

Using this class the reactor should be explicitly run after scheduling
your spiders. It's recommended you use [[`CrawlerRunner`{.xref .py
.py-class .docutils .literal
.notranslate}]{.pre}](index.html#scrapy.crawler.CrawlerRunner "scrapy.crawler.CrawlerRunner"){.reference
.internal} instead of [[`CrawlerProcess`{.xref .py .py-class .docutils
.literal
.notranslate}]{.pre}](index.html#scrapy.crawler.CrawlerProcess "scrapy.crawler.CrawlerProcess"){.reference
.internal} if your application is already using Twisted and you want to
run Scrapy in the same reactor.

Note that you will also have to shutdown the Twisted reactor yourself
after the spider is finished. This can be achieved by adding callbacks
to the deferred returned by the [[`CrawlerRunner.crawl`{.xref .py
.py-meth .docutils .literal
.notranslate}]{.pre}](index.html#scrapy.crawler.CrawlerRunner.crawl "scrapy.crawler.CrawlerRunner.crawl"){.reference
.internal} method.

Here's an example of its usage, along with a callback to manually stop
the reactor after [`MySpider`{.docutils .literal .notranslate}]{.pre}
has finished running.

::: {.highlight-python .notranslate}
::: highlight
    from twisted.internet import reactor
    import scrapy
    from scrapy.crawler import CrawlerRunner
    from scrapy.utils.log import configure_logging


    class MySpider(scrapy.Spider):
        # Your spider definition
        ...


    configure_logging({"LOG_FORMAT": "%(levelname)s: %(message)s"})
    runner = CrawlerRunner()

    d = runner.crawl(MySpider)
    d.addBoth(lambda _: reactor.stop())
    reactor.run()  # the script will block here until the crawling is finished
:::
:::

::: {.admonition .seealso}
See also

[Reactor
Overview](https://docs.twisted.org/en/stable/core/howto/reactor-basics.html "(in Twisted v23.10)"){.reference
.external}
:::
:::

::: {#running-multiple-spiders-in-the-same-process .section}
[]{#run-multiple-spiders}

#### Running multiple spiders in the same process[¶](#running-multiple-spiders-in-the-same-process "Permalink to this heading"){.headerlink}

By default, Scrapy runs a single spider per process when you run
[`scrapy`{.docutils .literal .notranslate}]{.pre}` `{.docutils .literal
.notranslate}[`crawl`{.docutils .literal .notranslate}]{.pre}. However,
Scrapy supports running multiple spiders per process using the
[[internal API]{.std .std-ref}](index.html#topics-api){.hoverxref
.tooltip .reference .internal}.

Here is an example that runs multiple spiders simultaneously:

::: {.highlight-python .notranslate}
::: highlight
    import scrapy
    from scrapy.crawler import CrawlerProcess
    from scrapy.utils.project import get_project_settings


    class MySpider1(scrapy.Spider):
        # Your first spider definition
        ...


    class MySpider2(scrapy.Spider):
        # Your second spider definition
        ...


    settings = get_project_settings()
    process = CrawlerProcess(settings)
    process.crawl(MySpider1)
    process.crawl(MySpider2)
    process.start()  # the script will block here until all crawling jobs are finished
:::
:::

Same example using [[`CrawlerRunner`{.xref .py .py-class .docutils
.literal
.notranslate}]{.pre}](index.html#scrapy.crawler.CrawlerRunner "scrapy.crawler.CrawlerRunner"){.reference
.internal}:

::: {.highlight-python .notranslate}
::: highlight
    import scrapy
    from twisted.internet import reactor
    from scrapy.crawler import CrawlerRunner
    from scrapy.utils.log import configure_logging
    from scrapy.utils.project import get_project_settings


    class MySpider1(scrapy.Spider):
        # Your first spider definition
        ...


    class MySpider2(scrapy.Spider):
        # Your second spider definition
        ...


    configure_logging()
    settings = get_project_settings()
    runner = CrawlerRunner(settings)
    runner.crawl(MySpider1)
    runner.crawl(MySpider2)
    d = runner.join()
    d.addBoth(lambda _: reactor.stop())

    reactor.run()  # the script will block here until all crawling jobs are finished
:::
:::

Same example but running the spiders sequentially by chaining the
deferreds:

::: {.highlight-python .notranslate}
::: highlight
    from twisted.internet import reactor, defer
    from scrapy.crawler import CrawlerRunner
    from scrapy.utils.log import configure_logging
    from scrapy.utils.project import get_project_settings


    class MySpider1(scrapy.Spider):
        # Your first spider definition
        ...


    class MySpider2(scrapy.Spider):
        # Your second spider definition
        ...


    settings = get_project_settings()
    configure_logging(settings)
    runner = CrawlerRunner(settings)


    @defer.inlineCallbacks
    def crawl():
        yield runner.crawl(MySpider1)
        yield runner.crawl(MySpider2)
        reactor.stop()


    crawl()
    reactor.run()  # the script will block here until the last crawl call is finished
:::
:::

Different spiders can set different values for the same setting, but
when they run in the same process it may be impossible, by design or
because of some limitations, to use these different values. What happens
in practice is different for different settings:

-   [[`SPIDER_LOADER_CLASS`{.xref .std .std-setting .docutils .literal
    .notranslate}]{.pre}](index.html#std-setting-SPIDER_LOADER_CLASS){.hoverxref
    .tooltip .reference .internal} and the ones used by its value
    ([[`SPIDER_MODULES`{.xref .std .std-setting .docutils .literal
    .notranslate}]{.pre}](index.html#std-setting-SPIDER_MODULES){.hoverxref
    .tooltip .reference .internal}, [[`SPIDER_LOADER_WARN_ONLY`{.xref
    .std .std-setting .docutils .literal
    .notranslate}]{.pre}](index.html#std-setting-SPIDER_LOADER_WARN_ONLY){.hoverxref
    .tooltip .reference .internal} for the default one) cannot be read
    from the per-spider settings. These are applied when the
    [[`CrawlerRunner`{.xref .py .py-class .docutils .literal
    .notranslate}]{.pre}](index.html#scrapy.crawler.CrawlerRunner "scrapy.crawler.CrawlerRunner"){.reference
    .internal} or [[`CrawlerProcess`{.xref .py .py-class .docutils
    .literal
    .notranslate}]{.pre}](index.html#scrapy.crawler.CrawlerProcess "scrapy.crawler.CrawlerProcess"){.reference
    .internal} object is created.

-   For [[`TWISTED_REACTOR`{.xref .std .std-setting .docutils .literal
    .notranslate}]{.pre}](index.html#std-setting-TWISTED_REACTOR){.hoverxref
    .tooltip .reference .internal} and [[`ASYNCIO_EVENT_LOOP`{.xref .std
    .std-setting .docutils .literal
    .notranslate}]{.pre}](index.html#std-setting-ASYNCIO_EVENT_LOOP){.hoverxref
    .tooltip .reference .internal} the first available value is used,
    and if a spider requests a different reactor an exception will be
    raised. These are applied when the reactor is installed.

-   For [[`REACTOR_THREADPOOL_MAXSIZE`{.xref .std .std-setting .docutils
    .literal
    .notranslate}]{.pre}](index.html#std-setting-REACTOR_THREADPOOL_MAXSIZE){.hoverxref
    .tooltip .reference .internal}, [[`DNS_RESOLVER`{.xref .std
    .std-setting .docutils .literal
    .notranslate}]{.pre}](index.html#std-setting-DNS_RESOLVER){.hoverxref
    .tooltip .reference .internal} and the ones used by the resolver
    ([[`DNSCACHE_ENABLED`{.xref .std .std-setting .docutils .literal
    .notranslate}]{.pre}](index.html#std-setting-DNSCACHE_ENABLED){.hoverxref
    .tooltip .reference .internal}, [[`DNSCACHE_SIZE`{.xref .std
    .std-setting .docutils .literal
    .notranslate}]{.pre}](index.html#std-setting-DNSCACHE_SIZE){.hoverxref
    .tooltip .reference .internal}, [[`DNS_TIMEOUT`{.xref .std
    .std-setting .docutils .literal
    .notranslate}]{.pre}](index.html#std-setting-DNS_TIMEOUT){.hoverxref
    .tooltip .reference .internal} for ones included in Scrapy) the
    first available value is used. These are applied when the reactor is
    started.

::: {.admonition .seealso}
See also

[[Run Scrapy from a script]{.std .std-ref}](#run-from-script){.hoverxref
.tooltip .reference .internal}.
:::
:::

::: {#distributed-crawls .section}
[]{#id1}

#### Distributed crawls[¶](#distributed-crawls "Permalink to this heading"){.headerlink}

Scrapy doesn't provide any built-in facility for running crawls in a
distribute (multi-server) manner. However, there are some ways to
distribute crawls, which vary depending on how you plan to distribute
them.

If you have many spiders, the obvious way to distribute the load is to
setup many Scrapyd instances and distribute spider runs among those.

If you instead want to run a single (big) spider through many machines,
what you usually do is partition the urls to crawl and send them to each
separate spider. Here is a concrete example:

First, you prepare the list of urls to crawl and put them into separate
files/urls:

::: {.highlight-default .notranslate}
::: highlight
    http://somedomain.com/urls-to-crawl/spider1/part1.list
    http://somedomain.com/urls-to-crawl/spider1/part2.list
    http://somedomain.com/urls-to-crawl/spider1/part3.list
:::
:::

Then you fire a spider run on 3 different Scrapyd servers. The spider
would receive a (spider) argument [`part`{.docutils .literal
.notranslate}]{.pre} with the number of the partition to crawl:

::: {.highlight-default .notranslate}
::: highlight
    curl http://scrapy1.mycompany.com:6800/schedule.json -d project=myproject -d spider=spider1 -d part=1
    curl http://scrapy2.mycompany.com:6800/schedule.json -d project=myproject -d spider=spider1 -d part=2
    curl http://scrapy3.mycompany.com:6800/schedule.json -d project=myproject -d spider=spider1 -d part=3
:::
:::
:::

::: {#avoiding-getting-banned .section}
[]{#bans}

#### Avoiding getting banned[¶](#avoiding-getting-banned "Permalink to this heading"){.headerlink}

Some websites implement certain measures to prevent bots from crawling
them, with varying degrees of sophistication. Getting around those
measures can be difficult and tricky, and may sometimes require special
infrastructure. Please consider contacting [commercial
support](https://scrapy.org/support/){.reference .external} if in doubt.

Here are some tips to keep in mind when dealing with these kinds of
sites:

-   rotate your user agent from a pool of well-known ones from browsers
    (google around to get a list of them)

-   disable cookies (see [[`COOKIES_ENABLED`{.xref .std .std-setting
    .docutils .literal
    .notranslate}]{.pre}](index.html#std-setting-COOKIES_ENABLED){.hoverxref
    .tooltip .reference .internal}) as some sites may use cookies to
    spot bot behaviour

-   use download delays (2 or higher). See [[`DOWNLOAD_DELAY`{.xref .std
    .std-setting .docutils .literal
    .notranslate}]{.pre}](index.html#std-setting-DOWNLOAD_DELAY){.hoverxref
    .tooltip .reference .internal} setting.

-   if possible, use [Common Crawl](https://commoncrawl.org/){.reference
    .external} to fetch pages, instead of hitting the sites directly

-   use a pool of rotating IPs. For example, the free [Tor
    project](https://www.torproject.org/){.reference .external} or paid
    services like [ProxyMesh](https://proxymesh.com/){.reference
    .external}. An open source alternative is
    [scrapoxy](https://scrapoxy.io/){.reference .external}, a super
    proxy that you can attach your own proxies to.

-   use a ban avoidance service, such as [Zyte
    API](https://docs.zyte.com/zyte-api/get-started.html){.reference
    .external}, which provides a [Scrapy
    plugin](https://github.com/scrapy-plugins/scrapy-zyte-api){.reference
    .external}

If you are still unable to prevent your bot getting banned, consider
contacting [commercial support](https://scrapy.org/support/){.reference
.external}.
:::
:::

[]{#document-topics/broad-crawls}

::: {#broad-crawls .section}
[]{#topics-broad-crawls}

### Broad Crawls[¶](#broad-crawls "Permalink to this heading"){.headerlink}

Scrapy defaults are optimized for crawling specific sites. These sites
are often handled by a single Scrapy spider, although this is not
necessary or required (for example, there are generic spiders that
handle any given site thrown at them).

In addition to this "focused crawl", there is another common type of
crawling which covers a large (potentially unlimited) number of domains,
and is only limited by time or other arbitrary constraint, rather than
stopping when the domain was crawled to completion or when there are no
more requests to perform. These are called "broad crawls" and is the
typical crawlers employed by search engines.

These are some common properties often found in broad crawls:

-   they crawl many domains (often, unbounded) instead of a specific set
    of sites

-   they don't necessarily crawl domains to completion, because it would
    be impractical (or impossible) to do so, and instead limit the crawl
    by time or number of pages crawled

-   they are simpler in logic (as opposed to very complex spiders with
    many extraction rules) because data is often post-processed in a
    separate stage

-   they crawl many domains concurrently, which allows them to achieve
    faster crawl speeds by not being limited by any particular site
    constraint (each site is crawled slowly to respect politeness, but
    many sites are crawled in parallel)

As said above, Scrapy default settings are optimized for focused crawls,
not broad crawls. However, due to its asynchronous architecture, Scrapy
is very well suited for performing fast broad crawls. This page
summarizes some things you need to keep in mind when using Scrapy for
doing broad crawls, along with concrete suggestions of Scrapy settings
to tune in order to achieve an efficient broad crawl.

::: {#use-the-right-scheduler-priority-queue .section}
[]{#broad-crawls-scheduler-priority-queue}

#### Use the right [[`SCHEDULER_PRIORITY_QUEUE`{.xref .std .std-setting .docutils .literal .notranslate}]{.pre}](index.html#std-setting-SCHEDULER_PRIORITY_QUEUE){.hoverxref .tooltip .reference .internal}[¶](#use-the-right-scheduler-priority-queue "Permalink to this heading"){.headerlink}

Scrapy's default scheduler priority queue is
[`'scrapy.pqueues.ScrapyPriorityQueue'`{.docutils .literal
.notranslate}]{.pre}. It works best during single-domain crawl. It does
not work well with crawling many different domains in parallel

To apply the recommended priority queue use:

::: {.highlight-python .notranslate}
::: highlight
    SCHEDULER_PRIORITY_QUEUE = "scrapy.pqueues.DownloaderAwarePriorityQueue"
:::
:::
:::

::: {#increase-concurrency .section}
[]{#broad-crawls-concurrency}

#### Increase concurrency[¶](#increase-concurrency "Permalink to this heading"){.headerlink}

Concurrency is the number of requests that are processed in parallel.
There is a global limit ([[`CONCURRENT_REQUESTS`{.xref .std .std-setting
.docutils .literal
.notranslate}]{.pre}](index.html#std-setting-CONCURRENT_REQUESTS){.hoverxref
.tooltip .reference .internal}) and an additional limit that can be set
either per domain ([[`CONCURRENT_REQUESTS_PER_DOMAIN`{.xref .std
.std-setting .docutils .literal
.notranslate}]{.pre}](index.html#std-setting-CONCURRENT_REQUESTS_PER_DOMAIN){.hoverxref
.tooltip .reference .internal}) or per IP
([[`CONCURRENT_REQUESTS_PER_IP`{.xref .std .std-setting .docutils
.literal
.notranslate}]{.pre}](index.html#std-setting-CONCURRENT_REQUESTS_PER_IP){.hoverxref
.tooltip .reference .internal}).

::: {.admonition .note}
Note

The scheduler priority queue [[recommended for broad crawls]{.std
.std-ref}](#broad-crawls-scheduler-priority-queue){.hoverxref .tooltip
.reference .internal} does not support
[[`CONCURRENT_REQUESTS_PER_IP`{.xref .std .std-setting .docutils
.literal
.notranslate}]{.pre}](index.html#std-setting-CONCURRENT_REQUESTS_PER_IP){.hoverxref
.tooltip .reference .internal}.
:::

The default global concurrency limit in Scrapy is not suitable for
crawling many different domains in parallel, so you will want to
increase it. How much to increase it will depend on how much CPU and
memory your crawler will have available.

A good starting point is [`100`{.docutils .literal .notranslate}]{.pre}:

::: {.highlight-python .notranslate}
::: highlight
    CONCURRENT_REQUESTS = 100
:::
:::

But the best way to find out is by doing some trials and identifying at
what concurrency your Scrapy process gets CPU bounded. For optimum
performance, you should pick a concurrency where CPU usage is at 80-90%.

Increasing concurrency also increases memory usage. If memory usage is a
concern, you might need to lower your global concurrency limit
accordingly.
:::

::: {#increase-twisted-io-thread-pool-maximum-size .section}
#### Increase Twisted IO thread pool maximum size[¶](#increase-twisted-io-thread-pool-maximum-size "Permalink to this heading"){.headerlink}

Currently Scrapy does DNS resolution in a blocking way with usage of
thread pool. With higher concurrency levels the crawling could be slow
or even fail hitting DNS resolver timeouts. Possible solution to
increase the number of threads handling DNS queries. The DNS queue will
be processed faster speeding up establishing of connection and crawling
overall.

To increase maximum thread pool size use:

::: {.highlight-python .notranslate}
::: highlight
    REACTOR_THREADPOOL_MAXSIZE = 20
:::
:::
:::

::: {#setup-your-own-dns .section}
#### Setup your own DNS[¶](#setup-your-own-dns "Permalink to this heading"){.headerlink}

If you have multiple crawling processes and single central DNS, it can
act like DoS attack on the DNS server resulting to slow down of entire
network or even blocking your machines. To avoid this setup your own DNS
server with local cache and upstream to some large DNS like OpenDNS or
Verizon.
:::

::: {#reduce-log-level .section}
#### Reduce log level[¶](#reduce-log-level "Permalink to this heading"){.headerlink}

When doing broad crawls you are often only interested in the crawl rates
you get and any errors found. These stats are reported by Scrapy when
using the [`INFO`{.docutils .literal .notranslate}]{.pre} log level. In
order to save CPU (and log storage requirements) you should not use
[`DEBUG`{.docutils .literal .notranslate}]{.pre} log level when
preforming large broad crawls in production. Using [`DEBUG`{.docutils
.literal .notranslate}]{.pre} level when developing your (broad) crawler
may be fine though.

To set the log level use:

::: {.highlight-python .notranslate}
::: highlight
    LOG_LEVEL = "INFO"
:::
:::
:::

::: {#disable-cookies .section}
#### Disable cookies[¶](#disable-cookies "Permalink to this heading"){.headerlink}

Disable cookies unless you *really* need. Cookies are often not needed
when doing broad crawls (search engine crawlers ignore them), and they
improve performance by saving some CPU cycles and reducing the memory
footprint of your Scrapy crawler.

To disable cookies use:

::: {.highlight-python .notranslate}
::: highlight
    COOKIES_ENABLED = False
:::
:::
:::

::: {#disable-retries .section}
#### Disable retries[¶](#disable-retries "Permalink to this heading"){.headerlink}

Retrying failed HTTP requests can slow down the crawls substantially,
specially when sites causes are very slow (or fail) to respond, thus
causing a timeout error which gets retried many times, unnecessarily,
preventing crawler capacity to be reused for other domains.

To disable retries use:

::: {.highlight-python .notranslate}
::: highlight
    RETRY_ENABLED = False
:::
:::
:::

::: {#reduce-download-timeout .section}
#### Reduce download timeout[¶](#reduce-download-timeout "Permalink to this heading"){.headerlink}

Unless you are crawling from a very slow connection (which shouldn't be
the case for broad crawls) reduce the download timeout so that stuck
requests are discarded quickly and free up capacity to process the next
ones.

To reduce the download timeout use:

::: {.highlight-python .notranslate}
::: highlight
    DOWNLOAD_TIMEOUT = 15
:::
:::
:::

::: {#disable-redirects .section}
#### Disable redirects[¶](#disable-redirects "Permalink to this heading"){.headerlink}

Consider disabling redirects, unless you are interested in following
them. When doing broad crawls it's common to save redirects and resolve
them when revisiting the site at a later crawl. This also help to keep
the number of request constant per crawl batch, otherwise redirect loops
may cause the crawler to dedicate too many resources on any specific
domain.

To disable redirects use:

::: {.highlight-python .notranslate}
::: highlight
    REDIRECT_ENABLED = False
:::
:::
:::

::: {#enable-crawling-of-ajax-crawlable-pages .section}
#### Enable crawling of "Ajax Crawlable Pages"[¶](#enable-crawling-of-ajax-crawlable-pages "Permalink to this heading"){.headerlink}

Some pages (up to 1%, based on empirical data from year 2013) declare
themselves as [ajax
crawlable](https://developers.google.com/search/docs/ajax-crawling/docs/getting-started){.reference
.external}. This means they provide plain HTML version of content that
is usually available only via AJAX. Pages can indicate it in two ways:

1.  by using [`#!`{.docutils .literal .notranslate}]{.pre} in URL - this
    is the default way;

2.  by using a special meta tag - this way is used on "main", "index"
    website pages.

Scrapy handles (1) automatically; to handle (2) enable
[[AjaxCrawlMiddleware]{.std
.std-ref}](index.html#ajaxcrawl-middleware){.hoverxref .tooltip
.reference .internal}:

::: {.highlight-python .notranslate}
::: highlight
    AJAXCRAWL_ENABLED = True
:::
:::

When doing broad crawls it's common to crawl a lot of "index" web pages;
AjaxCrawlMiddleware helps to crawl them correctly. It is turned OFF by
default because it has some performance overhead, and enabling it for
focused crawls doesn't make much sense.
:::

::: {#crawl-in-bfo-order .section}
[]{#broad-crawls-bfo}

#### Crawl in BFO order[¶](#crawl-in-bfo-order "Permalink to this heading"){.headerlink}

[[Scrapy crawls in DFO order by default]{.std
.std-ref}](index.html#faq-bfo-dfo){.hoverxref .tooltip .reference
.internal}.

In broad crawls, however, page crawling tends to be faster than page
processing. As a result, unprocessed early requests stay in memory until
the final depth is reached, which can significantly increase memory
usage.

[[Crawl in BFO order]{.std .std-ref}](index.html#faq-bfo-dfo){.hoverxref
.tooltip .reference .internal} instead to save memory.
:::

::: {#be-mindful-of-memory-leaks .section}
#### Be mindful of memory leaks[¶](#be-mindful-of-memory-leaks "Permalink to this heading"){.headerlink}

If your broad crawl shows a high memory usage, in addition to [[crawling
in BFO order]{.std .std-ref}](#broad-crawls-bfo){.hoverxref .tooltip
.reference .internal} and [[lowering concurrency]{.std
.std-ref}](#broad-crawls-concurrency){.hoverxref .tooltip .reference
.internal} you should [[debug your memory leaks]{.std
.std-ref}](index.html#topics-leaks){.hoverxref .tooltip .reference
.internal}.
:::

::: {#install-a-specific-twisted-reactor .section}
#### Install a specific Twisted reactor[¶](#install-a-specific-twisted-reactor "Permalink to this heading"){.headerlink}

If the crawl is exceeding the system's capabilities, you might want to
try installing a specific Twisted reactor, via the
[[`TWISTED_REACTOR`{.xref .std .std-setting .docutils .literal
.notranslate}]{.pre}](index.html#std-setting-TWISTED_REACTOR){.hoverxref
.tooltip .reference .internal} setting.
:::
:::

[]{#document-topics/developer-tools}

::: {#using-your-browser-s-developer-tools-for-scraping .section}
[]{#topics-developer-tools}

### Using your browser's Developer Tools for scraping[¶](#using-your-browser-s-developer-tools-for-scraping "Permalink to this heading"){.headerlink}

Here is a general guide on how to use your browser's Developer Tools to
ease the scraping process. Today almost all browsers come with built in
[Developer
Tools](https://en.wikipedia.org/wiki/Web_development_tools){.reference
.external} and although we will use Firefox in this guide, the concepts
are applicable to any other browser.

In this guide we'll introduce the basic tools to use from a browser's
Developer Tools by scraping
[quotes.toscrape.com](https://quotes.toscrape.com){.reference
.external}.

::: {#caveats-with-inspecting-the-live-browser-dom .section}
[]{#topics-livedom}

#### Caveats with inspecting the live browser DOM[¶](#caveats-with-inspecting-the-live-browser-dom "Permalink to this heading"){.headerlink}

Since Developer Tools operate on a live browser DOM, what you'll
actually see when inspecting the page source is not the original HTML,
but a modified one after applying some browser clean up and executing
JavaScript code. Firefox, in particular, is known for adding
[`<tbody>`{.docutils .literal .notranslate}]{.pre} elements to tables.
Scrapy, on the other hand, does not modify the original page HTML, so
you won't be able to extract any data if you use [`<tbody>`{.docutils
.literal .notranslate}]{.pre} in your XPath expressions.

Therefore, you should keep in mind the following things:

-   Disable JavaScript while inspecting the DOM looking for XPaths to be
    used in Scrapy (in the Developer Tools settings click Disable
    JavaScript)

-   Never use full XPath paths, use relative and clever ones based on
    attributes (such as [`id`{.docutils .literal .notranslate}]{.pre},
    [`class`{.docutils .literal .notranslate}]{.pre}, [`width`{.docutils
    .literal .notranslate}]{.pre}, etc) or any identifying features like
    [`contains(@href,`{.docutils .literal
    .notranslate}]{.pre}` `{.docutils .literal
    .notranslate}[`'image')`{.docutils .literal .notranslate}]{.pre}.

-   Never include [`<tbody>`{.docutils .literal .notranslate}]{.pre}
    elements in your XPath expressions unless you really know what
    you're doing
:::

::: {#inspecting-a-website .section}
[]{#topics-inspector}

#### Inspecting a website[¶](#inspecting-a-website "Permalink to this heading"){.headerlink}

By far the most handy feature of the Developer Tools is the Inspector
feature, which allows you to inspect the underlying HTML code of any
webpage. To demonstrate the Inspector, let's look at the
[quotes.toscrape.com](https://quotes.toscrape.com){.reference
.external}-site.

On the site we have a total of ten quotes from various authors with
specific tags, as well as the Top Ten Tags. Let's say we want to extract
all the quotes on this page, without any meta-information about authors,
tags, etc.

Instead of viewing the whole source code for the page, we can simply
right click on a quote and select [`Inspect`{.docutils .literal
.notranslate}]{.pre}` `{.docutils .literal
.notranslate}[`Element`{.docutils .literal
.notranslate}]{.pre}` `{.docutils .literal .notranslate}[`(Q)`{.docutils
.literal .notranslate}]{.pre}, which opens up the Inspector. In it you
should see something like this:

[![Firefox\'s
Inspector-tool](_images/inspector_01.png){style="width: 777px; height: 469px;"}](_images/inspector_01.png){.reference
.internal .image-reference}

The interesting part for us is this:

::: {.highlight-html .notranslate}
::: highlight
    <div class="quote" itemscope="" itemtype="http://schema.org/CreativeWork">
      <span class="text" itemprop="text">(...)</span>
      <span>(...)</span>
      <div class="tags">(...)</div>
    </div>
:::
:::

If you hover over the first [`div`{.docutils .literal
.notranslate}]{.pre} directly above the [`span`{.docutils .literal
.notranslate}]{.pre} tag highlighted in the screenshot, you'll see that
the corresponding section of the webpage gets highlighted as well. So
now we have a section, but we can't find our quote text anywhere.

The advantage of the Inspector is that it automatically expands and
collapses sections and tags of a webpage, which greatly improves
readability. You can expand and collapse a tag by clicking on the arrow
in front of it or by double clicking directly on the tag. If we expand
the [`span`{.docutils .literal .notranslate}]{.pre} tag with the
[`class=`{.docutils .literal .notranslate}]{.pre}` `{.docutils .literal
.notranslate}[`"text"`{.docutils .literal .notranslate}]{.pre} we will
see the quote-text we clicked on. The Inspector lets you copy XPaths to
selected elements. Let's try it out.

First open the Scrapy shell at
[https://quotes.toscrape.com/](https://quotes.toscrape.com/){.reference
.external} in a terminal:

::: {.highlight-none .notranslate}
::: highlight
    $ scrapy shell "https://quotes.toscrape.com/"
:::
:::

Then, back to your web browser, right-click on the [`span`{.docutils
.literal .notranslate}]{.pre} tag, select [`Copy`{.docutils .literal
.notranslate}]{.pre}` `{.docutils .literal .notranslate}[`>`{.docutils
.literal .notranslate}]{.pre}` `{.docutils .literal
.notranslate}[`XPath`{.docutils .literal .notranslate}]{.pre} and paste
it in the Scrapy shell like so:

::: {.highlight-pycon .notranslate}
::: highlight
    >>> response.xpath("/html/body/div/div[2]/div[1]/div[1]/span[1]/text()").getall()
    ['“The world as we have created it is a process of our thinking. It cannot be changed without changing our thinking.”']
:::
:::

Adding [`text()`{.docutils .literal .notranslate}]{.pre} at the end we
are able to extract the first quote with this basic selector. But this
XPath is not really that clever. All it does is go down a desired path
in the source code starting from [`html`{.docutils .literal
.notranslate}]{.pre}. So let's see if we can refine our XPath a bit:

If we check the Inspector again we'll see that directly beneath our
expanded [`div`{.docutils .literal .notranslate}]{.pre} tag we have nine
identical [`div`{.docutils .literal .notranslate}]{.pre} tags, each with
the same attributes as our first. If we expand any of them, we'll see
the same structure as with our first quote: Two [`span`{.docutils
.literal .notranslate}]{.pre} tags and one [`div`{.docutils .literal
.notranslate}]{.pre} tag. We can expand each [`span`{.docutils .literal
.notranslate}]{.pre} tag with the [`class="text"`{.docutils .literal
.notranslate}]{.pre} inside our [`div`{.docutils .literal
.notranslate}]{.pre} tags and see each quote:

::: {.highlight-html .notranslate}
::: highlight
    <div class="quote" itemscope="" itemtype="http://schema.org/CreativeWork">
      <span class="text" itemprop="text">
        “The world as we have created it is a process of our thinking. It cannot be changed without changing our thinking.”
      </span>
      <span>(...)</span>
      <div class="tags">(...)</div>
    </div>
:::
:::

With this knowledge we can refine our XPath: Instead of a path to
follow, we'll simply select all [`span`{.docutils .literal
.notranslate}]{.pre} tags with the [`class="text"`{.docutils .literal
.notranslate}]{.pre} by using the
[has-class-extension](https://parsel.readthedocs.io/en/latest/usage.html#other-xpath-extensions){.reference
.external}:

::: {.highlight-pycon .notranslate}
::: highlight
    >>> response.xpath('//span[has-class("text")]/text()').getall()
    ['“The world as we have created it is a process of our thinking. It cannot be changed without changing our thinking.”',
    '“It is our choices, Harry, that show what we truly are, far more than our abilities.”',
    '“There are only two ways to live your life. One is as though nothing is a miracle. The other is as though everything is a miracle.”',
    ...]
:::
:::

And with one simple, cleverer XPath we are able to extract all quotes
from the page. We could have constructed a loop over our first XPath to
increase the number of the last [`div`{.docutils .literal
.notranslate}]{.pre}, but this would have been unnecessarily complex and
by simply constructing an XPath with [`has-class("text")`{.docutils
.literal .notranslate}]{.pre} we were able to extract all quotes in one
line.

The Inspector has a lot of other helpful features, such as searching in
the source code or directly scrolling to an element you selected. Let's
demonstrate a use case:

Say you want to find the [`Next`{.docutils .literal .notranslate}]{.pre}
button on the page. Type [`Next`{.docutils .literal .notranslate}]{.pre}
into the search bar on the top right of the Inspector. You should get
two results. The first is a [`li`{.docutils .literal
.notranslate}]{.pre} tag with the [`class="next"`{.docutils .literal
.notranslate}]{.pre}, the second the text of an [`a`{.docutils .literal
.notranslate}]{.pre} tag. Right click on the [`a`{.docutils .literal
.notranslate}]{.pre} tag and select [`Scroll`{.docutils .literal
.notranslate}]{.pre}` `{.docutils .literal
.notranslate}[`into`{.docutils .literal
.notranslate}]{.pre}` `{.docutils .literal
.notranslate}[`View`{.docutils .literal .notranslate}]{.pre}. If you
hover over the tag, you'll see the button highlighted. From here we
could easily create a [[Link Extractor]{.std
.std-ref}](index.html#topics-link-extractors){.hoverxref .tooltip
.reference .internal} to follow the pagination. On a simple site such as
this, there may not be the need to find an element visually but the
[`Scroll`{.docutils .literal .notranslate}]{.pre}` `{.docutils .literal
.notranslate}[`into`{.docutils .literal
.notranslate}]{.pre}` `{.docutils .literal
.notranslate}[`View`{.docutils .literal .notranslate}]{.pre} function
can be quite useful on complex sites.

Note that the search bar can also be used to search for and test CSS
selectors. For example, you could search for [`span.text`{.docutils
.literal .notranslate}]{.pre} to find all quote texts. Instead of a full
text search, this searches for exactly the [`span`{.docutils .literal
.notranslate}]{.pre} tag with the [`class="text"`{.docutils .literal
.notranslate}]{.pre} in the page.
:::

::: {#the-network-tool .section}
[]{#topics-network-tool}

#### The Network-tool[¶](#the-network-tool "Permalink to this heading"){.headerlink}

While scraping you may come across dynamic webpages where some parts of
the page are loaded dynamically through multiple requests. While this
can be quite tricky, the Network-tool in the Developer Tools greatly
facilitates this task. To demonstrate the Network-tool, let's take a
look at the page
[quotes.toscrape.com/scroll](https://quotes.toscrape.com/scroll){.reference
.external}.

The page is quite similar to the basic
[quotes.toscrape.com](https://quotes.toscrape.com){.reference
.external}-page, but instead of the above-mentioned [`Next`{.docutils
.literal .notranslate}]{.pre} button, the page automatically loads new
quotes when you scroll to the bottom. We could go ahead and try out
different XPaths directly, but instead we'll check another quite useful
command from the Scrapy shell:

::: {.highlight-none .notranslate}
::: highlight
    $ scrapy shell "quotes.toscrape.com/scroll"
    (...)
    >>> view(response)
:::
:::

A browser window should open with the webpage but with one crucial
difference: Instead of the quotes we just see a greenish bar with the
word [`Loading...`{.docutils .literal .notranslate}]{.pre}.

[![Response from
quotes.toscrape.com/scroll](_images/network_01.png){style="width: 777px; height: 296px;"}](_images/network_01.png){.reference
.internal .image-reference}

The [`view(response)`{.docutils .literal .notranslate}]{.pre} command
let's us view the response our shell or later our spider receives from
the server. Here we see that some basic template is loaded which
includes the title, the login-button and the footer, but the quotes are
missing. This tells us that the quotes are being loaded from a different
request than [`quotes.toscrape/scroll`{.docutils .literal
.notranslate}]{.pre}.

If you click on the [`Network`{.docutils .literal .notranslate}]{.pre}
tab, you will probably only see two entries. The first thing we do is
enable persistent logs by clicking on [`Persist`{.docutils .literal
.notranslate}]{.pre}` `{.docutils .literal
.notranslate}[`Logs`{.docutils .literal .notranslate}]{.pre}. If this
option is disabled, the log is automatically cleared each time you
navigate to a different page. Enabling this option is a good default,
since it gives us control on when to clear the logs.

If we reload the page now, you'll see the log get populated with six new
requests.

[![Network tab with persistent logs and
requests](_images/network_02.png){style="width: 777px; height: 241px;"}](_images/network_02.png){.reference
.internal .image-reference}

Here we see every request that has been made when reloading the page and
can inspect each request and its response. So let's find out where our
quotes are coming from:

First click on the request with the name [`scroll`{.docutils .literal
.notranslate}]{.pre}. On the right you can now inspect the request. In
[`Headers`{.docutils .literal .notranslate}]{.pre} you'll find details
about the request headers, such as the URL, the method, the IP-address,
and so on. We'll ignore the other tabs and click directly on
[`Response`{.docutils .literal .notranslate}]{.pre}.

What you should see in the [`Preview`{.docutils .literal
.notranslate}]{.pre} pane is the rendered HTML-code, that is exactly
what we saw when we called [`view(response)`{.docutils .literal
.notranslate}]{.pre} in the shell. Accordingly the [`type`{.docutils
.literal .notranslate}]{.pre} of the request in the log is
[`html`{.docutils .literal .notranslate}]{.pre}. The other requests have
types like [`css`{.docutils .literal .notranslate}]{.pre} or
[`js`{.docutils .literal .notranslate}]{.pre}, but what interests us is
the one request called [`quotes?page=1`{.docutils .literal
.notranslate}]{.pre} with the type [`json`{.docutils .literal
.notranslate}]{.pre}.

If we click on this request, we see that the request URL is
[`https://quotes.toscrape.com/api/quotes?page=1`{.docutils .literal
.notranslate}]{.pre} and the response is a JSON-object that contains our
quotes. We can also right-click on the request and open
[`Open`{.docutils .literal .notranslate}]{.pre}` `{.docutils .literal
.notranslate}[`in`{.docutils .literal .notranslate}]{.pre}` `{.docutils
.literal .notranslate}[`new`{.docutils .literal
.notranslate}]{.pre}` `{.docutils .literal .notranslate}[`tab`{.docutils
.literal .notranslate}]{.pre} to get a better overview.

[![JSON-object returned from the quotes.toscrape
API](_images/network_03.png){style="width: 777px; height: 375px;"}](_images/network_03.png){.reference
.internal .image-reference}

With this response we can now easily parse the JSON-object and also
request each page to get every quote on the site:

::: {.highlight-python .notranslate}
::: highlight
    import scrapy
    import json


    class QuoteSpider(scrapy.Spider):
        name = "quote"
        allowed_domains = ["quotes.toscrape.com"]
        page = 1
        start_urls = ["https://quotes.toscrape.com/api/quotes?page=1"]

        def parse(self, response):
            data = json.loads(response.text)
            for quote in data["quotes"]:
                yield {"quote": quote["text"]}
            if data["has_next"]:
                self.page += 1
                url = f"https://quotes.toscrape.com/api/quotes?page={self.page}"
                yield scrapy.Request(url=url, callback=self.parse)
:::
:::

This spider starts at the first page of the quotes-API. With each
response, we parse the [`response.text`{.docutils .literal
.notranslate}]{.pre} and assign it to [`data`{.docutils .literal
.notranslate}]{.pre}. This lets us operate on the JSON-object like on a
Python dictionary. We iterate through the [`quotes`{.docutils .literal
.notranslate}]{.pre} and print out the [`quote["text"]`{.docutils
.literal .notranslate}]{.pre}. If the handy [`has_next`{.docutils
.literal .notranslate}]{.pre} element is [`true`{.docutils .literal
.notranslate}]{.pre} (try loading
[quotes.toscrape.com/api/quotes?page=10](https://quotes.toscrape.com/api/quotes?page=10){.reference
.external} in your browser or a page-number greater than 10), we
increment the [`page`{.docutils .literal .notranslate}]{.pre} attribute
and [`yield`{.docutils .literal .notranslate}]{.pre} a new request,
inserting the incremented page-number into our [`url`{.docutils .literal
.notranslate}]{.pre}.

In more complex websites, it could be difficult to easily reproduce the
requests, as we could need to add [`headers`{.docutils .literal
.notranslate}]{.pre} or [`cookies`{.docutils .literal
.notranslate}]{.pre} to make it work. In those cases you can export the
requests in [cURL](https://curl.haxx.se/){.reference .external} format,
by right-clicking on each of them in the network tool and using the
[`from_curl()`{.xref .py .py-meth .docutils .literal
.notranslate}]{.pre} method to generate an equivalent request:

::: {.highlight-python .notranslate}
::: highlight
    from scrapy import Request

    request = Request.from_curl(
        "curl 'https://quotes.toscrape.com/api/quotes?page=1' -H 'User-Agent: Mozil"
        "la/5.0 (X11; Linux x86_64; rv:67.0) Gecko/20100101 Firefox/67.0' -H 'Acce"
        "pt: */*' -H 'Accept-Language: ca,en-US;q=0.7,en;q=0.3' --compressed -H 'X"
        "-Requested-With: XMLHttpRequest' -H 'Proxy-Authorization: Basic QFRLLTAzM"
        "zEwZTAxLTk5MWUtNDFiNC1iZWRmLTJjNGI4M2ZiNDBmNDpAVEstMDMzMTBlMDEtOTkxZS00MW"
        "I0LWJlZGYtMmM0YjgzZmI0MGY0' -H 'Connection: keep-alive' -H 'Referer: http"
        "://quotes.toscrape.com/scroll' -H 'Cache-Control: max-age=0'"
    )
:::
:::

Alternatively, if you want to know the arguments needed to recreate that
request you can use the [[`curl_to_request_kwargs()`{.xref .py .py-func
.docutils .literal
.notranslate}]{.pre}](#scrapy.utils.curl.curl_to_request_kwargs "scrapy.utils.curl.curl_to_request_kwargs"){.reference
.internal} function to get a dictionary with the equivalent arguments:

[[scrapy.utils.curl.]{.pre}]{.sig-prename .descclassname}[[curl_to_request_kwargs]{.pre}]{.sig-name .descname}[(]{.sig-paren}*[[curl_command]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[[[str]{.pre}](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.12)"){.reference .external}]{.n}*, *[[ignore_unknown_options]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[[[bool]{.pre}](https://docs.python.org/3/library/functions.html#bool "(in Python v3.12)"){.reference .external}]{.n}[ ]{.w}[[=]{.pre}]{.o}[ ]{.w}[[True]{.pre}]{.default_value}*[)]{.sig-paren} [[→]{.sig-return-icon} [[[dict]{.pre}](https://docs.python.org/3/library/stdtypes.html#dict "(in Python v3.12)"){.reference .external}]{.sig-return-typehint}]{.sig-return}[[[\[source\]]{.pre}]{.viewcode-link}](_modules/scrapy/utils/curl.html#curl_to_request_kwargs){.reference .internal}[¶](#scrapy.utils.curl.curl_to_request_kwargs "Permalink to this definition"){.headerlink}

:   Convert a cURL command syntax to Request kwargs.

    Parameters

    :   -   **curl_command**
            ([*str*](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.12)"){.reference
            .external}) -- string containing the curl command

        -   **ignore_unknown_options**
            ([*bool*](https://docs.python.org/3/library/functions.html#bool "(in Python v3.12)"){.reference
            .external}) -- If true, only a warning is emitted when cURL
            options are unknown. Otherwise raises an error. (default:
            True)

    Returns

    :   dictionary of Request kwargs

Note that to translate a cURL command into a Scrapy request, you may use
[curl2scrapy](https://michael-shub.github.io/curl2scrapy/){.reference
.external}.

As you can see, with a few inspections in the Network-tool we were able
to easily replicate the dynamic requests of the scrolling functionality
of the page. Crawling dynamic pages can be quite daunting and pages can
be very complex, but it (mostly) boils down to identifying the correct
request and replicating it in your spider.
:::
:::

[]{#document-topics/dynamic-content}

::: {#selecting-dynamically-loaded-content .section}
[]{#topics-dynamic-content}

### Selecting dynamically-loaded content[¶](#selecting-dynamically-loaded-content "Permalink to this heading"){.headerlink}

Some webpages show the desired data when you load them in a web browser.
However, when you download them using Scrapy, you cannot reach the
desired data using [[selectors]{.std
.std-ref}](index.html#topics-selectors){.hoverxref .tooltip .reference
.internal}.

When this happens, the recommended approach is to [[find the data
source]{.std .std-ref}](#topics-finding-data-source){.hoverxref .tooltip
.reference .internal} and extract the data from it.

If you fail to do that, and you can nonetheless access the desired data
through the [[DOM]{.std .std-ref}](index.html#topics-livedom){.hoverxref
.tooltip .reference .internal} from your web browser, see
[[Pre-rendering JavaScript]{.std
.std-ref}](#topics-javascript-rendering){.hoverxref .tooltip .reference
.internal}.

::: {#finding-the-data-source .section}
[]{#topics-finding-data-source}

#### Finding the data source[¶](#finding-the-data-source "Permalink to this heading"){.headerlink}

To extract the desired data, you must first find its source location.

If the data is in a non-text-based format, such as an image or a PDF
document, use the [[network tool]{.std
.std-ref}](index.html#topics-network-tool){.hoverxref .tooltip
.reference .internal} of your web browser to find the corresponding
request, and [[reproduce it]{.std
.std-ref}](#topics-reproducing-requests){.hoverxref .tooltip .reference
.internal}.

If your web browser lets you select the desired data as text, the data
may be defined in embedded JavaScript code, or loaded from an external
resource in a text-based format.

In that case, you can use a tool like
[wgrep](https://github.com/stav/wgrep){.reference .external} to find the
URL of that resource.

If the data turns out to come from the original URL itself, you must
[[inspect the source code of the webpage]{.std
.std-ref}](#topics-inspecting-source){.hoverxref .tooltip .reference
.internal} to determine where the data is located.

If the data comes from a different URL, you will need to [[reproduce the
corresponding request]{.std
.std-ref}](#topics-reproducing-requests){.hoverxref .tooltip .reference
.internal}.
:::

::: {#inspecting-the-source-code-of-a-webpage .section}
[]{#topics-inspecting-source}

#### Inspecting the source code of a webpage[¶](#inspecting-the-source-code-of-a-webpage "Permalink to this heading"){.headerlink}

Sometimes you need to inspect the source code of a webpage (not the
[[DOM]{.std .std-ref}](index.html#topics-livedom){.hoverxref .tooltip
.reference .internal}) to determine where some desired data is located.

Use Scrapy's [[`fetch`{.xref .std .std-command .docutils .literal
.notranslate}]{.pre}](index.html#std-command-fetch){.hoverxref .tooltip
.reference .internal} command to download the webpage contents as seen
by Scrapy:

::: {.highlight-default .notranslate}
::: highlight
    scrapy fetch --nolog https://example.com > response.html
:::
:::

If the desired data is in embedded JavaScript code within a
[`<script/>`{.docutils .literal .notranslate}]{.pre} element, see
[[Parsing JavaScript code]{.std
.std-ref}](#topics-parsing-javascript){.hoverxref .tooltip .reference
.internal}.

If you cannot find the desired data, first make sure it's not just
Scrapy: download the webpage with an HTTP client like
[curl](https://curl.haxx.se/){.reference .external} or
[wget](https://www.gnu.org/software/wget/){.reference .external} and see
if the information can be found in the response they get.

If they get a response with the desired data, modify your Scrapy
[`Request`{.xref .py .py-class .docutils .literal .notranslate}]{.pre}
to match that of the other HTTP client. For example, try using the same
user-agent string ([[`USER_AGENT`{.xref .std .std-setting .docutils
.literal
.notranslate}]{.pre}](index.html#std-setting-USER_AGENT){.hoverxref
.tooltip .reference .internal}) or the same [`headers`{.xref .py
.py-attr .docutils .literal .notranslate}]{.pre}.

If they also get a response without the desired data, you'll need to
take steps to make your request more similar to that of the web browser.
See [[Reproducing requests]{.std
.std-ref}](#topics-reproducing-requests){.hoverxref .tooltip .reference
.internal}.
:::

::: {#reproducing-requests .section}
[]{#topics-reproducing-requests}

#### Reproducing requests[¶](#reproducing-requests "Permalink to this heading"){.headerlink}

Sometimes we need to reproduce a request the way our web browser
performs it.

Use the [[network tool]{.std
.std-ref}](index.html#topics-network-tool){.hoverxref .tooltip
.reference .internal} of your web browser to see how your web browser
performs the desired request, and try to reproduce that request with
Scrapy.

It might be enough to yield a [`Request`{.xref .py .py-class .docutils
.literal .notranslate}]{.pre} with the same HTTP method and URL.
However, you may also need to reproduce the body, headers and form
parameters (see [`FormRequest`{.xref .py .py-class .docutils .literal
.notranslate}]{.pre}) of that request.

As all major browsers allow to export the requests in
[cURL](https://curl.haxx.se/){.reference .external} format, Scrapy
incorporates the method [`from_curl()`{.xref .py .py-meth .docutils
.literal .notranslate}]{.pre} to generate an equivalent [`Request`{.xref
.py .py-class .docutils .literal .notranslate}]{.pre} from a cURL
command. To get more information visit [[request from curl]{.std
.std-ref}](index.html#requests-from-curl){.hoverxref .tooltip .reference
.internal} inside the network tool section.

Once you get the expected response, you can [[extract the desired data
from it]{.std .std-ref}](#topics-handling-response-formats){.hoverxref
.tooltip .reference .internal}.

You can reproduce any request with Scrapy. However, some times
reproducing all necessary requests may not seem efficient in developer
time. If that is your case, and crawling speed is not a major concern
for you, you can alternatively consider [[JavaScript pre-rendering]{.std
.std-ref}](#topics-javascript-rendering){.hoverxref .tooltip .reference
.internal}.

If you get the expected response sometimes, but not always, the issue is
probably not your request, but the target server. The target server
might be buggy, overloaded, or [[banning]{.std
.std-ref}](index.html#bans){.hoverxref .tooltip .reference .internal}
some of your requests.

Note that to translate a cURL command into a Scrapy request, you may use
[curl2scrapy](https://michael-shub.github.io/curl2scrapy/){.reference
.external}.
:::

::: {#handling-different-response-formats .section}
[]{#topics-handling-response-formats}

#### Handling different response formats[¶](#handling-different-response-formats "Permalink to this heading"){.headerlink}

Once you have a response with the desired data, how you extract the
desired data from it depends on the type of response:

-   If the response is HTML or XML, use [[selectors]{.std
    .std-ref}](index.html#topics-selectors){.hoverxref .tooltip
    .reference .internal} as usual.

-   If the response is JSON, use [[`json.loads()`{.xref .py .py-func
    .docutils .literal
    .notranslate}]{.pre}](https://docs.python.org/3/library/json.html#json.loads "(in Python v3.12)"){.reference
    .external} to load the desired data from [[`response.text`{.xref .py
    .py-attr .docutils .literal
    .notranslate}]{.pre}](index.html#scrapy.http.TextResponse.text "scrapy.http.TextResponse.text"){.reference
    .internal}:

    ::: {.highlight-python .notranslate}
    ::: highlight
        data = json.loads(response.text)
    :::
    :::

    If the desired data is inside HTML or XML code embedded within JSON
    data, you can load that HTML or XML code into a [`Selector`{.xref
    .py .py-class .docutils .literal .notranslate}]{.pre} and then [[use
    it]{.std .std-ref}](index.html#topics-selectors){.hoverxref .tooltip
    .reference .internal} as usual:

    ::: {.highlight-python .notranslate}
    ::: highlight
        selector = Selector(data["html"])
    :::
    :::

-   If the response is JavaScript, or HTML with a [`<script/>`{.docutils
    .literal .notranslate}]{.pre} element containing the desired data,
    see [[Parsing JavaScript code]{.std
    .std-ref}](#topics-parsing-javascript){.hoverxref .tooltip
    .reference .internal}.

-   If the response is CSS, use a [[regular expression]{.xref .std
    .std-doc}](https://docs.python.org/3/library/re.html "(in Python v3.12)"){.reference
    .external} to extract the desired data from [[`response.text`{.xref
    .py .py-attr .docutils .literal
    .notranslate}]{.pre}](index.html#scrapy.http.TextResponse.text "scrapy.http.TextResponse.text"){.reference
    .internal}.

```{=html}
<!-- -->
```
-   If the response is an image or another format based on images (e.g.
    PDF), read the response as bytes from [`response.body`{.xref .py
    .py-attr .docutils .literal .notranslate}]{.pre} and use an OCR
    solution to extract the desired data as text.

    For example, you can use
    [pytesseract](https://github.com/madmaze/pytesseract){.reference
    .external}. To read a table from a PDF,
    [tabula-py](https://github.com/chezou/tabula-py){.reference
    .external} may be a better choice.

-   If the response is SVG, or HTML with embedded SVG containing the
    desired data, you may be able to extract the desired data using
    [[selectors]{.std .std-ref}](index.html#topics-selectors){.hoverxref
    .tooltip .reference .internal}, since SVG is based on XML.

    Otherwise, you might need to convert the SVG code into a raster
    image, and [[handle that raster image]{.std
    .std-ref}](#topics-parsing-images){.hoverxref .tooltip .reference
    .internal}.
:::

::: {#parsing-javascript-code .section}
[]{#topics-parsing-javascript}

#### Parsing JavaScript code[¶](#parsing-javascript-code "Permalink to this heading"){.headerlink}

If the desired data is hardcoded in JavaScript, you first need to get
the JavaScript code:

-   If the JavaScript code is in a JavaScript file, simply read
    [[`response.text`{.xref .py .py-attr .docutils .literal
    .notranslate}]{.pre}](index.html#scrapy.http.TextResponse.text "scrapy.http.TextResponse.text"){.reference
    .internal}.

-   If the JavaScript code is within a [`<script/>`{.docutils .literal
    .notranslate}]{.pre} element of an HTML page, use [[selectors]{.std
    .std-ref}](index.html#topics-selectors){.hoverxref .tooltip
    .reference .internal} to extract the text within that
    [`<script/>`{.docutils .literal .notranslate}]{.pre} element.

Once you have a string with the JavaScript code, you can extract the
desired data from it:

-   You might be able to use a [[regular expression]{.xref .std
    .std-doc}](https://docs.python.org/3/library/re.html "(in Python v3.12)"){.reference
    .external} to extract the desired data in JSON format, which you can
    then parse with [[`json.loads()`{.xref .py .py-func .docutils
    .literal
    .notranslate}]{.pre}](https://docs.python.org/3/library/json.html#json.loads "(in Python v3.12)"){.reference
    .external}.

    For example, if the JavaScript code contains a separate line like
    [`var`{.docutils .literal .notranslate}]{.pre}` `{.docutils .literal
    .notranslate}[`data`{.docutils .literal
    .notranslate}]{.pre}` `{.docutils .literal
    .notranslate}[`=`{.docutils .literal
    .notranslate}]{.pre}` `{.docutils .literal
    .notranslate}[`{"field":`{.docutils .literal
    .notranslate}]{.pre}` `{.docutils .literal
    .notranslate}[`"value"};`{.docutils .literal .notranslate}]{.pre}
    you can extract that data as follows:

    ::: {.highlight-pycon .notranslate}
    ::: highlight
        >>> pattern = r"\bvar\s+data\s*=\s*(\{.*?\})\s*;\s*\n"
        >>> json_data = response.css("script::text").re_first(pattern)
        >>> json.loads(json_data)
        {'field': 'value'}
    :::
    :::

-   [chompjs](https://github.com/Nykakin/chompjs){.reference .external}
    provides an API to parse JavaScript objects into a [[`dict`{.xref
    .py .py-class .docutils .literal
    .notranslate}]{.pre}](https://docs.python.org/3/library/stdtypes.html#dict "(in Python v3.12)"){.reference
    .external}.

    For example, if the JavaScript code contains [`var`{.docutils
    .literal .notranslate}]{.pre}` `{.docutils .literal
    .notranslate}[`data`{.docutils .literal
    .notranslate}]{.pre}` `{.docutils .literal
    .notranslate}[`=`{.docutils .literal
    .notranslate}]{.pre}` `{.docutils .literal
    .notranslate}[`{field:`{.docutils .literal
    .notranslate}]{.pre}` `{.docutils .literal
    .notranslate}[`"value",`{.docutils .literal
    .notranslate}]{.pre}` `{.docutils .literal
    .notranslate}[`secondField:`{.docutils .literal
    .notranslate}]{.pre}` `{.docutils .literal
    .notranslate}[`"second`{.docutils .literal
    .notranslate}]{.pre}` `{.docutils .literal
    .notranslate}[`value"};`{.docutils .literal .notranslate}]{.pre} you
    can extract that data as follows:

    ::: {.highlight-pycon .notranslate}
    ::: highlight
        >>> import chompjs
        >>> javascript = response.css("script::text").get()
        >>> data = chompjs.parse_js_object(javascript)
        >>> data
        {'field': 'value', 'secondField': 'second value'}
    :::
    :::

-   Otherwise, use
    [js2xml](https://github.com/scrapinghub/js2xml){.reference
    .external} to convert the JavaScript code into an XML document that
    you can parse using [[selectors]{.std
    .std-ref}](index.html#topics-selectors){.hoverxref .tooltip
    .reference .internal}.

    For example, if the JavaScript code contains [`var`{.docutils
    .literal .notranslate}]{.pre}` `{.docutils .literal
    .notranslate}[`data`{.docutils .literal
    .notranslate}]{.pre}` `{.docutils .literal
    .notranslate}[`=`{.docutils .literal
    .notranslate}]{.pre}` `{.docutils .literal
    .notranslate}[`{field:`{.docutils .literal
    .notranslate}]{.pre}` `{.docutils .literal
    .notranslate}[`"value"};`{.docutils .literal .notranslate}]{.pre}
    you can extract that data as follows:

    ::: {.highlight-pycon .notranslate}
    ::: highlight
        >>> import js2xml
        >>> import lxml.etree
        >>> from parsel import Selector
        >>> javascript = response.css("script::text").get()
        >>> xml = lxml.etree.tostring(js2xml.parse(javascript), encoding="unicode")
        >>> selector = Selector(text=xml)
        >>> selector.css('var[name="data"]').get()
        '<var name="data"><object><property name="field"><string>value</string></property></object></var>'
    :::
    :::
:::

::: {#pre-rendering-javascript .section}
[]{#topics-javascript-rendering}

#### Pre-rendering JavaScript[¶](#pre-rendering-javascript "Permalink to this heading"){.headerlink}

On webpages that fetch data from additional requests, reproducing those
requests that contain the desired data is the preferred approach. The
effort is often worth the result: structured, complete data with minimum
parsing time and network transfer.

However, sometimes it can be really hard to reproduce certain requests.
Or you may need something that no request can give you, such as a
screenshot of a webpage as seen in a web browser.

In these cases use the
[Splash](https://github.com/scrapinghub/splash){.reference .external}
JavaScript-rendering service, along with
[scrapy-splash](https://github.com/scrapy-plugins/scrapy-splash){.reference
.external} for seamless integration.

Splash returns as HTML the [[DOM]{.std
.std-ref}](index.html#topics-livedom){.hoverxref .tooltip .reference
.internal} of a webpage, so that you can parse it with [[selectors]{.std
.std-ref}](index.html#topics-selectors){.hoverxref .tooltip .reference
.internal}. It provides great flexibility through
[configuration](https://splash.readthedocs.io/en/stable/api.html){.reference
.external} or
[scripting](https://splash.readthedocs.io/en/stable/scripting-tutorial.html){.reference
.external}.

If you need something beyond what Splash offers, such as interacting
with the DOM on-the-fly from Python code instead of using a
previously-written script, or handling multiple web browser windows, you
might need to [[use a headless browser]{.std
.std-ref}](#topics-headless-browsing){.hoverxref .tooltip .reference
.internal} instead.
:::

::: {#using-a-headless-browser .section}
[]{#topics-headless-browsing}

#### Using a headless browser[¶](#using-a-headless-browser "Permalink to this heading"){.headerlink}

A [headless
browser](https://en.wikipedia.org/wiki/Headless_browser){.reference
.external} is a special web browser that provides an API for automation.
By installing the [[asyncio reactor]{.std
.std-ref}](index.html#install-asyncio){.hoverxref .tooltip .reference
.internal}, it is possible to integrate [`asyncio`{.docutils .literal
.notranslate}]{.pre}-based libraries which handle headless browsers.

One such library is
[playwright-python](https://github.com/microsoft/playwright-python){.reference
.external} (an official Python port of
[playwright](https://github.com/microsoft/playwright){.reference
.external}). The following is a simple snippet to illustrate its usage
within a Scrapy spider:

::: {.highlight-python .notranslate}
::: highlight
    import scrapy
    from playwright.async_api import async_playwright


    class PlaywrightSpider(scrapy.Spider):
        name = "playwright"
        start_urls = ["data:,"]  # avoid using the default Scrapy downloader

        async def parse(self, response):
            async with async_playwright() as pw:
                browser = await pw.chromium.launch()
                page = await browser.new_page()
                await page.goto("https://example.org")
                title = await page.title()
                return {"title": title}
:::
:::

However, using
[playwright-python](https://github.com/microsoft/playwright-python){.reference
.external} directly as in the above example circumvents most of the
Scrapy components (middlewares, dupefilter, etc). We recommend using
[scrapy-playwright](https://github.com/scrapy-plugins/scrapy-playwright){.reference
.external} for a better integration.
:::
:::

[]{#document-topics/leaks}

::: {#debugging-memory-leaks .section}
[]{#topics-leaks}

### Debugging memory leaks[¶](#debugging-memory-leaks "Permalink to this heading"){.headerlink}

In Scrapy, objects such as requests, responses and items have a finite
lifetime: they are created, used for a while, and finally destroyed.

From all those objects, the Request is probably the one with the longest
lifetime, as it stays waiting in the Scheduler queue until it's time to
process it. For more info see [[Architecture overview]{.std
.std-ref}](index.html#topics-architecture){.hoverxref .tooltip
.reference .internal}.

As these Scrapy objects have a (rather long) lifetime, there is always
the risk of accumulating them in memory without releasing them properly
and thus causing what is known as a "memory leak".

To help debugging memory leaks, Scrapy provides a built-in mechanism for
tracking objects references called [[trackref]{.std
.std-ref}](#topics-leaks-trackrefs){.hoverxref .tooltip .reference
.internal}, and you can also use a third-party library called
[[muppy]{.std .std-ref}](#topics-leaks-muppy){.hoverxref .tooltip
.reference .internal} for more advanced memory debugging (see below for
more info). Both mechanisms must be used from the [[Telnet Console]{.std
.std-ref}](index.html#topics-telnetconsole){.hoverxref .tooltip
.reference .internal}.

::: {#common-causes-of-memory-leaks .section}
#### Common causes of memory leaks[¶](#common-causes-of-memory-leaks "Permalink to this heading"){.headerlink}

It happens quite often (sometimes by accident, sometimes on purpose)
that the Scrapy developer passes objects referenced in Requests (for
example, using the [`cb_kwargs`{.xref .py .py-attr .docutils .literal
.notranslate}]{.pre} or [`meta`{.xref .py .py-attr .docutils .literal
.notranslate}]{.pre} attributes or the request callback function) and
that effectively bounds the lifetime of those referenced objects to the
lifetime of the Request. This is, by far, the most common cause of
memory leaks in Scrapy projects, and a quite difficult one to debug for
newcomers.

In big projects, the spiders are typically written by different people
and some of those spiders could be "leaking" and thus affecting the rest
of the other (well-written) spiders when they get to run concurrently,
which, in turn, affects the whole crawling process.

The leak could also come from a custom middleware, pipeline or extension
that you have written, if you are not releasing the (previously
allocated) resources properly. For example, allocating resources on
[[`spider_opened`{.xref .std .std-signal .docutils .literal
.notranslate}]{.pre}](index.html#std-signal-spider_opened){.hoverxref
.tooltip .reference .internal} but not releasing them on
[[`spider_closed`{.xref .std .std-signal .docutils .literal
.notranslate}]{.pre}](index.html#std-signal-spider_closed){.hoverxref
.tooltip .reference .internal} may cause problems if you're running
[[multiple spiders per process]{.std
.std-ref}](index.html#run-multiple-spiders){.hoverxref .tooltip
.reference .internal}.

::: {#too-many-requests .section}
##### Too Many Requests?[¶](#too-many-requests "Permalink to this heading"){.headerlink}

By default Scrapy keeps the request queue in memory; it includes
[`Request`{.xref .py .py-class .docutils .literal .notranslate}]{.pre}
objects and all objects referenced in Request attributes (e.g. in
[`cb_kwargs`{.xref .py .py-attr .docutils .literal .notranslate}]{.pre}
and [`meta`{.xref .py .py-attr .docutils .literal .notranslate}]{.pre}).
While not necessarily a leak, this can take a lot of memory. Enabling
[[persistent job queue]{.std
.std-ref}](index.html#topics-jobs){.hoverxref .tooltip .reference
.internal} could help keeping memory usage in control.
:::
:::

::: {#debugging-memory-leaks-with-trackref .section}
[]{#topics-leaks-trackrefs}

#### Debugging memory leaks with [`trackref`{.docutils .literal .notranslate}]{.pre}[¶](#debugging-memory-leaks-with-trackref "Permalink to this heading"){.headerlink}

[`trackref`{.xref .py .py-mod .docutils .literal .notranslate}]{.pre} is
a module provided by Scrapy to debug the most common cases of memory
leaks. It basically tracks the references to all live Request, Response,
Item, Spider and Selector objects.

You can enter the telnet console and inspect how many objects (of the
classes mentioned above) are currently alive using the
[`prefs()`{.docutils .literal .notranslate}]{.pre} function which is an
alias to the [[`print_live_refs()`{.xref .py .py-func .docutils .literal
.notranslate}]{.pre}](#scrapy.utils.trackref.print_live_refs "scrapy.utils.trackref.print_live_refs"){.reference
.internal} function:

::: {.highlight-default .notranslate}
::: highlight
    telnet localhost 6023

    .. code-block:: pycon

        >>> prefs()
        Live References

        ExampleSpider                       1   oldest: 15s ago
        HtmlResponse                       10   oldest: 1s ago
        Selector                            2   oldest: 0s ago
        FormRequest                       878   oldest: 7s ago
:::
:::

As you can see, that report also shows the "age" of the oldest object in
each class. If you're running multiple spiders per process chances are
you can figure out which spider is leaking by looking at the oldest
request or response. You can get the oldest object of each class using
the [[`get_oldest()`{.xref .py .py-func .docutils .literal
.notranslate}]{.pre}](#scrapy.utils.trackref.get_oldest "scrapy.utils.trackref.get_oldest"){.reference
.internal} function (from the telnet console).

::: {#which-objects-are-tracked .section}
##### Which objects are tracked?[¶](#which-objects-are-tracked "Permalink to this heading"){.headerlink}

The objects tracked by [`trackrefs`{.docutils .literal
.notranslate}]{.pre} are all from these classes (and all its
subclasses):

-   [`scrapy.Request`{.xref .py .py-class .docutils .literal
    .notranslate}]{.pre}

-   [[`scrapy.http.Response`{.xref .py .py-class .docutils .literal
    .notranslate}]{.pre}](index.html#scrapy.http.Response "scrapy.http.Response"){.reference
    .internal}

-   [`scrapy.Item`{.xref .py .py-class .docutils .literal
    .notranslate}]{.pre}

-   [`scrapy.Selector`{.xref .py .py-class .docutils .literal
    .notranslate}]{.pre}

-   [[`scrapy.Spider`{.xref .py .py-class .docutils .literal
    .notranslate}]{.pre}](index.html#scrapy.Spider "scrapy.Spider"){.reference
    .internal}
:::

::: {#a-real-example .section}
##### A real example[¶](#a-real-example "Permalink to this heading"){.headerlink}

Let's see a concrete example of a hypothetical case of memory leaks.
Suppose we have some spider with a line similar to this one:

::: {.highlight-default .notranslate}
::: highlight
    return Request(f"http://www.somenastyspider.com/product.php?pid={product_id}",
                   callback=self.parse, cb_kwargs={'referer': response})
:::
:::

That line is passing a response reference inside a request which
effectively ties the response lifetime to the requests' one, and that
would definitely cause memory leaks.

Let's see how we can discover the cause (without knowing it a priori, of
course) by using the [`trackref`{.docutils .literal .notranslate}]{.pre}
tool.

After the crawler is running for a few minutes and we notice its memory
usage has grown a lot, we can enter its telnet console and check the
live references:

::: {.highlight-pycon .notranslate}
::: highlight
    >>> prefs()
    Live References

    SomenastySpider                     1   oldest: 15s ago
    HtmlResponse                     3890   oldest: 265s ago
    Selector                            2   oldest: 0s ago
    Request                          3878   oldest: 250s ago
:::
:::

The fact that there are so many live responses (and that they're so old)
is definitely suspicious, as responses should have a relatively short
lifetime compared to Requests. The number of responses is similar to the
number of requests, so it looks like they are tied in a some way. We can
now go and check the code of the spider to discover the nasty line that
is generating the leaks (passing response references inside requests).

Sometimes extra information about live objects can be helpful. Let's
check the oldest response:

::: {.highlight-pycon .notranslate}
::: highlight
    >>> from scrapy.utils.trackref import get_oldest
    >>> r = get_oldest("HtmlResponse")
    >>> r.url
    'http://www.somenastyspider.com/product.php?pid=123'
:::
:::

If you want to iterate over all objects, instead of getting the oldest
one, you can use the [[`scrapy.utils.trackref.iter_all()`{.xref .py
.py-func .docutils .literal
.notranslate}]{.pre}](#scrapy.utils.trackref.iter_all "scrapy.utils.trackref.iter_all"){.reference
.internal} function:

::: {.highlight-pycon .notranslate}
::: highlight
    >>> from scrapy.utils.trackref import iter_all
    >>> [r.url for r in iter_all("HtmlResponse")]
    ['http://www.somenastyspider.com/product.php?pid=123',
    'http://www.somenastyspider.com/product.php?pid=584',
    ...]
:::
:::
:::

::: {#too-many-spiders .section}
##### Too many spiders?[¶](#too-many-spiders "Permalink to this heading"){.headerlink}

If your project has too many spiders executed in parallel, the output of
[`prefs()`{.xref .py .py-func .docutils .literal .notranslate}]{.pre}
can be difficult to read. For this reason, that function has a
[`ignore`{.docutils .literal .notranslate}]{.pre} argument which can be
used to ignore a particular class (and all its subclasses). For example,
this won't show any live references to spiders:

::: {.highlight-pycon .notranslate}
::: highlight
    >>> from scrapy.spiders import Spider
    >>> prefs(ignore=Spider)
:::
:::

[]{#module-scrapy.utils.trackref .target}
:::

::: {#scrapy-utils-trackref-module .section}
##### scrapy.utils.trackref module[¶](#scrapy-utils-trackref-module "Permalink to this heading"){.headerlink}

Here are the functions available in the [[`trackref`{.xref .py .py-mod
.docutils .literal
.notranslate}]{.pre}](#module-scrapy.utils.trackref "scrapy.utils.trackref: Track references of live objects"){.reference
.internal} module.

*[class]{.pre}[ ]{.w}*[[scrapy.utils.trackref.]{.pre}]{.sig-prename .descclassname}[[object_ref]{.pre}]{.sig-name .descname}[[[\[source\]]{.pre}]{.viewcode-link}](_modules/scrapy/utils/trackref.html#object_ref){.reference .internal}[¶](#scrapy.utils.trackref.object_ref "Permalink to this definition"){.headerlink}

:   Inherit from this class if you want to track live instances with the
    [`trackref`{.docutils .literal .notranslate}]{.pre} module.

```{=html}
<!-- -->
```

[[scrapy.utils.trackref.]{.pre}]{.sig-prename .descclassname}[[print_live_refs]{.pre}]{.sig-name .descname}[(]{.sig-paren}*[[class_name]{.pre}]{.n}*, *[[ignore]{.pre}]{.n}[[=]{.pre}]{.o}[[NoneType]{.pre}]{.default_value}*[)]{.sig-paren}[[[\[source\]]{.pre}]{.viewcode-link}](_modules/scrapy/utils/trackref.html#print_live_refs){.reference .internal}[¶](#scrapy.utils.trackref.print_live_refs "Permalink to this definition"){.headerlink}

:   Print a report of live references, grouped by class name.

    Parameters

    :   **ignore**
        ([*type*](https://docs.python.org/3/library/functions.html#type "(in Python v3.12)"){.reference
        .external} *or*
        [*tuple*](https://docs.python.org/3/library/stdtypes.html#tuple "(in Python v3.12)"){.reference
        .external}) -- if given, all objects from the specified class
        (or tuple of classes) will be ignored.

```{=html}
<!-- -->
```

[[scrapy.utils.trackref.]{.pre}]{.sig-prename .descclassname}[[get_oldest]{.pre}]{.sig-name .descname}[(]{.sig-paren}*[[class_name]{.pre}]{.n}*[)]{.sig-paren}[[[\[source\]]{.pre}]{.viewcode-link}](_modules/scrapy/utils/trackref.html#get_oldest){.reference .internal}[¶](#scrapy.utils.trackref.get_oldest "Permalink to this definition"){.headerlink}

:   Return the oldest object alive with the given class name, or
    [`None`{.docutils .literal .notranslate}]{.pre} if none is found.
    Use [[`print_live_refs()`{.xref .py .py-func .docutils .literal
    .notranslate}]{.pre}](#scrapy.utils.trackref.print_live_refs "scrapy.utils.trackref.print_live_refs"){.reference
    .internal} first to get a list of all tracked live objects per class
    name.

```{=html}
<!-- -->
```

[[scrapy.utils.trackref.]{.pre}]{.sig-prename .descclassname}[[iter_all]{.pre}]{.sig-name .descname}[(]{.sig-paren}*[[class_name]{.pre}]{.n}*[)]{.sig-paren}[[[\[source\]]{.pre}]{.viewcode-link}](_modules/scrapy/utils/trackref.html#iter_all){.reference .internal}[¶](#scrapy.utils.trackref.iter_all "Permalink to this definition"){.headerlink}

:   Return an iterator over all objects alive with the given class name,
    or [`None`{.docutils .literal .notranslate}]{.pre} if none is found.
    Use [[`print_live_refs()`{.xref .py .py-func .docutils .literal
    .notranslate}]{.pre}](#scrapy.utils.trackref.print_live_refs "scrapy.utils.trackref.print_live_refs"){.reference
    .internal} first to get a list of all tracked live objects per class
    name.
:::
:::

::: {#debugging-memory-leaks-with-muppy .section}
[]{#topics-leaks-muppy}

#### Debugging memory leaks with muppy[¶](#debugging-memory-leaks-with-muppy "Permalink to this heading"){.headerlink}

[`trackref`{.docutils .literal .notranslate}]{.pre} provides a very
convenient mechanism for tracking down memory leaks, but it only keeps
track of the objects that are more likely to cause memory leaks.
However, there are other cases where the memory leaks could come from
other (more or less obscure) objects. If this is your case, and you
can't find your leaks using [`trackref`{.docutils .literal
.notranslate}]{.pre}, you still have another resource: the muppy
library.

You can use muppy from
[Pympler](https://pypi.org/project/Pympler/){.reference .external}.

If you use [`pip`{.docutils .literal .notranslate}]{.pre}, you can
install muppy with the following command:

::: {.highlight-default .notranslate}
::: highlight
    pip install Pympler
:::
:::

Here's an example to view all Python objects available in the heap using
muppy:

::: {.highlight-pycon .notranslate}
::: highlight
    >>> from pympler import muppy
    >>> all_objects = muppy.get_objects()
    >>> len(all_objects)
    28667
    >>> from pympler import summary
    >>> suml = summary.summarize(all_objects)
    >>> summary.print_(suml)
                                   types |   # objects |   total size
    ==================================== | =========== | ============
                             <class 'str |        9822 |      1.10 MB
                            <class 'dict |        1658 |    856.62 KB
                            <class 'type |         436 |    443.60 KB
                            <class 'code |        2974 |    419.56 KB
              <class '_io.BufferedWriter |           2 |    256.34 KB
                             <class 'set |         420 |    159.88 KB
              <class '_io.BufferedReader |           1 |    128.17 KB
              <class 'wrapper_descriptor |        1130 |     88.28 KB
                           <class 'tuple |        1304 |     86.57 KB
                         <class 'weakref |        1013 |     79.14 KB
      <class 'builtin_function_or_method |         958 |     67.36 KB
               <class 'method_descriptor |         865 |     60.82 KB
                     <class 'abc.ABCMeta |          62 |     59.96 KB
                            <class 'list |         446 |     58.52 KB
                             <class 'int |        1425 |     43.20 KB
:::
:::

For more info about muppy, refer to the [muppy
documentation](https://pythonhosted.org/Pympler/muppy.html){.reference
.external}.
:::

::: {#leaks-without-leaks .section}
[]{#topics-leaks-without-leaks}

#### Leaks without leaks[¶](#leaks-without-leaks "Permalink to this heading"){.headerlink}

Sometimes, you may notice that the memory usage of your Scrapy process
will only increase, but never decrease. Unfortunately, this could happen
even though neither Scrapy nor your project are leaking memory. This is
due to a (not so well) known problem of Python, which may not return
released memory to the operating system in some cases. For more
information on this issue see:

-   [Python Memory
    Management](https://www.evanjones.ca/python-memory.html){.reference
    .external}

-   [Python Memory Management Part
    2](https://www.evanjones.ca/python-memory-part2.html){.reference
    .external}

-   [Python Memory Management Part
    3](https://www.evanjones.ca/python-memory-part3.html){.reference
    .external}

The improvements proposed by Evan Jones, which are detailed in [this
paper](https://www.evanjones.ca/memoryallocator/){.reference .external},
got merged in Python 2.5, but this only reduces the problem, it doesn't
fix it completely. To quote the paper:

> <div>
>
> *Unfortunately, this patch can only free an arena if there are no more
> objects allocated in it anymore. This means that fragmentation is a
> large issue. An application could have many megabytes of free memory,
> scattered throughout all the arenas, but it will be unable to free any
> of it. This is a problem experienced by all memory allocators. The
> only way to solve it is to move to a compacting garbage collector,
> which is able to move objects in memory. This would require
> significant changes to the Python interpreter.*
>
> </div>

To keep memory consumption reasonable you can split the job into several
smaller jobs or enable [[persistent job queue]{.std
.std-ref}](index.html#topics-jobs){.hoverxref .tooltip .reference
.internal} and stop/start spider from time to time.
:::
:::

[]{#document-topics/media-pipeline}

::: {#downloading-and-processing-files-and-images .section}
[]{#topics-media-pipeline}

### Downloading and processing files and images[¶](#downloading-and-processing-files-and-images "Permalink to this heading"){.headerlink}

Scrapy provides reusable [[item
pipelines]{.doc}](index.html#document-topics/item-pipeline){.reference
.internal} for downloading files attached to a particular item (for
example, when you scrape products and also want to download their images
locally). These pipelines share a bit of functionality and structure (we
refer to them as media pipelines), but typically you'll either use the
Files Pipeline or the Images Pipeline.

Both pipelines implement these features:

-   Avoid re-downloading media that was downloaded recently

-   Specifying where to store the media (filesystem directory, FTP
    server, Amazon S3 bucket, Google Cloud Storage bucket)

The Images Pipeline has a few extra functions for processing images:

-   Convert all downloaded images to a common format (JPG) and mode
    (RGB)

-   Thumbnail generation

-   Check images width/height to make sure they meet a minimum
    constraint

The pipelines also keep an internal queue of those media URLs which are
currently being scheduled for download, and connect those responses that
arrive containing the same media to that queue. This avoids downloading
the same media more than once when it's shared by several items.

::: {#using-the-files-pipeline .section}
#### Using the Files Pipeline[¶](#using-the-files-pipeline "Permalink to this heading"){.headerlink}

The typical workflow, when using the [`FilesPipeline`{.xref .py
.py-class .docutils .literal .notranslate}]{.pre} goes like this:

1.  In a Spider, you scrape an item and put the URLs of the desired into
    a [`file_urls`{.docutils .literal .notranslate}]{.pre} field.

2.  The item is returned from the spider and goes to the item pipeline.

3.  When the item reaches the [`FilesPipeline`{.xref .py .py-class
    .docutils .literal .notranslate}]{.pre}, the URLs in the
    [`file_urls`{.docutils .literal .notranslate}]{.pre} field are
    scheduled for download using the standard Scrapy scheduler and
    downloader (which means the scheduler and downloader middlewares are
    reused), but with a higher priority, processing them before other
    pages are scraped. The item remains "locked" at that particular
    pipeline stage until the files have finish downloading (or fail for
    some reason).

4.  When the files are downloaded, another field ([`files`{.docutils
    .literal .notranslate}]{.pre}) will be populated with the results.
    This field will contain a list of dicts with information about the
    downloaded files, such as the downloaded path, the original scraped
    url (taken from the [`file_urls`{.docutils .literal
    .notranslate}]{.pre} field), the file checksum and the file status.
    The files in the list of the [`files`{.docutils .literal
    .notranslate}]{.pre} field will retain the same order of the
    original [`file_urls`{.docutils .literal .notranslate}]{.pre} field.
    If some file failed downloading, an error will be logged and the
    file won't be present in the [`files`{.docutils .literal
    .notranslate}]{.pre} field.
:::

::: {#using-the-images-pipeline .section}
[]{#images-pipeline}

#### Using the Images Pipeline[¶](#using-the-images-pipeline "Permalink to this heading"){.headerlink}

Using the [[`ImagesPipeline`{.xref .py .py-class .docutils .literal
.notranslate}]{.pre}](#scrapy.pipelines.images.ImagesPipeline "scrapy.pipelines.images.ImagesPipeline"){.reference
.internal} is a lot like using the [`FilesPipeline`{.xref .py .py-class
.docutils .literal .notranslate}]{.pre}, except the default field names
used are different: you use [`image_urls`{.docutils .literal
.notranslate}]{.pre} for the image URLs of an item and it will populate
an [`images`{.docutils .literal .notranslate}]{.pre} field for the
information about the downloaded images.

The advantage of using the [[`ImagesPipeline`{.xref .py .py-class
.docutils .literal
.notranslate}]{.pre}](#scrapy.pipelines.images.ImagesPipeline "scrapy.pipelines.images.ImagesPipeline"){.reference
.internal} for image files is that you can configure some extra
functions like generating thumbnails and filtering the images based on
their size.

The Images Pipeline requires
[Pillow](https://github.com/python-pillow/Pillow){.reference .external}
7.1.0 or greater. It is used for thumbnailing and normalizing images to
JPEG/RGB format.
:::

::: {#enabling-your-media-pipeline .section}
[]{#topics-media-pipeline-enabling}

#### Enabling your Media Pipeline[¶](#enabling-your-media-pipeline "Permalink to this heading"){.headerlink}

[]{#std-setting-IMAGES_STORE .target}

To enable your media pipeline you must first add it to your project
[[`ITEM_PIPELINES`{.xref .std .std-setting .docutils .literal
.notranslate}]{.pre}](index.html#std-setting-ITEM_PIPELINES){.hoverxref
.tooltip .reference .internal} setting.

For Images Pipeline, use:

::: {.highlight-python .notranslate}
::: highlight
    ITEM_PIPELINES = {"scrapy.pipelines.images.ImagesPipeline": 1}
:::
:::

For Files Pipeline, use:

::: {.highlight-python .notranslate}
::: highlight
    ITEM_PIPELINES = {"scrapy.pipelines.files.FilesPipeline": 1}
:::
:::

::: {.admonition .note}
Note

You can also use both the Files and Images Pipeline at the same time.
:::

Then, configure the target storage setting to a valid value that will be
used for storing the downloaded images. Otherwise the pipeline will
remain disabled, even if you include it in the [[`ITEM_PIPELINES`{.xref
.std .std-setting .docutils .literal
.notranslate}]{.pre}](index.html#std-setting-ITEM_PIPELINES){.hoverxref
.tooltip .reference .internal} setting.

For the Files Pipeline, set the [[`FILES_STORE`{.xref .std .std-setting
.docutils .literal
.notranslate}]{.pre}](#std-setting-FILES_STORE){.hoverxref .tooltip
.reference .internal} setting:

::: {.highlight-python .notranslate}
::: highlight
    FILES_STORE = "/path/to/valid/dir"
:::
:::

For the Images Pipeline, set the [[`IMAGES_STORE`{.xref .std
.std-setting .docutils .literal
.notranslate}]{.pre}](#std-setting-IMAGES_STORE){.hoverxref .tooltip
.reference .internal} setting:

::: {.highlight-python .notranslate}
::: highlight
    IMAGES_STORE = "/path/to/valid/dir"
:::
:::
:::

::: {#file-naming .section}
[]{#topics-file-naming}

#### File Naming[¶](#file-naming "Permalink to this heading"){.headerlink}

::: {#default-file-naming .section}
##### Default File Naming[¶](#default-file-naming "Permalink to this heading"){.headerlink}

By default, files are stored using an [SHA-1
hash](https://en.wikipedia.org/wiki/SHA_hash_functions){.reference
.external} of their URLs for the file names.

For example, the following image URL:

::: {.highlight-default .notranslate}
::: highlight
    http://www.example.com/image.jpg
:::
:::

Whose [`SHA-1`{.docutils .literal .notranslate}]{.pre}` `{.docutils
.literal .notranslate}[`hash`{.docutils .literal .notranslate}]{.pre}
is:

::: {.highlight-default .notranslate}
::: highlight
    3afec3b4765f8f0a07b78f98c07b83f013567a0a
:::
:::

Will be downloaded and stored using your chosen [[storage method]{.std
.std-ref}](#topics-supported-storage){.hoverxref .tooltip .reference
.internal} and the following file name:

::: {.highlight-default .notranslate}
::: highlight
    3afec3b4765f8f0a07b78f98c07b83f013567a0a.jpg
:::
:::
:::

::: {#custom-file-naming .section}
##### Custom File Naming[¶](#custom-file-naming "Permalink to this heading"){.headerlink}

You may wish to use a different calculated file name for saved files.
For example, classifying an image by including meta in the file name.

Customize file names by overriding the [`file_path`{.docutils .literal
.notranslate}]{.pre} method of your media pipeline.

For example, an image pipeline with image URL:

::: {.highlight-default .notranslate}
::: highlight
    http://www.example.com/product/images/large/front/0000000004166
:::
:::

Can be processed into a file name with a condensed hash and the
perspective [`front`{.docutils .literal .notranslate}]{.pre}:

::: {.highlight-default .notranslate}
::: highlight
    00b08510e4_front.jpg
:::
:::

By overriding [`file_path`{.docutils .literal .notranslate}]{.pre} like
this:

::: {.highlight-python .notranslate}
::: highlight
    import hashlib


    def file_path(self, request, response=None, info=None, *, item=None):
        image_url_hash = hashlib.shake_256(request.url.encode()).hexdigest(5)
        image_perspective = request.url.split("/")[-2]
        image_filename = f"{image_url_hash}_{image_perspective}.jpg"

        return image_filename
:::
:::

::: {.admonition .warning}
Warning

If your custom file name scheme relies on meta data that can vary
between scrapes it may lead to unexpected re-downloading of existing
media using new file names.

For example, if your custom file name scheme uses a product title and
the site changes an item's product title between scrapes, Scrapy will
re-download the same media using updated file names.
:::

For more information about the [`file_path`{.docutils .literal
.notranslate}]{.pre} method, see [[Extending the Media Pipelines]{.std
.std-ref}](#topics-media-pipeline-override){.hoverxref .tooltip
.reference .internal}.
:::
:::

::: {#supported-storage .section}
[]{#topics-supported-storage}

#### Supported Storage[¶](#supported-storage "Permalink to this heading"){.headerlink}

::: {#file-system-storage .section}
##### File system storage[¶](#file-system-storage "Permalink to this heading"){.headerlink}

File system storage will save files to the following path:

::: {.highlight-default .notranslate}
::: highlight
    <IMAGES_STORE>/full/<FILE_NAME>
:::
:::

Where:

-   [`<IMAGES_STORE>`{.docutils .literal .notranslate}]{.pre} is the
    directory defined in [[`IMAGES_STORE`{.xref .std .std-setting
    .docutils .literal
    .notranslate}]{.pre}](#std-setting-IMAGES_STORE){.hoverxref .tooltip
    .reference .internal} setting for the Images Pipeline.

-   [`full`{.docutils .literal .notranslate}]{.pre} is a sub-directory
    to separate full images from thumbnails (if used). For more info see
    [[Thumbnail generation for images]{.std
    .std-ref}](#topics-images-thumbnails){.hoverxref .tooltip .reference
    .internal}.

-   [`<FILE_NAME>`{.docutils .literal .notranslate}]{.pre} is the file
    name assigned to the file. For more info see [[File Naming]{.std
    .std-ref}](#topics-file-naming){.hoverxref .tooltip .reference
    .internal}.
:::

::: {#ftp-server-storage .section}
[]{#media-pipeline-ftp}

##### FTP server storage[¶](#ftp-server-storage "Permalink to this heading"){.headerlink}

::: versionadded
[New in version 2.0.]{.versionmodified .added}
:::

[[`FILES_STORE`{.xref .std .std-setting .docutils .literal
.notranslate}]{.pre}](#std-setting-FILES_STORE){.hoverxref .tooltip
.reference .internal} and [[`IMAGES_STORE`{.xref .std .std-setting
.docutils .literal
.notranslate}]{.pre}](#std-setting-IMAGES_STORE){.hoverxref .tooltip
.reference .internal} can point to an FTP server. Scrapy will
automatically upload the files to the server.

[[`FILES_STORE`{.xref .std .std-setting .docutils .literal
.notranslate}]{.pre}](#std-setting-FILES_STORE){.hoverxref .tooltip
.reference .internal} and [[`IMAGES_STORE`{.xref .std .std-setting
.docutils .literal
.notranslate}]{.pre}](#std-setting-IMAGES_STORE){.hoverxref .tooltip
.reference .internal} should be written in one of the following forms:

::: {.highlight-default .notranslate}
::: highlight
    ftp://username:password@address:port/path
    ftp://address:port/path
:::
:::

If [`username`{.docutils .literal .notranslate}]{.pre} and
[`password`{.docutils .literal .notranslate}]{.pre} are not provided,
they are taken from the [[`FTP_USER`{.xref .std .std-setting .docutils
.literal
.notranslate}]{.pre}](index.html#std-setting-FTP_USER){.hoverxref
.tooltip .reference .internal} and [[`FTP_PASSWORD`{.xref .std
.std-setting .docutils .literal
.notranslate}]{.pre}](index.html#std-setting-FTP_PASSWORD){.hoverxref
.tooltip .reference .internal} settings respectively.

FTP supports two different connection modes: active or passive. Scrapy
uses the passive connection mode by default. To use the active
connection mode instead, set the [[`FEED_STORAGE_FTP_ACTIVE`{.xref .std
.std-setting .docutils .literal
.notranslate}]{.pre}](index.html#std-setting-FEED_STORAGE_FTP_ACTIVE){.hoverxref
.tooltip .reference .internal} setting to [`True`{.docutils .literal
.notranslate}]{.pre}.
:::

::: {#amazon-s3-storage .section}
[]{#media-pipelines-s3}

##### Amazon S3 storage[¶](#amazon-s3-storage "Permalink to this heading"){.headerlink}

[]{#std-setting-FILES_STORE_S3_ACL .target}

If [botocore](https://github.com/boto/botocore){.reference .external}
\>= 1.4.87 is installed, [[`FILES_STORE`{.xref .std .std-setting
.docutils .literal
.notranslate}]{.pre}](#std-setting-FILES_STORE){.hoverxref .tooltip
.reference .internal} and [[`IMAGES_STORE`{.xref .std .std-setting
.docutils .literal
.notranslate}]{.pre}](#std-setting-IMAGES_STORE){.hoverxref .tooltip
.reference .internal} can represent an Amazon S3 bucket. Scrapy will
automatically upload the files to the bucket.

For example, this is a valid [[`IMAGES_STORE`{.xref .std .std-setting
.docutils .literal
.notranslate}]{.pre}](#std-setting-IMAGES_STORE){.hoverxref .tooltip
.reference .internal} value:

::: {.highlight-python .notranslate}
::: highlight
    IMAGES_STORE = "s3://bucket/images"
:::
:::

You can modify the Access Control List (ACL) policy used for the stored
files, which is defined by the [[`FILES_STORE_S3_ACL`{.xref .std
.std-setting .docutils .literal
.notranslate}]{.pre}](#std-setting-FILES_STORE_S3_ACL){.hoverxref
.tooltip .reference .internal} and [[`IMAGES_STORE_S3_ACL`{.xref .std
.std-setting .docutils .literal
.notranslate}]{.pre}](#std-setting-IMAGES_STORE_S3_ACL){.hoverxref
.tooltip .reference .internal} settings. By default, the ACL is set to
[`private`{.docutils .literal .notranslate}]{.pre}. To make the files
publicly available use the [`public-read`{.docutils .literal
.notranslate}]{.pre} policy:

::: {.highlight-python .notranslate}
::: highlight
    IMAGES_STORE_S3_ACL = "public-read"
:::
:::

For more information, see [canned
ACLs](https://docs.aws.amazon.com/AmazonS3/latest/dev/acl-overview.html#canned-acl){.reference
.external} in the Amazon S3 Developer Guide.

You can also use other S3-like storages. Storages like self-hosted
[Minio](https://github.com/minio/minio){.reference .external} or
[s3.scality](https://s3.scality.com/){.reference .external}. All you
need to do is set endpoint option in you Scrapy settings:

::: {.highlight-python .notranslate}
::: highlight
    AWS_ENDPOINT_URL = "http://minio.example.com:9000"
:::
:::

For self-hosting you also might feel the need not to use SSL and not to
verify SSL connection:

::: {.highlight-python .notranslate}
::: highlight
    AWS_USE_SSL = False  # or True (None by default)
    AWS_VERIFY = False  # or True (None by default)
:::
:::
:::

::: {#google-cloud-storage .section}
[]{#media-pipeline-gcs}

##### Google Cloud Storage[¶](#google-cloud-storage "Permalink to this heading"){.headerlink}

[]{#std-setting-FILES_STORE_GCS_ACL .target}

[[`FILES_STORE`{.xref .std .std-setting .docutils .literal
.notranslate}]{.pre}](#std-setting-FILES_STORE){.hoverxref .tooltip
.reference .internal} and [[`IMAGES_STORE`{.xref .std .std-setting
.docutils .literal
.notranslate}]{.pre}](#std-setting-IMAGES_STORE){.hoverxref .tooltip
.reference .internal} can represent a Google Cloud Storage bucket.
Scrapy will automatically upload the files to the bucket. (requires
[google-cloud-storage](https://cloud.google.com/storage/docs/reference/libraries#client-libraries-install-python){.reference
.external} )

For example, these are valid [[`IMAGES_STORE`{.xref .std .std-setting
.docutils .literal
.notranslate}]{.pre}](#std-setting-IMAGES_STORE){.hoverxref .tooltip
.reference .internal} and [[`GCS_PROJECT_ID`{.xref .std .std-setting
.docutils .literal
.notranslate}]{.pre}](index.html#std-setting-GCS_PROJECT_ID){.hoverxref
.tooltip .reference .internal} settings:

::: {.highlight-python .notranslate}
::: highlight
    IMAGES_STORE = "gs://bucket/images/"
    GCS_PROJECT_ID = "project_id"
:::
:::

For information about authentication, see this
[documentation](https://cloud.google.com/docs/authentication/production){.reference
.external}.

You can modify the Access Control List (ACL) policy used for the stored
files, which is defined by the [[`FILES_STORE_GCS_ACL`{.xref .std
.std-setting .docutils .literal
.notranslate}]{.pre}](#std-setting-FILES_STORE_GCS_ACL){.hoverxref
.tooltip .reference .internal} and [[`IMAGES_STORE_GCS_ACL`{.xref .std
.std-setting .docutils .literal
.notranslate}]{.pre}](#std-setting-IMAGES_STORE_GCS_ACL){.hoverxref
.tooltip .reference .internal} settings. By default, the ACL is set to
[`''`{.docutils .literal .notranslate}]{.pre} (empty string) which means
that Cloud Storage applies the bucket's default object ACL to the
object. To make the files publicly available use the
[`publicRead`{.docutils .literal .notranslate}]{.pre} policy:

::: {.highlight-python .notranslate}
::: highlight
    IMAGES_STORE_GCS_ACL = "publicRead"
:::
:::

For more information, see [Predefined
ACLs](https://cloud.google.com/storage/docs/access-control/lists#predefined-acl){.reference
.external} in the Google Cloud Platform Developer Guide.
:::
:::

::: {#usage-example .section}
#### Usage example[¶](#usage-example "Permalink to this heading"){.headerlink}

[]{#std-setting-FILES_URLS_FIELD
.target}[]{#std-setting-FILES_RESULT_FIELD
.target}[]{#std-setting-IMAGES_URLS_FIELD .target}

In order to use a media pipeline, first [[enable it]{.std
.std-ref}](#topics-media-pipeline-enabling){.hoverxref .tooltip
.reference .internal}.

Then, if a spider returns an [[item object]{.std
.std-ref}](index.html#topics-items){.hoverxref .tooltip .reference
.internal} with the URLs field ([`file_urls`{.docutils .literal
.notranslate}]{.pre} or [`image_urls`{.docutils .literal
.notranslate}]{.pre}, for the Files or Images Pipeline respectively),
the pipeline will put the results under the respective field
([`files`{.docutils .literal .notranslate}]{.pre} or [`images`{.docutils
.literal .notranslate}]{.pre}).

When using [[item types]{.std
.std-ref}](index.html#item-types){.hoverxref .tooltip .reference
.internal} for which fields are defined beforehand, you must define both
the URLs field and the results field. For example, when using the images
pipeline, items must define both the [`image_urls`{.docutils .literal
.notranslate}]{.pre} and the [`images`{.docutils .literal
.notranslate}]{.pre} field. For instance, using the [`Item`{.xref .py
.py-class .docutils .literal .notranslate}]{.pre} class:

::: {.highlight-python .notranslate}
::: highlight
    import scrapy


    class MyItem(scrapy.Item):
        # ... other item fields ...
        image_urls = scrapy.Field()
        images = scrapy.Field()
:::
:::

If you want to use another field name for the URLs key or for the
results key, it is also possible to override it.

For the Files Pipeline, set [[`FILES_URLS_FIELD`{.xref .std .std-setting
.docutils .literal
.notranslate}]{.pre}](#std-setting-FILES_URLS_FIELD){.hoverxref .tooltip
.reference .internal} and/or [[`FILES_RESULT_FIELD`{.xref .std
.std-setting .docutils .literal
.notranslate}]{.pre}](#std-setting-FILES_RESULT_FIELD){.hoverxref
.tooltip .reference .internal} settings:

::: {.highlight-python .notranslate}
::: highlight
    FILES_URLS_FIELD = "field_name_for_your_files_urls"
    FILES_RESULT_FIELD = "field_name_for_your_processed_files"
:::
:::

For the Images Pipeline, set [[`IMAGES_URLS_FIELD`{.xref .std
.std-setting .docutils .literal
.notranslate}]{.pre}](#std-setting-IMAGES_URLS_FIELD){.hoverxref
.tooltip .reference .internal} and/or [[`IMAGES_RESULT_FIELD`{.xref .std
.std-setting .docutils .literal
.notranslate}]{.pre}](#std-setting-IMAGES_RESULT_FIELD){.hoverxref
.tooltip .reference .internal} settings:

::: {.highlight-python .notranslate}
::: highlight
    IMAGES_URLS_FIELD = "field_name_for_your_images_urls"
    IMAGES_RESULT_FIELD = "field_name_for_your_processed_images"
:::
:::

If you need something more complex and want to override the custom
pipeline behaviour, see [[Extending the Media Pipelines]{.std
.std-ref}](#topics-media-pipeline-override){.hoverxref .tooltip
.reference .internal}.

If you have multiple image pipelines inheriting from ImagePipeline and
you want to have different settings in different pipelines you can set
setting keys preceded with uppercase name of your pipeline class. E.g.
if your pipeline is called MyPipeline and you want to have custom
IMAGES_URLS_FIELD you define setting MYPIPELINE_IMAGES_URLS_FIELD and
your custom settings will be used.
:::

::: {#additional-features .section}
#### Additional features[¶](#additional-features "Permalink to this heading"){.headerlink}

::: {#file-expiration .section}
[]{#id2}

##### File expiration[¶](#file-expiration "Permalink to this heading"){.headerlink}

[]{#std-setting-IMAGES_EXPIRES .target}

The Image Pipeline avoids downloading files that were downloaded
recently. To adjust this retention delay use the [[`FILES_EXPIRES`{.xref
.std .std-setting .docutils .literal
.notranslate}]{.pre}](#std-setting-FILES_EXPIRES){.hoverxref .tooltip
.reference .internal} setting (or [[`IMAGES_EXPIRES`{.xref .std
.std-setting .docutils .literal
.notranslate}]{.pre}](#std-setting-IMAGES_EXPIRES){.hoverxref .tooltip
.reference .internal}, in case of Images Pipeline), which specifies the
delay in number of days:

::: {.highlight-python .notranslate}
::: highlight
    # 120 days of delay for files expiration
    FILES_EXPIRES = 120

    # 30 days of delay for images expiration
    IMAGES_EXPIRES = 30
:::
:::

The default value for both settings is 90 days.

If you have pipeline that subclasses FilesPipeline and you'd like to
have different setting for it you can set setting keys preceded by
uppercase class name. E.g. given pipeline class called MyPipeline you
can set setting key:

> <div>
>
> MYPIPELINE_FILES_EXPIRES = 180
>
> </div>

and pipeline class MyPipeline will have expiration time set to 180.

The last modified time from the file is used to determine the age of the
file in days, which is then compared to the set expiration time to
determine if the file is expired.
:::

::: {#thumbnail-generation-for-images .section}
[]{#topics-images-thumbnails}

##### Thumbnail generation for images[¶](#thumbnail-generation-for-images "Permalink to this heading"){.headerlink}

The Images Pipeline can automatically create thumbnails of the
downloaded images.

In order to use this feature, you must set [[`IMAGES_THUMBS`{.xref .std
.std-setting .docutils .literal
.notranslate}]{.pre}](#std-setting-IMAGES_THUMBS){.hoverxref .tooltip
.reference .internal} to a dictionary where the keys are the thumbnail
names and the values are their dimensions.

For example:

::: {.highlight-python .notranslate}
::: highlight
    IMAGES_THUMBS = {
        "small": (50, 50),
        "big": (270, 270),
    }
:::
:::

When you use this feature, the Images Pipeline will create thumbnails of
the each specified size with this format:

::: {.highlight-default .notranslate}
::: highlight
    <IMAGES_STORE>/thumbs/<size_name>/<image_id>.jpg
:::
:::

Where:

-   [`<size_name>`{.docutils .literal .notranslate}]{.pre} is the one
    specified in the [[`IMAGES_THUMBS`{.xref .std .std-setting .docutils
    .literal
    .notranslate}]{.pre}](#std-setting-IMAGES_THUMBS){.hoverxref
    .tooltip .reference .internal} dictionary keys ([`small`{.docutils
    .literal .notranslate}]{.pre}, [`big`{.docutils .literal
    .notranslate}]{.pre}, etc)

-   [`<image_id>`{.docutils .literal .notranslate}]{.pre} is the [SHA-1
    hash](https://en.wikipedia.org/wiki/SHA_hash_functions){.reference
    .external} of the image url

Example of image files stored using [`small`{.docutils .literal
.notranslate}]{.pre} and [`big`{.docutils .literal .notranslate}]{.pre}
thumbnail names:

::: {.highlight-default .notranslate}
::: highlight
    <IMAGES_STORE>/full/63bbfea82b8880ed33cdb762aa11fab722a90a24.jpg
    <IMAGES_STORE>/thumbs/small/63bbfea82b8880ed33cdb762aa11fab722a90a24.jpg
    <IMAGES_STORE>/thumbs/big/63bbfea82b8880ed33cdb762aa11fab722a90a24.jpg
:::
:::

The first one is the full image, as downloaded from the site.
:::

::: {#filtering-out-small-images .section}
##### Filtering out small images[¶](#filtering-out-small-images "Permalink to this heading"){.headerlink}

[]{#std-setting-IMAGES_MIN_HEIGHT .target}

When using the Images Pipeline, you can drop images which are too small,
by specifying the minimum allowed size in the
[[`IMAGES_MIN_HEIGHT`{.xref .std .std-setting .docutils .literal
.notranslate}]{.pre}](#std-setting-IMAGES_MIN_HEIGHT){.hoverxref
.tooltip .reference .internal} and [[`IMAGES_MIN_WIDTH`{.xref .std
.std-setting .docutils .literal
.notranslate}]{.pre}](#std-setting-IMAGES_MIN_WIDTH){.hoverxref .tooltip
.reference .internal} settings.

For example:

::: {.highlight-default .notranslate}
::: highlight
    IMAGES_MIN_HEIGHT = 110
    IMAGES_MIN_WIDTH = 110
:::
:::

::: {.admonition .note}
Note

The size constraints don't affect thumbnail generation at all.
:::

It is possible to set just one size constraint or both. When setting
both of them, only images that satisfy both minimum sizes will be saved.
For the above example, images of sizes (105 x 105) or (105 x 200) or
(200 x 105) will all be dropped because at least one dimension is
shorter than the constraint.

By default, there are no size constraints, so all images are processed.
:::

::: {#allowing-redirections .section}
##### Allowing redirections[¶](#allowing-redirections "Permalink to this heading"){.headerlink}

By default media pipelines ignore redirects, i.e. an HTTP redirection to
a media file URL request will mean the media download is considered
failed.

To handle media redirections, set this setting to [`True`{.docutils
.literal .notranslate}]{.pre}:

::: {.highlight-default .notranslate}
::: highlight
    MEDIA_ALLOW_REDIRECTS = True
:::
:::
:::
:::

::: {#module-scrapy.pipelines.files .section}
[]{#extending-the-media-pipelines}[]{#topics-media-pipeline-override}

#### Extending the Media Pipelines[¶](#module-scrapy.pipelines.files "Permalink to this heading"){.headerlink}

See here the methods that you can override in your custom Files
Pipeline:

*[class]{.pre}[ ]{.w}*[[scrapy.pipelines.files.]{.pre}]{.sig-prename .descclassname}[[FilesPipeline]{.pre}]{.sig-name .descname}[[[\[source\]]{.pre}]{.viewcode-link}](_modules/scrapy/pipelines/files.html#FilesPipeline){.reference .internal}[¶](#scrapy.pipelines.files.FilesPipeline "Permalink to this definition"){.headerlink}

:   

    [[file_path]{.pre}]{.sig-name .descname}[(]{.sig-paren}*[[self]{.pre}]{.n}*, *[[request]{.pre}]{.n}*, *[[response]{.pre}]{.n}[[=]{.pre}]{.o}[[None]{.pre}]{.default_value}*, *[[info]{.pre}]{.n}[[=]{.pre}]{.o}[[None]{.pre}]{.default_value}*, *[[\*]{.pre}]{.o}*, *[[item]{.pre}]{.n}[[=]{.pre}]{.o}[[None]{.pre}]{.default_value}*[)]{.sig-paren}[[[\[source\]]{.pre}]{.viewcode-link}](_modules/scrapy/pipelines/files.html#FilesPipeline.file_path){.reference .internal}[¶](#scrapy.pipelines.files.FilesPipeline.file_path "Permalink to this definition"){.headerlink}

    :   This method is called once per downloaded item. It returns the
        download path of the file originating from the specified
        [[`response`{.xref .py .py-class .docutils .literal
        .notranslate}]{.pre}](index.html#scrapy.http.Response "scrapy.http.Response"){.reference
        .internal}.

        In addition to [`response`{.docutils .literal
        .notranslate}]{.pre}, this method receives the original
        [`request`{.xref .py .py-class .docutils .literal
        .notranslate}]{.pre}, [`info`{.xref .py .py-class .docutils
        .literal .notranslate}]{.pre} and [`item`{.xref .py .py-class
        .docutils .literal .notranslate}]{.pre}

        You can override this method to customize the download path of
        each file.

        For example, if file URLs end like regular paths (e.g.
        [`https://example.com/a/b/c/foo.png`{.docutils .literal
        .notranslate}]{.pre}), you can use the following approach to
        download all files into the [`files`{.docutils .literal
        .notranslate}]{.pre} folder with their original filenames (e.g.
        [`files/foo.png`{.docutils .literal .notranslate}]{.pre}):

        ::: {.highlight-python .notranslate}
        ::: highlight
            from pathlib import PurePosixPath
            from urllib.parse import urlparse

            from scrapy.pipelines.files import FilesPipeline


            class MyFilesPipeline(FilesPipeline):
                def file_path(self, request, response=None, info=None, *, item=None):
                    return "files/" + PurePosixPath(urlparse(request.url).path).name
        :::
        :::

        Similarly, you can use the [`item`{.docutils .literal
        .notranslate}]{.pre} to determine the file path based on some
        item property.

        By default the [[`file_path()`{.xref .py .py-meth .docutils
        .literal
        .notranslate}]{.pre}](#scrapy.pipelines.files.FilesPipeline.file_path "scrapy.pipelines.files.FilesPipeline.file_path"){.reference
        .internal} method returns [`full/<request`{.docutils .literal
        .notranslate}]{.pre}` `{.docutils .literal
        .notranslate}[`URL`{.docutils .literal
        .notranslate}]{.pre}` `{.docutils .literal
        .notranslate}[`hash>.<extension>`{.docutils .literal
        .notranslate}]{.pre}.

        ::: versionadded
        [New in version 2.4: ]{.versionmodified .added}The *item*
        parameter.
        :::

    [[get_media_requests]{.pre}]{.sig-name .descname}[(]{.sig-paren}*[[item]{.pre}]{.n}*, *[[info]{.pre}]{.n}*[)]{.sig-paren}[[[\[source\]]{.pre}]{.viewcode-link}](_modules/scrapy/pipelines/files.html#FilesPipeline.get_media_requests){.reference .internal}[¶](#scrapy.pipelines.files.FilesPipeline.get_media_requests "Permalink to this definition"){.headerlink}

    :   As seen on the workflow, the pipeline will get the URLs of the
        images to download from the item. In order to do this, you can
        override the [[`get_media_requests()`{.xref .py .py-meth
        .docutils .literal
        .notranslate}]{.pre}](#scrapy.pipelines.files.FilesPipeline.get_media_requests "scrapy.pipelines.files.FilesPipeline.get_media_requests"){.reference
        .internal} method and return a Request for each file URL:

        ::: {.highlight-python .notranslate}
        ::: highlight
            from itemadapter import ItemAdapter


            def get_media_requests(self, item, info):
                adapter = ItemAdapter(item)
                for file_url in adapter["file_urls"]:
                    yield scrapy.Request(file_url)
        :::
        :::

        Those requests will be processed by the pipeline and, when they
        have finished downloading, the results will be sent to the
        [[`item_completed()`{.xref .py .py-meth .docutils .literal
        .notranslate}]{.pre}](#scrapy.pipelines.files.FilesPipeline.item_completed "scrapy.pipelines.files.FilesPipeline.item_completed"){.reference
        .internal} method, as a list of 2-element tuples. Each tuple
        will contain [`(success,`{.docutils .literal
        .notranslate}]{.pre}` `{.docutils .literal
        .notranslate}[`file_info_or_error)`{.docutils .literal
        .notranslate}]{.pre} where:

        -   [`success`{.docutils .literal .notranslate}]{.pre} is a
            boolean which is [`True`{.docutils .literal
            .notranslate}]{.pre} if the image was downloaded
            successfully or [`False`{.docutils .literal
            .notranslate}]{.pre} if it failed for some reason

        -   [`file_info_or_error`{.docutils .literal
            .notranslate}]{.pre} is a dict containing the following keys
            (if success is [`True`{.docutils .literal
            .notranslate}]{.pre}) or a [[`Failure`{.xref .py .py-exc
            .docutils .literal
            .notranslate}]{.pre}](https://docs.twisted.org/en/stable/api/twisted.python.failure.Failure.html "(in Twisted)"){.reference
            .external} if there was a problem.

            -   [`url`{.docutils .literal .notranslate}]{.pre} - the url
                where the file was downloaded from. This is the url of
                the request returned from the
                [[`get_media_requests()`{.xref .py .py-meth .docutils
                .literal
                .notranslate}]{.pre}](#scrapy.pipelines.files.FilesPipeline.get_media_requests "scrapy.pipelines.files.FilesPipeline.get_media_requests"){.reference
                .internal} method.

            -   [`path`{.docutils .literal .notranslate}]{.pre} - the
                path (relative to [[`FILES_STORE`{.xref .std
                .std-setting .docutils .literal
                .notranslate}]{.pre}](#std-setting-FILES_STORE){.hoverxref
                .tooltip .reference .internal}) where the file was
                stored

            -   [`checksum`{.docutils .literal .notranslate}]{.pre} - a
                [MD5 hash](https://en.wikipedia.org/wiki/MD5){.reference
                .external} of the image contents

            -   [`status`{.docutils .literal .notranslate}]{.pre} - the
                file status indication.

                ::: versionadded
                [New in version 2.2.]{.versionmodified .added}
                :::

                It can be one of the following:

                -   [`downloaded`{.docutils .literal
                    .notranslate}]{.pre} - file was downloaded.

                -   [`uptodate`{.docutils .literal
                    .notranslate}]{.pre} - file was not downloaded, as
                    it was downloaded recently, according to the file
                    expiration policy.

                -   [`cached`{.docutils .literal .notranslate}]{.pre} -
                    file was already scheduled for download, by another
                    item sharing the same file.

        The list of tuples received by [[`item_completed()`{.xref .py
        .py-meth .docutils .literal
        .notranslate}]{.pre}](#scrapy.pipelines.files.FilesPipeline.item_completed "scrapy.pipelines.files.FilesPipeline.item_completed"){.reference
        .internal} is guaranteed to retain the same order of the
        requests returned from the [[`get_media_requests()`{.xref .py
        .py-meth .docutils .literal
        .notranslate}]{.pre}](#scrapy.pipelines.files.FilesPipeline.get_media_requests "scrapy.pipelines.files.FilesPipeline.get_media_requests"){.reference
        .internal} method.

        Here's a typical value of the [`results`{.docutils .literal
        .notranslate}]{.pre} argument:

        ::: {.highlight-python .notranslate}
        ::: highlight
            [
                (
                    True,
                    {
                        "checksum": "2b00042f7481c7b056c4b410d28f33cf",
                        "path": "full/0a79c461a4062ac383dc4fade7bc09f1384a3910.jpg",
                        "url": "http://www.example.com/files/product1.pdf",
                        "status": "downloaded",
                    },
                ),
                (False, Failure(...)),
            ]
        :::
        :::

        By default the [[`get_media_requests()`{.xref .py .py-meth
        .docutils .literal
        .notranslate}]{.pre}](#scrapy.pipelines.files.FilesPipeline.get_media_requests "scrapy.pipelines.files.FilesPipeline.get_media_requests"){.reference
        .internal} method returns [`None`{.docutils .literal
        .notranslate}]{.pre} which means there are no files to download
        for the item.

    [[item_completed]{.pre}]{.sig-name .descname}[(]{.sig-paren}*[[results]{.pre}]{.n}*, *[[item]{.pre}]{.n}*, *[[info]{.pre}]{.n}*[)]{.sig-paren}[[[\[source\]]{.pre}]{.viewcode-link}](_modules/scrapy/pipelines/files.html#FilesPipeline.item_completed){.reference .internal}[¶](#scrapy.pipelines.files.FilesPipeline.item_completed "Permalink to this definition"){.headerlink}

    :   The [[`FilesPipeline.item_completed()`{.xref .py .py-meth
        .docutils .literal
        .notranslate}]{.pre}](#scrapy.pipelines.files.FilesPipeline.item_completed "scrapy.pipelines.files.FilesPipeline.item_completed"){.reference
        .internal} method called when all file requests for a single
        item have completed (either finished downloading, or failed for
        some reason).

        The [[`item_completed()`{.xref .py .py-meth .docutils .literal
        .notranslate}]{.pre}](#scrapy.pipelines.files.FilesPipeline.item_completed "scrapy.pipelines.files.FilesPipeline.item_completed"){.reference
        .internal} method must return the output that will be sent to
        subsequent item pipeline stages, so you must return (or drop)
        the item, as you would in any pipeline.

        Here is an example of the [[`item_completed()`{.xref .py
        .py-meth .docutils .literal
        .notranslate}]{.pre}](#scrapy.pipelines.files.FilesPipeline.item_completed "scrapy.pipelines.files.FilesPipeline.item_completed"){.reference
        .internal} method where we store the downloaded file paths
        (passed in results) in the [`file_paths`{.docutils .literal
        .notranslate}]{.pre} item field, and we drop the item if it
        doesn't contain any files:

        ::: {.highlight-python .notranslate}
        ::: highlight
            from itemadapter import ItemAdapter
            from scrapy.exceptions import DropItem


            def item_completed(self, results, item, info):
                file_paths = [x["path"] for ok, x in results if ok]
                if not file_paths:
                    raise DropItem("Item contains no files")
                adapter = ItemAdapter(item)
                adapter["file_paths"] = file_paths
                return item
        :::
        :::

        By default, the [[`item_completed()`{.xref .py .py-meth
        .docutils .literal
        .notranslate}]{.pre}](#scrapy.pipelines.files.FilesPipeline.item_completed "scrapy.pipelines.files.FilesPipeline.item_completed"){.reference
        .internal} method returns the item.

[]{#module-scrapy.pipelines.images .target}

See here the methods that you can override in your custom Images
Pipeline:

*[class]{.pre}[ ]{.w}*[[scrapy.pipelines.images.]{.pre}]{.sig-prename .descclassname}[[ImagesPipeline]{.pre}]{.sig-name .descname}[[[\[source\]]{.pre}]{.viewcode-link}](_modules/scrapy/pipelines/images.html#ImagesPipeline){.reference .internal}[¶](#scrapy.pipelines.images.ImagesPipeline "Permalink to this definition"){.headerlink}

:   <div>
    >
    > The [[`ImagesPipeline`{.xref .py .py-class .docutils .literal
    > .notranslate}]{.pre}](#scrapy.pipelines.images.ImagesPipeline "scrapy.pipelines.images.ImagesPipeline"){.reference
    > .internal} is an extension of the [`FilesPipeline`{.xref .py
    > .py-class .docutils .literal .notranslate}]{.pre}, customizing the
    > field names and adding custom behavior for images.
    >
    > </div>

    [[file_path]{.pre}]{.sig-name .descname}[(]{.sig-paren}*[[self]{.pre}]{.n}*, *[[request]{.pre}]{.n}*, *[[response]{.pre}]{.n}[[=]{.pre}]{.o}[[None]{.pre}]{.default_value}*, *[[info]{.pre}]{.n}[[=]{.pre}]{.o}[[None]{.pre}]{.default_value}*, *[[\*]{.pre}]{.o}*, *[[item]{.pre}]{.n}[[=]{.pre}]{.o}[[None]{.pre}]{.default_value}*[)]{.sig-paren}[[[\[source\]]{.pre}]{.viewcode-link}](_modules/scrapy/pipelines/images.html#ImagesPipeline.file_path){.reference .internal}[¶](#scrapy.pipelines.images.ImagesPipeline.file_path "Permalink to this definition"){.headerlink}

    :   This method is called once per downloaded item. It returns the
        download path of the file originating from the specified
        [[`response`{.xref .py .py-class .docutils .literal
        .notranslate}]{.pre}](index.html#scrapy.http.Response "scrapy.http.Response"){.reference
        .internal}.

        In addition to [`response`{.docutils .literal
        .notranslate}]{.pre}, this method receives the original
        [`request`{.xref .py .py-class .docutils .literal
        .notranslate}]{.pre}, [`info`{.xref .py .py-class .docutils
        .literal .notranslate}]{.pre} and [`item`{.xref .py .py-class
        .docutils .literal .notranslate}]{.pre}

        You can override this method to customize the download path of
        each file.

        For example, if file URLs end like regular paths (e.g.
        [`https://example.com/a/b/c/foo.png`{.docutils .literal
        .notranslate}]{.pre}), you can use the following approach to
        download all files into the [`files`{.docutils .literal
        .notranslate}]{.pre} folder with their original filenames (e.g.
        [`files/foo.png`{.docutils .literal .notranslate}]{.pre}):

        ::: {.highlight-python .notranslate}
        ::: highlight
            from pathlib import PurePosixPath
            from urllib.parse import urlparse

            from scrapy.pipelines.images import ImagesPipeline


            class MyImagesPipeline(ImagesPipeline):
                def file_path(self, request, response=None, info=None, *, item=None):
                    return "files/" + PurePosixPath(urlparse(request.url).path).name
        :::
        :::

        Similarly, you can use the [`item`{.docutils .literal
        .notranslate}]{.pre} to determine the file path based on some
        item property.

        By default the [[`file_path()`{.xref .py .py-meth .docutils
        .literal
        .notranslate}]{.pre}](#scrapy.pipelines.images.ImagesPipeline.file_path "scrapy.pipelines.images.ImagesPipeline.file_path"){.reference
        .internal} method returns [`full/<request`{.docutils .literal
        .notranslate}]{.pre}` `{.docutils .literal
        .notranslate}[`URL`{.docutils .literal
        .notranslate}]{.pre}` `{.docutils .literal
        .notranslate}[`hash>.<extension>`{.docutils .literal
        .notranslate}]{.pre}.

        ::: versionadded
        [New in version 2.4: ]{.versionmodified .added}The *item*
        parameter.
        :::

    [[thumb_path]{.pre}]{.sig-name .descname}[(]{.sig-paren}*[[self]{.pre}]{.n}*, *[[request]{.pre}]{.n}*, *[[thumb_id]{.pre}]{.n}*, *[[response]{.pre}]{.n}[[=]{.pre}]{.o}[[None]{.pre}]{.default_value}*, *[[info]{.pre}]{.n}[[=]{.pre}]{.o}[[None]{.pre}]{.default_value}*, *[[\*]{.pre}]{.o}*, *[[item]{.pre}]{.n}[[=]{.pre}]{.o}[[None]{.pre}]{.default_value}*[)]{.sig-paren}[[[\[source\]]{.pre}]{.viewcode-link}](_modules/scrapy/pipelines/images.html#ImagesPipeline.thumb_path){.reference .internal}[¶](#scrapy.pipelines.images.ImagesPipeline.thumb_path "Permalink to this definition"){.headerlink}

    :   This method is called for every item of [[`IMAGES_THUMBS`{.xref
        .std .std-setting .docutils .literal
        .notranslate}]{.pre}](#std-setting-IMAGES_THUMBS){.hoverxref
        .tooltip .reference .internal} per downloaded item. It returns
        the thumbnail download path of the image originating from the
        specified [[`response`{.xref .py .py-class .docutils .literal
        .notranslate}]{.pre}](index.html#scrapy.http.Response "scrapy.http.Response"){.reference
        .internal}.

        In addition to [`response`{.docutils .literal
        .notranslate}]{.pre}, this method receives the original
        [`request`{.xref .py .py-class .docutils .literal
        .notranslate}]{.pre}, [`thumb_id`{.docutils .literal
        .notranslate}]{.pre}, [`info`{.xref .py .py-class .docutils
        .literal .notranslate}]{.pre} and [`item`{.xref .py .py-class
        .docutils .literal .notranslate}]{.pre}.

        You can override this method to customize the thumbnail download
        path of each image. You can use the [`item`{.docutils .literal
        .notranslate}]{.pre} to determine the file path based on some
        item property.

        By default the [[`thumb_path()`{.xref .py .py-meth .docutils
        .literal
        .notranslate}]{.pre}](#scrapy.pipelines.images.ImagesPipeline.thumb_path "scrapy.pipelines.images.ImagesPipeline.thumb_path"){.reference
        .internal} method returns [`thumbs/<size`{.docutils .literal
        .notranslate}]{.pre}` `{.docutils .literal
        .notranslate}[`name>/<request`{.docutils .literal
        .notranslate}]{.pre}` `{.docutils .literal
        .notranslate}[`URL`{.docutils .literal
        .notranslate}]{.pre}` `{.docutils .literal
        .notranslate}[`hash>.<extension>`{.docutils .literal
        .notranslate}]{.pre}.

    [[get_media_requests]{.pre}]{.sig-name .descname}[(]{.sig-paren}*[[item]{.pre}]{.n}*, *[[info]{.pre}]{.n}*[)]{.sig-paren}[[[\[source\]]{.pre}]{.viewcode-link}](_modules/scrapy/pipelines/images.html#ImagesPipeline.get_media_requests){.reference .internal}[¶](#scrapy.pipelines.images.ImagesPipeline.get_media_requests "Permalink to this definition"){.headerlink}

    :   Works the same way as
        [`FilesPipeline.get_media_requests()`{.xref .py .py-meth
        .docutils .literal .notranslate}]{.pre} method, but using a
        different field name for image urls.

        Must return a Request for each image URL.

    [[item_completed]{.pre}]{.sig-name .descname}[(]{.sig-paren}*[[results]{.pre}]{.n}*, *[[item]{.pre}]{.n}*, *[[info]{.pre}]{.n}*[)]{.sig-paren}[[[\[source\]]{.pre}]{.viewcode-link}](_modules/scrapy/pipelines/images.html#ImagesPipeline.item_completed){.reference .internal}[¶](#scrapy.pipelines.images.ImagesPipeline.item_completed "Permalink to this definition"){.headerlink}

    :   The [[`ImagesPipeline.item_completed()`{.xref .py .py-meth
        .docutils .literal
        .notranslate}]{.pre}](#scrapy.pipelines.images.ImagesPipeline.item_completed "scrapy.pipelines.images.ImagesPipeline.item_completed"){.reference
        .internal} method is called when all image requests for a single
        item have completed (either finished downloading, or failed for
        some reason).

        Works the same way as [`FilesPipeline.item_completed()`{.xref
        .py .py-meth .docutils .literal .notranslate}]{.pre} method, but
        using a different field names for storing image downloading
        results.

        By default, the [[`item_completed()`{.xref .py .py-meth
        .docutils .literal
        .notranslate}]{.pre}](#scrapy.pipelines.images.ImagesPipeline.item_completed "scrapy.pipelines.images.ImagesPipeline.item_completed"){.reference
        .internal} method returns the item.
:::

::: {#custom-images-pipeline-example .section}
[]{#media-pipeline-example}

#### Custom Images pipeline example[¶](#custom-images-pipeline-example "Permalink to this heading"){.headerlink}

Here is a full example of the Images Pipeline whose methods are
exemplified above:

::: {.highlight-python .notranslate}
::: highlight
    import scrapy
    from itemadapter import ItemAdapter
    from scrapy.exceptions import DropItem
    from scrapy.pipelines.images import ImagesPipeline


    class MyImagesPipeline(ImagesPipeline):
        def get_media_requests(self, item, info):
            for image_url in item["image_urls"]:
                yield scrapy.Request(image_url)

        def item_completed(self, results, item, info):
            image_paths = [x["path"] for ok, x in results if ok]
            if not image_paths:
                raise DropItem("Item contains no images")
            adapter = ItemAdapter(item)
            adapter["image_paths"] = image_paths
            return item
:::
:::

To enable your custom media pipeline component you must add its class
import path to the [[`ITEM_PIPELINES`{.xref .std .std-setting .docutils
.literal
.notranslate}]{.pre}](index.html#std-setting-ITEM_PIPELINES){.hoverxref
.tooltip .reference .internal} setting, like in the following example:

::: {.highlight-python .notranslate}
::: highlight
    ITEM_PIPELINES = {"myproject.pipelines.MyImagesPipeline": 300}
:::
:::
:::
:::

[]{#document-topics/deploy}

::: {#deploying-spiders .section}
[]{#topics-deploy}

### Deploying Spiders[¶](#deploying-spiders "Permalink to this heading"){.headerlink}

This section describes the different options you have for deploying your
Scrapy spiders to run them on a regular basis. Running Scrapy spiders in
your local machine is very convenient for the (early) development stage,
but not so much when you need to execute long-running spiders or move
spiders to run in production continuously. This is where the solutions
for deploying Scrapy spiders come in.

Popular choices for deploying Scrapy spiders are:

-   [[Scrapyd]{.std .std-ref}](#deploy-scrapyd){.hoverxref .tooltip
    .reference .internal} (open source)

-   [[Zyte Scrapy Cloud]{.std
    .std-ref}](#deploy-scrapy-cloud){.hoverxref .tooltip .reference
    .internal} (cloud-based)

::: {#deploying-to-a-scrapyd-server .section}
[]{#deploy-scrapyd}

#### Deploying to a Scrapyd Server[¶](#deploying-to-a-scrapyd-server "Permalink to this heading"){.headerlink}

[Scrapyd](https://github.com/scrapy/scrapyd){.reference .external} is an
open source application to run Scrapy spiders. It provides a server with
HTTP API, capable of running and monitoring Scrapy spiders.

To deploy spiders to Scrapyd, you can use the scrapyd-deploy tool
provided by the
[scrapyd-client](https://github.com/scrapy/scrapyd-client){.reference
.external} package. Please refer to the [scrapyd-deploy
documentation](https://scrapyd.readthedocs.io/en/latest/deploy.html){.reference
.external} for more information.

Scrapyd is maintained by some of the Scrapy developers.
:::

::: {#deploying-to-zyte-scrapy-cloud .section}
[]{#deploy-scrapy-cloud}

#### Deploying to Zyte Scrapy Cloud[¶](#deploying-to-zyte-scrapy-cloud "Permalink to this heading"){.headerlink}

[Zyte Scrapy Cloud](https://www.zyte.com/scrapy-cloud/){.reference
.external} is a hosted, cloud-based service by
[Zyte](https://zyte.com/){.reference .external}, the company behind
Scrapy.

Zyte Scrapy Cloud removes the need to setup and monitor servers and
provides a nice UI to manage spiders and review scraped items, logs and
stats.

To deploy spiders to Zyte Scrapy Cloud you can use the
[shub](https://shub.readthedocs.io/en/latest/){.reference .external}
command line tool. Please refer to the [Zyte Scrapy Cloud
documentation](https://docs.zyte.com/scrapy-cloud.html){.reference
.external} for more information.

Zyte Scrapy Cloud is compatible with Scrapyd and one can switch between
them as needed - the configuration is read from the
[`scrapy.cfg`{.docutils .literal .notranslate}]{.pre} file just like
[`scrapyd-deploy`{.docutils .literal .notranslate}]{.pre}.
:::
:::

[]{#document-topics/autothrottle}

::: {#autothrottle-extension .section}
[]{#topics-autothrottle}

### AutoThrottle extension[¶](#autothrottle-extension "Permalink to this heading"){.headerlink}

This is an extension for automatically throttling crawling speed based
on load of both the Scrapy server and the website you are crawling.

::: {#design-goals .section}
#### Design goals[¶](#design-goals "Permalink to this heading"){.headerlink}

1.  be nicer to sites instead of using default download delay of zero

2.  automatically adjust Scrapy to the optimum crawling speed, so the
    user doesn't have to tune the download delays to find the optimum
    one. The user only needs to specify the maximum concurrent requests
    it allows, and the extension does the rest.
:::

::: {#how-it-works .section}
[]{#autothrottle-algorithm}

#### How it works[¶](#how-it-works "Permalink to this heading"){.headerlink}

AutoThrottle extension adjusts download delays dynamically to make
spider send [[`AUTOTHROTTLE_TARGET_CONCURRENCY`{.xref .std .std-setting
.docutils .literal
.notranslate}]{.pre}](#std-setting-AUTOTHROTTLE_TARGET_CONCURRENCY){.hoverxref
.tooltip .reference .internal} concurrent requests on average to each
remote website.

It uses download latency to compute the delays. The main idea is the
following: if a server needs [`latency`{.docutils .literal
.notranslate}]{.pre} seconds to respond, a client should send a request
each [`latency/N`{.docutils .literal .notranslate}]{.pre} seconds to
have [`N`{.docutils .literal .notranslate}]{.pre} requests processed in
parallel.

Instead of adjusting the delays one can just set a small fixed download
delay and impose hard limits on concurrency using
[[`CONCURRENT_REQUESTS_PER_DOMAIN`{.xref .std .std-setting .docutils
.literal
.notranslate}]{.pre}](index.html#std-setting-CONCURRENT_REQUESTS_PER_DOMAIN){.hoverxref
.tooltip .reference .internal} or [[`CONCURRENT_REQUESTS_PER_IP`{.xref
.std .std-setting .docutils .literal
.notranslate}]{.pre}](index.html#std-setting-CONCURRENT_REQUESTS_PER_IP){.hoverxref
.tooltip .reference .internal} options. It will provide a similar
effect, but there are some important differences:

-   because the download delay is small there will be occasional bursts
    of requests;

-   often non-200 (error) responses can be returned faster than regular
    responses, so with a small download delay and a hard concurrency
    limit crawler will be sending requests to server faster when server
    starts to return errors. But this is an opposite of what crawler
    should do - in case of errors it makes more sense to slow down:
    these errors may be caused by the high request rate.

AutoThrottle doesn't have these issues.
:::

::: {#throttling-algorithm .section}
#### Throttling algorithm[¶](#throttling-algorithm "Permalink to this heading"){.headerlink}

AutoThrottle algorithm adjusts download delays based on the following
rules:

1.  spiders always start with a download delay of
    [[`AUTOTHROTTLE_START_DELAY`{.xref .std .std-setting .docutils
    .literal
    .notranslate}]{.pre}](#std-setting-AUTOTHROTTLE_START_DELAY){.hoverxref
    .tooltip .reference .internal};

2.  when a response is received, the target download delay is calculated
    as [`latency`{.docutils .literal .notranslate}]{.pre}` `{.docutils
    .literal .notranslate}[`/`{.docutils .literal
    .notranslate}]{.pre}` `{.docutils .literal
    .notranslate}[`N`{.docutils .literal .notranslate}]{.pre} where
    [`latency`{.docutils .literal .notranslate}]{.pre} is a latency of
    the response, and [`N`{.docutils .literal .notranslate}]{.pre} is
    [[`AUTOTHROTTLE_TARGET_CONCURRENCY`{.xref .std .std-setting
    .docutils .literal
    .notranslate}]{.pre}](#std-setting-AUTOTHROTTLE_TARGET_CONCURRENCY){.hoverxref
    .tooltip .reference .internal}.

3.  download delay for next requests is set to the average of previous
    download delay and the target download delay;

4.  latencies of non-200 responses are not allowed to decrease the
    delay;

5.  download delay can't become less than [[`DOWNLOAD_DELAY`{.xref .std
    .std-setting .docutils .literal
    .notranslate}]{.pre}](index.html#std-setting-DOWNLOAD_DELAY){.hoverxref
    .tooltip .reference .internal} or greater than
    [[`AUTOTHROTTLE_MAX_DELAY`{.xref .std .std-setting .docutils
    .literal
    .notranslate}]{.pre}](#std-setting-AUTOTHROTTLE_MAX_DELAY){.hoverxref
    .tooltip .reference .internal}

::: {.admonition .note}
Note

The AutoThrottle extension honours the standard Scrapy settings for
concurrency and delay. This means that it will respect
[[`CONCURRENT_REQUESTS_PER_DOMAIN`{.xref .std .std-setting .docutils
.literal
.notranslate}]{.pre}](index.html#std-setting-CONCURRENT_REQUESTS_PER_DOMAIN){.hoverxref
.tooltip .reference .internal} and [[`CONCURRENT_REQUESTS_PER_IP`{.xref
.std .std-setting .docutils .literal
.notranslate}]{.pre}](index.html#std-setting-CONCURRENT_REQUESTS_PER_IP){.hoverxref
.tooltip .reference .internal} options and never set a download delay
lower than [[`DOWNLOAD_DELAY`{.xref .std .std-setting .docutils .literal
.notranslate}]{.pre}](index.html#std-setting-DOWNLOAD_DELAY){.hoverxref
.tooltip .reference .internal}.
:::

In Scrapy, the download latency is measured as the time elapsed between
establishing the TCP connection and receiving the HTTP headers.

Note that these latencies are very hard to measure accurately in a
cooperative multitasking environment because Scrapy may be busy
processing a spider callback, for example, and unable to attend
downloads. However, these latencies should still give a reasonable
estimate of how busy Scrapy (and ultimately, the server) is, and this
extension builds on that premise.
:::

::: {#settings .section}
#### Settings[¶](#settings "Permalink to this heading"){.headerlink}

The settings used to control the AutoThrottle extension are:

-   [[`AUTOTHROTTLE_ENABLED`{.xref .std .std-setting .docutils .literal
    .notranslate}]{.pre}](#std-setting-AUTOTHROTTLE_ENABLED){.hoverxref
    .tooltip .reference .internal}

-   [[`AUTOTHROTTLE_START_DELAY`{.xref .std .std-setting .docutils
    .literal
    .notranslate}]{.pre}](#std-setting-AUTOTHROTTLE_START_DELAY){.hoverxref
    .tooltip .reference .internal}

-   [[`AUTOTHROTTLE_MAX_DELAY`{.xref .std .std-setting .docutils
    .literal
    .notranslate}]{.pre}](#std-setting-AUTOTHROTTLE_MAX_DELAY){.hoverxref
    .tooltip .reference .internal}

-   [[`AUTOTHROTTLE_TARGET_CONCURRENCY`{.xref .std .std-setting
    .docutils .literal
    .notranslate}]{.pre}](#std-setting-AUTOTHROTTLE_TARGET_CONCURRENCY){.hoverxref
    .tooltip .reference .internal}

-   [[`AUTOTHROTTLE_DEBUG`{.xref .std .std-setting .docutils .literal
    .notranslate}]{.pre}](#std-setting-AUTOTHROTTLE_DEBUG){.hoverxref
    .tooltip .reference .internal}

-   [[`CONCURRENT_REQUESTS_PER_DOMAIN`{.xref .std .std-setting .docutils
    .literal
    .notranslate}]{.pre}](index.html#std-setting-CONCURRENT_REQUESTS_PER_DOMAIN){.hoverxref
    .tooltip .reference .internal}

-   [[`CONCURRENT_REQUESTS_PER_IP`{.xref .std .std-setting .docutils
    .literal
    .notranslate}]{.pre}](index.html#std-setting-CONCURRENT_REQUESTS_PER_IP){.hoverxref
    .tooltip .reference .internal}

-   [[`DOWNLOAD_DELAY`{.xref .std .std-setting .docutils .literal
    .notranslate}]{.pre}](index.html#std-setting-DOWNLOAD_DELAY){.hoverxref
    .tooltip .reference .internal}

For more information see [[How it works]{.std
.std-ref}](#autothrottle-algorithm){.hoverxref .tooltip .reference
.internal}.

::: {#autothrottle-enabled .section}
[]{#std-setting-AUTOTHROTTLE_ENABLED}

##### AUTOTHROTTLE_ENABLED[¶](#autothrottle-enabled "Permalink to this heading"){.headerlink}

Default: [`False`{.docutils .literal .notranslate}]{.pre}

Enables the AutoThrottle extension.
:::

::: {#autothrottle-start-delay .section}
[]{#std-setting-AUTOTHROTTLE_START_DELAY}

##### AUTOTHROTTLE_START_DELAY[¶](#autothrottle-start-delay "Permalink to this heading"){.headerlink}

Default: [`5.0`{.docutils .literal .notranslate}]{.pre}

The initial download delay (in seconds).
:::

::: {#autothrottle-max-delay .section}
[]{#std-setting-AUTOTHROTTLE_MAX_DELAY}

##### AUTOTHROTTLE_MAX_DELAY[¶](#autothrottle-max-delay "Permalink to this heading"){.headerlink}

Default: [`60.0`{.docutils .literal .notranslate}]{.pre}

The maximum download delay (in seconds) to be set in case of high
latencies.
:::

::: {#autothrottle-target-concurrency .section}
[]{#std-setting-AUTOTHROTTLE_TARGET_CONCURRENCY}

##### AUTOTHROTTLE_TARGET_CONCURRENCY[¶](#autothrottle-target-concurrency "Permalink to this heading"){.headerlink}

Default: [`1.0`{.docutils .literal .notranslate}]{.pre}

Average number of requests Scrapy should be sending in parallel to
remote websites.

By default, AutoThrottle adjusts the delay to send a single concurrent
request to each of the remote websites. Set this option to a higher
value (e.g. [`2.0`{.docutils .literal .notranslate}]{.pre}) to increase
the throughput and the load on remote servers. A lower
[`AUTOTHROTTLE_TARGET_CONCURRENCY`{.docutils .literal
.notranslate}]{.pre} value (e.g. [`0.5`{.docutils .literal
.notranslate}]{.pre}) makes the crawler more conservative and polite.

Note that [[`CONCURRENT_REQUESTS_PER_DOMAIN`{.xref .std .std-setting
.docutils .literal
.notranslate}]{.pre}](index.html#std-setting-CONCURRENT_REQUESTS_PER_DOMAIN){.hoverxref
.tooltip .reference .internal} and [[`CONCURRENT_REQUESTS_PER_IP`{.xref
.std .std-setting .docutils .literal
.notranslate}]{.pre}](index.html#std-setting-CONCURRENT_REQUESTS_PER_IP){.hoverxref
.tooltip .reference .internal} options are still respected when
AutoThrottle extension is enabled. This means that if
[`AUTOTHROTTLE_TARGET_CONCURRENCY`{.docutils .literal
.notranslate}]{.pre} is set to a value higher than
[[`CONCURRENT_REQUESTS_PER_DOMAIN`{.xref .std .std-setting .docutils
.literal
.notranslate}]{.pre}](index.html#std-setting-CONCURRENT_REQUESTS_PER_DOMAIN){.hoverxref
.tooltip .reference .internal} or [[`CONCURRENT_REQUESTS_PER_IP`{.xref
.std .std-setting .docutils .literal
.notranslate}]{.pre}](index.html#std-setting-CONCURRENT_REQUESTS_PER_IP){.hoverxref
.tooltip .reference .internal}, the crawler won't reach this number of
concurrent requests.

At every given time point Scrapy can be sending more or less concurrent
requests than [`AUTOTHROTTLE_TARGET_CONCURRENCY`{.docutils .literal
.notranslate}]{.pre}; it is a suggested value the crawler tries to
approach, not a hard limit.
:::

::: {#autothrottle-debug .section}
[]{#std-setting-AUTOTHROTTLE_DEBUG}

##### AUTOTHROTTLE_DEBUG[¶](#autothrottle-debug "Permalink to this heading"){.headerlink}

Default: [`False`{.docutils .literal .notranslate}]{.pre}

Enable AutoThrottle debug mode which will display stats on every
response received, so you can see how the throttling parameters are
being adjusted in real time.
:::
:::
:::

[]{#document-topics/benchmarking}

::: {#benchmarking .section}
[]{#id1}

### Benchmarking[¶](#benchmarking "Permalink to this heading"){.headerlink}

Scrapy comes with a simple benchmarking suite that spawns a local HTTP
server and crawls it at the maximum possible speed. The goal of this
benchmarking is to get an idea of how Scrapy performs in your hardware,
in order to have a common baseline for comparisons. It uses a simple
spider that does nothing and just follows links.

To run it use:

::: {.highlight-default .notranslate}
::: highlight
    scrapy bench
:::
:::

You should see an output like this:

::: {.highlight-default .notranslate}
::: highlight
    2016-12-16 21:18:48 [scrapy.utils.log] INFO: Scrapy 1.2.2 started (bot: quotesbot)
    2016-12-16 21:18:48 [scrapy.utils.log] INFO: Overridden settings: {'CLOSESPIDER_TIMEOUT': 10, 'ROBOTSTXT_OBEY': True, 'SPIDER_MODULES': ['quotesbot.spiders'], 'LOGSTATS_INTERVAL': 1, 'BOT_NAME': 'quotesbot', 'LOG_LEVEL': 'INFO', 'NEWSPIDER_MODULE': 'quotesbot.spiders'}
    2016-12-16 21:18:49 [scrapy.middleware] INFO: Enabled extensions:
    ['scrapy.extensions.closespider.CloseSpider',
     'scrapy.extensions.logstats.LogStats',
     'scrapy.extensions.telnet.TelnetConsole',
     'scrapy.extensions.corestats.CoreStats']
    2016-12-16 21:18:49 [scrapy.middleware] INFO: Enabled downloader middlewares:
    ['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',
     'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
     'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
     'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
     'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
     'scrapy.downloadermiddlewares.retry.RetryMiddleware',
     'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
     'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
     'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
     'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
     'scrapy.downloadermiddlewares.stats.DownloaderStats']
    2016-12-16 21:18:49 [scrapy.middleware] INFO: Enabled spider middlewares:
    ['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
     'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',
     'scrapy.spidermiddlewares.referer.RefererMiddleware',
     'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
     'scrapy.spidermiddlewares.depth.DepthMiddleware']
    2016-12-16 21:18:49 [scrapy.middleware] INFO: Enabled item pipelines:
    []
    2016-12-16 21:18:49 [scrapy.core.engine] INFO: Spider opened
    2016-12-16 21:18:49 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
    2016-12-16 21:18:50 [scrapy.extensions.logstats] INFO: Crawled 70 pages (at 4200 pages/min), scraped 0 items (at 0 items/min)
    2016-12-16 21:18:51 [scrapy.extensions.logstats] INFO: Crawled 134 pages (at 3840 pages/min), scraped 0 items (at 0 items/min)
    2016-12-16 21:18:52 [scrapy.extensions.logstats] INFO: Crawled 198 pages (at 3840 pages/min), scraped 0 items (at 0 items/min)
    2016-12-16 21:18:53 [scrapy.extensions.logstats] INFO: Crawled 254 pages (at 3360 pages/min), scraped 0 items (at 0 items/min)
    2016-12-16 21:18:54 [scrapy.extensions.logstats] INFO: Crawled 302 pages (at 2880 pages/min), scraped 0 items (at 0 items/min)
    2016-12-16 21:18:55 [scrapy.extensions.logstats] INFO: Crawled 358 pages (at 3360 pages/min), scraped 0 items (at 0 items/min)
    2016-12-16 21:18:56 [scrapy.extensions.logstats] INFO: Crawled 406 pages (at 2880 pages/min), scraped 0 items (at 0 items/min)
    2016-12-16 21:18:57 [scrapy.extensions.logstats] INFO: Crawled 438 pages (at 1920 pages/min), scraped 0 items (at 0 items/min)
    2016-12-16 21:18:58 [scrapy.extensions.logstats] INFO: Crawled 470 pages (at 1920 pages/min), scraped 0 items (at 0 items/min)
    2016-12-16 21:18:59 [scrapy.core.engine] INFO: Closing spider (closespider_timeout)
    2016-12-16 21:18:59 [scrapy.extensions.logstats] INFO: Crawled 518 pages (at 2880 pages/min), scraped 0 items (at 0 items/min)
    2016-12-16 21:19:00 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
    {'downloader/request_bytes': 229995,
     'downloader/request_count': 534,
     'downloader/request_method_count/GET': 534,
     'downloader/response_bytes': 1565504,
     'downloader/response_count': 534,
     'downloader/response_status_count/200': 534,
     'finish_reason': 'closespider_timeout',
     'finish_time': datetime.datetime(2016, 12, 16, 16, 19, 0, 647725),
     'log_count/INFO': 17,
     'request_depth_max': 19,
     'response_received_count': 534,
     'scheduler/dequeued': 533,
     'scheduler/dequeued/memory': 533,
     'scheduler/enqueued': 10661,
     'scheduler/enqueued/memory': 10661,
     'start_time': datetime.datetime(2016, 12, 16, 16, 18, 49, 799869)}
    2016-12-16 21:19:00 [scrapy.core.engine] INFO: Spider closed (closespider_timeout)
:::
:::

That tells you that Scrapy is able to crawl about 3000 pages per minute
in the hardware where you run it. Note that this is a very simple spider
intended to follow links, any custom spider you write will probably do
more stuff which results in slower crawl rates. How slower depends on
how much your spider does and how well it's written.

Use [scrapy-bench](https://github.com/scrapy/scrapy-bench){.reference
.external} for more complex benchmarking.
:::

[]{#document-topics/jobs}

::: {#jobs-pausing-and-resuming-crawls .section}
[]{#topics-jobs}

### Jobs: pausing and resuming crawls[¶](#jobs-pausing-and-resuming-crawls "Permalink to this heading"){.headerlink}

Sometimes, for big sites, it's desirable to pause crawls and be able to
resume them later.

Scrapy supports this functionality out of the box by providing the
following facilities:

-   a scheduler that persists scheduled requests on disk

-   a duplicates filter that persists visited requests on disk

-   an extension that keeps some spider state (key/value pairs)
    persistent between batches

::: {#job-directory .section}
#### Job directory[¶](#job-directory "Permalink to this heading"){.headerlink}

To enable persistence support you just need to define a *job directory*
through the [`JOBDIR`{.docutils .literal .notranslate}]{.pre} setting.
This directory will be for storing all required data to keep the state
of a single job (i.e. a spider run). It's important to note that this
directory must not be shared by different spiders, or even different
jobs/runs of the same spider, as it's meant to be used for storing the
state of a *single* job.
:::

::: {#how-to-use-it .section}
#### How to use it[¶](#how-to-use-it "Permalink to this heading"){.headerlink}

To start a spider with persistence support enabled, run it like this:

::: {.highlight-default .notranslate}
::: highlight
    scrapy crawl somespider -s JOBDIR=crawls/somespider-1
:::
:::

Then, you can stop the spider safely at any time (by pressing Ctrl-C or
sending a signal), and resume it later by issuing the same command:

::: {.highlight-default .notranslate}
::: highlight
    scrapy crawl somespider -s JOBDIR=crawls/somespider-1
:::
:::
:::

::: {#keeping-persistent-state-between-batches .section}
[]{#topics-keeping-persistent-state-between-batches}

#### Keeping persistent state between batches[¶](#keeping-persistent-state-between-batches "Permalink to this heading"){.headerlink}

Sometimes you'll want to keep some persistent spider state between
pause/resume batches. You can use the [`spider.state`{.docutils .literal
.notranslate}]{.pre} attribute for that, which should be a dict. There's
a built-in extension that takes care of serializing, storing and loading
that attribute from the job directory, when the spider starts and stops.

Here's an example of a callback that uses the spider state (other spider
code is omitted for brevity):

::: {.highlight-python .notranslate}
::: highlight
    def parse_item(self, response):
        # parse item here
        self.state["items_count"] = self.state.get("items_count", 0) + 1
:::
:::
:::

::: {#persistence-gotchas .section}
#### Persistence gotchas[¶](#persistence-gotchas "Permalink to this heading"){.headerlink}

There are a few things to keep in mind if you want to be able to use the
Scrapy persistence support:

::: {#cookies-expiration .section}
##### Cookies expiration[¶](#cookies-expiration "Permalink to this heading"){.headerlink}

Cookies may expire. So, if you don't resume your spider quickly the
requests scheduled may no longer work. This won't be an issue if your
spider doesn't rely on cookies.
:::

::: {#request-serialization .section}
[]{#id1}

##### Request serialization[¶](#request-serialization "Permalink to this heading"){.headerlink}

For persistence to work, [`Request`{.xref .py .py-class .docutils
.literal .notranslate}]{.pre} objects must be serializable with
[[`pickle`{.xref .py .py-mod .docutils .literal
.notranslate}]{.pre}](https://docs.python.org/3/library/pickle.html#module-pickle "(in Python v3.12)"){.reference
.external}, except for the [`callback`{.docutils .literal
.notranslate}]{.pre} and [`errback`{.docutils .literal
.notranslate}]{.pre} values passed to their [`__init__`{.docutils
.literal .notranslate}]{.pre} method, which must be methods of the
running [[`Spider`{.xref .py .py-class .docutils .literal
.notranslate}]{.pre}](index.html#scrapy.Spider "scrapy.Spider"){.reference
.internal} class.

If you wish to log the requests that couldn't be serialized, you can set
the [[`SCHEDULER_DEBUG`{.xref .std .std-setting .docutils .literal
.notranslate}]{.pre}](index.html#std-setting-SCHEDULER_DEBUG){.hoverxref
.tooltip .reference .internal} setting to [`True`{.docutils .literal
.notranslate}]{.pre} in the project's settings page. It is
[`False`{.docutils .literal .notranslate}]{.pre} by default.
:::
:::
:::

[]{#document-topics/coroutines}

::: {#coroutines .section}
[]{#topics-coroutines}

### Coroutines[¶](#coroutines "Permalink to this heading"){.headerlink}

::: versionadded
[New in version 2.0.]{.versionmodified .added}
:::

Scrapy has [[partial support]{.std
.std-ref}](#coroutine-support){.hoverxref .tooltip .reference .internal}
for the [[coroutine syntax]{.xref .std
.std-ref}](https://docs.python.org/3/reference/compound_stmts.html#async "(in Python v3.12)"){.reference
.external}.

::: {#supported-callables .section}
[]{#coroutine-support}

#### Supported callables[¶](#supported-callables "Permalink to this heading"){.headerlink}

The following callables may be defined as coroutines using
[`async`{.docutils .literal .notranslate}]{.pre}` `{.docutils .literal
.notranslate}[`def`{.docutils .literal .notranslate}]{.pre}, and hence
use coroutine syntax (e.g. [`await`{.docutils .literal
.notranslate}]{.pre}, [`async`{.docutils .literal
.notranslate}]{.pre}` `{.docutils .literal .notranslate}[`for`{.docutils
.literal .notranslate}]{.pre}, [`async`{.docutils .literal
.notranslate}]{.pre}` `{.docutils .literal
.notranslate}[`with`{.docutils .literal .notranslate}]{.pre}):

-   [`Request`{.xref .py .py-class .docutils .literal
    .notranslate}]{.pre} callbacks.

    If you are using any custom or third-party [[spider middleware]{.std
    .std-ref}](index.html#topics-spider-middleware){.hoverxref .tooltip
    .reference .internal}, see [[Mixing synchronous and asynchronous
    spider middlewares]{.std
    .std-ref}](#sync-async-spider-middleware){.hoverxref .tooltip
    .reference .internal}.

    ::: versionchanged
    [Changed in version 2.7: ]{.versionmodified .changed}Output of async
    callbacks is now processed asynchronously instead of collecting all
    of it first.
    :::

-   The [[`process_item()`{.xref .py .py-meth .docutils .literal
    .notranslate}]{.pre}](index.html#process_item "process_item"){.reference
    .internal} method of [[item pipelines]{.std
    .std-ref}](index.html#topics-item-pipeline){.hoverxref .tooltip
    .reference .internal}.

-   The [[`process_request()`{.xref .py .py-meth .docutils .literal
    .notranslate}]{.pre}](index.html#scrapy.downloadermiddlewares.DownloaderMiddleware.process_request "scrapy.downloadermiddlewares.DownloaderMiddleware.process_request"){.reference
    .internal}, [[`process_response()`{.xref .py .py-meth .docutils
    .literal
    .notranslate}]{.pre}](index.html#scrapy.downloadermiddlewares.DownloaderMiddleware.process_response "scrapy.downloadermiddlewares.DownloaderMiddleware.process_response"){.reference
    .internal}, and [[`process_exception()`{.xref .py .py-meth .docutils
    .literal
    .notranslate}]{.pre}](index.html#scrapy.downloadermiddlewares.DownloaderMiddleware.process_exception "scrapy.downloadermiddlewares.DownloaderMiddleware.process_exception"){.reference
    .internal} methods of [[downloader middlewares]{.std
    .std-ref}](index.html#topics-downloader-middleware-custom){.hoverxref
    .tooltip .reference .internal}.

-   [[Signal handlers that support deferreds]{.std
    .std-ref}](index.html#signal-deferred){.hoverxref .tooltip
    .reference .internal}.

-   The [[`process_spider_output()`{.xref .py .py-meth .docutils
    .literal
    .notranslate}]{.pre}](index.html#scrapy.spidermiddlewares.SpiderMiddleware.process_spider_output "scrapy.spidermiddlewares.SpiderMiddleware.process_spider_output"){.reference
    .internal} method of [[spider middlewares]{.std
    .std-ref}](index.html#topics-spider-middleware){.hoverxref .tooltip
    .reference .internal}.

    It must be defined as an [[asynchronous generator]{.xref .std
    .std-term}](https://docs.python.org/3/glossary.html#term-asynchronous-generator "(in Python v3.12)"){.reference
    .external}. The input [`result`{.docutils .literal
    .notranslate}]{.pre} parameter is an [[asynchronous iterable]{.xref
    .std
    .std-term}](https://docs.python.org/3/glossary.html#term-asynchronous-iterable "(in Python v3.12)"){.reference
    .external}.

    See also [[Mixing synchronous and asynchronous spider
    middlewares]{.std
    .std-ref}](#sync-async-spider-middleware){.hoverxref .tooltip
    .reference .internal} and [[Universal spider middlewares]{.std
    .std-ref}](#universal-spider-middleware){.hoverxref .tooltip
    .reference .internal}.

    ::: versionadded
    [New in version 2.7.]{.versionmodified .added}
    :::
:::

::: {#general-usage .section}
#### General usage[¶](#general-usage "Permalink to this heading"){.headerlink}

There are several use cases for coroutines in Scrapy.

Code that would return Deferreds when written for previous Scrapy
versions, such as downloader middlewares and signal handlers, can be
rewritten to be shorter and cleaner:

::: {.highlight-python .notranslate}
::: highlight
    from itemadapter import ItemAdapter


    class DbPipeline:
        def _update_item(self, data, item):
            adapter = ItemAdapter(item)
            adapter["field"] = data
            return item

        def process_item(self, item, spider):
            adapter = ItemAdapter(item)
            dfd = db.get_some_data(adapter["id"])
            dfd.addCallback(self._update_item, item)
            return dfd
:::
:::

becomes:

::: {.highlight-python .notranslate}
::: highlight
    from itemadapter import ItemAdapter


    class DbPipeline:
        async def process_item(self, item, spider):
            adapter = ItemAdapter(item)
            adapter["field"] = await db.get_some_data(adapter["id"])
            return item
:::
:::

Coroutines may be used to call asynchronous code. This includes other
coroutines, functions that return Deferreds and functions that return
[[awaitable objects]{.xref .std
.std-term}](https://docs.python.org/3/glossary.html#term-awaitable "(in Python v3.12)"){.reference
.external} such as [[`Future`{.xref .py .py-class .docutils .literal
.notranslate}]{.pre}](https://docs.python.org/3/library/asyncio-future.html#asyncio.Future "(in Python v3.12)"){.reference
.external}. This means you can use many useful Python libraries
providing such code:

::: {.highlight-python .notranslate}
::: highlight
    class MySpiderDeferred(Spider):
        # ...
        async def parse(self, response):
            additional_response = await treq.get("https://additional.url")
            additional_data = await treq.content(additional_response)
            # ... use response and additional_data to yield items and requests


    class MySpiderAsyncio(Spider):
        # ...
        async def parse(self, response):
            async with aiohttp.ClientSession() as session:
                async with session.get("https://additional.url") as additional_response:
                    additional_data = await additional_response.text()
            # ... use response and additional_data to yield items and requests
:::
:::

::: {.admonition .note}
Note

Many libraries that use coroutines, such as
[aio-libs](https://github.com/aio-libs){.reference .external}, require
the [[`asyncio`{.xref .py .py-mod .docutils .literal
.notranslate}]{.pre}](https://docs.python.org/3/library/asyncio.html#module-asyncio "(in Python v3.12)"){.reference
.external} loop and to use them you need to [[enable asyncio support in
Scrapy]{.doc}](index.html#document-topics/asyncio){.reference
.internal}.
:::

::: {.admonition .note}
Note

If you want to [`await`{.docutils .literal .notranslate}]{.pre} on
Deferreds while using the asyncio reactor, you need to [[wrap them]{.std
.std-ref}](index.html#asyncio-await-dfd){.hoverxref .tooltip .reference
.internal}.
:::

Common use cases for asynchronous code include:

-   requesting data from websites, databases and other services (in
    callbacks, pipelines and middlewares);

-   storing data in databases (in pipelines and middlewares);

-   delaying the spider initialization until some external event (in the
    [[`spider_opened`{.xref .std .std-signal .docutils .literal
    .notranslate}]{.pre}](index.html#std-signal-spider_opened){.hoverxref
    .tooltip .reference .internal} handler);

-   calling asynchronous Scrapy methods like
    [`ExecutionEngine.download()`{.xref .py .py-meth .docutils .literal
    .notranslate}]{.pre} (see [[the screenshot pipeline example]{.std
    .std-ref}](index.html#screenshotpipeline){.hoverxref .tooltip
    .reference .internal}).
:::

::: {#inline-requests .section}
[]{#id1}

#### Inline requests[¶](#inline-requests "Permalink to this heading"){.headerlink}

The spider below shows how to send a request and await its response all
from within a spider callback:

::: {.highlight-python .notranslate}
::: highlight
    from scrapy import Spider, Request
    from scrapy.utils.defer import maybe_deferred_to_future


    class SingleRequestSpider(Spider):
        name = "single"
        start_urls = ["https://example.org/product"]

        async def parse(self, response, **kwargs):
            additional_request = Request("https://example.org/price")
            deferred = self.crawler.engine.download(additional_request)
            additional_response = await maybe_deferred_to_future(deferred)
            yield {
                "h1": response.css("h1").get(),
                "price": additional_response.css("#price").get(),
            }
:::
:::

You can also send multiple requests in parallel:

::: {.highlight-python .notranslate}
::: highlight
    from scrapy import Spider, Request
    from scrapy.utils.defer import maybe_deferred_to_future
    from twisted.internet.defer import DeferredList


    class MultipleRequestsSpider(Spider):
        name = "multiple"
        start_urls = ["https://example.com/product"]

        async def parse(self, response, **kwargs):
            additional_requests = [
                Request("https://example.com/price"),
                Request("https://example.com/color"),
            ]
            deferreds = []
            for r in additional_requests:
                deferred = self.crawler.engine.download(r)
                deferreds.append(deferred)
            responses = await maybe_deferred_to_future(DeferredList(deferreds))
            yield {
                "h1": response.css("h1::text").get(),
                "price": responses[0][1].css(".price::text").get(),
                "price2": responses[1][1].css(".color::text").get(),
            }
:::
:::
:::

::: {#mixing-synchronous-and-asynchronous-spider-middlewares .section}
[]{#sync-async-spider-middleware}

#### Mixing synchronous and asynchronous spider middlewares[¶](#mixing-synchronous-and-asynchronous-spider-middlewares "Permalink to this heading"){.headerlink}

::: versionadded
[New in version 2.7.]{.versionmodified .added}
:::

The output of a [`Request`{.xref .py .py-class .docutils .literal
.notranslate}]{.pre} callback is passed as the [`result`{.docutils
.literal .notranslate}]{.pre} parameter to the
[[`process_spider_output()`{.xref .py .py-meth .docutils .literal
.notranslate}]{.pre}](index.html#scrapy.spidermiddlewares.SpiderMiddleware.process_spider_output "scrapy.spidermiddlewares.SpiderMiddleware.process_spider_output"){.reference
.internal} method of the first [[spider middleware]{.std
.std-ref}](index.html#topics-spider-middleware){.hoverxref .tooltip
.reference .internal} from the [[list of active spider middlewares]{.std
.std-ref}](index.html#topics-spider-middleware-setting){.hoverxref
.tooltip .reference .internal}. Then the output of that
[`process_spider_output`{.docutils .literal .notranslate}]{.pre} method
is passed to the [`process_spider_output`{.docutils .literal
.notranslate}]{.pre} method of the next spider middleware, and so on for
every active spider middleware.

Scrapy supports mixing [[coroutine methods]{.xref .std
.std-ref}](https://docs.python.org/3/reference/compound_stmts.html#async "(in Python v3.12)"){.reference
.external} and synchronous methods in this chain of calls.

However, if any of the [`process_spider_output`{.docutils .literal
.notranslate}]{.pre} methods is defined as a synchronous method, and the
previous [`Request`{.docutils .literal .notranslate}]{.pre} callback or
[`process_spider_output`{.docutils .literal .notranslate}]{.pre} method
is a coroutine, there are some drawbacks to the
asynchronous-to-synchronous conversion that Scrapy does so that the
synchronous [`process_spider_output`{.docutils .literal
.notranslate}]{.pre} method gets a synchronous iterable as its
[`result`{.docutils .literal .notranslate}]{.pre} parameter:

-   The whole output of the previous [`Request`{.docutils .literal
    .notranslate}]{.pre} callback or [`process_spider_output`{.docutils
    .literal .notranslate}]{.pre} method is awaited at this point.

-   If an exception raises while awaiting the output of the previous
    [`Request`{.docutils .literal .notranslate}]{.pre} callback or
    [`process_spider_output`{.docutils .literal .notranslate}]{.pre}
    method, none of that output will be processed.

    This contrasts with the regular behavior, where all items yielded
    before an exception raises are processed.

Asynchronous-to-synchronous conversions are supported for backward
compatibility, but they are deprecated and will stop working in a future
version of Scrapy.

To avoid asynchronous-to-synchronous conversions, when defining
[`Request`{.docutils .literal .notranslate}]{.pre} callbacks as
coroutine methods or when using spider middlewares whose
[`process_spider_output`{.docutils .literal .notranslate}]{.pre} method
is an [[asynchronous generator]{.xref .std
.std-term}](https://docs.python.org/3/glossary.html#term-asynchronous-generator "(in Python v3.12)"){.reference
.external}, all active spider middlewares must either have their
[`process_spider_output`{.docutils .literal .notranslate}]{.pre} method
defined as an asynchronous generator or [[define a
process_spider_output_async method]{.std
.std-ref}](#universal-spider-middleware){.hoverxref .tooltip .reference
.internal}.

::: {.admonition .note}
Note

When using third-party spider middlewares that only define a synchronous
[`process_spider_output`{.docutils .literal .notranslate}]{.pre} method,
consider [[making them universal]{.std
.std-ref}](#universal-spider-middleware){.hoverxref .tooltip .reference
.internal} through [[subclassing]{.xref .std
.std-ref}](https://docs.python.org/3/tutorial/classes.html#tut-inheritance "(in Python v3.12)"){.reference
.external}.
:::
:::

::: {#universal-spider-middlewares .section}
[]{#universal-spider-middleware}

#### Universal spider middlewares[¶](#universal-spider-middlewares "Permalink to this heading"){.headerlink}

::: versionadded
[New in version 2.7.]{.versionmodified .added}
:::

To allow writing a spider middleware that supports asynchronous
execution of its [`process_spider_output`{.docutils .literal
.notranslate}]{.pre} method in Scrapy 2.7 and later (avoiding
[[asynchronous-to-synchronous conversions]{.std
.std-ref}](#sync-async-spider-middleware){.hoverxref .tooltip .reference
.internal}) while maintaining support for older Scrapy versions, you may
define [`process_spider_output`{.docutils .literal .notranslate}]{.pre}
as a synchronous method and define an [[asynchronous generator]{.xref
.std
.std-term}](https://docs.python.org/3/glossary.html#term-asynchronous-generator "(in Python v3.12)"){.reference
.external} version of that method with an alternative name:
[`process_spider_output_async`{.docutils .literal .notranslate}]{.pre}.

For example:

::: {.highlight-python .notranslate}
::: highlight
    class UniversalSpiderMiddleware:
        def process_spider_output(self, response, result, spider):
            for r in result:
                # ... do something with r
                yield r

        async def process_spider_output_async(self, response, result, spider):
            async for r in result:
                # ... do something with r
                yield r
:::
:::

::: {.admonition .note}
Note

This is an interim measure to allow, for a time, to write code that
works in Scrapy 2.7 and later without requiring
asynchronous-to-synchronous conversions, and works in earlier Scrapy
versions as well.

In some future version of Scrapy, however, this feature will be
deprecated and, eventually, in a later version of Scrapy, this feature
will be removed, and all spider middlewares will be expected to define
their [`process_spider_output`{.docutils .literal .notranslate}]{.pre}
method as an asynchronous generator.
:::
:::
:::

[]{#document-topics/asyncio}

::: {#asyncio .section}
[]{#using-asyncio}

### asyncio[¶](#asyncio "Permalink to this heading"){.headerlink}

::: versionadded
[New in version 2.0.]{.versionmodified .added}
:::

Scrapy has partial support for [[`asyncio`{.xref .py .py-mod .docutils
.literal
.notranslate}]{.pre}](https://docs.python.org/3/library/asyncio.html#module-asyncio "(in Python v3.12)"){.reference
.external}. After you [[install the asyncio reactor]{.std
.std-ref}](#install-asyncio){.hoverxref .tooltip .reference .internal},
you may use [[`asyncio`{.xref .py .py-mod .docutils .literal
.notranslate}]{.pre}](https://docs.python.org/3/library/asyncio.html#module-asyncio "(in Python v3.12)"){.reference
.external} and [[`asyncio`{.xref .py .py-mod .docutils .literal
.notranslate}]{.pre}](https://docs.python.org/3/library/asyncio.html#module-asyncio "(in Python v3.12)"){.reference
.external}-powered libraries in any
[[coroutine]{.doc}](index.html#document-topics/coroutines){.reference
.internal}.

::: {#installing-the-asyncio-reactor .section}
[]{#install-asyncio}

#### Installing the asyncio reactor[¶](#installing-the-asyncio-reactor "Permalink to this heading"){.headerlink}

To enable [[`asyncio`{.xref .py .py-mod .docutils .literal
.notranslate}]{.pre}](https://docs.python.org/3/library/asyncio.html#module-asyncio "(in Python v3.12)"){.reference
.external} support, set the [[`TWISTED_REACTOR`{.xref .std .std-setting
.docutils .literal
.notranslate}]{.pre}](index.html#std-setting-TWISTED_REACTOR){.hoverxref
.tooltip .reference .internal} setting to
[`'twisted.internet.asyncioreactor.AsyncioSelectorReactor'`{.docutils
.literal .notranslate}]{.pre}.

If you are using [[`CrawlerRunner`{.xref .py .py-class .docutils
.literal
.notranslate}]{.pre}](index.html#scrapy.crawler.CrawlerRunner "scrapy.crawler.CrawlerRunner"){.reference
.internal}, you also need to install the
[[`AsyncioSelectorReactor`{.xref .py .py-class .docutils .literal
.notranslate}]{.pre}](https://docs.twisted.org/en/stable/api/twisted.internet.asyncioreactor.AsyncioSelectorReactor.html "(in Twisted)"){.reference
.external} reactor manually. You can do that using
[[`install_reactor()`{.xref .py .py-func .docutils .literal
.notranslate}]{.pre}](index.html#scrapy.utils.reactor.install_reactor "scrapy.utils.reactor.install_reactor"){.reference
.internal}:

::: {.highlight-default .notranslate}
::: highlight
    install_reactor('twisted.internet.asyncioreactor.AsyncioSelectorReactor')
:::
:::
:::

::: {#handling-a-pre-installed-reactor .section}
[]{#asyncio-preinstalled-reactor}

#### Handling a pre-installed reactor[¶](#handling-a-pre-installed-reactor "Permalink to this heading"){.headerlink}

[`twisted.internet.reactor`{.docutils .literal .notranslate}]{.pre} and
some other Twisted imports install the default Twisted reactor as a side
effect. Once a Twisted reactor is installed, it is not possible to
switch to a different reactor at run time.

If you [[configure the asyncio Twisted reactor]{.std
.std-ref}](#install-asyncio){.hoverxref .tooltip .reference .internal}
and, at run time, Scrapy complains that a different reactor is already
installed, chances are you have some such imports in your code.

You can usually fix the issue by moving those offending module-level
Twisted imports to the method or function definitions where they are
used. For example, if you have something like:

::: {.highlight-python .notranslate}
::: highlight
    from twisted.internet import reactor


    def my_function():
        reactor.callLater(...)
:::
:::

Switch to something like:

::: {.highlight-python .notranslate}
::: highlight
    def my_function():
        from twisted.internet import reactor

        reactor.callLater(...)
:::
:::

Alternatively, you can try to [[manually install the asyncio
reactor]{.std .std-ref}](#install-asyncio){.hoverxref .tooltip
.reference .internal}, with [[`install_reactor()`{.xref .py .py-func
.docutils .literal
.notranslate}]{.pre}](index.html#scrapy.utils.reactor.install_reactor "scrapy.utils.reactor.install_reactor"){.reference
.internal}, before those imports happen.
:::

::: {#awaiting-on-deferreds .section}
[]{#asyncio-await-dfd}

#### Awaiting on Deferreds[¶](#awaiting-on-deferreds "Permalink to this heading"){.headerlink}

When the asyncio reactor isn't installed, you can await on Deferreds in
the coroutines directly. When it is installed, this is not possible
anymore, due to specifics of the Scrapy coroutine integration (the
coroutines are wrapped into [[`asyncio.Future`{.xref .py .py-class
.docutils .literal
.notranslate}]{.pre}](https://docs.python.org/3/library/asyncio-future.html#asyncio.Future "(in Python v3.12)"){.reference
.external} objects, not into [[`Deferred`{.xref .py .py-class .docutils
.literal
.notranslate}]{.pre}](https://docs.twisted.org/en/stable/api/twisted.internet.defer.Deferred.html "(in Twisted)"){.reference
.external} directly), and you need to wrap them into Futures. Scrapy
provides two helpers for this:

[[scrapy.utils.defer.]{.pre}]{.sig-prename .descclassname}[[deferred_to_future]{.pre}]{.sig-name .descname}[(]{.sig-paren}*[[d]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[[[Deferred]{.pre}](https://docs.twisted.org/en/stable/api/twisted.internet.defer.Deferred.html "(in Twisted)"){.reference .external}]{.n}*[)]{.sig-paren} [[→]{.sig-return-icon} [[Future]{.pre}]{.sig-return-typehint}]{.sig-return}[[[\[source\]]{.pre}]{.viewcode-link}](_modules/scrapy/utils/defer.html#deferred_to_future){.reference .internal}[¶](#scrapy.utils.defer.deferred_to_future "Permalink to this definition"){.headerlink}

:   ::: versionadded
    [New in version 2.6.0.]{.versionmodified .added}
    :::

    Return an [[`asyncio.Future`{.xref .py .py-class .docutils .literal
    .notranslate}]{.pre}](https://docs.python.org/3/library/asyncio-future.html#asyncio.Future "(in Python v3.12)"){.reference
    .external} object that wraps *d*.

    When [[using the asyncio reactor]{.std
    .std-ref}](#install-asyncio){.hoverxref .tooltip .reference
    .internal}, you cannot await on [[`Deferred`{.xref .py .py-class
    .docutils .literal
    .notranslate}]{.pre}](https://docs.twisted.org/en/stable/api/twisted.internet.defer.Deferred.html "(in Twisted)"){.reference
    .external} objects from [[Scrapy callables defined as
    coroutines]{.std .std-ref}](index.html#coroutine-support){.hoverxref
    .tooltip .reference .internal}, you can only await on
    [`Future`{.docutils .literal .notranslate}]{.pre} objects. Wrapping
    [`Deferred`{.docutils .literal .notranslate}]{.pre} objects into
    [`Future`{.docutils .literal .notranslate}]{.pre} objects allows you
    to wait on them:

    ::: {.highlight-default .notranslate}
    ::: highlight
        class MySpider(Spider):
            ...
            async def parse(self, response):
                additional_request = scrapy.Request('https://example.org/price')
                deferred = self.crawler.engine.download(additional_request)
                additional_response = await deferred_to_future(deferred)
    :::
    :::

```{=html}
<!-- -->
```

[[scrapy.utils.defer.]{.pre}]{.sig-prename .descclassname}[[maybe_deferred_to_future]{.pre}]{.sig-name .descname}[(]{.sig-paren}*[[d]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[[[Deferred]{.pre}](https://docs.twisted.org/en/stable/api/twisted.internet.defer.Deferred.html "(in Twisted)"){.reference .external}]{.n}*[)]{.sig-paren} [[→]{.sig-return-icon} [[[Union]{.pre}](https://docs.python.org/3/library/typing.html#typing.Union "(in Python v3.12)"){.reference .external}[[\[]{.pre}]{.p}[[Deferred]{.pre}](https://docs.twisted.org/en/stable/api/twisted.internet.defer.Deferred.html "(in Twisted)"){.reference .external}[[,]{.pre}]{.p}[ ]{.w}[Future]{.pre}[[\]]{.pre}]{.p}]{.sig-return-typehint}]{.sig-return}[[[\[source\]]{.pre}]{.viewcode-link}](_modules/scrapy/utils/defer.html#maybe_deferred_to_future){.reference .internal}[¶](#scrapy.utils.defer.maybe_deferred_to_future "Permalink to this definition"){.headerlink}

:   ::: versionadded
    [New in version 2.6.0.]{.versionmodified .added}
    :::

    Return *d* as an object that can be awaited from a [[Scrapy callable
    defined as a coroutine]{.std
    .std-ref}](index.html#coroutine-support){.hoverxref .tooltip
    .reference .internal}.

    What you can await in Scrapy callables defined as coroutines depends
    on the value of [[`TWISTED_REACTOR`{.xref .std .std-setting
    .docutils .literal
    .notranslate}]{.pre}](index.html#std-setting-TWISTED_REACTOR){.hoverxref
    .tooltip .reference .internal}:

    -   When not using the asyncio reactor, you can only await on
        [[`Deferred`{.xref .py .py-class .docutils .literal
        .notranslate}]{.pre}](https://docs.twisted.org/en/stable/api/twisted.internet.defer.Deferred.html "(in Twisted)"){.reference
        .external} objects.

    -   When [[using the asyncio reactor]{.std
        .std-ref}](#install-asyncio){.hoverxref .tooltip .reference
        .internal}, you can only await on [[`asyncio.Future`{.xref .py
        .py-class .docutils .literal
        .notranslate}]{.pre}](https://docs.python.org/3/library/asyncio-future.html#asyncio.Future "(in Python v3.12)"){.reference
        .external} objects.

    If you want to write code that uses [`Deferred`{.docutils .literal
    .notranslate}]{.pre} objects but works with any reactor, use this
    function on all [`Deferred`{.docutils .literal .notranslate}]{.pre}
    objects:

    ::: {.highlight-default .notranslate}
    ::: highlight
        class MySpider(Spider):
            ...
            async def parse(self, response):
                additional_request = scrapy.Request('https://example.org/price')
                deferred = self.crawler.engine.download(additional_request)
                additional_response = await maybe_deferred_to_future(deferred)
    :::
    :::

::: {.admonition .tip}
Tip

If you need to use these functions in code that aims to be compatible
with lower versions of Scrapy that do not provide these functions, down
to Scrapy 2.0 (earlier versions do not support [[`asyncio`{.xref .py
.py-mod .docutils .literal
.notranslate}]{.pre}](https://docs.python.org/3/library/asyncio.html#module-asyncio "(in Python v3.12)"){.reference
.external}), you can copy the implementation of these functions into
your own code.
:::
:::

::: {#enforcing-asyncio-as-a-requirement .section}
[]{#enforce-asyncio-requirement}

#### Enforcing asyncio as a requirement[¶](#enforcing-asyncio-as-a-requirement "Permalink to this heading"){.headerlink}

If you are writing a [[component]{.std
.std-ref}](index.html#topics-components){.hoverxref .tooltip .reference
.internal} that requires asyncio to work, use
[`scrapy.utils.reactor.is_asyncio_reactor_installed()`{.xref .py
.py-func .docutils .literal .notranslate}]{.pre} to [[enforce it as a
requirement]{.std
.std-ref}](index.html#enforce-component-requirements){.hoverxref
.tooltip .reference .internal}. For example:

::: {.highlight-python .notranslate}
::: highlight
    from scrapy.utils.reactor import is_asyncio_reactor_installed


    class MyComponent:
        def __init__(self):
            if not is_asyncio_reactor_installed():
                raise ValueError(
                    f"{MyComponent.__qualname__} requires the asyncio Twisted "
                    f"reactor. Make sure you have it configured in the "
                    f"TWISTED_REACTOR setting. See the asyncio documentation "
                    f"of Scrapy for more information."
                )
:::
:::
:::

::: {#windows-specific-notes .section}
[]{#asyncio-windows}

#### Windows-specific notes[¶](#windows-specific-notes "Permalink to this heading"){.headerlink}

The Windows implementation of [[`asyncio`{.xref .py .py-mod .docutils
.literal
.notranslate}]{.pre}](https://docs.python.org/3/library/asyncio.html#module-asyncio "(in Python v3.12)"){.reference
.external} can use two event loop implementations,
[[`ProactorEventLoop`{.xref .py .py-class .docutils .literal
.notranslate}]{.pre}](https://docs.python.org/3/library/asyncio-eventloop.html#asyncio.ProactorEventLoop "(in Python v3.12)"){.reference
.external} (default) and [[`SelectorEventLoop`{.xref .py .py-class
.docutils .literal
.notranslate}]{.pre}](https://docs.python.org/3/library/asyncio-eventloop.html#asyncio.SelectorEventLoop "(in Python v3.12)"){.reference
.external}. However, only [[`SelectorEventLoop`{.xref .py .py-class
.docutils .literal
.notranslate}]{.pre}](https://docs.python.org/3/library/asyncio-eventloop.html#asyncio.SelectorEventLoop "(in Python v3.12)"){.reference
.external} works with Twisted.

Scrapy changes the event loop class to [[`SelectorEventLoop`{.xref .py
.py-class .docutils .literal
.notranslate}]{.pre}](https://docs.python.org/3/library/asyncio-eventloop.html#asyncio.SelectorEventLoop "(in Python v3.12)"){.reference
.external} automatically when you change the [[`TWISTED_REACTOR`{.xref
.std .std-setting .docutils .literal
.notranslate}]{.pre}](index.html#std-setting-TWISTED_REACTOR){.hoverxref
.tooltip .reference .internal} setting or call
[[`install_reactor()`{.xref .py .py-func .docutils .literal
.notranslate}]{.pre}](index.html#scrapy.utils.reactor.install_reactor "scrapy.utils.reactor.install_reactor"){.reference
.internal}.

::: {.admonition .note}
Note

Other libraries you use may require [[`ProactorEventLoop`{.xref .py
.py-class .docutils .literal
.notranslate}]{.pre}](https://docs.python.org/3/library/asyncio-eventloop.html#asyncio.ProactorEventLoop "(in Python v3.12)"){.reference
.external}, e.g. because it supports subprocesses (this is the case with
[playwright](https://github.com/microsoft/playwright-python){.reference
.external}), so you cannot use them together with Scrapy on Windows (but
you should be able to use them on WSL or native Linux).
:::
:::

::: {#using-custom-asyncio-loops .section}
[]{#using-custom-loops}

#### Using custom asyncio loops[¶](#using-custom-asyncio-loops "Permalink to this heading"){.headerlink}

You can also use custom asyncio event loops with the asyncio reactor.
Set the [[`ASYNCIO_EVENT_LOOP`{.xref .std .std-setting .docutils
.literal
.notranslate}]{.pre}](index.html#std-setting-ASYNCIO_EVENT_LOOP){.hoverxref
.tooltip .reference .internal} setting to the import path of the desired
event loop class to use it instead of the default asyncio event loop.
:::
:::
:::

[[Frequently Asked Questions]{.doc}](index.html#document-faq){.reference .internal}

:   Get answers to most frequently asked questions.

[[Debugging Spiders]{.doc}](index.html#document-topics/debug){.reference .internal}

:   Learn how to debug common problems of your Scrapy spider.

[[Spiders Contracts]{.doc}](index.html#document-topics/contracts){.reference .internal}

:   Learn how to use contracts for testing your spiders.

[[Common Practices]{.doc}](index.html#document-topics/practices){.reference .internal}

:   Get familiar with some Scrapy common practices.

[[Broad Crawls]{.doc}](index.html#document-topics/broad-crawls){.reference .internal}

:   Tune Scrapy for crawling a lot domains in parallel.

[[Using your browser's Developer Tools for scraping]{.doc}](index.html#document-topics/developer-tools){.reference .internal}

:   Learn how to scrape with your browser's developer tools.

[[Selecting dynamically-loaded content]{.doc}](index.html#document-topics/dynamic-content){.reference .internal}

:   Read webpage data that is loaded dynamically.

[[Debugging memory leaks]{.doc}](index.html#document-topics/leaks){.reference .internal}

:   Learn how to find and get rid of memory leaks in your crawler.

[[Downloading and processing files and images]{.doc}](index.html#document-topics/media-pipeline){.reference .internal}

:   Download files and/or images associated with your scraped items.

[[Deploying Spiders]{.doc}](index.html#document-topics/deploy){.reference .internal}

:   Deploying your Scrapy spiders and run them in a remote server.

[[AutoThrottle extension]{.doc}](index.html#document-topics/autothrottle){.reference .internal}

:   Adjust crawl rate dynamically based on load.

[[Benchmarking]{.doc}](index.html#document-topics/benchmarking){.reference .internal}

:   Check how Scrapy performs on your hardware.

[[Jobs: pausing and resuming crawls]{.doc}](index.html#document-topics/jobs){.reference .internal}

:   Learn how to pause and resume crawls for large spiders.

[[Coroutines]{.doc}](index.html#document-topics/coroutines){.reference .internal}

:   Use the [[coroutine syntax]{.xref .std
    .std-ref}](https://docs.python.org/3/reference/compound_stmts.html#async "(in Python v3.12)"){.reference
    .external}.

[[asyncio]{.doc}](index.html#document-topics/asyncio){.reference .internal}

:   Use [[`asyncio`{.xref .py .py-mod .docutils .literal
    .notranslate}]{.pre}](https://docs.python.org/3/library/asyncio.html#module-asyncio "(in Python v3.12)"){.reference
    .external} and [[`asyncio`{.xref .py .py-mod .docutils .literal
    .notranslate}]{.pre}](https://docs.python.org/3/library/asyncio.html#module-asyncio "(in Python v3.12)"){.reference
    .external}-powered libraries.
:::

::: {#extending-scrapy .section}
[]{#id2}

## Extending Scrapy[¶](#extending-scrapy "Permalink to this heading"){.headerlink}

::: {.toctree-wrapper .compound}
[]{#document-topics/architecture}

::: {#architecture-overview .section}
[]{#topics-architecture}

### Architecture overview[¶](#architecture-overview "Permalink to this heading"){.headerlink}

This document describes the architecture of Scrapy and how its
components interact.

::: {#overview .section}
#### Overview[¶](#overview "Permalink to this heading"){.headerlink}

The following diagram shows an overview of the Scrapy architecture with
its components and an outline of the data flow that takes place inside
the system (shown by the red arrows). A brief description of the
components is included below with links for more detailed information
about them. The data flow is also described below.
:::

::: {#data-flow .section}
[]{#id1}

#### Data flow[¶](#data-flow "Permalink to this heading"){.headerlink}

[![Scrapy
architecture](_images/scrapy_architecture_02.png){style="width: 700px; height: 470px;"}](_images/scrapy_architecture_02.png){.reference
.internal .image-reference}

The data flow in Scrapy is controlled by the execution engine, and goes
like this:

1.  The [[Engine]{.std .std-ref}](#component-engine){.hoverxref .tooltip
    .reference .internal} gets the initial Requests to crawl from the
    [[Spider]{.std .std-ref}](#component-spiders){.hoverxref .tooltip
    .reference .internal}.

2.  The [[Engine]{.std .std-ref}](#component-engine){.hoverxref .tooltip
    .reference .internal} schedules the Requests in the
    [[Scheduler]{.std .std-ref}](#component-scheduler){.hoverxref
    .tooltip .reference .internal} and asks for the next Requests to
    crawl.

3.  The [[Scheduler]{.std .std-ref}](#component-scheduler){.hoverxref
    .tooltip .reference .internal} returns the next Requests to the
    [[Engine]{.std .std-ref}](#component-engine){.hoverxref .tooltip
    .reference .internal}.

4.  The [[Engine]{.std .std-ref}](#component-engine){.hoverxref .tooltip
    .reference .internal} sends the Requests to the [[Downloader]{.std
    .std-ref}](#component-downloader){.hoverxref .tooltip .reference
    .internal}, passing through the [[Downloader Middlewares]{.std
    .std-ref}](#component-downloader-middleware){.hoverxref .tooltip
    .reference .internal} (see [[`process_request()`{.xref .py .py-meth
    .docutils .literal
    .notranslate}]{.pre}](index.html#scrapy.downloadermiddlewares.DownloaderMiddleware.process_request "scrapy.downloadermiddlewares.DownloaderMiddleware.process_request"){.reference
    .internal}).

5.  Once the page finishes downloading the [[Downloader]{.std
    .std-ref}](#component-downloader){.hoverxref .tooltip .reference
    .internal} generates a Response (with that page) and sends it to the
    Engine, passing through the [[Downloader Middlewares]{.std
    .std-ref}](#component-downloader-middleware){.hoverxref .tooltip
    .reference .internal} (see [[`process_response()`{.xref .py .py-meth
    .docutils .literal
    .notranslate}]{.pre}](index.html#scrapy.downloadermiddlewares.DownloaderMiddleware.process_response "scrapy.downloadermiddlewares.DownloaderMiddleware.process_response"){.reference
    .internal}).

6.  The [[Engine]{.std .std-ref}](#component-engine){.hoverxref .tooltip
    .reference .internal} receives the Response from the
    [[Downloader]{.std .std-ref}](#component-downloader){.hoverxref
    .tooltip .reference .internal} and sends it to the [[Spider]{.std
    .std-ref}](#component-spiders){.hoverxref .tooltip .reference
    .internal} for processing, passing through the [[Spider
    Middleware]{.std .std-ref}](#component-spider-middleware){.hoverxref
    .tooltip .reference .internal} (see [[`process_spider_input()`{.xref
    .py .py-meth .docutils .literal
    .notranslate}]{.pre}](index.html#scrapy.spidermiddlewares.SpiderMiddleware.process_spider_input "scrapy.spidermiddlewares.SpiderMiddleware.process_spider_input"){.reference
    .internal}).

7.  The [[Spider]{.std .std-ref}](#component-spiders){.hoverxref
    .tooltip .reference .internal} processes the Response and returns
    scraped items and new Requests (to follow) to the [[Engine]{.std
    .std-ref}](#component-engine){.hoverxref .tooltip .reference
    .internal}, passing through the [[Spider Middleware]{.std
    .std-ref}](#component-spider-middleware){.hoverxref .tooltip
    .reference .internal} (see [[`process_spider_output()`{.xref .py
    .py-meth .docutils .literal
    .notranslate}]{.pre}](index.html#scrapy.spidermiddlewares.SpiderMiddleware.process_spider_output "scrapy.spidermiddlewares.SpiderMiddleware.process_spider_output"){.reference
    .internal}).

8.  The [[Engine]{.std .std-ref}](#component-engine){.hoverxref .tooltip
    .reference .internal} sends processed items to [[Item
    Pipelines]{.std .std-ref}](#component-pipelines){.hoverxref .tooltip
    .reference .internal}, then send processed Requests to the
    [[Scheduler]{.std .std-ref}](#component-scheduler){.hoverxref
    .tooltip .reference .internal} and asks for possible next Requests
    to crawl.

9.  The process repeats (from step 3) until there are no more requests
    from the [[Scheduler]{.std
    .std-ref}](#component-scheduler){.hoverxref .tooltip .reference
    .internal}.
:::

::: {#components .section}
#### Components[¶](#components "Permalink to this heading"){.headerlink}

::: {#scrapy-engine .section}
[]{#component-engine}

##### Scrapy Engine[¶](#scrapy-engine "Permalink to this heading"){.headerlink}

The engine is responsible for controlling the data flow between all
components of the system, and triggering events when certain actions
occur. See the [[Data Flow]{.std .std-ref}](#data-flow){.hoverxref
.tooltip .reference .internal} section above for more details.
:::

::: {#scheduler .section}
[]{#component-scheduler}

##### Scheduler[¶](#scheduler "Permalink to this heading"){.headerlink}

The [[scheduler]{.std .std-ref}](index.html#topics-scheduler){.hoverxref
.tooltip .reference .internal} receives requests from the engine and
enqueues them for feeding them later (also to the engine) when the
engine requests them.
:::

::: {#downloader .section}
[]{#component-downloader}

##### Downloader[¶](#downloader "Permalink to this heading"){.headerlink}

The Downloader is responsible for fetching web pages and feeding them to
the engine which, in turn, feeds them to the spiders.
:::

::: {#spiders .section}
[]{#component-spiders}

##### Spiders[¶](#spiders "Permalink to this heading"){.headerlink}

Spiders are custom classes written by Scrapy users to parse responses
and extract [[items]{.std .std-ref}](index.html#topics-items){.hoverxref
.tooltip .reference .internal} from them or additional requests to
follow. For more information see [[Spiders]{.std
.std-ref}](index.html#topics-spiders){.hoverxref .tooltip .reference
.internal}.
:::

::: {#item-pipeline .section}
[]{#component-pipelines}

##### Item Pipeline[¶](#item-pipeline "Permalink to this heading"){.headerlink}

The Item Pipeline is responsible for processing the items once they have
been extracted (or scraped) by the spiders. Typical tasks include
cleansing, validation and persistence (like storing the item in a
database). For more information see [[Item Pipeline]{.std
.std-ref}](index.html#topics-item-pipeline){.hoverxref .tooltip
.reference .internal}.
:::

::: {#downloader-middlewares .section}
[]{#component-downloader-middleware}

##### Downloader middlewares[¶](#downloader-middlewares "Permalink to this heading"){.headerlink}

Downloader middlewares are specific hooks that sit between the Engine
and the Downloader and process requests when they pass from the Engine
to the Downloader, and responses that pass from Downloader to the
Engine.

Use a Downloader middleware if you need to do one of the following:

-   process a request just before it is sent to the Downloader (i.e.
    right before Scrapy sends the request to the website);

-   change received response before passing it to a spider;

-   send a new Request instead of passing received response to a spider;

-   pass response to a spider without fetching a web page;

-   silently drop some requests.

For more information see [[Downloader Middleware]{.std
.std-ref}](index.html#topics-downloader-middleware){.hoverxref .tooltip
.reference .internal}.
:::

::: {#spider-middlewares .section}
[]{#component-spider-middleware}

##### Spider middlewares[¶](#spider-middlewares "Permalink to this heading"){.headerlink}

Spider middlewares are specific hooks that sit between the Engine and
the Spiders and are able to process spider input (responses) and output
(items and requests).

Use a Spider middleware if you need to

-   post-process output of spider callbacks - change/add/remove requests
    or items;

-   post-process start_requests;

-   handle spider exceptions;

-   call errback instead of callback for some of the requests based on
    response content.

For more information see [[Spider Middleware]{.std
.std-ref}](index.html#topics-spider-middleware){.hoverxref .tooltip
.reference .internal}.
:::
:::

::: {#event-driven-networking .section}
#### Event-driven networking[¶](#event-driven-networking "Permalink to this heading"){.headerlink}

Scrapy is written with
[Twisted](https://twistedmatrix.com/trac/){.reference .external}, a
popular event-driven networking framework for Python. Thus, it's
implemented using a non-blocking (aka asynchronous) code for
concurrency.

For more information about asynchronous programming and Twisted see
these links:

-   [Introduction to
    Deferreds](https://docs.twisted.org/en/stable/core/howto/defer-intro.html "(in Twisted v23.10)"){.reference
    .external}

-   [Twisted - hello, asynchronous
    programming](http://jessenoller.com/blog/2009/02/11/twisted-hello-asynchronous-programming/){.reference
    .external}

-   [Twisted Introduction -
    Krondo](http://krondo.com/an-introduction-to-asynchronous-programming-and-twisted/){.reference
    .external}
:::
:::

[]{#document-topics/addons}

::: {#add-ons .section}
[]{#topics-addons}

### Add-ons[¶](#add-ons "Permalink to this heading"){.headerlink}

Scrapy's add-on system is a framework which unifies managing and
configuring components that extend Scrapy's core functionality, such as
middlewares, extensions, or pipelines. It provides users with a
plug-and-play experience in Scrapy extension management, and grants
extensive configuration control to developers.

::: {#activating-and-configuring-add-ons .section}
#### Activating and configuring add-ons[¶](#activating-and-configuring-add-ons "Permalink to this heading"){.headerlink}

During [[`Crawler`{.xref .py .py-class .docutils .literal
.notranslate}]{.pre}](index.html#scrapy.crawler.Crawler "scrapy.crawler.Crawler"){.reference
.internal} initialization, the list of enabled add-ons is read from your
[`ADDONS`{.docutils .literal .notranslate}]{.pre} setting.

The [`ADDONS`{.docutils .literal .notranslate}]{.pre} setting is a dict
in which every key is an add-on class or its import path and the value
is its priority.

This is an example where two add-ons are enabled in a project's
[`settings.py`{.docutils .literal .notranslate}]{.pre}:

::: {.highlight-default .notranslate}
::: highlight
    ADDONS = {
        'path.to.someaddon': 0,
        SomeAddonClass: 1,
    }
:::
:::
:::

::: {#writing-your-own-add-ons .section}
#### Writing your own add-ons[¶](#writing-your-own-add-ons "Permalink to this heading"){.headerlink}

Add-ons are Python classes that include the following method:

[[update_settings]{.pre}]{.sig-name .descname}[(]{.sig-paren}*[[settings]{.pre}]{.n}*[)]{.sig-paren}[¶](#update_settings "Permalink to this definition"){.headerlink}

:   This method is called during the initialization of the
    [[`Crawler`{.xref .py .py-class .docutils .literal
    .notranslate}]{.pre}](index.html#scrapy.crawler.Crawler "scrapy.crawler.Crawler"){.reference
    .internal}. Here, you should perform dependency checks (e.g. for
    external Python libraries) and update the [[`Settings`{.xref .py
    .py-class .docutils .literal
    .notranslate}]{.pre}](index.html#scrapy.settings.Settings "scrapy.settings.Settings"){.reference
    .internal} object as wished, e.g. enable components for this add-on
    or set required configuration of other extensions.

    Parameters

    :   **settings** ([[`Settings`{.xref .py .py-class .docutils
        .literal
        .notranslate}]{.pre}](index.html#scrapy.settings.Settings "scrapy.settings.Settings"){.reference
        .internal}) -- The settings object storing Scrapy/component
        configuration

They can also have the following method:

*[classmethod]{.pre}[ ]{.w}*[[from_crawler]{.pre}]{.sig-name .descname}[(]{.sig-paren}*[[cls]{.pre}]{.n}*, *[[crawler]{.pre}]{.n}*[)]{.sig-paren}

:   If present, this class method is called to create an add-on instance
    from a [[`Crawler`{.xref .py .py-class .docutils .literal
    .notranslate}]{.pre}](index.html#scrapy.crawler.Crawler "scrapy.crawler.Crawler"){.reference
    .internal}. It must return a new instance of the add-on. The crawler
    object provides access to all Scrapy core components like settings
    and signals; it is a way for the add-on to access them and hook its
    functionality into Scrapy.

    Parameters

    :   **crawler** ([[`Crawler`{.xref .py .py-class .docutils .literal
        .notranslate}]{.pre}](index.html#scrapy.crawler.Crawler "scrapy.crawler.Crawler"){.reference
        .internal}) -- The crawler that uses this add-on

The settings set by the add-on should use the [`addon`{.docutils
.literal .notranslate}]{.pre} priority (see [[Populating the
settings]{.std .std-ref}](index.html#populating-settings){.hoverxref
.tooltip .reference .internal} and
[[`scrapy.settings.BaseSettings.set()`{.xref .py .py-func .docutils
.literal
.notranslate}]{.pre}](index.html#scrapy.settings.BaseSettings.set "scrapy.settings.BaseSettings.set"){.reference
.internal}):

::: {.highlight-default .notranslate}
::: highlight
    class MyAddon:
        def update_settings(self, settings):
            settings.set("DNSCACHE_ENABLED", True, "addon")
:::
:::

This allows users to override these settings in the project or spider
configuration. This is not possible with settings that are mutable
objects, such as the dict that is a value of [[`ITEM_PIPELINES`{.xref
.std .std-setting .docutils .literal
.notranslate}]{.pre}](index.html#std-setting-ITEM_PIPELINES){.hoverxref
.tooltip .reference .internal}. In these cases you can provide an
add-on-specific setting that governs whether the add-on will modify
[[`ITEM_PIPELINES`{.xref .std .std-setting .docutils .literal
.notranslate}]{.pre}](index.html#std-setting-ITEM_PIPELINES){.hoverxref
.tooltip .reference .internal}:

::: {.highlight-default .notranslate}
::: highlight
    class MyAddon:
        def update_settings(self, settings):
            if settings.getbool("MYADDON_ENABLE_PIPELINE"):
                settings["ITEM_PIPELINES"]["path.to.mypipeline"] = 200
:::
:::

If the [`update_settings`{.docutils .literal .notranslate}]{.pre} method
raises [[`scrapy.exceptions.NotConfigured`{.xref .py .py-exc .docutils
.literal
.notranslate}]{.pre}](index.html#scrapy.exceptions.NotConfigured "scrapy.exceptions.NotConfigured"){.reference
.internal}, the add-on will be skipped. This makes it easy to enable an
add-on only when some conditions are met.

::: {#fallbacks .section}
##### Fallbacks[¶](#fallbacks "Permalink to this heading"){.headerlink}

Some components provided by add-ons need to fall back to "default"
implementations, e.g. a custom download handler needs to send the
request that it doesn't handle via the default download handler, or a
stats collector that includes some additional processing but otherwise
uses the default stats collector. And it's possible that a project needs
to use several custom components of the same type, e.g. two custom
download handlers that support different kinds of custom requests and
still need to use the default download handler for other requests. To
make such use cases easier to configure, we recommend that such custom
components should be written in the following way:

1.  The custom component (e.g. [`MyDownloadHandler`{.docutils .literal
    .notranslate}]{.pre}) shouldn't inherit from the default Scrapy one
    (e.g.
    [`scrapy.core.downloader.handlers.http.HTTPDownloadHandler`{.docutils
    .literal .notranslate}]{.pre}), but instead be able to load the
    class of the fallback component from a special setting (e.g.
    [`MY_FALLBACK_DOWNLOAD_HANDLER`{.docutils .literal
    .notranslate}]{.pre}), create an instance of it and use it.

2.  The add-ons that include these components should read the current
    value of the default setting (e.g. [`DOWNLOAD_HANDLERS`{.docutils
    .literal .notranslate}]{.pre}) in their
    [`update_settings()`{.docutils .literal .notranslate}]{.pre}
    methods, save that value into the fallback setting
    ([`MY_FALLBACK_DOWNLOAD_HANDLER`{.docutils .literal
    .notranslate}]{.pre} mentioned earlier) and set the default setting
    to the component provided by the add-on (e.g.
    [`MyDownloadHandler`{.docutils .literal .notranslate}]{.pre}). If
    the fallback setting is already set by the user, they shouldn't
    change it.

3.  This way, if there are several add-ons that want to modify the same
    setting, all of them will fallback to the component from the
    previous one and then to the Scrapy default. The order of that
    depends on the priority order in the [`ADDONS`{.docutils .literal
    .notranslate}]{.pre} setting.
:::
:::

::: {#add-on-examples .section}
#### Add-on examples[¶](#add-on-examples "Permalink to this heading"){.headerlink}

Set some basic configuration:

::: {.highlight-python .notranslate}
::: highlight
    class MyAddon:
        def update_settings(self, settings):
            settings["ITEM_PIPELINES"]["path.to.mypipeline"] = 200
            settings.set("DNSCACHE_ENABLED", True, "addon")
:::
:::

Check dependencies:

::: {.highlight-python .notranslate}
::: highlight
    class MyAddon:
        def update_settings(self, settings):
            try:
                import boto
            except ImportError:
                raise NotConfigured("MyAddon requires the boto library")
            ...
:::
:::

Access the crawler instance:

::: {.highlight-python .notranslate}
::: highlight
    class MyAddon:
        def __init__(self, crawler) -> None:
            super().__init__()
            self.crawler = crawler

        @classmethod
        def from_crawler(cls, crawler):
            return cls(crawler)

        def update_settings(self, settings):
            ...
:::
:::

Use a fallback component:

::: {.highlight-python .notranslate}
::: highlight
    from scrapy.core.downloader.handlers.http import HTTPDownloadHandler


    FALLBACK_SETTING = "MY_FALLBACK_DOWNLOAD_HANDLER"


    class MyHandler:
        lazy = False

        def __init__(self, settings, crawler):
            dhcls = load_object(settings.get(FALLBACK_SETTING))
            self._fallback_handler = create_instance(
                dhcls,
                settings=None,
                crawler=crawler,
            )

        def download_request(self, request, spider):
            if request.meta.get("my_params"):
                # handle the request
                ...
            else:
                return self._fallback_handler.download_request(request, spider)


    class MyAddon:
        def update_settings(self, settings):
            if not settings.get(FALLBACK_SETTING):
                settings.set(
                    FALLBACK_SETTING,
                    settings.getwithbase("DOWNLOAD_HANDLERS")["https"],
                    "addon",
                )
            settings["DOWNLOAD_HANDLERS"]["https"] = MyHandler
:::
:::
:::
:::

[]{#document-topics/downloader-middleware}

::: {#downloader-middleware .section}
[]{#topics-downloader-middleware}

### Downloader Middleware[¶](#downloader-middleware "Permalink to this heading"){.headerlink}

The downloader middleware is a framework of hooks into Scrapy's
request/response processing. It's a light, low-level system for globally
altering Scrapy's requests and responses.

::: {#activating-a-downloader-middleware .section}
[]{#topics-downloader-middleware-setting}

#### Activating a downloader middleware[¶](#activating-a-downloader-middleware "Permalink to this heading"){.headerlink}

To activate a downloader middleware component, add it to the
[[`DOWNLOADER_MIDDLEWARES`{.xref .std .std-setting .docutils .literal
.notranslate}]{.pre}](index.html#std-setting-DOWNLOADER_MIDDLEWARES){.hoverxref
.tooltip .reference .internal} setting, which is a dict whose keys are
the middleware class paths and their values are the middleware orders.

Here's an example:

::: {.highlight-python .notranslate}
::: highlight
    DOWNLOADER_MIDDLEWARES = {
        "myproject.middlewares.CustomDownloaderMiddleware": 543,
    }
:::
:::

The [[`DOWNLOADER_MIDDLEWARES`{.xref .std .std-setting .docutils
.literal
.notranslate}]{.pre}](index.html#std-setting-DOWNLOADER_MIDDLEWARES){.hoverxref
.tooltip .reference .internal} setting is merged with the
[[`DOWNLOADER_MIDDLEWARES_BASE`{.xref .std .std-setting .docutils
.literal
.notranslate}]{.pre}](index.html#std-setting-DOWNLOADER_MIDDLEWARES_BASE){.hoverxref
.tooltip .reference .internal} setting defined in Scrapy (and not meant
to be overridden) and then sorted by order to get the final sorted list
of enabled middlewares: the first middleware is the one closer to the
engine and the last is the one closer to the downloader. In other words,
the [[`process_request()`{.xref .py .py-meth .docutils .literal
.notranslate}]{.pre}](#scrapy.downloadermiddlewares.DownloaderMiddleware.process_request "scrapy.downloadermiddlewares.DownloaderMiddleware.process_request"){.reference
.internal} method of each middleware will be invoked in increasing
middleware order (100, 200, 300, ...) and the
[[`process_response()`{.xref .py .py-meth .docutils .literal
.notranslate}]{.pre}](#scrapy.downloadermiddlewares.DownloaderMiddleware.process_response "scrapy.downloadermiddlewares.DownloaderMiddleware.process_response"){.reference
.internal} method of each middleware will be invoked in decreasing
order.

To decide which order to assign to your middleware see the
[[`DOWNLOADER_MIDDLEWARES_BASE`{.xref .std .std-setting .docutils
.literal
.notranslate}]{.pre}](index.html#std-setting-DOWNLOADER_MIDDLEWARES_BASE){.hoverxref
.tooltip .reference .internal} setting and pick a value according to
where you want to insert the middleware. The order does matter because
each middleware performs a different action and your middleware could
depend on some previous (or subsequent) middleware being applied.

If you want to disable a built-in middleware (the ones defined in
[[`DOWNLOADER_MIDDLEWARES_BASE`{.xref .std .std-setting .docutils
.literal
.notranslate}]{.pre}](index.html#std-setting-DOWNLOADER_MIDDLEWARES_BASE){.hoverxref
.tooltip .reference .internal} and enabled by default) you must define
it in your project's [[`DOWNLOADER_MIDDLEWARES`{.xref .std .std-setting
.docutils .literal
.notranslate}]{.pre}](index.html#std-setting-DOWNLOADER_MIDDLEWARES){.hoverxref
.tooltip .reference .internal} setting and assign [`None`{.docutils
.literal .notranslate}]{.pre} as its value. For example, if you want to
disable the user-agent middleware:

::: {.highlight-python .notranslate}
::: highlight
    DOWNLOADER_MIDDLEWARES = {
        "myproject.middlewares.CustomDownloaderMiddleware": 543,
        "scrapy.downloadermiddlewares.useragent.UserAgentMiddleware": None,
    }
:::
:::

Finally, keep in mind that some middlewares may need to be enabled
through a particular setting. See each middleware documentation for more
info.
:::

::: {#writing-your-own-downloader-middleware .section}
[]{#topics-downloader-middleware-custom}

#### Writing your own downloader middleware[¶](#writing-your-own-downloader-middleware "Permalink to this heading"){.headerlink}

Each downloader middleware is a Python class that defines one or more of
the methods defined below.

The main entry point is the [`from_crawler`{.docutils .literal
.notranslate}]{.pre} class method, which receives a [[`Crawler`{.xref
.py .py-class .docutils .literal
.notranslate}]{.pre}](index.html#scrapy.crawler.Crawler "scrapy.crawler.Crawler"){.reference
.internal} instance. The [[`Crawler`{.xref .py .py-class .docutils
.literal
.notranslate}]{.pre}](index.html#scrapy.crawler.Crawler "scrapy.crawler.Crawler"){.reference
.internal} object gives you access, for example, to the [[settings]{.std
.std-ref}](index.html#topics-settings){.hoverxref .tooltip .reference
.internal}.

[]{#module-scrapy.downloadermiddlewares .target}

*[class]{.pre}[ ]{.w}*[[scrapy.downloadermiddlewares.]{.pre}]{.sig-prename .descclassname}[[DownloaderMiddleware]{.pre}]{.sig-name .descname}[¶](#scrapy.downloadermiddlewares.DownloaderMiddleware "Permalink to this definition"){.headerlink}

:   ::: {.admonition .note}
    Note

    Any of the downloader middleware methods may also return a deferred.
    :::

    [[process_request]{.pre}]{.sig-name .descname}[(]{.sig-paren}*[[request]{.pre}]{.n}*, *[[spider]{.pre}]{.n}*[)]{.sig-paren}[¶](#scrapy.downloadermiddlewares.DownloaderMiddleware.process_request "Permalink to this definition"){.headerlink}

    :   This method is called for each request that goes through the
        download middleware.

        [[`process_request()`{.xref .py .py-meth .docutils .literal
        .notranslate}]{.pre}](#scrapy.downloadermiddlewares.DownloaderMiddleware.process_request "scrapy.downloadermiddlewares.DownloaderMiddleware.process_request"){.reference
        .internal} should either: return [`None`{.docutils .literal
        .notranslate}]{.pre}, return a [`Response`{.xref .py .py-class
        .docutils .literal .notranslate}]{.pre} object, return a
        [[`Request`{.xref .py .py-class .docutils .literal
        .notranslate}]{.pre}](index.html#scrapy.http.Request "scrapy.http.Request"){.reference
        .internal} object, or raise [[`IgnoreRequest`{.xref .py .py-exc
        .docutils .literal
        .notranslate}]{.pre}](index.html#scrapy.exceptions.IgnoreRequest "scrapy.exceptions.IgnoreRequest"){.reference
        .internal}.

        If it returns [`None`{.docutils .literal .notranslate}]{.pre},
        Scrapy will continue processing this request, executing all
        other middlewares until, finally, the appropriate downloader
        handler is called the request performed (and its response
        downloaded).

        If it returns a [[`Response`{.xref .py .py-class .docutils
        .literal
        .notranslate}]{.pre}](index.html#scrapy.http.Response "scrapy.http.Response"){.reference
        .internal} object, Scrapy won't bother calling *any* other
        [[`process_request()`{.xref .py .py-meth .docutils .literal
        .notranslate}]{.pre}](#scrapy.downloadermiddlewares.DownloaderMiddleware.process_request "scrapy.downloadermiddlewares.DownloaderMiddleware.process_request"){.reference
        .internal} or [[`process_exception()`{.xref .py .py-meth
        .docutils .literal
        .notranslate}]{.pre}](#scrapy.downloadermiddlewares.DownloaderMiddleware.process_exception "scrapy.downloadermiddlewares.DownloaderMiddleware.process_exception"){.reference
        .internal} methods, or the appropriate download function; it'll
        return that response. The [[`process_response()`{.xref .py
        .py-meth .docutils .literal
        .notranslate}]{.pre}](#scrapy.downloadermiddlewares.DownloaderMiddleware.process_response "scrapy.downloadermiddlewares.DownloaderMiddleware.process_response"){.reference
        .internal} methods of installed middleware is always called on
        every response.

        If it returns a [`Request`{.xref .py .py-class .docutils
        .literal .notranslate}]{.pre} object, Scrapy will stop calling
        [[`process_request()`{.xref .py .py-meth .docutils .literal
        .notranslate}]{.pre}](#scrapy.downloadermiddlewares.DownloaderMiddleware.process_request "scrapy.downloadermiddlewares.DownloaderMiddleware.process_request"){.reference
        .internal} methods and reschedule the returned request. Once the
        newly returned request is performed, the appropriate middleware
        chain will be called on the downloaded response.

        If it raises an [[`IgnoreRequest`{.xref .py .py-exc .docutils
        .literal
        .notranslate}]{.pre}](index.html#scrapy.exceptions.IgnoreRequest "scrapy.exceptions.IgnoreRequest"){.reference
        .internal} exception, the [[`process_exception()`{.xref .py
        .py-meth .docutils .literal
        .notranslate}]{.pre}](#scrapy.downloadermiddlewares.DownloaderMiddleware.process_exception "scrapy.downloadermiddlewares.DownloaderMiddleware.process_exception"){.reference
        .internal} methods of installed downloader middleware will be
        called. If none of them handle the exception, the errback
        function of the request ([`Request.errback`{.docutils .literal
        .notranslate}]{.pre}) is called. If no code handles the raised
        exception, it is ignored and not logged (unlike other
        exceptions).

        Parameters

        :   -   **request** ([`Request`{.xref .py .py-class .docutils
                .literal .notranslate}]{.pre} object) -- the request
                being processed

            -   **spider** ([[`Spider`{.xref .py .py-class .docutils
                .literal
                .notranslate}]{.pre}](index.html#scrapy.Spider "scrapy.Spider"){.reference
                .internal} object) -- the spider for which this request
                is intended

    [[process_response]{.pre}]{.sig-name .descname}[(]{.sig-paren}*[[request]{.pre}]{.n}*, *[[response]{.pre}]{.n}*, *[[spider]{.pre}]{.n}*[)]{.sig-paren}[¶](#scrapy.downloadermiddlewares.DownloaderMiddleware.process_response "Permalink to this definition"){.headerlink}

    :   [[`process_response()`{.xref .py .py-meth .docutils .literal
        .notranslate}]{.pre}](#scrapy.downloadermiddlewares.DownloaderMiddleware.process_response "scrapy.downloadermiddlewares.DownloaderMiddleware.process_response"){.reference
        .internal} should either: return a [[`Response`{.xref .py
        .py-class .docutils .literal
        .notranslate}]{.pre}](index.html#scrapy.http.Response "scrapy.http.Response"){.reference
        .internal} object, return a [`Request`{.xref .py .py-class
        .docutils .literal .notranslate}]{.pre} object or raise a
        [[`IgnoreRequest`{.xref .py .py-exc .docutils .literal
        .notranslate}]{.pre}](index.html#scrapy.exceptions.IgnoreRequest "scrapy.exceptions.IgnoreRequest"){.reference
        .internal} exception.

        If it returns a [[`Response`{.xref .py .py-class .docutils
        .literal
        .notranslate}]{.pre}](index.html#scrapy.http.Response "scrapy.http.Response"){.reference
        .internal} (it could be the same given response, or a brand-new
        one), that response will continue to be processed with the
        [[`process_response()`{.xref .py .py-meth .docutils .literal
        .notranslate}]{.pre}](#scrapy.downloadermiddlewares.DownloaderMiddleware.process_response "scrapy.downloadermiddlewares.DownloaderMiddleware.process_response"){.reference
        .internal} of the next middleware in the chain.

        If it returns a [`Request`{.xref .py .py-class .docutils
        .literal .notranslate}]{.pre} object, the middleware chain is
        halted and the returned request is rescheduled to be downloaded
        in the future. This is the same behavior as if a request is
        returned from [[`process_request()`{.xref .py .py-meth .docutils
        .literal
        .notranslate}]{.pre}](#scrapy.downloadermiddlewares.DownloaderMiddleware.process_request "scrapy.downloadermiddlewares.DownloaderMiddleware.process_request"){.reference
        .internal}.

        If it raises an [[`IgnoreRequest`{.xref .py .py-exc .docutils
        .literal
        .notranslate}]{.pre}](index.html#scrapy.exceptions.IgnoreRequest "scrapy.exceptions.IgnoreRequest"){.reference
        .internal} exception, the errback function of the request
        ([`Request.errback`{.docutils .literal .notranslate}]{.pre}) is
        called. If no code handles the raised exception, it is ignored
        and not logged (unlike other exceptions).

        Parameters

        :   -   **request** (is a [`Request`{.xref .py .py-class
                .docutils .literal .notranslate}]{.pre} object) -- the
                request that originated the response

            -   **response** ([[`Response`{.xref .py .py-class .docutils
                .literal
                .notranslate}]{.pre}](index.html#scrapy.http.Response "scrapy.http.Response"){.reference
                .internal} object) -- the response being processed

            -   **spider** ([[`Spider`{.xref .py .py-class .docutils
                .literal
                .notranslate}]{.pre}](index.html#scrapy.Spider "scrapy.Spider"){.reference
                .internal} object) -- the spider for which this response
                is intended

    [[process_exception]{.pre}]{.sig-name .descname}[(]{.sig-paren}*[[request]{.pre}]{.n}*, *[[exception]{.pre}]{.n}*, *[[spider]{.pre}]{.n}*[)]{.sig-paren}[¶](#scrapy.downloadermiddlewares.DownloaderMiddleware.process_exception "Permalink to this definition"){.headerlink}

    :   Scrapy calls [[`process_exception()`{.xref .py .py-meth
        .docutils .literal
        .notranslate}]{.pre}](#scrapy.downloadermiddlewares.DownloaderMiddleware.process_exception "scrapy.downloadermiddlewares.DownloaderMiddleware.process_exception"){.reference
        .internal} when a download handler or a
        [[`process_request()`{.xref .py .py-meth .docutils .literal
        .notranslate}]{.pre}](#scrapy.downloadermiddlewares.DownloaderMiddleware.process_request "scrapy.downloadermiddlewares.DownloaderMiddleware.process_request"){.reference
        .internal} (from a downloader middleware) raises an exception
        (including an [[`IgnoreRequest`{.xref .py .py-exc .docutils
        .literal
        .notranslate}]{.pre}](index.html#scrapy.exceptions.IgnoreRequest "scrapy.exceptions.IgnoreRequest"){.reference
        .internal} exception)

        [[`process_exception()`{.xref .py .py-meth .docutils .literal
        .notranslate}]{.pre}](#scrapy.downloadermiddlewares.DownloaderMiddleware.process_exception "scrapy.downloadermiddlewares.DownloaderMiddleware.process_exception"){.reference
        .internal} should return: either [`None`{.docutils .literal
        .notranslate}]{.pre}, a [[`Response`{.xref .py .py-class
        .docutils .literal
        .notranslate}]{.pre}](index.html#scrapy.http.Response "scrapy.http.Response"){.reference
        .internal} object, or a [`Request`{.xref .py .py-class .docutils
        .literal .notranslate}]{.pre} object.

        If it returns [`None`{.docutils .literal .notranslate}]{.pre},
        Scrapy will continue processing this exception, executing any
        other [[`process_exception()`{.xref .py .py-meth .docutils
        .literal
        .notranslate}]{.pre}](#scrapy.downloadermiddlewares.DownloaderMiddleware.process_exception "scrapy.downloadermiddlewares.DownloaderMiddleware.process_exception"){.reference
        .internal} methods of installed middleware, until no middleware
        is left and the default exception handling kicks in.

        If it returns a [[`Response`{.xref .py .py-class .docutils
        .literal
        .notranslate}]{.pre}](index.html#scrapy.http.Response "scrapy.http.Response"){.reference
        .internal} object, the [[`process_response()`{.xref .py .py-meth
        .docutils .literal
        .notranslate}]{.pre}](#scrapy.downloadermiddlewares.DownloaderMiddleware.process_response "scrapy.downloadermiddlewares.DownloaderMiddleware.process_response"){.reference
        .internal} method chain of installed middleware is started, and
        Scrapy won't bother calling any other
        [[`process_exception()`{.xref .py .py-meth .docutils .literal
        .notranslate}]{.pre}](#scrapy.downloadermiddlewares.DownloaderMiddleware.process_exception "scrapy.downloadermiddlewares.DownloaderMiddleware.process_exception"){.reference
        .internal} methods of middleware.

        If it returns a [`Request`{.xref .py .py-class .docutils
        .literal .notranslate}]{.pre} object, the returned request is
        rescheduled to be downloaded in the future. This stops the
        execution of [[`process_exception()`{.xref .py .py-meth
        .docutils .literal
        .notranslate}]{.pre}](#scrapy.downloadermiddlewares.DownloaderMiddleware.process_exception "scrapy.downloadermiddlewares.DownloaderMiddleware.process_exception"){.reference
        .internal} methods of the middleware the same as returning a
        response would.

        Parameters

        :   -   **request** (is a [`Request`{.xref .py .py-class
                .docutils .literal .notranslate}]{.pre} object) -- the
                request that generated the exception

            -   **exception** (an [`Exception`{.docutils .literal
                .notranslate}]{.pre} object) -- the raised exception

            -   **spider** ([[`Spider`{.xref .py .py-class .docutils
                .literal
                .notranslate}]{.pre}](index.html#scrapy.Spider "scrapy.Spider"){.reference
                .internal} object) -- the spider for which this request
                is intended

    [[from_crawler]{.pre}]{.sig-name .descname}[(]{.sig-paren}*[[cls]{.pre}]{.n}*, *[[crawler]{.pre}]{.n}*[)]{.sig-paren}[¶](#scrapy.downloadermiddlewares.DownloaderMiddleware.from_crawler "Permalink to this definition"){.headerlink}

    :   If present, this classmethod is called to create a middleware
        instance from a [[`Crawler`{.xref .py .py-class .docutils
        .literal
        .notranslate}]{.pre}](index.html#scrapy.crawler.Crawler "scrapy.crawler.Crawler"){.reference
        .internal}. It must return a new instance of the middleware.
        Crawler object provides access to all Scrapy core components
        like settings and signals; it is a way for middleware to access
        them and hook its functionality into Scrapy.

        Parameters

        :   **crawler** ([[`Crawler`{.xref .py .py-class .docutils
            .literal
            .notranslate}]{.pre}](index.html#scrapy.crawler.Crawler "scrapy.crawler.Crawler"){.reference
            .internal} object) -- crawler that uses this middleware
:::

::: {#built-in-downloader-middleware-reference .section}
[]{#topics-downloader-middleware-ref}

#### Built-in downloader middleware reference[¶](#built-in-downloader-middleware-reference "Permalink to this heading"){.headerlink}

This page describes all downloader middleware components that come with
Scrapy. For information on how to use them and how to write your own
downloader middleware, see the [[downloader middleware usage guide]{.std
.std-ref}](#topics-downloader-middleware){.hoverxref .tooltip .reference
.internal}.

For a list of the components enabled by default (and their orders) see
the [[`DOWNLOADER_MIDDLEWARES_BASE`{.xref .std .std-setting .docutils
.literal
.notranslate}]{.pre}](index.html#std-setting-DOWNLOADER_MIDDLEWARES_BASE){.hoverxref
.tooltip .reference .internal} setting.

::: {#module-scrapy.downloadermiddlewares.cookies .section}
[]{#cookiesmiddleware}[]{#cookies-mw}

##### CookiesMiddleware[¶](#module-scrapy.downloadermiddlewares.cookies "Permalink to this heading"){.headerlink}

*[class]{.pre}[ ]{.w}*[[scrapy.downloadermiddlewares.cookies.]{.pre}]{.sig-prename .descclassname}[[CookiesMiddleware]{.pre}]{.sig-name .descname}[[[\[source\]]{.pre}]{.viewcode-link}](_modules/scrapy/downloadermiddlewares/cookies.html#CookiesMiddleware){.reference .internal}[¶](#scrapy.downloadermiddlewares.cookies.CookiesMiddleware "Permalink to this definition"){.headerlink}

:   This middleware enables working with sites that require cookies,
    such as those that use sessions. It keeps track of cookies sent by
    web servers, and sends them back on subsequent requests (from that
    spider), just like web browsers do.

    ::: {.admonition .caution}
    Caution

    When non-UTF8 encoded byte sequences are passed to a
    [`Request`{.xref .py .py-class .docutils .literal
    .notranslate}]{.pre}, the [`CookiesMiddleware`{.docutils .literal
    .notranslate}]{.pre} will log a warning. Refer to [[Advanced
    customization]{.std
    .std-ref}](index.html#topics-logging-advanced-customization){.hoverxref
    .tooltip .reference .internal} to customize the logging behaviour.
    :::

    ::: {.admonition .caution}
    Caution

    Cookies set via the [`Cookie`{.docutils .literal
    .notranslate}]{.pre} header are not considered by the
    [[CookiesMiddleware]{.std .std-ref}](#cookies-mw){.hoverxref
    .tooltip .reference .internal}. If you need to set cookies for a
    request, use the [`Request.cookies`{.xref .py .py-class .docutils
    .literal .notranslate}]{.pre} parameter. This is a known current
    limitation that is being worked on.
    :::

The following settings can be used to configure the cookie middleware:

-   [[`COOKIES_ENABLED`{.xref .std .std-setting .docutils .literal
    .notranslate}]{.pre}](#std-setting-COOKIES_ENABLED){.hoverxref
    .tooltip .reference .internal}

-   [[`COOKIES_DEBUG`{.xref .std .std-setting .docutils .literal
    .notranslate}]{.pre}](#std-setting-COOKIES_DEBUG){.hoverxref
    .tooltip .reference .internal}

::: {#multiple-cookie-sessions-per-spider .section}
[]{#std-reqmeta-cookiejar}

###### Multiple cookie sessions per spider[¶](#multiple-cookie-sessions-per-spider "Permalink to this heading"){.headerlink}

There is support for keeping multiple cookie sessions per spider by
using the [[`cookiejar`{.xref .std .std-reqmeta .docutils .literal
.notranslate}]{.pre}](#std-reqmeta-cookiejar){.hoverxref .tooltip
.reference .internal} Request meta key. By default it uses a single
cookie jar (session), but you can pass an identifier to use different
ones.

For example:

::: {.highlight-python .notranslate}
::: highlight
    for i, url in enumerate(urls):
        yield scrapy.Request(url, meta={"cookiejar": i}, callback=self.parse_page)
:::
:::

Keep in mind that the [[`cookiejar`{.xref .std .std-reqmeta .docutils
.literal .notranslate}]{.pre}](#std-reqmeta-cookiejar){.hoverxref
.tooltip .reference .internal} meta key is not "sticky". You need to
keep passing it along on subsequent requests. For example:

::: {.highlight-python .notranslate}
::: highlight
    def parse_page(self, response):
        # do some processing
        return scrapy.Request(
            "http://www.example.com/otherpage",
            meta={"cookiejar": response.meta["cookiejar"]},
            callback=self.parse_other_page,
        )
:::
:::
:::

::: {#cookies-enabled .section}
[]{#std-setting-COOKIES_ENABLED}

###### COOKIES_ENABLED[¶](#cookies-enabled "Permalink to this heading"){.headerlink}

Default: [`True`{.docutils .literal .notranslate}]{.pre}

Whether to enable the cookies middleware. If disabled, no cookies will
be sent to web servers.

Notice that despite the value of [[`COOKIES_ENABLED`{.xref .std
.std-setting .docutils .literal
.notranslate}]{.pre}](#std-setting-COOKIES_ENABLED){.hoverxref .tooltip
.reference .internal} setting if [`Request.`{.docutils .literal
.notranslate}]{.pre}[[`meta['dont_merge_cookies']`{.xref .std
.std-reqmeta .docutils .literal
.notranslate}]{.pre}](index.html#std-reqmeta-dont_merge_cookies){.hoverxref
.tooltip .reference .internal} evaluates to [`True`{.docutils .literal
.notranslate}]{.pre} the request cookies will **not** be sent to the web
server and received cookies in [[`Response`{.xref .py .py-class
.docutils .literal
.notranslate}]{.pre}](index.html#scrapy.http.Response "scrapy.http.Response"){.reference
.internal} will **not** be merged with the existing cookies.

For more detailed information see the [`cookies`{.docutils .literal
.notranslate}]{.pre} parameter in [`Request`{.xref .py .py-class
.docutils .literal .notranslate}]{.pre}.
:::

::: {#cookies-debug .section}
[]{#std-setting-COOKIES_DEBUG}

###### COOKIES_DEBUG[¶](#cookies-debug "Permalink to this heading"){.headerlink}

Default: [`False`{.docutils .literal .notranslate}]{.pre}

If enabled, Scrapy will log all cookies sent in requests (i.e.
[`Cookie`{.docutils .literal .notranslate}]{.pre} header) and all
cookies received in responses (i.e. [`Set-Cookie`{.docutils .literal
.notranslate}]{.pre} header).

Here's an example of a log with [[`COOKIES_DEBUG`{.xref .std
.std-setting .docutils .literal
.notranslate}]{.pre}](#std-setting-COOKIES_DEBUG){.hoverxref .tooltip
.reference .internal} enabled:

::: {.highlight-default .notranslate}
::: highlight
    2011-04-06 14:35:10-0300 [scrapy.core.engine] INFO: Spider opened
    2011-04-06 14:35:10-0300 [scrapy.downloadermiddlewares.cookies] DEBUG: Sending cookies to: <GET http://www.diningcity.com/netherlands/index.html>
            Cookie: clientlanguage_nl=en_EN
    2011-04-06 14:35:14-0300 [scrapy.downloadermiddlewares.cookies] DEBUG: Received cookies from: <200 http://www.diningcity.com/netherlands/index.html>
            Set-Cookie: JSESSIONID=B~FA4DC0C496C8762AE4F1A620EAB34F38; Path=/
            Set-Cookie: ip_isocode=US
            Set-Cookie: clientlanguage_nl=en_EN; Expires=Thu, 07-Apr-2011 21:21:34 GMT; Path=/
    2011-04-06 14:49:50-0300 [scrapy.core.engine] DEBUG: Crawled (200) <GET http://www.diningcity.com/netherlands/index.html> (referer: None)
    [...]
:::
:::
:::
:::

::: {#module-scrapy.downloadermiddlewares.defaultheaders .section}
[]{#defaultheadersmiddleware}

##### DefaultHeadersMiddleware[¶](#module-scrapy.downloadermiddlewares.defaultheaders "Permalink to this heading"){.headerlink}

*[class]{.pre}[ ]{.w}*[[scrapy.downloadermiddlewares.defaultheaders.]{.pre}]{.sig-prename .descclassname}[[DefaultHeadersMiddleware]{.pre}]{.sig-name .descname}[[[\[source\]]{.pre}]{.viewcode-link}](_modules/scrapy/downloadermiddlewares/defaultheaders.html#DefaultHeadersMiddleware){.reference .internal}[¶](#scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware "Permalink to this definition"){.headerlink}

:   This middleware sets all default requests headers specified in the
    [[`DEFAULT_REQUEST_HEADERS`{.xref .std .std-setting .docutils
    .literal
    .notranslate}]{.pre}](index.html#std-setting-DEFAULT_REQUEST_HEADERS){.hoverxref
    .tooltip .reference .internal} setting.
:::

::: {#module-scrapy.downloadermiddlewares.downloadtimeout .section}
[]{#downloadtimeoutmiddleware}

##### DownloadTimeoutMiddleware[¶](#module-scrapy.downloadermiddlewares.downloadtimeout "Permalink to this heading"){.headerlink}

*[class]{.pre}[ ]{.w}*[[scrapy.downloadermiddlewares.downloadtimeout.]{.pre}]{.sig-prename .descclassname}[[DownloadTimeoutMiddleware]{.pre}]{.sig-name .descname}[[[\[source\]]{.pre}]{.viewcode-link}](_modules/scrapy/downloadermiddlewares/downloadtimeout.html#DownloadTimeoutMiddleware){.reference .internal}[¶](#scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware "Permalink to this definition"){.headerlink}

:   This middleware sets the download timeout for requests specified in
    the [[`DOWNLOAD_TIMEOUT`{.xref .std .std-setting .docutils .literal
    .notranslate}]{.pre}](index.html#std-setting-DOWNLOAD_TIMEOUT){.hoverxref
    .tooltip .reference .internal} setting or [`download_timeout`{.xref
    .py .py-attr .docutils .literal .notranslate}]{.pre} spider
    attribute.

::: {.admonition .note}
Note

You can also set download timeout per-request using
[[`download_timeout`{.xref .std .std-reqmeta .docutils .literal
.notranslate}]{.pre}](index.html#std-reqmeta-download_timeout){.hoverxref
.tooltip .reference .internal} Request.meta key; this is supported even
when DownloadTimeoutMiddleware is disabled.
:::
:::

::: {#module-scrapy.downloadermiddlewares.httpauth .section}
[]{#httpauthmiddleware}

##### HttpAuthMiddleware[¶](#module-scrapy.downloadermiddlewares.httpauth "Permalink to this heading"){.headerlink}

*[class]{.pre}[ ]{.w}*[[scrapy.downloadermiddlewares.httpauth.]{.pre}]{.sig-prename .descclassname}[[HttpAuthMiddleware]{.pre}]{.sig-name .descname}[[[\[source\]]{.pre}]{.viewcode-link}](_modules/scrapy/downloadermiddlewares/httpauth.html#HttpAuthMiddleware){.reference .internal}[¶](#scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware "Permalink to this definition"){.headerlink}

:   This middleware authenticates all requests generated from certain
    spiders using [Basic access
    authentication](https://en.wikipedia.org/wiki/Basic_access_authentication){.reference
    .external} (aka. HTTP auth).

    To enable HTTP authentication for a spider, set the
    [`http_user`{.docutils .literal .notranslate}]{.pre} and
    [`http_pass`{.docutils .literal .notranslate}]{.pre} spider
    attributes to the authentication data and the
    [`http_auth_domain`{.docutils .literal .notranslate}]{.pre} spider
    attribute to the domain which requires this authentication (its
    subdomains will be also handled in the same way). You can set
    [`http_auth_domain`{.docutils .literal .notranslate}]{.pre} to
    [`None`{.docutils .literal .notranslate}]{.pre} to enable the
    authentication for all requests but you risk leaking your
    authentication credentials to unrelated domains.

    ::: {.admonition .warning}
    Warning

    In previous Scrapy versions HttpAuthMiddleware sent the
    authentication data with all requests, which is a security problem
    if the spider makes requests to several different domains. Currently
    if the [`http_auth_domain`{.docutils .literal .notranslate}]{.pre}
    attribute is not set, the middleware will use the domain of the
    first request, which will work for some spiders but not for others.
    In the future the middleware will produce an error instead.
    :::

    Example:

    ::: {.highlight-python .notranslate}
    ::: highlight
        from scrapy.spiders import CrawlSpider


        class SomeIntranetSiteSpider(CrawlSpider):
            http_user = "someuser"
            http_pass = "somepass"
            http_auth_domain = "intranet.example.com"
            name = "intranet.example.com"

            # .. rest of the spider code omitted ...
    :::
    :::
:::

::: {#module-scrapy.downloadermiddlewares.httpcache .section}
[]{#httpcachemiddleware}

##### HttpCacheMiddleware[¶](#module-scrapy.downloadermiddlewares.httpcache "Permalink to this heading"){.headerlink}

*[class]{.pre}[ ]{.w}*[[scrapy.downloadermiddlewares.httpcache.]{.pre}]{.sig-prename .descclassname}[[HttpCacheMiddleware]{.pre}]{.sig-name .descname}[[[\[source\]]{.pre}]{.viewcode-link}](_modules/scrapy/downloadermiddlewares/httpcache.html#HttpCacheMiddleware){.reference .internal}[¶](#scrapy.downloadermiddlewares.httpcache.HttpCacheMiddleware "Permalink to this definition"){.headerlink}

:   This middleware provides low-level cache to all HTTP requests and
    responses. It has to be combined with a cache storage backend as
    well as a cache policy.

    Scrapy ships with the following HTTP cache storage backends:

    > <div>
    >
    > -   [[Filesystem storage backend (default)]{.std
    >     .std-ref}](#httpcache-storage-fs){.hoverxref .tooltip
    >     .reference .internal}
    >
    > -   [[DBM storage backend]{.std
    >     .std-ref}](#httpcache-storage-dbm){.hoverxref .tooltip
    >     .reference .internal}
    >
    > </div>

    You can change the HTTP cache storage backend with the
    [[`HTTPCACHE_STORAGE`{.xref .std .std-setting .docutils .literal
    .notranslate}]{.pre}](#std-setting-HTTPCACHE_STORAGE){.hoverxref
    .tooltip .reference .internal} setting. Or you can also [[implement
    your own storage backend.]{.std
    .std-ref}](#httpcache-storage-custom){.hoverxref .tooltip .reference
    .internal}

    Scrapy ships with two HTTP cache policies:

    > <div>
    >
    > -   [[RFC2616 policy]{.std
    >     .std-ref}](#httpcache-policy-rfc2616){.hoverxref .tooltip
    >     .reference .internal}
    >
    > -   [[Dummy policy (default)]{.std
    >     .std-ref}](#httpcache-policy-dummy){.hoverxref .tooltip
    >     .reference .internal}
    >
    > </div>

    You can change the HTTP cache policy with the
    [[`HTTPCACHE_POLICY`{.xref .std .std-setting .docutils .literal
    .notranslate}]{.pre}](#std-setting-HTTPCACHE_POLICY){.hoverxref
    .tooltip .reference .internal} setting. Or you can also implement
    your own policy.

    You can also avoid caching a response on every policy using
    [[`dont_cache`{.xref .std .std-reqmeta .docutils .literal
    .notranslate}]{.pre}](#std-reqmeta-dont_cache){.hoverxref .tooltip
    .reference .internal} meta key equals [`True`{.docutils .literal
    .notranslate}]{.pre}.

::: {#dummy-policy-default .section}
[]{#httpcache-policy-dummy}

###### Dummy policy (default)[¶](#dummy-policy-default "Permalink to this heading"){.headerlink}

*[class]{.pre}[ ]{.w}*[[scrapy.extensions.httpcache.]{.pre}]{.sig-prename .descclassname}[[DummyPolicy]{.pre}]{.sig-name .descname}[[[\[source\]]{.pre}]{.viewcode-link}](_modules/scrapy/extensions/httpcache.html#DummyPolicy){.reference .internal}[¶](#scrapy.extensions.httpcache.DummyPolicy "Permalink to this definition"){.headerlink}

:   This policy has no awareness of any HTTP Cache-Control directives.
    Every request and its corresponding response are cached. When the
    same request is seen again, the response is returned without
    transferring anything from the Internet.

    The Dummy policy is useful for testing spiders faster (without
    having to wait for downloads every time) and for trying your spider
    offline, when an Internet connection is not available. The goal is
    to be able to "replay" a spider run *exactly as it ran before*.
:::

::: {#rfc2616-policy .section}
[]{#httpcache-policy-rfc2616}

###### RFC2616 policy[¶](#rfc2616-policy "Permalink to this heading"){.headerlink}

*[class]{.pre}[ ]{.w}*[[scrapy.extensions.httpcache.]{.pre}]{.sig-prename .descclassname}[[RFC2616Policy]{.pre}]{.sig-name .descname}[[[\[source\]]{.pre}]{.viewcode-link}](_modules/scrapy/extensions/httpcache.html#RFC2616Policy){.reference .internal}[¶](#scrapy.extensions.httpcache.RFC2616Policy "Permalink to this definition"){.headerlink}

:   This policy provides a RFC2616 compliant HTTP cache, i.e. with HTTP
    Cache-Control awareness, aimed at production and used in continuous
    runs to avoid downloading unmodified data (to save bandwidth and
    speed up crawls).

    What is implemented:

    -   Do not attempt to store responses/requests with
        [`no-store`{.docutils .literal .notranslate}]{.pre}
        cache-control directive set

    -   Do not serve responses from cache if [`no-cache`{.docutils
        .literal .notranslate}]{.pre} cache-control directive is set
        even for fresh responses

    -   Compute freshness lifetime from [`max-age`{.docutils .literal
        .notranslate}]{.pre} cache-control directive

    -   Compute freshness lifetime from [`Expires`{.docutils .literal
        .notranslate}]{.pre} response header

    -   Compute freshness lifetime from [`Last-Modified`{.docutils
        .literal .notranslate}]{.pre} response header (heuristic used by
        Firefox)

    -   Compute current age from [`Age`{.docutils .literal
        .notranslate}]{.pre} response header

    -   Compute current age from [`Date`{.docutils .literal
        .notranslate}]{.pre} header

    -   Revalidate stale responses based on [`Last-Modified`{.docutils
        .literal .notranslate}]{.pre} response header

    -   Revalidate stale responses based on [`ETag`{.docutils .literal
        .notranslate}]{.pre} response header

    -   Set [`Date`{.docutils .literal .notranslate}]{.pre} header for
        any received response missing it

    -   Support [`max-stale`{.docutils .literal .notranslate}]{.pre}
        cache-control directive in requests

    This allows spiders to be configured with the full RFC2616 cache
    policy, but avoid revalidation on a request-by-request basis, while
    remaining conformant with the HTTP spec.

    Example:

    Add [`Cache-Control:`{.docutils .literal
    .notranslate}]{.pre}` `{.docutils .literal
    .notranslate}[`max-stale=600`{.docutils .literal
    .notranslate}]{.pre} to Request headers to accept responses that
    have exceeded their expiration time by no more than 600 seconds.

    See also: RFC2616, 14.9.3

    What is missing:

    -   [`Pragma:`{.docutils .literal .notranslate}]{.pre}` `{.docutils
        .literal .notranslate}[`no-cache`{.docutils .literal
        .notranslate}]{.pre} support
        [https://www.w3.org/Protocols/rfc2616/rfc2616-sec14.html#sec14.9.1](https://www.w3.org/Protocols/rfc2616/rfc2616-sec14.html#sec14.9.1){.reference
        .external}

    -   [`Vary`{.docutils .literal .notranslate}]{.pre} header support
        [https://www.w3.org/Protocols/rfc2616/rfc2616-sec13.html#sec13.6](https://www.w3.org/Protocols/rfc2616/rfc2616-sec13.html#sec13.6){.reference
        .external}

    -   Invalidation after updates or deletes
        [https://www.w3.org/Protocols/rfc2616/rfc2616-sec13.html#sec13.10](https://www.w3.org/Protocols/rfc2616/rfc2616-sec13.html#sec13.10){.reference
        .external}

    -   ... probably others ..
:::

::: {#filesystem-storage-backend-default .section}
[]{#httpcache-storage-fs}

###### Filesystem storage backend (default)[¶](#filesystem-storage-backend-default "Permalink to this heading"){.headerlink}

*[class]{.pre}[ ]{.w}*[[scrapy.extensions.httpcache.]{.pre}]{.sig-prename .descclassname}[[FilesystemCacheStorage]{.pre}]{.sig-name .descname}[[[\[source\]]{.pre}]{.viewcode-link}](_modules/scrapy/extensions/httpcache.html#FilesystemCacheStorage){.reference .internal}[¶](#scrapy.extensions.httpcache.FilesystemCacheStorage "Permalink to this definition"){.headerlink}

:   File system storage backend is available for the HTTP cache
    middleware.

    Each request/response pair is stored in a different directory
    containing the following files:

    -   [`request_body`{.docutils .literal .notranslate}]{.pre} - the
        plain request body

    -   [`request_headers`{.docutils .literal .notranslate}]{.pre} - the
        request headers (in raw HTTP format)

    -   [`response_body`{.docutils .literal .notranslate}]{.pre} - the
        plain response body

    -   [`response_headers`{.docutils .literal .notranslate}]{.pre} -
        the request headers (in raw HTTP format)

    -   [`meta`{.docutils .literal .notranslate}]{.pre} - some metadata
        of this cache resource in Python [`repr()`{.docutils .literal
        .notranslate}]{.pre} format (grep-friendly format)

    -   [`pickled_meta`{.docutils .literal .notranslate}]{.pre} - the
        same metadata in [`meta`{.docutils .literal .notranslate}]{.pre}
        but pickled for more efficient deserialization

    The directory name is made from the request fingerprint (see
    [`scrapy.utils.request.fingerprint`{.docutils .literal
    .notranslate}]{.pre}), and one level of subdirectories is used to
    avoid creating too many files into the same directory (which is
    inefficient in many file systems). An example directory could be:

    ::: {.highlight-default .notranslate}
    ::: highlight
        /path/to/cache/dir/example.com/72/72811f648e718090f041317756c03adb0ada46c7
    :::
    :::
:::

::: {#dbm-storage-backend .section}
[]{#httpcache-storage-dbm}

###### DBM storage backend[¶](#dbm-storage-backend "Permalink to this heading"){.headerlink}

*[class]{.pre}[ ]{.w}*[[scrapy.extensions.httpcache.]{.pre}]{.sig-prename .descclassname}[[DbmCacheStorage]{.pre}]{.sig-name .descname}[[[\[source\]]{.pre}]{.viewcode-link}](_modules/scrapy/extensions/httpcache.html#DbmCacheStorage){.reference .internal}[¶](#scrapy.extensions.httpcache.DbmCacheStorage "Permalink to this definition"){.headerlink}

:   A [DBM](https://en.wikipedia.org/wiki/Dbm){.reference .external}
    storage backend is also available for the HTTP cache middleware.

    By default, it uses the [[`dbm`{.xref .py .py-mod .docutils .literal
    .notranslate}]{.pre}](https://docs.python.org/3/library/dbm.html#module-dbm "(in Python v3.12)"){.reference
    .external}, but you can change it with the
    [[`HTTPCACHE_DBM_MODULE`{.xref .std .std-setting .docutils .literal
    .notranslate}]{.pre}](#std-setting-HTTPCACHE_DBM_MODULE){.hoverxref
    .tooltip .reference .internal} setting.
:::

::: {#writing-your-own-storage-backend .section}
[]{#httpcache-storage-custom}

###### Writing your own storage backend[¶](#writing-your-own-storage-backend "Permalink to this heading"){.headerlink}

You can implement a cache storage backend by creating a Python class
that defines the methods described below.

[]{#module-scrapy.extensions.httpcache .target}

*[class]{.pre}[ ]{.w}*[[scrapy.extensions.httpcache.]{.pre}]{.sig-prename .descclassname}[[CacheStorage]{.pre}]{.sig-name .descname}[¶](#scrapy.extensions.httpcache.CacheStorage "Permalink to this definition"){.headerlink}

:   

    [[open_spider]{.pre}]{.sig-name .descname}[(]{.sig-paren}*[[spider]{.pre}]{.n}*[)]{.sig-paren}[¶](#scrapy.extensions.httpcache.CacheStorage.open_spider "Permalink to this definition"){.headerlink}

    :   This method gets called after a spider has been opened for
        crawling. It handles the [[`open_spider`{.xref .std .std-signal
        .docutils .literal
        .notranslate}]{.pre}](index.html#std-signal-spider_opened){.hoverxref
        .tooltip .reference .internal} signal.

        Parameters

        :   **spider** ([[`Spider`{.xref .py .py-class .docutils
            .literal
            .notranslate}]{.pre}](index.html#scrapy.Spider "scrapy.Spider"){.reference
            .internal} object) -- the spider which has been opened

    [[close_spider]{.pre}]{.sig-name .descname}[(]{.sig-paren}*[[spider]{.pre}]{.n}*[)]{.sig-paren}[¶](#scrapy.extensions.httpcache.CacheStorage.close_spider "Permalink to this definition"){.headerlink}

    :   This method gets called after a spider has been closed. It
        handles the [[`close_spider`{.xref .std .std-signal .docutils
        .literal
        .notranslate}]{.pre}](index.html#std-signal-spider_closed){.hoverxref
        .tooltip .reference .internal} signal.

        Parameters

        :   **spider** ([[`Spider`{.xref .py .py-class .docutils
            .literal
            .notranslate}]{.pre}](index.html#scrapy.Spider "scrapy.Spider"){.reference
            .internal} object) -- the spider which has been closed

    [[retrieve_response]{.pre}]{.sig-name .descname}[(]{.sig-paren}*[[spider]{.pre}]{.n}*, *[[request]{.pre}]{.n}*[)]{.sig-paren}[¶](#scrapy.extensions.httpcache.CacheStorage.retrieve_response "Permalink to this definition"){.headerlink}

    :   Return response if present in cache, or [`None`{.docutils
        .literal .notranslate}]{.pre} otherwise.

        Parameters

        :   -   **spider** ([[`Spider`{.xref .py .py-class .docutils
                .literal
                .notranslate}]{.pre}](index.html#scrapy.Spider "scrapy.Spider"){.reference
                .internal} object) -- the spider which generated the
                request

            -   **request** ([`Request`{.xref .py .py-class .docutils
                .literal .notranslate}]{.pre} object) -- the request to
                find cached response for

    [[store_response]{.pre}]{.sig-name .descname}[(]{.sig-paren}*[[spider]{.pre}]{.n}*, *[[request]{.pre}]{.n}*, *[[response]{.pre}]{.n}*[)]{.sig-paren}[¶](#scrapy.extensions.httpcache.CacheStorage.store_response "Permalink to this definition"){.headerlink}

    :   Store the given response in the cache.

        Parameters

        :   -   **spider** ([[`Spider`{.xref .py .py-class .docutils
                .literal
                .notranslate}]{.pre}](index.html#scrapy.Spider "scrapy.Spider"){.reference
                .internal} object) -- the spider for which the response
                is intended

            -   **request** ([`Request`{.xref .py .py-class .docutils
                .literal .notranslate}]{.pre} object) -- the
                corresponding request the spider generated

            -   **response** ([[`Response`{.xref .py .py-class .docutils
                .literal
                .notranslate}]{.pre}](index.html#scrapy.http.Response "scrapy.http.Response"){.reference
                .internal} object) -- the response to store in the cache

In order to use your storage backend, set:

-   [[`HTTPCACHE_STORAGE`{.xref .std .std-setting .docutils .literal
    .notranslate}]{.pre}](#std-setting-HTTPCACHE_STORAGE){.hoverxref
    .tooltip .reference .internal} to the Python import path of your
    custom storage class.
:::

::: {#httpcache-middleware-settings .section}
###### HTTPCache middleware settings[¶](#httpcache-middleware-settings "Permalink to this heading"){.headerlink}

The [`HttpCacheMiddleware`{.xref .py .py-class .docutils .literal
.notranslate}]{.pre} can be configured through the following settings:

::: {#httpcache-enabled .section}
[]{#std-setting-HTTPCACHE_ENABLED}HTTPCACHE_ENABLED[¶](#httpcache-enabled "Permalink to this heading"){.headerlink}

Default: [`False`{.docutils .literal .notranslate}]{.pre}

Whether the HTTP cache will be enabled.
:::

::: {#httpcache-expiration-secs .section}
[]{#std-setting-HTTPCACHE_EXPIRATION_SECS}HTTPCACHE_EXPIRATION_SECS[¶](#httpcache-expiration-secs "Permalink to this heading"){.headerlink}

Default: [`0`{.docutils .literal .notranslate}]{.pre}

Expiration time for cached requests, in seconds.

Cached requests older than this time will be re-downloaded. If zero,
cached requests will never expire.
:::

::: {#httpcache-dir .section}
[]{#std-setting-HTTPCACHE_DIR}HTTPCACHE_DIR[¶](#httpcache-dir "Permalink to this heading"){.headerlink}

Default: [`'httpcache'`{.docutils .literal .notranslate}]{.pre}

The directory to use for storing the (low-level) HTTP cache. If empty,
the HTTP cache will be disabled. If a relative path is given, is taken
relative to the project data dir. For more info see: [[Default structure
of Scrapy projects]{.std
.std-ref}](index.html#topics-project-structure){.hoverxref .tooltip
.reference .internal}.
:::

::: {#httpcache-ignore-http-codes .section}
[]{#std-setting-HTTPCACHE_IGNORE_HTTP_CODES}HTTPCACHE_IGNORE_HTTP_CODES[¶](#httpcache-ignore-http-codes "Permalink to this heading"){.headerlink}

Default: [`[]`{.docutils .literal .notranslate}]{.pre}

Don't cache response with these HTTP codes.
:::

::: {#httpcache-ignore-missing .section}
[]{#std-setting-HTTPCACHE_IGNORE_MISSING}HTTPCACHE_IGNORE_MISSING[¶](#httpcache-ignore-missing "Permalink to this heading"){.headerlink}

Default: [`False`{.docutils .literal .notranslate}]{.pre}

If enabled, requests not found in the cache will be ignored instead of
downloaded.
:::

::: {#httpcache-ignore-schemes .section}
[]{#std-setting-HTTPCACHE_IGNORE_SCHEMES}HTTPCACHE_IGNORE_SCHEMES[¶](#httpcache-ignore-schemes "Permalink to this heading"){.headerlink}

Default: [`['file']`{.docutils .literal .notranslate}]{.pre}

Don't cache responses with these URI schemes.
:::

::: {#httpcache-storage .section}
[]{#std-setting-HTTPCACHE_STORAGE}HTTPCACHE_STORAGE[¶](#httpcache-storage "Permalink to this heading"){.headerlink}

Default:
[`'scrapy.extensions.httpcache.FilesystemCacheStorage'`{.docutils
.literal .notranslate}]{.pre}

The class which implements the cache storage backend.
:::

::: {#httpcache-dbm-module .section}
[]{#std-setting-HTTPCACHE_DBM_MODULE}HTTPCACHE_DBM_MODULE[¶](#httpcache-dbm-module "Permalink to this heading"){.headerlink}

Default: [`'dbm'`{.docutils .literal .notranslate}]{.pre}

The database module to use in the [[DBM storage backend]{.std
.std-ref}](#httpcache-storage-dbm){.hoverxref .tooltip .reference
.internal}. This setting is specific to the DBM backend.
:::

::: {#httpcache-policy .section}
[]{#std-setting-HTTPCACHE_POLICY}HTTPCACHE_POLICY[¶](#httpcache-policy "Permalink to this heading"){.headerlink}

Default: [`'scrapy.extensions.httpcache.DummyPolicy'`{.docutils .literal
.notranslate}]{.pre}

The class which implements the cache policy.
:::

::: {#httpcache-gzip .section}
[]{#std-setting-HTTPCACHE_GZIP}HTTPCACHE_GZIP[¶](#httpcache-gzip "Permalink to this heading"){.headerlink}

Default: [`False`{.docutils .literal .notranslate}]{.pre}

If enabled, will compress all cached data with gzip. This setting is
specific to the Filesystem backend.
:::

::: {#httpcache-always-store .section}
[]{#std-setting-HTTPCACHE_ALWAYS_STORE}HTTPCACHE_ALWAYS_STORE[¶](#httpcache-always-store "Permalink to this heading"){.headerlink}

Default: [`False`{.docutils .literal .notranslate}]{.pre}

If enabled, will cache pages unconditionally.

A spider may wish to have all responses available in the cache, for
future use with [`Cache-Control:`{.docutils .literal
.notranslate}]{.pre}` `{.docutils .literal
.notranslate}[`max-stale`{.docutils .literal .notranslate}]{.pre}, for
instance. The DummyPolicy caches all responses but never revalidates
them, and sometimes a more nuanced policy is desirable.

This setting still respects [`Cache-Control:`{.docutils .literal
.notranslate}]{.pre}` `{.docutils .literal
.notranslate}[`no-store`{.docutils .literal .notranslate}]{.pre}
directives in responses. If you don't want that, filter
[`no-store`{.docutils .literal .notranslate}]{.pre} out of the
Cache-Control headers in responses you feed to the cache middleware.
:::

::: {#httpcache-ignore-response-cache-controls .section}
[]{#std-setting-HTTPCACHE_IGNORE_RESPONSE_CACHE_CONTROLS}HTTPCACHE_IGNORE_RESPONSE_CACHE_CONTROLS[¶](#httpcache-ignore-response-cache-controls "Permalink to this heading"){.headerlink}

Default: [`[]`{.docutils .literal .notranslate}]{.pre}

List of Cache-Control directives in responses to be ignored.

Sites often set "no-store", "no-cache", "must-revalidate", etc., but get
upset at the traffic a spider can generate if it actually respects those
directives. This allows to selectively ignore Cache-Control directives
that are known to be unimportant for the sites being crawled.

We assume that the spider will not issue Cache-Control directives in
requests unless it actually needs them, so directives in requests are
not filtered.
:::
:::
:::

::: {#module-scrapy.downloadermiddlewares.httpcompression .section}
[]{#httpcompressionmiddleware}

##### HttpCompressionMiddleware[¶](#module-scrapy.downloadermiddlewares.httpcompression "Permalink to this heading"){.headerlink}

*[class]{.pre}[ ]{.w}*[[scrapy.downloadermiddlewares.httpcompression.]{.pre}]{.sig-prename .descclassname}[[HttpCompressionMiddleware]{.pre}]{.sig-name .descname}[[[\[source\]]{.pre}]{.viewcode-link}](_modules/scrapy/downloadermiddlewares/httpcompression.html#HttpCompressionMiddleware){.reference .internal}[¶](#scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware "Permalink to this definition"){.headerlink}

:   This middleware allows compressed (gzip, deflate) traffic to be
    sent/received from web sites.

    This middleware also supports decoding
    [brotli-compressed](https://www.ietf.org/rfc/rfc7932.txt){.reference
    .external} as well as
    [zstd-compressed](https://www.ietf.org/rfc/rfc8478.txt){.reference
    .external} responses, provided that
    [brotli](https://pypi.org/project/Brotli/){.reference .external} or
    [zstandard](https://pypi.org/project/zstandard/){.reference
    .external} is installed, respectively.

::: {#httpcompressionmiddleware-settings .section}
###### HttpCompressionMiddleware Settings[¶](#httpcompressionmiddleware-settings "Permalink to this heading"){.headerlink}

::: {#compression-enabled .section}
[]{#std-setting-COMPRESSION_ENABLED}COMPRESSION_ENABLED[¶](#compression-enabled "Permalink to this heading"){.headerlink}

Default: [`True`{.docutils .literal .notranslate}]{.pre}

Whether the Compression middleware will be enabled.
:::
:::
:::

::: {#module-scrapy.downloadermiddlewares.httpproxy .section}
[]{#httpproxymiddleware}

##### HttpProxyMiddleware[¶](#module-scrapy.downloadermiddlewares.httpproxy "Permalink to this heading"){.headerlink}

[]{#std-reqmeta-proxy .target}

*[class]{.pre}[ ]{.w}*[[scrapy.downloadermiddlewares.httpproxy.]{.pre}]{.sig-prename .descclassname}[[HttpProxyMiddleware]{.pre}]{.sig-name .descname}[[[\[source\]]{.pre}]{.viewcode-link}](_modules/scrapy/downloadermiddlewares/httpproxy.html#HttpProxyMiddleware){.reference .internal}[¶](#scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware "Permalink to this definition"){.headerlink}

:   This middleware sets the HTTP proxy to use for requests, by setting
    the [`proxy`{.docutils .literal .notranslate}]{.pre} meta value for
    [`Request`{.xref .py .py-class .docutils .literal
    .notranslate}]{.pre} objects.

    Like the Python standard library module [[`urllib.request`{.xref .py
    .py-mod .docutils .literal
    .notranslate}]{.pre}](https://docs.python.org/3/library/urllib.request.html#module-urllib.request "(in Python v3.12)"){.reference
    .external}, it obeys the following environment variables:

    -   [`http_proxy`{.docutils .literal .notranslate}]{.pre}

    -   [`https_proxy`{.docutils .literal .notranslate}]{.pre}

    -   [`no_proxy`{.docutils .literal .notranslate}]{.pre}

    You can also set the meta key [`proxy`{.docutils .literal
    .notranslate}]{.pre} per-request, to a value like
    [`http://some_proxy_server:port`{.docutils .literal
    .notranslate}]{.pre} or
    [`http://username:password@some_proxy_server:port`{.docutils
    .literal .notranslate}]{.pre}. Keep in mind this value will take
    precedence over [`http_proxy`{.docutils .literal
    .notranslate}]{.pre}/[`https_proxy`{.docutils .literal
    .notranslate}]{.pre} environment variables, and it will also ignore
    [`no_proxy`{.docutils .literal .notranslate}]{.pre} environment
    variable.
:::

::: {#module-scrapy.downloadermiddlewares.redirect .section}
[]{#redirectmiddleware}

##### RedirectMiddleware[¶](#module-scrapy.downloadermiddlewares.redirect "Permalink to this heading"){.headerlink}

*[class]{.pre}[ ]{.w}*[[scrapy.downloadermiddlewares.redirect.]{.pre}]{.sig-prename .descclassname}[[RedirectMiddleware]{.pre}]{.sig-name .descname}[[[\[source\]]{.pre}]{.viewcode-link}](_modules/scrapy/downloadermiddlewares/redirect.html#RedirectMiddleware){.reference .internal}[¶](#scrapy.downloadermiddlewares.redirect.RedirectMiddleware "Permalink to this definition"){.headerlink}

:   This middleware handles redirection of requests based on response
    status.

The urls which the request goes through (while being redirected) can be
found in the [`redirect_urls`{.docutils .literal .notranslate}]{.pre}
[`Request.meta`{.xref .py .py-attr .docutils .literal
.notranslate}]{.pre} key.

The reason behind each redirect in [[`redirect_urls`{.xref .std
.std-reqmeta .docutils .literal
.notranslate}]{.pre}](#std-reqmeta-redirect_urls){.hoverxref .tooltip
.reference .internal} can be found in the [`redirect_reasons`{.docutils
.literal .notranslate}]{.pre} [`Request.meta`{.xref .py .py-attr
.docutils .literal .notranslate}]{.pre} key. For example:
[`[301,`{.docutils .literal .notranslate}]{.pre}` `{.docutils .literal
.notranslate}[`302,`{.docutils .literal
.notranslate}]{.pre}` `{.docutils .literal
.notranslate}[`307,`{.docutils .literal
.notranslate}]{.pre}` `{.docutils .literal
.notranslate}[`'meta`{.docutils .literal
.notranslate}]{.pre}` `{.docutils .literal
.notranslate}[`refresh']`{.docutils .literal .notranslate}]{.pre}.

The format of a reason depends on the middleware that handled the
corresponding redirect. For example, [[`RedirectMiddleware`{.xref .py
.py-class .docutils .literal
.notranslate}]{.pre}](#scrapy.downloadermiddlewares.redirect.RedirectMiddleware "scrapy.downloadermiddlewares.redirect.RedirectMiddleware"){.reference
.internal} indicates the triggering response status code as an integer,
while [[`MetaRefreshMiddleware`{.xref .py .py-class .docutils .literal
.notranslate}]{.pre}](#scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware "scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware"){.reference
.internal} always uses the [`'meta`{.docutils .literal
.notranslate}]{.pre}` `{.docutils .literal
.notranslate}[`refresh'`{.docutils .literal .notranslate}]{.pre} string
as reason.

The [[`RedirectMiddleware`{.xref .py .py-class .docutils .literal
.notranslate}]{.pre}](#scrapy.downloadermiddlewares.redirect.RedirectMiddleware "scrapy.downloadermiddlewares.redirect.RedirectMiddleware"){.reference
.internal} can be configured through the following settings (see the
settings documentation for more info):

-   [[`REDIRECT_ENABLED`{.xref .std .std-setting .docutils .literal
    .notranslate}]{.pre}](#std-setting-REDIRECT_ENABLED){.hoverxref
    .tooltip .reference .internal}

-   [[`REDIRECT_MAX_TIMES`{.xref .std .std-setting .docutils .literal
    .notranslate}]{.pre}](#std-setting-REDIRECT_MAX_TIMES){.hoverxref
    .tooltip .reference .internal}

If [`Request.meta`{.xref .py .py-attr .docutils .literal
.notranslate}]{.pre} has [`dont_redirect`{.docutils .literal
.notranslate}]{.pre} key set to True, the request will be ignored by
this middleware.

If you want to handle some redirect status codes in your spider, you can
specify these in the [`handle_httpstatus_list`{.docutils .literal
.notranslate}]{.pre} spider attribute.

For example, if you want the redirect middleware to ignore 301 and 302
responses (and pass them through to your spider) you can do this:

::: {.highlight-python .notranslate}
::: highlight
    class MySpider(CrawlSpider):
        handle_httpstatus_list = [301, 302]
:::
:::

The [`handle_httpstatus_list`{.docutils .literal .notranslate}]{.pre}
key of [`Request.meta`{.xref .py .py-attr .docutils .literal
.notranslate}]{.pre} can also be used to specify which response codes to
allow on a per-request basis. You can also set the meta key
[`handle_httpstatus_all`{.docutils .literal .notranslate}]{.pre} to
[`True`{.docutils .literal .notranslate}]{.pre} if you want to allow any
response code for a request.

::: {#redirectmiddleware-settings .section}
###### RedirectMiddleware settings[¶](#redirectmiddleware-settings "Permalink to this heading"){.headerlink}

::: {#redirect-enabled .section}
[]{#std-setting-REDIRECT_ENABLED}REDIRECT_ENABLED[¶](#redirect-enabled "Permalink to this heading"){.headerlink}

Default: [`True`{.docutils .literal .notranslate}]{.pre}

Whether the Redirect middleware will be enabled.
:::

::: {#redirect-max-times .section}
[]{#std-setting-REDIRECT_MAX_TIMES}REDIRECT_MAX_TIMES[¶](#redirect-max-times "Permalink to this heading"){.headerlink}

Default: [`20`{.docutils .literal .notranslate}]{.pre}

The maximum number of redirections that will be followed for a single
request. After this maximum, the request's response is returned as is.
:::
:::
:::

::: {#metarefreshmiddleware .section}
##### MetaRefreshMiddleware[¶](#metarefreshmiddleware "Permalink to this heading"){.headerlink}

*[class]{.pre}[ ]{.w}*[[scrapy.downloadermiddlewares.redirect.]{.pre}]{.sig-prename .descclassname}[[MetaRefreshMiddleware]{.pre}]{.sig-name .descname}[[[\[source\]]{.pre}]{.viewcode-link}](_modules/scrapy/downloadermiddlewares/redirect.html#MetaRefreshMiddleware){.reference .internal}[¶](#scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware "Permalink to this definition"){.headerlink}

:   This middleware handles redirection of requests based on
    meta-refresh html tag.

The [[`MetaRefreshMiddleware`{.xref .py .py-class .docutils .literal
.notranslate}]{.pre}](#scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware "scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware"){.reference
.internal} can be configured through the following settings (see the
settings documentation for more info):

-   [[`METAREFRESH_ENABLED`{.xref .std .std-setting .docutils .literal
    .notranslate}]{.pre}](#std-setting-METAREFRESH_ENABLED){.hoverxref
    .tooltip .reference .internal}

-   [[`METAREFRESH_IGNORE_TAGS`{.xref .std .std-setting .docutils
    .literal
    .notranslate}]{.pre}](#std-setting-METAREFRESH_IGNORE_TAGS){.hoverxref
    .tooltip .reference .internal}

-   [[`METAREFRESH_MAXDELAY`{.xref .std .std-setting .docutils .literal
    .notranslate}]{.pre}](#std-setting-METAREFRESH_MAXDELAY){.hoverxref
    .tooltip .reference .internal}

This middleware obey [[`REDIRECT_MAX_TIMES`{.xref .std .std-setting
.docutils .literal
.notranslate}]{.pre}](#std-setting-REDIRECT_MAX_TIMES){.hoverxref
.tooltip .reference .internal} setting, [[`dont_redirect`{.xref .std
.std-reqmeta .docutils .literal
.notranslate}]{.pre}](#std-reqmeta-dont_redirect){.hoverxref .tooltip
.reference .internal}, [[`redirect_urls`{.xref .std .std-reqmeta
.docutils .literal
.notranslate}]{.pre}](#std-reqmeta-redirect_urls){.hoverxref .tooltip
.reference .internal} and [[`redirect_reasons`{.xref .std .std-reqmeta
.docutils .literal
.notranslate}]{.pre}](#std-reqmeta-redirect_reasons){.hoverxref .tooltip
.reference .internal} request meta keys as described for
[[`RedirectMiddleware`{.xref .py .py-class .docutils .literal
.notranslate}]{.pre}](#scrapy.downloadermiddlewares.redirect.RedirectMiddleware "scrapy.downloadermiddlewares.redirect.RedirectMiddleware"){.reference
.internal}

::: {#metarefreshmiddleware-settings .section}
###### MetaRefreshMiddleware settings[¶](#metarefreshmiddleware-settings "Permalink to this heading"){.headerlink}

::: {#metarefresh-enabled .section}
[]{#std-setting-METAREFRESH_ENABLED}METAREFRESH_ENABLED[¶](#metarefresh-enabled "Permalink to this heading"){.headerlink}

Default: [`True`{.docutils .literal .notranslate}]{.pre}

Whether the Meta Refresh middleware will be enabled.
:::

::: {#metarefresh-ignore-tags .section}
[]{#std-setting-METAREFRESH_IGNORE_TAGS}METAREFRESH_IGNORE_TAGS[¶](#metarefresh-ignore-tags "Permalink to this heading"){.headerlink}

Default: [`[]`{.docutils .literal .notranslate}]{.pre}

Meta tags within these tags are ignored.

::: versionchanged
[Changed in version 2.0: ]{.versionmodified .changed}The default value
of [[`METAREFRESH_IGNORE_TAGS`{.xref .std .std-setting .docutils
.literal
.notranslate}]{.pre}](#std-setting-METAREFRESH_IGNORE_TAGS){.hoverxref
.tooltip .reference .internal} changed from [`['script',`{.docutils
.literal .notranslate}]{.pre}` `{.docutils .literal
.notranslate}[`'noscript']`{.docutils .literal .notranslate}]{.pre} to
[`[]`{.docutils .literal .notranslate}]{.pre}.
:::
:::

::: {#metarefresh-maxdelay .section}
[]{#std-setting-METAREFRESH_MAXDELAY}METAREFRESH_MAXDELAY[¶](#metarefresh-maxdelay "Permalink to this heading"){.headerlink}

Default: [`100`{.docutils .literal .notranslate}]{.pre}

The maximum meta-refresh delay (in seconds) to follow the redirection.
Some sites use meta-refresh for redirecting to a session expired page,
so we restrict automatic redirection to the maximum delay.
:::
:::
:::

::: {#module-scrapy.downloadermiddlewares.retry .section}
[]{#retrymiddleware}

##### RetryMiddleware[¶](#module-scrapy.downloadermiddlewares.retry "Permalink to this heading"){.headerlink}

*[class]{.pre}[ ]{.w}*[[scrapy.downloadermiddlewares.retry.]{.pre}]{.sig-prename .descclassname}[[RetryMiddleware]{.pre}]{.sig-name .descname}[[[\[source\]]{.pre}]{.viewcode-link}](_modules/scrapy/downloadermiddlewares/retry.html#RetryMiddleware){.reference .internal}[¶](#scrapy.downloadermiddlewares.retry.RetryMiddleware "Permalink to this definition"){.headerlink}

:   A middleware to retry failed requests that are potentially caused by
    temporary problems such as a connection timeout or HTTP 500 error.

Failed pages are collected on the scraping process and rescheduled at
the end, once the spider has finished crawling all regular (non failed)
pages.

The [[`RetryMiddleware`{.xref .py .py-class .docutils .literal
.notranslate}]{.pre}](#scrapy.downloadermiddlewares.retry.RetryMiddleware "scrapy.downloadermiddlewares.retry.RetryMiddleware"){.reference
.internal} can be configured through the following settings (see the
settings documentation for more info):

-   [[`RETRY_ENABLED`{.xref .std .std-setting .docutils .literal
    .notranslate}]{.pre}](#std-setting-RETRY_ENABLED){.hoverxref
    .tooltip .reference .internal}

-   [[`RETRY_TIMES`{.xref .std .std-setting .docutils .literal
    .notranslate}]{.pre}](#std-setting-RETRY_TIMES){.hoverxref .tooltip
    .reference .internal}

-   [[`RETRY_HTTP_CODES`{.xref .std .std-setting .docutils .literal
    .notranslate}]{.pre}](#std-setting-RETRY_HTTP_CODES){.hoverxref
    .tooltip .reference .internal}

-   [[`RETRY_EXCEPTIONS`{.xref .std .std-setting .docutils .literal
    .notranslate}]{.pre}](#std-setting-RETRY_EXCEPTIONS){.hoverxref
    .tooltip .reference .internal}

If [`Request.meta`{.xref .py .py-attr .docutils .literal
.notranslate}]{.pre} has [`dont_retry`{.docutils .literal
.notranslate}]{.pre} key set to True, the request will be ignored by
this middleware.

To retry requests from a spider callback, you can use the
[[`get_retry_request()`{.xref .py .py-func .docutils .literal
.notranslate}]{.pre}](#scrapy.downloadermiddlewares.retry.get_retry_request "scrapy.downloadermiddlewares.retry.get_retry_request"){.reference
.internal} function:

[[scrapy.downloadermiddlewares.retry.]{.pre}]{.sig-prename .descclassname}[[get_retry_request]{.pre}]{.sig-name .descname}[(]{.sig-paren}*[[request:]{.pre} [\~scrapy.http.request.Request]{.pre}]{.n}*, *[[\*]{.pre}]{.n}*, *[[spider:]{.pre} [\~scrapy.spiders.Spider]{.pre}]{.n}*, *[[reason:]{.pre} [\~typing.Union\[str]{.pre}]{.n}*, *[[Exception]{.pre}]{.n}*, *[[\~typing.Type\[Exception\]\]]{.pre} [=]{.pre} [\'unspecified\']{.pre}]{.n}*, *[[max_retry_times:]{.pre} [\~typing.Optional\[int\]]{.pre} [=]{.pre} [None]{.pre}]{.n}*, *[[priority_adjust:]{.pre} [\~typing.Optional\[int\]]{.pre} [=]{.pre} [None]{.pre}]{.n}*, *[[logger:]{.pre} [\~logging.Logger]{.pre} [=]{.pre} [\<Logger]{.pre} [scrapy.downloadermiddlewares.retry]{.pre} [(WARNING)\>]{.pre}]{.n}*, *[[stats_base_key:]{.pre} [str]{.pre} [=]{.pre} [\'retry\']{.pre}]{.n}*[)]{.sig-paren} [[→]{.sig-return-icon} [[[Optional]{.pre}](https://docs.python.org/3/library/typing.html#typing.Optional "(in Python v3.12)"){.reference .external}[[\[]{.pre}]{.p}[[Request]{.pre}](index.html#scrapy.http.Request "scrapy.http.request.Request"){.reference .internal}[[\]]{.pre}]{.p}]{.sig-return-typehint}]{.sig-return}[[[\[source\]]{.pre}]{.viewcode-link}](_modules/scrapy/downloadermiddlewares/retry.html#get_retry_request){.reference .internal}[¶](#scrapy.downloadermiddlewares.retry.get_retry_request "Permalink to this definition"){.headerlink}

:   Returns a new [`Request`{.xref .py .py-class .docutils .literal
    .notranslate}]{.pre} object to retry the specified request, or
    [`None`{.docutils .literal .notranslate}]{.pre} if retries of the
    specified request have been exhausted.

    For example, in a [[`Spider`{.xref .py .py-class .docutils .literal
    .notranslate}]{.pre}](index.html#scrapy.Spider "scrapy.Spider"){.reference
    .internal} callback, you could use it as follows:

    ::: {.highlight-default .notranslate}
    ::: highlight
        def parse(self, response):
            if not response.text:
                new_request_or_none = get_retry_request(
                    response.request,
                    spider=self,
                    reason='empty',
                )
                return new_request_or_none
    :::
    :::

    *spider* is the [[`Spider`{.xref .py .py-class .docutils .literal
    .notranslate}]{.pre}](index.html#scrapy.Spider "scrapy.Spider"){.reference
    .internal} instance which is asking for the retry request. It is
    used to access the [[settings]{.std
    .std-ref}](index.html#topics-settings){.hoverxref .tooltip
    .reference .internal} and [[stats]{.std
    .std-ref}](index.html#topics-stats){.hoverxref .tooltip .reference
    .internal}, and to provide extra logging context (see
    [[`logging.debug()`{.xref .py .py-func .docutils .literal
    .notranslate}]{.pre}](https://docs.python.org/3/library/logging.html#logging.debug "(in Python v3.12)"){.reference
    .external}).

    *reason* is a string or an [[`Exception`{.xref .py .py-class
    .docutils .literal
    .notranslate}]{.pre}](https://docs.python.org/3/library/exceptions.html#Exception "(in Python v3.12)"){.reference
    .external} object that indicates the reason why the request needs to
    be retried. It is used to name retry stats.

    *max_retry_times* is a number that determines the maximum number of
    times that *request* can be retried. If not specified or
    [`None`{.docutils .literal .notranslate}]{.pre}, the number is read
    from the [[`max_retry_times`{.xref .std .std-reqmeta .docutils
    .literal
    .notranslate}]{.pre}](index.html#std-reqmeta-max_retry_times){.hoverxref
    .tooltip .reference .internal} meta key of the request. If the
    [[`max_retry_times`{.xref .std .std-reqmeta .docutils .literal
    .notranslate}]{.pre}](index.html#std-reqmeta-max_retry_times){.hoverxref
    .tooltip .reference .internal} meta key is not defined or
    [`None`{.docutils .literal .notranslate}]{.pre}, the number is read
    from the [[`RETRY_TIMES`{.xref .std .std-setting .docutils .literal
    .notranslate}]{.pre}](#std-setting-RETRY_TIMES){.hoverxref .tooltip
    .reference .internal} setting.

    *priority_adjust* is a number that determines how the priority of
    the new request changes in relation to *request*. If not specified,
    the number is read from the [[`RETRY_PRIORITY_ADJUST`{.xref .std
    .std-setting .docutils .literal
    .notranslate}]{.pre}](#std-setting-RETRY_PRIORITY_ADJUST){.hoverxref
    .tooltip .reference .internal} setting.

    *logger* is the logging.Logger object to be used when logging
    messages

    *stats_base_key* is a string to be used as the base key for the
    retry-related job stats

::: {#retrymiddleware-settings .section}
###### RetryMiddleware Settings[¶](#retrymiddleware-settings "Permalink to this heading"){.headerlink}

::: {#retry-enabled .section}
[]{#std-setting-RETRY_ENABLED}RETRY_ENABLED[¶](#retry-enabled "Permalink to this heading"){.headerlink}

Default: [`True`{.docutils .literal .notranslate}]{.pre}

Whether the Retry middleware will be enabled.
:::

::: {#retry-times .section}
[]{#std-setting-RETRY_TIMES}RETRY_TIMES[¶](#retry-times "Permalink to this heading"){.headerlink}

Default: [`2`{.docutils .literal .notranslate}]{.pre}

Maximum number of times to retry, in addition to the first download.

Maximum number of retries can also be specified per-request using
[[`max_retry_times`{.xref .std .std-reqmeta .docutils .literal
.notranslate}]{.pre}](index.html#std-reqmeta-max_retry_times){.hoverxref
.tooltip .reference .internal} attribute of [`Request.meta`{.xref .py
.py-attr .docutils .literal .notranslate}]{.pre}. When initialized, the
[[`max_retry_times`{.xref .std .std-reqmeta .docutils .literal
.notranslate}]{.pre}](index.html#std-reqmeta-max_retry_times){.hoverxref
.tooltip .reference .internal} meta key takes higher precedence over the
[[`RETRY_TIMES`{.xref .std .std-setting .docutils .literal
.notranslate}]{.pre}](#std-setting-RETRY_TIMES){.hoverxref .tooltip
.reference .internal} setting.
:::

::: {#retry-http-codes .section}
[]{#std-setting-RETRY_HTTP_CODES}RETRY_HTTP_CODES[¶](#retry-http-codes "Permalink to this heading"){.headerlink}

Default: [`[500,`{.docutils .literal .notranslate}]{.pre}` `{.docutils
.literal .notranslate}[`502,`{.docutils .literal
.notranslate}]{.pre}` `{.docutils .literal
.notranslate}[`503,`{.docutils .literal
.notranslate}]{.pre}` `{.docutils .literal
.notranslate}[`504,`{.docutils .literal
.notranslate}]{.pre}` `{.docutils .literal
.notranslate}[`522,`{.docutils .literal
.notranslate}]{.pre}` `{.docutils .literal
.notranslate}[`524,`{.docutils .literal
.notranslate}]{.pre}` `{.docutils .literal
.notranslate}[`408,`{.docutils .literal
.notranslate}]{.pre}` `{.docutils .literal
.notranslate}[`429]`{.docutils .literal .notranslate}]{.pre}

Which HTTP response codes to retry. Other errors (DNS lookup issues,
connections lost, etc) are always retried.

In some cases you may want to add 400 to [[`RETRY_HTTP_CODES`{.xref .std
.std-setting .docutils .literal
.notranslate}]{.pre}](#std-setting-RETRY_HTTP_CODES){.hoverxref .tooltip
.reference .internal} because it is a common code used to indicate
server overload. It is not included by default because HTTP specs say
so.
:::

::: {#retry-exceptions .section}
[]{#std-setting-RETRY_EXCEPTIONS}RETRY_EXCEPTIONS[¶](#retry-exceptions "Permalink to this heading"){.headerlink}

Default:

::: {.highlight-default .notranslate}
::: highlight
    [
        'twisted.internet.defer.TimeoutError',
        'twisted.internet.error.TimeoutError',
        'twisted.internet.error.DNSLookupError',
        'twisted.internet.error.ConnectionRefusedError',
        'twisted.internet.error.ConnectionDone',
        'twisted.internet.error.ConnectError',
        'twisted.internet.error.ConnectionLost',
        'twisted.internet.error.TCPTimedOutError',
        'twisted.web.client.ResponseFailed',
        IOError,
        'scrapy.core.downloader.handlers.http11.TunnelError',
    ]
:::
:::

List of exceptions to retry.

Each list entry may be an exception type or its import path as a string.

An exception will not be caught when the exception type is not in
[[`RETRY_EXCEPTIONS`{.xref .std .std-setting .docutils .literal
.notranslate}]{.pre}](#std-setting-RETRY_EXCEPTIONS){.hoverxref .tooltip
.reference .internal} or when the maximum number of retries for a
request has been exceeded (see [[`RETRY_TIMES`{.xref .std .std-setting
.docutils .literal
.notranslate}]{.pre}](#std-setting-RETRY_TIMES){.hoverxref .tooltip
.reference .internal}). To learn about uncaught exception propagation,
see [[`process_exception()`{.xref .py .py-meth .docutils .literal
.notranslate}]{.pre}](#scrapy.downloadermiddlewares.DownloaderMiddleware.process_exception "scrapy.downloadermiddlewares.DownloaderMiddleware.process_exception"){.reference
.internal}.
:::

::: {#retry-priority-adjust .section}
[]{#std-setting-RETRY_PRIORITY_ADJUST}RETRY_PRIORITY_ADJUST[¶](#retry-priority-adjust "Permalink to this heading"){.headerlink}

Default: [`-1`{.docutils .literal .notranslate}]{.pre}

Adjust retry request priority relative to original request:

-   a positive priority adjust means higher priority.

-   **a negative priority adjust (default) means lower priority.**
:::
:::
:::

::: {#module-scrapy.downloadermiddlewares.robotstxt .section}
[]{#robotstxtmiddleware}[]{#topics-dlmw-robots}

##### RobotsTxtMiddleware[¶](#module-scrapy.downloadermiddlewares.robotstxt "Permalink to this heading"){.headerlink}

*[class]{.pre}[ ]{.w}*[[scrapy.downloadermiddlewares.robotstxt.]{.pre}]{.sig-prename .descclassname}[[RobotsTxtMiddleware]{.pre}]{.sig-name .descname}[[[\[source\]]{.pre}]{.viewcode-link}](_modules/scrapy/downloadermiddlewares/robotstxt.html#RobotsTxtMiddleware){.reference .internal}[¶](#scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware "Permalink to this definition"){.headerlink}

:   This middleware filters out requests forbidden by the robots.txt
    exclusion standard.

    To make sure Scrapy respects robots.txt make sure the middleware is
    enabled and the [[`ROBOTSTXT_OBEY`{.xref .std .std-setting .docutils
    .literal
    .notranslate}]{.pre}](index.html#std-setting-ROBOTSTXT_OBEY){.hoverxref
    .tooltip .reference .internal} setting is enabled.

    The [[`ROBOTSTXT_USER_AGENT`{.xref .std .std-setting .docutils
    .literal
    .notranslate}]{.pre}](index.html#std-setting-ROBOTSTXT_USER_AGENT){.hoverxref
    .tooltip .reference .internal} setting can be used to specify the
    user agent string to use for matching in the
    [robots.txt](https://www.robotstxt.org/){.reference .external} file.
    If it is [`None`{.docutils .literal .notranslate}]{.pre}, the
    User-Agent header you are sending with the request or the
    [[`USER_AGENT`{.xref .std .std-setting .docutils .literal
    .notranslate}]{.pre}](index.html#std-setting-USER_AGENT){.hoverxref
    .tooltip .reference .internal} setting (in that order) will be used
    for determining the user agent to use in the
    [robots.txt](https://www.robotstxt.org/){.reference .external} file.

    This middleware has to be combined with a
    [robots.txt](https://www.robotstxt.org/){.reference .external}
    parser.

    Scrapy ships with support for the following
    [robots.txt](https://www.robotstxt.org/){.reference .external}
    parsers:

    -   [[Protego]{.std .std-ref}](#protego-parser){.hoverxref .tooltip
        .reference .internal} (default)

    -   [[RobotFileParser]{.std
        .std-ref}](#python-robotfileparser){.hoverxref .tooltip
        .reference .internal}

    -   [[Robotexclusionrulesparser]{.std
        .std-ref}](#rerp-parser){.hoverxref .tooltip .reference
        .internal}

    -   [[Reppy]{.std .std-ref}](#reppy-parser){.hoverxref .tooltip
        .reference .internal} (deprecated)

    You can change the
    [robots.txt](https://www.robotstxt.org/){.reference .external}
    parser with the [[`ROBOTSTXT_PARSER`{.xref .std .std-setting
    .docutils .literal
    .notranslate}]{.pre}](index.html#std-setting-ROBOTSTXT_PARSER){.hoverxref
    .tooltip .reference .internal} setting. Or you can also [[implement
    support for a new parser]{.std
    .std-ref}](#support-for-new-robots-parser){.hoverxref .tooltip
    .reference .internal}.

If [`Request.meta`{.xref .py .py-attr .docutils .literal
.notranslate}]{.pre} has [`dont_obey_robotstxt`{.docutils .literal
.notranslate}]{.pre} key set to True the request will be ignored by this
middleware even if [[`ROBOTSTXT_OBEY`{.xref .std .std-setting .docutils
.literal
.notranslate}]{.pre}](index.html#std-setting-ROBOTSTXT_OBEY){.hoverxref
.tooltip .reference .internal} is enabled.

Parsers vary in several aspects:

-   Language of implementation

-   Supported specification

-   Support for wildcard matching

-   Usage of [length based
    rule](https://developers.google.com/search/reference/robots_txt#order-of-precedence-for-group-member-lines){.reference
    .external}: in particular for [`Allow`{.docutils .literal
    .notranslate}]{.pre} and [`Disallow`{.docutils .literal
    .notranslate}]{.pre} directives, where the most specific rule based
    on the length of the path trumps the less specific (shorter) rule

Performance comparison of different parsers is available at [the
following link](https://github.com/scrapy/scrapy/issues/3969){.reference
.external}.

::: {#protego-parser .section}
[]{#id1}

###### Protego parser[¶](#protego-parser "Permalink to this heading"){.headerlink}

Based on [Protego](https://github.com/scrapy/protego){.reference
.external}:

-   implemented in Python

-   is compliant with [Google's Robots.txt
    Specification](https://developers.google.com/search/reference/robots_txt){.reference
    .external}

-   supports wildcard matching

-   uses the length based rule

Scrapy uses this parser by default.
:::

::: {#robotfileparser .section}
[]{#python-robotfileparser}

###### RobotFileParser[¶](#robotfileparser "Permalink to this heading"){.headerlink}

Based on [[`RobotFileParser`{.xref .py .py-class .docutils .literal
.notranslate}]{.pre}](https://docs.python.org/3/library/urllib.robotparser.html#urllib.robotparser.RobotFileParser "(in Python v3.12)"){.reference
.external}:

-   is Python's built-in
    [robots.txt](https://www.robotstxt.org/){.reference .external}
    parser

-   is compliant with [Martijn Koster's 1996 draft
    specification](https://www.robotstxt.org/norobots-rfc.txt){.reference
    .external}

-   lacks support for wildcard matching

-   doesn't use the length based rule

It is faster than Protego and backward-compatible with versions of
Scrapy before 1.8.0.

In order to use this parser, set:

-   [[`ROBOTSTXT_PARSER`{.xref .std .std-setting .docutils .literal
    .notranslate}]{.pre}](index.html#std-setting-ROBOTSTXT_PARSER){.hoverxref
    .tooltip .reference .internal} to
    [`scrapy.robotstxt.PythonRobotParser`{.docutils .literal
    .notranslate}]{.pre}
:::

::: {#reppy-parser .section}
[]{#id2}

###### Reppy parser[¶](#reppy-parser "Permalink to this heading"){.headerlink}

Based on [Reppy](https://github.com/seomoz/reppy/){.reference
.external}:

-   is a Python wrapper around [Robots Exclusion Protocol Parser for
    C++](https://github.com/seomoz/rep-cpp){.reference .external}

-   is compliant with [Martijn Koster's 1996 draft
    specification](https://www.robotstxt.org/norobots-rfc.txt){.reference
    .external}

-   supports wildcard matching

-   uses the length based rule

Native implementation, provides better speed than Protego.

In order to use this parser:

-   Install [Reppy](https://github.com/seomoz/reppy/){.reference
    .external} by running [`pip`{.docutils .literal
    .notranslate}]{.pre}` `{.docutils .literal
    .notranslate}[`install`{.docutils .literal
    .notranslate}]{.pre}` `{.docutils .literal
    .notranslate}[`reppy`{.docutils .literal .notranslate}]{.pre}

    > <div>
    >
    > ::: {.admonition .warning}
    > Warning
    >
    > [Upstream issue
    > #122](https://github.com/seomoz/reppy/issues/122){.reference
    > .external} prevents reppy usage in Python 3.9+. Because of this
    > the Reppy parser is deprecated.
    > :::
    >
    > </div>

-   Set [[`ROBOTSTXT_PARSER`{.xref .std .std-setting .docutils .literal
    .notranslate}]{.pre}](index.html#std-setting-ROBOTSTXT_PARSER){.hoverxref
    .tooltip .reference .internal} setting to
    [`scrapy.robotstxt.ReppyRobotParser`{.docutils .literal
    .notranslate}]{.pre}
:::

::: {#robotexclusionrulesparser .section}
[]{#rerp-parser}

###### Robotexclusionrulesparser[¶](#robotexclusionrulesparser "Permalink to this heading"){.headerlink}

Based on
[Robotexclusionrulesparser](http://nikitathespider.com/python/rerp/){.reference
.external}:

-   implemented in Python

-   is compliant with [Martijn Koster's 1996 draft
    specification](https://www.robotstxt.org/norobots-rfc.txt){.reference
    .external}

-   supports wildcard matching

-   doesn't use the length based rule

In order to use this parser:

-   Install
    [Robotexclusionrulesparser](http://nikitathespider.com/python/rerp/){.reference
    .external} by running [`pip`{.docutils .literal
    .notranslate}]{.pre}` `{.docutils .literal
    .notranslate}[`install`{.docutils .literal
    .notranslate}]{.pre}` `{.docutils .literal
    .notranslate}[`robotexclusionrulesparser`{.docutils .literal
    .notranslate}]{.pre}

-   Set [[`ROBOTSTXT_PARSER`{.xref .std .std-setting .docutils .literal
    .notranslate}]{.pre}](index.html#std-setting-ROBOTSTXT_PARSER){.hoverxref
    .tooltip .reference .internal} setting to
    [`scrapy.robotstxt.RerpRobotParser`{.docutils .literal
    .notranslate}]{.pre}
:::

::: {#implementing-support-for-a-new-parser .section}
[]{#support-for-new-robots-parser}

###### Implementing support for a new parser[¶](#implementing-support-for-a-new-parser "Permalink to this heading"){.headerlink}

You can implement support for a new
[robots.txt](https://www.robotstxt.org/){.reference .external} parser by
subclassing the abstract base class [[`RobotParser`{.xref .py .py-class
.docutils .literal
.notranslate}]{.pre}](#scrapy.robotstxt.RobotParser "scrapy.robotstxt.RobotParser"){.reference
.internal} and implementing the methods described below.

[]{#module-scrapy.robotstxt .target}

*[class]{.pre}[ ]{.w}*[[scrapy.robotstxt.]{.pre}]{.sig-prename .descclassname}[[RobotParser]{.pre}]{.sig-name .descname}[[[\[source\]]{.pre}]{.viewcode-link}](_modules/scrapy/robotstxt.html#RobotParser){.reference .internal}[¶](#scrapy.robotstxt.RobotParser "Permalink to this definition"){.headerlink}

:   

    *[abstract]{.pre}[ ]{.w}*[[allowed]{.pre}]{.sig-name .descname}[(]{.sig-paren}*[[url]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[[[Union]{.pre}](https://docs.python.org/3/library/typing.html#typing.Union "(in Python v3.12)"){.reference .external}[[\[]{.pre}]{.p}[[str]{.pre}](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.12)"){.reference .external}[[,]{.pre}]{.p}[ ]{.w}[[bytes]{.pre}](https://docs.python.org/3/library/stdtypes.html#bytes "(in Python v3.12)"){.reference .external}[[\]]{.pre}]{.p}]{.n}*, *[[user_agent]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[[[Union]{.pre}](https://docs.python.org/3/library/typing.html#typing.Union "(in Python v3.12)"){.reference .external}[[\[]{.pre}]{.p}[[str]{.pre}](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.12)"){.reference .external}[[,]{.pre}]{.p}[ ]{.w}[[bytes]{.pre}](https://docs.python.org/3/library/stdtypes.html#bytes "(in Python v3.12)"){.reference .external}[[\]]{.pre}]{.p}]{.n}*[)]{.sig-paren} [[→]{.sig-return-icon} [[[bool]{.pre}](https://docs.python.org/3/library/functions.html#bool "(in Python v3.12)"){.reference .external}]{.sig-return-typehint}]{.sig-return}[[[\[source\]]{.pre}]{.viewcode-link}](_modules/scrapy/robotstxt.html#RobotParser.allowed){.reference .internal}[¶](#scrapy.robotstxt.RobotParser.allowed "Permalink to this definition"){.headerlink}

    :   Return [`True`{.docutils .literal .notranslate}]{.pre} if
        [`user_agent`{.docutils .literal .notranslate}]{.pre} is allowed
        to crawl [`url`{.docutils .literal .notranslate}]{.pre},
        otherwise return [`False`{.docutils .literal
        .notranslate}]{.pre}.

        Parameters

        :   -   **url**
                ([*str*](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.12)"){.reference
                .external} *or*
                [*bytes*](https://docs.python.org/3/library/stdtypes.html#bytes "(in Python v3.12)"){.reference
                .external}) -- Absolute URL

            -   **user_agent**
                ([*str*](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.12)"){.reference
                .external} *or*
                [*bytes*](https://docs.python.org/3/library/stdtypes.html#bytes "(in Python v3.12)"){.reference
                .external}) -- User agent

    *[abstract]{.pre}[ ]{.w}[classmethod]{.pre}[ ]{.w}*[[from_crawler]{.pre}]{.sig-name .descname}[(]{.sig-paren}*[[crawler]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[[[Crawler]{.pre}](index.html#scrapy.crawler.Crawler "scrapy.crawler.Crawler"){.reference .internal}]{.n}*, *[[robotstxt_body]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[[[bytes]{.pre}](https://docs.python.org/3/library/stdtypes.html#bytes "(in Python v3.12)"){.reference .external}]{.n}*[)]{.sig-paren} [[→]{.sig-return-icon} [[Self]{.pre}]{.sig-return-typehint}]{.sig-return}[[[\[source\]]{.pre}]{.viewcode-link}](_modules/scrapy/robotstxt.html#RobotParser.from_crawler){.reference .internal}[¶](#scrapy.robotstxt.RobotParser.from_crawler "Permalink to this definition"){.headerlink}

    :   Parse the content of a
        [robots.txt](https://www.robotstxt.org/){.reference .external}
        file as bytes. This must be a class method. It must return a new
        instance of the parser backend.

        Parameters

        :   -   **crawler** ([[`Crawler`{.xref .py .py-class .docutils
                .literal
                .notranslate}]{.pre}](index.html#scrapy.crawler.Crawler "scrapy.crawler.Crawler"){.reference
                .internal} instance) -- crawler which made the request

            -   **robotstxt_body**
                ([*bytes*](https://docs.python.org/3/library/stdtypes.html#bytes "(in Python v3.12)"){.reference
                .external}) -- content of a
                [robots.txt](https://www.robotstxt.org/){.reference
                .external} file.
:::
:::

::: {#module-scrapy.downloadermiddlewares.stats .section}
[]{#downloaderstats}

##### DownloaderStats[¶](#module-scrapy.downloadermiddlewares.stats "Permalink to this heading"){.headerlink}

*[class]{.pre}[ ]{.w}*[[scrapy.downloadermiddlewares.stats.]{.pre}]{.sig-prename .descclassname}[[DownloaderStats]{.pre}]{.sig-name .descname}[[[\[source\]]{.pre}]{.viewcode-link}](_modules/scrapy/downloadermiddlewares/stats.html#DownloaderStats){.reference .internal}[¶](#scrapy.downloadermiddlewares.stats.DownloaderStats "Permalink to this definition"){.headerlink}

:   Middleware that stores stats of all requests, responses and
    exceptions that pass through it.

    To use this middleware you must enable the
    [[`DOWNLOADER_STATS`{.xref .std .std-setting .docutils .literal
    .notranslate}]{.pre}](index.html#std-setting-DOWNLOADER_STATS){.hoverxref
    .tooltip .reference .internal} setting.
:::

::: {#module-scrapy.downloadermiddlewares.useragent .section}
[]{#useragentmiddleware}

##### UserAgentMiddleware[¶](#module-scrapy.downloadermiddlewares.useragent "Permalink to this heading"){.headerlink}

*[class]{.pre}[ ]{.w}*[[scrapy.downloadermiddlewares.useragent.]{.pre}]{.sig-prename .descclassname}[[UserAgentMiddleware]{.pre}]{.sig-name .descname}[[[\[source\]]{.pre}]{.viewcode-link}](_modules/scrapy/downloadermiddlewares/useragent.html#UserAgentMiddleware){.reference .internal}[¶](#scrapy.downloadermiddlewares.useragent.UserAgentMiddleware "Permalink to this definition"){.headerlink}

:   Middleware that allows spiders to override the default user agent.

    In order for a spider to override the default user agent, its
    [`user_agent`{.docutils .literal .notranslate}]{.pre} attribute must
    be set.
:::

::: {#module-scrapy.downloadermiddlewares.ajaxcrawl .section}
[]{#ajaxcrawlmiddleware}[]{#ajaxcrawl-middleware}

##### AjaxCrawlMiddleware[¶](#module-scrapy.downloadermiddlewares.ajaxcrawl "Permalink to this heading"){.headerlink}

*[class]{.pre}[ ]{.w}*[[scrapy.downloadermiddlewares.ajaxcrawl.]{.pre}]{.sig-prename .descclassname}[[AjaxCrawlMiddleware]{.pre}]{.sig-name .descname}[[[\[source\]]{.pre}]{.viewcode-link}](_modules/scrapy/downloadermiddlewares/ajaxcrawl.html#AjaxCrawlMiddleware){.reference .internal}[¶](#scrapy.downloadermiddlewares.ajaxcrawl.AjaxCrawlMiddleware "Permalink to this definition"){.headerlink}

:   Middleware that finds 'AJAX crawlable' page variants based on
    meta-fragment html tag. See
    [https://developers.google.com/search/docs/ajax-crawling/docs/getting-started](https://developers.google.com/search/docs/ajax-crawling/docs/getting-started){.reference
    .external} for more info.

    ::: {.admonition .note}
    Note

    Scrapy finds 'AJAX crawlable' pages for URLs like
    [`'http://example.com/!#foo=bar'`{.docutils .literal
    .notranslate}]{.pre} even without this middleware.
    AjaxCrawlMiddleware is necessary when URL doesn't contain
    [`'!#'`{.docutils .literal .notranslate}]{.pre}. This is often a
    case for 'index' or 'main' website pages.
    :::

::: {#ajaxcrawlmiddleware-settings .section}
###### AjaxCrawlMiddleware Settings[¶](#ajaxcrawlmiddleware-settings "Permalink to this heading"){.headerlink}

::: {#ajaxcrawl-enabled .section}
[]{#std-setting-AJAXCRAWL_ENABLED}AJAXCRAWL_ENABLED[¶](#ajaxcrawl-enabled "Permalink to this heading"){.headerlink}

Default: [`False`{.docutils .literal .notranslate}]{.pre}

Whether the AjaxCrawlMiddleware will be enabled. You may want to enable
it for [[broad crawls]{.std
.std-ref}](index.html#topics-broad-crawls){.hoverxref .tooltip
.reference .internal}.
:::
:::

::: {#httpproxymiddleware-settings .section}
###### HttpProxyMiddleware settings[¶](#httpproxymiddleware-settings "Permalink to this heading"){.headerlink}

[]{#std-setting-HTTPPROXY_ENABLED .target}

::: {#httpproxy-enabled .section}
[]{#std-setting-HTTPPROXY_AUTH_ENCODING}HTTPPROXY_ENABLED[¶](#httpproxy-enabled "Permalink to this heading"){.headerlink}

Default: [`True`{.docutils .literal .notranslate}]{.pre}

Whether or not to enable the [`HttpProxyMiddleware`{.xref .py .py-class
.docutils .literal .notranslate}]{.pre}.
:::

::: {#httpproxy-auth-encoding .section}
HTTPPROXY_AUTH_ENCODING[¶](#httpproxy-auth-encoding "Permalink to this heading"){.headerlink}

Default: [`"latin-1"`{.docutils .literal .notranslate}]{.pre}

The default encoding for proxy authentication on
[`HttpProxyMiddleware`{.xref .py .py-class .docutils .literal
.notranslate}]{.pre}.
:::
:::
:::
:::
:::

[]{#document-topics/spider-middleware}

::: {#spider-middleware .section}
[]{#topics-spider-middleware}

### Spider Middleware[¶](#spider-middleware "Permalink to this heading"){.headerlink}

The spider middleware is a framework of hooks into Scrapy's spider
processing mechanism where you can plug custom functionality to process
the responses that are sent to [[Spiders]{.std
.std-ref}](index.html#topics-spiders){.hoverxref .tooltip .reference
.internal} for processing and to process the requests and items that are
generated from spiders.

::: {#activating-a-spider-middleware .section}
[]{#topics-spider-middleware-setting}

#### Activating a spider middleware[¶](#activating-a-spider-middleware "Permalink to this heading"){.headerlink}

To activate a spider middleware component, add it to the
[[`SPIDER_MIDDLEWARES`{.xref .std .std-setting .docutils .literal
.notranslate}]{.pre}](index.html#std-setting-SPIDER_MIDDLEWARES){.hoverxref
.tooltip .reference .internal} setting, which is a dict whose keys are
the middleware class path and their values are the middleware orders.

Here's an example:

::: {.highlight-python .notranslate}
::: highlight
    SPIDER_MIDDLEWARES = {
        "myproject.middlewares.CustomSpiderMiddleware": 543,
    }
:::
:::

The [[`SPIDER_MIDDLEWARES`{.xref .std .std-setting .docutils .literal
.notranslate}]{.pre}](index.html#std-setting-SPIDER_MIDDLEWARES){.hoverxref
.tooltip .reference .internal} setting is merged with the
[[`SPIDER_MIDDLEWARES_BASE`{.xref .std .std-setting .docutils .literal
.notranslate}]{.pre}](index.html#std-setting-SPIDER_MIDDLEWARES_BASE){.hoverxref
.tooltip .reference .internal} setting defined in Scrapy (and not meant
to be overridden) and then sorted by order to get the final sorted list
of enabled middlewares: the first middleware is the one closer to the
engine and the last is the one closer to the spider. In other words, the
[[`process_spider_input()`{.xref .py .py-meth .docutils .literal
.notranslate}]{.pre}](#scrapy.spidermiddlewares.SpiderMiddleware.process_spider_input "scrapy.spidermiddlewares.SpiderMiddleware.process_spider_input"){.reference
.internal} method of each middleware will be invoked in increasing
middleware order (100, 200, 300, ...), and the
[[`process_spider_output()`{.xref .py .py-meth .docutils .literal
.notranslate}]{.pre}](#scrapy.spidermiddlewares.SpiderMiddleware.process_spider_output "scrapy.spidermiddlewares.SpiderMiddleware.process_spider_output"){.reference
.internal} method of each middleware will be invoked in decreasing
order.

To decide which order to assign to your middleware see the
[[`SPIDER_MIDDLEWARES_BASE`{.xref .std .std-setting .docutils .literal
.notranslate}]{.pre}](index.html#std-setting-SPIDER_MIDDLEWARES_BASE){.hoverxref
.tooltip .reference .internal} setting and pick a value according to
where you want to insert the middleware. The order does matter because
each middleware performs a different action and your middleware could
depend on some previous (or subsequent) middleware being applied.

If you want to disable a builtin middleware (the ones defined in
[[`SPIDER_MIDDLEWARES_BASE`{.xref .std .std-setting .docutils .literal
.notranslate}]{.pre}](index.html#std-setting-SPIDER_MIDDLEWARES_BASE){.hoverxref
.tooltip .reference .internal}, and enabled by default) you must define
it in your project [[`SPIDER_MIDDLEWARES`{.xref .std .std-setting
.docutils .literal
.notranslate}]{.pre}](index.html#std-setting-SPIDER_MIDDLEWARES){.hoverxref
.tooltip .reference .internal} setting and assign [`None`{.docutils
.literal .notranslate}]{.pre} as its value. For example, if you want to
disable the off-site middleware:

::: {.highlight-python .notranslate}
::: highlight
    SPIDER_MIDDLEWARES = {
        "myproject.middlewares.CustomSpiderMiddleware": 543,
        "scrapy.spidermiddlewares.offsite.OffsiteMiddleware": None,
    }
:::
:::

Finally, keep in mind that some middlewares may need to be enabled
through a particular setting. See each middleware documentation for more
info.
:::

::: {#writing-your-own-spider-middleware .section}
[]{#custom-spider-middleware}

#### Writing your own spider middleware[¶](#writing-your-own-spider-middleware "Permalink to this heading"){.headerlink}

Each spider middleware is a Python class that defines one or more of the
methods defined below.

The main entry point is the [`from_crawler`{.docutils .literal
.notranslate}]{.pre} class method, which receives a [[`Crawler`{.xref
.py .py-class .docutils .literal
.notranslate}]{.pre}](index.html#scrapy.crawler.Crawler "scrapy.crawler.Crawler"){.reference
.internal} instance. The [[`Crawler`{.xref .py .py-class .docutils
.literal
.notranslate}]{.pre}](index.html#scrapy.crawler.Crawler "scrapy.crawler.Crawler"){.reference
.internal} object gives you access, for example, to the [[settings]{.std
.std-ref}](index.html#topics-settings){.hoverxref .tooltip .reference
.internal}.

[]{#module-scrapy.spidermiddlewares .target}

*[class]{.pre}[ ]{.w}*[[scrapy.spidermiddlewares.]{.pre}]{.sig-prename .descclassname}[[SpiderMiddleware]{.pre}]{.sig-name .descname}[¶](#scrapy.spidermiddlewares.SpiderMiddleware "Permalink to this definition"){.headerlink}

:   

    [[process_spider_input]{.pre}]{.sig-name .descname}[(]{.sig-paren}*[[response]{.pre}]{.n}*, *[[spider]{.pre}]{.n}*[)]{.sig-paren}[¶](#scrapy.spidermiddlewares.SpiderMiddleware.process_spider_input "Permalink to this definition"){.headerlink}

    :   This method is called for each response that goes through the
        spider middleware and into the spider, for processing.

        [[`process_spider_input()`{.xref .py .py-meth .docutils .literal
        .notranslate}]{.pre}](#scrapy.spidermiddlewares.SpiderMiddleware.process_spider_input "scrapy.spidermiddlewares.SpiderMiddleware.process_spider_input"){.reference
        .internal} should return [`None`{.docutils .literal
        .notranslate}]{.pre} or raise an exception.

        If it returns [`None`{.docutils .literal .notranslate}]{.pre},
        Scrapy will continue processing this response, executing all
        other middlewares until, finally, the response is handed to the
        spider for processing.

        If it raises an exception, Scrapy won't bother calling any other
        spider middleware [[`process_spider_input()`{.xref .py .py-meth
        .docutils .literal
        .notranslate}]{.pre}](#scrapy.spidermiddlewares.SpiderMiddleware.process_spider_input "scrapy.spidermiddlewares.SpiderMiddleware.process_spider_input"){.reference
        .internal} and will call the request errback if there is one,
        otherwise it will start the [[`process_spider_exception()`{.xref
        .py .py-meth .docutils .literal
        .notranslate}]{.pre}](#scrapy.spidermiddlewares.SpiderMiddleware.process_spider_exception "scrapy.spidermiddlewares.SpiderMiddleware.process_spider_exception"){.reference
        .internal} chain. The output of the errback is chained back in
        the other direction for [[`process_spider_output()`{.xref .py
        .py-meth .docutils .literal
        .notranslate}]{.pre}](#scrapy.spidermiddlewares.SpiderMiddleware.process_spider_output "scrapy.spidermiddlewares.SpiderMiddleware.process_spider_output"){.reference
        .internal} to process it, or
        [[`process_spider_exception()`{.xref .py .py-meth .docutils
        .literal
        .notranslate}]{.pre}](#scrapy.spidermiddlewares.SpiderMiddleware.process_spider_exception "scrapy.spidermiddlewares.SpiderMiddleware.process_spider_exception"){.reference
        .internal} if it raised an exception.

        Parameters

        :   -   **response** ([[`Response`{.xref .py .py-class .docutils
                .literal
                .notranslate}]{.pre}](index.html#scrapy.http.Response "scrapy.http.Response"){.reference
                .internal} object) -- the response being processed

            -   **spider** ([[`Spider`{.xref .py .py-class .docutils
                .literal
                .notranslate}]{.pre}](index.html#scrapy.Spider "scrapy.Spider"){.reference
                .internal} object) -- the spider for which this response
                is intended

    [[process_spider_output]{.pre}]{.sig-name .descname}[(]{.sig-paren}*[[response]{.pre}]{.n}*, *[[result]{.pre}]{.n}*, *[[spider]{.pre}]{.n}*[)]{.sig-paren}[¶](#scrapy.spidermiddlewares.SpiderMiddleware.process_spider_output "Permalink to this definition"){.headerlink}

    :   This method is called with the results returned from the Spider,
        after it has processed the response.

        [[`process_spider_output()`{.xref .py .py-meth .docutils
        .literal
        .notranslate}]{.pre}](#scrapy.spidermiddlewares.SpiderMiddleware.process_spider_output "scrapy.spidermiddlewares.SpiderMiddleware.process_spider_output"){.reference
        .internal} must return an iterable of [`Request`{.xref .py
        .py-class .docutils .literal .notranslate}]{.pre} objects and
        [[item objects]{.std
        .std-ref}](index.html#topics-items){.hoverxref .tooltip
        .reference .internal}.

        ::: versionchanged
        [Changed in version 2.7: ]{.versionmodified .changed}This method
        may be defined as an [[asynchronous generator]{.xref .std
        .std-term}](https://docs.python.org/3/glossary.html#term-asynchronous-generator "(in Python v3.12)"){.reference
        .external}, in which case [`result`{.docutils .literal
        .notranslate}]{.pre} is an [[asynchronous iterable]{.xref .std
        .std-term}](https://docs.python.org/3/glossary.html#term-asynchronous-iterable "(in Python v3.12)"){.reference
        .external}.
        :::

        Consider defining this method as an [[asynchronous
        generator]{.xref .std
        .std-term}](https://docs.python.org/3/glossary.html#term-asynchronous-generator "(in Python v3.12)"){.reference
        .external}, which will be a requirement in a future version of
        Scrapy. However, if you plan on sharing your spider middleware
        with other people, consider either [[enforcing Scrapy 2.7]{.std
        .std-ref}](index.html#enforce-component-requirements){.hoverxref
        .tooltip .reference .internal} as a minimum requirement of your
        spider middleware, or [[making your spider middleware
        universal]{.std
        .std-ref}](index.html#universal-spider-middleware){.hoverxref
        .tooltip .reference .internal} so that it works with Scrapy
        versions earlier than Scrapy 2.7.

        Parameters

        :   -   **response** ([[`Response`{.xref .py .py-class .docutils
                .literal
                .notranslate}]{.pre}](index.html#scrapy.http.Response "scrapy.http.Response"){.reference
                .internal} object) -- the response which generated this
                output from the spider

            -   **result** (an iterable of [`Request`{.xref .py
                .py-class .docutils .literal .notranslate}]{.pre}
                objects and [[item objects]{.std
                .std-ref}](index.html#topics-items){.hoverxref .tooltip
                .reference .internal}) -- the result returned by the
                spider

            -   **spider** ([[`Spider`{.xref .py .py-class .docutils
                .literal
                .notranslate}]{.pre}](index.html#scrapy.Spider "scrapy.Spider"){.reference
                .internal} object) -- the spider whose result is being
                processed

    [[process_spider_output_async]{.pre}]{.sig-name .descname}[(]{.sig-paren}*[[response]{.pre}]{.n}*, *[[result]{.pre}]{.n}*, *[[spider]{.pre}]{.n}*[)]{.sig-paren}[¶](#scrapy.spidermiddlewares.SpiderMiddleware.process_spider_output_async "Permalink to this definition"){.headerlink}

    :   ::: versionadded
        [New in version 2.7.]{.versionmodified .added}
        :::

        If defined, this method must be an [[asynchronous
        generator]{.xref .std
        .std-term}](https://docs.python.org/3/glossary.html#term-asynchronous-generator "(in Python v3.12)"){.reference
        .external}, which will be called instead of
        [[`process_spider_output()`{.xref .py .py-meth .docutils
        .literal
        .notranslate}]{.pre}](#scrapy.spidermiddlewares.SpiderMiddleware.process_spider_output "scrapy.spidermiddlewares.SpiderMiddleware.process_spider_output"){.reference
        .internal} if [`result`{.docutils .literal .notranslate}]{.pre}
        is an [[asynchronous iterable]{.xref .std
        .std-term}](https://docs.python.org/3/glossary.html#term-asynchronous-iterable "(in Python v3.12)"){.reference
        .external}.

    [[process_spider_exception]{.pre}]{.sig-name .descname}[(]{.sig-paren}*[[response]{.pre}]{.n}*, *[[exception]{.pre}]{.n}*, *[[spider]{.pre}]{.n}*[)]{.sig-paren}[¶](#scrapy.spidermiddlewares.SpiderMiddleware.process_spider_exception "Permalink to this definition"){.headerlink}

    :   This method is called when a spider or
        [[`process_spider_output()`{.xref .py .py-meth .docutils
        .literal
        .notranslate}]{.pre}](#scrapy.spidermiddlewares.SpiderMiddleware.process_spider_output "scrapy.spidermiddlewares.SpiderMiddleware.process_spider_output"){.reference
        .internal} method (from a previous spider middleware) raises an
        exception.

        [[`process_spider_exception()`{.xref .py .py-meth .docutils
        .literal
        .notranslate}]{.pre}](#scrapy.spidermiddlewares.SpiderMiddleware.process_spider_exception "scrapy.spidermiddlewares.SpiderMiddleware.process_spider_exception"){.reference
        .internal} should return either [`None`{.docutils .literal
        .notranslate}]{.pre} or an iterable of [`Request`{.xref .py
        .py-class .docutils .literal .notranslate}]{.pre} or
        [[item]{.std .std-ref}](index.html#topics-items){.hoverxref
        .tooltip .reference .internal} objects.

        If it returns [`None`{.docutils .literal .notranslate}]{.pre},
        Scrapy will continue processing this exception, executing any
        other [[`process_spider_exception()`{.xref .py .py-meth
        .docutils .literal
        .notranslate}]{.pre}](#scrapy.spidermiddlewares.SpiderMiddleware.process_spider_exception "scrapy.spidermiddlewares.SpiderMiddleware.process_spider_exception"){.reference
        .internal} in the following middleware components, until no
        middleware components are left and the exception reaches the
        engine (where it's logged and discarded).

        If it returns an iterable the [[`process_spider_output()`{.xref
        .py .py-meth .docutils .literal
        .notranslate}]{.pre}](#scrapy.spidermiddlewares.SpiderMiddleware.process_spider_output "scrapy.spidermiddlewares.SpiderMiddleware.process_spider_output"){.reference
        .internal} pipeline kicks in, starting from the next spider
        middleware, and no other [[`process_spider_exception()`{.xref
        .py .py-meth .docutils .literal
        .notranslate}]{.pre}](#scrapy.spidermiddlewares.SpiderMiddleware.process_spider_exception "scrapy.spidermiddlewares.SpiderMiddleware.process_spider_exception"){.reference
        .internal} will be called.

        Parameters

        :   -   **response** ([[`Response`{.xref .py .py-class .docutils
                .literal
                .notranslate}]{.pre}](index.html#scrapy.http.Response "scrapy.http.Response"){.reference
                .internal} object) -- the response being processed when
                the exception was raised

            -   **exception** ([[`Exception`{.xref .py .py-exc .docutils
                .literal
                .notranslate}]{.pre}](https://docs.python.org/3/library/exceptions.html#Exception "(in Python v3.12)"){.reference
                .external} object) -- the exception raised

            -   **spider** ([[`Spider`{.xref .py .py-class .docutils
                .literal
                .notranslate}]{.pre}](index.html#scrapy.Spider "scrapy.Spider"){.reference
                .internal} object) -- the spider which raised the
                exception

    [[process_start_requests]{.pre}]{.sig-name .descname}[(]{.sig-paren}*[[start_requests]{.pre}]{.n}*, *[[spider]{.pre}]{.n}*[)]{.sig-paren}[¶](#scrapy.spidermiddlewares.SpiderMiddleware.process_start_requests "Permalink to this definition"){.headerlink}

    :   This method is called with the start requests of the spider, and
        works similarly to the [[`process_spider_output()`{.xref .py
        .py-meth .docutils .literal
        .notranslate}]{.pre}](#scrapy.spidermiddlewares.SpiderMiddleware.process_spider_output "scrapy.spidermiddlewares.SpiderMiddleware.process_spider_output"){.reference
        .internal} method, except that it doesn't have a response
        associated and must return only requests (not items).

        It receives an iterable (in the [`start_requests`{.docutils
        .literal .notranslate}]{.pre} parameter) and must return another
        iterable of [`Request`{.xref .py .py-class .docutils .literal
        .notranslate}]{.pre} objects.

        ::: {.admonition .note}
        Note

        When implementing this method in your spider middleware, you
        should always return an iterable (that follows the input one)
        and not consume all [`start_requests`{.docutils .literal
        .notranslate}]{.pre} iterator because it can be very large (or
        even unbounded) and cause a memory overflow. The Scrapy engine
        is designed to pull start requests while it has capacity to
        process them, so the start requests iterator can be effectively
        endless where there is some other condition for stopping the
        spider (like a time limit or item/page count).
        :::

        Parameters

        :   -   **start_requests** (an iterable of [`Request`{.xref .py
                .py-class .docutils .literal .notranslate}]{.pre}) --
                the start requests

            -   **spider** ([[`Spider`{.xref .py .py-class .docutils
                .literal
                .notranslate}]{.pre}](index.html#scrapy.Spider "scrapy.Spider"){.reference
                .internal} object) -- the spider to whom the start
                requests belong

    [[from_crawler]{.pre}]{.sig-name .descname}[(]{.sig-paren}*[[cls]{.pre}]{.n}*, *[[crawler]{.pre}]{.n}*[)]{.sig-paren}[¶](#scrapy.spidermiddlewares.SpiderMiddleware.from_crawler "Permalink to this definition"){.headerlink}

    :   If present, this classmethod is called to create a middleware
        instance from a [[`Crawler`{.xref .py .py-class .docutils
        .literal
        .notranslate}]{.pre}](index.html#scrapy.crawler.Crawler "scrapy.crawler.Crawler"){.reference
        .internal}. It must return a new instance of the middleware.
        Crawler object provides access to all Scrapy core components
        like settings and signals; it is a way for middleware to access
        them and hook its functionality into Scrapy.

        Parameters

        :   **crawler** ([[`Crawler`{.xref .py .py-class .docutils
            .literal
            .notranslate}]{.pre}](index.html#scrapy.crawler.Crawler "scrapy.crawler.Crawler"){.reference
            .internal} object) -- crawler that uses this middleware
:::

::: {#built-in-spider-middleware-reference .section}
[]{#topics-spider-middleware-ref}

#### Built-in spider middleware reference[¶](#built-in-spider-middleware-reference "Permalink to this heading"){.headerlink}

This page describes all spider middleware components that come with
Scrapy. For information on how to use them and how to write your own
spider middleware, see the [[spider middleware usage guide]{.std
.std-ref}](#topics-spider-middleware){.hoverxref .tooltip .reference
.internal}.

For a list of the components enabled by default (and their orders) see
the [[`SPIDER_MIDDLEWARES_BASE`{.xref .std .std-setting .docutils
.literal
.notranslate}]{.pre}](index.html#std-setting-SPIDER_MIDDLEWARES_BASE){.hoverxref
.tooltip .reference .internal} setting.

::: {#module-scrapy.spidermiddlewares.depth .section}
[]{#depthmiddleware}

##### DepthMiddleware[¶](#module-scrapy.spidermiddlewares.depth "Permalink to this heading"){.headerlink}

*[class]{.pre}[ ]{.w}*[[scrapy.spidermiddlewares.depth.]{.pre}]{.sig-prename .descclassname}[[DepthMiddleware]{.pre}]{.sig-name .descname}[[[\[source\]]{.pre}]{.viewcode-link}](_modules/scrapy/spidermiddlewares/depth.html#DepthMiddleware){.reference .internal}[¶](#scrapy.spidermiddlewares.depth.DepthMiddleware "Permalink to this definition"){.headerlink}

:   DepthMiddleware is used for tracking the depth of each Request
    inside the site being scraped. It works by setting
    [`request.meta['depth']`{.docutils .literal
    .notranslate}]{.pre}` `{.docutils .literal
    .notranslate}[`=`{.docutils .literal
    .notranslate}]{.pre}` `{.docutils .literal
    .notranslate}[`0`{.docutils .literal .notranslate}]{.pre} whenever
    there is no value previously set (usually just the first Request)
    and incrementing it by 1 otherwise.

    It can be used to limit the maximum depth to scrape, control Request
    priority based on their depth, and things like that.

    The [[`DepthMiddleware`{.xref .py .py-class .docutils .literal
    .notranslate}]{.pre}](#scrapy.spidermiddlewares.depth.DepthMiddleware "scrapy.spidermiddlewares.depth.DepthMiddleware"){.reference
    .internal} can be configured through the following settings (see the
    settings documentation for more info):

    > <div>
    >
    > -   [[`DEPTH_LIMIT`{.xref .std .std-setting .docutils .literal
    >     .notranslate}]{.pre}](index.html#std-setting-DEPTH_LIMIT){.hoverxref
    >     .tooltip .reference .internal} - The maximum depth that will
    >     be allowed to crawl for any site. If zero, no limit will be
    >     imposed.
    >
    > -   [[`DEPTH_STATS_VERBOSE`{.xref .std .std-setting .docutils
    >     .literal
    >     .notranslate}]{.pre}](index.html#std-setting-DEPTH_STATS_VERBOSE){.hoverxref
    >     .tooltip .reference .internal} - Whether to collect the number
    >     of requests for each depth.
    >
    > -   [[`DEPTH_PRIORITY`{.xref .std .std-setting .docutils .literal
    >     .notranslate}]{.pre}](index.html#std-setting-DEPTH_PRIORITY){.hoverxref
    >     .tooltip .reference .internal} - Whether to prioritize the
    >     requests based on their depth.
    >
    > </div>
:::

::: {#module-scrapy.spidermiddlewares.httperror .section}
[]{#httperrormiddleware}

##### HttpErrorMiddleware[¶](#module-scrapy.spidermiddlewares.httperror "Permalink to this heading"){.headerlink}

*[class]{.pre}[ ]{.w}*[[scrapy.spidermiddlewares.httperror.]{.pre}]{.sig-prename .descclassname}[[HttpErrorMiddleware]{.pre}]{.sig-name .descname}[[[\[source\]]{.pre}]{.viewcode-link}](_modules/scrapy/spidermiddlewares/httperror.html#HttpErrorMiddleware){.reference .internal}[¶](#scrapy.spidermiddlewares.httperror.HttpErrorMiddleware "Permalink to this definition"){.headerlink}

:   Filter out unsuccessful (erroneous) HTTP responses so that spiders
    don't have to deal with them, which (most of the time) imposes an
    overhead, consumes more resources, and makes the spider logic more
    complex.

According to the [HTTP
standard](https://www.w3.org/Protocols/rfc2616/rfc2616-sec10.html){.reference
.external}, successful responses are those whose status codes are in the
200-300 range.

If you still want to process response codes outside that range, you can
specify which response codes the spider is able to handle using the
[`handle_httpstatus_list`{.docutils .literal .notranslate}]{.pre} spider
attribute or [[`HTTPERROR_ALLOWED_CODES`{.xref .std .std-setting
.docutils .literal
.notranslate}]{.pre}](#std-setting-HTTPERROR_ALLOWED_CODES){.hoverxref
.tooltip .reference .internal} setting.

For example, if you want your spider to handle 404 responses you can do
this:

::: {.highlight-python .notranslate}
::: highlight
    from scrapy.spiders import CrawlSpider


    class MySpider(CrawlSpider):
        handle_httpstatus_list = [404]
:::
:::

[]{#std-reqmeta-handle_httpstatus_list .target}

The [`handle_httpstatus_list`{.docutils .literal .notranslate}]{.pre}
key of [`Request.meta`{.xref .py .py-attr .docutils .literal
.notranslate}]{.pre} can also be used to specify which response codes to
allow on a per-request basis. You can also set the meta key
[`handle_httpstatus_all`{.docutils .literal .notranslate}]{.pre} to
[`True`{.docutils .literal .notranslate}]{.pre} if you want to allow any
response code for a request, and [`False`{.docutils .literal
.notranslate}]{.pre} to disable the effects of the
[`handle_httpstatus_all`{.docutils .literal .notranslate}]{.pre} key.

Keep in mind, however, that it's usually a bad idea to handle non-200
responses, unless you really know what you're doing.

For more information see: [HTTP Status Code
Definitions](https://www.w3.org/Protocols/rfc2616/rfc2616-sec10.html){.reference
.external}.

::: {#httperrormiddleware-settings .section}
###### HttpErrorMiddleware settings[¶](#httperrormiddleware-settings "Permalink to this heading"){.headerlink}

::: {#httperror-allowed-codes .section}
[]{#std-setting-HTTPERROR_ALLOWED_CODES}HTTPERROR_ALLOWED_CODES[¶](#httperror-allowed-codes "Permalink to this heading"){.headerlink}

Default: [`[]`{.docutils .literal .notranslate}]{.pre}

Pass all responses with non-200 status codes contained in this list.
:::

::: {#httperror-allow-all .section}
[]{#std-setting-HTTPERROR_ALLOW_ALL}HTTPERROR_ALLOW_ALL[¶](#httperror-allow-all "Permalink to this heading"){.headerlink}

Default: [`False`{.docutils .literal .notranslate}]{.pre}

Pass all responses, regardless of its status code.
:::
:::
:::

::: {#module-scrapy.spidermiddlewares.offsite .section}
[]{#offsitemiddleware}

##### OffsiteMiddleware[¶](#module-scrapy.spidermiddlewares.offsite "Permalink to this heading"){.headerlink}

*[class]{.pre}[ ]{.w}*[[scrapy.spidermiddlewares.offsite.]{.pre}]{.sig-prename .descclassname}[[OffsiteMiddleware]{.pre}]{.sig-name .descname}[[[\[source\]]{.pre}]{.viewcode-link}](_modules/scrapy/spidermiddlewares/offsite.html#OffsiteMiddleware){.reference .internal}[¶](#scrapy.spidermiddlewares.offsite.OffsiteMiddleware "Permalink to this definition"){.headerlink}

:   Filters out Requests for URLs outside the domains covered by the
    spider.

    This middleware filters out every request whose host names aren't in
    the spider's [[`allowed_domains`{.xref .py .py-attr .docutils
    .literal
    .notranslate}]{.pre}](index.html#scrapy.Spider.allowed_domains "scrapy.Spider.allowed_domains"){.reference
    .internal} attribute. All subdomains of any domain in the list are
    also allowed. E.g. the rule [`www.example.org`{.docutils .literal
    .notranslate}]{.pre} will also allow
    [`bob.www.example.org`{.docutils .literal .notranslate}]{.pre} but
    not [`www2.example.com`{.docutils .literal .notranslate}]{.pre} nor
    [`example.com`{.docutils .literal .notranslate}]{.pre}.

    When your spider returns a request for a domain not belonging to
    those covered by the spider, this middleware will log a debug
    message similar to this one:

    ::: {.highlight-default .notranslate}
    ::: highlight
        DEBUG: Filtered offsite request to 'www.othersite.com': <GET http://www.othersite.com/some/page.html>
    :::
    :::

    To avoid filling the log with too much noise, it will only print one
    of these messages for each new domain filtered. So, for example, if
    another request for [`www.othersite.com`{.docutils .literal
    .notranslate}]{.pre} is filtered, no log message will be printed.
    But if a request for [`someothersite.com`{.docutils .literal
    .notranslate}]{.pre} is filtered, a message will be printed (but
    only for the first request filtered).

    If the spider doesn't define an [[`allowed_domains`{.xref .py
    .py-attr .docutils .literal
    .notranslate}]{.pre}](index.html#scrapy.Spider.allowed_domains "scrapy.Spider.allowed_domains"){.reference
    .internal} attribute, or the attribute is empty, the offsite
    middleware will allow all requests.

    If the request has the [`dont_filter`{.xref .py .py-attr .docutils
    .literal .notranslate}]{.pre} attribute set, the offsite middleware
    will allow the request even if its domain is not listed in allowed
    domains.
:::

::: {#module-scrapy.spidermiddlewares.referer .section}
[]{#referermiddleware}

##### RefererMiddleware[¶](#module-scrapy.spidermiddlewares.referer "Permalink to this heading"){.headerlink}

*[class]{.pre}[ ]{.w}*[[scrapy.spidermiddlewares.referer.]{.pre}]{.sig-prename .descclassname}[[RefererMiddleware]{.pre}]{.sig-name .descname}[[[\[source\]]{.pre}]{.viewcode-link}](_modules/scrapy/spidermiddlewares/referer.html#RefererMiddleware){.reference .internal}[¶](#scrapy.spidermiddlewares.referer.RefererMiddleware "Permalink to this definition"){.headerlink}

:   Populates Request [`Referer`{.docutils .literal .notranslate}]{.pre}
    header, based on the URL of the Response which generated it.

::: {#referermiddleware-settings .section}
###### RefererMiddleware settings[¶](#referermiddleware-settings "Permalink to this heading"){.headerlink}

::: {#referer-enabled .section}
[]{#std-setting-REFERER_ENABLED}REFERER_ENABLED[¶](#referer-enabled "Permalink to this heading"){.headerlink}

Default: [`True`{.docutils .literal .notranslate}]{.pre}

Whether to enable referer middleware.
:::

::: {#referrer-policy .section}
[]{#std-setting-REFERRER_POLICY}REFERRER_POLICY[¶](#referrer-policy "Permalink to this heading"){.headerlink}

Default:
[`'scrapy.spidermiddlewares.referer.DefaultReferrerPolicy'`{.docutils
.literal .notranslate}]{.pre}

[Referrer Policy](https://www.w3.org/TR/referrer-policy){.reference
.external} to apply when populating Request "Referer" header.

::: {.admonition .note}
Note

You can also set the Referrer Policy per request, using the special
[`"referrer_policy"`{.docutils .literal .notranslate}]{.pre}
[[Request.meta]{.std
.std-ref}](index.html#topics-request-meta){.hoverxref .tooltip
.reference .internal} key, with the same acceptable values as for the
[`REFERRER_POLICY`{.docutils .literal .notranslate}]{.pre} setting.
:::

::: {#acceptable-values-for-referrer-policy .section}
Acceptable values for
REFERRER_POLICY[¶](#acceptable-values-for-referrer-policy "Permalink to this heading"){.headerlink}

-   either a path to a
    [`scrapy.spidermiddlewares.referer.ReferrerPolicy`{.docutils
    .literal .notranslate}]{.pre} subclass --- a custom policy or one of
    the built-in ones (see classes below),

-   or one of the standard W3C-defined string values,

-   or the special [`"scrapy-default"`{.docutils .literal
    .notranslate}]{.pre}.

+-----------------------+----------------------------------------------+
| String value          | Class name (as a string)                     |
+=======================+==============================================+
| [`"scrap              | [[`scrapy.spidermidd                         |
| y-default"`{.docutils | lewares.referer.DefaultReferrerPolicy`{.xref |
| .literal              | .py .py-class .docutils .literal             |
| .notranslate}]{.pre}  | .notranslate}]                               |
| (default)             | {.pre}](#scrapy.spidermiddlewares.referer.De |
|                       | faultReferrerPolicy "scrapy.spidermiddleware |
|                       | s.referer.DefaultReferrerPolicy"){.reference |
|                       | .internal}                                   |
+-----------------------+----------------------------------------------+
| ["no-refer            | [[`scrapy.spide                              |
| rer"](https://www.w3. | rmiddlewares.referer.NoReferrerPolicy`{.xref |
| org/TR/referrer-polic | .py .py-class .docutils .literal             |
| y/#referrer-policy-no | .not                                         |
| -referrer){.reference | ranslate}]{.pre}](#scrapy.spidermiddlewares. |
| .external}            | referer.NoReferrerPolicy "scrapy.spidermiddl |
|                       | ewares.referer.NoReferrerPolicy"){.reference |
|                       | .internal}                                   |
+-----------------------+----------------------------------------------+
| ["no-referrer-when-   | [[`scrapy.spidermiddlewares.                 |
| downgrade"](https://w | referer.NoReferrerWhenDowngradePolicy`{.xref |
| ww.w3.org/TR/referrer | .py .py-class .docutils .literal             |
| -policy/#referrer-pol | .notranslate}]{.pre}](#scrapy.               |
| icy-no-referrer-when- | spidermiddlewares.referer.NoReferrerWhenDown |
| downgrade){.reference | gradePolicy "scrapy.spidermiddlewares.refere |
| .external}            | r.NoReferrerWhenDowngradePolicy"){.reference |
|                       | .internal}                                   |
+-----------------------+----------------------------------------------+
| ["same-ori            | [[`scrapy.spide                              |
| gin"](https://www.w3. | rmiddlewares.referer.SameOriginPolicy`{.xref |
| org/TR/referrer-polic | .py .py-class .docutils .literal             |
| y/#referrer-policy-sa | .not                                         |
| me-origin){.reference | ranslate}]{.pre}](#scrapy.spidermiddlewares. |
| .external}            | referer.SameOriginPolicy "scrapy.spidermiddl |
|                       | ewares.referer.SameOriginPolicy"){.reference |
|                       | .internal}                                   |
+-----------------------+----------------------------------------------+
| ["origin"](https://ww | [[`scrapy.s                                  |
| w.w3.org/TR/referrer- | pidermiddlewares.referer.OriginPolicy`{.xref |
| policy/#referrer-poli | .py .py-class .docutils .literal             |
| cy-origin){.reference | .notranslate}]{.pre}](#scrapy.spidermidd     |
| .external}            | lewares.referer.OriginPolicy "scrapy.spiderm |
|                       | iddlewares.referer.OriginPolicy"){.reference |
|                       | .internal}                                   |
+-----------------------+----------------------------------------------+
| ["strict-origi        | [[`scrapy.spiderm                            |
| n"](https://www.w3.or | iddlewares.referer.StrictOriginPolicy`{.xref |
| g/TR/referrer-policy/ | .py .py-class .docutils .literal             |
| #referrer-policy-stri | .notrans                                     |
| ct-origin){.reference | late}]{.pre}](#scrapy.spidermiddlewares.refe |
| .external}            | rer.StrictOriginPolicy "scrapy.spidermiddlew |
|                       | ares.referer.StrictOriginPolicy"){.reference |
|                       | .internal}                                   |
+-----------------------+----------------------------------------------+
| ["origin-when-c       | [[`scrapy.spidermiddleware                   |
| ross-origin"](https:/ | s.referer.OriginWhenCrossOriginPolicy`{.xref |
| /www.w3.org/TR/referr | .py .py-class .docutils .literal             |
| er-policy/#referrer-p | .notranslate}]{.pre}](#scr                   |
| olicy-origin-when-cro | apy.spidermiddlewares.referer.OriginWhenCros |
| ss-origin){.reference | sOriginPolicy "scrapy.spidermiddlewares.refe |
| .external}            | rer.OriginWhenCrossOriginPolicy"){.reference |
|                       | .internal}                                   |
+-----------------------+----------------------------------------------+
| ["strict              | [[`scrapy.spidermiddlewares.refe             |
| -origin-when-cross-or | rer.StrictOriginWhenCrossOriginPolicy`{.xref |
| igin"](https://www.w3 | .py .py-class .docutils .literal             |
| .org/TR/referrer-poli | .notranslate}]{.pre}](#scrapy.spidermi       |
| cy/#referrer-policy-s | ddlewares.referer.StrictOriginWhenCrossOrigi |
| trict-origin-when-cro | nPolicy "scrapy.spidermiddlewares.referer.St |
| ss-origin){.reference | rictOriginWhenCrossOriginPolicy"){.reference |
| .external}            | .internal}                                   |
+-----------------------+----------------------------------------------+
| ["unsafe              | [[`scrapy.spid                               |
| -url"](https://www.w3 | ermiddlewares.referer.UnsafeUrlPolicy`{.xref |
| .org/TR/referrer-poli | .py .py-class .docutils .literal             |
| cy/#referrer-policy-u | .n                                           |
| nsafe-url){.reference | otranslate}]{.pre}](#scrapy.spidermiddleware |
| .external}            | s.referer.UnsafeUrlPolicy "scrapy.spidermidd |
|                       | lewares.referer.UnsafeUrlPolicy"){.reference |
|                       | .internal}                                   |
+-----------------------+----------------------------------------------+

*[class]{.pre}[ ]{.w}*[[scrapy.spidermiddlewares.referer.]{.pre}]{.sig-prename .descclassname}[[DefaultReferrerPolicy]{.pre}]{.sig-name .descname}[[[\[source\]]{.pre}]{.viewcode-link}](_modules/scrapy/spidermiddlewares/referer.html#DefaultReferrerPolicy){.reference .internal}[¶](#scrapy.spidermiddlewares.referer.DefaultReferrerPolicy "Permalink to this definition"){.headerlink}

:   A variant of "no-referrer-when-downgrade", with the addition that
    "Referer" is not sent if the parent request was using
    [`file://`{.docutils .literal .notranslate}]{.pre} or
    [`s3://`{.docutils .literal .notranslate}]{.pre} scheme.

::: {.admonition .warning}
Warning

Scrapy's default referrer policy --- just like
["no-referrer-when-downgrade"](https://www.w3.org/TR/referrer-policy/#referrer-policy-no-referrer-when-downgrade){.reference
.external}, the W3C-recommended value for browsers --- will send a
non-empty "Referer" header from any [`http(s)://`{.docutils .literal
.notranslate}]{.pre} to any [`https://`{.docutils .literal
.notranslate}]{.pre} URL, even if the domain is different.

["same-origin"](https://www.w3.org/TR/referrer-policy/#referrer-policy-same-origin){.reference
.external} may be a better choice if you want to remove referrer
information for cross-domain requests.
:::

*[class]{.pre}[ ]{.w}*[[scrapy.spidermiddlewares.referer.]{.pre}]{.sig-prename .descclassname}[[NoReferrerPolicy]{.pre}]{.sig-name .descname}[[[\[source\]]{.pre}]{.viewcode-link}](_modules/scrapy/spidermiddlewares/referer.html#NoReferrerPolicy){.reference .internal}[¶](#scrapy.spidermiddlewares.referer.NoReferrerPolicy "Permalink to this definition"){.headerlink}

:   [https://www.w3.org/TR/referrer-policy/#referrer-policy-no-referrer](https://www.w3.org/TR/referrer-policy/#referrer-policy-no-referrer){.reference
    .external}

    The simplest policy is "no-referrer", which specifies that no
    referrer information is to be sent along with requests made from a
    particular request client to any origin. The header will be omitted
    entirely.

```{=html}
<!-- -->
```

*[class]{.pre}[ ]{.w}*[[scrapy.spidermiddlewares.referer.]{.pre}]{.sig-prename .descclassname}[[NoReferrerWhenDowngradePolicy]{.pre}]{.sig-name .descname}[[[\[source\]]{.pre}]{.viewcode-link}](_modules/scrapy/spidermiddlewares/referer.html#NoReferrerWhenDowngradePolicy){.reference .internal}[¶](#scrapy.spidermiddlewares.referer.NoReferrerWhenDowngradePolicy "Permalink to this definition"){.headerlink}

:   [https://www.w3.org/TR/referrer-policy/#referrer-policy-no-referrer-when-downgrade](https://www.w3.org/TR/referrer-policy/#referrer-policy-no-referrer-when-downgrade){.reference
    .external}

    The "no-referrer-when-downgrade" policy sends a full URL along with
    requests from a TLS-protected environment settings object to a
    potentially trustworthy URL, and requests from clients which are not
    TLS-protected to any origin.

    Requests from TLS-protected clients to non-potentially trustworthy
    URLs, on the other hand, will contain no referrer information. A
    Referer HTTP header will not be sent.

    This is a user agent's default behavior, if no policy is otherwise
    specified.

::: {.admonition .note}
Note

"no-referrer-when-downgrade" policy is the W3C-recommended default, and
is used by major web browsers.

However, it is NOT Scrapy's default referrer policy (see
[[`DefaultReferrerPolicy`{.xref .py .py-class .docutils .literal
.notranslate}]{.pre}](#scrapy.spidermiddlewares.referer.DefaultReferrerPolicy "scrapy.spidermiddlewares.referer.DefaultReferrerPolicy"){.reference
.internal}).
:::

*[class]{.pre}[ ]{.w}*[[scrapy.spidermiddlewares.referer.]{.pre}]{.sig-prename .descclassname}[[SameOriginPolicy]{.pre}]{.sig-name .descname}[[[\[source\]]{.pre}]{.viewcode-link}](_modules/scrapy/spidermiddlewares/referer.html#SameOriginPolicy){.reference .internal}[¶](#scrapy.spidermiddlewares.referer.SameOriginPolicy "Permalink to this definition"){.headerlink}

:   [https://www.w3.org/TR/referrer-policy/#referrer-policy-same-origin](https://www.w3.org/TR/referrer-policy/#referrer-policy-same-origin){.reference
    .external}

    The "same-origin" policy specifies that a full URL, stripped for use
    as a referrer, is sent as referrer information when making
    same-origin requests from a particular request client.

    Cross-origin requests, on the other hand, will contain no referrer
    information. A Referer HTTP header will not be sent.

```{=html}
<!-- -->
```

*[class]{.pre}[ ]{.w}*[[scrapy.spidermiddlewares.referer.]{.pre}]{.sig-prename .descclassname}[[OriginPolicy]{.pre}]{.sig-name .descname}[[[\[source\]]{.pre}]{.viewcode-link}](_modules/scrapy/spidermiddlewares/referer.html#OriginPolicy){.reference .internal}[¶](#scrapy.spidermiddlewares.referer.OriginPolicy "Permalink to this definition"){.headerlink}

:   [https://www.w3.org/TR/referrer-policy/#referrer-policy-origin](https://www.w3.org/TR/referrer-policy/#referrer-policy-origin){.reference
    .external}

    The "origin" policy specifies that only the ASCII serialization of
    the origin of the request client is sent as referrer information
    when making both same-origin requests and cross-origin requests from
    a particular request client.

```{=html}
<!-- -->
```

*[class]{.pre}[ ]{.w}*[[scrapy.spidermiddlewares.referer.]{.pre}]{.sig-prename .descclassname}[[StrictOriginPolicy]{.pre}]{.sig-name .descname}[[[\[source\]]{.pre}]{.viewcode-link}](_modules/scrapy/spidermiddlewares/referer.html#StrictOriginPolicy){.reference .internal}[¶](#scrapy.spidermiddlewares.referer.StrictOriginPolicy "Permalink to this definition"){.headerlink}

:   [https://www.w3.org/TR/referrer-policy/#referrer-policy-strict-origin](https://www.w3.org/TR/referrer-policy/#referrer-policy-strict-origin){.reference
    .external}

    The "strict-origin" policy sends the ASCII serialization of the
    origin of the request client when making requests: - from a
    TLS-protected environment settings object to a potentially
    trustworthy URL, and - from non-TLS-protected environment settings
    objects to any origin.

    Requests from TLS-protected request clients to non- potentially
    trustworthy URLs, on the other hand, will contain no referrer
    information. A Referer HTTP header will not be sent.

```{=html}
<!-- -->
```

*[class]{.pre}[ ]{.w}*[[scrapy.spidermiddlewares.referer.]{.pre}]{.sig-prename .descclassname}[[OriginWhenCrossOriginPolicy]{.pre}]{.sig-name .descname}[[[\[source\]]{.pre}]{.viewcode-link}](_modules/scrapy/spidermiddlewares/referer.html#OriginWhenCrossOriginPolicy){.reference .internal}[¶](#scrapy.spidermiddlewares.referer.OriginWhenCrossOriginPolicy "Permalink to this definition"){.headerlink}

:   [https://www.w3.org/TR/referrer-policy/#referrer-policy-origin-when-cross-origin](https://www.w3.org/TR/referrer-policy/#referrer-policy-origin-when-cross-origin){.reference
    .external}

    The "origin-when-cross-origin" policy specifies that a full URL,
    stripped for use as a referrer, is sent as referrer information when
    making same-origin requests from a particular request client, and
    only the ASCII serialization of the origin of the request client is
    sent as referrer information when making cross-origin requests from
    a particular request client.

```{=html}
<!-- -->
```

*[class]{.pre}[ ]{.w}*[[scrapy.spidermiddlewares.referer.]{.pre}]{.sig-prename .descclassname}[[StrictOriginWhenCrossOriginPolicy]{.pre}]{.sig-name .descname}[[[\[source\]]{.pre}]{.viewcode-link}](_modules/scrapy/spidermiddlewares/referer.html#StrictOriginWhenCrossOriginPolicy){.reference .internal}[¶](#scrapy.spidermiddlewares.referer.StrictOriginWhenCrossOriginPolicy "Permalink to this definition"){.headerlink}

:   [https://www.w3.org/TR/referrer-policy/#referrer-policy-strict-origin-when-cross-origin](https://www.w3.org/TR/referrer-policy/#referrer-policy-strict-origin-when-cross-origin){.reference
    .external}

    The "strict-origin-when-cross-origin" policy specifies that a full
    URL, stripped for use as a referrer, is sent as referrer information
    when making same-origin requests from a particular request client,
    and only the ASCII serialization of the origin of the request client
    when making cross-origin requests:

    -   from a TLS-protected environment settings object to a
        potentially trustworthy URL, and

    -   from non-TLS-protected environment settings objects to any
        origin.

    Requests from TLS-protected clients to non- potentially trustworthy
    URLs, on the other hand, will contain no referrer information. A
    Referer HTTP header will not be sent.

```{=html}
<!-- -->
```

*[class]{.pre}[ ]{.w}*[[scrapy.spidermiddlewares.referer.]{.pre}]{.sig-prename .descclassname}[[UnsafeUrlPolicy]{.pre}]{.sig-name .descname}[[[\[source\]]{.pre}]{.viewcode-link}](_modules/scrapy/spidermiddlewares/referer.html#UnsafeUrlPolicy){.reference .internal}[¶](#scrapy.spidermiddlewares.referer.UnsafeUrlPolicy "Permalink to this definition"){.headerlink}

:   [https://www.w3.org/TR/referrer-policy/#referrer-policy-unsafe-url](https://www.w3.org/TR/referrer-policy/#referrer-policy-unsafe-url){.reference
    .external}

    The "unsafe-url" policy specifies that a full URL, stripped for use
    as a referrer, is sent along with both cross-origin requests and
    same-origin requests made from a particular request client.

    Note: The policy's name doesn't lie; it is unsafe. This policy will
    leak origins and paths from TLS-protected resources to insecure
    origins. Carefully consider the impact of setting such a policy for
    potentially sensitive documents.

::: {.admonition .warning}
Warning

"unsafe-url" policy is NOT recommended.
:::
:::
:::
:::
:::

::: {#module-scrapy.spidermiddlewares.urllength .section}
[]{#urllengthmiddleware}

##### UrlLengthMiddleware[¶](#module-scrapy.spidermiddlewares.urllength "Permalink to this heading"){.headerlink}

*[class]{.pre}[ ]{.w}*[[scrapy.spidermiddlewares.urllength.]{.pre}]{.sig-prename .descclassname}[[UrlLengthMiddleware]{.pre}]{.sig-name .descname}[[[\[source\]]{.pre}]{.viewcode-link}](_modules/scrapy/spidermiddlewares/urllength.html#UrlLengthMiddleware){.reference .internal}[¶](#scrapy.spidermiddlewares.urllength.UrlLengthMiddleware "Permalink to this definition"){.headerlink}

:   Filters out requests with URLs longer than URLLENGTH_LIMIT

    The [[`UrlLengthMiddleware`{.xref .py .py-class .docutils .literal
    .notranslate}]{.pre}](#scrapy.spidermiddlewares.urllength.UrlLengthMiddleware "scrapy.spidermiddlewares.urllength.UrlLengthMiddleware"){.reference
    .internal} can be configured through the following settings (see the
    settings documentation for more info):

    > <div>
    >
    > -   [[`URLLENGTH_LIMIT`{.xref .std .std-setting .docutils .literal
    >     .notranslate}]{.pre}](index.html#std-setting-URLLENGTH_LIMIT){.hoverxref
    >     .tooltip .reference .internal} - The maximum URL length to
    >     allow for crawled URLs.
    >
    > </div>
:::
:::
:::

[]{#document-topics/extensions}

::: {#extensions .section}
[]{#topics-extensions}

### Extensions[¶](#extensions "Permalink to this heading"){.headerlink}

The extensions framework provides a mechanism for inserting your own
custom functionality into Scrapy.

Extensions are just regular classes.

::: {#extension-settings .section}
#### Extension settings[¶](#extension-settings "Permalink to this heading"){.headerlink}

Extensions use the [[Scrapy settings]{.std
.std-ref}](index.html#topics-settings){.hoverxref .tooltip .reference
.internal} to manage their settings, just like any other Scrapy code.

It is customary for extensions to prefix their settings with their own
name, to avoid collision with existing (and future) extensions. For
example, a hypothetical extension to handle [Google
Sitemaps](https://en.wikipedia.org/wiki/Sitemaps){.reference .external}
would use settings like [`GOOGLESITEMAP_ENABLED`{.docutils .literal
.notranslate}]{.pre}, [`GOOGLESITEMAP_DEPTH`{.docutils .literal
.notranslate}]{.pre}, and so on.
:::

::: {#loading-activating-extensions .section}
#### Loading & activating extensions[¶](#loading-activating-extensions "Permalink to this heading"){.headerlink}

Extensions are loaded and activated at startup by instantiating a single
instance of the extension class per spider being run. All the extension
initialization code must be performed in the class [`__init__`{.docutils
.literal .notranslate}]{.pre} method.

To make an extension available, add it to the [[`EXTENSIONS`{.xref .std
.std-setting .docutils .literal
.notranslate}]{.pre}](index.html#std-setting-EXTENSIONS){.hoverxref
.tooltip .reference .internal} setting in your Scrapy settings. In
[[`EXTENSIONS`{.xref .std .std-setting .docutils .literal
.notranslate}]{.pre}](index.html#std-setting-EXTENSIONS){.hoverxref
.tooltip .reference .internal}, each extension is represented by a
string: the full Python path to the extension's class name. For example:

::: {.highlight-python .notranslate}
::: highlight
    EXTENSIONS = {
        "scrapy.extensions.corestats.CoreStats": 500,
        "scrapy.extensions.telnet.TelnetConsole": 500,
    }
:::
:::

As you can see, the [[`EXTENSIONS`{.xref .std .std-setting .docutils
.literal
.notranslate}]{.pre}](index.html#std-setting-EXTENSIONS){.hoverxref
.tooltip .reference .internal} setting is a dict where the keys are the
extension paths, and their values are the orders, which define the
extension *loading* order. The [[`EXTENSIONS`{.xref .std .std-setting
.docutils .literal
.notranslate}]{.pre}](index.html#std-setting-EXTENSIONS){.hoverxref
.tooltip .reference .internal} setting is merged with the
[[`EXTENSIONS_BASE`{.xref .std .std-setting .docutils .literal
.notranslate}]{.pre}](index.html#std-setting-EXTENSIONS_BASE){.hoverxref
.tooltip .reference .internal} setting defined in Scrapy (and not meant
to be overridden) and then sorted by order to get the final sorted list
of enabled extensions.

As extensions typically do not depend on each other, their loading order
is irrelevant in most cases. This is why the [[`EXTENSIONS_BASE`{.xref
.std .std-setting .docutils .literal
.notranslate}]{.pre}](index.html#std-setting-EXTENSIONS_BASE){.hoverxref
.tooltip .reference .internal} setting defines all extensions with the
same order ([`0`{.docutils .literal .notranslate}]{.pre}). However, this
feature can be exploited if you need to add an extension which depends
on other extensions already loaded.
:::

::: {#available-enabled-and-disabled-extensions .section}
#### Available, enabled and disabled extensions[¶](#available-enabled-and-disabled-extensions "Permalink to this heading"){.headerlink}

Not all available extensions will be enabled. Some of them usually
depend on a particular setting. For example, the HTTP Cache extension is
available by default but disabled unless the [[`HTTPCACHE_ENABLED`{.xref
.std .std-setting .docutils .literal
.notranslate}]{.pre}](index.html#std-setting-HTTPCACHE_ENABLED){.hoverxref
.tooltip .reference .internal} setting is set.
:::

::: {#disabling-an-extension .section}
#### Disabling an extension[¶](#disabling-an-extension "Permalink to this heading"){.headerlink}

In order to disable an extension that comes enabled by default (i.e.
those included in the [[`EXTENSIONS_BASE`{.xref .std .std-setting
.docutils .literal
.notranslate}]{.pre}](index.html#std-setting-EXTENSIONS_BASE){.hoverxref
.tooltip .reference .internal} setting) you must set its order to
[`None`{.docutils .literal .notranslate}]{.pre}. For example:

::: {.highlight-python .notranslate}
::: highlight
    EXTENSIONS = {
        "scrapy.extensions.corestats.CoreStats": None,
    }
:::
:::
:::

::: {#writing-your-own-extension .section}
#### Writing your own extension[¶](#writing-your-own-extension "Permalink to this heading"){.headerlink}

Each extension is a Python class. The main entry point for a Scrapy
extension (this also includes middlewares and pipelines) is the
[`from_crawler`{.docutils .literal .notranslate}]{.pre} class method
which receives a [`Crawler`{.docutils .literal .notranslate}]{.pre}
instance. Through the Crawler object you can access settings, signals,
stats, and also control the crawling behaviour.

Typically, extensions connect to [[signals]{.std
.std-ref}](index.html#topics-signals){.hoverxref .tooltip .reference
.internal} and perform tasks triggered by them.

Finally, if the [`from_crawler`{.docutils .literal .notranslate}]{.pre}
method raises the [[`NotConfigured`{.xref .py .py-exc .docutils .literal
.notranslate}]{.pre}](index.html#scrapy.exceptions.NotConfigured "scrapy.exceptions.NotConfigured"){.reference
.internal} exception, the extension will be disabled. Otherwise, the
extension will be enabled.

::: {#sample-extension .section}
##### Sample extension[¶](#sample-extension "Permalink to this heading"){.headerlink}

Here we will implement a simple extension to illustrate the concepts
described in the previous section. This extension will log a message
every time:

-   a spider is opened

-   a spider is closed

-   a specific number of items are scraped

The extension will be enabled through the [`MYEXT_ENABLED`{.docutils
.literal .notranslate}]{.pre} setting and the number of items will be
specified through the [`MYEXT_ITEMCOUNT`{.docutils .literal
.notranslate}]{.pre} setting.

Here is the code of such extension:

::: {.highlight-python .notranslate}
::: highlight
    import logging
    from scrapy import signals
    from scrapy.exceptions import NotConfigured

    logger = logging.getLogger(__name__)


    class SpiderOpenCloseLogging:
        def __init__(self, item_count):
            self.item_count = item_count
            self.items_scraped = 0

        @classmethod
        def from_crawler(cls, crawler):
            # first check if the extension should be enabled and raise
            # NotConfigured otherwise
            if not crawler.settings.getbool("MYEXT_ENABLED"):
                raise NotConfigured

            # get the number of items from settings
            item_count = crawler.settings.getint("MYEXT_ITEMCOUNT", 1000)

            # instantiate the extension object
            ext = cls(item_count)

            # connect the extension object to signals
            crawler.signals.connect(ext.spider_opened, signal=signals.spider_opened)
            crawler.signals.connect(ext.spider_closed, signal=signals.spider_closed)
            crawler.signals.connect(ext.item_scraped, signal=signals.item_scraped)

            # return the extension object
            return ext

        def spider_opened(self, spider):
            logger.info("opened spider %s", spider.name)

        def spider_closed(self, spider):
            logger.info("closed spider %s", spider.name)

        def item_scraped(self, item, spider):
            self.items_scraped += 1
            if self.items_scraped % self.item_count == 0:
                logger.info("scraped %d items", self.items_scraped)
:::
:::
:::
:::

::: {#built-in-extensions-reference .section}
[]{#topics-extensions-ref}

#### Built-in extensions reference[¶](#built-in-extensions-reference "Permalink to this heading"){.headerlink}

::: {#general-purpose-extensions .section}
##### General purpose extensions[¶](#general-purpose-extensions "Permalink to this heading"){.headerlink}

::: {#module-scrapy.extensions.logstats .section}
[]{#log-stats-extension}

###### Log Stats extension[¶](#module-scrapy.extensions.logstats "Permalink to this heading"){.headerlink}

*[class]{.pre}[ ]{.w}*[[scrapy.extensions.logstats.]{.pre}]{.sig-prename .descclassname}[[LogStats]{.pre}]{.sig-name .descname}[[[\[source\]]{.pre}]{.viewcode-link}](_modules/scrapy/extensions/logstats.html#LogStats){.reference .internal}[¶](#scrapy.extensions.logstats.LogStats "Permalink to this definition"){.headerlink}

:   

Log basic stats like crawled pages and scraped items.
:::

::: {#module-scrapy.extensions.corestats .section}
[]{#core-stats-extension}

###### Core Stats extension[¶](#module-scrapy.extensions.corestats "Permalink to this heading"){.headerlink}

*[class]{.pre}[ ]{.w}*[[scrapy.extensions.corestats.]{.pre}]{.sig-prename .descclassname}[[CoreStats]{.pre}]{.sig-name .descname}[[[\[source\]]{.pre}]{.viewcode-link}](_modules/scrapy/extensions/corestats.html#CoreStats){.reference .internal}[¶](#scrapy.extensions.corestats.CoreStats "Permalink to this definition"){.headerlink}

:   

Enable the collection of core statistics, provided the stats collection
is enabled (see [[Stats Collection]{.std
.std-ref}](index.html#topics-stats){.hoverxref .tooltip .reference
.internal}).
:::

::: {#module-scrapy.extensions.telnet .section}
[]{#telnet-console-extension}[]{#topics-extensions-ref-telnetconsole}

###### Telnet console extension[¶](#module-scrapy.extensions.telnet "Permalink to this heading"){.headerlink}

*[class]{.pre}[ ]{.w}*[[scrapy.extensions.telnet.]{.pre}]{.sig-prename .descclassname}[[TelnetConsole]{.pre}]{.sig-name .descname}[[[\[source\]]{.pre}]{.viewcode-link}](_modules/scrapy/extensions/telnet.html#TelnetConsole){.reference .internal}[¶](#scrapy.extensions.telnet.TelnetConsole "Permalink to this definition"){.headerlink}

:   

Provides a telnet console for getting into a Python interpreter inside
the currently running Scrapy process, which can be very useful for
debugging.

The telnet console must be enabled by the
[[`TELNETCONSOLE_ENABLED`{.xref .std .std-setting .docutils .literal
.notranslate}]{.pre}](index.html#std-setting-TELNETCONSOLE_ENABLED){.hoverxref
.tooltip .reference .internal} setting, and the server will listen in
the port specified in [[`TELNETCONSOLE_PORT`{.xref .std .std-setting
.docutils .literal
.notranslate}]{.pre}](index.html#std-setting-TELNETCONSOLE_PORT){.hoverxref
.tooltip .reference .internal}.
:::

::: {#module-scrapy.extensions.memusage .section}
[]{#memory-usage-extension}[]{#topics-extensions-ref-memusage}

###### Memory usage extension[¶](#module-scrapy.extensions.memusage "Permalink to this heading"){.headerlink}

*[class]{.pre}[ ]{.w}*[[scrapy.extensions.memusage.]{.pre}]{.sig-prename .descclassname}[[MemoryUsage]{.pre}]{.sig-name .descname}[[[\[source\]]{.pre}]{.viewcode-link}](_modules/scrapy/extensions/memusage.html#MemoryUsage){.reference .internal}[¶](#scrapy.extensions.memusage.MemoryUsage "Permalink to this definition"){.headerlink}

:   

::: {.admonition .note}
Note

This extension does not work in Windows.
:::

Monitors the memory used by the Scrapy process that runs the spider and:

1.  sends a notification e-mail when it exceeds a certain value

2.  closes the spider when it exceeds a certain value

The notification e-mails can be triggered when a certain warning value
is reached ([[`MEMUSAGE_WARNING_MB`{.xref .std .std-setting .docutils
.literal
.notranslate}]{.pre}](index.html#std-setting-MEMUSAGE_WARNING_MB){.hoverxref
.tooltip .reference .internal}) and when the maximum value is reached
([[`MEMUSAGE_LIMIT_MB`{.xref .std .std-setting .docutils .literal
.notranslate}]{.pre}](index.html#std-setting-MEMUSAGE_LIMIT_MB){.hoverxref
.tooltip .reference .internal}) which will also cause the spider to be
closed and the Scrapy process to be terminated.

This extension is enabled by the [[`MEMUSAGE_ENABLED`{.xref .std
.std-setting .docutils .literal
.notranslate}]{.pre}](index.html#std-setting-MEMUSAGE_ENABLED){.hoverxref
.tooltip .reference .internal} setting and can be configured with the
following settings:

-   [[`MEMUSAGE_LIMIT_MB`{.xref .std .std-setting .docutils .literal
    .notranslate}]{.pre}](index.html#std-setting-MEMUSAGE_LIMIT_MB){.hoverxref
    .tooltip .reference .internal}

-   [[`MEMUSAGE_WARNING_MB`{.xref .std .std-setting .docutils .literal
    .notranslate}]{.pre}](index.html#std-setting-MEMUSAGE_WARNING_MB){.hoverxref
    .tooltip .reference .internal}

-   [[`MEMUSAGE_NOTIFY_MAIL`{.xref .std .std-setting .docutils .literal
    .notranslate}]{.pre}](index.html#std-setting-MEMUSAGE_NOTIFY_MAIL){.hoverxref
    .tooltip .reference .internal}

-   [[`MEMUSAGE_CHECK_INTERVAL_SECONDS`{.xref .std .std-setting
    .docutils .literal
    .notranslate}]{.pre}](index.html#std-setting-MEMUSAGE_CHECK_INTERVAL_SECONDS){.hoverxref
    .tooltip .reference .internal}
:::

::: {#module-scrapy.extensions.memdebug .section}
[]{#memory-debugger-extension}

###### Memory debugger extension[¶](#module-scrapy.extensions.memdebug "Permalink to this heading"){.headerlink}

*[class]{.pre}[ ]{.w}*[[scrapy.extensions.memdebug.]{.pre}]{.sig-prename .descclassname}[[MemoryDebugger]{.pre}]{.sig-name .descname}[[[\[source\]]{.pre}]{.viewcode-link}](_modules/scrapy/extensions/memdebug.html#MemoryDebugger){.reference .internal}[¶](#scrapy.extensions.memdebug.MemoryDebugger "Permalink to this definition"){.headerlink}

:   

An extension for debugging memory usage. It collects information about:

-   objects uncollected by the Python garbage collector

-   objects left alive that shouldn't. For more info, see [[Debugging
    memory leaks with trackref]{.std
    .std-ref}](index.html#topics-leaks-trackrefs){.hoverxref .tooltip
    .reference .internal}

To enable this extension, turn on the [[`MEMDEBUG_ENABLED`{.xref .std
.std-setting .docutils .literal
.notranslate}]{.pre}](index.html#std-setting-MEMDEBUG_ENABLED){.hoverxref
.tooltip .reference .internal} setting. The info will be stored in the
stats.
:::

::: {#module-scrapy.extensions.closespider .section}
[]{#close-spider-extension}

###### Close spider extension[¶](#module-scrapy.extensions.closespider "Permalink to this heading"){.headerlink}

*[class]{.pre}[ ]{.w}*[[scrapy.extensions.closespider.]{.pre}]{.sig-prename .descclassname}[[CloseSpider]{.pre}]{.sig-name .descname}[[[\[source\]]{.pre}]{.viewcode-link}](_modules/scrapy/extensions/closespider.html#CloseSpider){.reference .internal}[¶](#scrapy.extensions.closespider.CloseSpider "Permalink to this definition"){.headerlink}

:   

Closes a spider automatically when some conditions are met, using a
specific closing reason for each condition.

The conditions for closing a spider can be configured through the
following settings:

-   [[`CLOSESPIDER_TIMEOUT`{.xref .std .std-setting .docutils .literal
    .notranslate}]{.pre}](#std-setting-CLOSESPIDER_TIMEOUT){.hoverxref
    .tooltip .reference .internal}

-   [[`CLOSESPIDER_TIMEOUT_NO_ITEM`{.xref .std .std-setting .docutils
    .literal
    .notranslate}]{.pre}](#std-setting-CLOSESPIDER_TIMEOUT_NO_ITEM){.hoverxref
    .tooltip .reference .internal}

-   [[`CLOSESPIDER_ITEMCOUNT`{.xref .std .std-setting .docutils .literal
    .notranslate}]{.pre}](#std-setting-CLOSESPIDER_ITEMCOUNT){.hoverxref
    .tooltip .reference .internal}

-   [[`CLOSESPIDER_PAGECOUNT`{.xref .std .std-setting .docutils .literal
    .notranslate}]{.pre}](#std-setting-CLOSESPIDER_PAGECOUNT){.hoverxref
    .tooltip .reference .internal}

-   [[`CLOSESPIDER_ERRORCOUNT`{.xref .std .std-setting .docutils
    .literal
    .notranslate}]{.pre}](#std-setting-CLOSESPIDER_ERRORCOUNT){.hoverxref
    .tooltip .reference .internal}

::: {.admonition .note}
Note

When a certain closing condition is met, requests which are currently in
the downloader queue (up to [[`CONCURRENT_REQUESTS`{.xref .std
.std-setting .docutils .literal
.notranslate}]{.pre}](index.html#std-setting-CONCURRENT_REQUESTS){.hoverxref
.tooltip .reference .internal} requests) are still processed.
:::

::: {#closespider-timeout .section}
[]{#std-setting-CLOSESPIDER_TIMEOUT}CLOSESPIDER_TIMEOUT[¶](#closespider-timeout "Permalink to this heading"){.headerlink}

Default: [`0`{.docutils .literal .notranslate}]{.pre}

An integer which specifies a number of seconds. If the spider remains
open for more than that number of second, it will be automatically
closed with the reason [`closespider_timeout`{.docutils .literal
.notranslate}]{.pre}. If zero (or non set), spiders won't be closed by
timeout.
:::

::: {#closespider-timeout-no-item .section}
[]{#std-setting-CLOSESPIDER_TIMEOUT_NO_ITEM}CLOSESPIDER_TIMEOUT_NO_ITEM[¶](#closespider-timeout-no-item "Permalink to this heading"){.headerlink}

Default: [`0`{.docutils .literal .notranslate}]{.pre}

An integer which specifies a number of seconds. If the spider has not
produced any items in the last number of seconds, it will be closed with
the reason [`closespider_timeout_no_item`{.docutils .literal
.notranslate}]{.pre}. If zero (or non set), spiders won't be closed
regardless if it hasn't produced any items.
:::

::: {#closespider-itemcount .section}
[]{#std-setting-CLOSESPIDER_ITEMCOUNT}CLOSESPIDER_ITEMCOUNT[¶](#closespider-itemcount "Permalink to this heading"){.headerlink}

Default: [`0`{.docutils .literal .notranslate}]{.pre}

An integer which specifies a number of items. If the spider scrapes more
than that amount and those items are passed by the item pipeline, the
spider will be closed with the reason [`closespider_itemcount`{.docutils
.literal .notranslate}]{.pre}. If zero (or non set), spiders won't be
closed by number of passed items.
:::

::: {#closespider-pagecount .section}
[]{#std-setting-CLOSESPIDER_PAGECOUNT}CLOSESPIDER_PAGECOUNT[¶](#closespider-pagecount "Permalink to this heading"){.headerlink}

Default: [`0`{.docutils .literal .notranslate}]{.pre}

An integer which specifies the maximum number of responses to crawl. If
the spider crawls more than that, the spider will be closed with the
reason [`closespider_pagecount`{.docutils .literal .notranslate}]{.pre}.
If zero (or non set), spiders won't be closed by number of crawled
responses.
:::

::: {#closespider-errorcount .section}
[]{#std-setting-CLOSESPIDER_ERRORCOUNT}CLOSESPIDER_ERRORCOUNT[¶](#closespider-errorcount "Permalink to this heading"){.headerlink}

Default: [`0`{.docutils .literal .notranslate}]{.pre}

An integer which specifies the maximum number of errors to receive
before closing the spider. If the spider generates more than that number
of errors, it will be closed with the reason
[`closespider_errorcount`{.docutils .literal .notranslate}]{.pre}. If
zero (or non set), spiders won't be closed by number of errors.
:::
:::

::: {#module-scrapy.extensions.statsmailer .section}
[]{#statsmailer-extension}

###### StatsMailer extension[¶](#module-scrapy.extensions.statsmailer "Permalink to this heading"){.headerlink}

*[class]{.pre}[ ]{.w}*[[scrapy.extensions.statsmailer.]{.pre}]{.sig-prename .descclassname}[[StatsMailer]{.pre}]{.sig-name .descname}[[[\[source\]]{.pre}]{.viewcode-link}](_modules/scrapy/extensions/statsmailer.html#StatsMailer){.reference .internal}[¶](#scrapy.extensions.statsmailer.StatsMailer "Permalink to this definition"){.headerlink}

:   

This simple extension can be used to send a notification e-mail every
time a domain has finished scraping, including the Scrapy stats
collected. The email will be sent to all recipients specified in the
[[`STATSMAILER_RCPTS`{.xref .std .std-setting .docutils .literal
.notranslate}]{.pre}](index.html#std-setting-STATSMAILER_RCPTS){.hoverxref
.tooltip .reference .internal} setting.

Emails can be sent using the [[`MailSender`{.xref .py .py-class
.docutils .literal
.notranslate}]{.pre}](index.html#scrapy.mail.MailSender "scrapy.mail.MailSender"){.reference
.internal} class. To see a full list of parameters, including examples
on how to instantiate [[`MailSender`{.xref .py .py-class .docutils
.literal
.notranslate}]{.pre}](index.html#scrapy.mail.MailSender "scrapy.mail.MailSender"){.reference
.internal} and use mail settings, see [[Sending e-mail]{.std
.std-ref}](index.html#topics-email){.hoverxref .tooltip .reference
.internal}.

[]{#module-scrapy.extensions.debug
.target}[]{#module-scrapy.extensions.periodic_log .target}
:::

::: {#periodic-log-extension .section}
###### Periodic log extension[¶](#periodic-log-extension "Permalink to this heading"){.headerlink}

*[class]{.pre}[ ]{.w}*[[scrapy.extensions.periodic_log.]{.pre}]{.sig-prename .descclassname}[[PeriodicLog]{.pre}]{.sig-name .descname}[[[\[source\]]{.pre}]{.viewcode-link}](_modules/scrapy/extensions/periodic_log.html#PeriodicLog){.reference .internal}[¶](#scrapy.extensions.periodic_log.PeriodicLog "Permalink to this definition"){.headerlink}

:   

This extension periodically logs rich stat data as a JSON object:

::: {.highlight-default .notranslate}
::: highlight
    2023-08-04 02:30:57 [scrapy.extensions.logstats] INFO: Crawled 976 pages (at 162 pages/min), scraped 925 items (at 161 items/min)
    2023-08-04 02:30:57 [scrapy.extensions.periodic_log] INFO: {
        "delta": {
            "downloader/request_bytes": 55582,
            "downloader/request_count": 162,
            "downloader/request_method_count/GET": 162,
            "downloader/response_bytes": 618133,
            "downloader/response_count": 162,
            "downloader/response_status_count/200": 162,
            "item_scraped_count": 161
        },
        "stats": {
            "downloader/request_bytes": 338243,
            "downloader/request_count": 992,
            "downloader/request_method_count/GET": 992,
            "downloader/response_bytes": 3836736,
            "downloader/response_count": 976,
            "downloader/response_status_count/200": 976,
            "item_scraped_count": 925,
            "log_count/INFO": 21,
            "log_count/WARNING": 1,
            "scheduler/dequeued": 992,
            "scheduler/dequeued/memory": 992,
            "scheduler/enqueued": 1050,
            "scheduler/enqueued/memory": 1050
        },
        "time": {
            "elapsed": 360.008903,
            "log_interval": 60.0,
            "log_interval_real": 60.006694,
            "start_time": "2023-08-03 23:24:57",
            "utcnow": "2023-08-03 23:30:57"
        }
    }
:::
:::

This extension logs the following configurable sections:

-   [`"delta"`{.docutils .literal .notranslate}]{.pre} shows how some
    numeric stats have changed since the last stats log message.

    The [[`PERIODIC_LOG_DELTA`{.xref .std .std-setting .docutils
    .literal
    .notranslate}]{.pre}](#std-setting-PERIODIC_LOG_DELTA){.hoverxref
    .tooltip .reference .internal} setting determines the target stats.
    They must have [`int`{.docutils .literal .notranslate}]{.pre} or
    [`float`{.docutils .literal .notranslate}]{.pre} values.

-   [`"stats"`{.docutils .literal .notranslate}]{.pre} shows the current
    value of some stats.

    The [[`PERIODIC_LOG_STATS`{.xref .std .std-setting .docutils
    .literal
    .notranslate}]{.pre}](#std-setting-PERIODIC_LOG_STATS){.hoverxref
    .tooltip .reference .internal} setting determines the target stats.

-   [`"time"`{.docutils .literal .notranslate}]{.pre} shows detailed
    timing data.

    The [[`PERIODIC_LOG_TIMING_ENABLED`{.xref .std .std-setting
    .docutils .literal
    .notranslate}]{.pre}](#std-setting-PERIODIC_LOG_TIMING_ENABLED){.hoverxref
    .tooltip .reference .internal} setting determines whether or not to
    show this section.

This extension logs data at the start, then on a fixed time interval
configurable through the [[`LOGSTATS_INTERVAL`{.xref .std .std-setting
.docutils .literal
.notranslate}]{.pre}](index.html#std-setting-LOGSTATS_INTERVAL){.hoverxref
.tooltip .reference .internal} setting, and finally right before the
crawl ends.

Example extension configuration:

::: {.highlight-python .notranslate}
::: highlight
    custom_settings = {
        "LOG_LEVEL": "INFO",
        "PERIODIC_LOG_STATS": {
            "include": ["downloader/", "scheduler/", "log_count/", "item_scraped_count/"],
        },
        "PERIODIC_LOG_DELTA": {"include": ["downloader/"]},
        "PERIODIC_LOG_TIMING_ENABLED": True,
        "EXTENSIONS": {
            "scrapy.extensions.periodic_log.PeriodicLog": 0,
        },
    }
:::
:::

::: {#periodic-log-delta .section}
[]{#std-setting-PERIODIC_LOG_DELTA}PERIODIC_LOG_DELTA[¶](#periodic-log-delta "Permalink to this heading"){.headerlink}

Default: [`None`{.docutils .literal .notranslate}]{.pre}

-   [`"PERIODIC_LOG_DELTA":`{.docutils .literal
    .notranslate}]{.pre}` `{.docutils .literal
    .notranslate}[`True`{.docutils .literal .notranslate}]{.pre} - show
    deltas for all [`int`{.docutils .literal .notranslate}]{.pre} and
    [`float`{.docutils .literal .notranslate}]{.pre} stat values.

-   [`"PERIODIC_LOG_DELTA":`{.docutils .literal
    .notranslate}]{.pre}` `{.docutils .literal
    .notranslate}[`{"include":`{.docutils .literal
    .notranslate}]{.pre}` `{.docutils .literal
    .notranslate}[`["downloader/",`{.docutils .literal
    .notranslate}]{.pre}` `{.docutils .literal
    .notranslate}[`"scheduler/"]}`{.docutils .literal
    .notranslate}]{.pre} - show deltas for stats with names containing
    any configured substring.

-   [`"PERIODIC_LOG_DELTA":`{.docutils .literal
    .notranslate}]{.pre}` `{.docutils .literal
    .notranslate}[`{"exclude":`{.docutils .literal
    .notranslate}]{.pre}` `{.docutils .literal
    .notranslate}[`["downloader/"]}`{.docutils .literal
    .notranslate}]{.pre} - show deltas for all stats with names not
    containing any configured substring.
:::

::: {#periodic-log-stats .section}
[]{#std-setting-PERIODIC_LOG_STATS}PERIODIC_LOG_STATS[¶](#periodic-log-stats "Permalink to this heading"){.headerlink}

Default: [`None`{.docutils .literal .notranslate}]{.pre}

-   [`"PERIODIC_LOG_STATS":`{.docutils .literal
    .notranslate}]{.pre}` `{.docutils .literal
    .notranslate}[`True`{.docutils .literal .notranslate}]{.pre} - show
    the current value of all stats.

-   [`"PERIODIC_LOG_STATS":`{.docutils .literal
    .notranslate}]{.pre}` `{.docutils .literal
    .notranslate}[`{"include":`{.docutils .literal
    .notranslate}]{.pre}` `{.docutils .literal
    .notranslate}[`["downloader/",`{.docutils .literal
    .notranslate}]{.pre}` `{.docutils .literal
    .notranslate}[`"scheduler/"]}`{.docutils .literal
    .notranslate}]{.pre} - show current values for stats with names
    containing any configured substring.

-   [`"PERIODIC_LOG_STATS":`{.docutils .literal
    .notranslate}]{.pre}` `{.docutils .literal
    .notranslate}[`{"exclude":`{.docutils .literal
    .notranslate}]{.pre}` `{.docutils .literal
    .notranslate}[`["downloader/"]}`{.docutils .literal
    .notranslate}]{.pre} - show current values for all stats with names
    not containing any configured substring.
:::

::: {#periodic-log-timing-enabled .section}
[]{#std-setting-PERIODIC_LOG_TIMING_ENABLED}PERIODIC_LOG_TIMING_ENABLED[¶](#periodic-log-timing-enabled "Permalink to this heading"){.headerlink}

Default: [`False`{.docutils .literal .notranslate}]{.pre}

[`True`{.docutils .literal .notranslate}]{.pre} enables logging of
timing data (i.e. the [`"time"`{.docutils .literal .notranslate}]{.pre}
section).
:::
:::
:::

::: {#debugging-extensions .section}
##### Debugging extensions[¶](#debugging-extensions "Permalink to this heading"){.headerlink}

::: {#stack-trace-dump-extension .section}
###### Stack trace dump extension[¶](#stack-trace-dump-extension "Permalink to this heading"){.headerlink}

*[class]{.pre}[ ]{.w}*[[scrapy.extensions.periodic_log.]{.pre}]{.sig-prename .descclassname}[[StackTraceDump]{.pre}]{.sig-name .descname}[¶](#scrapy.extensions.periodic_log.StackTraceDump "Permalink to this definition"){.headerlink}

:   

Dumps information about the running process when a
[SIGQUIT](https://en.wikipedia.org/wiki/SIGQUIT){.reference .external}
or
[SIGUSR2](https://en.wikipedia.org/wiki/SIGUSR1_and_SIGUSR2){.reference
.external} signal is received. The information dumped is the following:

1.  engine status (using
    [`scrapy.utils.engine.get_engine_status()`{.docutils .literal
    .notranslate}]{.pre})

2.  live references (see [[Debugging memory leaks with trackref]{.std
    .std-ref}](index.html#topics-leaks-trackrefs){.hoverxref .tooltip
    .reference .internal})

3.  stack trace of all threads

After the stack trace and engine status is dumped, the Scrapy process
continues running normally.

This extension only works on POSIX-compliant platforms (i.e. not
Windows), because the
[SIGQUIT](https://en.wikipedia.org/wiki/SIGQUIT){.reference .external}
and
[SIGUSR2](https://en.wikipedia.org/wiki/SIGUSR1_and_SIGUSR2){.reference
.external} signals are not available on Windows.

There are at least two ways to send Scrapy the
[SIGQUIT](https://en.wikipedia.org/wiki/SIGQUIT){.reference .external}
signal:

1.  By pressing Ctrl-while a Scrapy process is running (Linux only?)

2.  By running this command (assuming [`<pid>`{.docutils .literal
    .notranslate}]{.pre} is the process id of the Scrapy process):

    ::: {.highlight-default .notranslate}
    ::: highlight
        kill -QUIT <pid>
    :::
    :::
:::

::: {#debugger-extension .section}
###### Debugger extension[¶](#debugger-extension "Permalink to this heading"){.headerlink}

*[class]{.pre}[ ]{.w}*[[scrapy.extensions.periodic_log.]{.pre}]{.sig-prename .descclassname}[[Debugger]{.pre}]{.sig-name .descname}[¶](#scrapy.extensions.periodic_log.Debugger "Permalink to this definition"){.headerlink}

:   

Invokes a [[Python debugger]{.xref .std
.std-doc}](https://docs.python.org/3/library/pdb.html "(in Python v3.12)"){.reference
.external} inside a running Scrapy process when a
[SIGUSR2](https://en.wikipedia.org/wiki/SIGUSR1_and_SIGUSR2){.reference
.external} signal is received. After the debugger is exited, the Scrapy
process continues running normally.

For more info see [Debugging in
Python](https://pythonconquerstheuniverse.wordpress.com/2009/09/10/debugging-in-python/){.reference
.external}.

This extension only works on POSIX-compliant platforms (i.e. not
Windows).
:::
:::
:::
:::

[]{#document-topics/signals}

::: {#signals .section}
[]{#topics-signals}

### Signals[¶](#signals "Permalink to this heading"){.headerlink}

Scrapy uses signals extensively to notify when certain events occur. You
can catch some of those signals in your Scrapy project (using an
[[extension]{.std .std-ref}](index.html#topics-extensions){.hoverxref
.tooltip .reference .internal}, for example) to perform additional tasks
or extend Scrapy to add functionality not provided out of the box.

Even though signals provide several arguments, the handlers that catch
them don't need to accept all of them - the signal dispatching mechanism
will only deliver the arguments that the handler receives.

You can connect to signals (or send your own) through the [[Signals
API]{.std .std-ref}](index.html#topics-api-signals){.hoverxref .tooltip
.reference .internal}.

Here is a simple example showing how you can catch signals and perform
some action:

::: {.highlight-python .notranslate}
::: highlight
    from scrapy import signals
    from scrapy import Spider


    class DmozSpider(Spider):
        name = "dmoz"
        allowed_domains = ["dmoz.org"]
        start_urls = [
            "http://www.dmoz.org/Computers/Programming/Languages/Python/Books/",
            "http://www.dmoz.org/Computers/Programming/Languages/Python/Resources/",
        ]

        @classmethod
        def from_crawler(cls, crawler, *args, **kwargs):
            spider = super(DmozSpider, cls).from_crawler(crawler, *args, **kwargs)
            crawler.signals.connect(spider.spider_closed, signal=signals.spider_closed)
            return spider

        def spider_closed(self, spider):
            spider.logger.info("Spider closed: %s", spider.name)

        def parse(self, response):
            pass
:::
:::

::: {#deferred-signal-handlers .section}
[]{#signal-deferred}

#### Deferred signal handlers[¶](#deferred-signal-handlers "Permalink to this heading"){.headerlink}

Some signals support returning [[`Deferred`{.xref .py .py-class
.docutils .literal
.notranslate}]{.pre}](https://docs.twisted.org/en/stable/api/twisted.internet.defer.Deferred.html "(in Twisted)"){.reference
.external} or [[awaitable objects]{.xref .std
.std-term}](https://docs.python.org/3/glossary.html#term-awaitable "(in Python v3.12)"){.reference
.external} from their handlers, allowing you to run asynchronous code
that does not block Scrapy. If a signal handler returns one of these
objects, Scrapy waits for that asynchronous operation to finish.

Let's take an example using [[coroutines]{.std
.std-ref}](index.html#topics-coroutines){.hoverxref .tooltip .reference
.internal}:

::: {.highlight-python .notranslate}
::: highlight
    import scrapy


    class SignalSpider(scrapy.Spider):
        name = "signals"
        start_urls = ["https://quotes.toscrape.com/page/1/"]

        @classmethod
        def from_crawler(cls, crawler, *args, **kwargs):
            spider = super(SignalSpider, cls).from_crawler(crawler, *args, **kwargs)
            crawler.signals.connect(spider.item_scraped, signal=signals.item_scraped)
            return spider

        async def item_scraped(self, item):
            # Send the scraped item to the server
            response = await treq.post(
                "http://example.com/post",
                json.dumps(item).encode("ascii"),
                headers={b"Content-Type": [b"application/json"]},
            )

            return response

        def parse(self, response):
            for quote in response.css("div.quote"):
                yield {
                    "text": quote.css("span.text::text").get(),
                    "author": quote.css("small.author::text").get(),
                    "tags": quote.css("div.tags a.tag::text").getall(),
                }
:::
:::

See the [[Built-in signals reference]{.std
.std-ref}](#topics-signals-ref){.hoverxref .tooltip .reference
.internal} below to know which signals support [[`Deferred`{.xref .py
.py-class .docutils .literal
.notranslate}]{.pre}](https://docs.twisted.org/en/stable/api/twisted.internet.defer.Deferred.html "(in Twisted)"){.reference
.external} and [[awaitable objects]{.xref .std
.std-term}](https://docs.python.org/3/glossary.html#term-awaitable "(in Python v3.12)"){.reference
.external}.
:::

::: {#module-scrapy.signals .section}
[]{#built-in-signals-reference}[]{#topics-signals-ref}

#### Built-in signals reference[¶](#module-scrapy.signals "Permalink to this heading"){.headerlink}

Here's the list of Scrapy built-in signals and their meaning.

::: {#engine-signals .section}
##### Engine signals[¶](#engine-signals "Permalink to this heading"){.headerlink}

::: {#engine-started .section}
###### engine_started[¶](#engine-started "Permalink to this heading"){.headerlink}

[]{#std-signal-engine_started .target}

[[scrapy.signals.]{.pre}]{.sig-prename .descclassname}[[engine_started]{.pre}]{.sig-name .descname}[(]{.sig-paren}[)]{.sig-paren}[¶](#scrapy.signals.engine_started "Permalink to this definition"){.headerlink}

:   Sent when the Scrapy engine has started crawling.

    This signal supports returning deferreds from its handlers.

::: {.admonition .note}
Note

This signal may be fired *after* the [[`spider_opened`{.xref .std
.std-signal .docutils .literal
.notranslate}]{.pre}](#std-signal-spider_opened){.hoverxref .tooltip
.reference .internal} signal, depending on how the spider was started.
So **don't** rely on this signal getting fired before
[[`spider_opened`{.xref .std .std-signal .docutils .literal
.notranslate}]{.pre}](#std-signal-spider_opened){.hoverxref .tooltip
.reference .internal}.
:::
:::

::: {#engine-stopped .section}
###### engine_stopped[¶](#engine-stopped "Permalink to this heading"){.headerlink}

[]{#std-signal-engine_stopped .target}

[[scrapy.signals.]{.pre}]{.sig-prename .descclassname}[[engine_stopped]{.pre}]{.sig-name .descname}[(]{.sig-paren}[)]{.sig-paren}[¶](#scrapy.signals.engine_stopped "Permalink to this definition"){.headerlink}

:   Sent when the Scrapy engine is stopped (for example, when a crawling
    process has finished).

    This signal supports returning deferreds from its handlers.
:::
:::

::: {#item-signals .section}
##### Item signals[¶](#item-signals "Permalink to this heading"){.headerlink}

::: {.admonition .note}
Note

As at max [[`CONCURRENT_ITEMS`{.xref .std .std-setting .docutils
.literal
.notranslate}]{.pre}](index.html#std-setting-CONCURRENT_ITEMS){.hoverxref
.tooltip .reference .internal} items are processed in parallel, many
deferreds are fired together using [[`DeferredList`{.xref .py .py-class
.docutils .literal
.notranslate}]{.pre}](https://docs.twisted.org/en/stable/api/twisted.internet.defer.DeferredList.html "(in Twisted)"){.reference
.external}. Hence the next batch waits for the [[`DeferredList`{.xref
.py .py-class .docutils .literal
.notranslate}]{.pre}](https://docs.twisted.org/en/stable/api/twisted.internet.defer.DeferredList.html "(in Twisted)"){.reference
.external} to fire and then runs the respective item signal handler for
the next batch of scraped items.
:::

::: {#item-scraped .section}
###### item_scraped[¶](#item-scraped "Permalink to this heading"){.headerlink}

[]{#std-signal-item_scraped .target}

[[scrapy.signals.]{.pre}]{.sig-prename .descclassname}[[item_scraped]{.pre}]{.sig-name .descname}[(]{.sig-paren}*[[item]{.pre}]{.n}*, *[[response]{.pre}]{.n}*, *[[spider]{.pre}]{.n}*[)]{.sig-paren}[¶](#scrapy.signals.item_scraped "Permalink to this definition"){.headerlink}

:   Sent when an item has been scraped, after it has passed all the
    [[Item Pipeline]{.std
    .std-ref}](index.html#topics-item-pipeline){.hoverxref .tooltip
    .reference .internal} stages (without being dropped).

    This signal supports returning deferreds from its handlers.

    Parameters

    :   -   **item** ([[item object]{.std
            .std-ref}](index.html#item-types){.hoverxref .tooltip
            .reference .internal}) -- the scraped item

        -   **spider** ([[`Spider`{.xref .py .py-class .docutils
            .literal
            .notranslate}]{.pre}](index.html#scrapy.Spider "scrapy.Spider"){.reference
            .internal} object) -- the spider which scraped the item

        -   **response** ([[`Response`{.xref .py .py-class .docutils
            .literal
            .notranslate}]{.pre}](index.html#scrapy.http.Response "scrapy.http.Response"){.reference
            .internal} object) -- the response from where the item was
            scraped
:::

::: {#item-dropped .section}
###### item_dropped[¶](#item-dropped "Permalink to this heading"){.headerlink}

[]{#std-signal-item_dropped .target}

[[scrapy.signals.]{.pre}]{.sig-prename .descclassname}[[item_dropped]{.pre}]{.sig-name .descname}[(]{.sig-paren}*[[item]{.pre}]{.n}*, *[[response]{.pre}]{.n}*, *[[exception]{.pre}]{.n}*, *[[spider]{.pre}]{.n}*[)]{.sig-paren}[¶](#scrapy.signals.item_dropped "Permalink to this definition"){.headerlink}

:   Sent after an item has been dropped from the [[Item Pipeline]{.std
    .std-ref}](index.html#topics-item-pipeline){.hoverxref .tooltip
    .reference .internal} when some stage raised a [[`DropItem`{.xref
    .py .py-exc .docutils .literal
    .notranslate}]{.pre}](index.html#scrapy.exceptions.DropItem "scrapy.exceptions.DropItem"){.reference
    .internal} exception.

    This signal supports returning deferreds from its handlers.

    Parameters

    :   -   **item** ([[item object]{.std
            .std-ref}](index.html#item-types){.hoverxref .tooltip
            .reference .internal}) -- the item dropped from the [[Item
            Pipeline]{.std
            .std-ref}](index.html#topics-item-pipeline){.hoverxref
            .tooltip .reference .internal}

        -   **spider** ([[`Spider`{.xref .py .py-class .docutils
            .literal
            .notranslate}]{.pre}](index.html#scrapy.Spider "scrapy.Spider"){.reference
            .internal} object) -- the spider which scraped the item

        -   **response** ([[`Response`{.xref .py .py-class .docutils
            .literal
            .notranslate}]{.pre}](index.html#scrapy.http.Response "scrapy.http.Response"){.reference
            .internal} object) -- the response from where the item was
            dropped

        -   **exception** ([[`DropItem`{.xref .py .py-exc .docutils
            .literal
            .notranslate}]{.pre}](index.html#scrapy.exceptions.DropItem "scrapy.exceptions.DropItem"){.reference
            .internal} exception) -- the exception (which must be a
            [[`DropItem`{.xref .py .py-exc .docutils .literal
            .notranslate}]{.pre}](index.html#scrapy.exceptions.DropItem "scrapy.exceptions.DropItem"){.reference
            .internal} subclass) which caused the item to be dropped
:::

::: {#item-error .section}
###### item_error[¶](#item-error "Permalink to this heading"){.headerlink}

[]{#std-signal-item_error .target}

[[scrapy.signals.]{.pre}]{.sig-prename .descclassname}[[item_error]{.pre}]{.sig-name .descname}[(]{.sig-paren}*[[item]{.pre}]{.n}*, *[[response]{.pre}]{.n}*, *[[spider]{.pre}]{.n}*, *[[failure]{.pre}]{.n}*[)]{.sig-paren}[¶](#scrapy.signals.item_error "Permalink to this definition"){.headerlink}

:   Sent when a [[Item Pipeline]{.std
    .std-ref}](index.html#topics-item-pipeline){.hoverxref .tooltip
    .reference .internal} generates an error (i.e. raises an exception),
    except [[`DropItem`{.xref .py .py-exc .docutils .literal
    .notranslate}]{.pre}](index.html#scrapy.exceptions.DropItem "scrapy.exceptions.DropItem"){.reference
    .internal} exception.

    This signal supports returning deferreds from its handlers.

    Parameters

    :   -   **item** ([[item object]{.std
            .std-ref}](index.html#item-types){.hoverxref .tooltip
            .reference .internal}) -- the item that caused the error in
            the [[Item Pipeline]{.std
            .std-ref}](index.html#topics-item-pipeline){.hoverxref
            .tooltip .reference .internal}

        -   **response** ([[`Response`{.xref .py .py-class .docutils
            .literal
            .notranslate}]{.pre}](index.html#scrapy.http.Response "scrapy.http.Response"){.reference
            .internal} object) -- the response being processed when the
            exception was raised

        -   **spider** ([[`Spider`{.xref .py .py-class .docutils
            .literal
            .notranslate}]{.pre}](index.html#scrapy.Spider "scrapy.Spider"){.reference
            .internal} object) -- the spider which raised the exception

        -   **failure**
            ([*twisted.python.failure.Failure*](https://docs.twisted.org/en/stable/api/twisted.python.failure.Failure.html "(in Twisted)"){.reference
            .external}) -- the exception raised
:::
:::

::: {#spider-signals .section}
##### Spider signals[¶](#spider-signals "Permalink to this heading"){.headerlink}

::: {#spider-closed .section}
###### spider_closed[¶](#spider-closed "Permalink to this heading"){.headerlink}

[]{#std-signal-spider_closed .target}

[[scrapy.signals.]{.pre}]{.sig-prename .descclassname}[[spider_closed]{.pre}]{.sig-name .descname}[(]{.sig-paren}*[[spider]{.pre}]{.n}*, *[[reason]{.pre}]{.n}*[)]{.sig-paren}[¶](#scrapy.signals.spider_closed "Permalink to this definition"){.headerlink}

:   Sent after a spider has been closed. This can be used to release
    per-spider resources reserved on [[`spider_opened`{.xref .std
    .std-signal .docutils .literal
    .notranslate}]{.pre}](#std-signal-spider_opened){.hoverxref .tooltip
    .reference .internal}.

    This signal supports returning deferreds from its handlers.

    Parameters

    :   -   **spider** ([[`Spider`{.xref .py .py-class .docutils
            .literal
            .notranslate}]{.pre}](index.html#scrapy.Spider "scrapy.Spider"){.reference
            .internal} object) -- the spider which has been closed

        -   **reason**
            ([*str*](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.12)"){.reference
            .external}) -- a string which describes the reason why the
            spider was closed. If it was closed because the spider has
            completed scraping, the reason is [`'finished'`{.docutils
            .literal .notranslate}]{.pre}. Otherwise, if the spider was
            manually closed by calling the [`close_spider`{.docutils
            .literal .notranslate}]{.pre} engine method, then the reason
            is the one passed in the [`reason`{.docutils .literal
            .notranslate}]{.pre} argument of that method (which defaults
            to [`'cancelled'`{.docutils .literal .notranslate}]{.pre}).
            If the engine was shutdown (for example, by hitting Ctrl-C
            to stop it) the reason will be [`'shutdown'`{.docutils
            .literal .notranslate}]{.pre}.
:::

::: {#spider-opened .section}
###### spider_opened[¶](#spider-opened "Permalink to this heading"){.headerlink}

[]{#std-signal-spider_opened .target}

[[scrapy.signals.]{.pre}]{.sig-prename .descclassname}[[spider_opened]{.pre}]{.sig-name .descname}[(]{.sig-paren}*[[spider]{.pre}]{.n}*[)]{.sig-paren}[¶](#scrapy.signals.spider_opened "Permalink to this definition"){.headerlink}

:   Sent after a spider has been opened for crawling. This is typically
    used to reserve per-spider resources, but can be used for any task
    that needs to be performed when a spider is opened.

    This signal supports returning deferreds from its handlers.

    Parameters

    :   **spider** ([[`Spider`{.xref .py .py-class .docutils .literal
        .notranslate}]{.pre}](index.html#scrapy.Spider "scrapy.Spider"){.reference
        .internal} object) -- the spider which has been opened
:::

::: {#spider-idle .section}
###### spider_idle[¶](#spider-idle "Permalink to this heading"){.headerlink}

[]{#std-signal-spider_idle .target}

[[scrapy.signals.]{.pre}]{.sig-prename .descclassname}[[spider_idle]{.pre}]{.sig-name .descname}[(]{.sig-paren}*[[spider]{.pre}]{.n}*[)]{.sig-paren}[¶](#scrapy.signals.spider_idle "Permalink to this definition"){.headerlink}

:   Sent when a spider has gone idle, which means the spider has no
    further:

    > <div>
    >
    > -   requests waiting to be downloaded
    >
    > -   requests scheduled
    >
    > -   items being processed in the item pipeline
    >
    > </div>

    If the idle state persists after all handlers of this signal have
    finished, the engine starts closing the spider. After the spider has
    finished closing, the [[`spider_closed`{.xref .std .std-signal
    .docutils .literal
    .notranslate}]{.pre}](#std-signal-spider_closed){.hoverxref .tooltip
    .reference .internal} signal is sent.

    You may raise a [[`DontCloseSpider`{.xref .py .py-exc .docutils
    .literal
    .notranslate}]{.pre}](index.html#scrapy.exceptions.DontCloseSpider "scrapy.exceptions.DontCloseSpider"){.reference
    .internal} exception to prevent the spider from being closed.

    Alternatively, you may raise a [[`CloseSpider`{.xref .py .py-exc
    .docutils .literal
    .notranslate}]{.pre}](index.html#scrapy.exceptions.CloseSpider "scrapy.exceptions.CloseSpider"){.reference
    .internal} exception to provide a custom spider closing reason. An
    idle handler is the perfect place to put some code that assesses the
    final spider results and update the final closing reason accordingly
    (e.g. setting it to 'too_few_results' instead of 'finished').

    This signal does not support returning deferreds from its handlers.

    Parameters

    :   **spider** ([[`Spider`{.xref .py .py-class .docutils .literal
        .notranslate}]{.pre}](index.html#scrapy.Spider "scrapy.Spider"){.reference
        .internal} object) -- the spider which has gone idle

::: {.admonition .note}
Note

Scheduling some requests in your [[`spider_idle`{.xref .std .std-signal
.docutils .literal
.notranslate}]{.pre}](#std-signal-spider_idle){.hoverxref .tooltip
.reference .internal} handler does **not** guarantee that it can prevent
the spider from being closed, although it sometimes can. That's because
the spider may still remain idle if all the scheduled requests are
rejected by the scheduler (e.g. filtered due to duplication).
:::
:::

::: {#spider-error .section}
###### spider_error[¶](#spider-error "Permalink to this heading"){.headerlink}

[]{#std-signal-spider_error .target}

[[scrapy.signals.]{.pre}]{.sig-prename .descclassname}[[spider_error]{.pre}]{.sig-name .descname}[(]{.sig-paren}*[[failure]{.pre}]{.n}*, *[[response]{.pre}]{.n}*, *[[spider]{.pre}]{.n}*[)]{.sig-paren}[¶](#scrapy.signals.spider_error "Permalink to this definition"){.headerlink}

:   Sent when a spider callback generates an error (i.e. raises an
    exception).

    This signal does not support returning deferreds from its handlers.

    Parameters

    :   -   **failure**
            ([*twisted.python.failure.Failure*](https://docs.twisted.org/en/stable/api/twisted.python.failure.Failure.html "(in Twisted)"){.reference
            .external}) -- the exception raised

        -   **response** ([[`Response`{.xref .py .py-class .docutils
            .literal
            .notranslate}]{.pre}](index.html#scrapy.http.Response "scrapy.http.Response"){.reference
            .internal} object) -- the response being processed when the
            exception was raised

        -   **spider** ([[`Spider`{.xref .py .py-class .docutils
            .literal
            .notranslate}]{.pre}](index.html#scrapy.Spider "scrapy.Spider"){.reference
            .internal} object) -- the spider which raised the exception
:::

::: {#feed-slot-closed .section}
###### feed_slot_closed[¶](#feed-slot-closed "Permalink to this heading"){.headerlink}

[]{#std-signal-feed_slot_closed .target}

[[scrapy.signals.]{.pre}]{.sig-prename .descclassname}[[feed_slot_closed]{.pre}]{.sig-name .descname}[(]{.sig-paren}*[[slot]{.pre}]{.n}*[)]{.sig-paren}[¶](#scrapy.signals.feed_slot_closed "Permalink to this definition"){.headerlink}

:   Sent when a [[feed exports]{.std
    .std-ref}](index.html#topics-feed-exports){.hoverxref .tooltip
    .reference .internal} slot is closed.

    This signal supports returning deferreds from its handlers.

    Parameters

    :   **slot** (*scrapy.extensions.feedexport.FeedSlot*) -- the slot
        closed
:::

::: {#feed-exporter-closed .section}
###### feed_exporter_closed[¶](#feed-exporter-closed "Permalink to this heading"){.headerlink}

[]{#std-signal-feed_exporter_closed .target}

[[scrapy.signals.]{.pre}]{.sig-prename .descclassname}[[feed_exporter_closed]{.pre}]{.sig-name .descname}[(]{.sig-paren}[)]{.sig-paren}[¶](#scrapy.signals.feed_exporter_closed "Permalink to this definition"){.headerlink}

:   Sent when the [[feed exports]{.std
    .std-ref}](index.html#topics-feed-exports){.hoverxref .tooltip
    .reference .internal} extension is closed, during the handling of
    the [[`spider_closed`{.xref .std .std-signal .docutils .literal
    .notranslate}]{.pre}](#std-signal-spider_closed){.hoverxref .tooltip
    .reference .internal} signal by the extension, after all feed
    exporting has been handled.

    This signal supports returning deferreds from its handlers.
:::
:::

::: {#request-signals .section}
##### Request signals[¶](#request-signals "Permalink to this heading"){.headerlink}

::: {#request-scheduled .section}
###### request_scheduled[¶](#request-scheduled "Permalink to this heading"){.headerlink}

[]{#std-signal-request_scheduled .target}

[[scrapy.signals.]{.pre}]{.sig-prename .descclassname}[[request_scheduled]{.pre}]{.sig-name .descname}[(]{.sig-paren}*[[request]{.pre}]{.n}*, *[[spider]{.pre}]{.n}*[)]{.sig-paren}[¶](#scrapy.signals.request_scheduled "Permalink to this definition"){.headerlink}

:   Sent when the engine schedules a [`Request`{.xref .py .py-class
    .docutils .literal .notranslate}]{.pre}, to be downloaded later.

    This signal does not support returning deferreds from its handlers.

    Parameters

    :   -   **request** ([`Request`{.xref .py .py-class .docutils
            .literal .notranslate}]{.pre} object) -- the request that
            reached the scheduler

        -   **spider** ([[`Spider`{.xref .py .py-class .docutils
            .literal
            .notranslate}]{.pre}](index.html#scrapy.Spider "scrapy.Spider"){.reference
            .internal} object) -- the spider that yielded the request
:::

::: {#request-dropped .section}
###### request_dropped[¶](#request-dropped "Permalink to this heading"){.headerlink}

[]{#std-signal-request_dropped .target}

[[scrapy.signals.]{.pre}]{.sig-prename .descclassname}[[request_dropped]{.pre}]{.sig-name .descname}[(]{.sig-paren}*[[request]{.pre}]{.n}*, *[[spider]{.pre}]{.n}*[)]{.sig-paren}[¶](#scrapy.signals.request_dropped "Permalink to this definition"){.headerlink}

:   Sent when a [`Request`{.xref .py .py-class .docutils .literal
    .notranslate}]{.pre}, scheduled by the engine to be downloaded
    later, is rejected by the scheduler.

    This signal does not support returning deferreds from its handlers.

    Parameters

    :   -   **request** ([`Request`{.xref .py .py-class .docutils
            .literal .notranslate}]{.pre} object) -- the request that
            reached the scheduler

        -   **spider** ([[`Spider`{.xref .py .py-class .docutils
            .literal
            .notranslate}]{.pre}](index.html#scrapy.Spider "scrapy.Spider"){.reference
            .internal} object) -- the spider that yielded the request
:::

::: {#request-reached-downloader .section}
###### request_reached_downloader[¶](#request-reached-downloader "Permalink to this heading"){.headerlink}

[]{#std-signal-request_reached_downloader .target}

[[scrapy.signals.]{.pre}]{.sig-prename .descclassname}[[request_reached_downloader]{.pre}]{.sig-name .descname}[(]{.sig-paren}*[[request]{.pre}]{.n}*, *[[spider]{.pre}]{.n}*[)]{.sig-paren}[¶](#scrapy.signals.request_reached_downloader "Permalink to this definition"){.headerlink}

:   Sent when a [`Request`{.xref .py .py-class .docutils .literal
    .notranslate}]{.pre} reached downloader.

    This signal does not support returning deferreds from its handlers.

    Parameters

    :   -   **request** ([`Request`{.xref .py .py-class .docutils
            .literal .notranslate}]{.pre} object) -- the request that
            reached downloader

        -   **spider** ([[`Spider`{.xref .py .py-class .docutils
            .literal
            .notranslate}]{.pre}](index.html#scrapy.Spider "scrapy.Spider"){.reference
            .internal} object) -- the spider that yielded the request
:::

::: {#request-left-downloader .section}
###### request_left_downloader[¶](#request-left-downloader "Permalink to this heading"){.headerlink}

[]{#std-signal-request_left_downloader .target}

[[scrapy.signals.]{.pre}]{.sig-prename .descclassname}[[request_left_downloader]{.pre}]{.sig-name .descname}[(]{.sig-paren}*[[request]{.pre}]{.n}*, *[[spider]{.pre}]{.n}*[)]{.sig-paren}[¶](#scrapy.signals.request_left_downloader "Permalink to this definition"){.headerlink}

:   ::: versionadded
    [New in version 2.0.]{.versionmodified .added}
    :::

    Sent when a [`Request`{.xref .py .py-class .docutils .literal
    .notranslate}]{.pre} leaves the downloader, even in case of failure.

    This signal does not support returning deferreds from its handlers.

    Parameters

    :   -   **request** ([`Request`{.xref .py .py-class .docutils
            .literal .notranslate}]{.pre} object) -- the request that
            reached the downloader

        -   **spider** ([[`Spider`{.xref .py .py-class .docutils
            .literal
            .notranslate}]{.pre}](index.html#scrapy.Spider "scrapy.Spider"){.reference
            .internal} object) -- the spider that yielded the request
:::

::: {#bytes-received .section}
###### bytes_received[¶](#bytes-received "Permalink to this heading"){.headerlink}

::: versionadded
[New in version 2.2.]{.versionmodified .added}
:::

[]{#std-signal-bytes_received .target}

[[scrapy.signals.]{.pre}]{.sig-prename .descclassname}[[bytes_received]{.pre}]{.sig-name .descname}[(]{.sig-paren}*[[data]{.pre}]{.n}*, *[[request]{.pre}]{.n}*, *[[spider]{.pre}]{.n}*[)]{.sig-paren}[¶](#scrapy.signals.bytes_received "Permalink to this definition"){.headerlink}

:   Sent by the HTTP 1.1 and S3 download handlers when a group of bytes
    is received for a specific request. This signal might be fired
    multiple times for the same request, with partial data each time.
    For instance, a possible scenario for a 25 kb response would be two
    signals fired with 10 kb of data, and a final one with 5 kb of data.

    Handlers for this signal can stop the download of a response while
    it is in progress by raising the [[`StopDownload`{.xref .py .py-exc
    .docutils .literal
    .notranslate}]{.pre}](index.html#scrapy.exceptions.StopDownload "scrapy.exceptions.StopDownload"){.reference
    .internal} exception. Please refer to the [[Stopping the download of
    a Response]{.std
    .std-ref}](index.html#topics-stop-response-download){.hoverxref
    .tooltip .reference .internal} topic for additional information and
    examples.

    This signal does not support returning deferreds from its handlers.

    Parameters

    :   -   **data** ([[`bytes`{.xref .py .py-class .docutils .literal
            .notranslate}]{.pre}](https://docs.python.org/3/library/stdtypes.html#bytes "(in Python v3.12)"){.reference
            .external} object) -- the data received by the download
            handler

        -   **request** ([`Request`{.xref .py .py-class .docutils
            .literal .notranslate}]{.pre} object) -- the request that
            generated the download

        -   **spider** ([[`Spider`{.xref .py .py-class .docutils
            .literal
            .notranslate}]{.pre}](index.html#scrapy.Spider "scrapy.Spider"){.reference
            .internal} object) -- the spider associated with the
            response
:::

::: {#headers-received .section}
###### headers_received[¶](#headers-received "Permalink to this heading"){.headerlink}

::: versionadded
[New in version 2.5.]{.versionmodified .added}
:::

[]{#std-signal-headers_received .target}

[[scrapy.signals.]{.pre}]{.sig-prename .descclassname}[[headers_received]{.pre}]{.sig-name .descname}[(]{.sig-paren}*[[headers]{.pre}]{.n}*, *[[body_length]{.pre}]{.n}*, *[[request]{.pre}]{.n}*, *[[spider]{.pre}]{.n}*[)]{.sig-paren}[¶](#scrapy.signals.headers_received "Permalink to this definition"){.headerlink}

:   Sent by the HTTP 1.1 and S3 download handlers when the response
    headers are available for a given request, before downloading any
    additional content.

    Handlers for this signal can stop the download of a response while
    it is in progress by raising the [[`StopDownload`{.xref .py .py-exc
    .docutils .literal
    .notranslate}]{.pre}](index.html#scrapy.exceptions.StopDownload "scrapy.exceptions.StopDownload"){.reference
    .internal} exception. Please refer to the [[Stopping the download of
    a Response]{.std
    .std-ref}](index.html#topics-stop-response-download){.hoverxref
    .tooltip .reference .internal} topic for additional information and
    examples.

    This signal does not support returning deferreds from its handlers.

    Parameters

    :   -   **headers** ([`scrapy.http.headers.Headers`{.xref .py
            .py-class .docutils .literal .notranslate}]{.pre} object) --
            the headers received by the download handler

        -   **body_length** (int) -- expected size of the response body,
            in bytes

        -   **request** ([`Request`{.xref .py .py-class .docutils
            .literal .notranslate}]{.pre} object) -- the request that
            generated the download

        -   **spider** ([[`Spider`{.xref .py .py-class .docutils
            .literal
            .notranslate}]{.pre}](index.html#scrapy.Spider "scrapy.Spider"){.reference
            .internal} object) -- the spider associated with the
            response
:::
:::

::: {#response-signals .section}
##### Response signals[¶](#response-signals "Permalink to this heading"){.headerlink}

::: {#response-received .section}
###### response_received[¶](#response-received "Permalink to this heading"){.headerlink}

[]{#std-signal-response_received .target}

[[scrapy.signals.]{.pre}]{.sig-prename .descclassname}[[response_received]{.pre}]{.sig-name .descname}[(]{.sig-paren}*[[response]{.pre}]{.n}*, *[[request]{.pre}]{.n}*, *[[spider]{.pre}]{.n}*[)]{.sig-paren}[¶](#scrapy.signals.response_received "Permalink to this definition"){.headerlink}

:   Sent when the engine receives a new [[`Response`{.xref .py .py-class
    .docutils .literal
    .notranslate}]{.pre}](index.html#scrapy.http.Response "scrapy.http.Response"){.reference
    .internal} from the downloader.

    This signal does not support returning deferreds from its handlers.

    Parameters

    :   -   **response** ([[`Response`{.xref .py .py-class .docutils
            .literal
            .notranslate}]{.pre}](index.html#scrapy.http.Response "scrapy.http.Response"){.reference
            .internal} object) -- the response received

        -   **request** ([`Request`{.xref .py .py-class .docutils
            .literal .notranslate}]{.pre} object) -- the request that
            generated the response

        -   **spider** ([[`Spider`{.xref .py .py-class .docutils
            .literal
            .notranslate}]{.pre}](index.html#scrapy.Spider "scrapy.Spider"){.reference
            .internal} object) -- the spider for which the response is
            intended

::: {.admonition .note}
Note

The [`request`{.docutils .literal .notranslate}]{.pre} argument might
not contain the original request that reached the downloader, if a
[[Downloader Middleware]{.std
.std-ref}](index.html#topics-downloader-middleware){.hoverxref .tooltip
.reference .internal} modifies the [[`Response`{.xref .py .py-class
.docutils .literal
.notranslate}]{.pre}](index.html#scrapy.http.Response "scrapy.http.Response"){.reference
.internal} object and sets a specific [`request`{.docutils .literal
.notranslate}]{.pre} attribute.
:::
:::

::: {#response-downloaded .section}
###### response_downloaded[¶](#response-downloaded "Permalink to this heading"){.headerlink}

[]{#std-signal-response_downloaded .target}

[[scrapy.signals.]{.pre}]{.sig-prename .descclassname}[[response_downloaded]{.pre}]{.sig-name .descname}[(]{.sig-paren}*[[response]{.pre}]{.n}*, *[[request]{.pre}]{.n}*, *[[spider]{.pre}]{.n}*[)]{.sig-paren}[¶](#scrapy.signals.response_downloaded "Permalink to this definition"){.headerlink}

:   Sent by the downloader right after a [`HTTPResponse`{.docutils
    .literal .notranslate}]{.pre} is downloaded.

    This signal does not support returning deferreds from its handlers.

    Parameters

    :   -   **response** ([[`Response`{.xref .py .py-class .docutils
            .literal
            .notranslate}]{.pre}](index.html#scrapy.http.Response "scrapy.http.Response"){.reference
            .internal} object) -- the response downloaded

        -   **request** ([`Request`{.xref .py .py-class .docutils
            .literal .notranslate}]{.pre} object) -- the request that
            generated the response

        -   **spider** ([[`Spider`{.xref .py .py-class .docutils
            .literal
            .notranslate}]{.pre}](index.html#scrapy.Spider "scrapy.Spider"){.reference
            .internal} object) -- the spider for which the response is
            intended
:::
:::
:::
:::

[]{#document-topics/scheduler}

::: {#module-scrapy.core.scheduler .section}
[]{#scheduler}[]{#topics-scheduler}

### Scheduler[¶](#module-scrapy.core.scheduler "Permalink to this heading"){.headerlink}

The scheduler component receives requests from the [[engine]{.std
.std-ref}](index.html#component-engine){.hoverxref .tooltip .reference
.internal} and stores them into persistent and/or non-persistent data
structures. It also gets those requests and feeds them back to the
engine when it asks for a next request to be downloaded.

::: {#overriding-the-default-scheduler .section}
#### Overriding the default scheduler[¶](#overriding-the-default-scheduler "Permalink to this heading"){.headerlink}

You can use your own custom scheduler class by supplying its full Python
path in the [[`SCHEDULER`{.xref .std .std-setting .docutils .literal
.notranslate}]{.pre}](index.html#std-setting-SCHEDULER){.hoverxref
.tooltip .reference .internal} setting.
:::

::: {#minimal-scheduler-interface .section}
#### Minimal scheduler interface[¶](#minimal-scheduler-interface "Permalink to this heading"){.headerlink}

*[class]{.pre}[ ]{.w}*[[scrapy.core.scheduler.]{.pre}]{.sig-prename .descclassname}[[BaseScheduler]{.pre}]{.sig-name .descname}[[[\[source\]]{.pre}]{.viewcode-link}](_modules/scrapy/core/scheduler.html#BaseScheduler){.reference .internal}[¶](#scrapy.core.scheduler.BaseScheduler "Permalink to this definition"){.headerlink}

:   The scheduler component is responsible for storing requests received
    from the engine, and feeding them back upon request (also to the
    engine).

    The original sources of said requests are:

    -   Spider: [`start_requests`{.docutils .literal
        .notranslate}]{.pre} method, requests created for URLs in the
        [`start_urls`{.docutils .literal .notranslate}]{.pre} attribute,
        request callbacks

    -   Spider middleware: [`process_spider_output`{.docutils .literal
        .notranslate}]{.pre} and [`process_spider_exception`{.docutils
        .literal .notranslate}]{.pre} methods

    -   Downloader middleware: [`process_request`{.docutils .literal
        .notranslate}]{.pre}, [`process_response`{.docutils .literal
        .notranslate}]{.pre} and [`process_exception`{.docutils .literal
        .notranslate}]{.pre} methods

    The order in which the scheduler returns its stored requests (via
    the [`next_request`{.docutils .literal .notranslate}]{.pre} method)
    plays a great part in determining the order in which those requests
    are downloaded.

    The methods defined in this class constitute the minimal interface
    that the Scrapy engine will interact with.

    [[close]{.pre}]{.sig-name .descname}[(]{.sig-paren}*[[reason]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[[[str]{.pre}](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.12)"){.reference .external}]{.n}*[)]{.sig-paren} [[→]{.sig-return-icon} [[[Optional]{.pre}](https://docs.python.org/3/library/typing.html#typing.Optional "(in Python v3.12)"){.reference .external}[[\[]{.pre}]{.p}[[Deferred]{.pre}](https://docs.twisted.org/en/stable/api/twisted.internet.defer.Deferred.html "(in Twisted)"){.reference .external}[[\]]{.pre}]{.p}]{.sig-return-typehint}]{.sig-return}[[[\[source\]]{.pre}]{.viewcode-link}](_modules/scrapy/core/scheduler.html#BaseScheduler.close){.reference .internal}[¶](#scrapy.core.scheduler.BaseScheduler.close "Permalink to this definition"){.headerlink}

    :   Called when the spider is closed by the engine. It receives the
        reason why the crawl finished as argument and it's useful to
        execute cleaning code.

        Parameters

        :   **reason** ([[`str`{.xref .py .py-class .docutils .literal
            .notranslate}]{.pre}](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.12)"){.reference
            .external}) -- a string which describes the reason why the
            spider was closed

    *[abstract]{.pre}[ ]{.w}*[[enqueue_request]{.pre}]{.sig-name .descname}[(]{.sig-paren}*[[request]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[[[Request]{.pre}](index.html#scrapy.http.Request "scrapy.http.request.Request"){.reference .internal}]{.n}*[)]{.sig-paren} [[→]{.sig-return-icon} [[[bool]{.pre}](https://docs.python.org/3/library/functions.html#bool "(in Python v3.12)"){.reference .external}]{.sig-return-typehint}]{.sig-return}[[[\[source\]]{.pre}]{.viewcode-link}](_modules/scrapy/core/scheduler.html#BaseScheduler.enqueue_request){.reference .internal}[¶](#scrapy.core.scheduler.BaseScheduler.enqueue_request "Permalink to this definition"){.headerlink}

    :   Process a request received by the engine.

        Return [`True`{.docutils .literal .notranslate}]{.pre} if the
        request is stored correctly, [`False`{.docutils .literal
        .notranslate}]{.pre} otherwise.

        If [`False`{.docutils .literal .notranslate}]{.pre}, the engine
        will fire a [`request_dropped`{.docutils .literal
        .notranslate}]{.pre} signal, and will not make further attempts
        to schedule the request at a later time. For reference, the
        default Scrapy scheduler returns [`False`{.docutils .literal
        .notranslate}]{.pre} when the request is rejected by the
        dupefilter.

    *[classmethod]{.pre}[ ]{.w}*[[from_crawler]{.pre}]{.sig-name .descname}[(]{.sig-paren}*[[crawler]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[[[Crawler]{.pre}](index.html#scrapy.crawler.Crawler "scrapy.crawler.Crawler"){.reference .internal}]{.n}*[)]{.sig-paren} [[→]{.sig-return-icon} [[Self]{.pre}]{.sig-return-typehint}]{.sig-return}[[[\[source\]]{.pre}]{.viewcode-link}](_modules/scrapy/core/scheduler.html#BaseScheduler.from_crawler){.reference .internal}[¶](#scrapy.core.scheduler.BaseScheduler.from_crawler "Permalink to this definition"){.headerlink}

    :   Factory method which receives the current [[`Crawler`{.xref .py
        .py-class .docutils .literal
        .notranslate}]{.pre}](index.html#scrapy.crawler.Crawler "scrapy.crawler.Crawler"){.reference
        .internal} object as argument.

    *[abstract]{.pre}[ ]{.w}*[[has_pending_requests]{.pre}]{.sig-name .descname}[(]{.sig-paren}[)]{.sig-paren} [[→]{.sig-return-icon} [[[bool]{.pre}](https://docs.python.org/3/library/functions.html#bool "(in Python v3.12)"){.reference .external}]{.sig-return-typehint}]{.sig-return}[[[\[source\]]{.pre}]{.viewcode-link}](_modules/scrapy/core/scheduler.html#BaseScheduler.has_pending_requests){.reference .internal}[¶](#scrapy.core.scheduler.BaseScheduler.has_pending_requests "Permalink to this definition"){.headerlink}

    :   [`True`{.docutils .literal .notranslate}]{.pre} if the scheduler
        has enqueued requests, [`False`{.docutils .literal
        .notranslate}]{.pre} otherwise

    *[abstract]{.pre}[ ]{.w}*[[next_request]{.pre}]{.sig-name .descname}[(]{.sig-paren}[)]{.sig-paren} [[→]{.sig-return-icon} [[[Optional]{.pre}](https://docs.python.org/3/library/typing.html#typing.Optional "(in Python v3.12)"){.reference .external}[[\[]{.pre}]{.p}[[Request]{.pre}](index.html#scrapy.http.Request "scrapy.http.request.Request"){.reference .internal}[[\]]{.pre}]{.p}]{.sig-return-typehint}]{.sig-return}[[[\[source\]]{.pre}]{.viewcode-link}](_modules/scrapy/core/scheduler.html#BaseScheduler.next_request){.reference .internal}[¶](#scrapy.core.scheduler.BaseScheduler.next_request "Permalink to this definition"){.headerlink}

    :   Return the next [[`Request`{.xref .py .py-class .docutils
        .literal
        .notranslate}]{.pre}](index.html#scrapy.http.Request "scrapy.http.Request"){.reference
        .internal} to be processed, or [`None`{.docutils .literal
        .notranslate}]{.pre} to indicate that there are no requests to
        be considered ready at the moment.

        Returning [`None`{.docutils .literal .notranslate}]{.pre}
        implies that no request from the scheduler will be sent to the
        downloader in the current reactor cycle. The engine will
        continue calling [`next_request`{.docutils .literal
        .notranslate}]{.pre} until [`has_pending_requests`{.docutils
        .literal .notranslate}]{.pre} is [`False`{.docutils .literal
        .notranslate}]{.pre}.

    [[open]{.pre}]{.sig-name .descname}[(]{.sig-paren}*[[spider]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[[[Spider]{.pre}](index.html#scrapy.spiders.Spider "scrapy.spiders.Spider"){.reference .internal}]{.n}*[)]{.sig-paren} [[→]{.sig-return-icon} [[[Optional]{.pre}](https://docs.python.org/3/library/typing.html#typing.Optional "(in Python v3.12)"){.reference .external}[[\[]{.pre}]{.p}[[Deferred]{.pre}](https://docs.twisted.org/en/stable/api/twisted.internet.defer.Deferred.html "(in Twisted)"){.reference .external}[[\]]{.pre}]{.p}]{.sig-return-typehint}]{.sig-return}[[[\[source\]]{.pre}]{.viewcode-link}](_modules/scrapy/core/scheduler.html#BaseScheduler.open){.reference .internal}[¶](#scrapy.core.scheduler.BaseScheduler.open "Permalink to this definition"){.headerlink}

    :   Called when the spider is opened by the engine. It receives the
        spider instance as argument and it's useful to execute
        initialization code.

        Parameters

        :   **spider** ([[`Spider`{.xref .py .py-class .docutils
            .literal
            .notranslate}]{.pre}](index.html#scrapy.spiders.Spider "scrapy.spiders.Spider"){.reference
            .internal}) -- the spider object for the current crawl
:::

::: {#default-scrapy-scheduler .section}
#### Default Scrapy scheduler[¶](#default-scrapy-scheduler "Permalink to this heading"){.headerlink}

*[class]{.pre}[ ]{.w}*[[scrapy.core.scheduler.]{.pre}]{.sig-prename .descclassname}[[Scheduler]{.pre}]{.sig-name .descname}[(]{.sig-paren}*[[dupefilter]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[[BaseDupeFilter]{.pre}]{.n}*, *[[jobdir]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[[[Optional]{.pre}](https://docs.python.org/3/library/typing.html#typing.Optional "(in Python v3.12)"){.reference .external}[[\[]{.pre}]{.p}[[str]{.pre}](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.12)"){.reference .external}[[\]]{.pre}]{.p}]{.n}[ ]{.w}[[=]{.pre}]{.o}[ ]{.w}[[None]{.pre}]{.default_value}*, *[[dqclass]{.pre}]{.n}[[=]{.pre}]{.o}[[None]{.pre}]{.default_value}*, *[[mqclass]{.pre}]{.n}[[=]{.pre}]{.o}[[None]{.pre}]{.default_value}*, *[[logunser]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[[[bool]{.pre}](https://docs.python.org/3/library/functions.html#bool "(in Python v3.12)"){.reference .external}]{.n}[ ]{.w}[[=]{.pre}]{.o}[ ]{.w}[[False]{.pre}]{.default_value}*, *[[stats]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[[[Optional]{.pre}](https://docs.python.org/3/library/typing.html#typing.Optional "(in Python v3.12)"){.reference .external}[[\[]{.pre}]{.p}[[StatsCollector]{.pre}](index.html#scrapy.statscollectors.StatsCollector "scrapy.statscollectors.StatsCollector"){.reference .internal}[[\]]{.pre}]{.p}]{.n}[ ]{.w}[[=]{.pre}]{.o}[ ]{.w}[[None]{.pre}]{.default_value}*, *[[pqclass]{.pre}]{.n}[[=]{.pre}]{.o}[[None]{.pre}]{.default_value}*, *[[crawler]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[[[Optional]{.pre}](https://docs.python.org/3/library/typing.html#typing.Optional "(in Python v3.12)"){.reference .external}[[\[]{.pre}]{.p}[[Crawler]{.pre}](index.html#scrapy.crawler.Crawler "scrapy.crawler.Crawler"){.reference .internal}[[\]]{.pre}]{.p}]{.n}[ ]{.w}[[=]{.pre}]{.o}[ ]{.w}[[None]{.pre}]{.default_value}*[)]{.sig-paren}[[[\[source\]]{.pre}]{.viewcode-link}](_modules/scrapy/core/scheduler.html#Scheduler){.reference .internal}[¶](#scrapy.core.scheduler.Scheduler "Permalink to this definition"){.headerlink}

:   Default Scrapy scheduler. This implementation also handles
    duplication filtering via the [[`dupefilter`{.xref .std .std-setting
    .docutils .literal
    .notranslate}]{.pre}](index.html#std-setting-DUPEFILTER_CLASS){.hoverxref
    .tooltip .reference .internal}.

    This scheduler stores requests into several priority queues (defined
    by the [[`SCHEDULER_PRIORITY_QUEUE`{.xref .std .std-setting
    .docutils .literal
    .notranslate}]{.pre}](index.html#std-setting-SCHEDULER_PRIORITY_QUEUE){.hoverxref
    .tooltip .reference .internal} setting). In turn, said priority
    queues are backed by either memory or disk based queues
    (respectively defined by the [[`SCHEDULER_MEMORY_QUEUE`{.xref .std
    .std-setting .docutils .literal
    .notranslate}]{.pre}](index.html#std-setting-SCHEDULER_MEMORY_QUEUE){.hoverxref
    .tooltip .reference .internal} and [[`SCHEDULER_DISK_QUEUE`{.xref
    .std .std-setting .docutils .literal
    .notranslate}]{.pre}](index.html#std-setting-SCHEDULER_DISK_QUEUE){.hoverxref
    .tooltip .reference .internal} settings).

    Request prioritization is almost entirely delegated to the priority
    queue. The only prioritization performed by this scheduler is using
    the disk-based queue if present (i.e. if the [[`JOBDIR`{.xref .std
    .std-setting .docutils .literal
    .notranslate}]{.pre}](index.html#std-setting-JOBDIR){.hoverxref
    .tooltip .reference .internal} setting is defined) and falling back
    to the memory-based queue if a serialization error occurs. If the
    disk queue is not present, the memory one is used directly.

    Parameters

    :   -   **dupefilter** ([`scrapy.dupefilters.BaseDupeFilter`{.xref
            .py .py-class .docutils .literal .notranslate}]{.pre}
            instance or similar: any class that implements the
            BaseDupeFilter interface) -- An object responsible for
            checking and filtering duplicate requests. The value for the
            [[`DUPEFILTER_CLASS`{.xref .std .std-setting .docutils
            .literal
            .notranslate}]{.pre}](index.html#std-setting-DUPEFILTER_CLASS){.hoverxref
            .tooltip .reference .internal} setting is used by default.

        -   **jobdir** ([[`str`{.xref .py .py-class .docutils .literal
            .notranslate}]{.pre}](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.12)"){.reference
            .external} or [`None`{.docutils .literal
            .notranslate}]{.pre}) -- The path of a directory to be used
            for persisting the crawl's state. The value for the
            [[`JOBDIR`{.xref .std .std-setting .docutils .literal
            .notranslate}]{.pre}](index.html#std-setting-JOBDIR){.hoverxref
            .tooltip .reference .internal} setting is used by default.
            See [[Jobs: pausing and resuming crawls]{.std
            .std-ref}](index.html#topics-jobs){.hoverxref .tooltip
            .reference .internal}.

        -   **dqclass** (*class*) -- A class to be used as persistent
            request queue. The value for the
            [[`SCHEDULER_DISK_QUEUE`{.xref .std .std-setting .docutils
            .literal
            .notranslate}]{.pre}](index.html#std-setting-SCHEDULER_DISK_QUEUE){.hoverxref
            .tooltip .reference .internal} setting is used by default.

        -   **mqclass** (*class*) -- A class to be used as
            non-persistent request queue. The value for the
            [[`SCHEDULER_MEMORY_QUEUE`{.xref .std .std-setting .docutils
            .literal
            .notranslate}]{.pre}](index.html#std-setting-SCHEDULER_MEMORY_QUEUE){.hoverxref
            .tooltip .reference .internal} setting is used by default.

        -   **logunser**
            ([*bool*](https://docs.python.org/3/library/functions.html#bool "(in Python v3.12)"){.reference
            .external}) -- A boolean that indicates whether or not
            unserializable requests should be logged. The value for the
            [[`SCHEDULER_DEBUG`{.xref .std .std-setting .docutils
            .literal
            .notranslate}]{.pre}](index.html#std-setting-SCHEDULER_DEBUG){.hoverxref
            .tooltip .reference .internal} setting is used by default.

        -   **stats** ([[`scrapy.statscollectors.StatsCollector`{.xref
            .py .py-class .docutils .literal
            .notranslate}]{.pre}](index.html#scrapy.statscollectors.StatsCollector "scrapy.statscollectors.StatsCollector"){.reference
            .internal} instance or similar: any class that implements
            the StatsCollector interface) -- A stats collector object to
            record stats about the request scheduling process. The value
            for the [[`STATS_CLASS`{.xref .std .std-setting .docutils
            .literal
            .notranslate}]{.pre}](index.html#std-setting-STATS_CLASS){.hoverxref
            .tooltip .reference .internal} setting is used by default.

        -   **pqclass** (*class*) -- A class to be used as priority
            queue for requests. The value for the
            [[`SCHEDULER_PRIORITY_QUEUE`{.xref .std .std-setting
            .docutils .literal
            .notranslate}]{.pre}](index.html#std-setting-SCHEDULER_PRIORITY_QUEUE){.hoverxref
            .tooltip .reference .internal} setting is used by default.

        -   **crawler** ([[`scrapy.crawler.Crawler`{.xref .py .py-class
            .docutils .literal
            .notranslate}]{.pre}](index.html#scrapy.crawler.Crawler "scrapy.crawler.Crawler"){.reference
            .internal}) -- The crawler object corresponding to the
            current crawl.

    [[\_\_len\_\_]{.pre}]{.sig-name .descname}[(]{.sig-paren}[)]{.sig-paren} [[→]{.sig-return-icon} [[[int]{.pre}](https://docs.python.org/3/library/functions.html#int "(in Python v3.12)"){.reference .external}]{.sig-return-typehint}]{.sig-return}[[[\[source\]]{.pre}]{.viewcode-link}](_modules/scrapy/core/scheduler.html#Scheduler.__len__){.reference .internal}[¶](#scrapy.core.scheduler.Scheduler.__len__ "Permalink to this definition"){.headerlink}

    :   Return the total amount of enqueued requests

    [[close]{.pre}]{.sig-name .descname}[(]{.sig-paren}*[[reason]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[[[str]{.pre}](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.12)"){.reference .external}]{.n}*[)]{.sig-paren} [[→]{.sig-return-icon} [[[Optional]{.pre}](https://docs.python.org/3/library/typing.html#typing.Optional "(in Python v3.12)"){.reference .external}[[\[]{.pre}]{.p}[[Deferred]{.pre}](https://docs.twisted.org/en/stable/api/twisted.internet.defer.Deferred.html "(in Twisted)"){.reference .external}[[\]]{.pre}]{.p}]{.sig-return-typehint}]{.sig-return}[[[\[source\]]{.pre}]{.viewcode-link}](_modules/scrapy/core/scheduler.html#Scheduler.close){.reference .internal}[¶](#scrapy.core.scheduler.Scheduler.close "Permalink to this definition"){.headerlink}

    :   1.  dump pending requests to disk if there is a disk queue

        2.  return the result of the dupefilter's [`close`{.docutils
            .literal .notranslate}]{.pre} method

    [[enqueue_request]{.pre}]{.sig-name .descname}[(]{.sig-paren}*[[request]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[[[Request]{.pre}](index.html#scrapy.http.Request "scrapy.http.request.Request"){.reference .internal}]{.n}*[)]{.sig-paren} [[→]{.sig-return-icon} [[[bool]{.pre}](https://docs.python.org/3/library/functions.html#bool "(in Python v3.12)"){.reference .external}]{.sig-return-typehint}]{.sig-return}[[[\[source\]]{.pre}]{.viewcode-link}](_modules/scrapy/core/scheduler.html#Scheduler.enqueue_request){.reference .internal}[¶](#scrapy.core.scheduler.Scheduler.enqueue_request "Permalink to this definition"){.headerlink}

    :   Unless the received request is filtered out by the Dupefilter,
        attempt to push it into the disk queue, falling back to pushing
        it into the memory queue.

        Increment the appropriate stats, such as:
        [`scheduler/enqueued`{.docutils .literal .notranslate}]{.pre},
        [`scheduler/enqueued/disk`{.docutils .literal
        .notranslate}]{.pre}, [`scheduler/enqueued/memory`{.docutils
        .literal .notranslate}]{.pre}.

        Return [`True`{.docutils .literal .notranslate}]{.pre} if the
        request was stored successfully, [`False`{.docutils .literal
        .notranslate}]{.pre} otherwise.

    *[classmethod]{.pre}[ ]{.w}*[[from_crawler]{.pre}]{.sig-name .descname}[(]{.sig-paren}*[[crawler]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[[[Crawler]{.pre}](index.html#scrapy.crawler.Crawler "scrapy.crawler.Crawler"){.reference .internal}]{.n}*[)]{.sig-paren} [[→]{.sig-return-icon} [[SchedulerTV]{.pre}]{.sig-return-typehint}]{.sig-return}[[[\[source\]]{.pre}]{.viewcode-link}](_modules/scrapy/core/scheduler.html#Scheduler.from_crawler){.reference .internal}[¶](#scrapy.core.scheduler.Scheduler.from_crawler "Permalink to this definition"){.headerlink}

    :   Factory method, initializes the scheduler with arguments taken
        from the crawl settings

    [[has_pending_requests]{.pre}]{.sig-name .descname}[(]{.sig-paren}[)]{.sig-paren} [[→]{.sig-return-icon} [[[bool]{.pre}](https://docs.python.org/3/library/functions.html#bool "(in Python v3.12)"){.reference .external}]{.sig-return-typehint}]{.sig-return}[[[\[source\]]{.pre}]{.viewcode-link}](_modules/scrapy/core/scheduler.html#Scheduler.has_pending_requests){.reference .internal}[¶](#scrapy.core.scheduler.Scheduler.has_pending_requests "Permalink to this definition"){.headerlink}

    :   [`True`{.docutils .literal .notranslate}]{.pre} if the scheduler
        has enqueued requests, [`False`{.docutils .literal
        .notranslate}]{.pre} otherwise

    [[next_request]{.pre}]{.sig-name .descname}[(]{.sig-paren}[)]{.sig-paren} [[→]{.sig-return-icon} [[[Optional]{.pre}](https://docs.python.org/3/library/typing.html#typing.Optional "(in Python v3.12)"){.reference .external}[[\[]{.pre}]{.p}[[Request]{.pre}](index.html#scrapy.http.Request "scrapy.http.request.Request"){.reference .internal}[[\]]{.pre}]{.p}]{.sig-return-typehint}]{.sig-return}[[[\[source\]]{.pre}]{.viewcode-link}](_modules/scrapy/core/scheduler.html#Scheduler.next_request){.reference .internal}[¶](#scrapy.core.scheduler.Scheduler.next_request "Permalink to this definition"){.headerlink}

    :   Return a [[`Request`{.xref .py .py-class .docutils .literal
        .notranslate}]{.pre}](index.html#scrapy.http.Request "scrapy.http.Request"){.reference
        .internal} object from the memory queue, falling back to the
        disk queue if the memory queue is empty. Return
        [`None`{.docutils .literal .notranslate}]{.pre} if there are no
        more enqueued requests.

        Increment the appropriate stats, such as:
        [`scheduler/dequeued`{.docutils .literal .notranslate}]{.pre},
        [`scheduler/dequeued/disk`{.docutils .literal
        .notranslate}]{.pre}, [`scheduler/dequeued/memory`{.docutils
        .literal .notranslate}]{.pre}.

    [[open]{.pre}]{.sig-name .descname}[(]{.sig-paren}*[[spider]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[[[Spider]{.pre}](index.html#scrapy.spiders.Spider "scrapy.spiders.Spider"){.reference .internal}]{.n}*[)]{.sig-paren} [[→]{.sig-return-icon} [[[Optional]{.pre}](https://docs.python.org/3/library/typing.html#typing.Optional "(in Python v3.12)"){.reference .external}[[\[]{.pre}]{.p}[[Deferred]{.pre}](https://docs.twisted.org/en/stable/api/twisted.internet.defer.Deferred.html "(in Twisted)"){.reference .external}[[\]]{.pre}]{.p}]{.sig-return-typehint}]{.sig-return}[[[\[source\]]{.pre}]{.viewcode-link}](_modules/scrapy/core/scheduler.html#Scheduler.open){.reference .internal}[¶](#scrapy.core.scheduler.Scheduler.open "Permalink to this definition"){.headerlink}

    :   1.  initialize the memory queue

        2.  initialize the disk queue if the [`jobdir`{.docutils
            .literal .notranslate}]{.pre} attribute is a valid directory

        3.  return the result of the dupefilter's [`open`{.docutils
            .literal .notranslate}]{.pre} method
:::
:::

[]{#document-topics/exporters}

::: {#module-scrapy.exporters .section}
[]{#item-exporters}[]{#topics-exporters}

### Item Exporters[¶](#module-scrapy.exporters "Permalink to this heading"){.headerlink}

Once you have scraped your items, you often want to persist or export
those items, to use the data in some other application. That is, after
all, the whole purpose of the scraping process.

For this purpose Scrapy provides a collection of Item Exporters for
different output formats, such as XML, CSV or JSON.

::: {#using-item-exporters .section}
#### Using Item Exporters[¶](#using-item-exporters "Permalink to this heading"){.headerlink}

If you are in a hurry, and just want to use an Item Exporter to output
scraped data see the [[Feed exports]{.std
.std-ref}](index.html#topics-feed-exports){.hoverxref .tooltip
.reference .internal}. Otherwise, if you want to know how Item Exporters
work or need more custom functionality (not covered by the default
exports), continue reading below.

In order to use an Item Exporter, you must instantiate it with its
required args. Each Item Exporter requires different arguments, so check
each exporter documentation to be sure, in [[Built-in Item Exporters
reference]{.std .std-ref}](#topics-exporters-reference){.hoverxref
.tooltip .reference .internal}. After you have instantiated your
exporter, you have to:

1\. call the method [[`start_exporting()`{.xref .py .py-meth .docutils
.literal
.notranslate}]{.pre}](#scrapy.exporters.BaseItemExporter.start_exporting "scrapy.exporters.BaseItemExporter.start_exporting"){.reference
.internal} in order to signal the beginning of the exporting process

2\. call the [[`export_item()`{.xref .py .py-meth .docutils .literal
.notranslate}]{.pre}](#scrapy.exporters.BaseItemExporter.export_item "scrapy.exporters.BaseItemExporter.export_item"){.reference
.internal} method for each item you want to export

3\. and finally call the [[`finish_exporting()`{.xref .py .py-meth
.docutils .literal
.notranslate}]{.pre}](#scrapy.exporters.BaseItemExporter.finish_exporting "scrapy.exporters.BaseItemExporter.finish_exporting"){.reference
.internal} to signal the end of the exporting process

Here you can see an [[Item
Pipeline]{.doc}](index.html#document-topics/item-pipeline){.reference
.internal} which uses multiple Item Exporters to group scraped items to
different files according to the value of one of their fields:

::: {.highlight-python .notranslate}
::: highlight
    from itemadapter import ItemAdapter
    from scrapy.exporters import XmlItemExporter


    class PerYearXmlExportPipeline:
        """Distribute items across multiple XML files according to their 'year' field"""

        def open_spider(self, spider):
            self.year_to_exporter = {}

        def close_spider(self, spider):
            for exporter, xml_file in self.year_to_exporter.values():
                exporter.finish_exporting()
                xml_file.close()

        def _exporter_for_item(self, item):
            adapter = ItemAdapter(item)
            year = adapter["year"]
            if year not in self.year_to_exporter:
                xml_file = open(f"{year}.xml", "wb")
                exporter = XmlItemExporter(xml_file)
                exporter.start_exporting()
                self.year_to_exporter[year] = (exporter, xml_file)
            return self.year_to_exporter[year][0]

        def process_item(self, item, spider):
            exporter = self._exporter_for_item(item)
            exporter.export_item(item)
            return item
:::
:::
:::

::: {#serialization-of-item-fields .section}
[]{#topics-exporters-field-serialization}

#### Serialization of item fields[¶](#serialization-of-item-fields "Permalink to this heading"){.headerlink}

By default, the field values are passed unmodified to the underlying
serialization library, and the decision of how to serialize them is
delegated to each particular serialization library.

However, you can customize how each field value is serialized *before it
is passed to the serialization library*.

There are two ways to customize how a field will be serialized, which
are described next.

::: {#declaring-a-serializer-in-the-field .section}
[]{#topics-exporters-serializers}

##### 1. Declaring a serializer in the field[¶](#declaring-a-serializer-in-the-field "Permalink to this heading"){.headerlink}

If you use [`Item`{.xref .py .py-class .docutils .literal
.notranslate}]{.pre} you can declare a serializer in the [[field
metadata]{.std .std-ref}](index.html#topics-items-fields){.hoverxref
.tooltip .reference .internal}. The serializer must be a callable which
receives a value and returns its serialized form.

Example:

::: {.highlight-python .notranslate}
::: highlight
    import scrapy


    def serialize_price(value):
        return f"$ {str(value)}"


    class Product(scrapy.Item):
        name = scrapy.Field()
        price = scrapy.Field(serializer=serialize_price)
:::
:::
:::

::: {#overriding-the-serialize-field-method .section}
##### 2. Overriding the serialize_field() method[¶](#overriding-the-serialize-field-method "Permalink to this heading"){.headerlink}

You can also override the [[`serialize_field()`{.xref .py .py-meth
.docutils .literal
.notranslate}]{.pre}](#scrapy.exporters.BaseItemExporter.serialize_field "scrapy.exporters.BaseItemExporter.serialize_field"){.reference
.internal} method to customize how your field value will be exported.

Make sure you call the base class [[`serialize_field()`{.xref .py
.py-meth .docutils .literal
.notranslate}]{.pre}](#scrapy.exporters.BaseItemExporter.serialize_field "scrapy.exporters.BaseItemExporter.serialize_field"){.reference
.internal} method after your custom code.

Example:

::: {.highlight-python .notranslate}
::: highlight
    from scrapy.exporters import XmlItemExporter


    class ProductXmlExporter(XmlItemExporter):
        def serialize_field(self, field, name, value):
            if name == "price":
                return f"$ {str(value)}"
            return super().serialize_field(field, name, value)
:::
:::
:::
:::

::: {#built-in-item-exporters-reference .section}
[]{#topics-exporters-reference}

#### Built-in Item Exporters reference[¶](#built-in-item-exporters-reference "Permalink to this heading"){.headerlink}

Here is a list of the Item Exporters bundled with Scrapy. Some of them
contain output examples, which assume you're exporting these two items:

::: {.highlight-python .notranslate}
::: highlight
    Item(name="Color TV", price="1200")
    Item(name="DVD player", price="200")
:::
:::

::: {#baseitemexporter .section}
##### BaseItemExporter[¶](#baseitemexporter "Permalink to this heading"){.headerlink}

*[class]{.pre}[ ]{.w}*[[scrapy.exporters.]{.pre}]{.sig-prename .descclassname}[[BaseItemExporter]{.pre}]{.sig-name .descname}[(]{.sig-paren}*[[fields_to_export]{.pre}]{.n}[[=]{.pre}]{.o}[[None]{.pre}]{.default_value}*, *[[export_empty_fields]{.pre}]{.n}[[=]{.pre}]{.o}[[False]{.pre}]{.default_value}*, *[[encoding]{.pre}]{.n}[[=]{.pre}]{.o}[[\'utf-8\']{.pre}]{.default_value}*, *[[indent]{.pre}]{.n}[[=]{.pre}]{.o}[[0]{.pre}]{.default_value}*, *[[dont_fail]{.pre}]{.n}[[=]{.pre}]{.o}[[False]{.pre}]{.default_value}*[)]{.sig-paren}[[[\[source\]]{.pre}]{.viewcode-link}](_modules/scrapy/exporters.html#BaseItemExporter){.reference .internal}[¶](#scrapy.exporters.BaseItemExporter "Permalink to this definition"){.headerlink}

:   This is the (abstract) base class for all Item Exporters. It
    provides support for common features used by all (concrete) Item
    Exporters, such as defining what fields to export, whether to export
    empty fields, or which encoding to use.

    These features can be configured through the [`__init__`{.docutils
    .literal .notranslate}]{.pre} method arguments which populate their
    respective instance attributes: [[`fields_to_export`{.xref .py
    .py-attr .docutils .literal
    .notranslate}]{.pre}](#scrapy.exporters.BaseItemExporter.fields_to_export "scrapy.exporters.BaseItemExporter.fields_to_export"){.reference
    .internal}, [[`export_empty_fields`{.xref .py .py-attr .docutils
    .literal
    .notranslate}]{.pre}](#scrapy.exporters.BaseItemExporter.export_empty_fields "scrapy.exporters.BaseItemExporter.export_empty_fields"){.reference
    .internal}, [[`encoding`{.xref .py .py-attr .docutils .literal
    .notranslate}]{.pre}](#scrapy.exporters.BaseItemExporter.encoding "scrapy.exporters.BaseItemExporter.encoding"){.reference
    .internal}, [[`indent`{.xref .py .py-attr .docutils .literal
    .notranslate}]{.pre}](#scrapy.exporters.BaseItemExporter.indent "scrapy.exporters.BaseItemExporter.indent"){.reference
    .internal}.

    ::: versionadded
    [New in version 2.0: ]{.versionmodified .added}The *dont_fail*
    parameter.
    :::

    [[export_item]{.pre}]{.sig-name .descname}[(]{.sig-paren}*[[item]{.pre}]{.n}*[)]{.sig-paren}[[[\[source\]]{.pre}]{.viewcode-link}](_modules/scrapy/exporters.html#BaseItemExporter.export_item){.reference .internal}[¶](#scrapy.exporters.BaseItemExporter.export_item "Permalink to this definition"){.headerlink}

    :   Exports the given item. This method must be implemented in
        subclasses.

    [[serialize_field]{.pre}]{.sig-name .descname}[(]{.sig-paren}*[[field]{.pre}]{.n}*, *[[name]{.pre}]{.n}*, *[[value]{.pre}]{.n}*[)]{.sig-paren}[[[\[source\]]{.pre}]{.viewcode-link}](_modules/scrapy/exporters.html#BaseItemExporter.serialize_field){.reference .internal}[¶](#scrapy.exporters.BaseItemExporter.serialize_field "Permalink to this definition"){.headerlink}

    :   Return the serialized value for the given field. You can
        override this method (in your custom Item Exporters) if you want
        to control how a particular field or value will be
        serialized/exported.

        By default, this method looks for a serializer [[declared in the
        item field]{.std
        .std-ref}](#topics-exporters-serializers){.hoverxref .tooltip
        .reference .internal} and returns the result of applying that
        serializer to the value. If no serializer is found, it returns
        the value unchanged.

        Parameters

        :   -   **field** ([`Field`{.xref .py .py-class .docutils
                .literal .notranslate}]{.pre} object or a [[`dict`{.xref
                .py .py-class .docutils .literal
                .notranslate}]{.pre}](https://docs.python.org/3/library/stdtypes.html#dict "(in Python v3.12)"){.reference
                .external} instance) -- the field being serialized. If
                the source [[item object]{.std
                .std-ref}](index.html#item-types){.hoverxref .tooltip
                .reference .internal} does not define field metadata,
                *field* is an empty [[`dict`{.xref .py .py-class
                .docutils .literal
                .notranslate}]{.pre}](https://docs.python.org/3/library/stdtypes.html#dict "(in Python v3.12)"){.reference
                .external}.

            -   **name**
                ([*str*](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.12)"){.reference
                .external}) -- the name of the field being serialized

            -   **value** -- the value being serialized

    [[start_exporting]{.pre}]{.sig-name .descname}[(]{.sig-paren}[)]{.sig-paren}[[[\[source\]]{.pre}]{.viewcode-link}](_modules/scrapy/exporters.html#BaseItemExporter.start_exporting){.reference .internal}[¶](#scrapy.exporters.BaseItemExporter.start_exporting "Permalink to this definition"){.headerlink}

    :   Signal the beginning of the exporting process. Some exporters
        may use this to generate some required header (for example, the
        [[`XmlItemExporter`{.xref .py .py-class .docutils .literal
        .notranslate}]{.pre}](#scrapy.exporters.XmlItemExporter "scrapy.exporters.XmlItemExporter"){.reference
        .internal}). You must call this method before exporting any
        items.

    [[finish_exporting]{.pre}]{.sig-name .descname}[(]{.sig-paren}[)]{.sig-paren}[[[\[source\]]{.pre}]{.viewcode-link}](_modules/scrapy/exporters.html#BaseItemExporter.finish_exporting){.reference .internal}[¶](#scrapy.exporters.BaseItemExporter.finish_exporting "Permalink to this definition"){.headerlink}

    :   Signal the end of the exporting process. Some exporters may use
        this to generate some required footer (for example, the
        [[`XmlItemExporter`{.xref .py .py-class .docutils .literal
        .notranslate}]{.pre}](#scrapy.exporters.XmlItemExporter "scrapy.exporters.XmlItemExporter"){.reference
        .internal}). You must always call this method after you have no
        more items to export.

    [[fields_to_export]{.pre}]{.sig-name .descname}[¶](#scrapy.exporters.BaseItemExporter.fields_to_export "Permalink to this definition"){.headerlink}

    :   Fields to export, their order [1](#id3){#id1 .footnote-reference
        .brackets} and their output names.

        Possible values are:

        -   [`None`{.docutils .literal .notranslate}]{.pre} (all fields
            [2](#id4){#id2 .footnote-reference .brackets}, default)

        -   A list of fields:

            ::: {.highlight-default .notranslate}
            ::: highlight
                ['field1', 'field2']
            :::
            :::

        -   A dict where keys are fields and values are output names:

            ::: {.highlight-default .notranslate}
            ::: highlight
                {'field1': 'Field 1', 'field2': 'Field 2'}
            :::
            :::

        [[1](#id1){.fn-backref}]{.brackets}

        :   Not all exporters respect the specified field order.

        [[2](#id2){.fn-backref}]{.brackets}

        :   When using [[item objects]{.std
            .std-ref}](index.html#item-types){.hoverxref .tooltip
            .reference .internal} that do not expose all their possible
            fields, exporters that do not support exporting a different
            subset of fields per item will only export the fields found
            in the first item exported.

    [[export_empty_fields]{.pre}]{.sig-name .descname}[¶](#scrapy.exporters.BaseItemExporter.export_empty_fields "Permalink to this definition"){.headerlink}

    :   Whether to include empty/unpopulated item fields in the exported
        data. Defaults to [`False`{.docutils .literal
        .notranslate}]{.pre}. Some exporters (like
        [[`CsvItemExporter`{.xref .py .py-class .docutils .literal
        .notranslate}]{.pre}](#scrapy.exporters.CsvItemExporter "scrapy.exporters.CsvItemExporter"){.reference
        .internal}) ignore this attribute and always export all empty
        fields.

        This option is ignored for dict items.

    [[encoding]{.pre}]{.sig-name .descname}[¶](#scrapy.exporters.BaseItemExporter.encoding "Permalink to this definition"){.headerlink}

    :   The output character encoding.

    [[indent]{.pre}]{.sig-name .descname}[¶](#scrapy.exporters.BaseItemExporter.indent "Permalink to this definition"){.headerlink}

    :   Amount of spaces used to indent the output on each level.
        Defaults to [`0`{.docutils .literal .notranslate}]{.pre}.

        -   [`indent=None`{.docutils .literal .notranslate}]{.pre}
            selects the most compact representation, all items in the
            same line with no indentation

        -   [`indent<=0`{.docutils .literal .notranslate}]{.pre} each
            item on its own line, no indentation

        -   [`indent>0`{.docutils .literal .notranslate}]{.pre} each
            item on its own line, indented with the provided numeric
            value
:::

::: {#pythonitemexporter .section}
##### PythonItemExporter[¶](#pythonitemexporter "Permalink to this heading"){.headerlink}

*[class]{.pre}[ ]{.w}*[[scrapy.exporters.]{.pre}]{.sig-prename .descclassname}[[PythonItemExporter]{.pre}]{.sig-name .descname}[(]{.sig-paren}*[[\*]{.pre}]{.o}*, *[[dont_fail]{.pre}]{.n}[[=]{.pre}]{.o}[[False]{.pre}]{.default_value}*, *[[\*\*]{.pre}]{.o}[[kwargs]{.pre}]{.n}*[)]{.sig-paren}[[[\[source\]]{.pre}]{.viewcode-link}](_modules/scrapy/exporters.html#PythonItemExporter){.reference .internal}[¶](#scrapy.exporters.PythonItemExporter "Permalink to this definition"){.headerlink}

:   This is a base class for item exporters that extends
    [[`BaseItemExporter`{.xref .py .py-class .docutils .literal
    .notranslate}]{.pre}](#scrapy.exporters.BaseItemExporter "scrapy.exporters.BaseItemExporter"){.reference
    .internal} with support for nested items.

    It serializes items to built-in Python types, so that any
    serialization library (e.g. [[`json`{.xref .py .py-mod .docutils
    .literal
    .notranslate}]{.pre}](https://docs.python.org/3/library/json.html#module-json "(in Python v3.12)"){.reference
    .external} or
    [msgpack](https://pypi.org/project/msgpack/){.reference .external})
    can be used on top of it.
:::

::: {#xmlitemexporter .section}
##### XmlItemExporter[¶](#xmlitemexporter "Permalink to this heading"){.headerlink}

*[class]{.pre}[ ]{.w}*[[scrapy.exporters.]{.pre}]{.sig-prename .descclassname}[[XmlItemExporter]{.pre}]{.sig-name .descname}[(]{.sig-paren}*[[file]{.pre}]{.n}*, *[[item_element]{.pre}]{.n}[[=]{.pre}]{.o}[[\'item\']{.pre}]{.default_value}*, *[[root_element]{.pre}]{.n}[[=]{.pre}]{.o}[[\'items\']{.pre}]{.default_value}*, *[[\*\*]{.pre}]{.o}[[kwargs]{.pre}]{.n}*[)]{.sig-paren}[[[\[source\]]{.pre}]{.viewcode-link}](_modules/scrapy/exporters.html#XmlItemExporter){.reference .internal}[¶](#scrapy.exporters.XmlItemExporter "Permalink to this definition"){.headerlink}

:   Exports items in XML format to the specified file object.

    Parameters

    :   -   **file** -- the file-like object to use for exporting the
            data. Its [`write`{.docutils .literal .notranslate}]{.pre}
            method should accept [`bytes`{.docutils .literal
            .notranslate}]{.pre} (a disk file opened in binary mode, a
            [`io.BytesIO`{.docutils .literal .notranslate}]{.pre}
            object, etc)

        -   **root_element**
            ([*str*](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.12)"){.reference
            .external}) -- The name of root element in the exported XML.

        -   **item_element**
            ([*str*](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.12)"){.reference
            .external}) -- The name of each item element in the exported
            XML.

    The additional keyword arguments of this [`__init__`{.docutils
    .literal .notranslate}]{.pre} method are passed to the
    [[`BaseItemExporter`{.xref .py .py-class .docutils .literal
    .notranslate}]{.pre}](#scrapy.exporters.BaseItemExporter "scrapy.exporters.BaseItemExporter"){.reference
    .internal} [`__init__`{.docutils .literal .notranslate}]{.pre}
    method.

    A typical output of this exporter would be:

    ::: {.highlight-none .notranslate}
    ::: highlight
        <?xml version="1.0" encoding="utf-8"?>
        <items>
          <item>
            <name>Color TV</name>
            <price>1200</price>
         </item>
          <item>
            <name>DVD player</name>
            <price>200</price>
         </item>
        </items>
    :::
    :::

    Unless overridden in the [`serialize_field()`{.xref .py .py-meth
    .docutils .literal .notranslate}]{.pre} method, multi-valued fields
    are exported by serializing each value inside a [`<value>`{.docutils
    .literal .notranslate}]{.pre} element. This is for convenience, as
    multi-valued fields are very common.

    For example, the item:

    ::: {.highlight-none .notranslate}
    ::: highlight
        Item(name=['John', 'Doe'], age='23')
    :::
    :::

    Would be serialized as:

    ::: {.highlight-none .notranslate}
    ::: highlight
        <?xml version="1.0" encoding="utf-8"?>
        <items>
          <item>
            <name>
              <value>John</value>
              <value>Doe</value>
            </name>
            <age>23</age>
          </item>
        </items>
    :::
    :::
:::

::: {#csvitemexporter .section}
##### CsvItemExporter[¶](#csvitemexporter "Permalink to this heading"){.headerlink}

*[class]{.pre}[ ]{.w}*[[scrapy.exporters.]{.pre}]{.sig-prename .descclassname}[[CsvItemExporter]{.pre}]{.sig-name .descname}[(]{.sig-paren}*[[file]{.pre}]{.n}*, *[[include_headers_line]{.pre}]{.n}[[=]{.pre}]{.o}[[True]{.pre}]{.default_value}*, *[[join_multivalued]{.pre}]{.n}[[=]{.pre}]{.o}[[\',\']{.pre}]{.default_value}*, *[[errors]{.pre}]{.n}[[=]{.pre}]{.o}[[None]{.pre}]{.default_value}*, *[[\*\*]{.pre}]{.o}[[kwargs]{.pre}]{.n}*[)]{.sig-paren}[[[\[source\]]{.pre}]{.viewcode-link}](_modules/scrapy/exporters.html#CsvItemExporter){.reference .internal}[¶](#scrapy.exporters.CsvItemExporter "Permalink to this definition"){.headerlink}

:   Exports items in CSV format to the given file-like object. If the
    [`fields_to_export`{.xref .py .py-attr .docutils .literal
    .notranslate}]{.pre} attribute is set, it will be used to define the
    CSV columns, their order and their column names. The
    [`export_empty_fields`{.xref .py .py-attr .docutils .literal
    .notranslate}]{.pre} attribute has no effect on this exporter.

    Parameters

    :   -   **file** -- the file-like object to use for exporting the
            data. Its [`write`{.docutils .literal .notranslate}]{.pre}
            method should accept [`bytes`{.docutils .literal
            .notranslate}]{.pre} (a disk file opened in binary mode, a
            [`io.BytesIO`{.docutils .literal .notranslate}]{.pre}
            object, etc)

        -   **include_headers_line**
            ([*str*](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.12)"){.reference
            .external}) -- If enabled, makes the exporter output a
            header line with the field names taken from
            [[`BaseItemExporter.fields_to_export`{.xref .py .py-attr
            .docutils .literal
            .notranslate}]{.pre}](#scrapy.exporters.BaseItemExporter.fields_to_export "scrapy.exporters.BaseItemExporter.fields_to_export"){.reference
            .internal} or the first exported item fields.

        -   **join_multivalued** -- The char (or chars) that will be
            used for joining multi-valued fields, if found.

        -   **errors**
            ([*str*](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.12)"){.reference
            .external}) -- The optional string that specifies how
            encoding and decoding errors are to be handled. For more
            information see [[`io.TextIOWrapper`{.xref .py .py-class
            .docutils .literal
            .notranslate}]{.pre}](https://docs.python.org/3/library/io.html#io.TextIOWrapper "(in Python v3.12)"){.reference
            .external}.

    The additional keyword arguments of this [`__init__`{.docutils
    .literal .notranslate}]{.pre} method are passed to the
    [[`BaseItemExporter`{.xref .py .py-class .docutils .literal
    .notranslate}]{.pre}](#scrapy.exporters.BaseItemExporter "scrapy.exporters.BaseItemExporter"){.reference
    .internal} [`__init__`{.docutils .literal .notranslate}]{.pre}
    method, and the leftover arguments to the [[`csv.writer()`{.xref .py
    .py-func .docutils .literal
    .notranslate}]{.pre}](https://docs.python.org/3/library/csv.html#csv.writer "(in Python v3.12)"){.reference
    .external} function, so you can use any [[`csv.writer()`{.xref .py
    .py-func .docutils .literal
    .notranslate}]{.pre}](https://docs.python.org/3/library/csv.html#csv.writer "(in Python v3.12)"){.reference
    .external} function argument to customize this exporter.

    A typical output of this exporter would be:

    ::: {.highlight-none .notranslate}
    ::: highlight
        product,price
        Color TV,1200
        DVD player,200
    :::
    :::
:::

::: {#pickleitemexporter .section}
##### PickleItemExporter[¶](#pickleitemexporter "Permalink to this heading"){.headerlink}

*[class]{.pre}[ ]{.w}*[[scrapy.exporters.]{.pre}]{.sig-prename .descclassname}[[PickleItemExporter]{.pre}]{.sig-name .descname}[(]{.sig-paren}*[[file]{.pre}]{.n}*, *[[protocol]{.pre}]{.n}[[=]{.pre}]{.o}[[0]{.pre}]{.default_value}*, *[[\*\*]{.pre}]{.o}[[kwargs]{.pre}]{.n}*[)]{.sig-paren}[[[\[source\]]{.pre}]{.viewcode-link}](_modules/scrapy/exporters.html#PickleItemExporter){.reference .internal}[¶](#scrapy.exporters.PickleItemExporter "Permalink to this definition"){.headerlink}

:   Exports items in pickle format to the given file-like object.

    Parameters

    :   -   **file** -- the file-like object to use for exporting the
            data. Its [`write`{.docutils .literal .notranslate}]{.pre}
            method should accept [`bytes`{.docutils .literal
            .notranslate}]{.pre} (a disk file opened in binary mode, a
            [`io.BytesIO`{.docutils .literal .notranslate}]{.pre}
            object, etc)

        -   **protocol**
            ([*int*](https://docs.python.org/3/library/functions.html#int "(in Python v3.12)"){.reference
            .external}) -- The pickle protocol to use.

    For more information, see [[`pickle`{.xref .py .py-mod .docutils
    .literal
    .notranslate}]{.pre}](https://docs.python.org/3/library/pickle.html#module-pickle "(in Python v3.12)"){.reference
    .external}.

    The additional keyword arguments of this [`__init__`{.docutils
    .literal .notranslate}]{.pre} method are passed to the
    [[`BaseItemExporter`{.xref .py .py-class .docutils .literal
    .notranslate}]{.pre}](#scrapy.exporters.BaseItemExporter "scrapy.exporters.BaseItemExporter"){.reference
    .internal} [`__init__`{.docutils .literal .notranslate}]{.pre}
    method.

    Pickle isn't a human readable format, so no output examples are
    provided.
:::

::: {#pprintitemexporter .section}
##### PprintItemExporter[¶](#pprintitemexporter "Permalink to this heading"){.headerlink}

*[class]{.pre}[ ]{.w}*[[scrapy.exporters.]{.pre}]{.sig-prename .descclassname}[[PprintItemExporter]{.pre}]{.sig-name .descname}[(]{.sig-paren}*[[file]{.pre}]{.n}*, *[[\*\*]{.pre}]{.o}[[kwargs]{.pre}]{.n}*[)]{.sig-paren}[[[\[source\]]{.pre}]{.viewcode-link}](_modules/scrapy/exporters.html#PprintItemExporter){.reference .internal}[¶](#scrapy.exporters.PprintItemExporter "Permalink to this definition"){.headerlink}

:   Exports items in pretty print format to the specified file object.

    Parameters

    :   **file** -- the file-like object to use for exporting the data.
        Its [`write`{.docutils .literal .notranslate}]{.pre} method
        should accept [`bytes`{.docutils .literal .notranslate}]{.pre}
        (a disk file opened in binary mode, a [`io.BytesIO`{.docutils
        .literal .notranslate}]{.pre} object, etc)

    The additional keyword arguments of this [`__init__`{.docutils
    .literal .notranslate}]{.pre} method are passed to the
    [[`BaseItemExporter`{.xref .py .py-class .docutils .literal
    .notranslate}]{.pre}](#scrapy.exporters.BaseItemExporter "scrapy.exporters.BaseItemExporter"){.reference
    .internal} [`__init__`{.docutils .literal .notranslate}]{.pre}
    method.

    A typical output of this exporter would be:

    ::: {.highlight-none .notranslate}
    ::: highlight
        {'name': 'Color TV', 'price': '1200'}
        {'name': 'DVD player', 'price': '200'}
    :::
    :::

    Longer lines (when present) are pretty-formatted.
:::

::: {#jsonitemexporter .section}
##### JsonItemExporter[¶](#jsonitemexporter "Permalink to this heading"){.headerlink}

*[class]{.pre}[ ]{.w}*[[scrapy.exporters.]{.pre}]{.sig-prename .descclassname}[[JsonItemExporter]{.pre}]{.sig-name .descname}[(]{.sig-paren}*[[file]{.pre}]{.n}*, *[[\*\*]{.pre}]{.o}[[kwargs]{.pre}]{.n}*[)]{.sig-paren}[[[\[source\]]{.pre}]{.viewcode-link}](_modules/scrapy/exporters.html#JsonItemExporter){.reference .internal}[¶](#scrapy.exporters.JsonItemExporter "Permalink to this definition"){.headerlink}

:   Exports items in JSON format to the specified file-like object,
    writing all objects as a list of objects. The additional
    [`__init__`{.docutils .literal .notranslate}]{.pre} method arguments
    are passed to the [[`BaseItemExporter`{.xref .py .py-class .docutils
    .literal
    .notranslate}]{.pre}](#scrapy.exporters.BaseItemExporter "scrapy.exporters.BaseItemExporter"){.reference
    .internal} [`__init__`{.docutils .literal .notranslate}]{.pre}
    method, and the leftover arguments to the [[`JSONEncoder`{.xref .py
    .py-class .docutils .literal
    .notranslate}]{.pre}](https://docs.python.org/3/library/json.html#json.JSONEncoder "(in Python v3.12)"){.reference
    .external} [`__init__`{.docutils .literal .notranslate}]{.pre}
    method, so you can use any [[`JSONEncoder`{.xref .py .py-class
    .docutils .literal
    .notranslate}]{.pre}](https://docs.python.org/3/library/json.html#json.JSONEncoder "(in Python v3.12)"){.reference
    .external} [`__init__`{.docutils .literal .notranslate}]{.pre}
    method argument to customize this exporter.

    Parameters

    :   **file** -- the file-like object to use for exporting the data.
        Its [`write`{.docutils .literal .notranslate}]{.pre} method
        should accept [`bytes`{.docutils .literal .notranslate}]{.pre}
        (a disk file opened in binary mode, a [`io.BytesIO`{.docutils
        .literal .notranslate}]{.pre} object, etc)

    A typical output of this exporter would be:

    ::: {.highlight-none .notranslate}
    ::: highlight
        [{"name": "Color TV", "price": "1200"},
        {"name": "DVD player", "price": "200"}]
    :::
    :::

    ::: {#json-with-large-data .admonition .warning}
    Warning

    JSON is very simple and flexible serialization format, but it
    doesn't scale well for large amounts of data since incremental (aka.
    stream-mode) parsing is not well supported (if at all) among JSON
    parsers (on any language), and most of them just parse the entire
    object in memory. If you want the power and simplicity of JSON with
    a more stream-friendly format, consider using
    [[`JsonLinesItemExporter`{.xref .py .py-class .docutils .literal
    .notranslate}]{.pre}](#scrapy.exporters.JsonLinesItemExporter "scrapy.exporters.JsonLinesItemExporter"){.reference
    .internal} instead, or splitting the output in multiple chunks.
    :::
:::

::: {#jsonlinesitemexporter .section}
##### JsonLinesItemExporter[¶](#jsonlinesitemexporter "Permalink to this heading"){.headerlink}

*[class]{.pre}[ ]{.w}*[[scrapy.exporters.]{.pre}]{.sig-prename .descclassname}[[JsonLinesItemExporter]{.pre}]{.sig-name .descname}[(]{.sig-paren}*[[file]{.pre}]{.n}*, *[[\*\*]{.pre}]{.o}[[kwargs]{.pre}]{.n}*[)]{.sig-paren}[[[\[source\]]{.pre}]{.viewcode-link}](_modules/scrapy/exporters.html#JsonLinesItemExporter){.reference .internal}[¶](#scrapy.exporters.JsonLinesItemExporter "Permalink to this definition"){.headerlink}

:   Exports items in JSON format to the specified file-like object,
    writing one JSON-encoded item per line. The additional
    [`__init__`{.docutils .literal .notranslate}]{.pre} method arguments
    are passed to the [[`BaseItemExporter`{.xref .py .py-class .docutils
    .literal
    .notranslate}]{.pre}](#scrapy.exporters.BaseItemExporter "scrapy.exporters.BaseItemExporter"){.reference
    .internal} [`__init__`{.docutils .literal .notranslate}]{.pre}
    method, and the leftover arguments to the [[`JSONEncoder`{.xref .py
    .py-class .docutils .literal
    .notranslate}]{.pre}](https://docs.python.org/3/library/json.html#json.JSONEncoder "(in Python v3.12)"){.reference
    .external} [`__init__`{.docutils .literal .notranslate}]{.pre}
    method, so you can use any [[`JSONEncoder`{.xref .py .py-class
    .docutils .literal
    .notranslate}]{.pre}](https://docs.python.org/3/library/json.html#json.JSONEncoder "(in Python v3.12)"){.reference
    .external} [`__init__`{.docutils .literal .notranslate}]{.pre}
    method argument to customize this exporter.

    Parameters

    :   **file** -- the file-like object to use for exporting the data.
        Its [`write`{.docutils .literal .notranslate}]{.pre} method
        should accept [`bytes`{.docutils .literal .notranslate}]{.pre}
        (a disk file opened in binary mode, a [`io.BytesIO`{.docutils
        .literal .notranslate}]{.pre} object, etc)

    A typical output of this exporter would be:

    ::: {.highlight-none .notranslate}
    ::: highlight
        {"name": "Color TV", "price": "1200"}
        {"name": "DVD player", "price": "200"}
    :::
    :::

    Unlike the one produced by [[`JsonItemExporter`{.xref .py .py-class
    .docutils .literal
    .notranslate}]{.pre}](#scrapy.exporters.JsonItemExporter "scrapy.exporters.JsonItemExporter"){.reference
    .internal}, the format produced by this exporter is well suited for
    serializing large amounts of data.
:::

::: {#marshalitemexporter .section}
##### MarshalItemExporter[¶](#marshalitemexporter "Permalink to this heading"){.headerlink}

*[class]{.pre}[ ]{.w}*[[scrapy.exporters.]{.pre}]{.sig-prename .descclassname}[[MarshalItemExporter]{.pre}]{.sig-name .descname}[(]{.sig-paren}*[[file]{.pre}]{.n}*, *[[\*\*]{.pre}]{.o}[[kwargs]{.pre}]{.n}*[)]{.sig-paren}[[[\[source\]]{.pre}]{.viewcode-link}](_modules/scrapy/exporters.html#MarshalItemExporter){.reference .internal}[¶](#scrapy.exporters.MarshalItemExporter "Permalink to this definition"){.headerlink}

:   Exports items in a Python-specific binary format (see
    [[`marshal`{.xref .py .py-mod .docutils .literal
    .notranslate}]{.pre}](https://docs.python.org/3/library/marshal.html#module-marshal "(in Python v3.12)"){.reference
    .external}).

    Parameters

    :   **file** -- The file-like object to use for exporting the data.
        Its [`write`{.docutils .literal .notranslate}]{.pre} method
        should accept [[`bytes`{.xref .py .py-class .docutils .literal
        .notranslate}]{.pre}](https://docs.python.org/3/library/stdtypes.html#bytes "(in Python v3.12)"){.reference
        .external} (a disk file opened in binary mode, a
        [[`BytesIO`{.xref .py .py-class .docutils .literal
        .notranslate}]{.pre}](https://docs.python.org/3/library/io.html#io.BytesIO "(in Python v3.12)"){.reference
        .external} object, etc)
:::
:::
:::

[]{#document-topics/components}

::: {#components .section}
[]{#topics-components}

### Components[¶](#components "Permalink to this heading"){.headerlink}

A Scrapy component is any class whose objects are created using
[`scrapy.utils.misc.create_instance()`{.xref .py .py-func .docutils
.literal .notranslate}]{.pre}.

That includes the classes that you may assign to the following settings:

-   [[`DNS_RESOLVER`{.xref .std .std-setting .docutils .literal
    .notranslate}]{.pre}](index.html#std-setting-DNS_RESOLVER){.hoverxref
    .tooltip .reference .internal}

-   [[`DOWNLOAD_HANDLERS`{.xref .std .std-setting .docutils .literal
    .notranslate}]{.pre}](index.html#std-setting-DOWNLOAD_HANDLERS){.hoverxref
    .tooltip .reference .internal}

-   [[`DOWNLOADER_CLIENTCONTEXTFACTORY`{.xref .std .std-setting
    .docutils .literal
    .notranslate}]{.pre}](index.html#std-setting-DOWNLOADER_CLIENTCONTEXTFACTORY){.hoverxref
    .tooltip .reference .internal}

-   [[`DOWNLOADER_MIDDLEWARES`{.xref .std .std-setting .docutils
    .literal
    .notranslate}]{.pre}](index.html#std-setting-DOWNLOADER_MIDDLEWARES){.hoverxref
    .tooltip .reference .internal}

-   [[`DUPEFILTER_CLASS`{.xref .std .std-setting .docutils .literal
    .notranslate}]{.pre}](index.html#std-setting-DUPEFILTER_CLASS){.hoverxref
    .tooltip .reference .internal}

-   [[`EXTENSIONS`{.xref .std .std-setting .docutils .literal
    .notranslate}]{.pre}](index.html#std-setting-EXTENSIONS){.hoverxref
    .tooltip .reference .internal}

-   [[`FEED_EXPORTERS`{.xref .std .std-setting .docutils .literal
    .notranslate}]{.pre}](index.html#std-setting-FEED_EXPORTERS){.hoverxref
    .tooltip .reference .internal}

-   [[`FEED_STORAGES`{.xref .std .std-setting .docutils .literal
    .notranslate}]{.pre}](index.html#std-setting-FEED_STORAGES){.hoverxref
    .tooltip .reference .internal}

-   [[`ITEM_PIPELINES`{.xref .std .std-setting .docutils .literal
    .notranslate}]{.pre}](index.html#std-setting-ITEM_PIPELINES){.hoverxref
    .tooltip .reference .internal}

-   [[`SCHEDULER`{.xref .std .std-setting .docutils .literal
    .notranslate}]{.pre}](index.html#std-setting-SCHEDULER){.hoverxref
    .tooltip .reference .internal}

-   [[`SCHEDULER_DISK_QUEUE`{.xref .std .std-setting .docutils .literal
    .notranslate}]{.pre}](index.html#std-setting-SCHEDULER_DISK_QUEUE){.hoverxref
    .tooltip .reference .internal}

-   [[`SCHEDULER_MEMORY_QUEUE`{.xref .std .std-setting .docutils
    .literal
    .notranslate}]{.pre}](index.html#std-setting-SCHEDULER_MEMORY_QUEUE){.hoverxref
    .tooltip .reference .internal}

-   [[`SCHEDULER_PRIORITY_QUEUE`{.xref .std .std-setting .docutils
    .literal
    .notranslate}]{.pre}](index.html#std-setting-SCHEDULER_PRIORITY_QUEUE){.hoverxref
    .tooltip .reference .internal}

-   [[`SPIDER_MIDDLEWARES`{.xref .std .std-setting .docutils .literal
    .notranslate}]{.pre}](index.html#std-setting-SPIDER_MIDDLEWARES){.hoverxref
    .tooltip .reference .internal}

Third-party Scrapy components may also let you define additional Scrapy
components, usually configurable through [[settings]{.std
.std-ref}](index.html#topics-settings){.hoverxref .tooltip .reference
.internal}, to modify their behavior.

::: {#enforcing-component-requirements .section}
[]{#enforce-component-requirements}

#### Enforcing component requirements[¶](#enforcing-component-requirements "Permalink to this heading"){.headerlink}

Sometimes, your components may only be intended to work under certain
conditions. For example, they may require a minimum version of Scrapy to
work as intended, or they may require certain settings to have specific
values.

In addition to describing those conditions in the documentation of your
component, it is a good practice to raise an exception from the
[`__init__`{.docutils .literal .notranslate}]{.pre} method of your
component if those conditions are not met at run time.

In the case of [[downloader middlewares]{.std
.std-ref}](index.html#topics-downloader-middleware){.hoverxref .tooltip
.reference .internal}, [[extensions]{.std
.std-ref}](index.html#topics-extensions){.hoverxref .tooltip .reference
.internal}, [[item pipelines]{.std
.std-ref}](index.html#topics-item-pipeline){.hoverxref .tooltip
.reference .internal}, and [[spider middlewares]{.std
.std-ref}](index.html#topics-spider-middleware){.hoverxref .tooltip
.reference .internal}, you should raise
[[`scrapy.exceptions.NotConfigured`{.xref .py .py-exc .docutils .literal
.notranslate}]{.pre}](index.html#scrapy.exceptions.NotConfigured "scrapy.exceptions.NotConfigured"){.reference
.internal}, passing a description of the issue as a parameter to the
exception so that it is printed in the logs, for the user to see. For
other components, feel free to raise whatever other exception feels
right to you; for example, [[`RuntimeError`{.xref .py .py-exc .docutils
.literal
.notranslate}]{.pre}](https://docs.python.org/3/library/exceptions.html#RuntimeError "(in Python v3.12)"){.reference
.external} would make sense for a Scrapy version mismatch, while
[[`ValueError`{.xref .py .py-exc .docutils .literal
.notranslate}]{.pre}](https://docs.python.org/3/library/exceptions.html#ValueError "(in Python v3.12)"){.reference
.external} may be better if the issue is the value of a setting.

If your requirement is a minimum Scrapy version, you may use
[`scrapy.__version__`{.xref .py .py-attr .docutils .literal
.notranslate}]{.pre} to enforce your requirement. For example:

::: {.highlight-python .notranslate}
::: highlight
    from packaging.version import parse as parse_version

    import scrapy


    class MyComponent:
        def __init__(self):
            if parse_version(scrapy.__version__) < parse_version("2.7"):
                raise RuntimeError(
                    f"{MyComponent.__qualname__} requires Scrapy 2.7 or "
                    f"later, which allow defining the process_spider_output "
                    f"method of spider middlewares as an asynchronous "
                    f"generator."
                )
:::
:::
:::
:::

[]{#document-topics/api}

::: {#core-api .section}
[]{#topics-api}

### Core API[¶](#core-api "Permalink to this heading"){.headerlink}

This section documents the Scrapy core API, and it's intended for
developers of extensions and middlewares.

::: {#crawler-api .section}
[]{#topics-api-crawler}

#### Crawler API[¶](#crawler-api "Permalink to this heading"){.headerlink}

The main entry point to Scrapy API is the [[`Crawler`{.xref .py
.py-class .docutils .literal
.notranslate}]{.pre}](#scrapy.crawler.Crawler "scrapy.crawler.Crawler"){.reference
.internal} object, passed to extensions through the
[`from_crawler`{.docutils .literal .notranslate}]{.pre} class method.
This object provides access to all Scrapy core components, and it's the
only way for extensions to access them and hook their functionality into
Scrapy.

[]{#module-scrapy.crawler .target}

The Extension Manager is responsible for loading and keeping track of
installed extensions and it's configured through the
[[`EXTENSIONS`{.xref .std .std-setting .docutils .literal
.notranslate}]{.pre}](index.html#std-setting-EXTENSIONS){.hoverxref
.tooltip .reference .internal} setting which contains a dictionary of
all available extensions and their order similar to how you [[configure
the downloader middlewares]{.std
.std-ref}](index.html#topics-downloader-middleware-setting){.hoverxref
.tooltip .reference .internal}.

*[class]{.pre}[ ]{.w}*[[scrapy.crawler.]{.pre}]{.sig-prename .descclassname}[[Crawler]{.pre}]{.sig-name .descname}[(]{.sig-paren}*[[spidercls]{.pre}]{.n}*, *[[settings]{.pre}]{.n}*[)]{.sig-paren}[[[\[source\]]{.pre}]{.viewcode-link}](_modules/scrapy/crawler.html#Crawler){.reference .internal}[¶](#scrapy.crawler.Crawler "Permalink to this definition"){.headerlink}

:   The Crawler object must be instantiated with a
    [[`scrapy.Spider`{.xref .py .py-class .docutils .literal
    .notranslate}]{.pre}](index.html#scrapy.Spider "scrapy.Spider"){.reference
    .internal} subclass and a [[`scrapy.settings.Settings`{.xref .py
    .py-class .docutils .literal
    .notranslate}]{.pre}](#scrapy.settings.Settings "scrapy.settings.Settings"){.reference
    .internal} object.

    [[request_fingerprinter]{.pre}]{.sig-name .descname}[¶](#scrapy.crawler.Crawler.request_fingerprinter "Permalink to this definition"){.headerlink}

    :   The request fingerprint builder of this crawler.

        This is used from extensions and middlewares to build short,
        unique identifiers for requests. See [[Request
        fingerprints]{.std
        .std-ref}](index.html#request-fingerprints){.hoverxref .tooltip
        .reference .internal}.

    [[settings]{.pre}]{.sig-name .descname}[¶](#scrapy.crawler.Crawler.settings "Permalink to this definition"){.headerlink}

    :   The settings manager of this crawler.

        This is used by extensions & middlewares to access the Scrapy
        settings of this crawler.

        For an introduction on Scrapy settings see [[Settings]{.std
        .std-ref}](index.html#topics-settings){.hoverxref .tooltip
        .reference .internal}.

        For the API see [[`Settings`{.xref .py .py-class .docutils
        .literal
        .notranslate}]{.pre}](#scrapy.settings.Settings "scrapy.settings.Settings"){.reference
        .internal} class.

    [[signals]{.pre}]{.sig-name .descname}[¶](#scrapy.crawler.Crawler.signals "Permalink to this definition"){.headerlink}

    :   The signals manager of this crawler.

        This is used by extensions & middlewares to hook themselves into
        Scrapy functionality.

        For an introduction on signals see [[Signals]{.std
        .std-ref}](index.html#topics-signals){.hoverxref .tooltip
        .reference .internal}.

        For the API see [[`SignalManager`{.xref .py .py-class .docutils
        .literal
        .notranslate}]{.pre}](#scrapy.signalmanager.SignalManager "scrapy.signalmanager.SignalManager"){.reference
        .internal} class.

    [[stats]{.pre}]{.sig-name .descname}[¶](#scrapy.crawler.Crawler.stats "Permalink to this definition"){.headerlink}

    :   The stats collector of this crawler.

        This is used from extensions & middlewares to record stats of
        their behaviour, or access stats collected by other extensions.

        For an introduction on stats collection see [[Stats
        Collection]{.std .std-ref}](index.html#topics-stats){.hoverxref
        .tooltip .reference .internal}.

        For the API see [[`StatsCollector`{.xref .py .py-class .docutils
        .literal
        .notranslate}]{.pre}](#scrapy.statscollectors.StatsCollector "scrapy.statscollectors.StatsCollector"){.reference
        .internal} class.

    [[extensions]{.pre}]{.sig-name .descname}[¶](#scrapy.crawler.Crawler.extensions "Permalink to this definition"){.headerlink}

    :   The extension manager that keeps track of enabled extensions.

        Most extensions won't need to access this attribute.

        For an introduction on extensions and a list of available
        extensions on Scrapy see [[Extensions]{.std
        .std-ref}](index.html#topics-extensions){.hoverxref .tooltip
        .reference .internal}.

    [[engine]{.pre}]{.sig-name .descname}[¶](#scrapy.crawler.Crawler.engine "Permalink to this definition"){.headerlink}

    :   The execution engine, which coordinates the core crawling logic
        between the scheduler, downloader and spiders.

        Some extension may want to access the Scrapy engine, to inspect
        or modify the downloader and scheduler behaviour, although this
        is an advanced use and this API is not yet stable.

    [[spider]{.pre}]{.sig-name .descname}[¶](#scrapy.crawler.Crawler.spider "Permalink to this definition"){.headerlink}

    :   Spider currently being crawled. This is an instance of the
        spider class provided while constructing the crawler, and it is
        created after the arguments given in the [[`crawl()`{.xref .py
        .py-meth .docutils .literal
        .notranslate}]{.pre}](#scrapy.crawler.Crawler.crawl "scrapy.crawler.Crawler.crawl"){.reference
        .internal} method.

    [[crawl]{.pre}]{.sig-name .descname}[(]{.sig-paren}*[[\*]{.pre}]{.o}[[args]{.pre}]{.n}*, *[[\*\*]{.pre}]{.o}[[kwargs]{.pre}]{.n}*[)]{.sig-paren}[[[\[source\]]{.pre}]{.viewcode-link}](_modules/scrapy/crawler.html#Crawler.crawl){.reference .internal}[¶](#scrapy.crawler.Crawler.crawl "Permalink to this definition"){.headerlink}

    :   Starts the crawler by instantiating its spider class with the
        given [`args`{.docutils .literal .notranslate}]{.pre} and
        [`kwargs`{.docutils .literal .notranslate}]{.pre} arguments,
        while setting the execution engine in motion. Should be called
        only once.

        Returns a deferred that is fired when the crawl is finished.

    [[stop]{.pre}]{.sig-name .descname}[(]{.sig-paren}[)]{.sig-paren} [[→]{.sig-return-icon} [[[Generator]{.pre}](https://docs.python.org/3/library/typing.html#typing.Generator "(in Python v3.12)"){.reference .external}[[\[]{.pre}]{.p}[[Deferred]{.pre}](https://docs.twisted.org/en/stable/api/twisted.internet.defer.Deferred.html "(in Twisted)"){.reference .external}[[,]{.pre}]{.p}[ ]{.w}[[Any]{.pre}](https://docs.python.org/3/library/typing.html#typing.Any "(in Python v3.12)"){.reference .external}[[,]{.pre}]{.p}[ ]{.w}[[None]{.pre}](https://docs.python.org/3/library/constants.html#None "(in Python v3.12)"){.reference .external}[[\]]{.pre}]{.p}]{.sig-return-typehint}]{.sig-return}[[[\[source\]]{.pre}]{.viewcode-link}](_modules/scrapy/crawler.html#Crawler.stop){.reference .internal}[¶](#scrapy.crawler.Crawler.stop "Permalink to this definition"){.headerlink}

    :   Starts a graceful stop of the crawler and returns a deferred
        that is fired when the crawler is stopped.

```{=html}
<!-- -->
```

*[class]{.pre}[ ]{.w}*[[scrapy.crawler.]{.pre}]{.sig-prename .descclassname}[[CrawlerRunner]{.pre}]{.sig-name .descname}[(]{.sig-paren}*[[settings]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[[[Optional]{.pre}](https://docs.python.org/3/library/typing.html#typing.Optional "(in Python v3.12)"){.reference .external}[[\[]{.pre}]{.p}[[Union]{.pre}](https://docs.python.org/3/library/typing.html#typing.Union "(in Python v3.12)"){.reference .external}[[\[]{.pre}]{.p}[[Dict]{.pre}](https://docs.python.org/3/library/typing.html#typing.Dict "(in Python v3.12)"){.reference .external}[[\[]{.pre}]{.p}[[str]{.pre}](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.12)"){.reference .external}[[,]{.pre}]{.p}[ ]{.w}[[Any]{.pre}](https://docs.python.org/3/library/typing.html#typing.Any "(in Python v3.12)"){.reference .external}[[\]]{.pre}]{.p}[[,]{.pre}]{.p}[ ]{.w}[[Settings]{.pre}](index.html#scrapy.settings.Settings "scrapy.settings.Settings"){.reference .internal}[[\]]{.pre}]{.p}[[\]]{.pre}]{.p}]{.n}[ ]{.w}[[=]{.pre}]{.o}[ ]{.w}[[None]{.pre}]{.default_value}*[)]{.sig-paren}[[[\[source\]]{.pre}]{.viewcode-link}](_modules/scrapy/crawler.html#CrawlerRunner){.reference .internal}[¶](#scrapy.crawler.CrawlerRunner "Permalink to this definition"){.headerlink}

:   This is a convenient helper class that keeps track of, manages and
    runs crawlers inside an already setup [[`reactor`{.xref .py .py-mod
    .docutils .literal
    .notranslate}]{.pre}](https://docs.twisted.org/en/stable/api/twisted.internet.reactor.html "(in Twisted)"){.reference
    .external}.

    The CrawlerRunner object must be instantiated with a
    [[`Settings`{.xref .py .py-class .docutils .literal
    .notranslate}]{.pre}](#scrapy.settings.Settings "scrapy.settings.Settings"){.reference
    .internal} object.

    This class shouldn't be needed (since Scrapy is responsible of using
    it accordingly) unless writing scripts that manually handle the
    crawling process. See [[Run Scrapy from a script]{.std
    .std-ref}](index.html#run-from-script){.hoverxref .tooltip
    .reference .internal} for an example.

    [[crawl]{.pre}]{.sig-name .descname}[(]{.sig-paren}*[[crawler_or_spidercls]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[[[Union]{.pre}](https://docs.python.org/3/library/typing.html#typing.Union "(in Python v3.12)"){.reference .external}[[\[]{.pre}]{.p}[[Type]{.pre}](https://docs.python.org/3/library/typing.html#typing.Type "(in Python v3.12)"){.reference .external}[[\[]{.pre}]{.p}[[Spider]{.pre}](index.html#scrapy.spiders.Spider "scrapy.spiders.Spider"){.reference .internal}[[\]]{.pre}]{.p}[[,]{.pre}]{.p}[ ]{.w}[[str]{.pre}](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.12)"){.reference .external}[[,]{.pre}]{.p}[ ]{.w}[[Crawler]{.pre}](index.html#scrapy.crawler.Crawler "scrapy.crawler.Crawler"){.reference .internal}[[\]]{.pre}]{.p}]{.n}*, *[[\*]{.pre}]{.o}[[args]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[[[Any]{.pre}](https://docs.python.org/3/library/typing.html#typing.Any "(in Python v3.12)"){.reference .external}]{.n}*, *[[\*\*]{.pre}]{.o}[[kwargs]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[[[Any]{.pre}](https://docs.python.org/3/library/typing.html#typing.Any "(in Python v3.12)"){.reference .external}]{.n}*[)]{.sig-paren} [[→]{.sig-return-icon} [[[Deferred]{.pre}](https://docs.twisted.org/en/stable/api/twisted.internet.defer.Deferred.html "(in Twisted)"){.reference .external}]{.sig-return-typehint}]{.sig-return}[[[\[source\]]{.pre}]{.viewcode-link}](_modules/scrapy/crawler.html#CrawlerRunner.crawl){.reference .internal}[¶](#scrapy.crawler.CrawlerRunner.crawl "Permalink to this definition"){.headerlink}

    :   Run a crawler with the provided arguments.

        It will call the given Crawler's [[`crawl()`{.xref .py .py-meth
        .docutils .literal
        .notranslate}]{.pre}](#scrapy.crawler.Crawler.crawl "scrapy.crawler.Crawler.crawl"){.reference
        .internal} method, while keeping track of it so it can be
        stopped later.

        If [`crawler_or_spidercls`{.docutils .literal
        .notranslate}]{.pre} isn't a [[`Crawler`{.xref .py .py-class
        .docutils .literal
        .notranslate}]{.pre}](#scrapy.crawler.Crawler "scrapy.crawler.Crawler"){.reference
        .internal} instance, this method will try to create one using
        this parameter as the spider class given to it.

        Returns a deferred that is fired when the crawling is finished.

        Parameters

        :   -   **crawler_or_spidercls** ([[`Crawler`{.xref .py
                .py-class .docutils .literal
                .notranslate}]{.pre}](#scrapy.crawler.Crawler "scrapy.crawler.Crawler"){.reference
                .internal} instance, [[`Spider`{.xref .py .py-class
                .docutils .literal
                .notranslate}]{.pre}](index.html#scrapy.spiders.Spider "scrapy.spiders.Spider"){.reference
                .internal} subclass or string) -- already created
                crawler, or a spider class or spider's name inside the
                project to create it

            -   **args** -- arguments to initialize the spider

            -   **kwargs** -- keyword arguments to initialize the spider

    *[property]{.pre}[ ]{.w}*[[crawlers]{.pre}]{.sig-name .descname}[¶](#scrapy.crawler.CrawlerRunner.crawlers "Permalink to this definition"){.headerlink}

    :   Set of [[`crawlers`{.xref .py .py-class .docutils .literal
        .notranslate}]{.pre}](#scrapy.crawler.Crawler "scrapy.crawler.Crawler"){.reference
        .internal} started by [[`crawl()`{.xref .py .py-meth .docutils
        .literal
        .notranslate}]{.pre}](#scrapy.crawler.CrawlerRunner.crawl "scrapy.crawler.CrawlerRunner.crawl"){.reference
        .internal} and managed by this class.

    [[create_crawler]{.pre}]{.sig-name .descname}[(]{.sig-paren}*[[crawler_or_spidercls]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[[[Union]{.pre}](https://docs.python.org/3/library/typing.html#typing.Union "(in Python v3.12)"){.reference .external}[[\[]{.pre}]{.p}[[Type]{.pre}](https://docs.python.org/3/library/typing.html#typing.Type "(in Python v3.12)"){.reference .external}[[\[]{.pre}]{.p}[[Spider]{.pre}](index.html#scrapy.spiders.Spider "scrapy.spiders.Spider"){.reference .internal}[[\]]{.pre}]{.p}[[,]{.pre}]{.p}[ ]{.w}[[str]{.pre}](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.12)"){.reference .external}[[,]{.pre}]{.p}[ ]{.w}[[Crawler]{.pre}](index.html#scrapy.crawler.Crawler "scrapy.crawler.Crawler"){.reference .internal}[[\]]{.pre}]{.p}]{.n}*[)]{.sig-paren} [[→]{.sig-return-icon} [[[Crawler]{.pre}](index.html#scrapy.crawler.Crawler "scrapy.crawler.Crawler"){.reference .internal}]{.sig-return-typehint}]{.sig-return}[[[\[source\]]{.pre}]{.viewcode-link}](_modules/scrapy/crawler.html#CrawlerRunner.create_crawler){.reference .internal}[¶](#scrapy.crawler.CrawlerRunner.create_crawler "Permalink to this definition"){.headerlink}

    :   Return a [[`Crawler`{.xref .py .py-class .docutils .literal
        .notranslate}]{.pre}](#scrapy.crawler.Crawler "scrapy.crawler.Crawler"){.reference
        .internal} object.

        -   If [`crawler_or_spidercls`{.docutils .literal
            .notranslate}]{.pre} is a Crawler, it is returned as-is.

        -   If [`crawler_or_spidercls`{.docutils .literal
            .notranslate}]{.pre} is a Spider subclass, a new Crawler is
            constructed for it.

        -   If [`crawler_or_spidercls`{.docutils .literal
            .notranslate}]{.pre} is a string, this function finds a
            spider with this name in a Scrapy project (using spider
            loader), then creates a Crawler instance for it.

    [[join]{.pre}]{.sig-name .descname}[(]{.sig-paren}[)]{.sig-paren}[[[\[source\]]{.pre}]{.viewcode-link}](_modules/scrapy/crawler.html#CrawlerRunner.join){.reference .internal}[¶](#scrapy.crawler.CrawlerRunner.join "Permalink to this definition"){.headerlink}

    :   Returns a deferred that is fired when all managed
        [[`crawlers`{.xref .py .py-attr .docutils .literal
        .notranslate}]{.pre}](#scrapy.crawler.CrawlerRunner.crawlers "scrapy.crawler.CrawlerRunner.crawlers"){.reference
        .internal} have completed their executions.

    [[stop]{.pre}]{.sig-name .descname}[(]{.sig-paren}[)]{.sig-paren} [[→]{.sig-return-icon} [[[Deferred]{.pre}](https://docs.twisted.org/en/stable/api/twisted.internet.defer.Deferred.html "(in Twisted)"){.reference .external}]{.sig-return-typehint}]{.sig-return}[[[\[source\]]{.pre}]{.viewcode-link}](_modules/scrapy/crawler.html#CrawlerRunner.stop){.reference .internal}[¶](#scrapy.crawler.CrawlerRunner.stop "Permalink to this definition"){.headerlink}

    :   Stops simultaneously all the crawling jobs taking place.

        Returns a deferred that is fired when they all have ended.

```{=html}
<!-- -->
```

*[class]{.pre}[ ]{.w}*[[scrapy.crawler.]{.pre}]{.sig-prename .descclassname}[[CrawlerProcess]{.pre}]{.sig-name .descname}[(]{.sig-paren}*[[settings]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[[[Optional]{.pre}](https://docs.python.org/3/library/typing.html#typing.Optional "(in Python v3.12)"){.reference .external}[[\[]{.pre}]{.p}[[Union]{.pre}](https://docs.python.org/3/library/typing.html#typing.Union "(in Python v3.12)"){.reference .external}[[\[]{.pre}]{.p}[[Dict]{.pre}](https://docs.python.org/3/library/typing.html#typing.Dict "(in Python v3.12)"){.reference .external}[[\[]{.pre}]{.p}[[str]{.pre}](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.12)"){.reference .external}[[,]{.pre}]{.p}[ ]{.w}[[Any]{.pre}](https://docs.python.org/3/library/typing.html#typing.Any "(in Python v3.12)"){.reference .external}[[\]]{.pre}]{.p}[[,]{.pre}]{.p}[ ]{.w}[[Settings]{.pre}](index.html#scrapy.settings.Settings "scrapy.settings.Settings"){.reference .internal}[[\]]{.pre}]{.p}[[\]]{.pre}]{.p}]{.n}[ ]{.w}[[=]{.pre}]{.o}[ ]{.w}[[None]{.pre}]{.default_value}*, *[[install_root_handler]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[[[bool]{.pre}](https://docs.python.org/3/library/functions.html#bool "(in Python v3.12)"){.reference .external}]{.n}[ ]{.w}[[=]{.pre}]{.o}[ ]{.w}[[True]{.pre}]{.default_value}*[)]{.sig-paren}[[[\[source\]]{.pre}]{.viewcode-link}](_modules/scrapy/crawler.html#CrawlerProcess){.reference .internal}[¶](#scrapy.crawler.CrawlerProcess "Permalink to this definition"){.headerlink}

:   Bases: [[`CrawlerRunner`{.xref .py .py-class .docutils .literal
    .notranslate}]{.pre}](#scrapy.crawler.CrawlerRunner "scrapy.crawler.CrawlerRunner"){.reference
    .internal}

    A class to run multiple scrapy crawlers in a process simultaneously.

    This class extends [[`CrawlerRunner`{.xref .py .py-class .docutils
    .literal
    .notranslate}]{.pre}](#scrapy.crawler.CrawlerRunner "scrapy.crawler.CrawlerRunner"){.reference
    .internal} by adding support for starting a [[`reactor`{.xref .py
    .py-mod .docutils .literal
    .notranslate}]{.pre}](https://docs.twisted.org/en/stable/api/twisted.internet.reactor.html "(in Twisted)"){.reference
    .external} and handling shutdown signals, like the keyboard
    interrupt command Ctrl-C. It also configures top-level logging.

    This utility should be a better fit than [[`CrawlerRunner`{.xref .py
    .py-class .docutils .literal
    .notranslate}]{.pre}](#scrapy.crawler.CrawlerRunner "scrapy.crawler.CrawlerRunner"){.reference
    .internal} if you aren't running another [[`reactor`{.xref .py
    .py-mod .docutils .literal
    .notranslate}]{.pre}](https://docs.twisted.org/en/stable/api/twisted.internet.reactor.html "(in Twisted)"){.reference
    .external} within your application.

    The CrawlerProcess object must be instantiated with a
    [[`Settings`{.xref .py .py-class .docutils .literal
    .notranslate}]{.pre}](#scrapy.settings.Settings "scrapy.settings.Settings"){.reference
    .internal} object.

    Parameters

    :   **install_root_handler** -- whether to install root logging
        handler (default: True)

    This class shouldn't be needed (since Scrapy is responsible of using
    it accordingly) unless writing scripts that manually handle the
    crawling process. See [[Run Scrapy from a script]{.std
    .std-ref}](index.html#run-from-script){.hoverxref .tooltip
    .reference .internal} for an example.

    [[crawl]{.pre}]{.sig-name .descname}[(]{.sig-paren}*[[crawler_or_spidercls]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[[[Union]{.pre}](https://docs.python.org/3/library/typing.html#typing.Union "(in Python v3.12)"){.reference .external}[[\[]{.pre}]{.p}[[Type]{.pre}](https://docs.python.org/3/library/typing.html#typing.Type "(in Python v3.12)"){.reference .external}[[\[]{.pre}]{.p}[[Spider]{.pre}](index.html#scrapy.spiders.Spider "scrapy.spiders.Spider"){.reference .internal}[[\]]{.pre}]{.p}[[,]{.pre}]{.p}[ ]{.w}[[str]{.pre}](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.12)"){.reference .external}[[,]{.pre}]{.p}[ ]{.w}[[Crawler]{.pre}](index.html#scrapy.crawler.Crawler "scrapy.crawler.Crawler"){.reference .internal}[[\]]{.pre}]{.p}]{.n}*, *[[\*]{.pre}]{.o}[[args]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[[[Any]{.pre}](https://docs.python.org/3/library/typing.html#typing.Any "(in Python v3.12)"){.reference .external}]{.n}*, *[[\*\*]{.pre}]{.o}[[kwargs]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[[[Any]{.pre}](https://docs.python.org/3/library/typing.html#typing.Any "(in Python v3.12)"){.reference .external}]{.n}*[)]{.sig-paren} [[→]{.sig-return-icon} [[[Deferred]{.pre}](https://docs.twisted.org/en/stable/api/twisted.internet.defer.Deferred.html "(in Twisted)"){.reference .external}]{.sig-return-typehint}]{.sig-return}[¶](#scrapy.crawler.CrawlerProcess.crawl "Permalink to this definition"){.headerlink}

    :   Run a crawler with the provided arguments.

        It will call the given Crawler's [[`crawl()`{.xref .py .py-meth
        .docutils .literal
        .notranslate}]{.pre}](#scrapy.crawler.Crawler.crawl "scrapy.crawler.Crawler.crawl"){.reference
        .internal} method, while keeping track of it so it can be
        stopped later.

        If [`crawler_or_spidercls`{.docutils .literal
        .notranslate}]{.pre} isn't a [[`Crawler`{.xref .py .py-class
        .docutils .literal
        .notranslate}]{.pre}](#scrapy.crawler.Crawler "scrapy.crawler.Crawler"){.reference
        .internal} instance, this method will try to create one using
        this parameter as the spider class given to it.

        Returns a deferred that is fired when the crawling is finished.

        Parameters

        :   -   **crawler_or_spidercls** ([[`Crawler`{.xref .py
                .py-class .docutils .literal
                .notranslate}]{.pre}](#scrapy.crawler.Crawler "scrapy.crawler.Crawler"){.reference
                .internal} instance, [[`Spider`{.xref .py .py-class
                .docutils .literal
                .notranslate}]{.pre}](index.html#scrapy.spiders.Spider "scrapy.spiders.Spider"){.reference
                .internal} subclass or string) -- already created
                crawler, or a spider class or spider's name inside the
                project to create it

            -   **args** -- arguments to initialize the spider

            -   **kwargs** -- keyword arguments to initialize the spider

    *[property]{.pre}[ ]{.w}*[[crawlers]{.pre}]{.sig-name .descname}[¶](#scrapy.crawler.CrawlerProcess.crawlers "Permalink to this definition"){.headerlink}

    :   Set of [[`crawlers`{.xref .py .py-class .docutils .literal
        .notranslate}]{.pre}](#scrapy.crawler.Crawler "scrapy.crawler.Crawler"){.reference
        .internal} started by [[`crawl()`{.xref .py .py-meth .docutils
        .literal
        .notranslate}]{.pre}](#scrapy.crawler.CrawlerProcess.crawl "scrapy.crawler.CrawlerProcess.crawl"){.reference
        .internal} and managed by this class.

    [[create_crawler]{.pre}]{.sig-name .descname}[(]{.sig-paren}*[[crawler_or_spidercls]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[[[Union]{.pre}](https://docs.python.org/3/library/typing.html#typing.Union "(in Python v3.12)"){.reference .external}[[\[]{.pre}]{.p}[[Type]{.pre}](https://docs.python.org/3/library/typing.html#typing.Type "(in Python v3.12)"){.reference .external}[[\[]{.pre}]{.p}[[Spider]{.pre}](index.html#scrapy.spiders.Spider "scrapy.spiders.Spider"){.reference .internal}[[\]]{.pre}]{.p}[[,]{.pre}]{.p}[ ]{.w}[[str]{.pre}](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.12)"){.reference .external}[[,]{.pre}]{.p}[ ]{.w}[[Crawler]{.pre}](index.html#scrapy.crawler.Crawler "scrapy.crawler.Crawler"){.reference .internal}[[\]]{.pre}]{.p}]{.n}*[)]{.sig-paren} [[→]{.sig-return-icon} [[[Crawler]{.pre}](index.html#scrapy.crawler.Crawler "scrapy.crawler.Crawler"){.reference .internal}]{.sig-return-typehint}]{.sig-return}[¶](#scrapy.crawler.CrawlerProcess.create_crawler "Permalink to this definition"){.headerlink}

    :   Return a [[`Crawler`{.xref .py .py-class .docutils .literal
        .notranslate}]{.pre}](#scrapy.crawler.Crawler "scrapy.crawler.Crawler"){.reference
        .internal} object.

        -   If [`crawler_or_spidercls`{.docutils .literal
            .notranslate}]{.pre} is a Crawler, it is returned as-is.

        -   If [`crawler_or_spidercls`{.docutils .literal
            .notranslate}]{.pre} is a Spider subclass, a new Crawler is
            constructed for it.

        -   If [`crawler_or_spidercls`{.docutils .literal
            .notranslate}]{.pre} is a string, this function finds a
            spider with this name in a Scrapy project (using spider
            loader), then creates a Crawler instance for it.

    [[join]{.pre}]{.sig-name .descname}[(]{.sig-paren}[)]{.sig-paren}[¶](#scrapy.crawler.CrawlerProcess.join "Permalink to this definition"){.headerlink}

    :   Returns a deferred that is fired when all managed
        [[`crawlers`{.xref .py .py-attr .docutils .literal
        .notranslate}]{.pre}](#scrapy.crawler.CrawlerProcess.crawlers "scrapy.crawler.CrawlerProcess.crawlers"){.reference
        .internal} have completed their executions.

    [[start]{.pre}]{.sig-name .descname}[(]{.sig-paren}*[[stop_after_crawl]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[[[bool]{.pre}](https://docs.python.org/3/library/functions.html#bool "(in Python v3.12)"){.reference .external}]{.n}[ ]{.w}[[=]{.pre}]{.o}[ ]{.w}[[True]{.pre}]{.default_value}*, *[[install_signal_handlers]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[[[bool]{.pre}](https://docs.python.org/3/library/functions.html#bool "(in Python v3.12)"){.reference .external}]{.n}[ ]{.w}[[=]{.pre}]{.o}[ ]{.w}[[True]{.pre}]{.default_value}*[)]{.sig-paren} [[→]{.sig-return-icon} [[[None]{.pre}](https://docs.python.org/3/library/constants.html#None "(in Python v3.12)"){.reference .external}]{.sig-return-typehint}]{.sig-return}[[[\[source\]]{.pre}]{.viewcode-link}](_modules/scrapy/crawler.html#CrawlerProcess.start){.reference .internal}[¶](#scrapy.crawler.CrawlerProcess.start "Permalink to this definition"){.headerlink}

    :   This method starts a [[`reactor`{.xref .py .py-mod .docutils
        .literal
        .notranslate}]{.pre}](https://docs.twisted.org/en/stable/api/twisted.internet.reactor.html "(in Twisted)"){.reference
        .external}, adjusts its pool size to
        [[`REACTOR_THREADPOOL_MAXSIZE`{.xref .std .std-setting .docutils
        .literal
        .notranslate}]{.pre}](index.html#std-setting-REACTOR_THREADPOOL_MAXSIZE){.hoverxref
        .tooltip .reference .internal}, and installs a DNS cache based
        on [[`DNSCACHE_ENABLED`{.xref .std .std-setting .docutils
        .literal
        .notranslate}]{.pre}](index.html#std-setting-DNSCACHE_ENABLED){.hoverxref
        .tooltip .reference .internal} and [[`DNSCACHE_SIZE`{.xref .std
        .std-setting .docutils .literal
        .notranslate}]{.pre}](index.html#std-setting-DNSCACHE_SIZE){.hoverxref
        .tooltip .reference .internal}.

        If [`stop_after_crawl`{.docutils .literal .notranslate}]{.pre}
        is True, the reactor will be stopped after all crawlers have
        finished, using [[`join()`{.xref .py .py-meth .docutils .literal
        .notranslate}]{.pre}](#scrapy.crawler.CrawlerProcess.join "scrapy.crawler.CrawlerProcess.join"){.reference
        .internal}.

        Parameters

        :   -   **stop_after_crawl**
                ([*bool*](https://docs.python.org/3/library/functions.html#bool "(in Python v3.12)"){.reference
                .external}) -- stop or not the reactor when all crawlers
                have finished

            -   **install_signal_handlers**
                ([*bool*](https://docs.python.org/3/library/functions.html#bool "(in Python v3.12)"){.reference
                .external}) -- whether to install the OS signal handlers
                from Twisted and Scrapy (default: True)

    [[stop]{.pre}]{.sig-name .descname}[(]{.sig-paren}[)]{.sig-paren} [[→]{.sig-return-icon} [[[Deferred]{.pre}](https://docs.twisted.org/en/stable/api/twisted.internet.defer.Deferred.html "(in Twisted)"){.reference .external}]{.sig-return-typehint}]{.sig-return}[¶](#scrapy.crawler.CrawlerProcess.stop "Permalink to this definition"){.headerlink}

    :   Stops simultaneously all the crawling jobs taking place.

        Returns a deferred that is fired when they all have ended.
:::

::: {#module-scrapy.settings .section}
[]{#settings-api}[]{#topics-api-settings}

#### Settings API[¶](#module-scrapy.settings "Permalink to this heading"){.headerlink}

[[scrapy.settings.]{.pre}]{.sig-prename .descclassname}[[SETTINGS_PRIORITIES]{.pre}]{.sig-name .descname}[¶](#scrapy.settings.SETTINGS_PRIORITIES "Permalink to this definition"){.headerlink}

:   Dictionary that sets the key name and priority level of the default
    settings priorities used in Scrapy.

    Each item defines a settings entry point, giving it a code name for
    identification and an integer priority. Greater priorities take more
    precedence over lesser ones when setting and retrieving values in
    the [[`Settings`{.xref .py .py-class .docutils .literal
    .notranslate}]{.pre}](#scrapy.settings.Settings "scrapy.settings.Settings"){.reference
    .internal} class.

    ::: {.highlight-python .notranslate}
    ::: highlight
        SETTINGS_PRIORITIES = {
            "default": 0,
            "command": 10,
            "addon": 15,
            "project": 20,
            "spider": 30,
            "cmdline": 40,
        }
    :::
    :::

    For a detailed explanation on each settings sources, see:
    [[Settings]{.std .std-ref}](index.html#topics-settings){.hoverxref
    .tooltip .reference .internal}.

```{=html}
<!-- -->
```

[[scrapy.settings.]{.pre}]{.sig-prename .descclassname}[[get_settings_priority]{.pre}]{.sig-name .descname}[(]{.sig-paren}*[[priority]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[[[Union]{.pre}](https://docs.python.org/3/library/typing.html#typing.Union "(in Python v3.12)"){.reference .external}[[\[]{.pre}]{.p}[[int]{.pre}](https://docs.python.org/3/library/functions.html#int "(in Python v3.12)"){.reference .external}[[,]{.pre}]{.p}[ ]{.w}[[str]{.pre}](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.12)"){.reference .external}[[\]]{.pre}]{.p}]{.n}*[)]{.sig-paren} [[→]{.sig-return-icon} [[[int]{.pre}](https://docs.python.org/3/library/functions.html#int "(in Python v3.12)"){.reference .external}]{.sig-return-typehint}]{.sig-return}[[[\[source\]]{.pre}]{.viewcode-link}](_modules/scrapy/settings.html#get_settings_priority){.reference .internal}[¶](#scrapy.settings.get_settings_priority "Permalink to this definition"){.headerlink}

:   Small helper function that looks up a given string priority in the
    [[`SETTINGS_PRIORITIES`{.xref .py .py-attr .docutils .literal
    .notranslate}]{.pre}](#scrapy.settings.SETTINGS_PRIORITIES "scrapy.settings.SETTINGS_PRIORITIES"){.reference
    .internal} dictionary and returns its numerical value, or directly
    returns a given numerical priority.

```{=html}
<!-- -->
```

*[class]{.pre}[ ]{.w}*[[scrapy.settings.]{.pre}]{.sig-prename .descclassname}[[Settings]{.pre}]{.sig-name .descname}[(]{.sig-paren}*[[values]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[[\_SettingsInputT]{.pre}]{.n}[ ]{.w}[[=]{.pre}]{.o}[ ]{.w}[[None]{.pre}]{.default_value}*, *[[priority]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[[Union]{.pre}[[\[]{.pre}]{.p}[[int]{.pre}](https://docs.python.org/3/library/functions.html#int "(in Python v3.12)"){.reference .external}[[,]{.pre}]{.p}[ ]{.w}[[str]{.pre}](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.12)"){.reference .external}[[\]]{.pre}]{.p}]{.n}[ ]{.w}[[=]{.pre}]{.o}[ ]{.w}[[\'project\']{.pre}]{.default_value}*[)]{.sig-paren}[[[\[source\]]{.pre}]{.viewcode-link}](_modules/scrapy/settings.html#Settings){.reference .internal}[¶](#scrapy.settings.Settings "Permalink to this definition"){.headerlink}

:   Bases: [[`BaseSettings`{.xref .py .py-class .docutils .literal
    .notranslate}]{.pre}](#scrapy.settings.BaseSettings "scrapy.settings.BaseSettings"){.reference
    .internal}

    This object stores Scrapy settings for the configuration of internal
    components, and can be used for any further customization.

    It is a direct subclass and supports all methods of
    [[`BaseSettings`{.xref .py .py-class .docutils .literal
    .notranslate}]{.pre}](#scrapy.settings.BaseSettings "scrapy.settings.BaseSettings"){.reference
    .internal}. Additionally, after instantiation of this class, the new
    object will have the global default settings described on [[Built-in
    settings reference]{.std
    .std-ref}](index.html#topics-settings-ref){.hoverxref .tooltip
    .reference .internal} already populated.

```{=html}
<!-- -->
```

*[class]{.pre}[ ]{.w}*[[scrapy.settings.]{.pre}]{.sig-prename .descclassname}[[BaseSettings]{.pre}]{.sig-name .descname}[(]{.sig-paren}*[[values]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[[\_SettingsInputT]{.pre}]{.n}[ ]{.w}[[=]{.pre}]{.o}[ ]{.w}[[None]{.pre}]{.default_value}*, *[[priority]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[[Union]{.pre}[[\[]{.pre}]{.p}[[int]{.pre}](https://docs.python.org/3/library/functions.html#int "(in Python v3.12)"){.reference .external}[[,]{.pre}]{.p}[ ]{.w}[[str]{.pre}](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.12)"){.reference .external}[[\]]{.pre}]{.p}]{.n}[ ]{.w}[[=]{.pre}]{.o}[ ]{.w}[[\'project\']{.pre}]{.default_value}*[)]{.sig-paren}[[[\[source\]]{.pre}]{.viewcode-link}](_modules/scrapy/settings.html#BaseSettings){.reference .internal}[¶](#scrapy.settings.BaseSettings "Permalink to this definition"){.headerlink}

:   Instances of this class behave like dictionaries, but store
    priorities along with their [`(key,`{.docutils .literal
    .notranslate}]{.pre}` `{.docutils .literal
    .notranslate}[`value)`{.docutils .literal .notranslate}]{.pre}
    pairs, and can be frozen (i.e. marked immutable).

    Key-value entries can be passed on initialization with the
    [`values`{.docutils .literal .notranslate}]{.pre} argument, and they
    would take the [`priority`{.docutils .literal .notranslate}]{.pre}
    level (unless [`values`{.docutils .literal .notranslate}]{.pre} is
    already an instance of [[`BaseSettings`{.xref .py .py-class
    .docutils .literal
    .notranslate}]{.pre}](#scrapy.settings.BaseSettings "scrapy.settings.BaseSettings"){.reference
    .internal}, in which case the existing priority levels will be
    kept). If the [`priority`{.docutils .literal .notranslate}]{.pre}
    argument is a string, the priority name will be looked up in
    [[`SETTINGS_PRIORITIES`{.xref .py .py-attr .docutils .literal
    .notranslate}]{.pre}](#scrapy.settings.SETTINGS_PRIORITIES "scrapy.settings.SETTINGS_PRIORITIES"){.reference
    .internal}. Otherwise, a specific integer should be provided.

    Once the object is created, new settings can be loaded or updated
    with the [[`set()`{.xref .py .py-meth .docutils .literal
    .notranslate}]{.pre}](#scrapy.settings.BaseSettings.set "scrapy.settings.BaseSettings.set"){.reference
    .internal} method, and can be accessed with the square bracket
    notation of dictionaries, or with the [[`get()`{.xref .py .py-meth
    .docutils .literal
    .notranslate}]{.pre}](#scrapy.settings.BaseSettings.get "scrapy.settings.BaseSettings.get"){.reference
    .internal} method of the instance and its value conversion variants.
    When requesting a stored key, the value with the highest priority
    will be retrieved.

    [[copy]{.pre}]{.sig-name .descname}[(]{.sig-paren}[)]{.sig-paren} [[→]{.sig-return-icon} [[Self]{.pre}]{.sig-return-typehint}]{.sig-return}[[[\[source\]]{.pre}]{.viewcode-link}](_modules/scrapy/settings.html#BaseSettings.copy){.reference .internal}[¶](#scrapy.settings.BaseSettings.copy "Permalink to this definition"){.headerlink}

    :   Make a deep copy of current settings.

        This method returns a new instance of the [[`Settings`{.xref .py
        .py-class .docutils .literal
        .notranslate}]{.pre}](#scrapy.settings.Settings "scrapy.settings.Settings"){.reference
        .internal} class, populated with the same values and their
        priorities.

        Modifications to the new object won't be reflected on the
        original settings.

    [[copy_to_dict]{.pre}]{.sig-name .descname}[(]{.sig-paren}[)]{.sig-paren} [[→]{.sig-return-icon} [[[Dict]{.pre}](https://docs.python.org/3/library/typing.html#typing.Dict "(in Python v3.12)"){.reference .external}[[\[]{.pre}]{.p}[[Optional]{.pre}](https://docs.python.org/3/library/typing.html#typing.Optional "(in Python v3.12)"){.reference .external}[[\[]{.pre}]{.p}[[Union]{.pre}](https://docs.python.org/3/library/typing.html#typing.Union "(in Python v3.12)"){.reference .external}[[\[]{.pre}]{.p}[[bool]{.pre}](https://docs.python.org/3/library/functions.html#bool "(in Python v3.12)"){.reference .external}[[,]{.pre}]{.p}[ ]{.w}[[float]{.pre}](https://docs.python.org/3/library/functions.html#float "(in Python v3.12)"){.reference .external}[[,]{.pre}]{.p}[ ]{.w}[[int]{.pre}](https://docs.python.org/3/library/functions.html#int "(in Python v3.12)"){.reference .external}[[,]{.pre}]{.p}[ ]{.w}[[str]{.pre}](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.12)"){.reference .external}[[\]]{.pre}]{.p}[[\]]{.pre}]{.p}[[,]{.pre}]{.p}[ ]{.w}[[Any]{.pre}](https://docs.python.org/3/library/typing.html#typing.Any "(in Python v3.12)"){.reference .external}[[\]]{.pre}]{.p}]{.sig-return-typehint}]{.sig-return}[[[\[source\]]{.pre}]{.viewcode-link}](_modules/scrapy/settings.html#BaseSettings.copy_to_dict){.reference .internal}[¶](#scrapy.settings.BaseSettings.copy_to_dict "Permalink to this definition"){.headerlink}

    :   Make a copy of current settings and convert to a dict.

        This method returns a new dict populated with the same values
        and their priorities as the current settings.

        Modifications to the returned dict won't be reflected on the
        original settings.

        This method can be useful for example for printing settings in
        Scrapy shell.

    [[freeze]{.pre}]{.sig-name .descname}[(]{.sig-paren}[)]{.sig-paren} [[→]{.sig-return-icon} [[[None]{.pre}](https://docs.python.org/3/library/constants.html#None "(in Python v3.12)"){.reference .external}]{.sig-return-typehint}]{.sig-return}[[[\[source\]]{.pre}]{.viewcode-link}](_modules/scrapy/settings.html#BaseSettings.freeze){.reference .internal}[¶](#scrapy.settings.BaseSettings.freeze "Permalink to this definition"){.headerlink}

    :   Disable further changes to the current settings.

        After calling this method, the present state of the settings
        will become immutable. Trying to change values through the
        [[`set()`{.xref .py .py-meth .docutils .literal
        .notranslate}]{.pre}](#scrapy.settings.BaseSettings.set "scrapy.settings.BaseSettings.set"){.reference
        .internal} method and its variants won't be possible and will be
        alerted.

    [[frozencopy]{.pre}]{.sig-name .descname}[(]{.sig-paren}[)]{.sig-paren} [[→]{.sig-return-icon} [[Self]{.pre}]{.sig-return-typehint}]{.sig-return}[[[\[source\]]{.pre}]{.viewcode-link}](_modules/scrapy/settings.html#BaseSettings.frozencopy){.reference .internal}[¶](#scrapy.settings.BaseSettings.frozencopy "Permalink to this definition"){.headerlink}

    :   Return an immutable copy of the current settings.

        Alias for a [[`freeze()`{.xref .py .py-meth .docutils .literal
        .notranslate}]{.pre}](#scrapy.settings.BaseSettings.freeze "scrapy.settings.BaseSettings.freeze"){.reference
        .internal} call in the object returned by [[`copy()`{.xref .py
        .py-meth .docutils .literal
        .notranslate}]{.pre}](#scrapy.settings.BaseSettings.copy "scrapy.settings.BaseSettings.copy"){.reference
        .internal}.

    [[get]{.pre}]{.sig-name .descname}[(]{.sig-paren}*[[name]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[[[Optional]{.pre}](https://docs.python.org/3/library/typing.html#typing.Optional "(in Python v3.12)"){.reference .external}[[\[]{.pre}]{.p}[[Union]{.pre}](https://docs.python.org/3/library/typing.html#typing.Union "(in Python v3.12)"){.reference .external}[[\[]{.pre}]{.p}[[bool]{.pre}](https://docs.python.org/3/library/functions.html#bool "(in Python v3.12)"){.reference .external}[[,]{.pre}]{.p}[ ]{.w}[[float]{.pre}](https://docs.python.org/3/library/functions.html#float "(in Python v3.12)"){.reference .external}[[,]{.pre}]{.p}[ ]{.w}[[int]{.pre}](https://docs.python.org/3/library/functions.html#int "(in Python v3.12)"){.reference .external}[[,]{.pre}]{.p}[ ]{.w}[[str]{.pre}](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.12)"){.reference .external}[[\]]{.pre}]{.p}[[\]]{.pre}]{.p}]{.n}*, *[[default]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[[[Any]{.pre}](https://docs.python.org/3/library/typing.html#typing.Any "(in Python v3.12)"){.reference .external}]{.n}[ ]{.w}[[=]{.pre}]{.o}[ ]{.w}[[None]{.pre}]{.default_value}*[)]{.sig-paren} [[→]{.sig-return-icon} [[[Any]{.pre}](https://docs.python.org/3/library/typing.html#typing.Any "(in Python v3.12)"){.reference .external}]{.sig-return-typehint}]{.sig-return}[[[\[source\]]{.pre}]{.viewcode-link}](_modules/scrapy/settings.html#BaseSettings.get){.reference .internal}[¶](#scrapy.settings.BaseSettings.get "Permalink to this definition"){.headerlink}

    :   Get a setting value without affecting its original type.

        Parameters

        :   -   **name**
                ([*str*](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.12)"){.reference
                .external}) -- the setting name

            -   **default**
                ([*object*](https://docs.python.org/3/library/functions.html#object "(in Python v3.12)"){.reference
                .external}) -- the value to return if no setting is
                found

    [[getbool]{.pre}]{.sig-name .descname}[(]{.sig-paren}*[[name]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[[[Optional]{.pre}](https://docs.python.org/3/library/typing.html#typing.Optional "(in Python v3.12)"){.reference .external}[[\[]{.pre}]{.p}[[Union]{.pre}](https://docs.python.org/3/library/typing.html#typing.Union "(in Python v3.12)"){.reference .external}[[\[]{.pre}]{.p}[[bool]{.pre}](https://docs.python.org/3/library/functions.html#bool "(in Python v3.12)"){.reference .external}[[,]{.pre}]{.p}[ ]{.w}[[float]{.pre}](https://docs.python.org/3/library/functions.html#float "(in Python v3.12)"){.reference .external}[[,]{.pre}]{.p}[ ]{.w}[[int]{.pre}](https://docs.python.org/3/library/functions.html#int "(in Python v3.12)"){.reference .external}[[,]{.pre}]{.p}[ ]{.w}[[str]{.pre}](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.12)"){.reference .external}[[\]]{.pre}]{.p}[[\]]{.pre}]{.p}]{.n}*, *[[default]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[[[bool]{.pre}](https://docs.python.org/3/library/functions.html#bool "(in Python v3.12)"){.reference .external}]{.n}[ ]{.w}[[=]{.pre}]{.o}[ ]{.w}[[False]{.pre}]{.default_value}*[)]{.sig-paren} [[→]{.sig-return-icon} [[[bool]{.pre}](https://docs.python.org/3/library/functions.html#bool "(in Python v3.12)"){.reference .external}]{.sig-return-typehint}]{.sig-return}[[[\[source\]]{.pre}]{.viewcode-link}](_modules/scrapy/settings.html#BaseSettings.getbool){.reference .internal}[¶](#scrapy.settings.BaseSettings.getbool "Permalink to this definition"){.headerlink}

    :   Get a setting value as a boolean.

        [`1`{.docutils .literal .notranslate}]{.pre}, [`'1'`{.docutils
        .literal .notranslate}]{.pre}, True\` and [`'True'`{.docutils
        .literal .notranslate}]{.pre} return [`True`{.docutils .literal
        .notranslate}]{.pre}, while [`0`{.docutils .literal
        .notranslate}]{.pre}, [`'0'`{.docutils .literal
        .notranslate}]{.pre}, [`False`{.docutils .literal
        .notranslate}]{.pre}, [`'False'`{.docutils .literal
        .notranslate}]{.pre} and [`None`{.docutils .literal
        .notranslate}]{.pre} return [`False`{.docutils .literal
        .notranslate}]{.pre}.

        For example, settings populated through environment variables
        set to [`'0'`{.docutils .literal .notranslate}]{.pre} will
        return [`False`{.docutils .literal .notranslate}]{.pre} when
        using this method.

        Parameters

        :   -   **name**
                ([*str*](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.12)"){.reference
                .external}) -- the setting name

            -   **default**
                ([*object*](https://docs.python.org/3/library/functions.html#object "(in Python v3.12)"){.reference
                .external}) -- the value to return if no setting is
                found

    [[getdict]{.pre}]{.sig-name .descname}[(]{.sig-paren}*[[name]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[[[Optional]{.pre}](https://docs.python.org/3/library/typing.html#typing.Optional "(in Python v3.12)"){.reference .external}[[\[]{.pre}]{.p}[[Union]{.pre}](https://docs.python.org/3/library/typing.html#typing.Union "(in Python v3.12)"){.reference .external}[[\[]{.pre}]{.p}[[bool]{.pre}](https://docs.python.org/3/library/functions.html#bool "(in Python v3.12)"){.reference .external}[[,]{.pre}]{.p}[ ]{.w}[[float]{.pre}](https://docs.python.org/3/library/functions.html#float "(in Python v3.12)"){.reference .external}[[,]{.pre}]{.p}[ ]{.w}[[int]{.pre}](https://docs.python.org/3/library/functions.html#int "(in Python v3.12)"){.reference .external}[[,]{.pre}]{.p}[ ]{.w}[[str]{.pre}](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.12)"){.reference .external}[[\]]{.pre}]{.p}[[\]]{.pre}]{.p}]{.n}*, *[[default]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[[[Optional]{.pre}](https://docs.python.org/3/library/typing.html#typing.Optional "(in Python v3.12)"){.reference .external}[[\[]{.pre}]{.p}[[Dict]{.pre}](https://docs.python.org/3/library/typing.html#typing.Dict "(in Python v3.12)"){.reference .external}[[\[]{.pre}]{.p}[[Any]{.pre}](https://docs.python.org/3/library/typing.html#typing.Any "(in Python v3.12)"){.reference .external}[[,]{.pre}]{.p}[ ]{.w}[[Any]{.pre}](https://docs.python.org/3/library/typing.html#typing.Any "(in Python v3.12)"){.reference .external}[[\]]{.pre}]{.p}[[\]]{.pre}]{.p}]{.n}[ ]{.w}[[=]{.pre}]{.o}[ ]{.w}[[None]{.pre}]{.default_value}*[)]{.sig-paren} [[→]{.sig-return-icon} [[[Dict]{.pre}](https://docs.python.org/3/library/typing.html#typing.Dict "(in Python v3.12)"){.reference .external}[[\[]{.pre}]{.p}[[Any]{.pre}](https://docs.python.org/3/library/typing.html#typing.Any "(in Python v3.12)"){.reference .external}[[,]{.pre}]{.p}[ ]{.w}[[Any]{.pre}](https://docs.python.org/3/library/typing.html#typing.Any "(in Python v3.12)"){.reference .external}[[\]]{.pre}]{.p}]{.sig-return-typehint}]{.sig-return}[[[\[source\]]{.pre}]{.viewcode-link}](_modules/scrapy/settings.html#BaseSettings.getdict){.reference .internal}[¶](#scrapy.settings.BaseSettings.getdict "Permalink to this definition"){.headerlink}

    :   Get a setting value as a dictionary. If the setting original
        type is a dictionary, a copy of it will be returned. If it is a
        string it will be evaluated as a JSON dictionary. In the case
        that it is a [[`BaseSettings`{.xref .py .py-class .docutils
        .literal
        .notranslate}]{.pre}](#scrapy.settings.BaseSettings "scrapy.settings.BaseSettings"){.reference
        .internal} instance itself, it will be converted to a
        dictionary, containing all its current settings values as they
        would be returned by [[`get()`{.xref .py .py-meth .docutils
        .literal
        .notranslate}]{.pre}](#scrapy.settings.BaseSettings.get "scrapy.settings.BaseSettings.get"){.reference
        .internal}, and losing all information about priority and
        mutability.

        Parameters

        :   -   **name**
                ([*str*](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.12)"){.reference
                .external}) -- the setting name

            -   **default**
                ([*object*](https://docs.python.org/3/library/functions.html#object "(in Python v3.12)"){.reference
                .external}) -- the value to return if no setting is
                found

    [[getdictorlist]{.pre}]{.sig-name .descname}[(]{.sig-paren}*[[name]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[[[Optional]{.pre}](https://docs.python.org/3/library/typing.html#typing.Optional "(in Python v3.12)"){.reference .external}[[\[]{.pre}]{.p}[[Union]{.pre}](https://docs.python.org/3/library/typing.html#typing.Union "(in Python v3.12)"){.reference .external}[[\[]{.pre}]{.p}[[bool]{.pre}](https://docs.python.org/3/library/functions.html#bool "(in Python v3.12)"){.reference .external}[[,]{.pre}]{.p}[ ]{.w}[[float]{.pre}](https://docs.python.org/3/library/functions.html#float "(in Python v3.12)"){.reference .external}[[,]{.pre}]{.p}[ ]{.w}[[int]{.pre}](https://docs.python.org/3/library/functions.html#int "(in Python v3.12)"){.reference .external}[[,]{.pre}]{.p}[ ]{.w}[[str]{.pre}](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.12)"){.reference .external}[[\]]{.pre}]{.p}[[\]]{.pre}]{.p}]{.n}*, *[[default]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[[[Optional]{.pre}](https://docs.python.org/3/library/typing.html#typing.Optional "(in Python v3.12)"){.reference .external}[[\[]{.pre}]{.p}[[Union]{.pre}](https://docs.python.org/3/library/typing.html#typing.Union "(in Python v3.12)"){.reference .external}[[\[]{.pre}]{.p}[[Dict]{.pre}](https://docs.python.org/3/library/typing.html#typing.Dict "(in Python v3.12)"){.reference .external}[[\[]{.pre}]{.p}[[Any]{.pre}](https://docs.python.org/3/library/typing.html#typing.Any "(in Python v3.12)"){.reference .external}[[,]{.pre}]{.p}[ ]{.w}[[Any]{.pre}](https://docs.python.org/3/library/typing.html#typing.Any "(in Python v3.12)"){.reference .external}[[\]]{.pre}]{.p}[[,]{.pre}]{.p}[ ]{.w}[[List]{.pre}](https://docs.python.org/3/library/typing.html#typing.List "(in Python v3.12)"){.reference .external}[[\[]{.pre}]{.p}[[Any]{.pre}](https://docs.python.org/3/library/typing.html#typing.Any "(in Python v3.12)"){.reference .external}[[\]]{.pre}]{.p}[[,]{.pre}]{.p}[ ]{.w}[[Tuple]{.pre}](https://docs.python.org/3/library/typing.html#typing.Tuple "(in Python v3.12)"){.reference .external}[[\[]{.pre}]{.p}[[Any]{.pre}](https://docs.python.org/3/library/typing.html#typing.Any "(in Python v3.12)"){.reference .external}[[\]]{.pre}]{.p}[[\]]{.pre}]{.p}[[\]]{.pre}]{.p}]{.n}[ ]{.w}[[=]{.pre}]{.o}[ ]{.w}[[None]{.pre}]{.default_value}*[)]{.sig-paren} [[→]{.sig-return-icon} [[[Union]{.pre}](https://docs.python.org/3/library/typing.html#typing.Union "(in Python v3.12)"){.reference .external}[[\[]{.pre}]{.p}[[Dict]{.pre}](https://docs.python.org/3/library/typing.html#typing.Dict "(in Python v3.12)"){.reference .external}[[\[]{.pre}]{.p}[[Any]{.pre}](https://docs.python.org/3/library/typing.html#typing.Any "(in Python v3.12)"){.reference .external}[[,]{.pre}]{.p}[ ]{.w}[[Any]{.pre}](https://docs.python.org/3/library/typing.html#typing.Any "(in Python v3.12)"){.reference .external}[[\]]{.pre}]{.p}[[,]{.pre}]{.p}[ ]{.w}[[List]{.pre}](https://docs.python.org/3/library/typing.html#typing.List "(in Python v3.12)"){.reference .external}[[\[]{.pre}]{.p}[[Any]{.pre}](https://docs.python.org/3/library/typing.html#typing.Any "(in Python v3.12)"){.reference .external}[[\]]{.pre}]{.p}[[\]]{.pre}]{.p}]{.sig-return-typehint}]{.sig-return}[[[\[source\]]{.pre}]{.viewcode-link}](_modules/scrapy/settings.html#BaseSettings.getdictorlist){.reference .internal}[¶](#scrapy.settings.BaseSettings.getdictorlist "Permalink to this definition"){.headerlink}

    :   Get a setting value as either a [[`dict`{.xref .py .py-class
        .docutils .literal
        .notranslate}]{.pre}](https://docs.python.org/3/library/stdtypes.html#dict "(in Python v3.12)"){.reference
        .external} or a [[`list`{.xref .py .py-class .docutils .literal
        .notranslate}]{.pre}](https://docs.python.org/3/library/stdtypes.html#list "(in Python v3.12)"){.reference
        .external}.

        If the setting is already a dict or a list, a copy of it will be
        returned.

        If it is a string it will be evaluated as JSON, or as a
        comma-separated list of strings as a fallback.

        For example, settings populated from the command line will
        return:

        -   [`{'key1':`{.docutils .literal
            .notranslate}]{.pre}` `{.docutils .literal
            .notranslate}[`'value1',`{.docutils .literal
            .notranslate}]{.pre}` `{.docutils .literal
            .notranslate}[`'key2':`{.docutils .literal
            .notranslate}]{.pre}` `{.docutils .literal
            .notranslate}[`'value2'}`{.docutils .literal
            .notranslate}]{.pre} if set to [`'{"key1":`{.docutils
            .literal .notranslate}]{.pre}` `{.docutils .literal
            .notranslate}[`"value1",`{.docutils .literal
            .notranslate}]{.pre}` `{.docutils .literal
            .notranslate}[`"key2":`{.docutils .literal
            .notranslate}]{.pre}` `{.docutils .literal
            .notranslate}[`"value2"}'`{.docutils .literal
            .notranslate}]{.pre}

        -   [`['one',`{.docutils .literal
            .notranslate}]{.pre}` `{.docutils .literal
            .notranslate}[`'two']`{.docutils .literal
            .notranslate}]{.pre} if set to [`'["one",`{.docutils
            .literal .notranslate}]{.pre}` `{.docutils .literal
            .notranslate}[`"two"]'`{.docutils .literal
            .notranslate}]{.pre} or [`'one,two'`{.docutils .literal
            .notranslate}]{.pre}

        Parameters

        :   -   **name** (*string*) -- the setting name

            -   **default** (*any*) -- the value to return if no setting
                is found

    [[getfloat]{.pre}]{.sig-name .descname}[(]{.sig-paren}*[[name]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[[[Optional]{.pre}](https://docs.python.org/3/library/typing.html#typing.Optional "(in Python v3.12)"){.reference .external}[[\[]{.pre}]{.p}[[Union]{.pre}](https://docs.python.org/3/library/typing.html#typing.Union "(in Python v3.12)"){.reference .external}[[\[]{.pre}]{.p}[[bool]{.pre}](https://docs.python.org/3/library/functions.html#bool "(in Python v3.12)"){.reference .external}[[,]{.pre}]{.p}[ ]{.w}[[float]{.pre}](https://docs.python.org/3/library/functions.html#float "(in Python v3.12)"){.reference .external}[[,]{.pre}]{.p}[ ]{.w}[[int]{.pre}](https://docs.python.org/3/library/functions.html#int "(in Python v3.12)"){.reference .external}[[,]{.pre}]{.p}[ ]{.w}[[str]{.pre}](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.12)"){.reference .external}[[\]]{.pre}]{.p}[[\]]{.pre}]{.p}]{.n}*, *[[default]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[[[float]{.pre}](https://docs.python.org/3/library/functions.html#float "(in Python v3.12)"){.reference .external}]{.n}[ ]{.w}[[=]{.pre}]{.o}[ ]{.w}[[0.0]{.pre}]{.default_value}*[)]{.sig-paren} [[→]{.sig-return-icon} [[[float]{.pre}](https://docs.python.org/3/library/functions.html#float "(in Python v3.12)"){.reference .external}]{.sig-return-typehint}]{.sig-return}[[[\[source\]]{.pre}]{.viewcode-link}](_modules/scrapy/settings.html#BaseSettings.getfloat){.reference .internal}[¶](#scrapy.settings.BaseSettings.getfloat "Permalink to this definition"){.headerlink}

    :   Get a setting value as a float.

        Parameters

        :   -   **name**
                ([*str*](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.12)"){.reference
                .external}) -- the setting name

            -   **default**
                ([*object*](https://docs.python.org/3/library/functions.html#object "(in Python v3.12)"){.reference
                .external}) -- the value to return if no setting is
                found

    [[getint]{.pre}]{.sig-name .descname}[(]{.sig-paren}*[[name]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[[[Optional]{.pre}](https://docs.python.org/3/library/typing.html#typing.Optional "(in Python v3.12)"){.reference .external}[[\[]{.pre}]{.p}[[Union]{.pre}](https://docs.python.org/3/library/typing.html#typing.Union "(in Python v3.12)"){.reference .external}[[\[]{.pre}]{.p}[[bool]{.pre}](https://docs.python.org/3/library/functions.html#bool "(in Python v3.12)"){.reference .external}[[,]{.pre}]{.p}[ ]{.w}[[float]{.pre}](https://docs.python.org/3/library/functions.html#float "(in Python v3.12)"){.reference .external}[[,]{.pre}]{.p}[ ]{.w}[[int]{.pre}](https://docs.python.org/3/library/functions.html#int "(in Python v3.12)"){.reference .external}[[,]{.pre}]{.p}[ ]{.w}[[str]{.pre}](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.12)"){.reference .external}[[\]]{.pre}]{.p}[[\]]{.pre}]{.p}]{.n}*, *[[default]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[[[int]{.pre}](https://docs.python.org/3/library/functions.html#int "(in Python v3.12)"){.reference .external}]{.n}[ ]{.w}[[=]{.pre}]{.o}[ ]{.w}[[0]{.pre}]{.default_value}*[)]{.sig-paren} [[→]{.sig-return-icon} [[[int]{.pre}](https://docs.python.org/3/library/functions.html#int "(in Python v3.12)"){.reference .external}]{.sig-return-typehint}]{.sig-return}[[[\[source\]]{.pre}]{.viewcode-link}](_modules/scrapy/settings.html#BaseSettings.getint){.reference .internal}[¶](#scrapy.settings.BaseSettings.getint "Permalink to this definition"){.headerlink}

    :   Get a setting value as an int.

        Parameters

        :   -   **name**
                ([*str*](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.12)"){.reference
                .external}) -- the setting name

            -   **default**
                ([*object*](https://docs.python.org/3/library/functions.html#object "(in Python v3.12)"){.reference
                .external}) -- the value to return if no setting is
                found

    [[getlist]{.pre}]{.sig-name .descname}[(]{.sig-paren}*[[name]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[[[Optional]{.pre}](https://docs.python.org/3/library/typing.html#typing.Optional "(in Python v3.12)"){.reference .external}[[\[]{.pre}]{.p}[[Union]{.pre}](https://docs.python.org/3/library/typing.html#typing.Union "(in Python v3.12)"){.reference .external}[[\[]{.pre}]{.p}[[bool]{.pre}](https://docs.python.org/3/library/functions.html#bool "(in Python v3.12)"){.reference .external}[[,]{.pre}]{.p}[ ]{.w}[[float]{.pre}](https://docs.python.org/3/library/functions.html#float "(in Python v3.12)"){.reference .external}[[,]{.pre}]{.p}[ ]{.w}[[int]{.pre}](https://docs.python.org/3/library/functions.html#int "(in Python v3.12)"){.reference .external}[[,]{.pre}]{.p}[ ]{.w}[[str]{.pre}](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.12)"){.reference .external}[[\]]{.pre}]{.p}[[\]]{.pre}]{.p}]{.n}*, *[[default]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[[[Optional]{.pre}](https://docs.python.org/3/library/typing.html#typing.Optional "(in Python v3.12)"){.reference .external}[[\[]{.pre}]{.p}[[List]{.pre}](https://docs.python.org/3/library/typing.html#typing.List "(in Python v3.12)"){.reference .external}[[\[]{.pre}]{.p}[[Any]{.pre}](https://docs.python.org/3/library/typing.html#typing.Any "(in Python v3.12)"){.reference .external}[[\]]{.pre}]{.p}[[\]]{.pre}]{.p}]{.n}[ ]{.w}[[=]{.pre}]{.o}[ ]{.w}[[None]{.pre}]{.default_value}*[)]{.sig-paren} [[→]{.sig-return-icon} [[[List]{.pre}](https://docs.python.org/3/library/typing.html#typing.List "(in Python v3.12)"){.reference .external}[[\[]{.pre}]{.p}[[Any]{.pre}](https://docs.python.org/3/library/typing.html#typing.Any "(in Python v3.12)"){.reference .external}[[\]]{.pre}]{.p}]{.sig-return-typehint}]{.sig-return}[[[\[source\]]{.pre}]{.viewcode-link}](_modules/scrapy/settings.html#BaseSettings.getlist){.reference .internal}[¶](#scrapy.settings.BaseSettings.getlist "Permalink to this definition"){.headerlink}

    :   Get a setting value as a list. If the setting original type is a
        list, a copy of it will be returned. If it's a string it will be
        split by ",".

        For example, settings populated through environment variables
        set to [`'one,two'`{.docutils .literal .notranslate}]{.pre} will
        return a list \['one', 'two'\] when using this method.

        Parameters

        :   -   **name**
                ([*str*](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.12)"){.reference
                .external}) -- the setting name

            -   **default**
                ([*object*](https://docs.python.org/3/library/functions.html#object "(in Python v3.12)"){.reference
                .external}) -- the value to return if no setting is
                found

    [[getpriority]{.pre}]{.sig-name .descname}[(]{.sig-paren}*[[name]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[[[Optional]{.pre}](https://docs.python.org/3/library/typing.html#typing.Optional "(in Python v3.12)"){.reference .external}[[\[]{.pre}]{.p}[[Union]{.pre}](https://docs.python.org/3/library/typing.html#typing.Union "(in Python v3.12)"){.reference .external}[[\[]{.pre}]{.p}[[bool]{.pre}](https://docs.python.org/3/library/functions.html#bool "(in Python v3.12)"){.reference .external}[[,]{.pre}]{.p}[ ]{.w}[[float]{.pre}](https://docs.python.org/3/library/functions.html#float "(in Python v3.12)"){.reference .external}[[,]{.pre}]{.p}[ ]{.w}[[int]{.pre}](https://docs.python.org/3/library/functions.html#int "(in Python v3.12)"){.reference .external}[[,]{.pre}]{.p}[ ]{.w}[[str]{.pre}](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.12)"){.reference .external}[[\]]{.pre}]{.p}[[\]]{.pre}]{.p}]{.n}*[)]{.sig-paren} [[→]{.sig-return-icon} [[[Optional]{.pre}](https://docs.python.org/3/library/typing.html#typing.Optional "(in Python v3.12)"){.reference .external}[[\[]{.pre}]{.p}[[int]{.pre}](https://docs.python.org/3/library/functions.html#int "(in Python v3.12)"){.reference .external}[[\]]{.pre}]{.p}]{.sig-return-typehint}]{.sig-return}[[[\[source\]]{.pre}]{.viewcode-link}](_modules/scrapy/settings.html#BaseSettings.getpriority){.reference .internal}[¶](#scrapy.settings.BaseSettings.getpriority "Permalink to this definition"){.headerlink}

    :   Return the current numerical priority value of a setting, or
        [`None`{.docutils .literal .notranslate}]{.pre} if the given
        [`name`{.docutils .literal .notranslate}]{.pre} does not exist.

        Parameters

        :   **name**
            ([*str*](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.12)"){.reference
            .external}) -- the setting name

    [[getwithbase]{.pre}]{.sig-name .descname}[(]{.sig-paren}*[[name]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[[[Optional]{.pre}](https://docs.python.org/3/library/typing.html#typing.Optional "(in Python v3.12)"){.reference .external}[[\[]{.pre}]{.p}[[Union]{.pre}](https://docs.python.org/3/library/typing.html#typing.Union "(in Python v3.12)"){.reference .external}[[\[]{.pre}]{.p}[[bool]{.pre}](https://docs.python.org/3/library/functions.html#bool "(in Python v3.12)"){.reference .external}[[,]{.pre}]{.p}[ ]{.w}[[float]{.pre}](https://docs.python.org/3/library/functions.html#float "(in Python v3.12)"){.reference .external}[[,]{.pre}]{.p}[ ]{.w}[[int]{.pre}](https://docs.python.org/3/library/functions.html#int "(in Python v3.12)"){.reference .external}[[,]{.pre}]{.p}[ ]{.w}[[str]{.pre}](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.12)"){.reference .external}[[\]]{.pre}]{.p}[[\]]{.pre}]{.p}]{.n}*[)]{.sig-paren} [[→]{.sig-return-icon} [[[BaseSettings]{.pre}](index.html#scrapy.settings.BaseSettings "scrapy.settings.BaseSettings"){.reference .internal}]{.sig-return-typehint}]{.sig-return}[[[\[source\]]{.pre}]{.viewcode-link}](_modules/scrapy/settings.html#BaseSettings.getwithbase){.reference .internal}[¶](#scrapy.settings.BaseSettings.getwithbase "Permalink to this definition"){.headerlink}

    :   Get a composition of a dictionary-like setting and its \_BASE
        counterpart.

        Parameters

        :   **name**
            ([*str*](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.12)"){.reference
            .external}) -- name of the dictionary-like setting

    [[maxpriority]{.pre}]{.sig-name .descname}[(]{.sig-paren}[)]{.sig-paren} [[→]{.sig-return-icon} [[[int]{.pre}](https://docs.python.org/3/library/functions.html#int "(in Python v3.12)"){.reference .external}]{.sig-return-typehint}]{.sig-return}[[[\[source\]]{.pre}]{.viewcode-link}](_modules/scrapy/settings.html#BaseSettings.maxpriority){.reference .internal}[¶](#scrapy.settings.BaseSettings.maxpriority "Permalink to this definition"){.headerlink}

    :   Return the numerical value of the highest priority present
        throughout all settings, or the numerical value for
        [`default`{.docutils .literal .notranslate}]{.pre} from
        [[`SETTINGS_PRIORITIES`{.xref .py .py-attr .docutils .literal
        .notranslate}]{.pre}](#scrapy.settings.SETTINGS_PRIORITIES "scrapy.settings.SETTINGS_PRIORITIES"){.reference
        .internal} if there are no settings stored.

    [[pop]{.pre}]{.sig-name .descname}[(]{.sig-paren}*[[k]{.pre}]{.n}*[\[]{.optional}, *[[d]{.pre}]{.n}*[\]]{.optional}[)]{.sig-paren} [[→]{.sig-return-icon} [[v,]{.pre} [remove]{.pre} [specified]{.pre} [key]{.pre} [and]{.pre} [return]{.pre} [the]{.pre} [corresponding]{.pre} [value.]{.pre}]{.sig-return-typehint}]{.sig-return}[[[\[source\]]{.pre}]{.viewcode-link}](_modules/scrapy/settings.html#BaseSettings.pop){.reference .internal}[¶](#scrapy.settings.BaseSettings.pop "Permalink to this definition"){.headerlink}

    :   If key is not found, d is returned if given, otherwise KeyError
        is raised.

    [[set]{.pre}]{.sig-name .descname}[(]{.sig-paren}*[[name]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[[[Optional]{.pre}](https://docs.python.org/3/library/typing.html#typing.Optional "(in Python v3.12)"){.reference .external}[[\[]{.pre}]{.p}[[Union]{.pre}](https://docs.python.org/3/library/typing.html#typing.Union "(in Python v3.12)"){.reference .external}[[\[]{.pre}]{.p}[[bool]{.pre}](https://docs.python.org/3/library/functions.html#bool "(in Python v3.12)"){.reference .external}[[,]{.pre}]{.p}[ ]{.w}[[float]{.pre}](https://docs.python.org/3/library/functions.html#float "(in Python v3.12)"){.reference .external}[[,]{.pre}]{.p}[ ]{.w}[[int]{.pre}](https://docs.python.org/3/library/functions.html#int "(in Python v3.12)"){.reference .external}[[,]{.pre}]{.p}[ ]{.w}[[str]{.pre}](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.12)"){.reference .external}[[\]]{.pre}]{.p}[[\]]{.pre}]{.p}]{.n}*, *[[value]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[[[Any]{.pre}](https://docs.python.org/3/library/typing.html#typing.Any "(in Python v3.12)"){.reference .external}]{.n}*, *[[priority]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[[[Union]{.pre}](https://docs.python.org/3/library/typing.html#typing.Union "(in Python v3.12)"){.reference .external}[[\[]{.pre}]{.p}[[int]{.pre}](https://docs.python.org/3/library/functions.html#int "(in Python v3.12)"){.reference .external}[[,]{.pre}]{.p}[ ]{.w}[[str]{.pre}](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.12)"){.reference .external}[[\]]{.pre}]{.p}]{.n}[ ]{.w}[[=]{.pre}]{.o}[ ]{.w}[[\'project\']{.pre}]{.default_value}*[)]{.sig-paren} [[→]{.sig-return-icon} [[[None]{.pre}](https://docs.python.org/3/library/constants.html#None "(in Python v3.12)"){.reference .external}]{.sig-return-typehint}]{.sig-return}[[[\[source\]]{.pre}]{.viewcode-link}](_modules/scrapy/settings.html#BaseSettings.set){.reference .internal}[¶](#scrapy.settings.BaseSettings.set "Permalink to this definition"){.headerlink}

    :   Store a key/value attribute with a given priority.

        Settings should be populated *before* configuring the Crawler
        object (through the [`configure()`{.xref .py .py-meth .docutils
        .literal .notranslate}]{.pre} method), otherwise they won't have
        any effect.

        Parameters

        :   -   **name**
                ([*str*](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.12)"){.reference
                .external}) -- the setting name

            -   **value**
                ([*object*](https://docs.python.org/3/library/functions.html#object "(in Python v3.12)"){.reference
                .external}) -- the value to associate with the setting

            -   **priority**
                ([*str*](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.12)"){.reference
                .external} *or*
                [*int*](https://docs.python.org/3/library/functions.html#int "(in Python v3.12)"){.reference
                .external}) -- the priority of the setting. Should be a
                key of [[`SETTINGS_PRIORITIES`{.xref .py .py-attr
                .docutils .literal
                .notranslate}]{.pre}](#scrapy.settings.SETTINGS_PRIORITIES "scrapy.settings.SETTINGS_PRIORITIES"){.reference
                .internal} or an integer

    [[setdefault]{.pre}]{.sig-name .descname}[(]{.sig-paren}*[[k]{.pre}]{.n}*[\[]{.optional}, *[[d]{.pre}]{.n}*[\]]{.optional}[)]{.sig-paren} [[→]{.sig-return-icon} [[D.get(k,d),]{.pre} [also]{.pre} [set]{.pre} [D\[k\]=d]{.pre} [if]{.pre} [k]{.pre} [not]{.pre} [in]{.pre} [D]{.pre}]{.sig-return-typehint}]{.sig-return}[[[\[source\]]{.pre}]{.viewcode-link}](_modules/scrapy/settings.html#BaseSettings.setdefault){.reference .internal}[¶](#scrapy.settings.BaseSettings.setdefault "Permalink to this definition"){.headerlink}

    :   

    [[setmodule]{.pre}]{.sig-name .descname}[(]{.sig-paren}*[[module]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[[[Union]{.pre}](https://docs.python.org/3/library/typing.html#typing.Union "(in Python v3.12)"){.reference .external}[[\[]{.pre}]{.p}[module]{.pre}[[,]{.pre}]{.p}[ ]{.w}[[str]{.pre}](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.12)"){.reference .external}[[\]]{.pre}]{.p}]{.n}*, *[[priority]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[[[Union]{.pre}](https://docs.python.org/3/library/typing.html#typing.Union "(in Python v3.12)"){.reference .external}[[\[]{.pre}]{.p}[[int]{.pre}](https://docs.python.org/3/library/functions.html#int "(in Python v3.12)"){.reference .external}[[,]{.pre}]{.p}[ ]{.w}[[str]{.pre}](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.12)"){.reference .external}[[\]]{.pre}]{.p}]{.n}[ ]{.w}[[=]{.pre}]{.o}[ ]{.w}[[\'project\']{.pre}]{.default_value}*[)]{.sig-paren} [[→]{.sig-return-icon} [[[None]{.pre}](https://docs.python.org/3/library/constants.html#None "(in Python v3.12)"){.reference .external}]{.sig-return-typehint}]{.sig-return}[[[\[source\]]{.pre}]{.viewcode-link}](_modules/scrapy/settings.html#BaseSettings.setmodule){.reference .internal}[¶](#scrapy.settings.BaseSettings.setmodule "Permalink to this definition"){.headerlink}

    :   Store settings from a module with a given priority.

        This is a helper function that calls [[`set()`{.xref .py
        .py-meth .docutils .literal
        .notranslate}]{.pre}](#scrapy.settings.BaseSettings.set "scrapy.settings.BaseSettings.set"){.reference
        .internal} for every globally declared uppercase variable of
        [`module`{.docutils .literal .notranslate}]{.pre} with the
        provided [`priority`{.docutils .literal .notranslate}]{.pre}.

        Parameters

        :   -   **module**
                ([*types.ModuleType*](https://docs.python.org/3/library/types.html#types.ModuleType "(in Python v3.12)"){.reference
                .external} *or*
                [*str*](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.12)"){.reference
                .external}) -- the module or the path of the module

            -   **priority**
                ([*str*](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.12)"){.reference
                .external} *or*
                [*int*](https://docs.python.org/3/library/functions.html#int "(in Python v3.12)"){.reference
                .external}) -- the priority of the settings. Should be a
                key of [[`SETTINGS_PRIORITIES`{.xref .py .py-attr
                .docutils .literal
                .notranslate}]{.pre}](#scrapy.settings.SETTINGS_PRIORITIES "scrapy.settings.SETTINGS_PRIORITIES"){.reference
                .internal} or an integer

    [[update]{.pre}]{.sig-name .descname}[(]{.sig-paren}*[[values]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[[\_SettingsInputT]{.pre}]{.n}*, *[[priority]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[[Union]{.pre}[[\[]{.pre}]{.p}[[int]{.pre}](https://docs.python.org/3/library/functions.html#int "(in Python v3.12)"){.reference .external}[[,]{.pre}]{.p}[ ]{.w}[[str]{.pre}](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.12)"){.reference .external}[[\]]{.pre}]{.p}]{.n}[ ]{.w}[[=]{.pre}]{.o}[ ]{.w}[[\'project\']{.pre}]{.default_value}*[)]{.sig-paren} [[→]{.sig-return-icon} [[[None]{.pre}](https://docs.python.org/3/library/constants.html#None "(in Python v3.12)"){.reference .external}]{.sig-return-typehint}]{.sig-return}[[[\[source\]]{.pre}]{.viewcode-link}](_modules/scrapy/settings.html#BaseSettings.update){.reference .internal}[¶](#scrapy.settings.BaseSettings.update "Permalink to this definition"){.headerlink}

    :   Store key/value pairs with a given priority.

        This is a helper function that calls [[`set()`{.xref .py
        .py-meth .docutils .literal
        .notranslate}]{.pre}](#scrapy.settings.BaseSettings.set "scrapy.settings.BaseSettings.set"){.reference
        .internal} for every item of [`values`{.docutils .literal
        .notranslate}]{.pre} with the provided [`priority`{.docutils
        .literal .notranslate}]{.pre}.

        If [`values`{.docutils .literal .notranslate}]{.pre} is a
        string, it is assumed to be JSON-encoded and parsed into a dict
        with [`json.loads()`{.docutils .literal .notranslate}]{.pre}
        first. If it is a [[`BaseSettings`{.xref .py .py-class .docutils
        .literal
        .notranslate}]{.pre}](#scrapy.settings.BaseSettings "scrapy.settings.BaseSettings"){.reference
        .internal} instance, the per-key priorities will be used and the
        [`priority`{.docutils .literal .notranslate}]{.pre} parameter
        ignored. This allows inserting/updating settings with different
        priorities with a single command.

        Parameters

        :   -   **values** (dict or string or [[`BaseSettings`{.xref .py
                .py-class .docutils .literal
                .notranslate}]{.pre}](#scrapy.settings.BaseSettings "scrapy.settings.BaseSettings"){.reference
                .internal}) -- the settings names and values

            -   **priority**
                ([*str*](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.12)"){.reference
                .external} *or*
                [*int*](https://docs.python.org/3/library/functions.html#int "(in Python v3.12)"){.reference
                .external}) -- the priority of the settings. Should be a
                key of [[`SETTINGS_PRIORITIES`{.xref .py .py-attr
                .docutils .literal
                .notranslate}]{.pre}](#scrapy.settings.SETTINGS_PRIORITIES "scrapy.settings.SETTINGS_PRIORITIES"){.reference
                .internal} or an integer
:::

::: {#module-scrapy.spiderloader .section}
[]{#spiderloader-api}[]{#topics-api-spiderloader}

#### SpiderLoader API[¶](#module-scrapy.spiderloader "Permalink to this heading"){.headerlink}

*[class]{.pre}[ ]{.w}*[[scrapy.spiderloader.]{.pre}]{.sig-prename .descclassname}[[SpiderLoader]{.pre}]{.sig-name .descname}[[[\[source\]]{.pre}]{.viewcode-link}](_modules/scrapy/spiderloader.html#SpiderLoader){.reference .internal}[¶](#scrapy.spiderloader.SpiderLoader "Permalink to this definition"){.headerlink}

:   This class is in charge of retrieving and handling the spider
    classes defined across the project.

    Custom spider loaders can be employed by specifying their path in
    the [[`SPIDER_LOADER_CLASS`{.xref .std .std-setting .docutils
    .literal
    .notranslate}]{.pre}](index.html#std-setting-SPIDER_LOADER_CLASS){.hoverxref
    .tooltip .reference .internal} project setting. They must fully
    implement the [`scrapy.interfaces.ISpiderLoader`{.xref .py .py-class
    .docutils .literal .notranslate}]{.pre} interface to guarantee an
    errorless execution.

    [[from_settings]{.pre}]{.sig-name .descname}[(]{.sig-paren}*[[settings]{.pre}]{.n}*[)]{.sig-paren}[[[\[source\]]{.pre}]{.viewcode-link}](_modules/scrapy/spiderloader.html#SpiderLoader.from_settings){.reference .internal}[¶](#scrapy.spiderloader.SpiderLoader.from_settings "Permalink to this definition"){.headerlink}

    :   This class method is used by Scrapy to create an instance of the
        class. It's called with the current project settings, and it
        loads the spiders found recursively in the modules of the
        [[`SPIDER_MODULES`{.xref .std .std-setting .docutils .literal
        .notranslate}]{.pre}](index.html#std-setting-SPIDER_MODULES){.hoverxref
        .tooltip .reference .internal} setting.

        Parameters

        :   **settings** ([[`Settings`{.xref .py .py-class .docutils
            .literal
            .notranslate}]{.pre}](#scrapy.settings.Settings "scrapy.settings.Settings"){.reference
            .internal} instance) -- project settings

    [[load]{.pre}]{.sig-name .descname}[(]{.sig-paren}*[[spider_name]{.pre}]{.n}*[)]{.sig-paren}[[[\[source\]]{.pre}]{.viewcode-link}](_modules/scrapy/spiderloader.html#SpiderLoader.load){.reference .internal}[¶](#scrapy.spiderloader.SpiderLoader.load "Permalink to this definition"){.headerlink}

    :   Get the Spider class with the given name. It'll look into the
        previously loaded spiders for a spider class with name
        [`spider_name`{.docutils .literal .notranslate}]{.pre} and will
        raise a KeyError if not found.

        Parameters

        :   **spider_name**
            ([*str*](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.12)"){.reference
            .external}) -- spider class name

    [[list]{.pre}]{.sig-name .descname}[(]{.sig-paren}[)]{.sig-paren}[[[\[source\]]{.pre}]{.viewcode-link}](_modules/scrapy/spiderloader.html#SpiderLoader.list){.reference .internal}[¶](#scrapy.spiderloader.SpiderLoader.list "Permalink to this definition"){.headerlink}

    :   Get the names of the available spiders in the project.

    [[find_by_request]{.pre}]{.sig-name .descname}[(]{.sig-paren}*[[request]{.pre}]{.n}*[)]{.sig-paren}[[[\[source\]]{.pre}]{.viewcode-link}](_modules/scrapy/spiderloader.html#SpiderLoader.find_by_request){.reference .internal}[¶](#scrapy.spiderloader.SpiderLoader.find_by_request "Permalink to this definition"){.headerlink}

    :   List the spiders' names that can handle the given request. Will
        try to match the request's url against the domains of the
        spiders.

        Parameters

        :   **request** ([`Request`{.xref .py .py-class .docutils
            .literal .notranslate}]{.pre} instance) -- queried request
:::

::: {#module-scrapy.signalmanager .section}
[]{#signals-api}[]{#topics-api-signals}

#### Signals API[¶](#module-scrapy.signalmanager "Permalink to this heading"){.headerlink}

*[class]{.pre}[ ]{.w}*[[scrapy.signalmanager.]{.pre}]{.sig-prename .descclassname}[[SignalManager]{.pre}]{.sig-name .descname}[(]{.sig-paren}*[[sender]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[[[Any]{.pre}](https://docs.python.org/3/library/typing.html#typing.Any "(in Python v3.12)"){.reference .external}]{.n}[ ]{.w}[[=]{.pre}]{.o}[ ]{.w}[[\_Anonymous]{.pre}]{.default_value}*[)]{.sig-paren}[[[\[source\]]{.pre}]{.viewcode-link}](_modules/scrapy/signalmanager.html#SignalManager){.reference .internal}[¶](#scrapy.signalmanager.SignalManager "Permalink to this definition"){.headerlink}

:   

    [[connect]{.pre}]{.sig-name .descname}[(]{.sig-paren}*[[receiver]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[[[Any]{.pre}](https://docs.python.org/3/library/typing.html#typing.Any "(in Python v3.12)"){.reference .external}]{.n}*, *[[signal]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[[[Any]{.pre}](https://docs.python.org/3/library/typing.html#typing.Any "(in Python v3.12)"){.reference .external}]{.n}*, *[[\*\*]{.pre}]{.o}[[kwargs]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[[[Any]{.pre}](https://docs.python.org/3/library/typing.html#typing.Any "(in Python v3.12)"){.reference .external}]{.n}*[)]{.sig-paren} [[→]{.sig-return-icon} [[[None]{.pre}](https://docs.python.org/3/library/constants.html#None "(in Python v3.12)"){.reference .external}]{.sig-return-typehint}]{.sig-return}[[[\[source\]]{.pre}]{.viewcode-link}](_modules/scrapy/signalmanager.html#SignalManager.connect){.reference .internal}[¶](#scrapy.signalmanager.SignalManager.connect "Permalink to this definition"){.headerlink}

    :   Connect a receiver function to a signal.

        The signal can be any object, although Scrapy comes with some
        predefined signals that are documented in the [[Signals]{.std
        .std-ref}](index.html#topics-signals){.hoverxref .tooltip
        .reference .internal} section.

        Parameters

        :   -   **receiver**
                ([*collections.abc.Callable*](https://docs.python.org/3/library/collections.abc.html#collections.abc.Callable "(in Python v3.12)"){.reference
                .external}) -- the function to be connected

            -   **signal**
                ([*object*](https://docs.python.org/3/library/functions.html#object "(in Python v3.12)"){.reference
                .external}) -- the signal to connect to

    [[disconnect]{.pre}]{.sig-name .descname}[(]{.sig-paren}*[[receiver]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[[[Any]{.pre}](https://docs.python.org/3/library/typing.html#typing.Any "(in Python v3.12)"){.reference .external}]{.n}*, *[[signal]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[[[Any]{.pre}](https://docs.python.org/3/library/typing.html#typing.Any "(in Python v3.12)"){.reference .external}]{.n}*, *[[\*\*]{.pre}]{.o}[[kwargs]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[[[Any]{.pre}](https://docs.python.org/3/library/typing.html#typing.Any "(in Python v3.12)"){.reference .external}]{.n}*[)]{.sig-paren} [[→]{.sig-return-icon} [[[None]{.pre}](https://docs.python.org/3/library/constants.html#None "(in Python v3.12)"){.reference .external}]{.sig-return-typehint}]{.sig-return}[[[\[source\]]{.pre}]{.viewcode-link}](_modules/scrapy/signalmanager.html#SignalManager.disconnect){.reference .internal}[¶](#scrapy.signalmanager.SignalManager.disconnect "Permalink to this definition"){.headerlink}

    :   Disconnect a receiver function from a signal. This has the
        opposite effect of the [[`connect()`{.xref .py .py-meth
        .docutils .literal
        .notranslate}]{.pre}](#scrapy.signalmanager.SignalManager.connect "scrapy.signalmanager.SignalManager.connect"){.reference
        .internal} method, and the arguments are the same.

    [[disconnect_all]{.pre}]{.sig-name .descname}[(]{.sig-paren}*[[signal]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[[[Any]{.pre}](https://docs.python.org/3/library/typing.html#typing.Any "(in Python v3.12)"){.reference .external}]{.n}*, *[[\*\*]{.pre}]{.o}[[kwargs]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[[[Any]{.pre}](https://docs.python.org/3/library/typing.html#typing.Any "(in Python v3.12)"){.reference .external}]{.n}*[)]{.sig-paren} [[→]{.sig-return-icon} [[[None]{.pre}](https://docs.python.org/3/library/constants.html#None "(in Python v3.12)"){.reference .external}]{.sig-return-typehint}]{.sig-return}[[[\[source\]]{.pre}]{.viewcode-link}](_modules/scrapy/signalmanager.html#SignalManager.disconnect_all){.reference .internal}[¶](#scrapy.signalmanager.SignalManager.disconnect_all "Permalink to this definition"){.headerlink}

    :   Disconnect all receivers from the given signal.

        Parameters

        :   **signal**
            ([*object*](https://docs.python.org/3/library/functions.html#object "(in Python v3.12)"){.reference
            .external}) -- the signal to disconnect from

    [[send_catch_log]{.pre}]{.sig-name .descname}[(]{.sig-paren}*[[signal]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[[[Any]{.pre}](https://docs.python.org/3/library/typing.html#typing.Any "(in Python v3.12)"){.reference .external}]{.n}*, *[[\*\*]{.pre}]{.o}[[kwargs]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[[[Any]{.pre}](https://docs.python.org/3/library/typing.html#typing.Any "(in Python v3.12)"){.reference .external}]{.n}*[)]{.sig-paren} [[→]{.sig-return-icon} [[[List]{.pre}](https://docs.python.org/3/library/typing.html#typing.List "(in Python v3.12)"){.reference .external}[[\[]{.pre}]{.p}[[Tuple]{.pre}](https://docs.python.org/3/library/typing.html#typing.Tuple "(in Python v3.12)"){.reference .external}[[\[]{.pre}]{.p}[[Any]{.pre}](https://docs.python.org/3/library/typing.html#typing.Any "(in Python v3.12)"){.reference .external}[[,]{.pre}]{.p}[ ]{.w}[[Any]{.pre}](https://docs.python.org/3/library/typing.html#typing.Any "(in Python v3.12)"){.reference .external}[[\]]{.pre}]{.p}[[\]]{.pre}]{.p}]{.sig-return-typehint}]{.sig-return}[[[\[source\]]{.pre}]{.viewcode-link}](_modules/scrapy/signalmanager.html#SignalManager.send_catch_log){.reference .internal}[¶](#scrapy.signalmanager.SignalManager.send_catch_log "Permalink to this definition"){.headerlink}

    :   Send a signal, catch exceptions and log them.

        The keyword arguments are passed to the signal handlers
        (connected through the [[`connect()`{.xref .py .py-meth
        .docutils .literal
        .notranslate}]{.pre}](#scrapy.signalmanager.SignalManager.connect "scrapy.signalmanager.SignalManager.connect"){.reference
        .internal} method).

    [[send_catch_log_deferred]{.pre}]{.sig-name .descname}[(]{.sig-paren}*[[signal]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[[[Any]{.pre}](https://docs.python.org/3/library/typing.html#typing.Any "(in Python v3.12)"){.reference .external}]{.n}*, *[[\*\*]{.pre}]{.o}[[kwargs]{.pre}]{.n}[[:]{.pre}]{.p}[ ]{.w}[[[Any]{.pre}](https://docs.python.org/3/library/typing.html#typing.Any "(in Python v3.12)"){.reference .external}]{.n}*[)]{.sig-paren} [[→]{.sig-return-icon} [[[Deferred]{.pre}](https://docs.twisted.org/en/stable/api/twisted.internet.defer.Deferred.html "(in Twisted)"){.reference .external}]{.sig-return-typehint}]{.sig-return}[[[\[source\]]{.pre}]{.viewcode-link}](_modules/scrapy/signalmanager.html#SignalManager.send_catch_log_deferred){.reference .internal}[¶](#scrapy.signalmanager.SignalManager.send_catch_log_deferred "Permalink to this definition"){.headerlink}

    :   Like [[`send_catch_log()`{.xref .py .py-meth .docutils .literal
        .notranslate}]{.pre}](#scrapy.signalmanager.SignalManager.send_catch_log "scrapy.signalmanager.SignalManager.send_catch_log"){.reference
        .internal} but supports returning [[`Deferred`{.xref .py
        .py-class .docutils .literal
        .notranslate}]{.pre}](https://docs.twisted.org/en/stable/api/twisted.internet.defer.Deferred.html "(in Twisted)"){.reference
        .external} objects from signal handlers.

        Returns a Deferred that gets fired once all signal handlers
        deferreds were fired. Send a signal, catch exceptions and log
        them.

        The keyword arguments are passed to the signal handlers
        (connected through the [[`connect()`{.xref .py .py-meth
        .docutils .literal
        .notranslate}]{.pre}](#scrapy.signalmanager.SignalManager.connect "scrapy.signalmanager.SignalManager.connect"){.reference
        .internal} method).
:::

::: {#stats-collector-api .section}
[]{#topics-api-stats}

#### Stats Collector API[¶](#stats-collector-api "Permalink to this heading"){.headerlink}

There are several Stats Collectors available under the
[[`scrapy.statscollectors`{.xref .py .py-mod .docutils .literal
.notranslate}]{.pre}](#module-scrapy.statscollectors "scrapy.statscollectors: Stats Collectors"){.reference
.internal} module and they all implement the Stats Collector API defined
by the [[`StatsCollector`{.xref .py .py-class .docutils .literal
.notranslate}]{.pre}](#scrapy.statscollectors.StatsCollector "scrapy.statscollectors.StatsCollector"){.reference
.internal} class (which they all inherit from).

[]{#module-scrapy.statscollectors .target}

*[class]{.pre}[ ]{.w}*[[scrapy.statscollectors.]{.pre}]{.sig-prename .descclassname}[[StatsCollector]{.pre}]{.sig-name .descname}[[[\[source\]]{.pre}]{.viewcode-link}](_modules/scrapy/statscollectors.html#StatsCollector){.reference .internal}[¶](#scrapy.statscollectors.StatsCollector "Permalink to this definition"){.headerlink}

:   

    [[get_value]{.pre}]{.sig-name .descname}[(]{.sig-paren}*[[key]{.pre}]{.n}*, *[[default]{.pre}]{.n}[[=]{.pre}]{.o}[[None]{.pre}]{.default_value}*[)]{.sig-paren}[[[\[source\]]{.pre}]{.viewcode-link}](_modules/scrapy/statscollectors.html#StatsCollector.get_value){.reference .internal}[¶](#scrapy.statscollectors.StatsCollector.get_value "Permalink to this definition"){.headerlink}

    :   Return the value for the given stats key or default if it
        doesn't exist.

    [[get_stats]{.pre}]{.sig-name .descname}[(]{.sig-paren}[)]{.sig-paren}[[[\[source\]]{.pre}]{.viewcode-link}](_modules/scrapy/statscollectors.html#StatsCollector.get_stats){.reference .internal}[¶](#scrapy.statscollectors.StatsCollector.get_stats "Permalink to this definition"){.headerlink}

    :   Get all stats from the currently running spider as a dict.

    [[set_value]{.pre}]{.sig-name .descname}[(]{.sig-paren}*[[key]{.pre}]{.n}*, *[[value]{.pre}]{.n}*[)]{.sig-paren}[[[\[source\]]{.pre}]{.viewcode-link}](_modules/scrapy/statscollectors.html#StatsCollector.set_value){.reference .internal}[¶](#scrapy.statscollectors.StatsCollector.set_value "Permalink to this definition"){.headerlink}

    :   Set the given value for the given stats key.

    [[set_stats]{.pre}]{.sig-name .descname}[(]{.sig-paren}*[[stats]{.pre}]{.n}*[)]{.sig-paren}[[[\[source\]]{.pre}]{.viewcode-link}](_modules/scrapy/statscollectors.html#StatsCollector.set_stats){.reference .internal}[¶](#scrapy.statscollectors.StatsCollector.set_stats "Permalink to this definition"){.headerlink}

    :   Override the current stats with the dict passed in
        [`stats`{.docutils .literal .notranslate}]{.pre} argument.

    [[inc_value]{.pre}]{.sig-name .descname}[(]{.sig-paren}*[[key]{.pre}]{.n}*, *[[count]{.pre}]{.n}[[=]{.pre}]{.o}[[1]{.pre}]{.default_value}*, *[[start]{.pre}]{.n}[[=]{.pre}]{.o}[[0]{.pre}]{.default_value}*[)]{.sig-paren}[[[\[source\]]{.pre}]{.viewcode-link}](_modules/scrapy/statscollectors.html#StatsCollector.inc_value){.reference .internal}[¶](#scrapy.statscollectors.StatsCollector.inc_value "Permalink to this definition"){.headerlink}

    :   Increment the value of the given stats key, by the given count,
        assuming the start value given (when it's not set).

    [[max_value]{.pre}]{.sig-name .descname}[(]{.sig-paren}*[[key]{.pre}]{.n}*, *[[value]{.pre}]{.n}*[)]{.sig-paren}[[[\[source\]]{.pre}]{.viewcode-link}](_modules/scrapy/statscollectors.html#StatsCollector.max_value){.reference .internal}[¶](#scrapy.statscollectors.StatsCollector.max_value "Permalink to this definition"){.headerlink}

    :   Set the given value for the given key only if current value for
        the same key is lower than value. If there is no current value
        for the given key, the value is always set.

    [[min_value]{.pre}]{.sig-name .descname}[(]{.sig-paren}*[[key]{.pre}]{.n}*, *[[value]{.pre}]{.n}*[)]{.sig-paren}[[[\[source\]]{.pre}]{.viewcode-link}](_modules/scrapy/statscollectors.html#StatsCollector.min_value){.reference .internal}[¶](#scrapy.statscollectors.StatsCollector.min_value "Permalink to this definition"){.headerlink}

    :   Set the given value for the given key only if current value for
        the same key is greater than value. If there is no current value
        for the given key, the value is always set.

    [[clear_stats]{.pre}]{.sig-name .descname}[(]{.sig-paren}[)]{.sig-paren}[[[\[source\]]{.pre}]{.viewcode-link}](_modules/scrapy/statscollectors.html#StatsCollector.clear_stats){.reference .internal}[¶](#scrapy.statscollectors.StatsCollector.clear_stats "Permalink to this definition"){.headerlink}

    :   Clear all stats.

    The following methods are not part of the stats collection api but
    instead used when implementing custom stats collectors:

    [[open_spider]{.pre}]{.sig-name .descname}[(]{.sig-paren}*[[spider]{.pre}]{.n}*[)]{.sig-paren}[[[\[source\]]{.pre}]{.viewcode-link}](_modules/scrapy/statscollectors.html#StatsCollector.open_spider){.reference .internal}[¶](#scrapy.statscollectors.StatsCollector.open_spider "Permalink to this definition"){.headerlink}

    :   Open the given spider for stats collection.

    [[close_spider]{.pre}]{.sig-name .descname}[(]{.sig-paren}*[[spider]{.pre}]{.n}*[)]{.sig-paren}[[[\[source\]]{.pre}]{.viewcode-link}](_modules/scrapy/statscollectors.html#StatsCollector.close_spider){.reference .internal}[¶](#scrapy.statscollectors.StatsCollector.close_spider "Permalink to this definition"){.headerlink}

    :   Close the given spider. After this is called, no more specific
        stats can be accessed or collected.
:::
:::
:::

[[Architecture overview]{.doc}](index.html#document-topics/architecture){.reference .internal}

:   Understand the Scrapy architecture.

[[Add-ons]{.doc}](index.html#document-topics/addons){.reference .internal}

:   Enable and configure third-party extensions.

[[Downloader Middleware]{.doc}](index.html#document-topics/downloader-middleware){.reference .internal}

:   Customize how pages get requested and downloaded.

[[Spider Middleware]{.doc}](index.html#document-topics/spider-middleware){.reference .internal}

:   Customize the input and output of your spiders.

[[Extensions]{.doc}](index.html#document-topics/extensions){.reference .internal}

:   Extend Scrapy with your custom functionality

[[Signals]{.doc}](index.html#document-topics/signals){.reference .internal}

:   See all available signals and how to work with them.

[[Scheduler]{.doc}](index.html#document-topics/scheduler){.reference .internal}

:   Understand the scheduler component.

[[Item Exporters]{.doc}](index.html#document-topics/exporters){.reference .internal}

:   Quickly export your scraped items to a file (XML, CSV, etc).

[[Components]{.doc}](index.html#document-topics/components){.reference .internal}

:   Learn the common API and some good practices when building custom
    Scrapy components.

[[Core API]{.doc}](index.html#document-topics/api){.reference .internal}

:   Use it on extensions and middlewares to extend Scrapy functionality.
:::

::: {#all-the-rest .section}
## All the rest[¶](#all-the-rest "Permalink to this heading"){.headerlink}

::: {.toctree-wrapper .compound}
[]{#document-news}

::: {#release-notes .section}
[]{#news}

### Release notes[¶](#release-notes "Permalink to this heading"){.headerlink}

::: {#scrapy-2-11-0-2023-09-18 .section}
[]{#release-2-11-0}

#### Scrapy 2.11.0 (2023-09-18)[¶](#scrapy-2-11-0-2023-09-18 "Permalink to this heading"){.headerlink}

Highlights:

-   Spiders can now modify [[settings]{.std
    .std-ref}](index.html#topics-settings){.hoverxref .tooltip
    .reference .internal} in their [[`from_crawler()`{.xref .py .py-meth
    .docutils .literal
    .notranslate}]{.pre}](index.html#scrapy.Spider.from_crawler "scrapy.Spider.from_crawler"){.reference
    .internal} methods, e.g. based on [[spider arguments]{.std
    .std-ref}](index.html#spiderargs){.hoverxref .tooltip .reference
    .internal}.

-   Periodic logging of stats.

::: {#backward-incompatible-changes .section}
##### Backward-incompatible changes[¶](#backward-incompatible-changes "Permalink to this heading"){.headerlink}

-   Most of the initialization of [[`scrapy.crawler.Crawler`{.xref .py
    .py-class .docutils .literal
    .notranslate}]{.pre}](index.html#scrapy.crawler.Crawler "scrapy.crawler.Crawler"){.reference
    .internal} instances is now done in [[`crawl()`{.xref .py .py-meth
    .docutils .literal
    .notranslate}]{.pre}](index.html#scrapy.crawler.Crawler.crawl "scrapy.crawler.Crawler.crawl"){.reference
    .internal}, so the state of instances before that method is called
    is now different compared to older Scrapy versions. We do not
    recommend using the [[`Crawler`{.xref .py .py-class .docutils
    .literal
    .notranslate}]{.pre}](index.html#scrapy.crawler.Crawler "scrapy.crawler.Crawler"){.reference
    .internal} instances before [[`crawl()`{.xref .py .py-meth .docutils
    .literal
    .notranslate}]{.pre}](index.html#scrapy.crawler.Crawler.crawl "scrapy.crawler.Crawler.crawl"){.reference
    .internal} is called. ([issue
    6038](https://github.com/scrapy/scrapy/issues/6038){.reference
    .external})

-   [[`scrapy.Spider.from_crawler()`{.xref .py .py-meth .docutils
    .literal
    .notranslate}]{.pre}](index.html#scrapy.Spider.from_crawler "scrapy.Spider.from_crawler"){.reference
    .internal} is now called before the initialization of various
    components previously initialized in
    [`scrapy.crawler.Crawler.__init__()`{.xref .py .py-meth .docutils
    .literal .notranslate}]{.pre} and before the settings are finalized
    and frozen. This change was needed to allow changing the settings in
    [[`scrapy.Spider.from_crawler()`{.xref .py .py-meth .docutils
    .literal
    .notranslate}]{.pre}](index.html#scrapy.Spider.from_crawler "scrapy.Spider.from_crawler"){.reference
    .internal}. If you want to access the final setting values and the
    initialized [[`Crawler`{.xref .py .py-class .docutils .literal
    .notranslate}]{.pre}](index.html#scrapy.crawler.Crawler "scrapy.crawler.Crawler"){.reference
    .internal} attributes in the spider code as early as possible you
    can do this in [[`start_requests()`{.xref .py .py-meth .docutils
    .literal
    .notranslate}]{.pre}](index.html#scrapy.Spider.start_requests "scrapy.Spider.start_requests"){.reference
    .internal} or in a handler of the [[`engine_started`{.xref .std
    .std-signal .docutils .literal
    .notranslate}]{.pre}](index.html#std-signal-engine_started){.hoverxref
    .tooltip .reference .internal} signal. ([issue
    6038](https://github.com/scrapy/scrapy/issues/6038){.reference
    .external})

-   The [[`TextResponse.json`{.xref .py .py-meth .docutils .literal
    .notranslate}]{.pre}](index.html#scrapy.http.TextResponse.json "scrapy.http.TextResponse.json"){.reference
    .internal} method now requires the response to be in a valid JSON
    encoding (UTF-8, UTF-16, or UTF-32). If you need to deal with JSON
    documents in an invalid encoding, use
    [`json.loads(response.text)`{.docutils .literal .notranslate}]{.pre}
    instead. ([issue
    6016](https://github.com/scrapy/scrapy/issues/6016){.reference
    .external})

-   [[`PythonItemExporter`{.xref .py .py-class .docutils .literal
    .notranslate}]{.pre}](index.html#scrapy.exporters.PythonItemExporter "scrapy.exporters.PythonItemExporter"){.reference
    .internal} used the binary output by default but it no longer does.
    ([issue
    6006](https://github.com/scrapy/scrapy/issues/6006){.reference
    .external}, [issue
    6007](https://github.com/scrapy/scrapy/issues/6007){.reference
    .external})
:::

::: {#deprecation-removals .section}
##### Deprecation removals[¶](#deprecation-removals "Permalink to this heading"){.headerlink}

-   Removed the binary export mode of [[`PythonItemExporter`{.xref .py
    .py-class .docutils .literal
    .notranslate}]{.pre}](index.html#scrapy.exporters.PythonItemExporter "scrapy.exporters.PythonItemExporter"){.reference
    .internal}, deprecated in Scrapy 1.1.0. ([issue
    6006](https://github.com/scrapy/scrapy/issues/6006){.reference
    .external}, [issue
    6007](https://github.com/scrapy/scrapy/issues/6007){.reference
    .external})

    ::: {.admonition .note}
    Note

    If you are using this Scrapy version on Scrapy Cloud with a stack
    that includes an older Scrapy version and get a "TypeError:
    Unexpected options: binary" error, you may need to add
    [`scrapinghub-entrypoint-scrapy`{.docutils .literal
    .notranslate}]{.pre}` `{.docutils .literal
    .notranslate}[`>=`{.docutils .literal
    .notranslate}]{.pre}` `{.docutils .literal
    .notranslate}[`0.14.1`{.docutils .literal .notranslate}]{.pre} to
    your project requirements or switch to a stack that includes Scrapy
    2.11.
    :::

-   Removed the [`CrawlerRunner.spiders`{.docutils .literal
    .notranslate}]{.pre} attribute, deprecated in Scrapy 1.0.0, use
    [`CrawlerRunner.spider_loader`{.xref .py .py-attr .docutils .literal
    .notranslate}]{.pre} instead. ([issue
    6010](https://github.com/scrapy/scrapy/issues/6010){.reference
    .external})
:::

::: {#deprecations .section}
##### Deprecations[¶](#deprecations "Permalink to this heading"){.headerlink}

-   Running [[`crawl()`{.xref .py .py-meth .docutils .literal
    .notranslate}]{.pre}](index.html#scrapy.crawler.Crawler.crawl "scrapy.crawler.Crawler.crawl"){.reference
    .internal} more than once on the same
    [[`scrapy.crawler.Crawler`{.xref .py .py-class .docutils .literal
    .notranslate}]{.pre}](index.html#scrapy.crawler.Crawler "scrapy.crawler.Crawler"){.reference
    .internal} instance is now deprecated. ([issue
    1587](https://github.com/scrapy/scrapy/issues/1587){.reference
    .external}, [issue
    6040](https://github.com/scrapy/scrapy/issues/6040){.reference
    .external})
:::

::: {#new-features .section}
##### New features[¶](#new-features "Permalink to this heading"){.headerlink}

-   Spiders can now modify settings in their [[`from_crawler()`{.xref
    .py .py-meth .docutils .literal
    .notranslate}]{.pre}](index.html#scrapy.Spider.from_crawler "scrapy.Spider.from_crawler"){.reference
    .internal} method, e.g. based on [[spider arguments]{.std
    .std-ref}](index.html#spiderargs){.hoverxref .tooltip .reference
    .internal}. ([issue
    1305](https://github.com/scrapy/scrapy/issues/1305){.reference
    .external}, [issue
    1580](https://github.com/scrapy/scrapy/issues/1580){.reference
    .external}, [issue
    2392](https://github.com/scrapy/scrapy/issues/2392){.reference
    .external}, [issue
    3663](https://github.com/scrapy/scrapy/issues/3663){.reference
    .external}, [issue
    6038](https://github.com/scrapy/scrapy/issues/6038){.reference
    .external})

-   Added the [[`PeriodicLog`{.xref .py .py-class .docutils .literal
    .notranslate}]{.pre}](index.html#scrapy.extensions.periodic_log.PeriodicLog "scrapy.extensions.periodic_log.PeriodicLog"){.reference
    .internal} extension which can be enabled to log stats and/or their
    differences periodically. ([issue
    5926](https://github.com/scrapy/scrapy/issues/5926){.reference
    .external})

-   Optimized the memory usage in [[`TextResponse.json`{.xref .py
    .py-meth .docutils .literal
    .notranslate}]{.pre}](index.html#scrapy.http.TextResponse.json "scrapy.http.TextResponse.json"){.reference
    .internal} by removing unnecessary body decoding. ([issue
    5968](https://github.com/scrapy/scrapy/issues/5968){.reference
    .external}, [issue
    6016](https://github.com/scrapy/scrapy/issues/6016){.reference
    .external})

-   Links to [`.webp`{.docutils .literal .notranslate}]{.pre} files are
    now ignored by [[link extractors]{.std
    .std-ref}](index.html#topics-link-extractors){.hoverxref .tooltip
    .reference .internal}. ([issue
    6021](https://github.com/scrapy/scrapy/issues/6021){.reference
    .external})
:::

::: {#bug-fixes .section}
##### Bug fixes[¶](#bug-fixes "Permalink to this heading"){.headerlink}

-   Fixed logging enabled add-ons. ([issue
    6036](https://github.com/scrapy/scrapy/issues/6036){.reference
    .external})

-   Fixed [[`MailSender`{.xref .py .py-class .docutils .literal
    .notranslate}]{.pre}](index.html#scrapy.mail.MailSender "scrapy.mail.MailSender"){.reference
    .internal} producing invalid message bodies when the
    [`charset`{.docutils .literal .notranslate}]{.pre} argument is
    passed to [[`send()`{.xref .py .py-meth .docutils .literal
    .notranslate}]{.pre}](index.html#scrapy.mail.MailSender.send "scrapy.mail.MailSender.send"){.reference
    .internal}. ([issue
    5096](https://github.com/scrapy/scrapy/issues/5096){.reference
    .external}, [issue
    5118](https://github.com/scrapy/scrapy/issues/5118){.reference
    .external})

-   Fixed an exception when accessing
    [`self.EXCEPTIONS_TO_RETRY`{.docutils .literal .notranslate}]{.pre}
    from a subclass of [[`RetryMiddleware`{.xref .py .py-class .docutils
    .literal
    .notranslate}]{.pre}](index.html#scrapy.downloadermiddlewares.retry.RetryMiddleware "scrapy.downloadermiddlewares.retry.RetryMiddleware"){.reference
    .internal}. ([issue
    6049](https://github.com/scrapy/scrapy/issues/6049){.reference
    .external}, [issue
    6050](https://github.com/scrapy/scrapy/issues/6050){.reference
    .external})

-   [[`scrapy.settings.BaseSettings.getdictorlist()`{.xref .py .py-meth
    .docutils .literal
    .notranslate}]{.pre}](index.html#scrapy.settings.BaseSettings.getdictorlist "scrapy.settings.BaseSettings.getdictorlist"){.reference
    .internal}, used to parse [[`FEED_EXPORT_FIELDS`{.xref .std
    .std-setting .docutils .literal
    .notranslate}]{.pre}](index.html#std-setting-FEED_EXPORT_FIELDS){.hoverxref
    .tooltip .reference .internal}, now handles tuple values. ([issue
    6011](https://github.com/scrapy/scrapy/issues/6011){.reference
    .external}, [issue
    6013](https://github.com/scrapy/scrapy/issues/6013){.reference
    .external})

-   Calls to [`datetime.utcnow()`{.docutils .literal
    .notranslate}]{.pre}, no longer recommended to be used, have been
    replaced with calls to [`datetime.now()`{.docutils .literal
    .notranslate}]{.pre} with a timezone. ([issue
    6014](https://github.com/scrapy/scrapy/issues/6014){.reference
    .external})
:::

::: {#documentation .section}
##### Documentation[¶](#documentation "Permalink to this heading"){.headerlink}

-   Updated a deprecated function call in a pipeline example. ([issue
    6008](https://github.com/scrapy/scrapy/issues/6008){.reference
    .external}, [issue
    6009](https://github.com/scrapy/scrapy/issues/6009){.reference
    .external})
:::

::: {#quality-assurance .section}
##### Quality assurance[¶](#quality-assurance "Permalink to this heading"){.headerlink}

-   Extended typing hints. ([issue
    6003](https://github.com/scrapy/scrapy/issues/6003){.reference
    .external}, [issue
    6005](https://github.com/scrapy/scrapy/issues/6005){.reference
    .external}, [issue
    6031](https://github.com/scrapy/scrapy/issues/6031){.reference
    .external}, [issue
    6034](https://github.com/scrapy/scrapy/issues/6034){.reference
    .external})

-   Pinned [brotli](https://github.com/google/brotli){.reference
    .external} to 1.0.9 for the PyPy tests as 1.1.0 breaks them. ([issue
    6044](https://github.com/scrapy/scrapy/issues/6044){.reference
    .external}, [issue
    6045](https://github.com/scrapy/scrapy/issues/6045){.reference
    .external})

-   Other CI and pre-commit improvements. ([issue
    6002](https://github.com/scrapy/scrapy/issues/6002){.reference
    .external}, [issue
    6013](https://github.com/scrapy/scrapy/issues/6013){.reference
    .external}, [issue
    6046](https://github.com/scrapy/scrapy/issues/6046){.reference
    .external})
:::
:::

::: {#scrapy-2-10-1-2023-08-30 .section}
[]{#release-2-10-1}

#### Scrapy 2.10.1 (2023-08-30)[¶](#scrapy-2-10-1-2023-08-30 "Permalink to this heading"){.headerlink}

Marked [`Twisted`{.docutils .literal .notranslate}]{.pre}` `{.docutils
.literal .notranslate}[`>=`{.docutils .literal
.notranslate}]{.pre}` `{.docutils .literal
.notranslate}[`23.8.0`{.docutils .literal .notranslate}]{.pre} as
unsupported. ([issue
6024](https://github.com/scrapy/scrapy/issues/6024){.reference
.external}, [issue
6026](https://github.com/scrapy/scrapy/issues/6026){.reference
.external})
:::

::: {#scrapy-2-10-0-2023-08-04 .section}
[]{#release-2-10-0}

#### Scrapy 2.10.0 (2023-08-04)[¶](#scrapy-2-10-0-2023-08-04 "Permalink to this heading"){.headerlink}

Highlights:

-   Added Python 3.12 support, dropped Python 3.7 support.

-   The new add-ons framework simplifies configuring 3rd-party
    components that support it.

-   Exceptions to retry can now be configured.

-   Many fixes and improvements for feed exports.

::: {#modified-requirements .section}
##### Modified requirements[¶](#modified-requirements "Permalink to this heading"){.headerlink}

-   Dropped support for Python 3.7. ([issue
    5953](https://github.com/scrapy/scrapy/issues/5953){.reference
    .external})

-   Added support for the upcoming Python 3.12. ([issue
    5984](https://github.com/scrapy/scrapy/issues/5984){.reference
    .external})

-   Minimum versions increased for these dependencies:

    -   [lxml](https://lxml.de/){.reference .external}: 4.3.0 → 4.4.1

    -   [cryptography](https://cryptography.io/en/latest/){.reference
        .external}: 3.4.6 → 36.0.0

-   [`pkg_resources`{.docutils .literal .notranslate}]{.pre} is no
    longer used. ([issue
    5956](https://github.com/scrapy/scrapy/issues/5956){.reference
    .external}, [issue
    5958](https://github.com/scrapy/scrapy/issues/5958){.reference
    .external})

-   [boto3](https://github.com/boto/boto3){.reference .external} is now
    recommended instead of
    [botocore](https://github.com/boto/botocore){.reference .external}
    for exporting to S3. ([issue
    5833](https://github.com/scrapy/scrapy/issues/5833){.reference
    .external}).
:::

::: {#id1 .section}
##### Backward-incompatible changes[¶](#id1 "Permalink to this heading"){.headerlink}

-   The value of the [[`FEED_STORE_EMPTY`{.xref .std .std-setting
    .docutils .literal
    .notranslate}]{.pre}](index.html#std-setting-FEED_STORE_EMPTY){.hoverxref
    .tooltip .reference .internal} setting is now [`True`{.docutils
    .literal .notranslate}]{.pre} instead of [`False`{.docutils .literal
    .notranslate}]{.pre}. In earlier Scrapy versions empty files were
    created even when this setting was [`False`{.docutils .literal
    .notranslate}]{.pre} (which was a bug that is now fixed), so the new
    default should keep the old behavior. ([issue
    872](https://github.com/scrapy/scrapy/issues/872){.reference
    .external}, [issue
    5847](https://github.com/scrapy/scrapy/issues/5847){.reference
    .external})
:::

::: {#id2 .section}
##### Deprecation removals[¶](#id2 "Permalink to this heading"){.headerlink}

-   When a function is assigned to the [[`FEED_URI_PARAMS`{.xref .std
    .std-setting .docutils .literal
    .notranslate}]{.pre}](index.html#std-setting-FEED_URI_PARAMS){.hoverxref
    .tooltip .reference .internal} setting, returning [`None`{.docutils
    .literal .notranslate}]{.pre} or modifying the [`params`{.docutils
    .literal .notranslate}]{.pre} input parameter, deprecated in Scrapy
    2.6, is no longer supported. ([issue
    5994](https://github.com/scrapy/scrapy/issues/5994){.reference
    .external}, [issue
    5996](https://github.com/scrapy/scrapy/issues/5996){.reference
    .external})

-   The [`scrapy.utils.reqser`{.docutils .literal .notranslate}]{.pre}
    module, deprecated in Scrapy 2.6, is removed. ([issue
    5994](https://github.com/scrapy/scrapy/issues/5994){.reference
    .external}, [issue
    5996](https://github.com/scrapy/scrapy/issues/5996){.reference
    .external})

-   The [`scrapy.squeues`{.docutils .literal .notranslate}]{.pre}
    classes [`PickleFifoDiskQueueNonRequest`{.docutils .literal
    .notranslate}]{.pre}, [`PickleLifoDiskQueueNonRequest`{.docutils
    .literal .notranslate}]{.pre},
    [`MarshalFifoDiskQueueNonRequest`{.docutils .literal
    .notranslate}]{.pre}, and
    [`MarshalLifoDiskQueueNonRequest`{.docutils .literal
    .notranslate}]{.pre}, deprecated in Scrapy 2.6, are removed. ([issue
    5994](https://github.com/scrapy/scrapy/issues/5994){.reference
    .external}, [issue
    5996](https://github.com/scrapy/scrapy/issues/5996){.reference
    .external})

-   The property [`open_spiders`{.docutils .literal .notranslate}]{.pre}
    and the methods [`has_capacity`{.docutils .literal
    .notranslate}]{.pre} and [`schedule`{.docutils .literal
    .notranslate}]{.pre} of [`scrapy.core.engine.ExecutionEngine`{.xref
    .py .py-class .docutils .literal .notranslate}]{.pre}, deprecated in
    Scrapy 2.6, are removed. ([issue
    5994](https://github.com/scrapy/scrapy/issues/5994){.reference
    .external}, [issue
    5998](https://github.com/scrapy/scrapy/issues/5998){.reference
    .external})

-   Passing a [`spider`{.docutils .literal .notranslate}]{.pre} argument
    to the [`spider_is_idle()`{.xref .py .py-meth .docutils .literal
    .notranslate}]{.pre}, [`crawl()`{.xref .py .py-meth .docutils
    .literal .notranslate}]{.pre} and [`download()`{.xref .py .py-meth
    .docutils .literal .notranslate}]{.pre} methods of
    [`scrapy.core.engine.ExecutionEngine`{.xref .py .py-class .docutils
    .literal .notranslate}]{.pre}, deprecated in Scrapy 2.6, is no
    longer supported. ([issue
    5994](https://github.com/scrapy/scrapy/issues/5994){.reference
    .external}, [issue
    5998](https://github.com/scrapy/scrapy/issues/5998){.reference
    .external})
:::

::: {#id3 .section}
##### Deprecations[¶](#id3 "Permalink to this heading"){.headerlink}

-   [`scrapy.utils.datatypes.CaselessDict`{.xref .py .py-class .docutils
    .literal .notranslate}]{.pre} is deprecated, use
    [`scrapy.utils.datatypes.CaseInsensitiveDict`{.xref .py .py-class
    .docutils .literal .notranslate}]{.pre} instead. ([issue
    5146](https://github.com/scrapy/scrapy/issues/5146){.reference
    .external})

-   Passing the [`custom`{.docutils .literal .notranslate}]{.pre}
    argument to [`scrapy.utils.conf.build_component_list()`{.xref .py
    .py-func .docutils .literal .notranslate}]{.pre} is deprecated, it
    was used in the past to merge [`FOO`{.docutils .literal
    .notranslate}]{.pre} and [`FOO_BASE`{.docutils .literal
    .notranslate}]{.pre} setting values but now Scrapy uses
    [[`scrapy.settings.BaseSettings.getwithbase()`{.xref .py .py-func
    .docutils .literal
    .notranslate}]{.pre}](index.html#scrapy.settings.BaseSettings.getwithbase "scrapy.settings.BaseSettings.getwithbase"){.reference
    .internal} to do the same. Code that uses this argument and cannot
    be switched to [`getwithbase()`{.docutils .literal
    .notranslate}]{.pre} can be switched to merging the values
    explicitly. ([issue
    5726](https://github.com/scrapy/scrapy/issues/5726){.reference
    .external}, [issue
    5923](https://github.com/scrapy/scrapy/issues/5923){.reference
    .external})
:::

::: {#id4 .section}
##### New features[¶](#id4 "Permalink to this heading"){.headerlink}

-   Added support for [[Scrapy add-ons]{.std
    .std-ref}](index.html#topics-addons){.hoverxref .tooltip .reference
    .internal}. ([issue
    5950](https://github.com/scrapy/scrapy/issues/5950){.reference
    .external})

-   Added the [[`RETRY_EXCEPTIONS`{.xref .std .std-setting .docutils
    .literal
    .notranslate}]{.pre}](index.html#std-setting-RETRY_EXCEPTIONS){.hoverxref
    .tooltip .reference .internal} setting that configures which
    exceptions will be retried by [[`RetryMiddleware`{.xref .py
    .py-class .docutils .literal
    .notranslate}]{.pre}](index.html#scrapy.downloadermiddlewares.retry.RetryMiddleware "scrapy.downloadermiddlewares.retry.RetryMiddleware"){.reference
    .internal}. ([issue
    2701](https://github.com/scrapy/scrapy/issues/2701){.reference
    .external}, [issue
    5929](https://github.com/scrapy/scrapy/issues/5929){.reference
    .external})

-   Added the possiiblity to close the spider if no items were produced
    in the specified time, configured by
    [[`CLOSESPIDER_TIMEOUT_NO_ITEM`{.xref .std .std-setting .docutils
    .literal
    .notranslate}]{.pre}](index.html#std-setting-CLOSESPIDER_TIMEOUT_NO_ITEM){.hoverxref
    .tooltip .reference .internal}. ([issue
    5979](https://github.com/scrapy/scrapy/issues/5979){.reference
    .external})

-   Added support for the [[`AWS_REGION_NAME`{.xref .std .std-setting
    .docutils .literal
    .notranslate}]{.pre}](index.html#std-setting-AWS_REGION_NAME){.hoverxref
    .tooltip .reference .internal} setting to feed exports. ([issue
    5980](https://github.com/scrapy/scrapy/issues/5980){.reference
    .external})

-   Added support for using [[`pathlib.Path`{.xref .py .py-class
    .docutils .literal
    .notranslate}]{.pre}](https://docs.python.org/3/library/pathlib.html#pathlib.Path "(in Python v3.12)"){.reference
    .external} objects that refer to absolute Windows paths in the
    [[`FEEDS`{.xref .std .std-setting .docutils .literal
    .notranslate}]{.pre}](index.html#std-setting-FEEDS){.hoverxref
    .tooltip .reference .internal} setting. ([issue
    5939](https://github.com/scrapy/scrapy/issues/5939){.reference
    .external})
:::

::: {#id5 .section}
##### Bug fixes[¶](#id5 "Permalink to this heading"){.headerlink}

-   Fixed creating empty feeds even with
    [`FEED_STORE_EMPTY=False`{.docutils .literal .notranslate}]{.pre}.
    ([issue 872](https://github.com/scrapy/scrapy/issues/872){.reference
    .external}, [issue
    5847](https://github.com/scrapy/scrapy/issues/5847){.reference
    .external})

-   Fixed using absolute Windows paths when specifying output files.
    ([issue
    5969](https://github.com/scrapy/scrapy/issues/5969){.reference
    .external}, [issue
    5971](https://github.com/scrapy/scrapy/issues/5971){.reference
    .external})

-   Fixed problems with uploading large files to S3 by switching to
    multipart uploads (requires
    [boto3](https://github.com/boto/boto3){.reference .external}).
    ([issue 960](https://github.com/scrapy/scrapy/issues/960){.reference
    .external}, [issue
    5735](https://github.com/scrapy/scrapy/issues/5735){.reference
    .external}, [issue
    5833](https://github.com/scrapy/scrapy/issues/5833){.reference
    .external})

-   Fixed the JSON exporter writing extra commas when some exceptions
    occur. ([issue
    3090](https://github.com/scrapy/scrapy/issues/3090){.reference
    .external}, [issue
    5952](https://github.com/scrapy/scrapy/issues/5952){.reference
    .external})

-   Fixed the "read of closed file" error in the CSV exporter. ([issue
    5043](https://github.com/scrapy/scrapy/issues/5043){.reference
    .external}, [issue
    5705](https://github.com/scrapy/scrapy/issues/5705){.reference
    .external})

-   Fixed an error when a component added by the class object throws
    [[`NotConfigured`{.xref .py .py-exc .docutils .literal
    .notranslate}]{.pre}](index.html#scrapy.exceptions.NotConfigured "scrapy.exceptions.NotConfigured"){.reference
    .internal} with a message. ([issue
    5950](https://github.com/scrapy/scrapy/issues/5950){.reference
    .external}, [issue
    5992](https://github.com/scrapy/scrapy/issues/5992){.reference
    .external})

-   Added the missing [[`scrapy.settings.BaseSettings.pop()`{.xref .py
    .py-meth .docutils .literal
    .notranslate}]{.pre}](index.html#scrapy.settings.BaseSettings.pop "scrapy.settings.BaseSettings.pop"){.reference
    .internal} method. ([issue
    5959](https://github.com/scrapy/scrapy/issues/5959){.reference
    .external}, [issue
    5960](https://github.com/scrapy/scrapy/issues/5960){.reference
    .external}, [issue
    5963](https://github.com/scrapy/scrapy/issues/5963){.reference
    .external})

-   Added [`CaseInsensitiveDict`{.xref .py .py-class .docutils .literal
    .notranslate}]{.pre} as a replacement for [`CaselessDict`{.xref .py
    .py-class .docutils .literal .notranslate}]{.pre} that fixes some
    API inconsistencies. ([issue
    5146](https://github.com/scrapy/scrapy/issues/5146){.reference
    .external})
:::

::: {#id6 .section}
##### Documentation[¶](#id6 "Permalink to this heading"){.headerlink}

-   Documented [[`scrapy.Spider.update_settings()`{.xref .py .py-meth
    .docutils .literal
    .notranslate}]{.pre}](index.html#scrapy.Spider.update_settings "scrapy.Spider.update_settings"){.reference
    .internal}. ([issue
    5745](https://github.com/scrapy/scrapy/issues/5745){.reference
    .external}, [issue
    5846](https://github.com/scrapy/scrapy/issues/5846){.reference
    .external})

-   Documented possible problems with early Twisted reactor installation
    and their solutions. ([issue
    5981](https://github.com/scrapy/scrapy/issues/5981){.reference
    .external}, [issue
    6000](https://github.com/scrapy/scrapy/issues/6000){.reference
    .external})

-   Added examples of making additional requests in callbacks. ([issue
    5927](https://github.com/scrapy/scrapy/issues/5927){.reference
    .external})

-   Improved the feed export docs. ([issue
    5579](https://github.com/scrapy/scrapy/issues/5579){.reference
    .external}, [issue
    5931](https://github.com/scrapy/scrapy/issues/5931){.reference
    .external})

-   Clarified the docs about request objects on redirection. ([issue
    5707](https://github.com/scrapy/scrapy/issues/5707){.reference
    .external}, [issue
    5937](https://github.com/scrapy/scrapy/issues/5937){.reference
    .external})
:::

::: {#id7 .section}
##### Quality assurance[¶](#id7 "Permalink to this heading"){.headerlink}

-   Added support for running tests against the installed Scrapy
    version. ([issue
    4914](https://github.com/scrapy/scrapy/issues/4914){.reference
    .external}, [issue
    5949](https://github.com/scrapy/scrapy/issues/5949){.reference
    .external})

-   Extended typing hints. ([issue
    5925](https://github.com/scrapy/scrapy/issues/5925){.reference
    .external}, [issue
    5977](https://github.com/scrapy/scrapy/issues/5977){.reference
    .external})

-   Fixed the
    [`test_utils_asyncio.AsyncioTest.test_set_asyncio_event_loop`{.docutils
    .literal .notranslate}]{.pre} test. ([issue
    5951](https://github.com/scrapy/scrapy/issues/5951){.reference
    .external})

-   Fixed the
    [`test_feedexport.BatchDeliveriesTest.test_batch_path_differ`{.docutils
    .literal .notranslate}]{.pre} test on Windows. ([issue
    5847](https://github.com/scrapy/scrapy/issues/5847){.reference
    .external})

-   Enabled CI runs for Python 3.11 on Windows. ([issue
    5999](https://github.com/scrapy/scrapy/issues/5999){.reference
    .external})

-   Simplified skipping tests that depend on [`uvloop`{.docutils
    .literal .notranslate}]{.pre}. ([issue
    5984](https://github.com/scrapy/scrapy/issues/5984){.reference
    .external})

-   Fixed the [`extra-deps-pinned`{.docutils .literal
    .notranslate}]{.pre} tox env. ([issue
    5948](https://github.com/scrapy/scrapy/issues/5948){.reference
    .external})

-   Implemented cleanups. ([issue
    5965](https://github.com/scrapy/scrapy/issues/5965){.reference
    .external}, [issue
    5986](https://github.com/scrapy/scrapy/issues/5986){.reference
    .external})
:::
:::

::: {#scrapy-2-9-0-2023-05-08 .section}
[]{#release-2-9-0}

#### Scrapy 2.9.0 (2023-05-08)[¶](#scrapy-2-9-0-2023-05-08 "Permalink to this heading"){.headerlink}

Highlights:

-   Per-domain download settings.

-   Compatibility with new
    [cryptography](https://cryptography.io/en/latest/){.reference
    .external} and new
    [parsel](https://github.com/scrapy/parsel){.reference .external}.

-   JMESPath selectors from the new
    [parsel](https://github.com/scrapy/parsel){.reference .external}.

-   Bug fixes.

::: {#id8 .section}
##### Deprecations[¶](#id8 "Permalink to this heading"){.headerlink}

-   [`scrapy.extensions.feedexport._FeedSlot`{.xref .py .py-class
    .docutils .literal .notranslate}]{.pre} is renamed to
    [`scrapy.extensions.feedexport.FeedSlot`{.xref .py .py-class
    .docutils .literal .notranslate}]{.pre} and the old name is
    deprecated. ([issue
    5876](https://github.com/scrapy/scrapy/issues/5876){.reference
    .external})
:::

::: {#id9 .section}
##### New features[¶](#id9 "Permalink to this heading"){.headerlink}

-   Settings corresponding to [[`DOWNLOAD_DELAY`{.xref .std .std-setting
    .docutils .literal
    .notranslate}]{.pre}](index.html#std-setting-DOWNLOAD_DELAY){.hoverxref
    .tooltip .reference .internal},
    [[`CONCURRENT_REQUESTS_PER_DOMAIN`{.xref .std .std-setting .docutils
    .literal
    .notranslate}]{.pre}](index.html#std-setting-CONCURRENT_REQUESTS_PER_DOMAIN){.hoverxref
    .tooltip .reference .internal} and
    [[`RANDOMIZE_DOWNLOAD_DELAY`{.xref .std .std-setting .docutils
    .literal
    .notranslate}]{.pre}](index.html#std-setting-RANDOMIZE_DOWNLOAD_DELAY){.hoverxref
    .tooltip .reference .internal} can now be set on a per-domain basis
    via the new [[`DOWNLOAD_SLOTS`{.xref .std .std-setting .docutils
    .literal
    .notranslate}]{.pre}](index.html#std-setting-DOWNLOAD_SLOTS){.hoverxref
    .tooltip .reference .internal} setting. ([issue
    5328](https://github.com/scrapy/scrapy/issues/5328){.reference
    .external})

-   Added [`TextResponse.jmespath()`{.xref .py .py-meth .docutils
    .literal .notranslate}]{.pre}, a shortcut for JMESPath selectors
    available since
    [parsel](https://github.com/scrapy/parsel){.reference .external}
    1.8.1. ([issue
    5894](https://github.com/scrapy/scrapy/issues/5894){.reference
    .external}, [issue
    5915](https://github.com/scrapy/scrapy/issues/5915){.reference
    .external})

-   Added [[`feed_slot_closed`{.xref .std .std-signal .docutils .literal
    .notranslate}]{.pre}](index.html#std-signal-feed_slot_closed){.hoverxref
    .tooltip .reference .internal} and [[`feed_exporter_closed`{.xref
    .std .std-signal .docutils .literal
    .notranslate}]{.pre}](index.html#std-signal-feed_exporter_closed){.hoverxref
    .tooltip .reference .internal} signals. ([issue
    5876](https://github.com/scrapy/scrapy/issues/5876){.reference
    .external})

-   Added [`scrapy.utils.request.request_to_curl()`{.xref .py .py-func
    .docutils .literal .notranslate}]{.pre}, a function to produce a
    curl command from a [`Request`{.xref .py .py-class .docutils
    .literal .notranslate}]{.pre} object. ([issue
    5892](https://github.com/scrapy/scrapy/issues/5892){.reference
    .external})

-   Values of [[`FILES_STORE`{.xref .std .std-setting .docutils .literal
    .notranslate}]{.pre}](index.html#std-setting-FILES_STORE){.hoverxref
    .tooltip .reference .internal} and [[`IMAGES_STORE`{.xref .std
    .std-setting .docutils .literal
    .notranslate}]{.pre}](index.html#std-setting-IMAGES_STORE){.hoverxref
    .tooltip .reference .internal} can now be [[`pathlib.Path`{.xref .py
    .py-class .docutils .literal
    .notranslate}]{.pre}](https://docs.python.org/3/library/pathlib.html#pathlib.Path "(in Python v3.12)"){.reference
    .external} instances. ([issue
    5801](https://github.com/scrapy/scrapy/issues/5801){.reference
    .external})
:::

::: {#id10 .section}
##### Bug fixes[¶](#id10 "Permalink to this heading"){.headerlink}

-   Fixed a warning with Parsel 1.8.1+. ([issue
    5903](https://github.com/scrapy/scrapy/issues/5903){.reference
    .external}, [issue
    5918](https://github.com/scrapy/scrapy/issues/5918){.reference
    .external})

-   Fixed an error when using feed postprocessing with S3 storage.
    ([issue
    5500](https://github.com/scrapy/scrapy/issues/5500){.reference
    .external}, [issue
    5581](https://github.com/scrapy/scrapy/issues/5581){.reference
    .external})

-   Added the missing
    [[`scrapy.settings.BaseSettings.setdefault()`{.xref .py .py-meth
    .docutils .literal
    .notranslate}]{.pre}](index.html#scrapy.settings.BaseSettings.setdefault "scrapy.settings.BaseSettings.setdefault"){.reference
    .internal} method. ([issue
    5811](https://github.com/scrapy/scrapy/issues/5811){.reference
    .external}, [issue
    5821](https://github.com/scrapy/scrapy/issues/5821){.reference
    .external})

-   Fixed an error when using
    [cryptography](https://cryptography.io/en/latest/){.reference
    .external} 40.0.0+ and
    [[`DOWNLOADER_CLIENT_TLS_VERBOSE_LOGGING`{.xref .std .std-setting
    .docutils .literal
    .notranslate}]{.pre}](index.html#std-setting-DOWNLOADER_CLIENT_TLS_VERBOSE_LOGGING){.hoverxref
    .tooltip .reference .internal} is enabled. ([issue
    5857](https://github.com/scrapy/scrapy/issues/5857){.reference
    .external}, [issue
    5858](https://github.com/scrapy/scrapy/issues/5858){.reference
    .external})

-   The checksums returned by [[`FilesPipeline`{.xref .py .py-class
    .docutils .literal
    .notranslate}]{.pre}](index.html#scrapy.pipelines.files.FilesPipeline "scrapy.pipelines.files.FilesPipeline"){.reference
    .internal} for files on Google Cloud Storage are no longer
    Base64-encoded. ([issue
    5874](https://github.com/scrapy/scrapy/issues/5874){.reference
    .external}, [issue
    5891](https://github.com/scrapy/scrapy/issues/5891){.reference
    .external})

-   [`scrapy.utils.request.request_from_curl()`{.xref .py .py-func
    .docutils .literal .notranslate}]{.pre} now supports \$-prefixed
    string values for the curl [`--data-raw`{.docutils .literal
    .notranslate}]{.pre} argument, which are produced by browsers for
    data that includes certain symbols. ([issue
    5899](https://github.com/scrapy/scrapy/issues/5899){.reference
    .external}, [issue
    5901](https://github.com/scrapy/scrapy/issues/5901){.reference
    .external})

-   The [[`parse`{.xref .std .std-command .docutils .literal
    .notranslate}]{.pre}](index.html#std-command-parse){.hoverxref
    .tooltip .reference .internal} command now also works with async
    generator callbacks. ([issue
    5819](https://github.com/scrapy/scrapy/issues/5819){.reference
    .external}, [issue
    5824](https://github.com/scrapy/scrapy/issues/5824){.reference
    .external})

-   The [[`genspider`{.xref .std .std-command .docutils .literal
    .notranslate}]{.pre}](index.html#std-command-genspider){.hoverxref
    .tooltip .reference .internal} command now properly works with HTTPS
    URLs. ([issue
    3553](https://github.com/scrapy/scrapy/issues/3553){.reference
    .external}, [issue
    5808](https://github.com/scrapy/scrapy/issues/5808){.reference
    .external})

-   Improved handling of asyncio loops. ([issue
    5831](https://github.com/scrapy/scrapy/issues/5831){.reference
    .external}, [issue
    5832](https://github.com/scrapy/scrapy/issues/5832){.reference
    .external})

-   [[`LinkExtractor`{.xref .py .py-class .docutils .literal
    .notranslate}]{.pre}](index.html#scrapy.linkextractors.lxmlhtml.LxmlLinkExtractor "scrapy.linkextractors.lxmlhtml.LxmlLinkExtractor"){.reference
    .internal} now skips certain malformed URLs instead of raising an
    exception. ([issue
    5881](https://github.com/scrapy/scrapy/issues/5881){.reference
    .external})

-   [`scrapy.utils.python.get_func_args()`{.xref .py .py-func .docutils
    .literal .notranslate}]{.pre} now supports more types of callables.
    ([issue
    5872](https://github.com/scrapy/scrapy/issues/5872){.reference
    .external}, [issue
    5885](https://github.com/scrapy/scrapy/issues/5885){.reference
    .external})

-   Fixed an error when processing non-UTF8 values of
    [`Content-Type`{.docutils .literal .notranslate}]{.pre} headers.
    ([issue
    5914](https://github.com/scrapy/scrapy/issues/5914){.reference
    .external}, [issue
    5917](https://github.com/scrapy/scrapy/issues/5917){.reference
    .external})

-   Fixed an error breaking user handling of send failures in
    [[`scrapy.mail.MailSender.send()`{.xref .py .py-meth .docutils
    .literal
    .notranslate}]{.pre}](index.html#scrapy.mail.MailSender.send "scrapy.mail.MailSender.send"){.reference
    .internal}. ([issue
    1611](https://github.com/scrapy/scrapy/issues/1611){.reference
    .external}, [issue
    5880](https://github.com/scrapy/scrapy/issues/5880){.reference
    .external})
:::

::: {#id11 .section}
##### Documentation[¶](#id11 "Permalink to this heading"){.headerlink}

-   Expanded contributing docs. ([issue
    5109](https://github.com/scrapy/scrapy/issues/5109){.reference
    .external}, [issue
    5851](https://github.com/scrapy/scrapy/issues/5851){.reference
    .external})

-   Added
    [blacken-docs](https://github.com/adamchainz/blacken-docs){.reference
    .external} to pre-commit and reformatted the docs with it. ([issue
    5813](https://github.com/scrapy/scrapy/issues/5813){.reference
    .external}, [issue
    5816](https://github.com/scrapy/scrapy/issues/5816){.reference
    .external})

-   Fixed a JS issue. ([issue
    5875](https://github.com/scrapy/scrapy/issues/5875){.reference
    .external}, [issue
    5877](https://github.com/scrapy/scrapy/issues/5877){.reference
    .external})

-   Fixed [`make`{.docutils .literal .notranslate}]{.pre}` `{.docutils
    .literal .notranslate}[`htmlview`{.docutils .literal
    .notranslate}]{.pre}. ([issue
    5878](https://github.com/scrapy/scrapy/issues/5878){.reference
    .external}, [issue
    5879](https://github.com/scrapy/scrapy/issues/5879){.reference
    .external})

-   Fixed typos and other small errors. ([issue
    5827](https://github.com/scrapy/scrapy/issues/5827){.reference
    .external}, [issue
    5839](https://github.com/scrapy/scrapy/issues/5839){.reference
    .external}, [issue
    5883](https://github.com/scrapy/scrapy/issues/5883){.reference
    .external}, [issue
    5890](https://github.com/scrapy/scrapy/issues/5890){.reference
    .external}, [issue
    5895](https://github.com/scrapy/scrapy/issues/5895){.reference
    .external}, [issue
    5904](https://github.com/scrapy/scrapy/issues/5904){.reference
    .external})
:::

::: {#id12 .section}
##### Quality assurance[¶](#id12 "Permalink to this heading"){.headerlink}

-   Extended typing hints. ([issue
    5805](https://github.com/scrapy/scrapy/issues/5805){.reference
    .external}, [issue
    5889](https://github.com/scrapy/scrapy/issues/5889){.reference
    .external}, [issue
    5896](https://github.com/scrapy/scrapy/issues/5896){.reference
    .external})

-   Tests for most of the examples in the docs are now run as a part of
    CI, found problems were fixed. ([issue
    5816](https://github.com/scrapy/scrapy/issues/5816){.reference
    .external}, [issue
    5826](https://github.com/scrapy/scrapy/issues/5826){.reference
    .external}, [issue
    5919](https://github.com/scrapy/scrapy/issues/5919){.reference
    .external})

-   Removed usage of deprecated Python classes. ([issue
    5849](https://github.com/scrapy/scrapy/issues/5849){.reference
    .external})

-   Silenced [`include-ignored`{.docutils .literal .notranslate}]{.pre}
    warnings from coverage. ([issue
    5820](https://github.com/scrapy/scrapy/issues/5820){.reference
    .external})

-   Fixed a random failure of the
    [`test_feedexport.test_batch_path_differ`{.docutils .literal
    .notranslate}]{.pre} test. ([issue
    5855](https://github.com/scrapy/scrapy/issues/5855){.reference
    .external}, [issue
    5898](https://github.com/scrapy/scrapy/issues/5898){.reference
    .external})

-   Updated docstrings to match output produced by
    [parsel](https://github.com/scrapy/parsel){.reference .external}
    1.8.1 so that they don't cause test failures. ([issue
    5902](https://github.com/scrapy/scrapy/issues/5902){.reference
    .external}, [issue
    5919](https://github.com/scrapy/scrapy/issues/5919){.reference
    .external})

-   Other CI and pre-commit improvements. ([issue
    5802](https://github.com/scrapy/scrapy/issues/5802){.reference
    .external}, [issue
    5823](https://github.com/scrapy/scrapy/issues/5823){.reference
    .external}, [issue
    5908](https://github.com/scrapy/scrapy/issues/5908){.reference
    .external})
:::
:::

::: {#scrapy-2-8-0-2023-02-02 .section}
[]{#release-2-8-0}

#### Scrapy 2.8.0 (2023-02-02)[¶](#scrapy-2-8-0-2023-02-02 "Permalink to this heading"){.headerlink}

This is a maintenance release, with minor features, bug fixes, and
cleanups.

::: {#id13 .section}
##### Deprecation removals[¶](#id13 "Permalink to this heading"){.headerlink}

-   The [`scrapy.utils.gz.read1`{.docutils .literal .notranslate}]{.pre}
    function, deprecated in Scrapy 2.0, has now been removed. Use the
    [[`read1()`{.xref .py .py-meth .docutils .literal
    .notranslate}]{.pre}](https://docs.python.org/3/library/io.html#io.BufferedIOBase.read1 "(in Python v3.12)"){.reference
    .external} method of [[`GzipFile`{.xref .py .py-class .docutils
    .literal
    .notranslate}]{.pre}](https://docs.python.org/3/library/gzip.html#gzip.GzipFile "(in Python v3.12)"){.reference
    .external} instead. ([issue
    5719](https://github.com/scrapy/scrapy/issues/5719){.reference
    .external})

-   The [`scrapy.utils.python.to_native_str`{.docutils .literal
    .notranslate}]{.pre} function, deprecated in Scrapy 2.0, has now
    been removed. Use [`scrapy.utils.python.to_unicode()`{.xref .py
    .py-func .docutils .literal .notranslate}]{.pre} instead. ([issue
    5719](https://github.com/scrapy/scrapy/issues/5719){.reference
    .external})

-   The [`scrapy.utils.python.MutableChain.next`{.docutils .literal
    .notranslate}]{.pre} method, deprecated in Scrapy 2.0, has now been
    removed. Use [`__next__()`{.xref .py .py-meth .docutils .literal
    .notranslate}]{.pre} instead. ([issue
    5719](https://github.com/scrapy/scrapy/issues/5719){.reference
    .external})

-   The [`scrapy.linkextractors.FilteringLinkExtractor`{.docutils
    .literal .notranslate}]{.pre} class, deprecated in Scrapy 2.0, has
    now been removed. Use [[`LinkExtractor`{.xref .py .py-class
    .docutils .literal
    .notranslate}]{.pre}](index.html#scrapy.linkextractors.lxmlhtml.LxmlLinkExtractor "scrapy.linkextractors.lxmlhtml.LxmlLinkExtractor"){.reference
    .internal} instead. ([issue
    5720](https://github.com/scrapy/scrapy/issues/5720){.reference
    .external})

-   Support for using environment variables prefixed with
    [`SCRAPY_`{.docutils .literal .notranslate}]{.pre} to override
    settings, deprecated in Scrapy 2.0, has now been removed. ([issue
    5724](https://github.com/scrapy/scrapy/issues/5724){.reference
    .external})

-   Support for the [`noconnect`{.docutils .literal .notranslate}]{.pre}
    query string argument in proxy URLs, deprecated in Scrapy 2.0, has
    now been removed. We expect proxies that used to need it to work
    fine without it. ([issue
    5731](https://github.com/scrapy/scrapy/issues/5731){.reference
    .external})

-   The [`scrapy.utils.python.retry_on_eintr`{.docutils .literal
    .notranslate}]{.pre} function, deprecated in Scrapy 2.3, has now
    been removed. ([issue
    5719](https://github.com/scrapy/scrapy/issues/5719){.reference
    .external})

-   The [`scrapy.utils.python.WeakKeyCache`{.docutils .literal
    .notranslate}]{.pre} class, deprecated in Scrapy 2.4, has now been
    removed. ([issue
    5719](https://github.com/scrapy/scrapy/issues/5719){.reference
    .external})

-   The [`scrapy.utils.boto.is_botocore()`{.docutils .literal
    .notranslate}]{.pre} function, deprecated in Scrapy 2.4, has now
    been removed. ([issue
    5719](https://github.com/scrapy/scrapy/issues/5719){.reference
    .external})
:::

::: {#id14 .section}
##### Deprecations[¶](#id14 "Permalink to this heading"){.headerlink}

-   [`scrapy.pipelines.images.NoimagesDrop`{.xref .py .py-exc .docutils
    .literal .notranslate}]{.pre} is now deprecated. ([issue
    5368](https://github.com/scrapy/scrapy/issues/5368){.reference
    .external}, [issue
    5489](https://github.com/scrapy/scrapy/issues/5489){.reference
    .external})

-   [`ImagesPipeline.convert_image`{.xref .py .py-meth .docutils
    .literal .notranslate}]{.pre} must now accept a
    [`response_body`{.docutils .literal .notranslate}]{.pre} parameter.
    ([issue
    3055](https://github.com/scrapy/scrapy/issues/3055){.reference
    .external}, [issue
    3689](https://github.com/scrapy/scrapy/issues/3689){.reference
    .external}, [issue
    4753](https://github.com/scrapy/scrapy/issues/4753){.reference
    .external})
:::

::: {#id15 .section}
##### New features[¶](#id15 "Permalink to this heading"){.headerlink}

-   Applied [black](https://black.readthedocs.io/en/stable/){.reference
    .external} coding style to files generated with the
    [[`genspider`{.xref .std .std-command .docutils .literal
    .notranslate}]{.pre}](index.html#std-command-genspider){.hoverxref
    .tooltip .reference .internal} and [[`startproject`{.xref .std
    .std-command .docutils .literal
    .notranslate}]{.pre}](index.html#std-command-startproject){.hoverxref
    .tooltip .reference .internal} commands. ([issue
    5809](https://github.com/scrapy/scrapy/issues/5809){.reference
    .external}, [issue
    5814](https://github.com/scrapy/scrapy/issues/5814){.reference
    .external})

-   [[`FEED_EXPORT_ENCODING`{.xref .std .std-setting .docutils .literal
    .notranslate}]{.pre}](index.html#std-setting-FEED_EXPORT_ENCODING){.hoverxref
    .tooltip .reference .internal} is now set to [`"utf-8"`{.docutils
    .literal .notranslate}]{.pre} in the [`settings.py`{.docutils
    .literal .notranslate}]{.pre} file that the [[`startproject`{.xref
    .std .std-command .docutils .literal
    .notranslate}]{.pre}](index.html#std-command-startproject){.hoverxref
    .tooltip .reference .internal} command generates. With this value,
    JSON exports won't force the use of escape sequences for non-ASCII
    characters. ([issue
    5797](https://github.com/scrapy/scrapy/issues/5797){.reference
    .external}, [issue
    5800](https://github.com/scrapy/scrapy/issues/5800){.reference
    .external})

-   The [[`MemoryUsage`{.xref .py .py-class .docutils .literal
    .notranslate}]{.pre}](index.html#scrapy.extensions.memusage.MemoryUsage "scrapy.extensions.memusage.MemoryUsage"){.reference
    .internal} extension now logs the peak memory usage during checks,
    and the binary unit MiB is now used to avoid confusion. ([issue
    5717](https://github.com/scrapy/scrapy/issues/5717){.reference
    .external}, [issue
    5722](https://github.com/scrapy/scrapy/issues/5722){.reference
    .external}, [issue
    5727](https://github.com/scrapy/scrapy/issues/5727){.reference
    .external})

-   The [`callback`{.docutils .literal .notranslate}]{.pre} parameter of
    [[`Request`{.xref .py .py-class .docutils .literal
    .notranslate}]{.pre}](index.html#scrapy.http.Request "scrapy.http.Request"){.reference
    .internal} can now be set to
    [[`scrapy.http.request.NO_CALLBACK()`{.xref .py .py-func .docutils
    .literal
    .notranslate}]{.pre}](index.html#scrapy.http.request.NO_CALLBACK "scrapy.http.request.NO_CALLBACK"){.reference
    .internal}, to distinguish it from [`None`{.docutils .literal
    .notranslate}]{.pre}, as the latter indicates that the default
    spider callback ([[`parse()`{.xref .py .py-meth .docutils .literal
    .notranslate}]{.pre}](index.html#scrapy.Spider.parse "scrapy.Spider.parse"){.reference
    .internal}) is to be used. ([issue
    5798](https://github.com/scrapy/scrapy/issues/5798){.reference
    .external})
:::

::: {#id16 .section}
##### Bug fixes[¶](#id16 "Permalink to this heading"){.headerlink}

-   Enabled unsafe legacy SSL renegotiation to fix access to some
    outdated websites. ([issue
    5491](https://github.com/scrapy/scrapy/issues/5491){.reference
    .external}, [issue
    5790](https://github.com/scrapy/scrapy/issues/5790){.reference
    .external})

-   Fixed STARTTLS-based email delivery not working with Twisted 21.2.0
    and better. ([issue
    5386](https://github.com/scrapy/scrapy/issues/5386){.reference
    .external}, [issue
    5406](https://github.com/scrapy/scrapy/issues/5406){.reference
    .external})

-   Fixed the [`finish_exporting()`{.xref .py .py-meth .docutils
    .literal .notranslate}]{.pre} method of [[item exporters]{.std
    .std-ref}](index.html#topics-exporters){.hoverxref .tooltip
    .reference .internal} not being called for empty files. ([issue
    5537](https://github.com/scrapy/scrapy/issues/5537){.reference
    .external}, [issue
    5758](https://github.com/scrapy/scrapy/issues/5758){.reference
    .external})

-   Fixed HTTP/2 responses getting only the last value for a header when
    multiple headers with the same name are received. ([issue
    5777](https://github.com/scrapy/scrapy/issues/5777){.reference
    .external})

-   Fixed an exception raised by the [[`shell`{.xref .std .std-command
    .docutils .literal
    .notranslate}]{.pre}](index.html#std-command-shell){.hoverxref
    .tooltip .reference .internal} command on some cases when [[using
    asyncio]{.std .std-ref}](index.html#using-asyncio){.hoverxref
    .tooltip .reference .internal}. ([issue
    5740](https://github.com/scrapy/scrapy/issues/5740){.reference
    .external}, [issue
    5742](https://github.com/scrapy/scrapy/issues/5742){.reference
    .external}, [issue
    5748](https://github.com/scrapy/scrapy/issues/5748){.reference
    .external}, [issue
    5759](https://github.com/scrapy/scrapy/issues/5759){.reference
    .external}, [issue
    5760](https://github.com/scrapy/scrapy/issues/5760){.reference
    .external}, [issue
    5771](https://github.com/scrapy/scrapy/issues/5771){.reference
    .external})

-   When using [[`CrawlSpider`{.xref .py .py-class .docutils .literal
    .notranslate}]{.pre}](index.html#scrapy.spiders.CrawlSpider "scrapy.spiders.CrawlSpider"){.reference
    .internal}, callback keyword arguments ([`cb_kwargs`{.docutils
    .literal .notranslate}]{.pre}) added to a request in the
    [`process_request`{.docutils .literal .notranslate}]{.pre} callback
    of a [[`Rule`{.xref .py .py-class .docutils .literal
    .notranslate}]{.pre}](index.html#scrapy.spiders.Rule "scrapy.spiders.Rule"){.reference
    .internal} will no longer be ignored. ([issue
    5699](https://github.com/scrapy/scrapy/issues/5699){.reference
    .external})

-   The [[images pipeline]{.std
    .std-ref}](index.html#images-pipeline){.hoverxref .tooltip
    .reference .internal} no longer re-encodes JPEG files. ([issue
    3055](https://github.com/scrapy/scrapy/issues/3055){.reference
    .external}, [issue
    3689](https://github.com/scrapy/scrapy/issues/3689){.reference
    .external}, [issue
    4753](https://github.com/scrapy/scrapy/issues/4753){.reference
    .external})

-   Fixed the handling of transparent WebP images by the [[images
    pipeline]{.std .std-ref}](index.html#images-pipeline){.hoverxref
    .tooltip .reference .internal}. ([issue
    3072](https://github.com/scrapy/scrapy/issues/3072){.reference
    .external}, [issue
    5766](https://github.com/scrapy/scrapy/issues/5766){.reference
    .external}, [issue
    5767](https://github.com/scrapy/scrapy/issues/5767){.reference
    .external})

-   [`scrapy.shell.inspect_response()`{.xref .py .py-func .docutils
    .literal .notranslate}]{.pre} no longer inhibits [`SIGINT`{.docutils
    .literal .notranslate}]{.pre} (Ctrl+C). ([issue
    2918](https://github.com/scrapy/scrapy/issues/2918){.reference
    .external})

-   [[`LinkExtractor`{.xref .py .py-class .docutils .literal
    .notranslate}]{.pre}](index.html#scrapy.linkextractors.lxmlhtml.LxmlLinkExtractor "scrapy.linkextractors.lxmlhtml.LxmlLinkExtractor"){.reference
    .internal} with [`unique=False`{.docutils .literal
    .notranslate}]{.pre} no longer filters out links that have identical
    URL *and* text. ([issue
    3798](https://github.com/scrapy/scrapy/issues/3798){.reference
    .external}, [issue
    3799](https://github.com/scrapy/scrapy/issues/3799){.reference
    .external}, [issue
    4695](https://github.com/scrapy/scrapy/issues/4695){.reference
    .external}, [issue
    5458](https://github.com/scrapy/scrapy/issues/5458){.reference
    .external})

-   [[`RobotsTxtMiddleware`{.xref .py .py-class .docutils .literal
    .notranslate}]{.pre}](index.html#scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware "scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware"){.reference
    .internal} now ignores URL protocols that do not support
    [`robots.txt`{.docutils .literal .notranslate}]{.pre}
    ([`data://`{.docutils .literal .notranslate}]{.pre},
    [`file://`{.docutils .literal .notranslate}]{.pre}). ([issue
    5807](https://github.com/scrapy/scrapy/issues/5807){.reference
    .external})

-   Silenced the [`filelock`{.docutils .literal .notranslate}]{.pre}
    debug log messages introduced in Scrapy 2.6. ([issue
    5753](https://github.com/scrapy/scrapy/issues/5753){.reference
    .external}, [issue
    5754](https://github.com/scrapy/scrapy/issues/5754){.reference
    .external})

-   Fixed the output of [`scrapy`{.docutils .literal
    .notranslate}]{.pre}` `{.docutils .literal
    .notranslate}[`-h`{.docutils .literal .notranslate}]{.pre} showing
    an unintended [`**commands**`{.docutils .literal
    .notranslate}]{.pre} line. ([issue
    5709](https://github.com/scrapy/scrapy/issues/5709){.reference
    .external}, [issue
    5711](https://github.com/scrapy/scrapy/issues/5711){.reference
    .external}, [issue
    5712](https://github.com/scrapy/scrapy/issues/5712){.reference
    .external})

-   Made the active project indication in the output of [[commands]{.std
    .std-ref}](index.html#topics-commands){.hoverxref .tooltip
    .reference .internal} more clear. ([issue
    5715](https://github.com/scrapy/scrapy/issues/5715){.reference
    .external})
:::

::: {#id17 .section}
##### Documentation[¶](#id17 "Permalink to this heading"){.headerlink}

-   Documented how to [[debug spiders from Visual Studio Code]{.std
    .std-ref}](index.html#debug-vscode){.hoverxref .tooltip .reference
    .internal}. ([issue
    5721](https://github.com/scrapy/scrapy/issues/5721){.reference
    .external})

-   Documented how [[`DOWNLOAD_DELAY`{.xref .std .std-setting .docutils
    .literal
    .notranslate}]{.pre}](index.html#std-setting-DOWNLOAD_DELAY){.hoverxref
    .tooltip .reference .internal} affects per-domain concurrency.
    ([issue
    5083](https://github.com/scrapy/scrapy/issues/5083){.reference
    .external}, [issue
    5540](https://github.com/scrapy/scrapy/issues/5540){.reference
    .external})

-   Improved consistency. ([issue
    5761](https://github.com/scrapy/scrapy/issues/5761){.reference
    .external})

-   Fixed typos. ([issue
    5714](https://github.com/scrapy/scrapy/issues/5714){.reference
    .external}, [issue
    5744](https://github.com/scrapy/scrapy/issues/5744){.reference
    .external}, [issue
    5764](https://github.com/scrapy/scrapy/issues/5764){.reference
    .external})
:::

::: {#id18 .section}
##### Quality assurance[¶](#id18 "Permalink to this heading"){.headerlink}

-   Applied [[black coding style]{.std
    .std-ref}](index.html#coding-style){.hoverxref .tooltip .reference
    .internal}, sorted import statements, and introduced
    [[pre-commit]{.std
    .std-ref}](index.html#scrapy-pre-commit){.hoverxref .tooltip
    .reference .internal}. ([issue
    4654](https://github.com/scrapy/scrapy/issues/4654){.reference
    .external}, [issue
    4658](https://github.com/scrapy/scrapy/issues/4658){.reference
    .external}, [issue
    5734](https://github.com/scrapy/scrapy/issues/5734){.reference
    .external}, [issue
    5737](https://github.com/scrapy/scrapy/issues/5737){.reference
    .external}, [issue
    5806](https://github.com/scrapy/scrapy/issues/5806){.reference
    .external}, [issue
    5810](https://github.com/scrapy/scrapy/issues/5810){.reference
    .external})

-   Switched from [[`os.path`{.xref .py .py-mod .docutils .literal
    .notranslate}]{.pre}](https://docs.python.org/3/library/os.path.html#module-os.path "(in Python v3.12)"){.reference
    .external} to [[`pathlib`{.xref .py .py-mod .docutils .literal
    .notranslate}]{.pre}](https://docs.python.org/3/library/pathlib.html#module-pathlib "(in Python v3.12)"){.reference
    .external}. ([issue
    4916](https://github.com/scrapy/scrapy/issues/4916){.reference
    .external}, [issue
    4497](https://github.com/scrapy/scrapy/issues/4497){.reference
    .external}, [issue
    5682](https://github.com/scrapy/scrapy/issues/5682){.reference
    .external})

-   Addressed many issues reported by Pylint. ([issue
    5677](https://github.com/scrapy/scrapy/issues/5677){.reference
    .external})

-   Improved code readability. ([issue
    5736](https://github.com/scrapy/scrapy/issues/5736){.reference
    .external})

-   Improved package metadata. ([issue
    5768](https://github.com/scrapy/scrapy/issues/5768){.reference
    .external})

-   Removed direct invocations of [`setup.py`{.docutils .literal
    .notranslate}]{.pre}. ([issue
    5774](https://github.com/scrapy/scrapy/issues/5774){.reference
    .external}, [issue
    5776](https://github.com/scrapy/scrapy/issues/5776){.reference
    .external})

-   Removed unnecessary [[`OrderedDict`{.xref .py .py-class .docutils
    .literal
    .notranslate}]{.pre}](https://docs.python.org/3/library/collections.html#collections.OrderedDict "(in Python v3.12)"){.reference
    .external} usages. ([issue
    5795](https://github.com/scrapy/scrapy/issues/5795){.reference
    .external})

-   Removed unnecessary [`__str__`{.docutils .literal
    .notranslate}]{.pre} definitions. ([issue
    5150](https://github.com/scrapy/scrapy/issues/5150){.reference
    .external})

-   Removed obsolete code and comments. ([issue
    5725](https://github.com/scrapy/scrapy/issues/5725){.reference
    .external}, [issue
    5729](https://github.com/scrapy/scrapy/issues/5729){.reference
    .external}, [issue
    5730](https://github.com/scrapy/scrapy/issues/5730){.reference
    .external}, [issue
    5732](https://github.com/scrapy/scrapy/issues/5732){.reference
    .external})

-   Fixed test and CI issues. ([issue
    5749](https://github.com/scrapy/scrapy/issues/5749){.reference
    .external}, [issue
    5750](https://github.com/scrapy/scrapy/issues/5750){.reference
    .external}, [issue
    5756](https://github.com/scrapy/scrapy/issues/5756){.reference
    .external}, [issue
    5762](https://github.com/scrapy/scrapy/issues/5762){.reference
    .external}, [issue
    5765](https://github.com/scrapy/scrapy/issues/5765){.reference
    .external}, [issue
    5780](https://github.com/scrapy/scrapy/issues/5780){.reference
    .external}, [issue
    5781](https://github.com/scrapy/scrapy/issues/5781){.reference
    .external}, [issue
    5782](https://github.com/scrapy/scrapy/issues/5782){.reference
    .external}, [issue
    5783](https://github.com/scrapy/scrapy/issues/5783){.reference
    .external}, [issue
    5785](https://github.com/scrapy/scrapy/issues/5785){.reference
    .external}, [issue
    5786](https://github.com/scrapy/scrapy/issues/5786){.reference
    .external})
:::
:::

::: {#scrapy-2-7-1-2022-11-02 .section}
[]{#release-2-7-1}

#### Scrapy 2.7.1 (2022-11-02)[¶](#scrapy-2-7-1-2022-11-02 "Permalink to this heading"){.headerlink}

::: {#id19 .section}
##### New features[¶](#id19 "Permalink to this heading"){.headerlink}

-   Relaxed the restriction introduced in 2.6.2 so that the
    [`Proxy-Authorization`{.docutils .literal .notranslate}]{.pre}
    header can again be set explicitly, as long as the proxy URL in the
    [[`proxy`{.xref .std .std-reqmeta .docutils .literal
    .notranslate}]{.pre}](index.html#std-reqmeta-proxy){.hoverxref
    .tooltip .reference .internal} metadata has no other credentials,
    and for as long as that proxy URL remains the same; this restores
    compatibility with scrapy-zyte-smartproxy 2.1.0 and older ([issue
    5626](https://github.com/scrapy/scrapy/issues/5626){.reference
    .external}).
:::

::: {#id20 .section}
##### Bug fixes[¶](#id20 "Permalink to this heading"){.headerlink}

-   Using [`-O`{.docutils .literal
    .notranslate}]{.pre}/[`--overwrite-output`{.docutils .literal
    .notranslate}]{.pre} and [`-t`{.docutils .literal
    .notranslate}]{.pre}/[`--output-format`{.docutils .literal
    .notranslate}]{.pre} options together now produces an error instead
    of ignoring the former option ([issue
    5516](https://github.com/scrapy/scrapy/issues/5516){.reference
    .external}, [issue
    5605](https://github.com/scrapy/scrapy/issues/5605){.reference
    .external}).

-   Replaced deprecated [[`asyncio`{.xref .py .py-mod .docutils .literal
    .notranslate}]{.pre}](https://docs.python.org/3/library/asyncio.html#module-asyncio "(in Python v3.12)"){.reference
    .external} APIs that implicitly use the current event loop with code
    that explicitly requests a loop from the event loop policy ([issue
    5685](https://github.com/scrapy/scrapy/issues/5685){.reference
    .external}, [issue
    5689](https://github.com/scrapy/scrapy/issues/5689){.reference
    .external}).

-   Fixed uses of deprecated Scrapy APIs in Scrapy itself ([issue
    5588](https://github.com/scrapy/scrapy/issues/5588){.reference
    .external}, [issue
    5589](https://github.com/scrapy/scrapy/issues/5589){.reference
    .external}).

-   Fixed uses of a deprecated Pillow API ([issue
    5684](https://github.com/scrapy/scrapy/issues/5684){.reference
    .external}, [issue
    5692](https://github.com/scrapy/scrapy/issues/5692){.reference
    .external}).

-   Improved code that checks if generators return values, so that it no
    longer fails on decorated methods and partial methods ([issue
    5323](https://github.com/scrapy/scrapy/issues/5323){.reference
    .external}, [issue
    5592](https://github.com/scrapy/scrapy/issues/5592){.reference
    .external}, [issue
    5599](https://github.com/scrapy/scrapy/issues/5599){.reference
    .external}, [issue
    5691](https://github.com/scrapy/scrapy/issues/5691){.reference
    .external}).
:::

::: {#id21 .section}
##### Documentation[¶](#id21 "Permalink to this heading"){.headerlink}

-   Upgraded the Code of Conduct to Contributor Covenant v2.1 ([issue
    5698](https://github.com/scrapy/scrapy/issues/5698){.reference
    .external}).

-   Fixed typos ([issue
    5681](https://github.com/scrapy/scrapy/issues/5681){.reference
    .external}, [issue
    5694](https://github.com/scrapy/scrapy/issues/5694){.reference
    .external}).
:::

::: {#id22 .section}
##### Quality assurance[¶](#id22 "Permalink to this heading"){.headerlink}

-   Re-enabled some erroneously disabled flake8 checks ([issue
    5688](https://github.com/scrapy/scrapy/issues/5688){.reference
    .external}).

-   Ignored harmless deprecation warnings from [[`typing`{.xref .py
    .py-mod .docutils .literal
    .notranslate}]{.pre}](https://docs.python.org/3/library/typing.html#module-typing "(in Python v3.12)"){.reference
    .external} in tests ([issue
    5686](https://github.com/scrapy/scrapy/issues/5686){.reference
    .external}, [issue
    5697](https://github.com/scrapy/scrapy/issues/5697){.reference
    .external}).

-   Modernized our CI configuration ([issue
    5695](https://github.com/scrapy/scrapy/issues/5695){.reference
    .external}, [issue
    5696](https://github.com/scrapy/scrapy/issues/5696){.reference
    .external}).
:::
:::

::: {#scrapy-2-7-0-2022-10-17 .section}
[]{#release-2-7-0}

#### Scrapy 2.7.0 (2022-10-17)[¶](#scrapy-2-7-0-2022-10-17 "Permalink to this heading"){.headerlink}

Highlights:

-   Added Python 3.11 support, dropped Python 3.6 support

-   Improved support for [[asynchronous callbacks]{.std
    .std-ref}](index.html#topics-coroutines){.hoverxref .tooltip
    .reference .internal}

-   [[Asyncio support]{.std
    .std-ref}](index.html#using-asyncio){.hoverxref .tooltip .reference
    .internal} is enabled by default on new projects

-   Output names of item fields can now be arbitrary strings

-   Centralized [[request fingerprinting]{.std
    .std-ref}](index.html#request-fingerprints){.hoverxref .tooltip
    .reference .internal} configuration is now possible

::: {#id23 .section}
##### Modified requirements[¶](#id23 "Permalink to this heading"){.headerlink}

Python 3.7 or greater is now required; support for Python 3.6 has been
dropped. Support for the upcoming Python 3.11 has been added.

The minimum required version of some dependencies has changed as well:

-   [lxml](https://lxml.de/){.reference .external}: 3.5.0 → 4.3.0

-   [Pillow](https://python-pillow.org/){.reference .external} ([[images
    pipeline]{.std .std-ref}](index.html#images-pipeline){.hoverxref
    .tooltip .reference .internal}): 4.0.0 → 7.1.0

-   [zope.interface](https://zopeinterface.readthedocs.io/en/latest/){.reference
    .external}: 5.0.0 → 5.1.0

([issue 5512](https://github.com/scrapy/scrapy/issues/5512){.reference
.external}, [issue
5514](https://github.com/scrapy/scrapy/issues/5514){.reference
.external}, [issue
5524](https://github.com/scrapy/scrapy/issues/5524){.reference
.external}, [issue
5563](https://github.com/scrapy/scrapy/issues/5563){.reference
.external}, [issue
5664](https://github.com/scrapy/scrapy/issues/5664){.reference
.external}, [issue
5670](https://github.com/scrapy/scrapy/issues/5670){.reference
.external}, [issue
5678](https://github.com/scrapy/scrapy/issues/5678){.reference
.external})
:::

::: {#id24 .section}
##### Deprecations[¶](#id24 "Permalink to this heading"){.headerlink}

-   [[`ImagesPipeline.thumb_path`{.xref .py .py-meth .docutils .literal
    .notranslate}]{.pre}](index.html#scrapy.pipelines.images.ImagesPipeline.thumb_path "scrapy.pipelines.images.ImagesPipeline.thumb_path"){.reference
    .internal} must now accept an [`item`{.docutils .literal
    .notranslate}]{.pre} parameter ([issue
    5504](https://github.com/scrapy/scrapy/issues/5504){.reference
    .external}, [issue
    5508](https://github.com/scrapy/scrapy/issues/5508){.reference
    .external}).

-   The [`scrapy.downloadermiddlewares.decompression`{.docutils .literal
    .notranslate}]{.pre} module is now deprecated ([issue
    5546](https://github.com/scrapy/scrapy/issues/5546){.reference
    .external}, [issue
    5547](https://github.com/scrapy/scrapy/issues/5547){.reference
    .external}).
:::

::: {#id25 .section}
##### New features[¶](#id25 "Permalink to this heading"){.headerlink}

-   The [[`process_spider_output()`{.xref .py .py-meth .docutils
    .literal
    .notranslate}]{.pre}](index.html#scrapy.spidermiddlewares.SpiderMiddleware.process_spider_output "scrapy.spidermiddlewares.SpiderMiddleware.process_spider_output"){.reference
    .internal} method of [[spider middlewares]{.std
    .std-ref}](index.html#topics-spider-middleware){.hoverxref .tooltip
    .reference .internal} can now be defined as an [[asynchronous
    generator]{.xref .std
    .std-term}](https://docs.python.org/3/glossary.html#term-asynchronous-generator "(in Python v3.12)"){.reference
    .external} ([issue
    4978](https://github.com/scrapy/scrapy/issues/4978){.reference
    .external}).

-   The output of [`Request`{.xref .py .py-class .docutils .literal
    .notranslate}]{.pre} callbacks defined as [[coroutines]{.std
    .std-ref}](index.html#topics-coroutines){.hoverxref .tooltip
    .reference .internal} is now processed asynchronously ([issue
    4978](https://github.com/scrapy/scrapy/issues/4978){.reference
    .external}).

-   [`CrawlSpider`{.xref .py .py-class .docutils .literal
    .notranslate}]{.pre} now supports [[asynchronous callbacks]{.std
    .std-ref}](index.html#topics-coroutines){.hoverxref .tooltip
    .reference .internal} ([issue
    5657](https://github.com/scrapy/scrapy/issues/5657){.reference
    .external}).

-   New projects created with the [[`startproject`{.xref .std
    .std-command .docutils .literal
    .notranslate}]{.pre}](index.html#std-command-startproject){.hoverxref
    .tooltip .reference .internal} command have [[asyncio support]{.std
    .std-ref}](index.html#using-asyncio){.hoverxref .tooltip .reference
    .internal} enabled by default ([issue
    5590](https://github.com/scrapy/scrapy/issues/5590){.reference
    .external}, [issue
    5679](https://github.com/scrapy/scrapy/issues/5679){.reference
    .external}).

-   The [[`FEED_EXPORT_FIELDS`{.xref .std .std-setting .docutils
    .literal
    .notranslate}]{.pre}](index.html#std-setting-FEED_EXPORT_FIELDS){.hoverxref
    .tooltip .reference .internal} setting can now be defined as a
    dictionary to customize the output name of item fields, lifting the
    restriction that required output names to be valid Python
    identifiers, e.g. preventing them to have whitespace ([issue
    1008](https://github.com/scrapy/scrapy/issues/1008){.reference
    .external}, [issue
    3266](https://github.com/scrapy/scrapy/issues/3266){.reference
    .external}, [issue
    3696](https://github.com/scrapy/scrapy/issues/3696){.reference
    .external}).

-   You can now customize [[request fingerprinting]{.std
    .std-ref}](index.html#request-fingerprints){.hoverxref .tooltip
    .reference .internal} through the new
    [[`REQUEST_FINGERPRINTER_CLASS`{.xref .std .std-setting .docutils
    .literal
    .notranslate}]{.pre}](index.html#std-setting-REQUEST_FINGERPRINTER_CLASS){.hoverxref
    .tooltip .reference .internal} setting, instead of having to change
    it on every Scrapy component that relies on request fingerprinting
    ([issue 900](https://github.com/scrapy/scrapy/issues/900){.reference
    .external}, [issue
    3420](https://github.com/scrapy/scrapy/issues/3420){.reference
    .external}, [issue
    4113](https://github.com/scrapy/scrapy/issues/4113){.reference
    .external}, [issue
    4762](https://github.com/scrapy/scrapy/issues/4762){.reference
    .external}, [issue
    4524](https://github.com/scrapy/scrapy/issues/4524){.reference
    .external}).

-   [`jsonl`{.docutils .literal .notranslate}]{.pre} is now supported
    and encouraged as a file extension for [JSON
    Lines](https://jsonlines.org/){.reference .external} files ([issue
    4848](https://github.com/scrapy/scrapy/issues/4848){.reference
    .external}).

-   [[`ImagesPipeline.thumb_path`{.xref .py .py-meth .docutils .literal
    .notranslate}]{.pre}](index.html#scrapy.pipelines.images.ImagesPipeline.thumb_path "scrapy.pipelines.images.ImagesPipeline.thumb_path"){.reference
    .internal} now receives the source [[item]{.std
    .std-ref}](index.html#topics-items){.hoverxref .tooltip .reference
    .internal} ([issue
    5504](https://github.com/scrapy/scrapy/issues/5504){.reference
    .external}, [issue
    5508](https://github.com/scrapy/scrapy/issues/5508){.reference
    .external}).
:::

::: {#id26 .section}
##### Bug fixes[¶](#id26 "Permalink to this heading"){.headerlink}

-   When using Google Cloud Storage with a [[media pipeline]{.std
    .std-ref}](index.html#topics-media-pipeline){.hoverxref .tooltip
    .reference .internal}, [[`FILES_EXPIRES`{.xref .std .std-setting
    .docutils .literal
    .notranslate}]{.pre}](index.html#std-setting-FILES_EXPIRES){.hoverxref
    .tooltip .reference .internal} now also works when
    [[`FILES_STORE`{.xref .std .std-setting .docutils .literal
    .notranslate}]{.pre}](index.html#std-setting-FILES_STORE){.hoverxref
    .tooltip .reference .internal} does not point at the root of your
    Google Cloud Storage bucket ([issue
    5317](https://github.com/scrapy/scrapy/issues/5317){.reference
    .external}, [issue
    5318](https://github.com/scrapy/scrapy/issues/5318){.reference
    .external}).

-   The [[`parse`{.xref .std .std-command .docutils .literal
    .notranslate}]{.pre}](index.html#std-command-parse){.hoverxref
    .tooltip .reference .internal} command now supports [[asynchronous
    callbacks]{.std .std-ref}](index.html#topics-coroutines){.hoverxref
    .tooltip .reference .internal} ([issue
    5424](https://github.com/scrapy/scrapy/issues/5424){.reference
    .external}, [issue
    5577](https://github.com/scrapy/scrapy/issues/5577){.reference
    .external}).

-   When using the [[`parse`{.xref .std .std-command .docutils .literal
    .notranslate}]{.pre}](index.html#std-command-parse){.hoverxref
    .tooltip .reference .internal} command with a URL for which there is
    no available spider, an exception is no longer raised ([issue
    3264](https://github.com/scrapy/scrapy/issues/3264){.reference
    .external}, [issue
    3265](https://github.com/scrapy/scrapy/issues/3265){.reference
    .external}, [issue
    5375](https://github.com/scrapy/scrapy/issues/5375){.reference
    .external}, [issue
    5376](https://github.com/scrapy/scrapy/issues/5376){.reference
    .external}, [issue
    5497](https://github.com/scrapy/scrapy/issues/5497){.reference
    .external}).

-   [[`TextResponse`{.xref .py .py-class .docutils .literal
    .notranslate}]{.pre}](index.html#scrapy.http.TextResponse "scrapy.http.TextResponse"){.reference
    .internal} now gives higher priority to the [byte order
    mark](https://en.wikipedia.org/wiki/Byte_order_mark){.reference
    .external} when determining the text encoding of the response body,
    following the [HTML living
    standard](https://html.spec.whatwg.org/multipage/parsing.html#determining-the-character-encoding){.reference
    .external} ([issue
    5601](https://github.com/scrapy/scrapy/issues/5601){.reference
    .external}, [issue
    5611](https://github.com/scrapy/scrapy/issues/5611){.reference
    .external}).

-   MIME sniffing takes the response body into account in FTP and
    HTTP/1.0 requests, as well as in cached requests ([issue
    4873](https://github.com/scrapy/scrapy/issues/4873){.reference
    .external}).

-   MIME sniffing now detects valid HTML 5 documents even if the
    [`html`{.docutils .literal .notranslate}]{.pre} tag is missing
    ([issue
    4873](https://github.com/scrapy/scrapy/issues/4873){.reference
    .external}).

-   An exception is now raised if [[`ASYNCIO_EVENT_LOOP`{.xref .std
    .std-setting .docutils .literal
    .notranslate}]{.pre}](index.html#std-setting-ASYNCIO_EVENT_LOOP){.hoverxref
    .tooltip .reference .internal} has a value that does not match the
    asyncio event loop actually installed ([issue
    5529](https://github.com/scrapy/scrapy/issues/5529){.reference
    .external}).

-   Fixed [`Headers.getlist`{.xref .py .py-meth .docutils .literal
    .notranslate}]{.pre} returning only the last header ([issue
    5515](https://github.com/scrapy/scrapy/issues/5515){.reference
    .external}, [issue
    5526](https://github.com/scrapy/scrapy/issues/5526){.reference
    .external}).

-   Fixed [[`LinkExtractor`{.xref .py .py-class .docutils .literal
    .notranslate}]{.pre}](index.html#scrapy.linkextractors.lxmlhtml.LxmlLinkExtractor "scrapy.linkextractors.lxmlhtml.LxmlLinkExtractor"){.reference
    .internal} not ignoring the [`tar.gz`{.docutils .literal
    .notranslate}]{.pre} file extension by default ([issue
    1837](https://github.com/scrapy/scrapy/issues/1837){.reference
    .external}, [issue
    2067](https://github.com/scrapy/scrapy/issues/2067){.reference
    .external}, [issue
    4066](https://github.com/scrapy/scrapy/issues/4066){.reference
    .external})
:::

::: {#id27 .section}
##### Documentation[¶](#id27 "Permalink to this heading"){.headerlink}

-   Clarified the return type of [[`Spider.parse`{.xref .py .py-meth
    .docutils .literal
    .notranslate}]{.pre}](index.html#scrapy.Spider.parse "scrapy.Spider.parse"){.reference
    .internal} ([issue
    5602](https://github.com/scrapy/scrapy/issues/5602){.reference
    .external}, [issue
    5608](https://github.com/scrapy/scrapy/issues/5608){.reference
    .external}).

-   To enable [[`HttpCompressionMiddleware`{.xref .py .py-class
    .docutils .literal
    .notranslate}]{.pre}](index.html#scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware "scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware"){.reference
    .internal} to do [brotli
    compression](https://www.ietf.org/rfc/rfc7932.txt){.reference
    .external}, installing
    [brotli](https://github.com/google/brotli){.reference .external} is
    now recommended instead of installing
    [brotlipy](https://github.com/python-hyper/brotlipy/){.reference
    .external}, as the former provides a more recent version of brotli.

-   [[Signal documentation]{.std
    .std-ref}](index.html#topics-signals){.hoverxref .tooltip .reference
    .internal} now mentions [[coroutine support]{.std
    .std-ref}](index.html#topics-coroutines){.hoverxref .tooltip
    .reference .internal} and uses it in code examples ([issue
    4852](https://github.com/scrapy/scrapy/issues/4852){.reference
    .external}, [issue
    5358](https://github.com/scrapy/scrapy/issues/5358){.reference
    .external}).

-   [[Avoiding getting banned]{.std
    .std-ref}](index.html#bans){.hoverxref .tooltip .reference
    .internal} now recommends [Common
    Crawl](https://commoncrawl.org/){.reference .external} instead of
    [Google
    cache](http://www.googleguide.com/cached_pages.html){.reference
    .external} ([issue
    3582](https://github.com/scrapy/scrapy/issues/3582){.reference
    .external}, [issue
    5432](https://github.com/scrapy/scrapy/issues/5432){.reference
    .external}).

-   The new [[Components]{.std
    .std-ref}](index.html#topics-components){.hoverxref .tooltip
    .reference .internal} topic covers enforcing requirements on Scrapy
    components, like [[downloader middlewares]{.std
    .std-ref}](index.html#topics-downloader-middleware){.hoverxref
    .tooltip .reference .internal}, [[extensions]{.std
    .std-ref}](index.html#topics-extensions){.hoverxref .tooltip
    .reference .internal}, [[item pipelines]{.std
    .std-ref}](index.html#topics-item-pipeline){.hoverxref .tooltip
    .reference .internal}, [[spider middlewares]{.std
    .std-ref}](index.html#topics-spider-middleware){.hoverxref .tooltip
    .reference .internal}, and more; [[Enforcing asyncio as a
    requirement]{.std
    .std-ref}](index.html#enforce-asyncio-requirement){.hoverxref
    .tooltip .reference .internal} has also been added ([issue
    4978](https://github.com/scrapy/scrapy/issues/4978){.reference
    .external}).

-   [[Settings]{.std .std-ref}](index.html#topics-settings){.hoverxref
    .tooltip .reference .internal} now indicates that setting values
    must be [[picklable]{.xref .std
    .std-ref}](https://docs.python.org/3/library/pickle.html#pickle-picklable "(in Python v3.12)"){.reference
    .external} ([issue
    5607](https://github.com/scrapy/scrapy/issues/5607){.reference
    .external}, [issue
    5629](https://github.com/scrapy/scrapy/issues/5629){.reference
    .external}).

-   Removed outdated documentation ([issue
    5446](https://github.com/scrapy/scrapy/issues/5446){.reference
    .external}, [issue
    5373](https://github.com/scrapy/scrapy/issues/5373){.reference
    .external}, [issue
    5369](https://github.com/scrapy/scrapy/issues/5369){.reference
    .external}, [issue
    5370](https://github.com/scrapy/scrapy/issues/5370){.reference
    .external}, [issue
    5554](https://github.com/scrapy/scrapy/issues/5554){.reference
    .external}).

-   Fixed typos ([issue
    5442](https://github.com/scrapy/scrapy/issues/5442){.reference
    .external}, [issue
    5455](https://github.com/scrapy/scrapy/issues/5455){.reference
    .external}, [issue
    5457](https://github.com/scrapy/scrapy/issues/5457){.reference
    .external}, [issue
    5461](https://github.com/scrapy/scrapy/issues/5461){.reference
    .external}, [issue
    5538](https://github.com/scrapy/scrapy/issues/5538){.reference
    .external}, [issue
    5553](https://github.com/scrapy/scrapy/issues/5553){.reference
    .external}, [issue
    5558](https://github.com/scrapy/scrapy/issues/5558){.reference
    .external}, [issue
    5624](https://github.com/scrapy/scrapy/issues/5624){.reference
    .external}, [issue
    5631](https://github.com/scrapy/scrapy/issues/5631){.reference
    .external}).

-   Fixed other issues ([issue
    5283](https://github.com/scrapy/scrapy/issues/5283){.reference
    .external}, [issue
    5284](https://github.com/scrapy/scrapy/issues/5284){.reference
    .external}, [issue
    5559](https://github.com/scrapy/scrapy/issues/5559){.reference
    .external}, [issue
    5567](https://github.com/scrapy/scrapy/issues/5567){.reference
    .external}, [issue
    5648](https://github.com/scrapy/scrapy/issues/5648){.reference
    .external}, [issue
    5659](https://github.com/scrapy/scrapy/issues/5659){.reference
    .external}, [issue
    5665](https://github.com/scrapy/scrapy/issues/5665){.reference
    .external}).
:::

::: {#id28 .section}
##### Quality assurance[¶](#id28 "Permalink to this heading"){.headerlink}

-   Added a continuous integration job to run [twine
    check](https://twine.readthedocs.io/en/stable/#twine-check){.reference
    .external} ([issue
    5655](https://github.com/scrapy/scrapy/issues/5655){.reference
    .external}, [issue
    5656](https://github.com/scrapy/scrapy/issues/5656){.reference
    .external}).

-   Addressed test issues and warnings ([issue
    5560](https://github.com/scrapy/scrapy/issues/5560){.reference
    .external}, [issue
    5561](https://github.com/scrapy/scrapy/issues/5561){.reference
    .external}, [issue
    5612](https://github.com/scrapy/scrapy/issues/5612){.reference
    .external}, [issue
    5617](https://github.com/scrapy/scrapy/issues/5617){.reference
    .external}, [issue
    5639](https://github.com/scrapy/scrapy/issues/5639){.reference
    .external}, [issue
    5645](https://github.com/scrapy/scrapy/issues/5645){.reference
    .external}, [issue
    5662](https://github.com/scrapy/scrapy/issues/5662){.reference
    .external}, [issue
    5671](https://github.com/scrapy/scrapy/issues/5671){.reference
    .external}, [issue
    5675](https://github.com/scrapy/scrapy/issues/5675){.reference
    .external}).

-   Cleaned up code ([issue
    4991](https://github.com/scrapy/scrapy/issues/4991){.reference
    .external}, [issue
    4995](https://github.com/scrapy/scrapy/issues/4995){.reference
    .external}, [issue
    5451](https://github.com/scrapy/scrapy/issues/5451){.reference
    .external}, [issue
    5487](https://github.com/scrapy/scrapy/issues/5487){.reference
    .external}, [issue
    5542](https://github.com/scrapy/scrapy/issues/5542){.reference
    .external}, [issue
    5667](https://github.com/scrapy/scrapy/issues/5667){.reference
    .external}, [issue
    5668](https://github.com/scrapy/scrapy/issues/5668){.reference
    .external}, [issue
    5672](https://github.com/scrapy/scrapy/issues/5672){.reference
    .external}).

-   Applied minor code improvements ([issue
    5661](https://github.com/scrapy/scrapy/issues/5661){.reference
    .external}).
:::
:::

::: {#scrapy-2-6-3-2022-09-27 .section}
[]{#release-2-6-3}

#### Scrapy 2.6.3 (2022-09-27)[¶](#scrapy-2-6-3-2022-09-27 "Permalink to this heading"){.headerlink}

-   Added support for
    [pyOpenSSL](https://www.pyopenssl.org/en/stable/){.reference
    .external} 22.1.0, removing support for SSLv3 ([issue
    5634](https://github.com/scrapy/scrapy/issues/5634){.reference
    .external}, [issue
    5635](https://github.com/scrapy/scrapy/issues/5635){.reference
    .external}, [issue
    5636](https://github.com/scrapy/scrapy/issues/5636){.reference
    .external}).

-   Upgraded the minimum versions of the following dependencies:

    -   [cryptography](https://cryptography.io/en/latest/){.reference
        .external}: 2.0 → 3.3

    -   [pyOpenSSL](https://www.pyopenssl.org/en/stable/){.reference
        .external}: 16.2.0 → 21.0.0

    -   [service_identity](https://service-identity.readthedocs.io/en/stable/){.reference
        .external}: 16.0.0 → 18.1.0

    -   [Twisted](https://twistedmatrix.com/trac/){.reference
        .external}: 17.9.0 → 18.9.0

    -   [zope.interface](https://zopeinterface.readthedocs.io/en/latest/){.reference
        .external}: 4.1.3 → 5.0.0

    ([issue
    5621](https://github.com/scrapy/scrapy/issues/5621){.reference
    .external}, [issue
    5632](https://github.com/scrapy/scrapy/issues/5632){.reference
    .external})

-   Fixes test and documentation issues ([issue
    5612](https://github.com/scrapy/scrapy/issues/5612){.reference
    .external}, [issue
    5617](https://github.com/scrapy/scrapy/issues/5617){.reference
    .external}, [issue
    5631](https://github.com/scrapy/scrapy/issues/5631){.reference
    .external}).
:::

::: {#scrapy-2-6-2-2022-07-25 .section}
[]{#release-2-6-2}

#### Scrapy 2.6.2 (2022-07-25)[¶](#scrapy-2-6-2-2022-07-25 "Permalink to this heading"){.headerlink}

**Security bug fix:**

-   When [[`HttpProxyMiddleware`{.xref .py .py-class .docutils .literal
    .notranslate}]{.pre}](index.html#scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware "scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware"){.reference
    .internal} processes a request with [[`proxy`{.xref .std
    .std-reqmeta .docutils .literal
    .notranslate}]{.pre}](index.html#std-reqmeta-proxy){.hoverxref
    .tooltip .reference .internal} metadata, and that [[`proxy`{.xref
    .std .std-reqmeta .docutils .literal
    .notranslate}]{.pre}](index.html#std-reqmeta-proxy){.hoverxref
    .tooltip .reference .internal} metadata includes proxy credentials,
    [[`HttpProxyMiddleware`{.xref .py .py-class .docutils .literal
    .notranslate}]{.pre}](index.html#scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware "scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware"){.reference
    .internal} sets the [`Proxy-Authorization`{.docutils .literal
    .notranslate}]{.pre} header, but only if that header is not already
    set.

    There are third-party proxy-rotation downloader middlewares that set
    different [[`proxy`{.xref .std .std-reqmeta .docutils .literal
    .notranslate}]{.pre}](index.html#std-reqmeta-proxy){.hoverxref
    .tooltip .reference .internal} metadata every time they process a
    request.

    Because of request retries and redirects, the same request can be
    processed by downloader middlewares more than once, including both
    [[`HttpProxyMiddleware`{.xref .py .py-class .docutils .literal
    .notranslate}]{.pre}](index.html#scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware "scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware"){.reference
    .internal} and any third-party proxy-rotation downloader middleware.

    These third-party proxy-rotation downloader middlewares could change
    the [[`proxy`{.xref .std .std-reqmeta .docutils .literal
    .notranslate}]{.pre}](index.html#std-reqmeta-proxy){.hoverxref
    .tooltip .reference .internal} metadata of a request to a new value,
    but fail to remove the [`Proxy-Authorization`{.docutils .literal
    .notranslate}]{.pre} header from the previous value of the
    [[`proxy`{.xref .std .std-reqmeta .docutils .literal
    .notranslate}]{.pre}](index.html#std-reqmeta-proxy){.hoverxref
    .tooltip .reference .internal} metadata, causing the credentials of
    one proxy to be sent to a different proxy.

    To prevent the unintended leaking of proxy credentials, the behavior
    of [[`HttpProxyMiddleware`{.xref .py .py-class .docutils .literal
    .notranslate}]{.pre}](index.html#scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware "scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware"){.reference
    .internal} is now as follows when processing a request:

    -   If the request being processed defines [[`proxy`{.xref .std
        .std-reqmeta .docutils .literal
        .notranslate}]{.pre}](index.html#std-reqmeta-proxy){.hoverxref
        .tooltip .reference .internal} metadata that includes
        credentials, the [`Proxy-Authorization`{.docutils .literal
        .notranslate}]{.pre} header is always updated to feature those
        credentials.

    -   If the request being processed defines [[`proxy`{.xref .std
        .std-reqmeta .docutils .literal
        .notranslate}]{.pre}](index.html#std-reqmeta-proxy){.hoverxref
        .tooltip .reference .internal} metadata without credentials, the
        [`Proxy-Authorization`{.docutils .literal .notranslate}]{.pre}
        header is removed *unless* it was originally defined for the
        same proxy URL.

        To remove proxy credentials while keeping the same proxy URL,
        remove the [`Proxy-Authorization`{.docutils .literal
        .notranslate}]{.pre} header.

    -   If the request has no [[`proxy`{.xref .std .std-reqmeta
        .docutils .literal
        .notranslate}]{.pre}](index.html#std-reqmeta-proxy){.hoverxref
        .tooltip .reference .internal} metadata, or that metadata is a
        falsy value (e.g. [`None`{.docutils .literal
        .notranslate}]{.pre}), the [`Proxy-Authorization`{.docutils
        .literal .notranslate}]{.pre} header is removed.

        It is no longer possible to set a proxy URL through the
        [[`proxy`{.xref .std .std-reqmeta .docutils .literal
        .notranslate}]{.pre}](index.html#std-reqmeta-proxy){.hoverxref
        .tooltip .reference .internal} metadata but set the credentials
        through the [`Proxy-Authorization`{.docutils .literal
        .notranslate}]{.pre} header. Set proxy credentials through the
        [[`proxy`{.xref .std .std-reqmeta .docutils .literal
        .notranslate}]{.pre}](index.html#std-reqmeta-proxy){.hoverxref
        .tooltip .reference .internal} metadata instead.

Also fixes the following regressions introduced in 2.6.0:

-   [[`CrawlerProcess`{.xref .py .py-class .docutils .literal
    .notranslate}]{.pre}](index.html#scrapy.crawler.CrawlerProcess "scrapy.crawler.CrawlerProcess"){.reference
    .internal} supports again crawling multiple spiders ([issue
    5435](https://github.com/scrapy/scrapy/issues/5435){.reference
    .external}, [issue
    5436](https://github.com/scrapy/scrapy/issues/5436){.reference
    .external})

-   Installing a Twisted reactor before Scrapy does (e.g. importing
    [[`twisted.internet.reactor`{.xref .py .py-mod .docutils .literal
    .notranslate}]{.pre}](https://docs.twisted.org/en/stable/api/twisted.internet.reactor.html "(in Twisted)"){.reference
    .external} somewhere at the module level) no longer prevents Scrapy
    from starting, as long as a different reactor is not specified in
    [[`TWISTED_REACTOR`{.xref .std .std-setting .docutils .literal
    .notranslate}]{.pre}](index.html#std-setting-TWISTED_REACTOR){.hoverxref
    .tooltip .reference .internal} ([issue
    5525](https://github.com/scrapy/scrapy/issues/5525){.reference
    .external}, [issue
    5528](https://github.com/scrapy/scrapy/issues/5528){.reference
    .external})

-   Fixed an exception that was being logged after the spider finished
    under certain conditions ([issue
    5437](https://github.com/scrapy/scrapy/issues/5437){.reference
    .external}, [issue
    5440](https://github.com/scrapy/scrapy/issues/5440){.reference
    .external})

-   The [`--output`{.docutils .literal
    .notranslate}]{.pre}/[`-o`{.docutils .literal .notranslate}]{.pre}
    command-line parameter supports again a value starting with a hyphen
    ([issue
    5444](https://github.com/scrapy/scrapy/issues/5444){.reference
    .external}, [issue
    5445](https://github.com/scrapy/scrapy/issues/5445){.reference
    .external})

-   The [`scrapy`{.docutils .literal .notranslate}]{.pre}` `{.docutils
    .literal .notranslate}[`parse`{.docutils .literal
    .notranslate}]{.pre}` `{.docutils .literal
    .notranslate}[`-h`{.docutils .literal .notranslate}]{.pre} command
    no longer throws an error ([issue
    5481](https://github.com/scrapy/scrapy/issues/5481){.reference
    .external}, [issue
    5482](https://github.com/scrapy/scrapy/issues/5482){.reference
    .external})
:::

::: {#scrapy-2-6-1-2022-03-01 .section}
[]{#release-2-6-1}

#### Scrapy 2.6.1 (2022-03-01)[¶](#scrapy-2-6-1-2022-03-01 "Permalink to this heading"){.headerlink}

Fixes a regression introduced in 2.6.0 that would unset the request
method when following redirects.
:::

::: {#scrapy-2-6-0-2022-03-01 .section}
[]{#release-2-6-0}

#### Scrapy 2.6.0 (2022-03-01)[¶](#scrapy-2-6-0-2022-03-01 "Permalink to this heading"){.headerlink}

Highlights:

-   [[Security fixes for cookie handling]{.std
    .std-ref}](#security-fixes){.hoverxref .tooltip .reference
    .internal}

-   Python 3.10 support

-   [[asyncio support]{.std
    .std-ref}](index.html#using-asyncio){.hoverxref .tooltip .reference
    .internal} is no longer considered experimental, and works
    out-of-the-box on Windows regardless of your Python version

-   Feed exports now support [[`pathlib.Path`{.xref .py .py-class
    .docutils .literal
    .notranslate}]{.pre}](https://docs.python.org/3/library/pathlib.html#pathlib.Path "(in Python v3.12)"){.reference
    .external} output paths and per-feed [[item filtering]{.std
    .std-ref}](index.html#item-filter){.hoverxref .tooltip .reference
    .internal} and [[post-processing]{.std
    .std-ref}](index.html#post-processing){.hoverxref .tooltip
    .reference .internal}

::: {#security-bug-fixes .section}
[]{#security-fixes}

##### Security bug fixes[¶](#security-bug-fixes "Permalink to this heading"){.headerlink}

-   When a [[`Request`{.xref .py .py-class .docutils .literal
    .notranslate}]{.pre}](index.html#scrapy.http.Request "scrapy.http.Request"){.reference
    .internal} object with cookies defined gets a redirect response
    causing a new [[`Request`{.xref .py .py-class .docutils .literal
    .notranslate}]{.pre}](index.html#scrapy.http.Request "scrapy.http.Request"){.reference
    .internal} object to be scheduled, the cookies defined in the
    original [[`Request`{.xref .py .py-class .docutils .literal
    .notranslate}]{.pre}](index.html#scrapy.http.Request "scrapy.http.Request"){.reference
    .internal} object are no longer copied into the new
    [[`Request`{.xref .py .py-class .docutils .literal
    .notranslate}]{.pre}](index.html#scrapy.http.Request "scrapy.http.Request"){.reference
    .internal} object.

    If you manually set the [`Cookie`{.docutils .literal
    .notranslate}]{.pre} header on a [[`Request`{.xref .py .py-class
    .docutils .literal
    .notranslate}]{.pre}](index.html#scrapy.http.Request "scrapy.http.Request"){.reference
    .internal} object and the domain name of the redirect URL is not an
    exact match for the domain of the URL of the original
    [[`Request`{.xref .py .py-class .docutils .literal
    .notranslate}]{.pre}](index.html#scrapy.http.Request "scrapy.http.Request"){.reference
    .internal} object, your [`Cookie`{.docutils .literal
    .notranslate}]{.pre} header is now dropped from the new
    [[`Request`{.xref .py .py-class .docutils .literal
    .notranslate}]{.pre}](index.html#scrapy.http.Request "scrapy.http.Request"){.reference
    .internal} object.

    The old behavior could be exploited by an attacker to gain access to
    your cookies. Please, see the [cjvr-mfj7-j4j8 security
    advisory](https://github.com/scrapy/scrapy/security/advisories/GHSA-cjvr-mfj7-j4j8){.reference
    .external} for more information.

    ::: {.admonition .note}
    Note

    It is still possible to enable the sharing of cookies between
    different domains with a shared domain suffix (e.g.
    [`example.com`{.docutils .literal .notranslate}]{.pre} and any
    subdomain) by defining the shared domain suffix (e.g.
    [`example.com`{.docutils .literal .notranslate}]{.pre}) as the
    cookie domain when defining your cookies. See the documentation of
    the [[`Request`{.xref .py .py-class .docutils .literal
    .notranslate}]{.pre}](index.html#scrapy.http.Request "scrapy.http.Request"){.reference
    .internal} class for more information.
    :::

-   When the domain of a cookie, either received in the
    [`Set-Cookie`{.docutils .literal .notranslate}]{.pre} header of a
    response or defined in a [[`Request`{.xref .py .py-class .docutils
    .literal
    .notranslate}]{.pre}](index.html#scrapy.http.Request "scrapy.http.Request"){.reference
    .internal} object, is set to a [public
    suffix](https://publicsuffix.org/){.reference .external}, the cookie
    is now ignored unless the cookie domain is the same as the request
    domain.

    The old behavior could be exploited by an attacker to inject cookies
    from a controlled domain into your cookiejar that could be sent to
    other domains not controlled by the attacker. Please, see the
    [mfjm-vh54-3f96 security
    advisory](https://github.com/scrapy/scrapy/security/advisories/GHSA-mfjm-vh54-3f96){.reference
    .external} for more information.
:::

::: {#id29 .section}
##### Modified requirements[¶](#id29 "Permalink to this heading"){.headerlink}

-   The [h2](https://pypi.org/project/h2/){.reference .external}
    dependency is now optional, only needed to [[enable HTTP/2
    support]{.std .std-ref}](index.html#http2){.hoverxref .tooltip
    .reference .internal}. ([issue
    5113](https://github.com/scrapy/scrapy/issues/5113){.reference
    .external})
:::

::: {#id30 .section}
##### Backward-incompatible changes[¶](#id30 "Permalink to this heading"){.headerlink}

-   The [`formdata`{.docutils .literal .notranslate}]{.pre} parameter of
    [`FormRequest`{.xref .py .py-class .docutils .literal
    .notranslate}]{.pre}, if specified for a non-POST request, now
    overrides the URL query string, instead of being appended to it.
    ([issue
    2919](https://github.com/scrapy/scrapy/issues/2919){.reference
    .external}, [issue
    3579](https://github.com/scrapy/scrapy/issues/3579){.reference
    .external})

-   When a function is assigned to the [[`FEED_URI_PARAMS`{.xref .std
    .std-setting .docutils .literal
    .notranslate}]{.pre}](index.html#std-setting-FEED_URI_PARAMS){.hoverxref
    .tooltip .reference .internal} setting, now the return value of that
    function, and not the [`params`{.docutils .literal
    .notranslate}]{.pre} input parameter, will determine the feed URI
    parameters, unless that return value is [`None`{.docutils .literal
    .notranslate}]{.pre}. ([issue
    4962](https://github.com/scrapy/scrapy/issues/4962){.reference
    .external}, [issue
    4966](https://github.com/scrapy/scrapy/issues/4966){.reference
    .external})

-   In [`scrapy.core.engine.ExecutionEngine`{.xref .py .py-class
    .docutils .literal .notranslate}]{.pre}, methods [`crawl()`{.xref
    .py .py-meth .docutils .literal .notranslate}]{.pre},
    [`download()`{.xref .py .py-meth .docutils .literal
    .notranslate}]{.pre}, [`schedule()`{.xref .py .py-meth .docutils
    .literal .notranslate}]{.pre}, and [`spider_is_idle()`{.xref .py
    .py-meth .docutils .literal .notranslate}]{.pre} now raise
    [[`RuntimeError`{.xref .py .py-exc .docutils .literal
    .notranslate}]{.pre}](https://docs.python.org/3/library/exceptions.html#RuntimeError "(in Python v3.12)"){.reference
    .external} if called before [`open_spider()`{.xref .py .py-meth
    .docutils .literal .notranslate}]{.pre}. ([issue
    5090](https://github.com/scrapy/scrapy/issues/5090){.reference
    .external})

    These methods used to assume that [`ExecutionEngine.slot`{.xref .py
    .py-attr .docutils .literal .notranslate}]{.pre} had been defined by
    a prior call to [`open_spider()`{.xref .py .py-meth .docutils
    .literal .notranslate}]{.pre}, so they were raising
    [[`AttributeError`{.xref .py .py-exc .docutils .literal
    .notranslate}]{.pre}](https://docs.python.org/3/library/exceptions.html#AttributeError "(in Python v3.12)"){.reference
    .external} instead.

-   If the API of the configured [[scheduler]{.std
    .std-ref}](index.html#topics-scheduler){.hoverxref .tooltip
    .reference .internal} does not meet expectations,
    [[`TypeError`{.xref .py .py-exc .docutils .literal
    .notranslate}]{.pre}](https://docs.python.org/3/library/exceptions.html#TypeError "(in Python v3.12)"){.reference
    .external} is now raised at startup time. Before, other exceptions
    would be raised at run time. ([issue
    3559](https://github.com/scrapy/scrapy/issues/3559){.reference
    .external})

-   The [`_encoding`{.docutils .literal .notranslate}]{.pre} field of
    serialized [[`Request`{.xref .py .py-class .docutils .literal
    .notranslate}]{.pre}](index.html#scrapy.http.Request "scrapy.http.Request"){.reference
    .internal} objects is now named [`encoding`{.docutils .literal
    .notranslate}]{.pre}, in line with all other fields ([issue
    5130](https://github.com/scrapy/scrapy/issues/5130){.reference
    .external})
:::

::: {#id31 .section}
##### Deprecation removals[¶](#id31 "Permalink to this heading"){.headerlink}

-   [`scrapy.http.TextResponse.body_as_unicode`{.docutils .literal
    .notranslate}]{.pre}, deprecated in Scrapy 2.2, has now been
    removed. ([issue
    5393](https://github.com/scrapy/scrapy/issues/5393){.reference
    .external})

-   [`scrapy.item.BaseItem`{.docutils .literal .notranslate}]{.pre},
    deprecated in Scrapy 2.2, has now been removed. ([issue
    5398](https://github.com/scrapy/scrapy/issues/5398){.reference
    .external})

-   [`scrapy.item.DictItem`{.docutils .literal .notranslate}]{.pre},
    deprecated in Scrapy 1.8, has now been removed. ([issue
    5398](https://github.com/scrapy/scrapy/issues/5398){.reference
    .external})

-   [`scrapy.Spider.make_requests_from_url`{.docutils .literal
    .notranslate}]{.pre}, deprecated in Scrapy 1.4, has now been
    removed. ([issue
    4178](https://github.com/scrapy/scrapy/issues/4178){.reference
    .external}, [issue
    4356](https://github.com/scrapy/scrapy/issues/4356){.reference
    .external})
:::

::: {#id32 .section}
##### Deprecations[¶](#id32 "Permalink to this heading"){.headerlink}

-   When a function is assigned to the [[`FEED_URI_PARAMS`{.xref .std
    .std-setting .docutils .literal
    .notranslate}]{.pre}](index.html#std-setting-FEED_URI_PARAMS){.hoverxref
    .tooltip .reference .internal} setting, returning [`None`{.docutils
    .literal .notranslate}]{.pre} or modifying the [`params`{.docutils
    .literal .notranslate}]{.pre} input parameter is now deprecated.
    Return a new dictionary instead. ([issue
    4962](https://github.com/scrapy/scrapy/issues/4962){.reference
    .external}, [issue
    4966](https://github.com/scrapy/scrapy/issues/4966){.reference
    .external})

-   [`scrapy.utils.reqser`{.xref .py .py-mod .docutils .literal
    .notranslate}]{.pre} is deprecated. ([issue
    5130](https://github.com/scrapy/scrapy/issues/5130){.reference
    .external})

    -   Instead of [`request_to_dict()`{.xref .py .py-func .docutils
        .literal .notranslate}]{.pre}, use the new
        [[`Request.to_dict`{.xref .py .py-meth .docutils .literal
        .notranslate}]{.pre}](index.html#scrapy.http.Request.to_dict "scrapy.http.Request.to_dict"){.reference
        .internal} method.

    -   Instead of [`request_from_dict()`{.xref .py .py-func .docutils
        .literal .notranslate}]{.pre}, use the new
        [[`scrapy.utils.request.request_from_dict()`{.xref .py .py-func
        .docutils .literal
        .notranslate}]{.pre}](index.html#scrapy.utils.request.request_from_dict "scrapy.utils.request.request_from_dict"){.reference
        .internal} function.

-   In [`scrapy.squeues`{.xref .py .py-mod .docutils .literal
    .notranslate}]{.pre}, the following queue classes are deprecated:
    [`PickleFifoDiskQueueNonRequest`{.xref .py .py-class .docutils
    .literal .notranslate}]{.pre},
    [`PickleLifoDiskQueueNonRequest`{.xref .py .py-class .docutils
    .literal .notranslate}]{.pre},
    [`MarshalFifoDiskQueueNonRequest`{.xref .py .py-class .docutils
    .literal .notranslate}]{.pre}, and
    [`MarshalLifoDiskQueueNonRequest`{.xref .py .py-class .docutils
    .literal .notranslate}]{.pre}. You should instead use:
    [`PickleFifoDiskQueue`{.xref .py .py-class .docutils .literal
    .notranslate}]{.pre}, [`PickleLifoDiskQueue`{.xref .py .py-class
    .docutils .literal .notranslate}]{.pre},
    [`MarshalFifoDiskQueue`{.xref .py .py-class .docutils .literal
    .notranslate}]{.pre}, and [`MarshalLifoDiskQueue`{.xref .py
    .py-class .docutils .literal .notranslate}]{.pre}. ([issue
    5117](https://github.com/scrapy/scrapy/issues/5117){.reference
    .external})

-   Many aspects of [`scrapy.core.engine.ExecutionEngine`{.xref .py
    .py-class .docutils .literal .notranslate}]{.pre} that come from a
    time when this class could handle multiple [[`Spider`{.xref .py
    .py-class .docutils .literal
    .notranslate}]{.pre}](index.html#scrapy.Spider "scrapy.Spider"){.reference
    .internal} objects at a time have been deprecated. ([issue
    5090](https://github.com/scrapy/scrapy/issues/5090){.reference
    .external})

    -   The [`has_capacity()`{.xref .py .py-meth .docutils .literal
        .notranslate}]{.pre} method is deprecated.

    -   The [`schedule()`{.xref .py .py-meth .docutils .literal
        .notranslate}]{.pre} method is deprecated, use [`crawl()`{.xref
        .py .py-meth .docutils .literal .notranslate}]{.pre} or
        [`download()`{.xref .py .py-meth .docutils .literal
        .notranslate}]{.pre} instead.

    -   The [`open_spiders`{.xref .py .py-attr .docutils .literal
        .notranslate}]{.pre} attribute is deprecated, use
        [`spider`{.xref .py .py-attr .docutils .literal
        .notranslate}]{.pre} instead.

    -   The [`spider`{.docutils .literal .notranslate}]{.pre} parameter
        is deprecated for the following methods:

        -   [`spider_is_idle()`{.xref .py .py-meth .docutils .literal
            .notranslate}]{.pre}

        -   [`crawl()`{.xref .py .py-meth .docutils .literal
            .notranslate}]{.pre}

        -   [`download()`{.xref .py .py-meth .docutils .literal
            .notranslate}]{.pre}

        Instead, call [`open_spider()`{.xref .py .py-meth .docutils
        .literal .notranslate}]{.pre} first to set the [[`Spider`{.xref
        .py .py-class .docutils .literal
        .notranslate}]{.pre}](index.html#scrapy.Spider "scrapy.Spider"){.reference
        .internal} object.
:::

::: {#id33 .section}
##### New features[¶](#id33 "Permalink to this heading"){.headerlink}

-   You can now use [[item filtering]{.std
    .std-ref}](index.html#item-filter){.hoverxref .tooltip .reference
    .internal} to control which items are exported to each output feed.
    ([issue
    4575](https://github.com/scrapy/scrapy/issues/4575){.reference
    .external}, [issue
    5178](https://github.com/scrapy/scrapy/issues/5178){.reference
    .external}, [issue
    5161](https://github.com/scrapy/scrapy/issues/5161){.reference
    .external}, [issue
    5203](https://github.com/scrapy/scrapy/issues/5203){.reference
    .external})

-   You can now apply [[post-processing]{.std
    .std-ref}](index.html#post-processing){.hoverxref .tooltip
    .reference .internal} to feeds, and [[built-in post-processing
    plugins]{.std .std-ref}](index.html#builtin-plugins){.hoverxref
    .tooltip .reference .internal} are provided for output file
    compression. ([issue
    2174](https://github.com/scrapy/scrapy/issues/2174){.reference
    .external}, [issue
    5168](https://github.com/scrapy/scrapy/issues/5168){.reference
    .external}, [issue
    5190](https://github.com/scrapy/scrapy/issues/5190){.reference
    .external})

-   The [[`FEEDS`{.xref .std .std-setting .docutils .literal
    .notranslate}]{.pre}](index.html#std-setting-FEEDS){.hoverxref
    .tooltip .reference .internal} setting now supports
    [[`pathlib.Path`{.xref .py .py-class .docutils .literal
    .notranslate}]{.pre}](https://docs.python.org/3/library/pathlib.html#pathlib.Path "(in Python v3.12)"){.reference
    .external} objects as keys. ([issue
    5383](https://github.com/scrapy/scrapy/issues/5383){.reference
    .external}, [issue
    5384](https://github.com/scrapy/scrapy/issues/5384){.reference
    .external})

-   Enabling [[asyncio]{.std
    .std-ref}](index.html#using-asyncio){.hoverxref .tooltip .reference
    .internal} while using Windows and Python 3.8 or later will
    automatically switch the asyncio event loop to one that allows
    Scrapy to work. See [[Windows-specific notes]{.std
    .std-ref}](index.html#asyncio-windows){.hoverxref .tooltip
    .reference .internal}. ([issue
    4976](https://github.com/scrapy/scrapy/issues/4976){.reference
    .external}, [issue
    5315](https://github.com/scrapy/scrapy/issues/5315){.reference
    .external})

-   The [[`genspider`{.xref .std .std-command .docutils .literal
    .notranslate}]{.pre}](index.html#std-command-genspider){.hoverxref
    .tooltip .reference .internal} command now supports a start URL
    instead of a domain name. ([issue
    4439](https://github.com/scrapy/scrapy/issues/4439){.reference
    .external})

-   [`scrapy.utils.defer`{.xref .py .py-mod .docutils .literal
    .notranslate}]{.pre} gained 2 new functions,
    [[`deferred_to_future()`{.xref .py .py-func .docutils .literal
    .notranslate}]{.pre}](index.html#scrapy.utils.defer.deferred_to_future "scrapy.utils.defer.deferred_to_future"){.reference
    .internal} and [[`maybe_deferred_to_future()`{.xref .py .py-func
    .docutils .literal
    .notranslate}]{.pre}](index.html#scrapy.utils.defer.maybe_deferred_to_future "scrapy.utils.defer.maybe_deferred_to_future"){.reference
    .internal}, to help [[await on Deferreds when using the asyncio
    reactor]{.std .std-ref}](index.html#asyncio-await-dfd){.hoverxref
    .tooltip .reference .internal}. ([issue
    5288](https://github.com/scrapy/scrapy/issues/5288){.reference
    .external})

-   [[Amazon S3 feed export storage]{.std
    .std-ref}](index.html#topics-feed-storage-s3){.hoverxref .tooltip
    .reference .internal} gained support for [temporary security
    credentials](https://docs.aws.amazon.com/general/latest/gr/aws-sec-cred-types.html#temporary-access-keys){.reference
    .external} ([[`AWS_SESSION_TOKEN`{.xref .std .std-setting .docutils
    .literal
    .notranslate}]{.pre}](index.html#std-setting-AWS_SESSION_TOKEN){.hoverxref
    .tooltip .reference .internal}) and endpoint customization
    ([[`AWS_ENDPOINT_URL`{.xref .std .std-setting .docutils .literal
    .notranslate}]{.pre}](index.html#std-setting-AWS_ENDPOINT_URL){.hoverxref
    .tooltip .reference .internal}). ([issue
    4998](https://github.com/scrapy/scrapy/issues/4998){.reference
    .external}, [issue
    5210](https://github.com/scrapy/scrapy/issues/5210){.reference
    .external})

-   New [[`LOG_FILE_APPEND`{.xref .std .std-setting .docutils .literal
    .notranslate}]{.pre}](index.html#std-setting-LOG_FILE_APPEND){.hoverxref
    .tooltip .reference .internal} setting to allow truncating the log
    file. ([issue
    5279](https://github.com/scrapy/scrapy/issues/5279){.reference
    .external})

-   [`Request.cookies`{.xref .py .py-attr .docutils .literal
    .notranslate}]{.pre} values that are [[`bool`{.xref .py .py-class
    .docutils .literal
    .notranslate}]{.pre}](https://docs.python.org/3/library/functions.html#bool "(in Python v3.12)"){.reference
    .external}, [[`float`{.xref .py .py-class .docutils .literal
    .notranslate}]{.pre}](https://docs.python.org/3/library/functions.html#float "(in Python v3.12)"){.reference
    .external} or [[`int`{.xref .py .py-class .docutils .literal
    .notranslate}]{.pre}](https://docs.python.org/3/library/functions.html#int "(in Python v3.12)"){.reference
    .external} are cast to [[`str`{.xref .py .py-class .docutils
    .literal
    .notranslate}]{.pre}](https://docs.python.org/3/library/stdtypes.html#str "(in Python v3.12)"){.reference
    .external}. ([issue
    5252](https://github.com/scrapy/scrapy/issues/5252){.reference
    .external}, [issue
    5253](https://github.com/scrapy/scrapy/issues/5253){.reference
    .external})

-   You may now raise [[`CloseSpider`{.xref .py .py-exc .docutils
    .literal
    .notranslate}]{.pre}](index.html#scrapy.exceptions.CloseSpider "scrapy.exceptions.CloseSpider"){.reference
    .internal} from a handler of the [[`spider_idle`{.xref .std
    .std-signal .docutils .literal
    .notranslate}]{.pre}](index.html#std-signal-spider_idle){.hoverxref
    .tooltip .reference .internal} signal to customize the reason why
    the spider is stopping. ([issue
    5191](https://github.com/scrapy/scrapy/issues/5191){.reference
    .external})

-   When using [[`HttpProxyMiddleware`{.xref .py .py-class .docutils
    .literal
    .notranslate}]{.pre}](index.html#scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware "scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware"){.reference
    .internal}, the proxy URL for non-HTTPS HTTP/1.1 requests no longer
    needs to include a URL scheme. ([issue
    4505](https://github.com/scrapy/scrapy/issues/4505){.reference
    .external}, [issue
    4649](https://github.com/scrapy/scrapy/issues/4649){.reference
    .external})

-   All built-in queues now expose a [`peek`{.docutils .literal
    .notranslate}]{.pre} method that returns the next queue object (like
    [`pop`{.docutils .literal .notranslate}]{.pre}) but does not remove
    the returned object from the queue. ([issue
    5112](https://github.com/scrapy/scrapy/issues/5112){.reference
    .external})

    If the underlying queue does not support peeking (e.g. because you
    are not using [`queuelib`{.docutils .literal .notranslate}]{.pre}
    1.6.1 or later), the [`peek`{.docutils .literal .notranslate}]{.pre}
    method raises [[`NotImplementedError`{.xref .py .py-exc .docutils
    .literal
    .notranslate}]{.pre}](https://docs.python.org/3/library/exceptions.html#NotImplementedError "(in Python v3.12)"){.reference
    .external}.

-   [[`Request`{.xref .py .py-class .docutils .literal
    .notranslate}]{.pre}](index.html#scrapy.http.Request "scrapy.http.Request"){.reference
    .internal} and [[`Response`{.xref .py .py-class .docutils .literal
    .notranslate}]{.pre}](index.html#scrapy.http.Response "scrapy.http.Response"){.reference
    .internal} now have an [`attributes`{.docutils .literal
    .notranslate}]{.pre} attribute that makes subclassing easier. For
    [[`Request`{.xref .py .py-class .docutils .literal
    .notranslate}]{.pre}](index.html#scrapy.http.Request "scrapy.http.Request"){.reference
    .internal}, it also allows subclasses to work with
    [[`scrapy.utils.request.request_from_dict()`{.xref .py .py-func
    .docutils .literal
    .notranslate}]{.pre}](index.html#scrapy.utils.request.request_from_dict "scrapy.utils.request.request_from_dict"){.reference
    .internal}. ([issue
    1877](https://github.com/scrapy/scrapy/issues/1877){.reference
    .external}, [issue
    5130](https://github.com/scrapy/scrapy/issues/5130){.reference
    .external}, [issue
    5218](https://github.com/scrapy/scrapy/issues/5218){.reference
    .external})

-   The [[`open()`{.xref .py .py-meth .docutils .literal
    .notranslate}]{.pre}](index.html#scrapy.core.scheduler.BaseScheduler.open "scrapy.core.scheduler.BaseScheduler.open"){.reference
    .internal} and [[`close()`{.xref .py .py-meth .docutils .literal
    .notranslate}]{.pre}](index.html#scrapy.core.scheduler.BaseScheduler.close "scrapy.core.scheduler.BaseScheduler.close"){.reference
    .internal} methods of the [[scheduler]{.std
    .std-ref}](index.html#topics-scheduler){.hoverxref .tooltip
    .reference .internal} are now optional. ([issue
    3559](https://github.com/scrapy/scrapy/issues/3559){.reference
    .external})

-   HTTP/1.1 [`TunnelError`{.xref .py .py-exc .docutils .literal
    .notranslate}]{.pre} exceptions now only truncate response bodies
    longer than 1000 characters, instead of those longer than 32
    characters, making it easier to debug such errors. ([issue
    4881](https://github.com/scrapy/scrapy/issues/4881){.reference
    .external}, [issue
    5007](https://github.com/scrapy/scrapy/issues/5007){.reference
    .external})

-   [[`ItemLoader`{.xref .py .py-class .docutils .literal
    .notranslate}]{.pre}](index.html#scrapy.loader.ItemLoader "scrapy.loader.ItemLoader"){.reference
    .internal} now supports non-text responses. ([issue
    5145](https://github.com/scrapy/scrapy/issues/5145){.reference
    .external}, [issue
    5269](https://github.com/scrapy/scrapy/issues/5269){.reference
    .external})
:::

::: {#id34 .section}
##### Bug fixes[¶](#id34 "Permalink to this heading"){.headerlink}

-   The [[`TWISTED_REACTOR`{.xref .std .std-setting .docutils .literal
    .notranslate}]{.pre}](index.html#std-setting-TWISTED_REACTOR){.hoverxref
    .tooltip .reference .internal} and [[`ASYNCIO_EVENT_LOOP`{.xref .std
    .std-setting .docutils .literal
    .notranslate}]{.pre}](index.html#std-setting-ASYNCIO_EVENT_LOOP){.hoverxref
    .tooltip .reference .internal} settings are no longer ignored if
    defined in [[`custom_settings`{.xref .py .py-attr .docutils .literal
    .notranslate}]{.pre}](index.html#scrapy.Spider.custom_settings "scrapy.Spider.custom_settings"){.reference
    .internal}. ([issue
    4485](https://github.com/scrapy/scrapy/issues/4485){.reference
    .external}, [issue
    5352](https://github.com/scrapy/scrapy/issues/5352){.reference
    .external})

-   Removed a module-level Twisted reactor import that could prevent
    [[using the asyncio reactor]{.std
    .std-ref}](index.html#using-asyncio){.hoverxref .tooltip .reference
    .internal}. ([issue
    5357](https://github.com/scrapy/scrapy/issues/5357){.reference
    .external})

-   The [[`startproject`{.xref .std .std-command .docutils .literal
    .notranslate}]{.pre}](index.html#std-command-startproject){.hoverxref
    .tooltip .reference .internal} command works with existing folders
    again. ([issue
    4665](https://github.com/scrapy/scrapy/issues/4665){.reference
    .external}, [issue
    4676](https://github.com/scrapy/scrapy/issues/4676){.reference
    .external})

-   The [[`FEED_URI_PARAMS`{.xref .std .std-setting .docutils .literal
    .notranslate}]{.pre}](index.html#std-setting-FEED_URI_PARAMS){.hoverxref
    .tooltip .reference .internal} setting now behaves as documented.
    ([issue
    4962](https://github.com/scrapy/scrapy/issues/4962){.reference
    .external}, [issue
    4966](https://github.com/scrapy/scrapy/issues/4966){.reference
    .external})

-   [`Request.cb_kwargs`{.xref .py .py-attr .docutils .literal
    .notranslate}]{.pre} once again allows the [`callback`{.docutils
    .literal .notranslate}]{.pre} keyword. ([issue
    5237](https://github.com/scrapy/scrapy/issues/5237){.reference
    .external}, [issue
    5251](https://github.com/scrapy/scrapy/issues/5251){.reference
    .external}, [issue
    5264](https://github.com/scrapy/scrapy/issues/5264){.reference
    .external})

-   Made [`scrapy.utils.response.open_in_browser()`{.xref .py .py-func
    .docutils .literal .notranslate}]{.pre} support more complex HTML.
    ([issue
    5319](https://github.com/scrapy/scrapy/issues/5319){.reference
    .external}, [issue
    5320](https://github.com/scrapy/scrapy/issues/5320){.reference
    .external})

-   Fixed [[`CSVFeedSpider.quotechar`{.xref .py .py-attr .docutils
    .literal
    .notranslate}]{.pre}](index.html#scrapy.spiders.CSVFeedSpider.quotechar "scrapy.spiders.CSVFeedSpider.quotechar"){.reference
    .internal} being interpreted as the CSV file encoding. ([issue
    5391](https://github.com/scrapy/scrapy/issues/5391){.reference
    .external}, [issue
    5394](https://github.com/scrapy/scrapy/issues/5394){.reference
    .external})

-   Added missing
    [setuptools](https://pypi.org/project/setuptools/){.reference
    .external} to the list of dependencies. ([issue
    5122](https://github.com/scrapy/scrapy/issues/5122){.reference
    .external})

-   [[`LinkExtractor`{.xref .py .py-class .docutils .literal
    .notranslate}]{.pre}](index.html#scrapy.linkextractors.lxmlhtml.LxmlLinkExtractor "scrapy.linkextractors.lxmlhtml.LxmlLinkExtractor"){.reference
    .internal} now also works as expected with links that have
    comma-separated [`rel`{.docutils .literal .notranslate}]{.pre}
    attribute values including [`nofollow`{.docutils .literal
    .notranslate}]{.pre}. ([issue
    5225](https://github.com/scrapy/scrapy/issues/5225){.reference
    .external})

-   Fixed a [[`TypeError`{.xref .py .py-exc .docutils .literal
    .notranslate}]{.pre}](https://docs.python.org/3/library/exceptions.html#TypeError "(in Python v3.12)"){.reference
    .external} that could be raised during [[feed export]{.std
    .std-ref}](index.html#topics-feed-exports){.hoverxref .tooltip
    .reference .internal} parameter parsing. ([issue
    5359](https://github.com/scrapy/scrapy/issues/5359){.reference
    .external})
:::

::: {#id35 .section}
##### Documentation[¶](#id35 "Permalink to this heading"){.headerlink}

-   [[asyncio support]{.std
    .std-ref}](index.html#using-asyncio){.hoverxref .tooltip .reference
    .internal} is no longer considered experimental. ([issue
    5332](https://github.com/scrapy/scrapy/issues/5332){.reference
    .external})

-   Included [[Windows-specific help for asyncio usage]{.std
    .std-ref}](index.html#asyncio-windows){.hoverxref .tooltip
    .reference .internal}. ([issue
    4976](https://github.com/scrapy/scrapy/issues/4976){.reference
    .external}, [issue
    5315](https://github.com/scrapy/scrapy/issues/5315){.reference
    .external})

-   Rewrote [[Using a headless browser]{.std
    .std-ref}](index.html#topics-headless-browsing){.hoverxref .tooltip
    .reference .internal} with up-to-date best practices. ([issue
    4484](https://github.com/scrapy/scrapy/issues/4484){.reference
    .external}, [issue
    4613](https://github.com/scrapy/scrapy/issues/4613){.reference
    .external})

-   Documented [[local file naming in media pipelines]{.std
    .std-ref}](index.html#topics-file-naming){.hoverxref .tooltip
    .reference .internal}. ([issue
    5069](https://github.com/scrapy/scrapy/issues/5069){.reference
    .external}, [issue
    5152](https://github.com/scrapy/scrapy/issues/5152){.reference
    .external})

-   [[Frequently Asked Questions]{.std
    .std-ref}](index.html#faq){.hoverxref .tooltip .reference .internal}
    now covers spider file name collision issues. ([issue
    2680](https://github.com/scrapy/scrapy/issues/2680){.reference
    .external}, [issue
    3669](https://github.com/scrapy/scrapy/issues/3669){.reference
    .external})

-   Provided better context and instructions to disable the
    [[`URLLENGTH_LIMIT`{.xref .std .std-setting .docutils .literal
    .notranslate}]{.pre}](index.html#std-setting-URLLENGTH_LIMIT){.hoverxref
    .tooltip .reference .internal} setting. ([issue
    5135](https://github.com/scrapy/scrapy/issues/5135){.reference
    .external}, [issue
    5250](https://github.com/scrapy/scrapy/issues/5250){.reference
    .external})

-   Documented that [[Reppy parser]{.std
    .std-ref}](index.html#reppy-parser){.hoverxref .tooltip .reference
    .internal} does not support Python 3.9+. ([issue
    5226](https://github.com/scrapy/scrapy/issues/5226){.reference
    .external}, [issue
    5231](https://github.com/scrapy/scrapy/issues/5231){.reference
    .external})

-   Documented [[the scheduler component]{.std
    .std-ref}](index.html#topics-scheduler){.hoverxref .tooltip
    .reference .internal}. ([issue
    3537](https://github.com/scrapy/scrapy/issues/3537){.reference
    .external}, [issue
    3559](https://github.com/scrapy/scrapy/issues/3559){.reference
    .external})

-   Documented the method used by [[media pipelines]{.std
    .std-ref}](index.html#topics-media-pipeline){.hoverxref .tooltip
    .reference .internal} to [[determine if a file has expired]{.std
    .std-ref}](index.html#file-expiration){.hoverxref .tooltip
    .reference .internal}. ([issue
    5120](https://github.com/scrapy/scrapy/issues/5120){.reference
    .external}, [issue
    5254](https://github.com/scrapy/scrapy/issues/5254){.reference
    .external})

-   [[Running multiple spiders in the same process]{.std
    .std-ref}](index.html#run-multiple-spiders){.hoverxref .tooltip
    .reference .internal} now features
    [`scrapy.utils.project.get_project_settings()`{.xref .py .py-func
    .docutils .literal .notranslate}]{.pre} usage. ([issue
    5070](https://github.com/scrapy/scrapy/issues/5070){.reference
    .external})

-   [[Running multiple spiders in the same process]{.std
    .std-ref}](index.html#run-multiple-spiders){.hoverxref .tooltip
    .reference .internal} now covers what happens when you define
    different per-spider values for some settings that cannot differ at
    run time. ([issue
    4485](https://github.com/scrapy/scrapy/issues/4485){.reference
    .external}, [issue
    5352](https://github.com/scrapy/scrapy/issues/5352){.reference
    .external})

-   Extended the documentation of the [[`StatsMailer`{.xref .py
    .py-class .docutils .literal
    .notranslate}]{.pre}](index.html#scrapy.extensions.statsmailer.StatsMailer "scrapy.extensions.statsmailer.StatsMailer"){.reference
    .internal} extension. ([issue
    5199](https://github.com/scrapy/scrapy/issues/5199){.reference
    .external}, [issue
    5217](https://github.com/scrapy/scrapy/issues/5217){.reference
    .external})

-   Added [[`JOBDIR`{.xref .std .std-setting .docutils .literal
    .notranslate}]{.pre}](index.html#std-setting-JOBDIR){.hoverxref
    .tooltip .reference .internal} to [[Settings]{.std
    .std-ref}](index.html#topics-settings){.hoverxref .tooltip
    .reference .internal}. ([issue
    5173](https://github.com/scrapy/scrapy/issues/5173){.reference
    .external}, [issue
    5224](https://github.com/scrapy/scrapy/issues/5224){.reference
    .external})

-   Documented [`Spider.attribute`{.xref .py .py-attr .docutils .literal
    .notranslate}]{.pre}. ([issue
    5174](https://github.com/scrapy/scrapy/issues/5174){.reference
    .external}, [issue
    5244](https://github.com/scrapy/scrapy/issues/5244){.reference
    .external})

-   Documented [[`TextResponse.urljoin`{.xref .py .py-attr .docutils
    .literal
    .notranslate}]{.pre}](index.html#scrapy.http.TextResponse.urljoin "scrapy.http.TextResponse.urljoin"){.reference
    .internal}. ([issue
    1582](https://github.com/scrapy/scrapy/issues/1582){.reference
    .external})

-   Added the [`body_length`{.docutils .literal .notranslate}]{.pre}
    parameter to the documented signature of the
    [[`headers_received`{.xref .std .std-signal .docutils .literal
    .notranslate}]{.pre}](index.html#std-signal-headers_received){.hoverxref
    .tooltip .reference .internal} signal. ([issue
    5270](https://github.com/scrapy/scrapy/issues/5270){.reference
    .external})

-   Clarified [[`SelectorList.get`{.xref .py .py-meth .docutils .literal
    .notranslate}]{.pre}](index.html#scrapy.selector.SelectorList.get "scrapy.selector.SelectorList.get"){.reference
    .internal} usage in the [[tutorial]{.std
    .std-ref}](index.html#intro-tutorial){.hoverxref .tooltip .reference
    .internal}. ([issue
    5256](https://github.com/scrapy/scrapy/issues/5256){.reference
    .external})

-   The documentation now features the shortest import path of classes
    with multiple import paths. ([issue
    2733](https://github.com/scrapy/scrapy/issues/2733){.reference
    .external}, [issue
    5099](https://github.com/scrapy/scrapy/issues/5099){.reference
    .external})

-   [`quotes.toscrape.com`{.docutils .literal .notranslate}]{.pre}
    references now use HTTPS instead of HTTP. ([issue
    5395](https://github.com/scrapy/scrapy/issues/5395){.reference
    .external}, [issue
    5396](https://github.com/scrapy/scrapy/issues/5396){.reference
    .external})

-   Added a link to [our Discord
    server](https://discord.gg/mv3yErfpvq){.reference .external} to
    [[Getting help]{.std .std-ref}](index.html#getting-help){.hoverxref
    .tooltip .reference .internal}. ([issue
    5421](https://github.com/scrapy/scrapy/issues/5421){.reference
    .external}, [issue
    5422](https://github.com/scrapy/scrapy/issues/5422){.reference
    .external})

-   The pronunciation of the project name is now [[officially]{.std
    .std-ref}](index.html#intro-overview){.hoverxref .tooltip .reference
    .internal} /ˈskreɪpaɪ/. ([issue
    5280](https://github.com/scrapy/scrapy/issues/5280){.reference
    .external}, [issue
    5281](https://github.com/scrapy/scrapy/issues/5281){.reference
    .external})

-   Added the Scrapy logo to the README. ([issue
    5255](https://github.com/scrapy/scrapy/issues/5255){.reference
    .external}, [issue
    5258](https://github.com/scrapy/scrapy/issues/5258){.reference
    .external})

-   Fixed issues and implemented minor improvements. ([issue
    3155](https://github.com/scrapy/scrapy/issues/3155){.reference
    .external}, [issue
    4335](https://github.com/scrapy/scrapy/issues/4335){.reference
    .external}, [issue
    5074](https://github.com/scrapy/scrapy/issues/5074){.reference
    .external}, [issue
    5098](https://github.com/scrapy/scrapy/issues/5098){.reference
    .external}, [issue
    5134](https://github.com/scrapy/scrapy/issues/5134){.reference
    .external}, [issue
    5180](https://github.com/scrapy/scrapy/issues/5180){.reference
    .external}, [issue
    5194](https://github.com/scrapy/scrapy/issues/5194){.reference
    .external}, [issue
    5239](https://github.com/scrapy/scrapy/issues/5239){.reference
    .external}, [issue
    5266](https://github.com/scrapy/scrapy/issues/5266){.reference
    .external}, [issue
    5271](https://github.com/scrapy/scrapy/issues/5271){.reference
    .external}, [issue
    5273](https://github.com/scrapy/scrapy/issues/5273){.reference
    .external}, [issue
    5274](https://github.com/scrapy/scrapy/issues/5274){.reference
    .external}, [issue
    5276](https://github.com/scrapy/scrapy/issues/5276){.reference
    .external}, [issue
    5347](https://github.com/scrapy/scrapy/issues/5347){.reference
    .external}, [issue
    5356](https://github.com/scrapy/scrapy/issues/5356){.reference
    .external}, [issue
    5414](https://github.com/scrapy/scrapy/issues/5414){.reference
    .external}, [issue
    5415](https://github.com/scrapy/scrapy/issues/5415){.reference
    .external}, [issue
    5416](https://github.com/scrapy/scrapy/issues/5416){.reference
    .external}, [issue
    5419](https://github.com/scrapy/scrapy/issues/5419){.reference
    .external}, [issue
    5420](https://github.com/scrapy/scrapy/issues/5420){.reference
    .external})
:::

::: {#id36 .section}
##### Quality Assurance[¶](#id36 "Permalink to this heading"){.headerlink}

-   Added support for Python 3.10. ([issue
    5212](https://github.com/scrapy/scrapy/issues/5212){.reference
    .external}, [issue
    5221](https://github.com/scrapy/scrapy/issues/5221){.reference
    .external}, [issue
    5265](https://github.com/scrapy/scrapy/issues/5265){.reference
    .external})

-   Significantly reduced memory usage by
    [`scrapy.utils.response.response_httprepr()`{.xref .py .py-func
    .docutils .literal .notranslate}]{.pre}, used by the
    [[`DownloaderStats`{.xref .py .py-class .docutils .literal
    .notranslate}]{.pre}](index.html#scrapy.downloadermiddlewares.stats.DownloaderStats "scrapy.downloadermiddlewares.stats.DownloaderStats"){.reference
    .internal} downloader middleware, which is enabled by default.
    ([issue
    4964](https://github.com/scrapy/scrapy/issues/4964){.reference
    .external}, [issue
    4972](https://github.com/scrapy/scrapy/issues/4972){.reference
    .external})

-   Removed uses of the deprecated [[`optparse`{.xref .py .py-mod
    .docutils .literal
    .notranslate}]{.pre}](https://docs.python.org/3/library/optparse.html#module-optparse "(in Python v3.12)"){.reference
    .external} module. ([issue
    5366](https://github.com/scrapy/scrapy/issues/5366){.reference
    .external}, [issue
    5374](https://github.com/scrapy/scrapy/issues/5374){.reference
    .external})

-   Extended typing hints. ([issue
    5077](https://github.com/scrapy/scrapy/issues/5077){.reference
    .external}, [issue
    5090](https://github.com/scrapy/scrapy/issues/5090){.reference
    .external}, [issue
    5100](https://github.com/scrapy/scrapy/issues/5100){.reference
    .external}, [issue
    5108](https://github.com/scrapy/scrapy/issues/5108){.reference
    .external}, [issue
    5171](https://github.com/scrapy/scrapy/issues/5171){.reference
    .external}, [issue
    5215](https://github.com/scrapy/scrapy/issues/5215){.reference
    .external}, [issue
    5334](https://github.com/scrapy/scrapy/issues/5334){.reference
    .external})

-   Improved tests, fixed CI issues, removed unused code. ([issue
    5094](https://github.com/scrapy/scrapy/issues/5094){.reference
    .external}, [issue
    5157](https://github.com/scrapy/scrapy/issues/5157){.reference
    .external}, [issue
    5162](https://github.com/scrapy/scrapy/issues/5162){.reference
    .external}, [issue
    5198](https://github.com/scrapy/scrapy/issues/5198){.reference
    .external}, [issue
    5207](https://github.com/scrapy/scrapy/issues/5207){.reference
    .external}, [issue
    5208](https://github.com/scrapy/scrapy/issues/5208){.reference
    .external}, [issue
    5229](https://github.com/scrapy/scrapy/issues/5229){.reference
    .external}, [issue
    5298](https://github.com/scrapy/scrapy/issues/5298){.reference
    .external}, [issue
    5299](https://github.com/scrapy/scrapy/issues/5299){.reference
    .external}, [issue
    5310](https://github.com/scrapy/scrapy/issues/5310){.reference
    .external}, [issue
    5316](https://github.com/scrapy/scrapy/issues/5316){.reference
    .external}, [issue
    5333](https://github.com/scrapy/scrapy/issues/5333){.reference
    .external}, [issue
    5388](https://github.com/scrapy/scrapy/issues/5388){.reference
    .external}, [issue
    5389](https://github.com/scrapy/scrapy/issues/5389){.reference
    .external}, [issue
    5400](https://github.com/scrapy/scrapy/issues/5400){.reference
    .external}, [issue
    5401](https://github.com/scrapy/scrapy/issues/5401){.reference
    .external}, [issue
    5404](https://github.com/scrapy/scrapy/issues/5404){.reference
    .external}, [issue
    5405](https://github.com/scrapy/scrapy/issues/5405){.reference
    .external}, [issue
    5407](https://github.com/scrapy/scrapy/issues/5407){.reference
    .external}, [issue
    5410](https://github.com/scrapy/scrapy/issues/5410){.reference
    .external}, [issue
    5412](https://github.com/scrapy/scrapy/issues/5412){.reference
    .external}, [issue
    5425](https://github.com/scrapy/scrapy/issues/5425){.reference
    .external}, [issue
    5427](https://github.com/scrapy/scrapy/issues/5427){.reference
    .external})

-   Implemented improvements for contributors. ([issue
    5080](https://github.com/scrapy/scrapy/issues/5080){.reference
    .external}, [issue
    5082](https://github.com/scrapy/scrapy/issues/5082){.reference
    .external}, [issue
    5177](https://github.com/scrapy/scrapy/issues/5177){.reference
    .external}, [issue
    5200](https://github.com/scrapy/scrapy/issues/5200){.reference
    .external})

-   Implemented cleanups. ([issue
    5095](https://github.com/scrapy/scrapy/issues/5095){.reference
    .external}, [issue
    5106](https://github.com/scrapy/scrapy/issues/5106){.reference
    .external}, [issue
    5209](https://github.com/scrapy/scrapy/issues/5209){.reference
    .external}, [issue
    5228](https://github.com/scrapy/scrapy/issues/5228){.reference
    .external}, [issue
    5235](https://github.com/scrapy/scrapy/issues/5235){.reference
    .external}, [issue
    5245](https://github.com/scrapy/scrapy/issues/5245){.reference
    .external}, [issue
    5246](https://github.com/scrapy/scrapy/issues/5246){.reference
    .external}, [issue
    5292](https://github.com/scrapy/scrapy/issues/5292){.reference
    .external}, [issue
    5314](https://github.com/scrapy/scrapy/issues/5314){.reference
    .external}, [issue
    5322](https://github.com/scrapy/scrapy/issues/5322){.reference
    .external})
:::
:::

::: {#scrapy-2-5-1-2021-10-05 .section}
[]{#release-2-5-1}

#### Scrapy 2.5.1 (2021-10-05)[¶](#scrapy-2-5-1-2021-10-05 "Permalink to this heading"){.headerlink}

-   **Security bug fix:**

    If you use [[`HttpAuthMiddleware`{.xref .py .py-class .docutils
    .literal
    .notranslate}]{.pre}](index.html#scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware "scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware"){.reference
    .internal} (i.e. the [`http_user`{.docutils .literal
    .notranslate}]{.pre} and [`http_pass`{.docutils .literal
    .notranslate}]{.pre} spider attributes) for HTTP authentication, any
    request exposes your credentials to the request target.

    To prevent unintended exposure of authentication credentials to
    unintended domains, you must now additionally set a new, additional
    spider attribute, [`http_auth_domain`{.docutils .literal
    .notranslate}]{.pre}, and point it to the specific domain to which
    the authentication credentials must be sent.

    If the [`http_auth_domain`{.docutils .literal .notranslate}]{.pre}
    spider attribute is not set, the domain of the first request will be
    considered the HTTP authentication target, and authentication
    credentials will only be sent in requests targeting that domain.

    If you need to send the same HTTP authentication credentials to
    multiple domains, you can use
    [[`w3lib.http.basic_auth_header()`{.xref .py .py-func .docutils
    .literal
    .notranslate}]{.pre}](https://w3lib.readthedocs.io/en/latest/w3lib.html#w3lib.http.basic_auth_header "(in w3lib v2.1)"){.reference
    .external} instead to set the value of the
    [`Authorization`{.docutils .literal .notranslate}]{.pre} header of
    your requests.

    If you *really* want your spider to send the same HTTP
    authentication credentials to any domain, set the
    [`http_auth_domain`{.docutils .literal .notranslate}]{.pre} spider
    attribute to [`None`{.docutils .literal .notranslate}]{.pre}.

    Finally, if you are a user of
    [scrapy-splash](https://github.com/scrapy-plugins/scrapy-splash){.reference
    .external}, know that this version of Scrapy breaks compatibility
    with scrapy-splash 0.7.2 and earlier. You will need to upgrade
    scrapy-splash to a greater version for it to continue to work.
:::

::: {#scrapy-2-5-0-2021-04-06 .section}
[]{#release-2-5-0}

#### Scrapy 2.5.0 (2021-04-06)[¶](#scrapy-2-5-0-2021-04-06 "Permalink to this heading"){.headerlink}

Highlights:

-   Official Python 3.9 support

-   Experimental [[HTTP/2 support]{.std
    .std-ref}](index.html#http2){.hoverxref .tooltip .reference
    .internal}

-   New [[`get_retry_request()`{.xref .py .py-func .docutils .literal
    .notranslate}]{.pre}](index.html#scrapy.downloadermiddlewares.retry.get_retry_request "scrapy.downloadermiddlewares.retry.get_retry_request"){.reference
    .internal} function to retry requests from spider callbacks

-   New [[`headers_received`{.xref .py .py-class .docutils .literal
    .notranslate}]{.pre}](index.html#scrapy.signals.headers_received "scrapy.signals.headers_received"){.reference
    .internal} signal that allows stopping downloads early

-   New [[`Response.protocol`{.xref .py .py-class .docutils .literal
    .notranslate}]{.pre}](index.html#scrapy.http.Response.protocol "scrapy.http.Response.protocol"){.reference
    .internal} attribute

::: {#id37 .section}
##### Deprecation removals[¶](#id37 "Permalink to this heading"){.headerlink}

-   Removed all code that [[was deprecated in 1.7.0]{.std
    .std-ref}](#id96){.hoverxref .tooltip .reference .internal} and had
    not [[already been removed in 2.4.0]{.std
    .std-ref}](#id45){.hoverxref .tooltip .reference .internal}. ([issue
    4901](https://github.com/scrapy/scrapy/issues/4901){.reference
    .external})

-   Removed support for the
    [`SCRAPY_PICKLED_SETTINGS_TO_OVERRIDE`{.docutils .literal
    .notranslate}]{.pre} environment variable, [[deprecated in
    1.8.0]{.std .std-ref}](#id88){.hoverxref .tooltip .reference
    .internal}. ([issue
    4912](https://github.com/scrapy/scrapy/issues/4912){.reference
    .external})
:::

::: {#id38 .section}
##### Deprecations[¶](#id38 "Permalink to this heading"){.headerlink}

-   The [`scrapy.utils.py36`{.xref .py .py-mod .docutils .literal
    .notranslate}]{.pre} module is now deprecated in favor of
    [`scrapy.utils.asyncgen`{.xref .py .py-mod .docutils .literal
    .notranslate}]{.pre}. ([issue
    4900](https://github.com/scrapy/scrapy/issues/4900){.reference
    .external})
:::

::: {#id39 .section}
##### New features[¶](#id39 "Permalink to this heading"){.headerlink}

-   Experimental [[HTTP/2 support]{.std
    .std-ref}](index.html#http2){.hoverxref .tooltip .reference
    .internal} through a new download handler that can be assigned to
    the [`https`{.docutils .literal .notranslate}]{.pre} protocol in the
    [[`DOWNLOAD_HANDLERS`{.xref .std .std-setting .docutils .literal
    .notranslate}]{.pre}](index.html#std-setting-DOWNLOAD_HANDLERS){.hoverxref
    .tooltip .reference .internal} setting. ([issue
    1854](https://github.com/scrapy/scrapy/issues/1854){.reference
    .external}, [issue
    4769](https://github.com/scrapy/scrapy/issues/4769){.reference
    .external}, [issue
    5058](https://github.com/scrapy/scrapy/issues/5058){.reference
    .external}, [issue
    5059](https://github.com/scrapy/scrapy/issues/5059){.reference
    .external}, [issue
    5066](https://github.com/scrapy/scrapy/issues/5066){.reference
    .external})

-   The new
    [[`scrapy.downloadermiddlewares.retry.get_retry_request()`{.xref .py
    .py-func .docutils .literal
    .notranslate}]{.pre}](index.html#scrapy.downloadermiddlewares.retry.get_retry_request "scrapy.downloadermiddlewares.retry.get_retry_request"){.reference
    .internal} function may be used from spider callbacks or middlewares
    to handle the retrying of a request beyond the scenarios that
    [[`RetryMiddleware`{.xref .py .py-class .docutils .literal
    .notranslate}]{.pre}](index.html#scrapy.downloadermiddlewares.retry.RetryMiddleware "scrapy.downloadermiddlewares.retry.RetryMiddleware"){.reference
    .internal} supports. ([issue
    3590](https://github.com/scrapy/scrapy/issues/3590){.reference
    .external}, [issue
    3685](https://github.com/scrapy/scrapy/issues/3685){.reference
    .external}, [issue
    4902](https://github.com/scrapy/scrapy/issues/4902){.reference
    .external})

-   The new [[`headers_received`{.xref .py .py-class .docutils .literal
    .notranslate}]{.pre}](index.html#scrapy.signals.headers_received "scrapy.signals.headers_received"){.reference
    .internal} signal gives early access to response headers and allows
    [[stopping downloads]{.std
    .std-ref}](index.html#topics-stop-response-download){.hoverxref
    .tooltip .reference .internal}. ([issue
    1772](https://github.com/scrapy/scrapy/issues/1772){.reference
    .external}, [issue
    4897](https://github.com/scrapy/scrapy/issues/4897){.reference
    .external})

-   The new [[`Response.protocol`{.xref .py .py-attr .docutils .literal
    .notranslate}]{.pre}](index.html#scrapy.http.Response.protocol "scrapy.http.Response.protocol"){.reference
    .internal} attribute gives access to the string that identifies the
    protocol used to download a response. ([issue
    4878](https://github.com/scrapy/scrapy/issues/4878){.reference
    .external})

-   [[Stats]{.std .std-ref}](index.html#topics-stats){.hoverxref
    .tooltip .reference .internal} now include the following entries
    that indicate the number of successes and failures in storing
    [[feeds]{.std .std-ref}](index.html#topics-feed-exports){.hoverxref
    .tooltip .reference .internal}:

    ::: {.highlight-default .notranslate}
    ::: highlight
        feedexport/success_count/<storage type>
        feedexport/failed_count/<storage type>
    :::
    :::

    Where [`<storage`{.docutils .literal
    .notranslate}]{.pre}` `{.docutils .literal
    .notranslate}[`type>`{.docutils .literal .notranslate}]{.pre} is the
    feed storage backend class name, such as [`FileFeedStorage`{.xref
    .py .py-class .docutils .literal .notranslate}]{.pre} or
    [`FTPFeedStorage`{.xref .py .py-class .docutils .literal
    .notranslate}]{.pre}.

    ([issue
    3947](https://github.com/scrapy/scrapy/issues/3947){.reference
    .external}, [issue
    4850](https://github.com/scrapy/scrapy/issues/4850){.reference
    .external})

-   The [[`UrlLengthMiddleware`{.xref .py .py-class .docutils .literal
    .notranslate}]{.pre}](index.html#scrapy.spidermiddlewares.urllength.UrlLengthMiddleware "scrapy.spidermiddlewares.urllength.UrlLengthMiddleware"){.reference
    .internal} spider middleware now logs ignored URLs with
    [`INFO`{.docutils .literal .notranslate}]{.pre} [[logging
    level]{.xref .std
    .std-ref}](https://docs.python.org/3/library/logging.html#levels "(in Python v3.12)"){.reference
    .external} instead of [`DEBUG`{.docutils .literal
    .notranslate}]{.pre}, and it now includes the following entry into
    [[stats]{.std .std-ref}](index.html#topics-stats){.hoverxref
    .tooltip .reference .internal} to keep track of the number of
    ignored URLs:

    ::: {.highlight-default .notranslate}
    ::: highlight
        urllength/request_ignored_count
    :::
    :::

    ([issue
    5036](https://github.com/scrapy/scrapy/issues/5036){.reference
    .external})

-   The [[`HttpCompressionMiddleware`{.xref .py .py-class .docutils
    .literal
    .notranslate}]{.pre}](index.html#scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware "scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware"){.reference
    .internal} downloader middleware now logs the number of decompressed
    responses and the total count of resulting bytes:

    ::: {.highlight-default .notranslate}
    ::: highlight
        httpcompression/response_bytes
        httpcompression/response_count
    :::
    :::

    ([issue
    4797](https://github.com/scrapy/scrapy/issues/4797){.reference
    .external}, [issue
    4799](https://github.com/scrapy/scrapy/issues/4799){.reference
    .external})
:::

::: {#id40 .section}
##### Bug fixes[¶](#id40 "Permalink to this heading"){.headerlink}

-   Fixed installation on PyPy installing PyDispatcher in addition to
    PyPyDispatcher, which could prevent Scrapy from working depending on
    which package got imported. ([issue
    4710](https://github.com/scrapy/scrapy/issues/4710){.reference
    .external}, [issue
    4814](https://github.com/scrapy/scrapy/issues/4814){.reference
    .external})

-   When inspecting a callback to check if it is a generator that also
    returns a value, an exception is no longer raised if the callback
    has a docstring with lower indentation than the following code.
    ([issue
    4477](https://github.com/scrapy/scrapy/issues/4477){.reference
    .external}, [issue
    4935](https://github.com/scrapy/scrapy/issues/4935){.reference
    .external})

-   The
    [Content-Length](https://tools.ietf.org/html/rfc2616#section-14.13){.reference
    .external} header is no longer omitted from responses when using the
    default, HTTP/1.1 download handler (see [[`DOWNLOAD_HANDLERS`{.xref
    .std .std-setting .docutils .literal
    .notranslate}]{.pre}](index.html#std-setting-DOWNLOAD_HANDLERS){.hoverxref
    .tooltip .reference .internal}). ([issue
    5009](https://github.com/scrapy/scrapy/issues/5009){.reference
    .external}, [issue
    5034](https://github.com/scrapy/scrapy/issues/5034){.reference
    .external}, [issue
    5045](https://github.com/scrapy/scrapy/issues/5045){.reference
    .external}, [issue
    5057](https://github.com/scrapy/scrapy/issues/5057){.reference
    .external}, [issue
    5062](https://github.com/scrapy/scrapy/issues/5062){.reference
    .external})

-   Setting the [[`handle_httpstatus_all`{.xref .std .std-reqmeta
    .docutils .literal
    .notranslate}]{.pre}](index.html#std-reqmeta-handle_httpstatus_all){.hoverxref
    .tooltip .reference .internal} request meta key to
    [`False`{.docutils .literal .notranslate}]{.pre} now has the same
    effect as not setting it at all, instead of having the same effect
    as setting it to [`True`{.docutils .literal .notranslate}]{.pre}.
    ([issue
    3851](https://github.com/scrapy/scrapy/issues/3851){.reference
    .external}, [issue
    4694](https://github.com/scrapy/scrapy/issues/4694){.reference
    .external})
:::

::: {#id41 .section}
##### Documentation[¶](#id41 "Permalink to this heading"){.headerlink}

-   Added instructions to [[install Scrapy in Windows using pip]{.std
    .std-ref}](index.html#intro-install-windows){.hoverxref .tooltip
    .reference .internal}. ([issue
    4715](https://github.com/scrapy/scrapy/issues/4715){.reference
    .external}, [issue
    4736](https://github.com/scrapy/scrapy/issues/4736){.reference
    .external})

-   Logging documentation now includes [[additional ways to filter
    logs]{.std
    .std-ref}](index.html#topics-logging-advanced-customization){.hoverxref
    .tooltip .reference .internal}. ([issue
    4216](https://github.com/scrapy/scrapy/issues/4216){.reference
    .external}, [issue
    4257](https://github.com/scrapy/scrapy/issues/4257){.reference
    .external}, [issue
    4965](https://github.com/scrapy/scrapy/issues/4965){.reference
    .external})

-   Covered how to deal with long lists of allowed domains in the
    [[FAQ]{.std .std-ref}](index.html#faq){.hoverxref .tooltip
    .reference .internal}. ([issue
    2263](https://github.com/scrapy/scrapy/issues/2263){.reference
    .external}, [issue
    3667](https://github.com/scrapy/scrapy/issues/3667){.reference
    .external})

-   Covered
    [scrapy-bench](https://github.com/scrapy/scrapy-bench){.reference
    .external} in [[Benchmarking]{.std
    .std-ref}](index.html#benchmarking){.hoverxref .tooltip .reference
    .internal}. ([issue
    4996](https://github.com/scrapy/scrapy/issues/4996){.reference
    .external}, [issue
    5016](https://github.com/scrapy/scrapy/issues/5016){.reference
    .external})

-   Clarified that one [[extension]{.std
    .std-ref}](index.html#topics-extensions){.hoverxref .tooltip
    .reference .internal} instance is created per crawler. ([issue
    5014](https://github.com/scrapy/scrapy/issues/5014){.reference
    .external})

-   Fixed some errors in examples. ([issue
    4829](https://github.com/scrapy/scrapy/issues/4829){.reference
    .external}, [issue
    4830](https://github.com/scrapy/scrapy/issues/4830){.reference
    .external}, [issue
    4907](https://github.com/scrapy/scrapy/issues/4907){.reference
    .external}, [issue
    4909](https://github.com/scrapy/scrapy/issues/4909){.reference
    .external}, [issue
    5008](https://github.com/scrapy/scrapy/issues/5008){.reference
    .external})

-   Fixed some external links, typos, and so on. ([issue
    4892](https://github.com/scrapy/scrapy/issues/4892){.reference
    .external}, [issue
    4899](https://github.com/scrapy/scrapy/issues/4899){.reference
    .external}, [issue
    4936](https://github.com/scrapy/scrapy/issues/4936){.reference
    .external}, [issue
    4942](https://github.com/scrapy/scrapy/issues/4942){.reference
    .external}, [issue
    5005](https://github.com/scrapy/scrapy/issues/5005){.reference
    .external}, [issue
    5063](https://github.com/scrapy/scrapy/issues/5063){.reference
    .external})

-   The [[list of Request.meta keys]{.std
    .std-ref}](index.html#topics-request-meta){.hoverxref .tooltip
    .reference .internal} is now sorted alphabetically. ([issue
    5061](https://github.com/scrapy/scrapy/issues/5061){.reference
    .external}, [issue
    5065](https://github.com/scrapy/scrapy/issues/5065){.reference
    .external})

-   Updated references to Scrapinghub, which is now called Zyte. ([issue
    4973](https://github.com/scrapy/scrapy/issues/4973){.reference
    .external}, [issue
    5072](https://github.com/scrapy/scrapy/issues/5072){.reference
    .external})

-   Added a mention to contributors in the README. ([issue
    4956](https://github.com/scrapy/scrapy/issues/4956){.reference
    .external})

-   Reduced the top margin of lists. ([issue
    4974](https://github.com/scrapy/scrapy/issues/4974){.reference
    .external})
:::

::: {#id42 .section}
##### Quality Assurance[¶](#id42 "Permalink to this heading"){.headerlink}

-   Made Python 3.9 support official ([issue
    4757](https://github.com/scrapy/scrapy/issues/4757){.reference
    .external}, [issue
    4759](https://github.com/scrapy/scrapy/issues/4759){.reference
    .external})

-   Extended typing hints ([issue
    4895](https://github.com/scrapy/scrapy/issues/4895){.reference
    .external})

-   Fixed deprecated uses of the Twisted API. ([issue
    4940](https://github.com/scrapy/scrapy/issues/4940){.reference
    .external}, [issue
    4950](https://github.com/scrapy/scrapy/issues/4950){.reference
    .external}, [issue
    5073](https://github.com/scrapy/scrapy/issues/5073){.reference
    .external})

-   Made our tests run with the new pip resolver. ([issue
    4710](https://github.com/scrapy/scrapy/issues/4710){.reference
    .external}, [issue
    4814](https://github.com/scrapy/scrapy/issues/4814){.reference
    .external})

-   Added tests to ensure that [[coroutine support]{.std
    .std-ref}](index.html#coroutine-support){.hoverxref .tooltip
    .reference .internal} is tested. ([issue
    4987](https://github.com/scrapy/scrapy/issues/4987){.reference
    .external})

-   Migrated from Travis CI to GitHub Actions. ([issue
    4924](https://github.com/scrapy/scrapy/issues/4924){.reference
    .external})

-   Fixed CI issues. ([issue
    4986](https://github.com/scrapy/scrapy/issues/4986){.reference
    .external}, [issue
    5020](https://github.com/scrapy/scrapy/issues/5020){.reference
    .external}, [issue
    5022](https://github.com/scrapy/scrapy/issues/5022){.reference
    .external}, [issue
    5027](https://github.com/scrapy/scrapy/issues/5027){.reference
    .external}, [issue
    5052](https://github.com/scrapy/scrapy/issues/5052){.reference
    .external}, [issue
    5053](https://github.com/scrapy/scrapy/issues/5053){.reference
    .external})

-   Implemented code refactorings, style fixes and cleanups. ([issue
    4911](https://github.com/scrapy/scrapy/issues/4911){.reference
    .external}, [issue
    4982](https://github.com/scrapy/scrapy/issues/4982){.reference
    .external}, [issue
    5001](https://github.com/scrapy/scrapy/issues/5001){.reference
    .external}, [issue
    5002](https://github.com/scrapy/scrapy/issues/5002){.reference
    .external}, [issue
    5076](https://github.com/scrapy/scrapy/issues/5076){.reference
    .external})
:::
:::

::: {#scrapy-2-4-1-2020-11-17 .section}
[]{#release-2-4-1}

#### Scrapy 2.4.1 (2020-11-17)[¶](#scrapy-2-4-1-2020-11-17 "Permalink to this heading"){.headerlink}

-   Fixed [[feed exports]{.std
    .std-ref}](index.html#topics-feed-exports){.hoverxref .tooltip
    .reference .internal} overwrite support ([issue
    4845](https://github.com/scrapy/scrapy/issues/4845){.reference
    .external}, [issue
    4857](https://github.com/scrapy/scrapy/issues/4857){.reference
    .external}, [issue
    4859](https://github.com/scrapy/scrapy/issues/4859){.reference
    .external})

-   Fixed the AsyncIO event loop handling, which could make code hang
    ([issue
    4855](https://github.com/scrapy/scrapy/issues/4855){.reference
    .external}, [issue
    4872](https://github.com/scrapy/scrapy/issues/4872){.reference
    .external})

-   Fixed the IPv6-capable DNS resolver [`CachingHostnameResolver`{.xref
    .py .py-class .docutils .literal .notranslate}]{.pre} for download
    handlers that call [[`reactor.resolve`{.xref .py .py-meth .docutils
    .literal
    .notranslate}]{.pre}](https://docs.twisted.org/en/stable/api/twisted.internet.interfaces.IReactorCore.html#resolve "(in Twisted)"){.reference
    .external} ([issue
    4802](https://github.com/scrapy/scrapy/issues/4802){.reference
    .external}, [issue
    4803](https://github.com/scrapy/scrapy/issues/4803){.reference
    .external})

-   Fixed the output of the [[`genspider`{.xref .std .std-command
    .docutils .literal
    .notranslate}]{.pre}](index.html#std-command-genspider){.hoverxref
    .tooltip .reference .internal} command showing placeholders instead
    of the import path of the generated spider module ([issue
    4874](https://github.com/scrapy/scrapy/issues/4874){.reference
    .external})

-   Migrated Windows CI from Azure Pipelines to GitHub Actions ([issue
    4869](https://github.com/scrapy/scrapy/issues/4869){.reference
    .external}, [issue
    4876](https://github.com/scrapy/scrapy/issues/4876){.reference
    .external})
:::

::: {#scrapy-2-4-0-2020-10-11 .section}
[]{#release-2-4-0}

#### Scrapy 2.4.0 (2020-10-11)[¶](#scrapy-2-4-0-2020-10-11 "Permalink to this heading"){.headerlink}

Highlights:

-   Python 3.5 support has been dropped.

-   The [`file_path`{.docutils .literal .notranslate}]{.pre} method of
    [[media pipelines]{.std
    .std-ref}](index.html#topics-media-pipeline){.hoverxref .tooltip
    .reference .internal} can now access the source [[item]{.std
    .std-ref}](index.html#topics-items){.hoverxref .tooltip .reference
    .internal}.

    This allows you to set a download file path based on item data.

-   The new [`item_export_kwargs`{.docutils .literal
    .notranslate}]{.pre} key of the [[`FEEDS`{.xref .std .std-setting
    .docutils .literal
    .notranslate}]{.pre}](index.html#std-setting-FEEDS){.hoverxref
    .tooltip .reference .internal} setting allows to define keyword
    parameters to pass to [[item exporter classes]{.std
    .std-ref}](index.html#topics-exporters){.hoverxref .tooltip
    .reference .internal}

-   You can now choose whether [[feed exports]{.std
    .std-ref}](index.html#topics-feed-exports){.hoverxref .tooltip
    .reference .internal} overwrite or append to the output file.

    For example, when using the [[`crawl`{.xref .std .std-command
    .docutils .literal
    .notranslate}]{.pre}](index.html#std-command-crawl){.hoverxref
    .tooltip .reference .internal} or [[`runspider`{.xref .std
    .std-command .docutils .literal
    .notranslate}]{.pre}](index.html#std-command-runspider){.hoverxref
    .tooltip .reference .internal} commands, you can use the
    [`-O`{.docutils .literal .notranslate}]{.pre} option instead of
    [`-o`{.docutils .literal .notranslate}]{.pre} to overwrite the
    output file.

-   Zstd-compressed responses are now supported if
    [zstandard](https://pypi.org/project/zstandard/){.reference
    .external} is installed.

-   In settings, where the import path of a class is required, it is now
    possible to pass a class object instead.

::: {#id43 .section}
##### Modified requirements[¶](#id43 "Permalink to this heading"){.headerlink}

-   Python 3.6 or greater is now required; support for Python 3.5 has
    been dropped

    As a result:

    -   When using PyPy, PyPy 7.2.0 or greater [[is now required]{.std
        .std-ref}](index.html#faq-python-versions){.hoverxref .tooltip
        .reference .internal}

    -   For Amazon S3 storage support in [[feed exports]{.std
        .std-ref}](index.html#topics-feed-storage-s3){.hoverxref
        .tooltip .reference .internal} or [[media pipelines]{.std
        .std-ref}](index.html#media-pipelines-s3){.hoverxref .tooltip
        .reference .internal},
        [botocore](https://github.com/boto/botocore){.reference
        .external} 1.4.87 or greater is now required

    -   To use the [[images pipeline]{.std
        .std-ref}](index.html#images-pipeline){.hoverxref .tooltip
        .reference .internal},
        [Pillow](https://python-pillow.org/){.reference .external} 4.0.0
        or greater is now required

    ([issue
    4718](https://github.com/scrapy/scrapy/issues/4718){.reference
    .external}, [issue
    4732](https://github.com/scrapy/scrapy/issues/4732){.reference
    .external}, [issue
    4733](https://github.com/scrapy/scrapy/issues/4733){.reference
    .external}, [issue
    4742](https://github.com/scrapy/scrapy/issues/4742){.reference
    .external}, [issue
    4743](https://github.com/scrapy/scrapy/issues/4743){.reference
    .external}, [issue
    4764](https://github.com/scrapy/scrapy/issues/4764){.reference
    .external})
:::

::: {#id44 .section}
##### Backward-incompatible changes[¶](#id44 "Permalink to this heading"){.headerlink}

-   [[`CookiesMiddleware`{.xref .py .py-class .docutils .literal
    .notranslate}]{.pre}](index.html#scrapy.downloadermiddlewares.cookies.CookiesMiddleware "scrapy.downloadermiddlewares.cookies.CookiesMiddleware"){.reference
    .internal} once again discards cookies defined in
    [[`Request.headers`{.xref .py .py-attr .docutils .literal
    .notranslate}]{.pre}](index.html#scrapy.http.Request.headers "scrapy.http.Request.headers"){.reference
    .internal}.

    We decided to revert this bug fix, introduced in Scrapy 2.2.0,
    because it was reported that the current implementation could break
    existing code.

    If you need to set cookies for a request, use the
    [[`Request.cookies`{.xref .py .py-class .docutils .literal
    .notranslate}]{.pre}](index.html#scrapy.http.Request "scrapy.http.Request"){.reference
    .internal} parameter.

    A future version of Scrapy will include a new, better implementation
    of the reverted bug fix.

    ([issue
    4717](https://github.com/scrapy/scrapy/issues/4717){.reference
    .external}, [issue
    4823](https://github.com/scrapy/scrapy/issues/4823){.reference
    .external})
:::

::: {#id45 .section}
[]{#id46}

##### Deprecation removals[¶](#id45 "Permalink to this heading"){.headerlink}

-   [`scrapy.extensions.feedexport.S3FeedStorage`{.xref .py .py-class
    .docutils .literal .notranslate}]{.pre} no longer reads the values
    of [`access_key`{.docutils .literal .notranslate}]{.pre} and
    [`secret_key`{.docutils .literal .notranslate}]{.pre} from the
    running project settings when they are not passed to its
    [`__init__`{.docutils .literal .notranslate}]{.pre} method; you must
    either pass those parameters to its [`__init__`{.docutils .literal
    .notranslate}]{.pre} method or use
    [`S3FeedStorage.from_crawler`{.xref .py .py-class .docutils .literal
    .notranslate}]{.pre} ([issue
    4356](https://github.com/scrapy/scrapy/issues/4356){.reference
    .external}, [issue
    4411](https://github.com/scrapy/scrapy/issues/4411){.reference
    .external}, [issue
    4688](https://github.com/scrapy/scrapy/issues/4688){.reference
    .external})

-   [`Rule.process_request`{.xref .py .py-attr .docutils .literal
    .notranslate}]{.pre} no longer admits callables which expect a
    single [`request`{.docutils .literal .notranslate}]{.pre} parameter,
    rather than both [`request`{.docutils .literal .notranslate}]{.pre}
    and [`response`{.docutils .literal .notranslate}]{.pre} ([issue
    4818](https://github.com/scrapy/scrapy/issues/4818){.reference
    .external})
:::

::: {#id47 .section}
##### Deprecations[¶](#id47 "Permalink to this heading"){.headerlink}

-   In custom [[media pipelines]{.std
    .std-ref}](index.html#topics-media-pipeline){.hoverxref .tooltip
    .reference .internal}, signatures that do not accept a keyword-only
    [`item`{.docutils .literal .notranslate}]{.pre} parameter in any of
    the methods that [[now support this parameter]{.std
    .std-ref}](#media-pipeline-item-parameter){.hoverxref .tooltip
    .reference .internal} are now deprecated ([issue
    4628](https://github.com/scrapy/scrapy/issues/4628){.reference
    .external}, [issue
    4686](https://github.com/scrapy/scrapy/issues/4686){.reference
    .external})

-   In custom [[feed storage backend classes]{.std
    .std-ref}](index.html#topics-feed-storage){.hoverxref .tooltip
    .reference .internal}, [`__init__`{.docutils .literal
    .notranslate}]{.pre} method signatures that do not accept a
    keyword-only [`feed_options`{.docutils .literal .notranslate}]{.pre}
    parameter are now deprecated ([issue
    547](https://github.com/scrapy/scrapy/issues/547){.reference
    .external}, [issue
    716](https://github.com/scrapy/scrapy/issues/716){.reference
    .external}, [issue
    4512](https://github.com/scrapy/scrapy/issues/4512){.reference
    .external})

-   The [`scrapy.utils.python.WeakKeyCache`{.xref .py .py-class
    .docutils .literal .notranslate}]{.pre} class is now deprecated
    ([issue
    4684](https://github.com/scrapy/scrapy/issues/4684){.reference
    .external}, [issue
    4701](https://github.com/scrapy/scrapy/issues/4701){.reference
    .external})

-   The [`scrapy.utils.boto.is_botocore()`{.xref .py .py-func .docutils
    .literal .notranslate}]{.pre} function is now deprecated, use
    [`scrapy.utils.boto.is_botocore_available()`{.xref .py .py-func
    .docutils .literal .notranslate}]{.pre} instead ([issue
    4734](https://github.com/scrapy/scrapy/issues/4734){.reference
    .external}, [issue
    4776](https://github.com/scrapy/scrapy/issues/4776){.reference
    .external})
:::

::: {#id48 .section}
##### New features[¶](#id48 "Permalink to this heading"){.headerlink}

-   The following methods of [[media pipelines]{.std
    .std-ref}](index.html#topics-media-pipeline){.hoverxref .tooltip
    .reference .internal} now accept an [`item`{.docutils .literal
    .notranslate}]{.pre} keyword-only parameter containing the source
    [[item]{.std .std-ref}](index.html#topics-items){.hoverxref .tooltip
    .reference .internal}:

    -   In [[`scrapy.pipelines.files.FilesPipeline`{.xref .py .py-class
        .docutils .literal
        .notranslate}]{.pre}](index.html#scrapy.pipelines.files.FilesPipeline "scrapy.pipelines.files.FilesPipeline"){.reference
        .internal}:

        -   [`file_downloaded()`{.xref .py .py-meth .docutils .literal
            .notranslate}]{.pre}

        -   [[`file_path()`{.xref .py .py-meth .docutils .literal
            .notranslate}]{.pre}](index.html#scrapy.pipelines.files.FilesPipeline.file_path "scrapy.pipelines.files.FilesPipeline.file_path"){.reference
            .internal}

        -   [`media_downloaded()`{.xref .py .py-meth .docutils .literal
            .notranslate}]{.pre}

        -   [`media_to_download()`{.xref .py .py-meth .docutils .literal
            .notranslate}]{.pre}

    -   In [[`scrapy.pipelines.images.ImagesPipeline`{.xref .py
        .py-class .docutils .literal
        .notranslate}]{.pre}](index.html#scrapy.pipelines.images.ImagesPipeline "scrapy.pipelines.images.ImagesPipeline"){.reference
        .internal}:

        -   [`file_downloaded()`{.xref .py .py-meth .docutils .literal
            .notranslate}]{.pre}

        -   [[`file_path()`{.xref .py .py-meth .docutils .literal
            .notranslate}]{.pre}](index.html#scrapy.pipelines.images.ImagesPipeline.file_path "scrapy.pipelines.images.ImagesPipeline.file_path"){.reference
            .internal}

        -   [`get_images()`{.xref .py .py-meth .docutils .literal
            .notranslate}]{.pre}

        -   [`image_downloaded()`{.xref .py .py-meth .docutils .literal
            .notranslate}]{.pre}

        -   [`media_downloaded()`{.xref .py .py-meth .docutils .literal
            .notranslate}]{.pre}

        -   [`media_to_download()`{.xref .py .py-meth .docutils .literal
            .notranslate}]{.pre}

    ([issue
    4628](https://github.com/scrapy/scrapy/issues/4628){.reference
    .external}, [issue
    4686](https://github.com/scrapy/scrapy/issues/4686){.reference
    .external})

-   The new [`item_export_kwargs`{.docutils .literal
    .notranslate}]{.pre} key of the [[`FEEDS`{.xref .std .std-setting
    .docutils .literal
    .notranslate}]{.pre}](index.html#std-setting-FEEDS){.hoverxref
    .tooltip .reference .internal} setting allows to define keyword
    parameters to pass to [[item exporter classes]{.std
    .std-ref}](index.html#topics-exporters){.hoverxref .tooltip
    .reference .internal} ([issue
    4606](https://github.com/scrapy/scrapy/issues/4606){.reference
    .external}, [issue
    4768](https://github.com/scrapy/scrapy/issues/4768){.reference
    .external})

-   [[Feed exports]{.std
    .std-ref}](index.html#topics-feed-exports){.hoverxref .tooltip
    .reference .internal} gained overwrite support:

    -   When using the [[`crawl`{.xref .std .std-command .docutils
        .literal
        .notranslate}]{.pre}](index.html#std-command-crawl){.hoverxref
        .tooltip .reference .internal} or [[`runspider`{.xref .std
        .std-command .docutils .literal
        .notranslate}]{.pre}](index.html#std-command-runspider){.hoverxref
        .tooltip .reference .internal} commands, you can use the
        [`-O`{.docutils .literal .notranslate}]{.pre} option instead of
        [`-o`{.docutils .literal .notranslate}]{.pre} to overwrite the
        output file

    -   You can use the [`overwrite`{.docutils .literal
        .notranslate}]{.pre} key in the [[`FEEDS`{.xref .std
        .std-setting .docutils .literal
        .notranslate}]{.pre}](index.html#std-setting-FEEDS){.hoverxref
        .tooltip .reference .internal} setting to configure whether to
        overwrite the output file ([`True`{.docutils .literal
        .notranslate}]{.pre}) or append to its content
        ([`False`{.docutils .literal .notranslate}]{.pre})

    -   The [`__init__`{.docutils .literal .notranslate}]{.pre} and
        [`from_crawler`{.docutils .literal .notranslate}]{.pre} methods
        of [[feed storage backend classes]{.std
        .std-ref}](index.html#topics-feed-storage){.hoverxref .tooltip
        .reference .internal} now receive a new keyword-only parameter,
        [`feed_options`{.docutils .literal .notranslate}]{.pre}, which
        is a dictionary of [[feed options]{.std
        .std-ref}](index.html#feed-options){.hoverxref .tooltip
        .reference .internal}

    ([issue 547](https://github.com/scrapy/scrapy/issues/547){.reference
    .external}, [issue
    716](https://github.com/scrapy/scrapy/issues/716){.reference
    .external}, [issue
    4512](https://github.com/scrapy/scrapy/issues/4512){.reference
    .external})

-   Zstd-compressed responses are now supported if
    [zstandard](https://pypi.org/project/zstandard/){.reference
    .external} is installed ([issue
    4831](https://github.com/scrapy/scrapy/issues/4831){.reference
    .external})

-   In settings, where the import path of a class is required, it is now
    possible to pass a class object instead ([issue
    3870](https://github.com/scrapy/scrapy/issues/3870){.reference
    .external}, [issue
    3873](https://github.com/scrapy/scrapy/issues/3873){.reference
    .external}).

    This includes also settings where only part of its value is made of
    an import path, such as [[`DOWNLOADER_MIDDLEWARES`{.xref .std
    .std-setting .docutils .literal
    .notranslate}]{.pre}](index.html#std-setting-DOWNLOADER_MIDDLEWARES){.hoverxref
    .tooltip .reference .internal} or [[`DOWNLOAD_HANDLERS`{.xref .std
    .std-setting .docutils .literal
    .notranslate}]{.pre}](index.html#std-setting-DOWNLOAD_HANDLERS){.hoverxref
    .tooltip .reference .internal}.

-   [[Downloader middlewares]{.std
    .std-ref}](index.html#topics-downloader-middleware){.hoverxref
    .tooltip .reference .internal} can now override
    [[`response.request`{.xref .py .py-class .docutils .literal
    .notranslate}]{.pre}](index.html#scrapy.http.Response.request "scrapy.http.Response.request"){.reference
    .internal}.

    If a [[downloader middleware]{.std
    .std-ref}](index.html#topics-downloader-middleware){.hoverxref
    .tooltip .reference .internal} returns a [[`Response`{.xref .py
    .py-class .docutils .literal
    .notranslate}]{.pre}](index.html#scrapy.http.Response "scrapy.http.Response"){.reference
    .internal} object from [[`process_response()`{.xref .py .py-meth
    .docutils .literal
    .notranslate}]{.pre}](index.html#scrapy.downloadermiddlewares.DownloaderMiddleware.process_response "scrapy.downloadermiddlewares.DownloaderMiddleware.process_response"){.reference
    .internal} or [[`process_exception()`{.xref .py .py-meth .docutils
    .literal
    .notranslate}]{.pre}](index.html#scrapy.downloadermiddlewares.DownloaderMiddleware.process_exception "scrapy.downloadermiddlewares.DownloaderMiddleware.process_exception"){.reference
    .internal} with a custom [[`Request`{.xref .py .py-class .docutils
    .literal
    .notranslate}]{.pre}](index.html#scrapy.http.Request "scrapy.http.Request"){.reference
    .internal} object assigned to [[`response.request`{.xref .py
    .py-class .docutils .literal
    .notranslate}]{.pre}](index.html#scrapy.http.Response.request "scrapy.http.Response.request"){.reference
    .internal}:

    -   The response is handled by the callback of that custom
        [[`Request`{.xref .py .py-class .docutils .literal
        .notranslate}]{.pre}](index.html#scrapy.http.Request "scrapy.http.Request"){.reference
        .internal} object, instead of being handled by the callback of
        the original [[`Request`{.xref .py .py-class .docutils .literal
        .notranslate}]{.pre}](index.html#scrapy.http.Request "scrapy.http.Request"){.reference
        .internal} object

    -   That custom [[`Request`{.xref .py .py-class .docutils .literal
        .notranslate}]{.pre}](index.html#scrapy.http.Request "scrapy.http.Request"){.reference
        .internal} object is now sent as the [`request`{.docutils
        .literal .notranslate}]{.pre} argument to the
        [[`response_received`{.xref .std .std-signal .docutils .literal
        .notranslate}]{.pre}](index.html#std-signal-response_received){.hoverxref
        .tooltip .reference .internal} signal, instead of the original
        [[`Request`{.xref .py .py-class .docutils .literal
        .notranslate}]{.pre}](index.html#scrapy.http.Request "scrapy.http.Request"){.reference
        .internal} object

    ([issue
    4529](https://github.com/scrapy/scrapy/issues/4529){.reference
    .external}, [issue
    4632](https://github.com/scrapy/scrapy/issues/4632){.reference
    .external})

-   When using the [[FTP feed storage backend]{.std
    .std-ref}](index.html#topics-feed-storage-ftp){.hoverxref .tooltip
    .reference .internal}:

    -   It is now possible to set the new [`overwrite`{.docutils
        .literal .notranslate}]{.pre} [[feed option]{.std
        .std-ref}](index.html#feed-options){.hoverxref .tooltip
        .reference .internal} to [`False`{.docutils .literal
        .notranslate}]{.pre} to append to an existing file instead of
        overwriting it

    -   The FTP password can now be omitted if it is not necessary

    ([issue 547](https://github.com/scrapy/scrapy/issues/547){.reference
    .external}, [issue
    716](https://github.com/scrapy/scrapy/issues/716){.reference
    .external}, [issue
    4512](https://github.com/scrapy/scrapy/issues/4512){.reference
    .external})

-   The [`__init__`{.docutils .literal .notranslate}]{.pre} method of
    [[`CsvItemExporter`{.xref .py .py-class .docutils .literal
    .notranslate}]{.pre}](index.html#scrapy.exporters.CsvItemExporter "scrapy.exporters.CsvItemExporter"){.reference
    .internal} now supports an [`errors`{.docutils .literal
    .notranslate}]{.pre} parameter to indicate how to handle encoding
    errors ([issue
    4755](https://github.com/scrapy/scrapy/issues/4755){.reference
    .external})

-   When [[using asyncio]{.std
    .std-ref}](index.html#using-asyncio){.hoverxref .tooltip .reference
    .internal}, it is now possible to [[set a custom asyncio loop]{.std
    .std-ref}](index.html#using-custom-loops){.hoverxref .tooltip
    .reference .internal} ([issue
    4306](https://github.com/scrapy/scrapy/issues/4306){.reference
    .external}, [issue
    4414](https://github.com/scrapy/scrapy/issues/4414){.reference
    .external})

-   Serialized requests (see [[Jobs: pausing and resuming crawls]{.std
    .std-ref}](index.html#topics-jobs){.hoverxref .tooltip .reference
    .internal}) now support callbacks that are spider methods that
    delegate on other callable ([issue
    4756](https://github.com/scrapy/scrapy/issues/4756){.reference
    .external})

-   When a response is larger than [[`DOWNLOAD_MAXSIZE`{.xref .std
    .std-setting .docutils .literal
    .notranslate}]{.pre}](index.html#std-setting-DOWNLOAD_MAXSIZE){.hoverxref
    .tooltip .reference .internal}, the logged message is now a warning,
    instead of an error ([issue
    3874](https://github.com/scrapy/scrapy/issues/3874){.reference
    .external}, [issue
    3886](https://github.com/scrapy/scrapy/issues/3886){.reference
    .external}, [issue
    4752](https://github.com/scrapy/scrapy/issues/4752){.reference
    .external})
:::

::: {#id49 .section}
##### Bug fixes[¶](#id49 "Permalink to this heading"){.headerlink}

-   The [[`genspider`{.xref .std .std-command .docutils .literal
    .notranslate}]{.pre}](index.html#std-command-genspider){.hoverxref
    .tooltip .reference .internal} command no longer overwrites existing
    files unless the [`--force`{.docutils .literal .notranslate}]{.pre}
    option is used ([issue
    4561](https://github.com/scrapy/scrapy/issues/4561){.reference
    .external}, [issue
    4616](https://github.com/scrapy/scrapy/issues/4616){.reference
    .external}, [issue
    4623](https://github.com/scrapy/scrapy/issues/4623){.reference
    .external})

-   Cookies with an empty value are no longer considered invalid cookies
    ([issue
    4772](https://github.com/scrapy/scrapy/issues/4772){.reference
    .external})

-   The [[`runspider`{.xref .std .std-command .docutils .literal
    .notranslate}]{.pre}](index.html#std-command-runspider){.hoverxref
    .tooltip .reference .internal} command now supports files with the
    [`.pyw`{.docutils .literal .notranslate}]{.pre} file extension
    ([issue
    4643](https://github.com/scrapy/scrapy/issues/4643){.reference
    .external}, [issue
    4646](https://github.com/scrapy/scrapy/issues/4646){.reference
    .external})

-   The [[`HttpProxyMiddleware`{.xref .py .py-class .docutils .literal
    .notranslate}]{.pre}](index.html#scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware "scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware"){.reference
    .internal} middleware now simply ignores unsupported proxy values
    ([issue
    3331](https://github.com/scrapy/scrapy/issues/3331){.reference
    .external}, [issue
    4778](https://github.com/scrapy/scrapy/issues/4778){.reference
    .external})

-   Checks for generator callbacks with a [`return`{.docutils .literal
    .notranslate}]{.pre} statement no longer warn about
    [`return`{.docutils .literal .notranslate}]{.pre} statements in
    nested functions ([issue
    4720](https://github.com/scrapy/scrapy/issues/4720){.reference
    .external}, [issue
    4721](https://github.com/scrapy/scrapy/issues/4721){.reference
    .external})

-   The system file mode creation mask no longer affects the permissions
    of files generated using the [[`startproject`{.xref .std
    .std-command .docutils .literal
    .notranslate}]{.pre}](index.html#std-command-startproject){.hoverxref
    .tooltip .reference .internal} command ([issue
    4722](https://github.com/scrapy/scrapy/issues/4722){.reference
    .external})

-   [`scrapy.utils.iterators.xmliter()`{.xref .py .py-func .docutils
    .literal .notranslate}]{.pre} now supports namespaced node names
    ([issue 861](https://github.com/scrapy/scrapy/issues/861){.reference
    .external}, [issue
    4746](https://github.com/scrapy/scrapy/issues/4746){.reference
    .external})

-   [`Request`{.xref .py .py-class .docutils .literal
    .notranslate}]{.pre} objects can now have [`about:`{.docutils
    .literal .notranslate}]{.pre} URLs, which can work when using a
    headless browser ([issue
    4835](https://github.com/scrapy/scrapy/issues/4835){.reference
    .external})
:::

::: {#id50 .section}
##### Documentation[¶](#id50 "Permalink to this heading"){.headerlink}

-   The [[`FEED_URI_PARAMS`{.xref .std .std-setting .docutils .literal
    .notranslate}]{.pre}](index.html#std-setting-FEED_URI_PARAMS){.hoverxref
    .tooltip .reference .internal} setting is now documented ([issue
    4671](https://github.com/scrapy/scrapy/issues/4671){.reference
    .external}, [issue
    4724](https://github.com/scrapy/scrapy/issues/4724){.reference
    .external})

-   Improved the documentation of [[link extractors]{.std
    .std-ref}](index.html#topics-link-extractors){.hoverxref .tooltip
    .reference .internal} with an usage example from a spider callback
    and reference documentation for the [[`Link`{.xref .py .py-class
    .docutils .literal
    .notranslate}]{.pre}](index.html#scrapy.link.Link "scrapy.link.Link"){.reference
    .internal} class ([issue
    4751](https://github.com/scrapy/scrapy/issues/4751){.reference
    .external}, [issue
    4775](https://github.com/scrapy/scrapy/issues/4775){.reference
    .external})

-   Clarified the impact of [[`CONCURRENT_REQUESTS`{.xref .std
    .std-setting .docutils .literal
    .notranslate}]{.pre}](index.html#std-setting-CONCURRENT_REQUESTS){.hoverxref
    .tooltip .reference .internal} when using the [[`CloseSpider`{.xref
    .py .py-class .docutils .literal
    .notranslate}]{.pre}](index.html#scrapy.extensions.closespider.CloseSpider "scrapy.extensions.closespider.CloseSpider"){.reference
    .internal} extension ([issue
    4836](https://github.com/scrapy/scrapy/issues/4836){.reference
    .external})

-   Removed references to Python 2's [`unicode`{.docutils .literal
    .notranslate}]{.pre} type ([issue
    4547](https://github.com/scrapy/scrapy/issues/4547){.reference
    .external}, [issue
    4703](https://github.com/scrapy/scrapy/issues/4703){.reference
    .external})

-   We now have an [[official deprecation policy]{.std
    .std-ref}](index.html#deprecation-policy){.hoverxref .tooltip
    .reference .internal} ([issue
    4705](https://github.com/scrapy/scrapy/issues/4705){.reference
    .external})

-   Our [[documentation policies]{.std
    .std-ref}](index.html#documentation-policies){.hoverxref .tooltip
    .reference .internal} now cover usage of Sphinx's
    [[`versionadded`{.xref .rst .rst-dir .docutils .literal
    .notranslate}]{.pre}](https://www.sphinx-doc.org/en/master/usage/restructuredtext/directives.html#directive-versionadded "(in Sphinx v7.3.0)"){.reference
    .external} and [[`versionchanged`{.xref .rst .rst-dir .docutils
    .literal
    .notranslate}]{.pre}](https://www.sphinx-doc.org/en/master/usage/restructuredtext/directives.html#directive-versionchanged "(in Sphinx v7.3.0)"){.reference
    .external} directives, and we have removed usages referencing Scrapy
    1.4.0 and earlier versions ([issue
    3971](https://github.com/scrapy/scrapy/issues/3971){.reference
    .external}, [issue
    4310](https://github.com/scrapy/scrapy/issues/4310){.reference
    .external})

-   Other documentation cleanups ([issue
    4090](https://github.com/scrapy/scrapy/issues/4090){.reference
    .external}, [issue
    4782](https://github.com/scrapy/scrapy/issues/4782){.reference
    .external}, [issue
    4800](https://github.com/scrapy/scrapy/issues/4800){.reference
    .external}, [issue
    4801](https://github.com/scrapy/scrapy/issues/4801){.reference
    .external}, [issue
    4809](https://github.com/scrapy/scrapy/issues/4809){.reference
    .external}, [issue
    4816](https://github.com/scrapy/scrapy/issues/4816){.reference
    .external}, [issue
    4825](https://github.com/scrapy/scrapy/issues/4825){.reference
    .external})
:::

::: {#id51 .section}
##### Quality assurance[¶](#id51 "Permalink to this heading"){.headerlink}

-   Extended typing hints ([issue
    4243](https://github.com/scrapy/scrapy/issues/4243){.reference
    .external}, [issue
    4691](https://github.com/scrapy/scrapy/issues/4691){.reference
    .external})

-   Added tests for the [[`check`{.xref .std .std-command .docutils
    .literal
    .notranslate}]{.pre}](index.html#std-command-check){.hoverxref
    .tooltip .reference .internal} command ([issue
    4663](https://github.com/scrapy/scrapy/issues/4663){.reference
    .external})

-   Fixed test failures on Debian ([issue
    4726](https://github.com/scrapy/scrapy/issues/4726){.reference
    .external}, [issue
    4727](https://github.com/scrapy/scrapy/issues/4727){.reference
    .external}, [issue
    4735](https://github.com/scrapy/scrapy/issues/4735){.reference
    .external})

-   Improved Windows test coverage ([issue
    4723](https://github.com/scrapy/scrapy/issues/4723){.reference
    .external})

-   Switched to [[formatted string literals]{.xref .std
    .std-ref}](https://docs.python.org/3/reference/lexical_analysis.html#f-strings "(in Python v3.12)"){.reference
    .external} where possible ([issue
    4307](https://github.com/scrapy/scrapy/issues/4307){.reference
    .external}, [issue
    4324](https://github.com/scrapy/scrapy/issues/4324){.reference
    .external}, [issue
    4672](https://github.com/scrapy/scrapy/issues/4672){.reference
    .external})

-   Modernized [`super()`{.xref .py .py-func .docutils .literal
    .notranslate}]{.pre} usage ([issue
    4707](https://github.com/scrapy/scrapy/issues/4707){.reference
    .external})

-   Other code and test cleanups ([issue
    1790](https://github.com/scrapy/scrapy/issues/1790){.reference
    .external}, [issue
    3288](https://github.com/scrapy/scrapy/issues/3288){.reference
    .external}, [issue
    4165](https://github.com/scrapy/scrapy/issues/4165){.reference
    .external}, [issue
    4564](https://github.com/scrapy/scrapy/issues/4564){.reference
    .external}, [issue
    4651](https://github.com/scrapy/scrapy/issues/4651){.reference
    .external}, [issue
    4714](https://github.com/scrapy/scrapy/issues/4714){.reference
    .external}, [issue
    4738](https://github.com/scrapy/scrapy/issues/4738){.reference
    .external}, [issue
    4745](https://github.com/scrapy/scrapy/issues/4745){.reference
    .external}, [issue
    4747](https://github.com/scrapy/scrapy/issues/4747){.reference
    .external}, [issue
    4761](https://github.com/scrapy/scrapy/issues/4761){.reference
    .external}, [issue
    4765](https://github.com/scrapy/scrapy/issues/4765){.reference
    .external}, [issue
    4804](https://github.com/scrapy/scrapy/issues/4804){.reference
    .external}, [issue
    4817](https://github.com/scrapy/scrapy/issues/4817){.reference
    .external}, [issue
    4820](https://github.com/scrapy/scrapy/issues/4820){.reference
    .external}, [issue
    4822](https://github.com/scrapy/scrapy/issues/4822){.reference
    .external}, [issue
    4839](https://github.com/scrapy/scrapy/issues/4839){.reference
    .external})
:::
:::

::: {#scrapy-2-3-0-2020-08-04 .section}
[]{#release-2-3-0}

#### Scrapy 2.3.0 (2020-08-04)[¶](#scrapy-2-3-0-2020-08-04 "Permalink to this heading"){.headerlink}

Highlights:

-   [[Feed exports]{.std
    .std-ref}](index.html#topics-feed-exports){.hoverxref .tooltip
    .reference .internal} now support [[Google Cloud Storage]{.std
    .std-ref}](index.html#topics-feed-storage-gcs){.hoverxref .tooltip
    .reference .internal} as a storage backend

-   The new [[`FEED_EXPORT_BATCH_ITEM_COUNT`{.xref .std .std-setting
    .docutils .literal
    .notranslate}]{.pre}](index.html#std-setting-FEED_EXPORT_BATCH_ITEM_COUNT){.hoverxref
    .tooltip .reference .internal} setting allows to deliver output
    items in batches of up to the specified number of items.

    It also serves as a workaround for [[delayed file delivery]{.std
    .std-ref}](index.html#delayed-file-delivery){.hoverxref .tooltip
    .reference .internal}, which causes Scrapy to only start item
    delivery after the crawl has finished when using certain storage
    backends ([[S3]{.std
    .std-ref}](index.html#topics-feed-storage-s3){.hoverxref .tooltip
    .reference .internal}, [[FTP]{.std
    .std-ref}](index.html#topics-feed-storage-ftp){.hoverxref .tooltip
    .reference .internal}, and now [[GCS]{.std
    .std-ref}](index.html#topics-feed-storage-gcs){.hoverxref .tooltip
    .reference .internal}).

-   The base implementation of [[item loaders]{.std
    .std-ref}](index.html#topics-loaders){.hoverxref .tooltip .reference
    .internal} has been moved into a separate library,
    [[itemloaders]{.xref .std
    .std-doc}](https://itemloaders.readthedocs.io/en/latest/index.html "(in itemloaders)"){.reference
    .external}, allowing usage from outside Scrapy and a separate
    release schedule

::: {#id52 .section}
##### Deprecation removals[¶](#id52 "Permalink to this heading"){.headerlink}

-   Removed the following classes and their parent modules from
    [`scrapy.linkextractors`{.docutils .literal .notranslate}]{.pre}:

    -   [`htmlparser.HtmlParserLinkExtractor`{.docutils .literal
        .notranslate}]{.pre}

    -   [`regex.RegexLinkExtractor`{.docutils .literal
        .notranslate}]{.pre}

    -   [`sgml.BaseSgmlLinkExtractor`{.docutils .literal
        .notranslate}]{.pre}

    -   [`sgml.SgmlLinkExtractor`{.docutils .literal
        .notranslate}]{.pre}

    Use [[`LinkExtractor`{.xref .py .py-class .docutils .literal
    .notranslate}]{.pre}](index.html#scrapy.linkextractors.lxmlhtml.LxmlLinkExtractor "scrapy.linkextractors.lxmlhtml.LxmlLinkExtractor"){.reference
    .internal} instead ([issue
    4356](https://github.com/scrapy/scrapy/issues/4356){.reference
    .external}, [issue
    4679](https://github.com/scrapy/scrapy/issues/4679){.reference
    .external})
:::

::: {#id53 .section}
##### Deprecations[¶](#id53 "Permalink to this heading"){.headerlink}

-   The [`scrapy.utils.python.retry_on_eintr`{.docutils .literal
    .notranslate}]{.pre} function is now deprecated ([issue
    4683](https://github.com/scrapy/scrapy/issues/4683){.reference
    .external})
:::

::: {#id54 .section}
##### New features[¶](#id54 "Permalink to this heading"){.headerlink}

-   [[Feed exports]{.std
    .std-ref}](index.html#topics-feed-exports){.hoverxref .tooltip
    .reference .internal} support [[Google Cloud Storage]{.std
    .std-ref}](index.html#topics-feed-storage-gcs){.hoverxref .tooltip
    .reference .internal} ([issue
    685](https://github.com/scrapy/scrapy/issues/685){.reference
    .external}, [issue
    3608](https://github.com/scrapy/scrapy/issues/3608){.reference
    .external})

-   New [[`FEED_EXPORT_BATCH_ITEM_COUNT`{.xref .std .std-setting
    .docutils .literal
    .notranslate}]{.pre}](index.html#std-setting-FEED_EXPORT_BATCH_ITEM_COUNT){.hoverxref
    .tooltip .reference .internal} setting for batch deliveries ([issue
    4250](https://github.com/scrapy/scrapy/issues/4250){.reference
    .external}, [issue
    4434](https://github.com/scrapy/scrapy/issues/4434){.reference
    .external})

-   The [[`parse`{.xref .std .std-command .docutils .literal
    .notranslate}]{.pre}](index.html#std-command-parse){.hoverxref
    .tooltip .reference .internal} command now allows specifying an
    output file ([issue
    4317](https://github.com/scrapy/scrapy/issues/4317){.reference
    .external}, [issue
    4377](https://github.com/scrapy/scrapy/issues/4377){.reference
    .external})

-   [[`Request.from_curl`{.xref .py .py-meth .docutils .literal
    .notranslate}]{.pre}](index.html#scrapy.http.Request.from_curl "scrapy.http.Request.from_curl"){.reference
    .internal} and [[`curl_to_request_kwargs()`{.xref .py .py-func
    .docutils .literal
    .notranslate}]{.pre}](index.html#scrapy.utils.curl.curl_to_request_kwargs "scrapy.utils.curl.curl_to_request_kwargs"){.reference
    .internal} now also support [`--data-raw`{.docutils .literal
    .notranslate}]{.pre} ([issue
    4612](https://github.com/scrapy/scrapy/issues/4612){.reference
    .external})

-   A [`parse`{.docutils .literal .notranslate}]{.pre} callback may now
    be used in built-in spider subclasses, such as [[`CrawlSpider`{.xref
    .py .py-class .docutils .literal
    .notranslate}]{.pre}](index.html#scrapy.spiders.CrawlSpider "scrapy.spiders.CrawlSpider"){.reference
    .internal} ([issue
    712](https://github.com/scrapy/scrapy/issues/712){.reference
    .external}, [issue
    732](https://github.com/scrapy/scrapy/issues/732){.reference
    .external}, [issue
    781](https://github.com/scrapy/scrapy/issues/781){.reference
    .external}, [issue
    4254](https://github.com/scrapy/scrapy/issues/4254){.reference
    .external} )
:::

::: {#id55 .section}
##### Bug fixes[¶](#id55 "Permalink to this heading"){.headerlink}

-   Fixed the [[CSV exporting]{.std
    .std-ref}](index.html#topics-feed-format-csv){.hoverxref .tooltip
    .reference .internal} of [[dataclass items]{.std
    .std-ref}](index.html#dataclass-items){.hoverxref .tooltip
    .reference .internal} and [[attr.s items]{.std
    .std-ref}](index.html#attrs-items){.hoverxref .tooltip .reference
    .internal} ([issue
    4667](https://github.com/scrapy/scrapy/issues/4667){.reference
    .external}, [issue
    4668](https://github.com/scrapy/scrapy/issues/4668){.reference
    .external})

-   [[`Request.from_curl`{.xref .py .py-meth .docutils .literal
    .notranslate}]{.pre}](index.html#scrapy.http.Request.from_curl "scrapy.http.Request.from_curl"){.reference
    .internal} and [[`curl_to_request_kwargs()`{.xref .py .py-func
    .docutils .literal
    .notranslate}]{.pre}](index.html#scrapy.utils.curl.curl_to_request_kwargs "scrapy.utils.curl.curl_to_request_kwargs"){.reference
    .internal} now set the request method to [`POST`{.docutils .literal
    .notranslate}]{.pre} when a request body is specified and no request
    method is specified ([issue
    4612](https://github.com/scrapy/scrapy/issues/4612){.reference
    .external})

-   The processing of ANSI escape sequences in enabled in Windows
    10.0.14393 and later, where it is required for colored output
    ([issue
    4393](https://github.com/scrapy/scrapy/issues/4393){.reference
    .external}, [issue
    4403](https://github.com/scrapy/scrapy/issues/4403){.reference
    .external})
:::

::: {#id56 .section}
##### Documentation[¶](#id56 "Permalink to this heading"){.headerlink}

-   Updated the [OpenSSL cipher list
    format](https://www.openssl.org/docs/manmaster/man1/openssl-ciphers.html#CIPHER-LIST-FORMAT){.reference
    .external} link in the documentation about the
    [[`DOWNLOADER_CLIENT_TLS_CIPHERS`{.xref .std .std-setting .docutils
    .literal
    .notranslate}]{.pre}](index.html#std-setting-DOWNLOADER_CLIENT_TLS_CIPHERS){.hoverxref
    .tooltip .reference .internal} setting ([issue
    4653](https://github.com/scrapy/scrapy/issues/4653){.reference
    .external})

-   Simplified the code example in [[Working with dataclass items]{.std
    .std-ref}](index.html#topics-loaders-dataclass){.hoverxref .tooltip
    .reference .internal} ([issue
    4652](https://github.com/scrapy/scrapy/issues/4652){.reference
    .external})
:::

::: {#id57 .section}
##### Quality assurance[¶](#id57 "Permalink to this heading"){.headerlink}

-   The base implementation of [[item loaders]{.std
    .std-ref}](index.html#topics-loaders){.hoverxref .tooltip .reference
    .internal} has been moved into [[itemloaders]{.xref .std
    .std-doc}](https://itemloaders.readthedocs.io/en/latest/index.html "(in itemloaders)"){.reference
    .external} ([issue
    4005](https://github.com/scrapy/scrapy/issues/4005){.reference
    .external}, [issue
    4516](https://github.com/scrapy/scrapy/issues/4516){.reference
    .external})

-   Fixed a silenced error in some scheduler tests ([issue
    4644](https://github.com/scrapy/scrapy/issues/4644){.reference
    .external}, [issue
    4645](https://github.com/scrapy/scrapy/issues/4645){.reference
    .external})

-   Renewed the localhost certificate used for SSL tests ([issue
    4650](https://github.com/scrapy/scrapy/issues/4650){.reference
    .external})

-   Removed cookie-handling code specific to Python 2 ([issue
    4682](https://github.com/scrapy/scrapy/issues/4682){.reference
    .external})

-   Stopped using Python 2 unicode literal syntax ([issue
    4704](https://github.com/scrapy/scrapy/issues/4704){.reference
    .external})

-   Stopped using a backlash for line continuation ([issue
    4673](https://github.com/scrapy/scrapy/issues/4673){.reference
    .external})

-   Removed unneeded entries from the MyPy exception list ([issue
    4690](https://github.com/scrapy/scrapy/issues/4690){.reference
    .external})

-   Automated tests now pass on Windows as part of our continuous
    integration system ([issue
    4458](https://github.com/scrapy/scrapy/issues/4458){.reference
    .external})

-   Automated tests now pass on the latest PyPy version for supported
    Python versions in our continuous integration system ([issue
    4504](https://github.com/scrapy/scrapy/issues/4504){.reference
    .external})
:::
:::

::: {#scrapy-2-2-1-2020-07-17 .section}
[]{#release-2-2-1}

#### Scrapy 2.2.1 (2020-07-17)[¶](#scrapy-2-2-1-2020-07-17 "Permalink to this heading"){.headerlink}

-   The [[`startproject`{.xref .std .std-command .docutils .literal
    .notranslate}]{.pre}](index.html#std-command-startproject){.hoverxref
    .tooltip .reference .internal} command no longer makes unintended
    changes to the permissions of files in the destination folder, such
    as removing execution permissions ([issue
    4662](https://github.com/scrapy/scrapy/issues/4662){.reference
    .external}, [issue
    4666](https://github.com/scrapy/scrapy/issues/4666){.reference
    .external})
:::

::: {#scrapy-2-2-0-2020-06-24 .section}
[]{#release-2-2-0}

#### Scrapy 2.2.0 (2020-06-24)[¶](#scrapy-2-2-0-2020-06-24 "Permalink to this heading"){.headerlink}

Highlights:

-   Python 3.5.2+ is required now

-   [[dataclass objects]{.std
    .std-ref}](index.html#dataclass-items){.hoverxref .tooltip
    .reference .internal} and [[attrs objects]{.std
    .std-ref}](index.html#attrs-items){.hoverxref .tooltip .reference
    .internal} are now valid [[item types]{.std
    .std-ref}](index.html#item-types){.hoverxref .tooltip .reference
    .internal}

-   New [[`TextResponse.json`{.xref .py .py-meth .docutils .literal
    .notranslate}]{.pre}](index.html#scrapy.http.TextResponse.json "scrapy.http.TextResponse.json"){.reference
    .internal} method

-   New [[`bytes_received`{.xref .std .std-signal .docutils .literal
    .notranslate}]{.pre}](index.html#std-signal-bytes_received){.hoverxref
    .tooltip .reference .internal} signal that allows canceling response
    download

-   [[`CookiesMiddleware`{.xref .py .py-class .docutils .literal
    .notranslate}]{.pre}](index.html#scrapy.downloadermiddlewares.cookies.CookiesMiddleware "scrapy.downloadermiddlewares.cookies.CookiesMiddleware"){.reference
    .internal} fixes

::: {#id58 .section}
##### Backward-incompatible changes[¶](#id58 "Permalink to this heading"){.headerlink}

-   Support for Python 3.5.0 and 3.5.1 has been dropped; Scrapy now
    refuses to run with a Python version lower than 3.5.2, which
    introduced [[`typing.Type`{.xref .py .py-class .docutils .literal
    .notranslate}]{.pre}](https://docs.python.org/3/library/typing.html#typing.Type "(in Python v3.12)"){.reference
    .external} ([issue
    4615](https://github.com/scrapy/scrapy/issues/4615){.reference
    .external})
:::

::: {#id59 .section}
##### Deprecations[¶](#id59 "Permalink to this heading"){.headerlink}

-   [`TextResponse.body_as_unicode`{.xref .py .py-meth .docutils
    .literal .notranslate}]{.pre} is now deprecated, use
    [[`TextResponse.text`{.xref .py .py-attr .docutils .literal
    .notranslate}]{.pre}](index.html#scrapy.http.TextResponse.text "scrapy.http.TextResponse.text"){.reference
    .internal} instead ([issue
    4546](https://github.com/scrapy/scrapy/issues/4546){.reference
    .external}, [issue
    4555](https://github.com/scrapy/scrapy/issues/4555){.reference
    .external}, [issue
    4579](https://github.com/scrapy/scrapy/issues/4579){.reference
    .external})

-   [`scrapy.item.BaseItem`{.xref .py .py-class .docutils .literal
    .notranslate}]{.pre} is now deprecated, use
    [`scrapy.item.Item`{.xref .py .py-class .docutils .literal
    .notranslate}]{.pre} instead ([issue
    4534](https://github.com/scrapy/scrapy/issues/4534){.reference
    .external})
:::

::: {#id60 .section}
##### New features[¶](#id60 "Permalink to this heading"){.headerlink}

-   [[dataclass objects]{.std
    .std-ref}](index.html#dataclass-items){.hoverxref .tooltip
    .reference .internal} and [[attrs objects]{.std
    .std-ref}](index.html#attrs-items){.hoverxref .tooltip .reference
    .internal} are now valid [[item types]{.std
    .std-ref}](index.html#item-types){.hoverxref .tooltip .reference
    .internal}, and a new
    [itemadapter](https://github.com/scrapy/itemadapter){.reference
    .external} library makes it easy to write code that [[supports any
    item type]{.std
    .std-ref}](index.html#supporting-item-types){.hoverxref .tooltip
    .reference .internal} ([issue
    2749](https://github.com/scrapy/scrapy/issues/2749){.reference
    .external}, [issue
    2807](https://github.com/scrapy/scrapy/issues/2807){.reference
    .external}, [issue
    3761](https://github.com/scrapy/scrapy/issues/3761){.reference
    .external}, [issue
    3881](https://github.com/scrapy/scrapy/issues/3881){.reference
    .external}, [issue
    4642](https://github.com/scrapy/scrapy/issues/4642){.reference
    .external})

-   A new [[`TextResponse.json`{.xref .py .py-meth .docutils .literal
    .notranslate}]{.pre}](index.html#scrapy.http.TextResponse.json "scrapy.http.TextResponse.json"){.reference
    .internal} method allows to deserialize JSON responses ([issue
    2444](https://github.com/scrapy/scrapy/issues/2444){.reference
    .external}, [issue
    4460](https://github.com/scrapy/scrapy/issues/4460){.reference
    .external}, [issue
    4574](https://github.com/scrapy/scrapy/issues/4574){.reference
    .external})

-   A new [[`bytes_received`{.xref .std .std-signal .docutils .literal
    .notranslate}]{.pre}](index.html#std-signal-bytes_received){.hoverxref
    .tooltip .reference .internal} signal allows monitoring response
    download progress and [[stopping downloads]{.std
    .std-ref}](index.html#topics-stop-response-download){.hoverxref
    .tooltip .reference .internal} ([issue
    4205](https://github.com/scrapy/scrapy/issues/4205){.reference
    .external}, [issue
    4559](https://github.com/scrapy/scrapy/issues/4559){.reference
    .external})

-   The dictionaries in the result list of a [[media pipeline]{.std
    .std-ref}](index.html#topics-media-pipeline){.hoverxref .tooltip
    .reference .internal} now include a new key, [`status`{.docutils
    .literal .notranslate}]{.pre}, which indicates if the file was
    downloaded or, if the file was not downloaded, why it was not
    downloaded; see [[`FilesPipeline.get_media_requests`{.xref .py
    .py-meth .docutils .literal
    .notranslate}]{.pre}](index.html#scrapy.pipelines.files.FilesPipeline.get_media_requests "scrapy.pipelines.files.FilesPipeline.get_media_requests"){.reference
    .internal} for more information ([issue
    2893](https://github.com/scrapy/scrapy/issues/2893){.reference
    .external}, [issue
    4486](https://github.com/scrapy/scrapy/issues/4486){.reference
    .external})

-   When using [[Google Cloud Storage]{.std
    .std-ref}](index.html#media-pipeline-gcs){.hoverxref .tooltip
    .reference .internal} for a [[media pipeline]{.std
    .std-ref}](index.html#topics-media-pipeline){.hoverxref .tooltip
    .reference .internal}, a warning is now logged if the configured
    credentials do not grant the required permissions ([issue
    4346](https://github.com/scrapy/scrapy/issues/4346){.reference
    .external}, [issue
    4508](https://github.com/scrapy/scrapy/issues/4508){.reference
    .external})

-   [[Link extractors]{.std
    .std-ref}](index.html#topics-link-extractors){.hoverxref .tooltip
    .reference .internal} are now serializable, as long as you do not
    use [[lambdas]{.xref .std
    .std-ref}](https://docs.python.org/3/reference/expressions.html#lambda "(in Python v3.12)"){.reference
    .external} for parameters; for example, you can now pass link
    extractors in [[`Request.cb_kwargs`{.xref .py .py-attr .docutils
    .literal
    .notranslate}]{.pre}](index.html#scrapy.http.Request.cb_kwargs "scrapy.http.Request.cb_kwargs"){.reference
    .internal} or [[`Request.meta`{.xref .py .py-attr .docutils .literal
    .notranslate}]{.pre}](index.html#scrapy.http.Request.meta "scrapy.http.Request.meta"){.reference
    .internal} when [[persisting scheduled requests]{.std
    .std-ref}](index.html#topics-jobs){.hoverxref .tooltip .reference
    .internal} ([issue
    4554](https://github.com/scrapy/scrapy/issues/4554){.reference
    .external})

-   Upgraded the [[pickle protocol]{.xref .std
    .std-ref}](https://docs.python.org/3/library/pickle.html#pickle-protocols "(in Python v3.12)"){.reference
    .external} that Scrapy uses from protocol 2 to protocol 4, improving
    serialization capabilities and performance ([issue
    4135](https://github.com/scrapy/scrapy/issues/4135){.reference
    .external}, [issue
    4541](https://github.com/scrapy/scrapy/issues/4541){.reference
    .external})

-   [`scrapy.utils.misc.create_instance()`{.xref .py .py-func .docutils
    .literal .notranslate}]{.pre} now raises a [[`TypeError`{.xref .py
    .py-exc .docutils .literal
    .notranslate}]{.pre}](https://docs.python.org/3/library/exceptions.html#TypeError "(in Python v3.12)"){.reference
    .external} exception if the resulting instance is [`None`{.docutils
    .literal .notranslate}]{.pre} ([issue
    4528](https://github.com/scrapy/scrapy/issues/4528){.reference
    .external}, [issue
    4532](https://github.com/scrapy/scrapy/issues/4532){.reference
    .external})
:::

::: {#id61 .section}
##### Bug fixes[¶](#id61 "Permalink to this heading"){.headerlink}

-   [[`CookiesMiddleware`{.xref .py .py-class .docutils .literal
    .notranslate}]{.pre}](index.html#scrapy.downloadermiddlewares.cookies.CookiesMiddleware "scrapy.downloadermiddlewares.cookies.CookiesMiddleware"){.reference
    .internal} no longer discards cookies defined in
    [[`Request.headers`{.xref .py .py-attr .docutils .literal
    .notranslate}]{.pre}](index.html#scrapy.http.Request.headers "scrapy.http.Request.headers"){.reference
    .internal} ([issue
    1992](https://github.com/scrapy/scrapy/issues/1992){.reference
    .external}, [issue
    2400](https://github.com/scrapy/scrapy/issues/2400){.reference
    .external})

-   [[`CookiesMiddleware`{.xref .py .py-class .docutils .literal
    .notranslate}]{.pre}](index.html#scrapy.downloadermiddlewares.cookies.CookiesMiddleware "scrapy.downloadermiddlewares.cookies.CookiesMiddleware"){.reference
    .internal} no longer re-encodes cookies defined as [[`bytes`{.xref
    .py .py-class .docutils .literal
    .notranslate}]{.pre}](https://docs.python.org/3/library/stdtypes.html#bytes "(in Python v3.12)"){.reference
    .external} in the [`cookies`{.docutils .literal .notranslate}]{.pre}
    parameter of the [`__init__`{.docutils .literal .notranslate}]{.pre}
    method of [[`Request`{.xref .py .py-class .docutils .literal
    .notranslate}]{.pre}](index.html#scrapy.http.Request "scrapy.http.Request"){.reference
    .internal} ([issue
    2400](https://github.com/scrapy/scrapy/issues/2400){.reference
    .external}, [issue
    3575](https://github.com/scrapy/scrapy/issues/3575){.reference
    .external})

-   When [[`FEEDS`{.xref .std .std-setting .docutils .literal
    .notranslate}]{.pre}](index.html#std-setting-FEEDS){.hoverxref
    .tooltip .reference .internal} defines multiple URIs,
    [[`FEED_STORE_EMPTY`{.xref .std .std-setting .docutils .literal
    .notranslate}]{.pre}](index.html#std-setting-FEED_STORE_EMPTY){.hoverxref
    .tooltip .reference .internal} is [`False`{.docutils .literal
    .notranslate}]{.pre} and the crawl yields no items, Scrapy no longer
    stops feed exports after the first URI ([issue
    4621](https://github.com/scrapy/scrapy/issues/4621){.reference
    .external}, [issue
    4626](https://github.com/scrapy/scrapy/issues/4626){.reference
    .external})

-   [[`Spider`{.xref .py .py-class .docutils .literal
    .notranslate}]{.pre}](index.html#scrapy.spiders.Spider "scrapy.spiders.Spider"){.reference
    .internal} callbacks defined using [[coroutine
    syntax]{.doc}](index.html#document-topics/coroutines){.reference
    .internal} no longer need to return an iterable, and may instead
    return a [[`Request`{.xref .py .py-class .docutils .literal
    .notranslate}]{.pre}](index.html#scrapy.http.Request "scrapy.http.Request"){.reference
    .internal} object, an [[item]{.std
    .std-ref}](index.html#topics-items){.hoverxref .tooltip .reference
    .internal}, or [`None`{.docutils .literal .notranslate}]{.pre}
    ([issue
    4609](https://github.com/scrapy/scrapy/issues/4609){.reference
    .external})

-   The [[`startproject`{.xref .std .std-command .docutils .literal
    .notranslate}]{.pre}](index.html#std-command-startproject){.hoverxref
    .tooltip .reference .internal} command now ensures that the
    generated project folders and files have the right permissions
    ([issue
    4604](https://github.com/scrapy/scrapy/issues/4604){.reference
    .external})

-   Fix a [[`KeyError`{.xref .py .py-exc .docutils .literal
    .notranslate}]{.pre}](https://docs.python.org/3/library/exceptions.html#KeyError "(in Python v3.12)"){.reference
    .external} exception being sometimes raised from
    [`scrapy.utils.datatypes.LocalWeakReferencedCache`{.xref .py
    .py-class .docutils .literal .notranslate}]{.pre} ([issue
    4597](https://github.com/scrapy/scrapy/issues/4597){.reference
    .external}, [issue
    4599](https://github.com/scrapy/scrapy/issues/4599){.reference
    .external})

-   When [[`FEEDS`{.xref .std .std-setting .docutils .literal
    .notranslate}]{.pre}](index.html#std-setting-FEEDS){.hoverxref
    .tooltip .reference .internal} defines multiple URIs, log messages
    about items being stored now contain information from the
    corresponding feed, instead of always containing information about
    only one of the feeds ([issue
    4619](https://github.com/scrapy/scrapy/issues/4619){.reference
    .external}, [issue
    4629](https://github.com/scrapy/scrapy/issues/4629){.reference
    .external})
:::

::: {#id62 .section}
##### Documentation[¶](#id62 "Permalink to this heading"){.headerlink}

-   Added a new section about [[accessing cb_kwargs from errbacks]{.std
    .std-ref}](index.html#errback-cb-kwargs){.hoverxref .tooltip
    .reference .internal} ([issue
    4598](https://github.com/scrapy/scrapy/issues/4598){.reference
    .external}, [issue
    4634](https://github.com/scrapy/scrapy/issues/4634){.reference
    .external})

-   Covered [chompjs](https://github.com/Nykakin/chompjs){.reference
    .external} in [[Parsing JavaScript code]{.std
    .std-ref}](index.html#topics-parsing-javascript){.hoverxref .tooltip
    .reference .internal} ([issue
    4556](https://github.com/scrapy/scrapy/issues/4556){.reference
    .external}, [issue
    4562](https://github.com/scrapy/scrapy/issues/4562){.reference
    .external})

-   Removed from
    [[Coroutines]{.doc}](index.html#document-topics/coroutines){.reference
    .internal} the warning about the API being experimental ([issue
    4511](https://github.com/scrapy/scrapy/issues/4511){.reference
    .external}, [issue
    4513](https://github.com/scrapy/scrapy/issues/4513){.reference
    .external})

-   Removed references to unsupported versions of [[Twisted]{.xref .std
    .std-doc}](https://docs.twisted.org/en/stable/index.html "(in Twisted v23.10)"){.reference
    .external} ([issue
    4533](https://github.com/scrapy/scrapy/issues/4533){.reference
    .external})

-   Updated the description of the [[screenshot pipeline example]{.std
    .std-ref}](index.html#screenshotpipeline){.hoverxref .tooltip
    .reference .internal}, which now uses [[coroutine
    syntax]{.doc}](index.html#document-topics/coroutines){.reference
    .internal} instead of returning a [[`Deferred`{.xref .py .py-class
    .docutils .literal
    .notranslate}]{.pre}](https://docs.twisted.org/en/stable/api/twisted.internet.defer.Deferred.html "(in Twisted)"){.reference
    .external} ([issue
    4514](https://github.com/scrapy/scrapy/issues/4514){.reference
    .external}, [issue
    4593](https://github.com/scrapy/scrapy/issues/4593){.reference
    .external})

-   Removed a misleading import line from the
    [[`scrapy.utils.log.configure_logging()`{.xref .py .py-func
    .docutils .literal
    .notranslate}]{.pre}](index.html#scrapy.utils.log.configure_logging "scrapy.utils.log.configure_logging"){.reference
    .internal} code example ([issue
    4510](https://github.com/scrapy/scrapy/issues/4510){.reference
    .external}, [issue
    4587](https://github.com/scrapy/scrapy/issues/4587){.reference
    .external})

-   The display-on-hover behavior of internal documentation references
    now also covers links to [[commands]{.std
    .std-ref}](index.html#topics-commands){.hoverxref .tooltip
    .reference .internal}, [[`Request.meta`{.xref .py .py-attr .docutils
    .literal
    .notranslate}]{.pre}](index.html#scrapy.http.Request.meta "scrapy.http.Request.meta"){.reference
    .internal} keys, [[settings]{.std
    .std-ref}](index.html#topics-settings){.hoverxref .tooltip
    .reference .internal} and [[signals]{.std
    .std-ref}](index.html#topics-signals){.hoverxref .tooltip .reference
    .internal} ([issue
    4495](https://github.com/scrapy/scrapy/issues/4495){.reference
    .external}, [issue
    4563](https://github.com/scrapy/scrapy/issues/4563){.reference
    .external})

-   It is again possible to download the documentation for offline
    reading ([issue
    4578](https://github.com/scrapy/scrapy/issues/4578){.reference
    .external}, [issue
    4585](https://github.com/scrapy/scrapy/issues/4585){.reference
    .external})

-   Removed backslashes preceding [`*args`{.docutils .literal
    .notranslate}]{.pre} and [`**kwargs`{.docutils .literal
    .notranslate}]{.pre} in some function and method signatures ([issue
    4592](https://github.com/scrapy/scrapy/issues/4592){.reference
    .external}, [issue
    4596](https://github.com/scrapy/scrapy/issues/4596){.reference
    .external})
:::

::: {#id63 .section}
##### Quality assurance[¶](#id63 "Permalink to this heading"){.headerlink}

-   Adjusted the code base further to our [[style guidelines]{.std
    .std-ref}](index.html#coding-style){.hoverxref .tooltip .reference
    .internal} ([issue
    4237](https://github.com/scrapy/scrapy/issues/4237){.reference
    .external}, [issue
    4525](https://github.com/scrapy/scrapy/issues/4525){.reference
    .external}, [issue
    4538](https://github.com/scrapy/scrapy/issues/4538){.reference
    .external}, [issue
    4539](https://github.com/scrapy/scrapy/issues/4539){.reference
    .external}, [issue
    4540](https://github.com/scrapy/scrapy/issues/4540){.reference
    .external}, [issue
    4542](https://github.com/scrapy/scrapy/issues/4542){.reference
    .external}, [issue
    4543](https://github.com/scrapy/scrapy/issues/4543){.reference
    .external}, [issue
    4544](https://github.com/scrapy/scrapy/issues/4544){.reference
    .external}, [issue
    4545](https://github.com/scrapy/scrapy/issues/4545){.reference
    .external}, [issue
    4557](https://github.com/scrapy/scrapy/issues/4557){.reference
    .external}, [issue
    4558](https://github.com/scrapy/scrapy/issues/4558){.reference
    .external}, [issue
    4566](https://github.com/scrapy/scrapy/issues/4566){.reference
    .external}, [issue
    4568](https://github.com/scrapy/scrapy/issues/4568){.reference
    .external}, [issue
    4572](https://github.com/scrapy/scrapy/issues/4572){.reference
    .external})

-   Removed remnants of Python 2 support ([issue
    4550](https://github.com/scrapy/scrapy/issues/4550){.reference
    .external}, [issue
    4553](https://github.com/scrapy/scrapy/issues/4553){.reference
    .external}, [issue
    4568](https://github.com/scrapy/scrapy/issues/4568){.reference
    .external})

-   Improved code sharing between the [[`crawl`{.xref .std .std-command
    .docutils .literal
    .notranslate}]{.pre}](index.html#std-command-crawl){.hoverxref
    .tooltip .reference .internal} and [[`runspider`{.xref .std
    .std-command .docutils .literal
    .notranslate}]{.pre}](index.html#std-command-runspider){.hoverxref
    .tooltip .reference .internal} commands ([issue
    4548](https://github.com/scrapy/scrapy/issues/4548){.reference
    .external}, [issue
    4552](https://github.com/scrapy/scrapy/issues/4552){.reference
    .external})

-   Replaced [`chain(*iterable)`{.docutils .literal .notranslate}]{.pre}
    with [`chain.from_iterable(iterable)`{.docutils .literal
    .notranslate}]{.pre} ([issue
    4635](https://github.com/scrapy/scrapy/issues/4635){.reference
    .external})

-   You may now run the [[`asyncio`{.xref .py .py-mod .docutils .literal
    .notranslate}]{.pre}](https://docs.python.org/3/library/asyncio.html#module-asyncio "(in Python v3.12)"){.reference
    .external} tests with Tox on any Python version ([issue
    4521](https://github.com/scrapy/scrapy/issues/4521){.reference
    .external})

-   Updated test requirements to reflect an incompatibility with pytest
    5.4 and 5.4.1 ([issue
    4588](https://github.com/scrapy/scrapy/issues/4588){.reference
    .external})

-   Improved [[`SpiderLoader`{.xref .py .py-class .docutils .literal
    .notranslate}]{.pre}](index.html#scrapy.spiderloader.SpiderLoader "scrapy.spiderloader.SpiderLoader"){.reference
    .internal} test coverage for scenarios involving duplicate spider
    names ([issue
    4549](https://github.com/scrapy/scrapy/issues/4549){.reference
    .external}, [issue
    4560](https://github.com/scrapy/scrapy/issues/4560){.reference
    .external})

-   Configured Travis CI to also run the tests with Python 3.5.2 ([issue
    4518](https://github.com/scrapy/scrapy/issues/4518){.reference
    .external}, [issue
    4615](https://github.com/scrapy/scrapy/issues/4615){.reference
    .external})

-   Added a [Pylint](https://www.pylint.org/){.reference .external} job
    to Travis CI ([issue
    3727](https://github.com/scrapy/scrapy/issues/3727){.reference
    .external})

-   Added a [Mypy](http://mypy-lang.org/){.reference .external} job to
    Travis CI ([issue
    4637](https://github.com/scrapy/scrapy/issues/4637){.reference
    .external})

-   Made use of set literals in tests ([issue
    4573](https://github.com/scrapy/scrapy/issues/4573){.reference
    .external})

-   Cleaned up the Travis CI configuration ([issue
    4517](https://github.com/scrapy/scrapy/issues/4517){.reference
    .external}, [issue
    4519](https://github.com/scrapy/scrapy/issues/4519){.reference
    .external}, [issue
    4522](https://github.com/scrapy/scrapy/issues/4522){.reference
    .external}, [issue
    4537](https://github.com/scrapy/scrapy/issues/4537){.reference
    .external})
:::
:::

::: {#scrapy-2-1-0-2020-04-24 .section}
[]{#release-2-1-0}

#### Scrapy 2.1.0 (2020-04-24)[¶](#scrapy-2-1-0-2020-04-24 "Permalink to this heading"){.headerlink}

Highlights:

-   New [[`FEEDS`{.xref .std .std-setting .docutils .literal
    .notranslate}]{.pre}](index.html#std-setting-FEEDS){.hoverxref
    .tooltip .reference .internal} setting to export to multiple feeds

-   New [[`Response.ip_address`{.xref .py .py-attr .docutils .literal
    .notranslate}]{.pre}](index.html#scrapy.http.Response.ip_address "scrapy.http.Response.ip_address"){.reference
    .internal} attribute

::: {#id64 .section}
##### Backward-incompatible changes[¶](#id64 "Permalink to this heading"){.headerlink}

-   [[`AssertionError`{.xref .py .py-exc .docutils .literal
    .notranslate}]{.pre}](https://docs.python.org/3/library/exceptions.html#AssertionError "(in Python v3.12)"){.reference
    .external} exceptions triggered by [[assert]{.xref .std
    .std-ref}](https://docs.python.org/3/reference/simple_stmts.html#assert "(in Python v3.12)"){.reference
    .external} statements have been replaced by new exception types, to
    support running Python in optimized mode (see [[`-O`{.xref .std
    .std-option .docutils .literal
    .notranslate}]{.pre}](https://docs.python.org/3/using/cmdline.html#cmdoption-O "(in Python v3.12)"){.reference
    .external}) without changing Scrapy's behavior in any unexpected
    ways.

    If you catch an [[`AssertionError`{.xref .py .py-exc .docutils
    .literal
    .notranslate}]{.pre}](https://docs.python.org/3/library/exceptions.html#AssertionError "(in Python v3.12)"){.reference
    .external} exception from Scrapy, update your code to catch the
    corresponding new exception.

    ([issue
    4440](https://github.com/scrapy/scrapy/issues/4440){.reference
    .external})
:::

::: {#id65 .section}
##### Deprecation removals[¶](#id65 "Permalink to this heading"){.headerlink}

-   The [`LOG_UNSERIALIZABLE_REQUESTS`{.docutils .literal
    .notranslate}]{.pre} setting is no longer supported, use
    [[`SCHEDULER_DEBUG`{.xref .std .std-setting .docutils .literal
    .notranslate}]{.pre}](index.html#std-setting-SCHEDULER_DEBUG){.hoverxref
    .tooltip .reference .internal} instead ([issue
    4385](https://github.com/scrapy/scrapy/issues/4385){.reference
    .external})

-   The [`REDIRECT_MAX_METAREFRESH_DELAY`{.docutils .literal
    .notranslate}]{.pre} setting is no longer supported, use
    [[`METAREFRESH_MAXDELAY`{.xref .std .std-setting .docutils .literal
    .notranslate}]{.pre}](index.html#std-setting-METAREFRESH_MAXDELAY){.hoverxref
    .tooltip .reference .internal} instead ([issue
    4385](https://github.com/scrapy/scrapy/issues/4385){.reference
    .external})

-   The [`ChunkedTransferMiddleware`{.xref .py .py-class .docutils
    .literal .notranslate}]{.pre} middleware has been removed, including
    the entire [`scrapy.downloadermiddlewares.chunked`{.xref .py
    .py-class .docutils .literal .notranslate}]{.pre} module; chunked
    transfers work out of the box ([issue
    4431](https://github.com/scrapy/scrapy/issues/4431){.reference
    .external})

-   The [`spiders`{.docutils .literal .notranslate}]{.pre} property has
    been removed from [[`Crawler`{.xref .py .py-class .docutils .literal
    .notranslate}]{.pre}](index.html#scrapy.crawler.Crawler "scrapy.crawler.Crawler"){.reference
    .internal}, use [`CrawlerRunner.spider_loader`{.xref .py .py-class
    .docutils .literal .notranslate}]{.pre} or instantiate
    [[`SPIDER_LOADER_CLASS`{.xref .std .std-setting .docutils .literal
    .notranslate}]{.pre}](index.html#std-setting-SPIDER_LOADER_CLASS){.hoverxref
    .tooltip .reference .internal} with your settings instead ([issue
    4398](https://github.com/scrapy/scrapy/issues/4398){.reference
    .external})

-   The [`MultiValueDict`{.docutils .literal .notranslate}]{.pre},
    [`MultiValueDictKeyError`{.docutils .literal .notranslate}]{.pre},
    and [`SiteNode`{.docutils .literal .notranslate}]{.pre} classes have
    been removed from [`scrapy.utils.datatypes`{.xref .py .py-mod
    .docutils .literal .notranslate}]{.pre} ([issue
    4400](https://github.com/scrapy/scrapy/issues/4400){.reference
    .external})
:::

::: {#id66 .section}
##### Deprecations[¶](#id66 "Permalink to this heading"){.headerlink}

-   The [`FEED_FORMAT`{.docutils .literal .notranslate}]{.pre} and
    [`FEED_URI`{.docutils .literal .notranslate}]{.pre} settings have
    been deprecated in favor of the new [[`FEEDS`{.xref .std
    .std-setting .docutils .literal
    .notranslate}]{.pre}](index.html#std-setting-FEEDS){.hoverxref
    .tooltip .reference .internal} setting ([issue
    1336](https://github.com/scrapy/scrapy/issues/1336){.reference
    .external}, [issue
    3858](https://github.com/scrapy/scrapy/issues/3858){.reference
    .external}, [issue
    4507](https://github.com/scrapy/scrapy/issues/4507){.reference
    .external})
:::

::: {#id67 .section}
##### New features[¶](#id67 "Permalink to this heading"){.headerlink}

-   A new setting, [[`FEEDS`{.xref .std .std-setting .docutils .literal
    .notranslate}]{.pre}](index.html#std-setting-FEEDS){.hoverxref
    .tooltip .reference .internal}, allows configuring multiple output
    feeds with different settings each ([issue
    1336](https://github.com/scrapy/scrapy/issues/1336){.reference
    .external}, [issue
    3858](https://github.com/scrapy/scrapy/issues/3858){.reference
    .external}, [issue
    4507](https://github.com/scrapy/scrapy/issues/4507){.reference
    .external})

-   The [[`crawl`{.xref .std .std-command .docutils .literal
    .notranslate}]{.pre}](index.html#std-command-crawl){.hoverxref
    .tooltip .reference .internal} and [[`runspider`{.xref .std
    .std-command .docutils .literal
    .notranslate}]{.pre}](index.html#std-command-runspider){.hoverxref
    .tooltip .reference .internal} commands now support multiple
    [`-o`{.docutils .literal .notranslate}]{.pre} parameters ([issue
    1336](https://github.com/scrapy/scrapy/issues/1336){.reference
    .external}, [issue
    3858](https://github.com/scrapy/scrapy/issues/3858){.reference
    .external}, [issue
    4507](https://github.com/scrapy/scrapy/issues/4507){.reference
    .external})

-   The [[`crawl`{.xref .std .std-command .docutils .literal
    .notranslate}]{.pre}](index.html#std-command-crawl){.hoverxref
    .tooltip .reference .internal} and [[`runspider`{.xref .std
    .std-command .docutils .literal
    .notranslate}]{.pre}](index.html#std-command-runspider){.hoverxref
    .tooltip .reference .internal} commands now support specifying an
    output format by appending [`:<format>`{.docutils .literal
    .notranslate}]{.pre} to the output file ([issue
    1336](https://github.com/scrapy/scrapy/issues/1336){.reference
    .external}, [issue
    3858](https://github.com/scrapy/scrapy/issues/3858){.reference
    .external}, [issue
    4507](https://github.com/scrapy/scrapy/issues/4507){.reference
    .external})

-   The new [[`Response.ip_address`{.xref .py .py-attr .docutils
    .literal
    .notranslate}]{.pre}](index.html#scrapy.http.Response.ip_address "scrapy.http.Response.ip_address"){.reference
    .internal} attribute gives access to the IP address that originated
    a response ([issue
    3903](https://github.com/scrapy/scrapy/issues/3903){.reference
    .external}, [issue
    3940](https://github.com/scrapy/scrapy/issues/3940){.reference
    .external})

-   A warning is now issued when a value in [`allowed_domains`{.xref .py
    .py-attr .docutils .literal .notranslate}]{.pre} includes a port
    ([issue 50](https://github.com/scrapy/scrapy/issues/50){.reference
    .external}, [issue
    3198](https://github.com/scrapy/scrapy/issues/3198){.reference
    .external}, [issue
    4413](https://github.com/scrapy/scrapy/issues/4413){.reference
    .external})

-   Zsh completion now excludes used option aliases from the completion
    list ([issue
    4438](https://github.com/scrapy/scrapy/issues/4438){.reference
    .external})
:::

::: {#id68 .section}
##### Bug fixes[¶](#id68 "Permalink to this heading"){.headerlink}

-   [[Request serialization]{.std
    .std-ref}](index.html#request-serialization){.hoverxref .tooltip
    .reference .internal} no longer breaks for callbacks that are spider
    attributes which are assigned a function with a different name
    ([issue
    4500](https://github.com/scrapy/scrapy/issues/4500){.reference
    .external})

-   [`None`{.docutils .literal .notranslate}]{.pre} values in
    [`allowed_domains`{.xref .py .py-attr .docutils .literal
    .notranslate}]{.pre} no longer cause a [[`TypeError`{.xref .py
    .py-exc .docutils .literal
    .notranslate}]{.pre}](https://docs.python.org/3/library/exceptions.html#TypeError "(in Python v3.12)"){.reference
    .external} exception ([issue
    4410](https://github.com/scrapy/scrapy/issues/4410){.reference
    .external})

-   Zsh completion no longer allows options after arguments ([issue
    4438](https://github.com/scrapy/scrapy/issues/4438){.reference
    .external})

-   zope.interface 5.0.0 and later versions are now supported ([issue
    4447](https://github.com/scrapy/scrapy/issues/4447){.reference
    .external}, [issue
    4448](https://github.com/scrapy/scrapy/issues/4448){.reference
    .external})

-   [`Spider.make_requests_from_url`{.docutils .literal
    .notranslate}]{.pre}, deprecated in Scrapy 1.4.0, now issues a
    warning when used ([issue
    4412](https://github.com/scrapy/scrapy/issues/4412){.reference
    .external})
:::

::: {#id69 .section}
##### Documentation[¶](#id69 "Permalink to this heading"){.headerlink}

-   Improved the documentation about signals that allow their handlers
    to return a [[`Deferred`{.xref .py .py-class .docutils .literal
    .notranslate}]{.pre}](https://docs.twisted.org/en/stable/api/twisted.internet.defer.Deferred.html "(in Twisted)"){.reference
    .external} ([issue
    4295](https://github.com/scrapy/scrapy/issues/4295){.reference
    .external}, [issue
    4390](https://github.com/scrapy/scrapy/issues/4390){.reference
    .external})

-   Our PyPI entry now includes links for our documentation, our source
    code repository and our issue tracker ([issue
    4456](https://github.com/scrapy/scrapy/issues/4456){.reference
    .external})

-   Covered the
    [curl2scrapy](https://michael-shub.github.io/curl2scrapy/){.reference
    .external} service in the documentation ([issue
    4206](https://github.com/scrapy/scrapy/issues/4206){.reference
    .external}, [issue
    4455](https://github.com/scrapy/scrapy/issues/4455){.reference
    .external})

-   Removed references to the Guppy library, which only works in Python
    2 ([issue
    4285](https://github.com/scrapy/scrapy/issues/4285){.reference
    .external}, [issue
    4343](https://github.com/scrapy/scrapy/issues/4343){.reference
    .external})

-   Extended use of InterSphinx to link to Python 3 documentation
    ([issue
    4444](https://github.com/scrapy/scrapy/issues/4444){.reference
    .external}, [issue
    4445](https://github.com/scrapy/scrapy/issues/4445){.reference
    .external})

-   Added support for Sphinx 3.0 and later ([issue
    4475](https://github.com/scrapy/scrapy/issues/4475){.reference
    .external}, [issue
    4480](https://github.com/scrapy/scrapy/issues/4480){.reference
    .external}, [issue
    4496](https://github.com/scrapy/scrapy/issues/4496){.reference
    .external}, [issue
    4503](https://github.com/scrapy/scrapy/issues/4503){.reference
    .external})
:::

::: {#id70 .section}
##### Quality assurance[¶](#id70 "Permalink to this heading"){.headerlink}

-   Removed warnings about using old, removed settings ([issue
    4404](https://github.com/scrapy/scrapy/issues/4404){.reference
    .external})

-   Removed a warning about importing [[`StringTransport`{.xref .py
    .py-class .docutils .literal
    .notranslate}]{.pre}](https://docs.twisted.org/en/stable/api/twisted.internet.testing.StringTransport.html "(in Twisted)"){.reference
    .external} from [`twisted.test.proto_helpers`{.docutils .literal
    .notranslate}]{.pre} in Twisted 19.7.0 or newer ([issue
    4409](https://github.com/scrapy/scrapy/issues/4409){.reference
    .external})

-   Removed outdated Debian package build files ([issue
    4384](https://github.com/scrapy/scrapy/issues/4384){.reference
    .external})

-   Removed [[`object`{.xref .py .py-class .docutils .literal
    .notranslate}]{.pre}](https://docs.python.org/3/library/functions.html#object "(in Python v3.12)"){.reference
    .external} usage as a base class ([issue
    4430](https://github.com/scrapy/scrapy/issues/4430){.reference
    .external})

-   Removed code that added support for old versions of Twisted that we
    no longer support ([issue
    4472](https://github.com/scrapy/scrapy/issues/4472){.reference
    .external})

-   Fixed code style issues ([issue
    4468](https://github.com/scrapy/scrapy/issues/4468){.reference
    .external}, [issue
    4469](https://github.com/scrapy/scrapy/issues/4469){.reference
    .external}, [issue
    4471](https://github.com/scrapy/scrapy/issues/4471){.reference
    .external}, [issue
    4481](https://github.com/scrapy/scrapy/issues/4481){.reference
    .external})

-   Removed [[`twisted.internet.defer.returnValue()`{.xref .py .py-func
    .docutils .literal
    .notranslate}]{.pre}](https://docs.twisted.org/en/stable/api/twisted.internet.defer.html#returnValue "(in Twisted)"){.reference
    .external} calls ([issue
    4443](https://github.com/scrapy/scrapy/issues/4443){.reference
    .external}, [issue
    4446](https://github.com/scrapy/scrapy/issues/4446){.reference
    .external}, [issue
    4489](https://github.com/scrapy/scrapy/issues/4489){.reference
    .external})
:::
:::

::: {#scrapy-2-0-1-2020-03-18 .section}
[]{#release-2-0-1}

#### Scrapy 2.0.1 (2020-03-18)[¶](#scrapy-2-0-1-2020-03-18 "Permalink to this heading"){.headerlink}

-   [[`Response.follow_all`{.xref .py .py-meth .docutils .literal
    .notranslate}]{.pre}](index.html#scrapy.http.Response.follow_all "scrapy.http.Response.follow_all"){.reference
    .internal} now supports an empty URL iterable as input ([issue
    4408](https://github.com/scrapy/scrapy/issues/4408){.reference
    .external}, [issue
    4420](https://github.com/scrapy/scrapy/issues/4420){.reference
    .external})

-   Removed top-level [[`reactor`{.xref .py .py-mod .docutils .literal
    .notranslate}]{.pre}](https://docs.twisted.org/en/stable/api/twisted.internet.reactor.html "(in Twisted)"){.reference
    .external} imports to prevent errors about the wrong Twisted reactor
    being installed when setting a different Twisted reactor using
    [[`TWISTED_REACTOR`{.xref .std .std-setting .docutils .literal
    .notranslate}]{.pre}](index.html#std-setting-TWISTED_REACTOR){.hoverxref
    .tooltip .reference .internal} ([issue
    4401](https://github.com/scrapy/scrapy/issues/4401){.reference
    .external}, [issue
    4406](https://github.com/scrapy/scrapy/issues/4406){.reference
    .external})

-   Fixed tests ([issue
    4422](https://github.com/scrapy/scrapy/issues/4422){.reference
    .external})
:::

::: {#scrapy-2-0-0-2020-03-03 .section}
[]{#release-2-0-0}

#### Scrapy 2.0.0 (2020-03-03)[¶](#scrapy-2-0-0-2020-03-03 "Permalink to this heading"){.headerlink}

Highlights:

-   Python 2 support has been removed

-   [[Partial]{.doc}](index.html#document-topics/coroutines){.reference
    .internal} [[coroutine syntax]{.xref .std
    .std-ref}](https://docs.python.org/3/reference/compound_stmts.html#async "(in Python v3.12)"){.reference
    .external} support and
    [[experimental]{.doc}](index.html#document-topics/asyncio){.reference
    .internal} [[`asyncio`{.xref .py .py-mod .docutils .literal
    .notranslate}]{.pre}](https://docs.python.org/3/library/asyncio.html#module-asyncio "(in Python v3.12)"){.reference
    .external} support

-   New [[`Response.follow_all`{.xref .py .py-meth .docutils .literal
    .notranslate}]{.pre}](index.html#scrapy.http.Response.follow_all "scrapy.http.Response.follow_all"){.reference
    .internal} method

-   [[FTP support]{.std
    .std-ref}](index.html#media-pipeline-ftp){.hoverxref .tooltip
    .reference .internal} for media pipelines

-   New [[`Response.certificate`{.xref .py .py-attr .docutils .literal
    .notranslate}]{.pre}](index.html#scrapy.http.Response.certificate "scrapy.http.Response.certificate"){.reference
    .internal} attribute

-   IPv6 support through [[`DNS_RESOLVER`{.xref .std .std-setting
    .docutils .literal
    .notranslate}]{.pre}](index.html#std-setting-DNS_RESOLVER){.hoverxref
    .tooltip .reference .internal}

::: {#id71 .section}
##### Backward-incompatible changes[¶](#id71 "Permalink to this heading"){.headerlink}

-   Python 2 support has been removed, following [Python 2 end-of-life
    on January 1,
    2020](https://www.python.org/doc/sunset-python-2/){.reference
    .external} ([issue
    4091](https://github.com/scrapy/scrapy/issues/4091){.reference
    .external}, [issue
    4114](https://github.com/scrapy/scrapy/issues/4114){.reference
    .external}, [issue
    4115](https://github.com/scrapy/scrapy/issues/4115){.reference
    .external}, [issue
    4121](https://github.com/scrapy/scrapy/issues/4121){.reference
    .external}, [issue
    4138](https://github.com/scrapy/scrapy/issues/4138){.reference
    .external}, [issue
    4231](https://github.com/scrapy/scrapy/issues/4231){.reference
    .external}, [issue
    4242](https://github.com/scrapy/scrapy/issues/4242){.reference
    .external}, [issue
    4304](https://github.com/scrapy/scrapy/issues/4304){.reference
    .external}, [issue
    4309](https://github.com/scrapy/scrapy/issues/4309){.reference
    .external}, [issue
    4373](https://github.com/scrapy/scrapy/issues/4373){.reference
    .external})

-   Retry gaveups (see [[`RETRY_TIMES`{.xref .std .std-setting .docutils
    .literal
    .notranslate}]{.pre}](index.html#std-setting-RETRY_TIMES){.hoverxref
    .tooltip .reference .internal}) are now logged as errors instead of
    as debug information ([issue
    3171](https://github.com/scrapy/scrapy/issues/3171){.reference
    .external}, [issue
    3566](https://github.com/scrapy/scrapy/issues/3566){.reference
    .external})

-   File extensions that [[`LinkExtractor`{.xref .py .py-class .docutils
    .literal
    .notranslate}]{.pre}](index.html#scrapy.linkextractors.lxmlhtml.LxmlLinkExtractor "scrapy.linkextractors.lxmlhtml.LxmlLinkExtractor"){.reference
    .internal} ignores by default now also include [`7z`{.docutils
    .literal .notranslate}]{.pre}, [`7zip`{.docutils .literal
    .notranslate}]{.pre}, [`apk`{.docutils .literal
    .notranslate}]{.pre}, [`bz2`{.docutils .literal
    .notranslate}]{.pre}, [`cdr`{.docutils .literal
    .notranslate}]{.pre}, [`dmg`{.docutils .literal
    .notranslate}]{.pre}, [`ico`{.docutils .literal
    .notranslate}]{.pre}, [`iso`{.docutils .literal
    .notranslate}]{.pre}, [`tar`{.docutils .literal
    .notranslate}]{.pre}, [`tar.gz`{.docutils .literal
    .notranslate}]{.pre}, [`webm`{.docutils .literal
    .notranslate}]{.pre}, and [`xz`{.docutils .literal
    .notranslate}]{.pre} ([issue
    1837](https://github.com/scrapy/scrapy/issues/1837){.reference
    .external}, [issue
    2067](https://github.com/scrapy/scrapy/issues/2067){.reference
    .external}, [issue
    4066](https://github.com/scrapy/scrapy/issues/4066){.reference
    .external})

-   The [[`METAREFRESH_IGNORE_TAGS`{.xref .std .std-setting .docutils
    .literal
    .notranslate}]{.pre}](index.html#std-setting-METAREFRESH_IGNORE_TAGS){.hoverxref
    .tooltip .reference .internal} setting is now an empty list by
    default, following web browser behavior ([issue
    3844](https://github.com/scrapy/scrapy/issues/3844){.reference
    .external}, [issue
    4311](https://github.com/scrapy/scrapy/issues/4311){.reference
    .external})

-   The [[`HttpCompressionMiddleware`{.xref .py .py-class .docutils
    .literal
    .notranslate}]{.pre}](index.html#scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware "scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware"){.reference
    .internal} now includes spaces after commas in the value of the
    [`Accept-Encoding`{.docutils .literal .notranslate}]{.pre} header
    that it sets, following web browser behavior ([issue
    4293](https://github.com/scrapy/scrapy/issues/4293){.reference
    .external})

-   The [`__init__`{.docutils .literal .notranslate}]{.pre} method of
    custom download handlers (see [[`DOWNLOAD_HANDLERS`{.xref .std
    .std-setting .docutils .literal
    .notranslate}]{.pre}](index.html#std-setting-DOWNLOAD_HANDLERS){.hoverxref
    .tooltip .reference .internal}) or subclasses of the following
    downloader handlers no longer receives a [`settings`{.docutils
    .literal .notranslate}]{.pre} parameter:

    -   [`scrapy.core.downloader.handlers.datauri.DataURIDownloadHandler`{.xref
        .py .py-class .docutils .literal .notranslate}]{.pre}

    -   [`scrapy.core.downloader.handlers.file.FileDownloadHandler`{.xref
        .py .py-class .docutils .literal .notranslate}]{.pre}

    Use the [`from_settings`{.docutils .literal .notranslate}]{.pre} or
    [`from_crawler`{.docutils .literal .notranslate}]{.pre} class
    methods to expose such a parameter to your custom download handlers.

    ([issue
    4126](https://github.com/scrapy/scrapy/issues/4126){.reference
    .external})

-   We have refactored the [[`scrapy.core.scheduler.Scheduler`{.xref .py
    .py-class .docutils .literal
    .notranslate}]{.pre}](index.html#scrapy.core.scheduler.Scheduler "scrapy.core.scheduler.Scheduler"){.reference
    .internal} class and related queue classes (see
    [[`SCHEDULER_PRIORITY_QUEUE`{.xref .std .std-setting .docutils
    .literal
    .notranslate}]{.pre}](index.html#std-setting-SCHEDULER_PRIORITY_QUEUE){.hoverxref
    .tooltip .reference .internal}, [[`SCHEDULER_DISK_QUEUE`{.xref .std
    .std-setting .docutils .literal
    .notranslate}]{.pre}](index.html#std-setting-SCHEDULER_DISK_QUEUE){.hoverxref
    .tooltip .reference .internal} and [[`SCHEDULER_MEMORY_QUEUE`{.xref
    .std .std-setting .docutils .literal
    .notranslate}]{.pre}](index.html#std-setting-SCHEDULER_MEMORY_QUEUE){.hoverxref
    .tooltip .reference .internal}) to make it easier to implement
    custom scheduler queue classes. See [[Changes to scheduler queue
    classes]{.std .std-ref}](#scheduler-queue-changes){.hoverxref
    .tooltip .reference .internal} below for details.

-   Overridden settings are now logged in a different format. This is
    more in line with similar information logged at startup ([issue
    4199](https://github.com/scrapy/scrapy/issues/4199){.reference
    .external})
:::

::: {#id72 .section}
##### Deprecation removals[¶](#id72 "Permalink to this heading"){.headerlink}

-   The [[Scrapy shell]{.std
    .std-ref}](index.html#topics-shell){.hoverxref .tooltip .reference
    .internal} no longer provides a sel proxy object, use
    [`response.selector`{.xref .py .py-meth .docutils .literal
    .notranslate}]{.pre} instead ([issue
    4347](https://github.com/scrapy/scrapy/issues/4347){.reference
    .external})

-   LevelDB support has been removed ([issue
    4112](https://github.com/scrapy/scrapy/issues/4112){.reference
    .external})

-   The following functions have been removed from
    [`scrapy.utils.python`{.xref .py .py-mod .docutils .literal
    .notranslate}]{.pre}: [`isbinarytext`{.docutils .literal
    .notranslate}]{.pre}, [`is_writable`{.docutils .literal
    .notranslate}]{.pre}, [`setattr_default`{.docutils .literal
    .notranslate}]{.pre}, [`stringify_dict`{.docutils .literal
    .notranslate}]{.pre} ([issue
    4362](https://github.com/scrapy/scrapy/issues/4362){.reference
    .external})
:::

::: {#id73 .section}
##### Deprecations[¶](#id73 "Permalink to this heading"){.headerlink}

-   Using environment variables prefixed with [`SCRAPY_`{.docutils
    .literal .notranslate}]{.pre} to override settings is deprecated
    ([issue
    4300](https://github.com/scrapy/scrapy/issues/4300){.reference
    .external}, [issue
    4374](https://github.com/scrapy/scrapy/issues/4374){.reference
    .external}, [issue
    4375](https://github.com/scrapy/scrapy/issues/4375){.reference
    .external})

-   [`scrapy.linkextractors.FilteringLinkExtractor`{.xref .py .py-class
    .docutils .literal .notranslate}]{.pre} is deprecated, use
    [[`scrapy.linkextractors.LinkExtractor`{.xref .py .py-class
    .docutils .literal
    .notranslate}]{.pre}](index.html#scrapy.linkextractors.lxmlhtml.LxmlLinkExtractor "scrapy.linkextractors.lxmlhtml.LxmlLinkExtractor"){.reference
    .internal} instead ([issue
    4045](https://github.com/scrapy/scrapy/issues/4045){.reference
    .external})

-   The [`noconnect`{.docutils .literal .notranslate}]{.pre} query
    string argument of proxy URLs is deprecated and should be removed
    from proxy URLs ([issue
    4198](https://github.com/scrapy/scrapy/issues/4198){.reference
    .external})

-   The [`next`{.xref .py .py-meth .docutils .literal
    .notranslate}]{.pre} method of
    [`scrapy.utils.python.MutableChain`{.xref .py .py-class .docutils
    .literal .notranslate}]{.pre} is deprecated, use the global
    [[`next()`{.xref .py .py-func .docutils .literal
    .notranslate}]{.pre}](https://docs.python.org/3/library/functions.html#next "(in Python v3.12)"){.reference
    .external} function or [`MutableChain.__next__`{.xref .py .py-meth
    .docutils .literal .notranslate}]{.pre} instead ([issue
    4153](https://github.com/scrapy/scrapy/issues/4153){.reference
    .external})
:::

::: {#id74 .section}
##### New features[¶](#id74 "Permalink to this heading"){.headerlink}

-   Added [[partial
    support]{.doc}](index.html#document-topics/coroutines){.reference
    .internal} for Python's [[coroutine syntax]{.xref .std
    .std-ref}](https://docs.python.org/3/reference/compound_stmts.html#async "(in Python v3.12)"){.reference
    .external} and [[experimental
    support]{.doc}](index.html#document-topics/asyncio){.reference
    .internal} for [[`asyncio`{.xref .py .py-mod .docutils .literal
    .notranslate}]{.pre}](https://docs.python.org/3/library/asyncio.html#module-asyncio "(in Python v3.12)"){.reference
    .external} and [[`asyncio`{.xref .py .py-mod .docutils .literal
    .notranslate}]{.pre}](https://docs.python.org/3/library/asyncio.html#module-asyncio "(in Python v3.12)"){.reference
    .external}-powered libraries ([issue
    4010](https://github.com/scrapy/scrapy/issues/4010){.reference
    .external}, [issue
    4259](https://github.com/scrapy/scrapy/issues/4259){.reference
    .external}, [issue
    4269](https://github.com/scrapy/scrapy/issues/4269){.reference
    .external}, [issue
    4270](https://github.com/scrapy/scrapy/issues/4270){.reference
    .external}, [issue
    4271](https://github.com/scrapy/scrapy/issues/4271){.reference
    .external}, [issue
    4316](https://github.com/scrapy/scrapy/issues/4316){.reference
    .external}, [issue
    4318](https://github.com/scrapy/scrapy/issues/4318){.reference
    .external})

-   The new [[`Response.follow_all`{.xref .py .py-meth .docutils
    .literal
    .notranslate}]{.pre}](index.html#scrapy.http.Response.follow_all "scrapy.http.Response.follow_all"){.reference
    .internal} method offers the same functionality as
    [[`Response.follow`{.xref .py .py-meth .docutils .literal
    .notranslate}]{.pre}](index.html#scrapy.http.Response.follow "scrapy.http.Response.follow"){.reference
    .internal} but supports an iterable of URLs as input and returns an
    iterable of requests ([issue
    2582](https://github.com/scrapy/scrapy/issues/2582){.reference
    .external}, [issue
    4057](https://github.com/scrapy/scrapy/issues/4057){.reference
    .external}, [issue
    4286](https://github.com/scrapy/scrapy/issues/4286){.reference
    .external})

-   [[Media pipelines]{.std
    .std-ref}](index.html#topics-media-pipeline){.hoverxref .tooltip
    .reference .internal} now support [[FTP storage]{.std
    .std-ref}](index.html#media-pipeline-ftp){.hoverxref .tooltip
    .reference .internal} ([issue
    3928](https://github.com/scrapy/scrapy/issues/3928){.reference
    .external}, [issue
    3961](https://github.com/scrapy/scrapy/issues/3961){.reference
    .external})

-   The new [[`Response.certificate`{.xref .py .py-attr .docutils
    .literal
    .notranslate}]{.pre}](index.html#scrapy.http.Response.certificate "scrapy.http.Response.certificate"){.reference
    .internal} attribute exposes the SSL certificate of the server as a
    [[`twisted.internet.ssl.Certificate`{.xref .py .py-class .docutils
    .literal
    .notranslate}]{.pre}](https://docs.twisted.org/en/stable/api/twisted.internet.ssl.Certificate.html "(in Twisted)"){.reference
    .external} object for HTTPS responses ([issue
    2726](https://github.com/scrapy/scrapy/issues/2726){.reference
    .external}, [issue
    4054](https://github.com/scrapy/scrapy/issues/4054){.reference
    .external})

-   A new [[`DNS_RESOLVER`{.xref .std .std-setting .docutils .literal
    .notranslate}]{.pre}](index.html#std-setting-DNS_RESOLVER){.hoverxref
    .tooltip .reference .internal} setting allows enabling IPv6 support
    ([issue
    1031](https://github.com/scrapy/scrapy/issues/1031){.reference
    .external}, [issue
    4227](https://github.com/scrapy/scrapy/issues/4227){.reference
    .external})

-   A new [[`SCRAPER_SLOT_MAX_ACTIVE_SIZE`{.xref .std .std-setting
    .docutils .literal
    .notranslate}]{.pre}](index.html#std-setting-SCRAPER_SLOT_MAX_ACTIVE_SIZE){.hoverxref
    .tooltip .reference .internal} setting allows configuring the
    existing soft limit that pauses request downloads when the total
    response data being processed is too high ([issue
    1410](https://github.com/scrapy/scrapy/issues/1410){.reference
    .external}, [issue
    3551](https://github.com/scrapy/scrapy/issues/3551){.reference
    .external})

-   A new [[`TWISTED_REACTOR`{.xref .std .std-setting .docutils .literal
    .notranslate}]{.pre}](index.html#std-setting-TWISTED_REACTOR){.hoverxref
    .tooltip .reference .internal} setting allows customizing the
    [[`reactor`{.xref .py .py-mod .docutils .literal
    .notranslate}]{.pre}](https://docs.twisted.org/en/stable/api/twisted.internet.reactor.html "(in Twisted)"){.reference
    .external} that Scrapy uses, allowing to [[enable asyncio
    support]{.doc}](index.html#document-topics/asyncio){.reference
    .internal} or deal with a [[common macOS issue]{.std
    .std-ref}](index.html#faq-specific-reactor){.hoverxref .tooltip
    .reference .internal} ([issue
    2905](https://github.com/scrapy/scrapy/issues/2905){.reference
    .external}, [issue
    4294](https://github.com/scrapy/scrapy/issues/4294){.reference
    .external})

-   Scheduler disk and memory queues may now use the class methods
    [`from_crawler`{.docutils .literal .notranslate}]{.pre} or
    [`from_settings`{.docutils .literal .notranslate}]{.pre} ([issue
    3884](https://github.com/scrapy/scrapy/issues/3884){.reference
    .external})

-   The new [[`Response.cb_kwargs`{.xref .py .py-attr .docutils .literal
    .notranslate}]{.pre}](index.html#scrapy.http.Response.cb_kwargs "scrapy.http.Response.cb_kwargs"){.reference
    .internal} attribute serves as a shortcut for
    [[`Response.request.cb_kwargs`{.xref .py .py-attr .docutils .literal
    .notranslate}]{.pre}](index.html#scrapy.http.Request.cb_kwargs "scrapy.http.Request.cb_kwargs"){.reference
    .internal} ([issue
    4331](https://github.com/scrapy/scrapy/issues/4331){.reference
    .external})

-   [[`Response.follow`{.xref .py .py-meth .docutils .literal
    .notranslate}]{.pre}](index.html#scrapy.http.Response.follow "scrapy.http.Response.follow"){.reference
    .internal} now supports a [`flags`{.docutils .literal
    .notranslate}]{.pre} parameter, for consistency with
    [[`Request`{.xref .py .py-class .docutils .literal
    .notranslate}]{.pre}](index.html#scrapy.http.Request "scrapy.http.Request"){.reference
    .internal} ([issue
    4277](https://github.com/scrapy/scrapy/issues/4277){.reference
    .external}, [issue
    4279](https://github.com/scrapy/scrapy/issues/4279){.reference
    .external})

-   [[Item loader processors]{.std
    .std-ref}](index.html#topics-loaders-processors){.hoverxref .tooltip
    .reference .internal} can now be regular functions, they no longer
    need to be methods ([issue
    3899](https://github.com/scrapy/scrapy/issues/3899){.reference
    .external})

-   [[`Rule`{.xref .py .py-class .docutils .literal
    .notranslate}]{.pre}](index.html#scrapy.spiders.Rule "scrapy.spiders.Rule"){.reference
    .internal} now accepts an [`errback`{.docutils .literal
    .notranslate}]{.pre} parameter ([issue
    4000](https://github.com/scrapy/scrapy/issues/4000){.reference
    .external})

-   [[`Request`{.xref .py .py-class .docutils .literal
    .notranslate}]{.pre}](index.html#scrapy.http.Request "scrapy.http.Request"){.reference
    .internal} no longer requires a [`callback`{.docutils .literal
    .notranslate}]{.pre} parameter when an [`errback`{.docutils .literal
    .notranslate}]{.pre} parameter is specified ([issue
    3586](https://github.com/scrapy/scrapy/issues/3586){.reference
    .external}, [issue
    4008](https://github.com/scrapy/scrapy/issues/4008){.reference
    .external})

-   [[`LogFormatter`{.xref .py .py-class .docutils .literal
    .notranslate}]{.pre}](index.html#scrapy.logformatter.LogFormatter "scrapy.logformatter.LogFormatter"){.reference
    .internal} now supports some additional methods:

    -   [[`download_error`{.xref .py .py-class .docutils .literal
        .notranslate}]{.pre}](index.html#scrapy.logformatter.LogFormatter.download_error "scrapy.logformatter.LogFormatter.download_error"){.reference
        .internal} for download errors

    -   [[`item_error`{.xref .py .py-class .docutils .literal
        .notranslate}]{.pre}](index.html#scrapy.logformatter.LogFormatter.item_error "scrapy.logformatter.LogFormatter.item_error"){.reference
        .internal} for exceptions raised during item processing by
        [[item pipelines]{.std
        .std-ref}](index.html#topics-item-pipeline){.hoverxref .tooltip
        .reference .internal}

    -   [[`spider_error`{.xref .py .py-class .docutils .literal
        .notranslate}]{.pre}](index.html#scrapy.logformatter.LogFormatter.spider_error "scrapy.logformatter.LogFormatter.spider_error"){.reference
        .internal} for exceptions raised from [[spider callbacks]{.std
        .std-ref}](index.html#topics-spiders){.hoverxref .tooltip
        .reference .internal}

    ([issue 374](https://github.com/scrapy/scrapy/issues/374){.reference
    .external}, [issue
    3986](https://github.com/scrapy/scrapy/issues/3986){.reference
    .external}, [issue
    3989](https://github.com/scrapy/scrapy/issues/3989){.reference
    .external}, [issue
    4176](https://github.com/scrapy/scrapy/issues/4176){.reference
    .external}, [issue
    4188](https://github.com/scrapy/scrapy/issues/4188){.reference
    .external})

-   The [`FEED_URI`{.xref .std .std-setting .docutils .literal
    .notranslate}]{.pre} setting now supports [[`pathlib.Path`{.xref .py
    .py-class .docutils .literal
    .notranslate}]{.pre}](https://docs.python.org/3/library/pathlib.html#pathlib.Path "(in Python v3.12)"){.reference
    .external} values ([issue
    3731](https://github.com/scrapy/scrapy/issues/3731){.reference
    .external}, [issue
    4074](https://github.com/scrapy/scrapy/issues/4074){.reference
    .external})

-   A new [[`request_left_downloader`{.xref .std .std-signal .docutils
    .literal
    .notranslate}]{.pre}](index.html#std-signal-request_left_downloader){.hoverxref
    .tooltip .reference .internal} signal is sent when a request leaves
    the downloader ([issue
    4303](https://github.com/scrapy/scrapy/issues/4303){.reference
    .external})

-   Scrapy logs a warning when it detects a request callback or errback
    that uses [`yield`{.docutils .literal .notranslate}]{.pre} but also
    returns a value, since the returned value would be lost ([issue
    3484](https://github.com/scrapy/scrapy/issues/3484){.reference
    .external}, [issue
    3869](https://github.com/scrapy/scrapy/issues/3869){.reference
    .external})

-   [[`Spider`{.xref .py .py-class .docutils .literal
    .notranslate}]{.pre}](index.html#scrapy.spiders.Spider "scrapy.spiders.Spider"){.reference
    .internal} objects now raise an [[`AttributeError`{.xref .py .py-exc
    .docutils .literal
    .notranslate}]{.pre}](https://docs.python.org/3/library/exceptions.html#AttributeError "(in Python v3.12)"){.reference
    .external} exception if they do not have a [`start_urls`{.xref .py
    .py-class .docutils .literal .notranslate}]{.pre} attribute nor
    reimplement [`start_requests`{.xref .py .py-class .docutils .literal
    .notranslate}]{.pre}, but have a [`start_url`{.docutils .literal
    .notranslate}]{.pre} attribute ([issue
    4133](https://github.com/scrapy/scrapy/issues/4133){.reference
    .external}, [issue
    4170](https://github.com/scrapy/scrapy/issues/4170){.reference
    .external})

-   [[`BaseItemExporter`{.xref .py .py-class .docutils .literal
    .notranslate}]{.pre}](index.html#scrapy.exporters.BaseItemExporter "scrapy.exporters.BaseItemExporter"){.reference
    .internal} subclasses may now use
    [`super().__init__(**kwargs)`{.docutils .literal
    .notranslate}]{.pre} instead of [`self._configure(kwargs)`{.docutils
    .literal .notranslate}]{.pre} in their [`__init__`{.docutils
    .literal .notranslate}]{.pre} method, passing
    [`dont_fail=True`{.docutils .literal .notranslate}]{.pre} to the
    parent [`__init__`{.docutils .literal .notranslate}]{.pre} method if
    needed, and accessing [`kwargs`{.docutils .literal
    .notranslate}]{.pre} at [`self._kwargs`{.docutils .literal
    .notranslate}]{.pre} after calling their parent
    [`__init__`{.docutils .literal .notranslate}]{.pre} method ([issue
    4193](https://github.com/scrapy/scrapy/issues/4193){.reference
    .external}, [issue
    4370](https://github.com/scrapy/scrapy/issues/4370){.reference
    .external})

-   A new [`keep_fragments`{.docutils .literal .notranslate}]{.pre}
    parameter of [`scrapy.utils.request.request_fingerprint`{.docutils
    .literal .notranslate}]{.pre} allows to generate different
    fingerprints for requests with different fragments in their URL
    ([issue
    4104](https://github.com/scrapy/scrapy/issues/4104){.reference
    .external})

-   Download handlers (see [[`DOWNLOAD_HANDLERS`{.xref .std .std-setting
    .docutils .literal
    .notranslate}]{.pre}](index.html#std-setting-DOWNLOAD_HANDLERS){.hoverxref
    .tooltip .reference .internal}) may now use the
    [`from_settings`{.docutils .literal .notranslate}]{.pre} and
    [`from_crawler`{.docutils .literal .notranslate}]{.pre} class
    methods that other Scrapy components already supported ([issue
    4126](https://github.com/scrapy/scrapy/issues/4126){.reference
    .external})

-   [`scrapy.utils.python.MutableChain.__iter__`{.xref .py .py-class
    .docutils .literal .notranslate}]{.pre} now returns
    [`self`{.docutils .literal .notranslate}]{.pre}, [allowing it to be
    used as a sequence](https://lgtm.com/rules/4850080/){.reference
    .external} ([issue
    4153](https://github.com/scrapy/scrapy/issues/4153){.reference
    .external})
:::

::: {#id75 .section}
##### Bug fixes[¶](#id75 "Permalink to this heading"){.headerlink}

-   The [[`crawl`{.xref .std .std-command .docutils .literal
    .notranslate}]{.pre}](index.html#std-command-crawl){.hoverxref
    .tooltip .reference .internal} command now also exits with exit code
    1 when an exception happens before the crawling starts ([issue
    4175](https://github.com/scrapy/scrapy/issues/4175){.reference
    .external}, [issue
    4207](https://github.com/scrapy/scrapy/issues/4207){.reference
    .external})

-   [[`LinkExtractor.extract_links`{.xref .py .py-class .docutils
    .literal
    .notranslate}]{.pre}](index.html#scrapy.linkextractors.lxmlhtml.LxmlLinkExtractor.extract_links "scrapy.linkextractors.lxmlhtml.LxmlLinkExtractor.extract_links"){.reference
    .internal} no longer re-encodes the query string or URLs from
    non-UTF-8 responses in UTF-8 ([issue
    998](https://github.com/scrapy/scrapy/issues/998){.reference
    .external}, [issue
    1403](https://github.com/scrapy/scrapy/issues/1403){.reference
    .external}, [issue
    1949](https://github.com/scrapy/scrapy/issues/1949){.reference
    .external}, [issue
    4321](https://github.com/scrapy/scrapy/issues/4321){.reference
    .external})

-   The first spider middleware (see [[`SPIDER_MIDDLEWARES`{.xref .std
    .std-setting .docutils .literal
    .notranslate}]{.pre}](index.html#std-setting-SPIDER_MIDDLEWARES){.hoverxref
    .tooltip .reference .internal}) now also processes exceptions raised
    from callbacks that are generators ([issue
    4260](https://github.com/scrapy/scrapy/issues/4260){.reference
    .external}, [issue
    4272](https://github.com/scrapy/scrapy/issues/4272){.reference
    .external})

-   Redirects to URLs starting with 3 slashes ([`///`{.docutils .literal
    .notranslate}]{.pre}) are now supported ([issue
    4032](https://github.com/scrapy/scrapy/issues/4032){.reference
    .external}, [issue
    4042](https://github.com/scrapy/scrapy/issues/4042){.reference
    .external})

-   [[`Request`{.xref .py .py-class .docutils .literal
    .notranslate}]{.pre}](index.html#scrapy.http.Request "scrapy.http.Request"){.reference
    .internal} no longer accepts strings as [`url`{.docutils .literal
    .notranslate}]{.pre} simply because they have a colon ([issue
    2552](https://github.com/scrapy/scrapy/issues/2552){.reference
    .external}, [issue
    4094](https://github.com/scrapy/scrapy/issues/4094){.reference
    .external})

-   The correct encoding is now used for attach names in
    [[`MailSender`{.xref .py .py-class .docutils .literal
    .notranslate}]{.pre}](index.html#scrapy.mail.MailSender "scrapy.mail.MailSender"){.reference
    .internal} ([issue
    4229](https://github.com/scrapy/scrapy/issues/4229){.reference
    .external}, [issue
    4239](https://github.com/scrapy/scrapy/issues/4239){.reference
    .external})

-   [`RFPDupeFilter`{.xref .py .py-class .docutils .literal
    .notranslate}]{.pre}, the default [[`DUPEFILTER_CLASS`{.xref .std
    .std-setting .docutils .literal
    .notranslate}]{.pre}](index.html#std-setting-DUPEFILTER_CLASS){.hoverxref
    .tooltip .reference .internal}, no longer writes an extra
    [`\r`{.docutils .literal .notranslate}]{.pre} character on each line
    in Windows, which made the size of the [`requests.seen`{.docutils
    .literal .notranslate}]{.pre} file unnecessarily large on that
    platform ([issue
    4283](https://github.com/scrapy/scrapy/issues/4283){.reference
    .external})

-   Z shell auto-completion now looks for [`.html`{.docutils .literal
    .notranslate}]{.pre} files, not [`.http`{.docutils .literal
    .notranslate}]{.pre} files, and covers the [`-h`{.docutils .literal
    .notranslate}]{.pre} command-line switch ([issue
    4122](https://github.com/scrapy/scrapy/issues/4122){.reference
    .external}, [issue
    4291](https://github.com/scrapy/scrapy/issues/4291){.reference
    .external})

-   Adding items to a [`scrapy.utils.datatypes.LocalCache`{.xref .py
    .py-class .docutils .literal .notranslate}]{.pre} object without a
    [`limit`{.docutils .literal .notranslate}]{.pre} defined no longer
    raises a [[`TypeError`{.xref .py .py-exc .docutils .literal
    .notranslate}]{.pre}](https://docs.python.org/3/library/exceptions.html#TypeError "(in Python v3.12)"){.reference
    .external} exception ([issue
    4123](https://github.com/scrapy/scrapy/issues/4123){.reference
    .external})

-   Fixed a typo in the message of the [[`ValueError`{.xref .py .py-exc
    .docutils .literal
    .notranslate}]{.pre}](https://docs.python.org/3/library/exceptions.html#ValueError "(in Python v3.12)"){.reference
    .external} exception raised when
    [`scrapy.utils.misc.create_instance()`{.xref .py .py-func .docutils
    .literal .notranslate}]{.pre} gets both [`settings`{.docutils
    .literal .notranslate}]{.pre} and [`crawler`{.docutils .literal
    .notranslate}]{.pre} set to [`None`{.docutils .literal
    .notranslate}]{.pre} ([issue
    4128](https://github.com/scrapy/scrapy/issues/4128){.reference
    .external})
:::

::: {#id76 .section}
##### Documentation[¶](#id76 "Permalink to this heading"){.headerlink}

-   API documentation now links to an online, syntax-highlighted view of
    the corresponding source code ([issue
    4148](https://github.com/scrapy/scrapy/issues/4148){.reference
    .external})

-   Links to unexisting documentation pages now allow access to the
    sidebar ([issue
    4152](https://github.com/scrapy/scrapy/issues/4152){.reference
    .external}, [issue
    4169](https://github.com/scrapy/scrapy/issues/4169){.reference
    .external})

-   Cross-references within our documentation now display a tooltip when
    hovered ([issue
    4173](https://github.com/scrapy/scrapy/issues/4173){.reference
    .external}, [issue
    4183](https://github.com/scrapy/scrapy/issues/4183){.reference
    .external})

-   Improved the documentation about
    [[`LinkExtractor.extract_links`{.xref .py .py-meth .docutils
    .literal
    .notranslate}]{.pre}](index.html#scrapy.linkextractors.lxmlhtml.LxmlLinkExtractor.extract_links "scrapy.linkextractors.lxmlhtml.LxmlLinkExtractor.extract_links"){.reference
    .internal} and simplified [[Link Extractors]{.std
    .std-ref}](index.html#topics-link-extractors){.hoverxref .tooltip
    .reference .internal} ([issue
    4045](https://github.com/scrapy/scrapy/issues/4045){.reference
    .external})

-   Clarified how [[`ItemLoader.item`{.xref .py .py-class .docutils
    .literal
    .notranslate}]{.pre}](index.html#scrapy.loader.ItemLoader.item "scrapy.loader.ItemLoader.item"){.reference
    .internal} works ([issue
    3574](https://github.com/scrapy/scrapy/issues/3574){.reference
    .external}, [issue
    4099](https://github.com/scrapy/scrapy/issues/4099){.reference
    .external})

-   Clarified that [[`logging.basicConfig()`{.xref .py .py-func
    .docutils .literal
    .notranslate}]{.pre}](https://docs.python.org/3/library/logging.html#logging.basicConfig "(in Python v3.12)"){.reference
    .external} should not be used when also using
    [[`CrawlerProcess`{.xref .py .py-class .docutils .literal
    .notranslate}]{.pre}](index.html#scrapy.crawler.CrawlerProcess "scrapy.crawler.CrawlerProcess"){.reference
    .internal} ([issue
    2149](https://github.com/scrapy/scrapy/issues/2149){.reference
    .external}, [issue
    2352](https://github.com/scrapy/scrapy/issues/2352){.reference
    .external}, [issue
    3146](https://github.com/scrapy/scrapy/issues/3146){.reference
    .external}, [issue
    3960](https://github.com/scrapy/scrapy/issues/3960){.reference
    .external})

-   Clarified the requirements for [[`Request`{.xref .py .py-class
    .docutils .literal
    .notranslate}]{.pre}](index.html#scrapy.http.Request "scrapy.http.Request"){.reference
    .internal} objects [[when using persistence]{.std
    .std-ref}](index.html#request-serialization){.hoverxref .tooltip
    .reference .internal} ([issue
    4124](https://github.com/scrapy/scrapy/issues/4124){.reference
    .external}, [issue
    4139](https://github.com/scrapy/scrapy/issues/4139){.reference
    .external})

-   Clarified how to install a [[custom image pipeline]{.std
    .std-ref}](index.html#media-pipeline-example){.hoverxref .tooltip
    .reference .internal} ([issue
    4034](https://github.com/scrapy/scrapy/issues/4034){.reference
    .external}, [issue
    4252](https://github.com/scrapy/scrapy/issues/4252){.reference
    .external})

-   Fixed the signatures of the [`file_path`{.docutils .literal
    .notranslate}]{.pre} method in [[media pipeline]{.std
    .std-ref}](index.html#topics-media-pipeline){.hoverxref .tooltip
    .reference .internal} examples ([issue
    4290](https://github.com/scrapy/scrapy/issues/4290){.reference
    .external})

-   Covered a backward-incompatible change in Scrapy 1.7.0 affecting
    custom [[`scrapy.core.scheduler.Scheduler`{.xref .py .py-class
    .docutils .literal
    .notranslate}]{.pre}](index.html#scrapy.core.scheduler.Scheduler "scrapy.core.scheduler.Scheduler"){.reference
    .internal} subclasses ([issue
    4274](https://github.com/scrapy/scrapy/issues/4274){.reference
    .external})

-   Improved the [`README.rst`{.docutils .literal .notranslate}]{.pre}
    and [`CODE_OF_CONDUCT.md`{.docutils .literal .notranslate}]{.pre}
    files ([issue
    4059](https://github.com/scrapy/scrapy/issues/4059){.reference
    .external})

-   Documentation examples are now checked as part of our test suite and
    we have fixed some of the issues detected ([issue
    4142](https://github.com/scrapy/scrapy/issues/4142){.reference
    .external}, [issue
    4146](https://github.com/scrapy/scrapy/issues/4146){.reference
    .external}, [issue
    4171](https://github.com/scrapy/scrapy/issues/4171){.reference
    .external}, [issue
    4184](https://github.com/scrapy/scrapy/issues/4184){.reference
    .external}, [issue
    4190](https://github.com/scrapy/scrapy/issues/4190){.reference
    .external})

-   Fixed logic issues, broken links and typos ([issue
    4247](https://github.com/scrapy/scrapy/issues/4247){.reference
    .external}, [issue
    4258](https://github.com/scrapy/scrapy/issues/4258){.reference
    .external}, [issue
    4282](https://github.com/scrapy/scrapy/issues/4282){.reference
    .external}, [issue
    4288](https://github.com/scrapy/scrapy/issues/4288){.reference
    .external}, [issue
    4305](https://github.com/scrapy/scrapy/issues/4305){.reference
    .external}, [issue
    4308](https://github.com/scrapy/scrapy/issues/4308){.reference
    .external}, [issue
    4323](https://github.com/scrapy/scrapy/issues/4323){.reference
    .external}, [issue
    4338](https://github.com/scrapy/scrapy/issues/4338){.reference
    .external}, [issue
    4359](https://github.com/scrapy/scrapy/issues/4359){.reference
    .external}, [issue
    4361](https://github.com/scrapy/scrapy/issues/4361){.reference
    .external})

-   Improved consistency when referring to the [`__init__`{.docutils
    .literal .notranslate}]{.pre} method of an object ([issue
    4086](https://github.com/scrapy/scrapy/issues/4086){.reference
    .external}, [issue
    4088](https://github.com/scrapy/scrapy/issues/4088){.reference
    .external})

-   Fixed an inconsistency between code and output in [[Scrapy at a
    glance]{.std .std-ref}](index.html#intro-overview){.hoverxref
    .tooltip .reference .internal} ([issue
    4213](https://github.com/scrapy/scrapy/issues/4213){.reference
    .external})

-   Extended [[`intersphinx`{.xref .py .py-mod .docutils .literal
    .notranslate}]{.pre}](https://www.sphinx-doc.org/en/master/usage/extensions/intersphinx.html#module-sphinx.ext.intersphinx "(in Sphinx v7.3.0)"){.reference
    .external} usage ([issue
    4147](https://github.com/scrapy/scrapy/issues/4147){.reference
    .external}, [issue
    4172](https://github.com/scrapy/scrapy/issues/4172){.reference
    .external}, [issue
    4185](https://github.com/scrapy/scrapy/issues/4185){.reference
    .external}, [issue
    4194](https://github.com/scrapy/scrapy/issues/4194){.reference
    .external}, [issue
    4197](https://github.com/scrapy/scrapy/issues/4197){.reference
    .external})

-   We now use a recent version of Python to build the documentation
    ([issue
    4140](https://github.com/scrapy/scrapy/issues/4140){.reference
    .external}, [issue
    4249](https://github.com/scrapy/scrapy/issues/4249){.reference
    .external})

-   Cleaned up documentation ([issue
    4143](https://github.com/scrapy/scrapy/issues/4143){.reference
    .external}, [issue
    4275](https://github.com/scrapy/scrapy/issues/4275){.reference
    .external})
:::

::: {#id77 .section}
##### Quality assurance[¶](#id77 "Permalink to this heading"){.headerlink}

-   Re-enabled proxy [`CONNECT`{.docutils .literal .notranslate}]{.pre}
    tests ([issue
    2545](https://github.com/scrapy/scrapy/issues/2545){.reference
    .external}, [issue
    4114](https://github.com/scrapy/scrapy/issues/4114){.reference
    .external})

-   Added [Bandit](https://bandit.readthedocs.io/){.reference .external}
    security checks to our test suite ([issue
    4162](https://github.com/scrapy/scrapy/issues/4162){.reference
    .external}, [issue
    4181](https://github.com/scrapy/scrapy/issues/4181){.reference
    .external})

-   Added [Flake8](https://flake8.pycqa.org/en/latest/){.reference
    .external} style checks to our test suite and applied many of the
    corresponding changes ([issue
    3944](https://github.com/scrapy/scrapy/issues/3944){.reference
    .external}, [issue
    3945](https://github.com/scrapy/scrapy/issues/3945){.reference
    .external}, [issue
    4137](https://github.com/scrapy/scrapy/issues/4137){.reference
    .external}, [issue
    4157](https://github.com/scrapy/scrapy/issues/4157){.reference
    .external}, [issue
    4167](https://github.com/scrapy/scrapy/issues/4167){.reference
    .external}, [issue
    4174](https://github.com/scrapy/scrapy/issues/4174){.reference
    .external}, [issue
    4186](https://github.com/scrapy/scrapy/issues/4186){.reference
    .external}, [issue
    4195](https://github.com/scrapy/scrapy/issues/4195){.reference
    .external}, [issue
    4238](https://github.com/scrapy/scrapy/issues/4238){.reference
    .external}, [issue
    4246](https://github.com/scrapy/scrapy/issues/4246){.reference
    .external}, [issue
    4355](https://github.com/scrapy/scrapy/issues/4355){.reference
    .external}, [issue
    4360](https://github.com/scrapy/scrapy/issues/4360){.reference
    .external}, [issue
    4365](https://github.com/scrapy/scrapy/issues/4365){.reference
    .external})

-   Improved test coverage ([issue
    4097](https://github.com/scrapy/scrapy/issues/4097){.reference
    .external}, [issue
    4218](https://github.com/scrapy/scrapy/issues/4218){.reference
    .external}, [issue
    4236](https://github.com/scrapy/scrapy/issues/4236){.reference
    .external})

-   Started reporting slowest tests, and improved the performance of
    some of them ([issue
    4163](https://github.com/scrapy/scrapy/issues/4163){.reference
    .external}, [issue
    4164](https://github.com/scrapy/scrapy/issues/4164){.reference
    .external})

-   Fixed broken tests and refactored some tests ([issue
    4014](https://github.com/scrapy/scrapy/issues/4014){.reference
    .external}, [issue
    4095](https://github.com/scrapy/scrapy/issues/4095){.reference
    .external}, [issue
    4244](https://github.com/scrapy/scrapy/issues/4244){.reference
    .external}, [issue
    4268](https://github.com/scrapy/scrapy/issues/4268){.reference
    .external}, [issue
    4372](https://github.com/scrapy/scrapy/issues/4372){.reference
    .external})

-   Modified the [[tox]{.xref .std
    .std-doc}](https://tox.wiki/en/latest/index.html "(in Python v4.11)"){.reference
    .external} configuration to allow running tests with any Python
    version, run [Bandit](https://bandit.readthedocs.io/){.reference
    .external} and
    [Flake8](https://flake8.pycqa.org/en/latest/){.reference .external}
    tests by default, and enforce a minimum tox version programmatically
    ([issue
    4179](https://github.com/scrapy/scrapy/issues/4179){.reference
    .external})

-   Cleaned up code ([issue
    3937](https://github.com/scrapy/scrapy/issues/3937){.reference
    .external}, [issue
    4208](https://github.com/scrapy/scrapy/issues/4208){.reference
    .external}, [issue
    4209](https://github.com/scrapy/scrapy/issues/4209){.reference
    .external}, [issue
    4210](https://github.com/scrapy/scrapy/issues/4210){.reference
    .external}, [issue
    4212](https://github.com/scrapy/scrapy/issues/4212){.reference
    .external}, [issue
    4369](https://github.com/scrapy/scrapy/issues/4369){.reference
    .external}, [issue
    4376](https://github.com/scrapy/scrapy/issues/4376){.reference
    .external}, [issue
    4378](https://github.com/scrapy/scrapy/issues/4378){.reference
    .external})
:::

::: {#changes-to-scheduler-queue-classes .section}
[]{#scheduler-queue-changes}

##### Changes to scheduler queue classes[¶](#changes-to-scheduler-queue-classes "Permalink to this heading"){.headerlink}

The following changes may impact any custom queue classes of all types:

-   The [`push`{.docutils .literal .notranslate}]{.pre} method no longer
    receives a second positional parameter containing
    [`request.priority`{.docutils .literal
    .notranslate}]{.pre}` `{.docutils .literal
    .notranslate}[`*`{.docutils .literal
    .notranslate}]{.pre}` `{.docutils .literal
    .notranslate}[`-1`{.docutils .literal .notranslate}]{.pre}. If you
    need that value, get it from the first positional parameter,
    [`request`{.docutils .literal .notranslate}]{.pre}, instead, or use
    the new [`priority()`{.xref .py .py-meth .docutils .literal
    .notranslate}]{.pre} method in
    [`scrapy.core.scheduler.ScrapyPriorityQueue`{.xref .py .py-class
    .docutils .literal .notranslate}]{.pre} subclasses.

The following changes may impact custom priority queue classes:

-   In the [`__init__`{.docutils .literal .notranslate}]{.pre} method or
    the [`from_crawler`{.docutils .literal .notranslate}]{.pre} or
    [`from_settings`{.docutils .literal .notranslate}]{.pre} class
    methods:

    -   The parameter that used to contain a factory function,
        [`qfactory`{.docutils .literal .notranslate}]{.pre}, is now
        passed as a keyword parameter named
        [`downstream_queue_cls`{.docutils .literal .notranslate}]{.pre}.

    -   A new keyword parameter has been added: [`key`{.docutils
        .literal .notranslate}]{.pre}. It is a string that is always an
        empty string for memory queues and indicates the
        [`JOB_DIR`{.xref .std .std-setting .docutils .literal
        .notranslate}]{.pre} value for disk queues.

    -   The parameter for disk queues that contains data from the
        previous crawl, [`startprios`{.docutils .literal
        .notranslate}]{.pre} or [`slot_startprios`{.docutils .literal
        .notranslate}]{.pre}, is now passed as a keyword parameter named
        [`startprios`{.docutils .literal .notranslate}]{.pre}.

    -   The [`serialize`{.docutils .literal .notranslate}]{.pre}
        parameter is no longer passed. The disk queue class must take
        care of request serialization on its own before writing to disk,
        using the [`request_to_dict()`{.xref .py .py-func .docutils
        .literal .notranslate}]{.pre} and [`request_from_dict()`{.xref
        .py .py-func .docutils .literal .notranslate}]{.pre} functions
        from the [`scrapy.utils.reqser`{.xref .py .py-mod .docutils
        .literal .notranslate}]{.pre} module.

The following changes may impact custom disk and memory queue classes:

-   The signature of the [`__init__`{.docutils .literal
    .notranslate}]{.pre} method is now [`__init__(self,`{.docutils
    .literal .notranslate}]{.pre}` `{.docutils .literal
    .notranslate}[`crawler,`{.docutils .literal
    .notranslate}]{.pre}` `{.docutils .literal
    .notranslate}[`key)`{.docutils .literal .notranslate}]{.pre}.

The following changes affect specifically the
[`ScrapyPriorityQueue`{.xref .py .py-class .docutils .literal
.notranslate}]{.pre} and [`DownloaderAwarePriorityQueue`{.xref .py
.py-class .docutils .literal .notranslate}]{.pre} classes from
[[`scrapy.core.scheduler`{.xref .py .py-mod .docutils .literal
.notranslate}]{.pre}](index.html#module-scrapy.core.scheduler "scrapy.core.scheduler"){.reference
.internal} and may affect subclasses:

-   In the [`__init__`{.docutils .literal .notranslate}]{.pre} method,
    most of the changes described above apply.

    [`__init__`{.docutils .literal .notranslate}]{.pre} may still
    receive all parameters as positional parameters, however:

    -   [`downstream_queue_cls`{.docutils .literal .notranslate}]{.pre},
        which replaced [`qfactory`{.docutils .literal
        .notranslate}]{.pre}, must be instantiated differently.

        [`qfactory`{.docutils .literal .notranslate}]{.pre} was
        instantiated with a priority value (integer).

        Instances of [`downstream_queue_cls`{.docutils .literal
        .notranslate}]{.pre} should be created using the new
        [`ScrapyPriorityQueue.qfactory`{.xref .py .py-meth .docutils
        .literal .notranslate}]{.pre} or
        [`DownloaderAwarePriorityQueue.pqfactory`{.xref .py .py-meth
        .docutils .literal .notranslate}]{.pre} methods.

    -   The new [`key`{.docutils .literal .notranslate}]{.pre} parameter
        displaced the [`startprios`{.docutils .literal
        .notranslate}]{.pre} parameter 1 position to the right.

-   The following class attributes have been added:

    -   [`crawler`{.xref .py .py-attr .docutils .literal
        .notranslate}]{.pre}

    -   [`downstream_queue_cls`{.xref .py .py-attr .docutils .literal
        .notranslate}]{.pre} (details above)

    -   [`key`{.xref .py .py-attr .docutils .literal
        .notranslate}]{.pre} (details above)

-   The [`serialize`{.docutils .literal .notranslate}]{.pre} attribute
    has been removed (details above)

The following changes affect specifically the
[`ScrapyPriorityQueue`{.xref .py .py-class .docutils .literal
.notranslate}]{.pre} class and may affect subclasses:

-   A new [`priority()`{.xref .py .py-meth .docutils .literal
    .notranslate}]{.pre} method has been added which, given a request,
    returns [`request.priority`{.docutils .literal
    .notranslate}]{.pre}` `{.docutils .literal
    .notranslate}[`*`{.docutils .literal
    .notranslate}]{.pre}` `{.docutils .literal
    .notranslate}[`-1`{.docutils .literal .notranslate}]{.pre}.

    It is used in [`push()`{.xref .py .py-meth .docutils .literal
    .notranslate}]{.pre} to make up for the removal of its
    [`priority`{.docutils .literal .notranslate}]{.pre} parameter.

-   The [`spider`{.docutils .literal .notranslate}]{.pre} attribute has
    been removed. Use [`crawler.spider`{.xref .py .py-attr .docutils
    .literal .notranslate}]{.pre} instead.

The following changes affect specifically the
[`DownloaderAwarePriorityQueue`{.xref .py .py-class .docutils .literal
.notranslate}]{.pre} class and may affect subclasses:

-   A new [`pqueues`{.xref .py .py-attr .docutils .literal
    .notranslate}]{.pre} attribute offers a mapping of downloader slot
    names to the corresponding instances of
    [`downstream_queue_cls`{.xref .py .py-attr .docutils .literal
    .notranslate}]{.pre}.

([issue 3884](https://github.com/scrapy/scrapy/issues/3884){.reference
.external})
:::
:::

::: {#scrapy-1-8-3-2022-07-25 .section}
[]{#release-1-8-3}

#### Scrapy 1.8.3 (2022-07-25)[¶](#scrapy-1-8-3-2022-07-25 "Permalink to this heading"){.headerlink}

**Security bug fix:**

-   When [[`HttpProxyMiddleware`{.xref .py .py-class .docutils .literal
    .notranslate}]{.pre}](index.html#scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware "scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware"){.reference
    .internal} processes a request with [[`proxy`{.xref .std
    .std-reqmeta .docutils .literal
    .notranslate}]{.pre}](index.html#std-reqmeta-proxy){.hoverxref
    .tooltip .reference .internal} metadata, and that [[`proxy`{.xref
    .std .std-reqmeta .docutils .literal
    .notranslate}]{.pre}](index.html#std-reqmeta-proxy){.hoverxref
    .tooltip .reference .internal} metadata includes proxy credentials,
    [[`HttpProxyMiddleware`{.xref .py .py-class .docutils .literal
    .notranslate}]{.pre}](index.html#scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware "scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware"){.reference
    .internal} sets the [`Proxy-Authorization`{.docutils .literal
    .notranslate}]{.pre} header, but only if that header is not already
    set.

    There are third-party proxy-rotation downloader middlewares that set
    different [[`proxy`{.xref .std .std-reqmeta .docutils .literal
    .notranslate}]{.pre}](index.html#std-reqmeta-proxy){.hoverxref
    .tooltip .reference .internal} metadata every time they process a
    request.

    Because of request retries and redirects, the same request can be
    processed by downloader middlewares more than once, including both
    [[`HttpProxyMiddleware`{.xref .py .py-class .docutils .literal
    .notranslate}]{.pre}](index.html#scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware "scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware"){.reference
    .internal} and any third-party proxy-rotation downloader middleware.

    These third-party proxy-rotation downloader middlewares could change
    the [[`proxy`{.xref .std .std-reqmeta .docutils .literal
    .notranslate}]{.pre}](index.html#std-reqmeta-proxy){.hoverxref
    .tooltip .reference .internal} metadata of a request to a new value,
    but fail to remove the [`Proxy-Authorization`{.docutils .literal
    .notranslate}]{.pre} header from the previous value of the
    [[`proxy`{.xref .std .std-reqmeta .docutils .literal
    .notranslate}]{.pre}](index.html#std-reqmeta-proxy){.hoverxref
    .tooltip .reference .internal} metadata, causing the credentials of
    one proxy to be sent to a different proxy.

    To prevent the unintended leaking of proxy credentials, the behavior
    of [[`HttpProxyMiddleware`{.xref .py .py-class .docutils .literal
    .notranslate}]{.pre}](index.html#scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware "scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware"){.reference
    .internal} is now as follows when processing a request:

    -   If the request being processed defines [[`proxy`{.xref .std
        .std-reqmeta .docutils .literal
        .notranslate}]{.pre}](index.html#std-reqmeta-proxy){.hoverxref
        .tooltip .reference .internal} metadata that includes
        credentials, the [`Proxy-Authorization`{.docutils .literal
        .notranslate}]{.pre} header is always updated to feature those
        credentials.

    -   If the request being processed defines [[`proxy`{.xref .std
        .std-reqmeta .docutils .literal
        .notranslate}]{.pre}](index.html#std-reqmeta-proxy){.hoverxref
        .tooltip .reference .internal} metadata without credentials, the
        [`Proxy-Authorization`{.docutils .literal .notranslate}]{.pre}
        header is removed *unless* it was originally defined for the
        same proxy URL.

        To remove proxy credentials while keeping the same proxy URL,
        remove the [`Proxy-Authorization`{.docutils .literal
        .notranslate}]{.pre} header.

    -   If the request has no [[`proxy`{.xref .std .std-reqmeta
        .docutils .literal
        .notranslate}]{.pre}](index.html#std-reqmeta-proxy){.hoverxref
        .tooltip .reference .internal} metadata, or that metadata is a
        falsy value (e.g. [`None`{.docutils .literal
        .notranslate}]{.pre}), the [`Proxy-Authorization`{.docutils
        .literal .notranslate}]{.pre} header is removed.

        It is no longer possible to set a proxy URL through the
        [[`proxy`{.xref .std .std-reqmeta .docutils .literal
        .notranslate}]{.pre}](index.html#std-reqmeta-proxy){.hoverxref
        .tooltip .reference .internal} metadata but set the credentials
        through the [`Proxy-Authorization`{.docutils .literal
        .notranslate}]{.pre} header. Set proxy credentials through the
        [[`proxy`{.xref .std .std-reqmeta .docutils .literal
        .notranslate}]{.pre}](index.html#std-reqmeta-proxy){.hoverxref
        .tooltip .reference .internal} metadata instead.
:::

::: {#scrapy-1-8-2-2022-03-01 .section}
[]{#release-1-8-2}

#### Scrapy 1.8.2 (2022-03-01)[¶](#scrapy-1-8-2-2022-03-01 "Permalink to this heading"){.headerlink}

**Security bug fixes:**

-   When a [[`Request`{.xref .py .py-class .docutils .literal
    .notranslate}]{.pre}](index.html#scrapy.http.Request "scrapy.http.Request"){.reference
    .internal} object with cookies defined gets a redirect response
    causing a new [[`Request`{.xref .py .py-class .docutils .literal
    .notranslate}]{.pre}](index.html#scrapy.http.Request "scrapy.http.Request"){.reference
    .internal} object to be scheduled, the cookies defined in the
    original [[`Request`{.xref .py .py-class .docutils .literal
    .notranslate}]{.pre}](index.html#scrapy.http.Request "scrapy.http.Request"){.reference
    .internal} object are no longer copied into the new
    [[`Request`{.xref .py .py-class .docutils .literal
    .notranslate}]{.pre}](index.html#scrapy.http.Request "scrapy.http.Request"){.reference
    .internal} object.

    If you manually set the [`Cookie`{.docutils .literal
    .notranslate}]{.pre} header on a [[`Request`{.xref .py .py-class
    .docutils .literal
    .notranslate}]{.pre}](index.html#scrapy.http.Request "scrapy.http.Request"){.reference
    .internal} object and the domain name of the redirect URL is not an
    exact match for the domain of the URL of the original
    [[`Request`{.xref .py .py-class .docutils .literal
    .notranslate}]{.pre}](index.html#scrapy.http.Request "scrapy.http.Request"){.reference
    .internal} object, your [`Cookie`{.docutils .literal
    .notranslate}]{.pre} header is now dropped from the new
    [[`Request`{.xref .py .py-class .docutils .literal
    .notranslate}]{.pre}](index.html#scrapy.http.Request "scrapy.http.Request"){.reference
    .internal} object.

    The old behavior could be exploited by an attacker to gain access to
    your cookies. Please, see the [cjvr-mfj7-j4j8 security
    advisory](https://github.com/scrapy/scrapy/security/advisories/GHSA-cjvr-mfj7-j4j8){.reference
    .external} for more information.

    ::: {.admonition .note}
    Note

    It is still possible to enable the sharing of cookies between
    different domains with a shared domain suffix (e.g.
    [`example.com`{.docutils .literal .notranslate}]{.pre} and any
    subdomain) by defining the shared domain suffix (e.g.
    [`example.com`{.docutils .literal .notranslate}]{.pre}) as the
    cookie domain when defining your cookies. See the documentation of
    the [[`Request`{.xref .py .py-class .docutils .literal
    .notranslate}]{.pre}](index.html#scrapy.http.Request "scrapy.http.Request"){.reference
    .internal} class for more information.
    :::

-   When the domain of a cookie, either received in the
    [`Set-Cookie`{.docutils .literal .notranslate}]{.pre} header of a
    response or defined in a [[`Request`{.xref .py .py-class .docutils
    .literal
    .notranslate}]{.pre}](index.html#scrapy.http.Request "scrapy.http.Request"){.reference
    .internal} object, is set to a [public
    suffix](https://publicsuffix.org/){.reference .external}, the cookie
    is now ignored unless the cookie domain is the same as the request
    domain.

    The old behavior could be exploited by an attacker to inject cookies
    into your requests to some other domains. Please, see the
    [mfjm-vh54-3f96 security
    advisory](https://github.com/scrapy/scrapy/security/advisories/GHSA-mfjm-vh54-3f96){.reference
    .external} for more information.
:::

::: {#scrapy-1-8-1-2021-10-05 .section}
[]{#release-1-8-1}

#### Scrapy 1.8.1 (2021-10-05)[¶](#scrapy-1-8-1-2021-10-05 "Permalink to this heading"){.headerlink}

-   **Security bug fix:**

    If you use [[`HttpAuthMiddleware`{.xref .py .py-class .docutils
    .literal
    .notranslate}]{.pre}](index.html#scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware "scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware"){.reference
    .internal} (i.e. the [`http_user`{.docutils .literal
    .notranslate}]{.pre} and [`http_pass`{.docutils .literal
    .notranslate}]{.pre} spider attributes) for HTTP authentication, any
    request exposes your credentials to the request target.

    To prevent unintended exposure of authentication credentials to
    unintended domains, you must now additionally set a new, additional
    spider attribute, [`http_auth_domain`{.docutils .literal
    .notranslate}]{.pre}, and point it to the specific domain to which
    the authentication credentials must be sent.

    If the [`http_auth_domain`{.docutils .literal .notranslate}]{.pre}
    spider attribute is not set, the domain of the first request will be
    considered the HTTP authentication target, and authentication
    credentials will only be sent in requests targeting that domain.

    If you need to send the same HTTP authentication credentials to
    multiple domains, you can use
    [[`w3lib.http.basic_auth_header()`{.xref .py .py-func .docutils
    .literal
    .notranslate}]{.pre}](https://w3lib.readthedocs.io/en/latest/w3lib.html#w3lib.http.basic_auth_header "(in w3lib v2.1)"){.reference
    .external} instead to set the value of the
    [`Authorization`{.docutils .literal .notranslate}]{.pre} header of
    your requests.

    If you *really* want your spider to send the same HTTP
    authentication credentials to any domain, set the
    [`http_auth_domain`{.docutils .literal .notranslate}]{.pre} spider
    attribute to [`None`{.docutils .literal .notranslate}]{.pre}.

    Finally, if you are a user of
    [scrapy-splash](https://github.com/scrapy-plugins/scrapy-splash){.reference
    .external}, know that this version of Scrapy breaks compatibility
    with scrapy-splash 0.7.2 and earlier. You will need to upgrade
    scrapy-splash to a greater version for it to continue to work.
:::

::: {#scrapy-1-8-0-2019-10-28 .section}
[]{#release-1-8-0}

#### Scrapy 1.8.0 (2019-10-28)[¶](#scrapy-1-8-0-2019-10-28 "Permalink to this heading"){.headerlink}

Highlights:

-   Dropped Python 3.4 support and updated minimum requirements; made
    Python 3.8 support official

-   New [[`Request.from_curl`{.xref .py .py-meth .docutils .literal
    .notranslate}]{.pre}](index.html#scrapy.http.Request.from_curl "scrapy.http.Request.from_curl"){.reference
    .internal} class method

-   New [[`ROBOTSTXT_PARSER`{.xref .std .std-setting .docutils .literal
    .notranslate}]{.pre}](index.html#std-setting-ROBOTSTXT_PARSER){.hoverxref
    .tooltip .reference .internal} and [[`ROBOTSTXT_USER_AGENT`{.xref
    .std .std-setting .docutils .literal
    .notranslate}]{.pre}](index.html#std-setting-ROBOTSTXT_USER_AGENT){.hoverxref
    .tooltip .reference .internal} settings

-   New [[`DOWNLOADER_CLIENT_TLS_CIPHERS`{.xref .std .std-setting
    .docutils .literal
    .notranslate}]{.pre}](index.html#std-setting-DOWNLOADER_CLIENT_TLS_CIPHERS){.hoverxref
    .tooltip .reference .internal} and
    [[`DOWNLOADER_CLIENT_TLS_VERBOSE_LOGGING`{.xref .std .std-setting
    .docutils .literal
    .notranslate}]{.pre}](index.html#std-setting-DOWNLOADER_CLIENT_TLS_VERBOSE_LOGGING){.hoverxref
    .tooltip .reference .internal} settings

::: {#id82 .section}
##### Backward-incompatible changes[¶](#id82 "Permalink to this heading"){.headerlink}

-   Python 3.4 is no longer supported, and some of the minimum
    requirements of Scrapy have also changed:

    -   [[cssselect]{.xref .std
        .std-doc}](https://cssselect.readthedocs.io/en/latest/index.html "(in cssselect v1.2.0)"){.reference
        .external} 0.9.1

    -   [cryptography](https://cryptography.io/en/latest/){.reference
        .external} 2.0

    -   [lxml](https://lxml.de/){.reference .external} 3.5.0

    -   [pyOpenSSL](https://www.pyopenssl.org/en/stable/){.reference
        .external} 16.2.0

    -   [queuelib](https://github.com/scrapy/queuelib){.reference
        .external} 1.4.2

    -   [service_identity](https://service-identity.readthedocs.io/en/stable/){.reference
        .external} 16.0.0

    -   [six](https://six.readthedocs.io/){.reference .external} 1.10.0

    -   [Twisted](https://twistedmatrix.com/trac/){.reference .external}
        17.9.0 (16.0.0 with Python 2)

    -   [zope.interface](https://zopeinterface.readthedocs.io/en/latest/){.reference
        .external} 4.1.3

    ([issue
    3892](https://github.com/scrapy/scrapy/issues/3892){.reference
    .external})

-   [`JSONRequest`{.docutils .literal .notranslate}]{.pre} is now called
    [[`JsonRequest`{.xref .py .py-class .docutils .literal
    .notranslate}]{.pre}](index.html#scrapy.http.JsonRequest "scrapy.http.JsonRequest"){.reference
    .internal} for consistency with similar classes ([issue
    3929](https://github.com/scrapy/scrapy/issues/3929){.reference
    .external}, [issue
    3982](https://github.com/scrapy/scrapy/issues/3982){.reference
    .external})

-   If you are using a custom context factory
    ([[`DOWNLOADER_CLIENTCONTEXTFACTORY`{.xref .std .std-setting
    .docutils .literal
    .notranslate}]{.pre}](index.html#std-setting-DOWNLOADER_CLIENTCONTEXTFACTORY){.hoverxref
    .tooltip .reference .internal}), its [`__init__`{.docutils .literal
    .notranslate}]{.pre} method must accept two new parameters:
    [`tls_verbose_logging`{.docutils .literal .notranslate}]{.pre} and
    [`tls_ciphers`{.docutils .literal .notranslate}]{.pre} ([issue
    2111](https://github.com/scrapy/scrapy/issues/2111){.reference
    .external}, [issue
    3392](https://github.com/scrapy/scrapy/issues/3392){.reference
    .external}, [issue
    3442](https://github.com/scrapy/scrapy/issues/3442){.reference
    .external}, [issue
    3450](https://github.com/scrapy/scrapy/issues/3450){.reference
    .external})

-   [[`ItemLoader`{.xref .py .py-class .docutils .literal
    .notranslate}]{.pre}](index.html#scrapy.loader.ItemLoader "scrapy.loader.ItemLoader"){.reference
    .internal} now turns the values of its input item into lists:

    ::: {.highlight-pycon .notranslate}
    ::: highlight
        >>> item = MyItem()
        >>> item["field"] = "value1"
        >>> loader = ItemLoader(item=item)
        >>> item["field"]
        ['value1']
    :::
    :::

    This is needed to allow adding values to existing fields
    ([`loader.add_value('field',`{.docutils .literal
    .notranslate}]{.pre}` `{.docutils .literal
    .notranslate}[`'value2')`{.docutils .literal .notranslate}]{.pre}).

    ([issue
    3804](https://github.com/scrapy/scrapy/issues/3804){.reference
    .external}, [issue
    3819](https://github.com/scrapy/scrapy/issues/3819){.reference
    .external}, [issue
    3897](https://github.com/scrapy/scrapy/issues/3897){.reference
    .external}, [issue
    3976](https://github.com/scrapy/scrapy/issues/3976){.reference
    .external}, [issue
    3998](https://github.com/scrapy/scrapy/issues/3998){.reference
    .external}, [issue
    4036](https://github.com/scrapy/scrapy/issues/4036){.reference
    .external})

See also [[Deprecation removals]{.std .std-ref}](#id86){.hoverxref
.tooltip .reference .internal} below.
:::

::: {#id83 .section}
##### New features[¶](#id83 "Permalink to this heading"){.headerlink}

-   A new [[`Request.from_curl`{.xref .py .py-meth .docutils .literal
    .notranslate}]{.pre}](index.html#scrapy.http.Request.from_curl "scrapy.http.Request.from_curl"){.reference
    .internal} class method allows [[creating a request from a cURL
    command]{.std .std-ref}](index.html#requests-from-curl){.hoverxref
    .tooltip .reference .internal} ([issue
    2985](https://github.com/scrapy/scrapy/issues/2985){.reference
    .external}, [issue
    3862](https://github.com/scrapy/scrapy/issues/3862){.reference
    .external})

-   A new [[`ROBOTSTXT_PARSER`{.xref .std .std-setting .docutils
    .literal
    .notranslate}]{.pre}](index.html#std-setting-ROBOTSTXT_PARSER){.hoverxref
    .tooltip .reference .internal} setting allows choosing which
    [robots.txt](https://www.robotstxt.org/){.reference .external}
    parser to use. It includes built-in support for
    [[RobotFileParser]{.std
    .std-ref}](index.html#python-robotfileparser){.hoverxref .tooltip
    .reference .internal}, [[Protego]{.std
    .std-ref}](index.html#protego-parser){.hoverxref .tooltip .reference
    .internal} (default), [[Reppy]{.std
    .std-ref}](index.html#reppy-parser){.hoverxref .tooltip .reference
    .internal}, and [[Robotexclusionrulesparser]{.std
    .std-ref}](index.html#rerp-parser){.hoverxref .tooltip .reference
    .internal}, and allows you to [[implement support for additional
    parsers]{.std
    .std-ref}](index.html#support-for-new-robots-parser){.hoverxref
    .tooltip .reference .internal} ([issue
    754](https://github.com/scrapy/scrapy/issues/754){.reference
    .external}, [issue
    2669](https://github.com/scrapy/scrapy/issues/2669){.reference
    .external}, [issue
    3796](https://github.com/scrapy/scrapy/issues/3796){.reference
    .external}, [issue
    3935](https://github.com/scrapy/scrapy/issues/3935){.reference
    .external}, [issue
    3969](https://github.com/scrapy/scrapy/issues/3969){.reference
    .external}, [issue
    4006](https://github.com/scrapy/scrapy/issues/4006){.reference
    .external})

-   A new [[`ROBOTSTXT_USER_AGENT`{.xref .std .std-setting .docutils
    .literal
    .notranslate}]{.pre}](index.html#std-setting-ROBOTSTXT_USER_AGENT){.hoverxref
    .tooltip .reference .internal} setting allows defining a separate
    user agent string to use for
    [robots.txt](https://www.robotstxt.org/){.reference .external}
    parsing ([issue
    3931](https://github.com/scrapy/scrapy/issues/3931){.reference
    .external}, [issue
    3966](https://github.com/scrapy/scrapy/issues/3966){.reference
    .external})

-   [[`Rule`{.xref .py .py-class .docutils .literal
    .notranslate}]{.pre}](index.html#scrapy.spiders.Rule "scrapy.spiders.Rule"){.reference
    .internal} no longer requires a [[`LinkExtractor`{.xref .py
    .py-class .docutils .literal
    .notranslate}]{.pre}](index.html#scrapy.linkextractors.lxmlhtml.LxmlLinkExtractor "scrapy.linkextractors.lxmlhtml.LxmlLinkExtractor"){.reference
    .internal} parameter ([issue
    781](https://github.com/scrapy/scrapy/issues/781){.reference
    .external}, [issue
    4016](https://github.com/scrapy/scrapy/issues/4016){.reference
    .external})

-   Use the new [[`DOWNLOADER_CLIENT_TLS_CIPHERS`{.xref .std
    .std-setting .docutils .literal
    .notranslate}]{.pre}](index.html#std-setting-DOWNLOADER_CLIENT_TLS_CIPHERS){.hoverxref
    .tooltip .reference .internal} setting to customize the TLS/SSL
    ciphers used by the default HTTP/1.1 downloader ([issue
    3392](https://github.com/scrapy/scrapy/issues/3392){.reference
    .external}, [issue
    3442](https://github.com/scrapy/scrapy/issues/3442){.reference
    .external})

-   Set the new [[`DOWNLOADER_CLIENT_TLS_VERBOSE_LOGGING`{.xref .std
    .std-setting .docutils .literal
    .notranslate}]{.pre}](index.html#std-setting-DOWNLOADER_CLIENT_TLS_VERBOSE_LOGGING){.hoverxref
    .tooltip .reference .internal} setting to [`True`{.docutils .literal
    .notranslate}]{.pre} to enable debug-level messages about TLS
    connection parameters after establishing HTTPS connections ([issue
    2111](https://github.com/scrapy/scrapy/issues/2111){.reference
    .external}, [issue
    3450](https://github.com/scrapy/scrapy/issues/3450){.reference
    .external})

-   Callbacks that receive keyword arguments (see
    [[`Request.cb_kwargs`{.xref .py .py-attr .docutils .literal
    .notranslate}]{.pre}](index.html#scrapy.http.Request.cb_kwargs "scrapy.http.Request.cb_kwargs"){.reference
    .internal}) can now be tested using the new [[`@cb_kwargs`{.xref .py
    .py-class .docutils .literal
    .notranslate}]{.pre}](index.html#scrapy.contracts.default.CallbackKeywordArgumentsContract "scrapy.contracts.default.CallbackKeywordArgumentsContract"){.reference
    .internal} [[spider contract]{.std
    .std-ref}](index.html#topics-contracts){.hoverxref .tooltip
    .reference .internal} ([issue
    3985](https://github.com/scrapy/scrapy/issues/3985){.reference
    .external}, [issue
    3988](https://github.com/scrapy/scrapy/issues/3988){.reference
    .external})

-   When a [[`@scrapes`{.xref .py .py-class .docutils .literal
    .notranslate}]{.pre}](index.html#scrapy.contracts.default.ScrapesContract "scrapy.contracts.default.ScrapesContract"){.reference
    .internal} spider contract fails, all missing fields are now
    reported ([issue
    766](https://github.com/scrapy/scrapy/issues/766){.reference
    .external}, [issue
    3939](https://github.com/scrapy/scrapy/issues/3939){.reference
    .external})

-   [[Custom log formats]{.std
    .std-ref}](index.html#custom-log-formats){.hoverxref .tooltip
    .reference .internal} can now drop messages by having the
    corresponding methods of the configured [[`LOG_FORMATTER`{.xref .std
    .std-setting .docutils .literal
    .notranslate}]{.pre}](index.html#std-setting-LOG_FORMATTER){.hoverxref
    .tooltip .reference .internal} return [`None`{.docutils .literal
    .notranslate}]{.pre} ([issue
    3984](https://github.com/scrapy/scrapy/issues/3984){.reference
    .external}, [issue
    3987](https://github.com/scrapy/scrapy/issues/3987){.reference
    .external})

-   A much improved completion definition is now available for
    [Zsh](https://www.zsh.org/){.reference .external} ([issue
    4069](https://github.com/scrapy/scrapy/issues/4069){.reference
    .external})
:::

::: {#id84 .section}
##### Bug fixes[¶](#id84 "Permalink to this heading"){.headerlink}

-   [[`ItemLoader.load_item()`{.xref .py .py-meth .docutils .literal
    .notranslate}]{.pre}](index.html#scrapy.loader.ItemLoader.load_item "scrapy.loader.ItemLoader.load_item"){.reference
    .internal} no longer makes later calls to
    [[`ItemLoader.get_output_value()`{.xref .py .py-meth .docutils
    .literal
    .notranslate}]{.pre}](index.html#scrapy.loader.ItemLoader.get_output_value "scrapy.loader.ItemLoader.get_output_value"){.reference
    .internal} or [[`ItemLoader.load_item()`{.xref .py .py-meth
    .docutils .literal
    .notranslate}]{.pre}](index.html#scrapy.loader.ItemLoader.load_item "scrapy.loader.ItemLoader.load_item"){.reference
    .internal} return empty data ([issue
    3804](https://github.com/scrapy/scrapy/issues/3804){.reference
    .external}, [issue
    3819](https://github.com/scrapy/scrapy/issues/3819){.reference
    .external}, [issue
    3897](https://github.com/scrapy/scrapy/issues/3897){.reference
    .external}, [issue
    3976](https://github.com/scrapy/scrapy/issues/3976){.reference
    .external}, [issue
    3998](https://github.com/scrapy/scrapy/issues/3998){.reference
    .external}, [issue
    4036](https://github.com/scrapy/scrapy/issues/4036){.reference
    .external})

-   Fixed [[`DummyStatsCollector`{.xref .py .py-class .docutils .literal
    .notranslate}]{.pre}](index.html#scrapy.statscollectors.DummyStatsCollector "scrapy.statscollectors.DummyStatsCollector"){.reference
    .internal} raising a [[`TypeError`{.xref .py .py-exc .docutils
    .literal
    .notranslate}]{.pre}](https://docs.python.org/3/library/exceptions.html#TypeError "(in Python v3.12)"){.reference
    .external} exception ([issue
    4007](https://github.com/scrapy/scrapy/issues/4007){.reference
    .external}, [issue
    4052](https://github.com/scrapy/scrapy/issues/4052){.reference
    .external})

-   [[`FilesPipeline.file_path`{.xref .py .py-meth .docutils .literal
    .notranslate}]{.pre}](index.html#scrapy.pipelines.files.FilesPipeline.file_path "scrapy.pipelines.files.FilesPipeline.file_path"){.reference
    .internal} and [[`ImagesPipeline.file_path`{.xref .py .py-meth
    .docutils .literal
    .notranslate}]{.pre}](index.html#scrapy.pipelines.images.ImagesPipeline.file_path "scrapy.pipelines.images.ImagesPipeline.file_path"){.reference
    .internal} no longer choose file extensions that are not [registered
    with
    IANA](https://www.iana.org/assignments/media-types/media-types.xhtml){.reference
    .external} ([issue
    1287](https://github.com/scrapy/scrapy/issues/1287){.reference
    .external}, [issue
    3953](https://github.com/scrapy/scrapy/issues/3953){.reference
    .external}, [issue
    3954](https://github.com/scrapy/scrapy/issues/3954){.reference
    .external})

-   When using [botocore](https://github.com/boto/botocore){.reference
    .external} to persist files in S3, all botocore-supported headers
    are properly mapped now ([issue
    3904](https://github.com/scrapy/scrapy/issues/3904){.reference
    .external}, [issue
    3905](https://github.com/scrapy/scrapy/issues/3905){.reference
    .external})

-   FTP passwords in [`FEED_URI`{.xref .std .std-setting .docutils
    .literal .notranslate}]{.pre} containing percent-escaped characters
    are now properly decoded ([issue
    3941](https://github.com/scrapy/scrapy/issues/3941){.reference
    .external})

-   A memory-handling and error-handling issue in
    [`scrapy.utils.ssl.get_temp_key_info()`{.xref .py .py-func .docutils
    .literal .notranslate}]{.pre} has been fixed ([issue
    3920](https://github.com/scrapy/scrapy/issues/3920){.reference
    .external})
:::

::: {#id85 .section}
##### Documentation[¶](#id85 "Permalink to this heading"){.headerlink}

-   The documentation now covers how to define and configure a [[custom
    log format]{.std
    .std-ref}](index.html#custom-log-formats){.hoverxref .tooltip
    .reference .internal} ([issue
    3616](https://github.com/scrapy/scrapy/issues/3616){.reference
    .external}, [issue
    3660](https://github.com/scrapy/scrapy/issues/3660){.reference
    .external})

-   API documentation added for [[`MarshalItemExporter`{.xref .py
    .py-class .docutils .literal
    .notranslate}]{.pre}](index.html#scrapy.exporters.MarshalItemExporter "scrapy.exporters.MarshalItemExporter"){.reference
    .internal} and [[`PythonItemExporter`{.xref .py .py-class .docutils
    .literal
    .notranslate}]{.pre}](index.html#scrapy.exporters.PythonItemExporter "scrapy.exporters.PythonItemExporter"){.reference
    .internal} ([issue
    3973](https://github.com/scrapy/scrapy/issues/3973){.reference
    .external})

-   API documentation added for [`BaseItem`{.xref .py .py-class
    .docutils .literal .notranslate}]{.pre} and [[`ItemMeta`{.xref .py
    .py-class .docutils .literal
    .notranslate}]{.pre}](index.html#scrapy.item.ItemMeta "scrapy.item.ItemMeta"){.reference
    .internal} ([issue
    3999](https://github.com/scrapy/scrapy/issues/3999){.reference
    .external})

-   Minor documentation fixes ([issue
    2998](https://github.com/scrapy/scrapy/issues/2998){.reference
    .external}, [issue
    3398](https://github.com/scrapy/scrapy/issues/3398){.reference
    .external}, [issue
    3597](https://github.com/scrapy/scrapy/issues/3597){.reference
    .external}, [issue
    3894](https://github.com/scrapy/scrapy/issues/3894){.reference
    .external}, [issue
    3934](https://github.com/scrapy/scrapy/issues/3934){.reference
    .external}, [issue
    3978](https://github.com/scrapy/scrapy/issues/3978){.reference
    .external}, [issue
    3993](https://github.com/scrapy/scrapy/issues/3993){.reference
    .external}, [issue
    4022](https://github.com/scrapy/scrapy/issues/4022){.reference
    .external}, [issue
    4028](https://github.com/scrapy/scrapy/issues/4028){.reference
    .external}, [issue
    4033](https://github.com/scrapy/scrapy/issues/4033){.reference
    .external}, [issue
    4046](https://github.com/scrapy/scrapy/issues/4046){.reference
    .external}, [issue
    4050](https://github.com/scrapy/scrapy/issues/4050){.reference
    .external}, [issue
    4055](https://github.com/scrapy/scrapy/issues/4055){.reference
    .external}, [issue
    4056](https://github.com/scrapy/scrapy/issues/4056){.reference
    .external}, [issue
    4061](https://github.com/scrapy/scrapy/issues/4061){.reference
    .external}, [issue
    4072](https://github.com/scrapy/scrapy/issues/4072){.reference
    .external}, [issue
    4071](https://github.com/scrapy/scrapy/issues/4071){.reference
    .external}, [issue
    4079](https://github.com/scrapy/scrapy/issues/4079){.reference
    .external}, [issue
    4081](https://github.com/scrapy/scrapy/issues/4081){.reference
    .external}, [issue
    4089](https://github.com/scrapy/scrapy/issues/4089){.reference
    .external}, [issue
    4093](https://github.com/scrapy/scrapy/issues/4093){.reference
    .external})
:::

::: {#id86 .section}
[]{#id87}

##### Deprecation removals[¶](#id86 "Permalink to this heading"){.headerlink}

-   [`scrapy.xlib`{.docutils .literal .notranslate}]{.pre} has been
    removed ([issue
    4015](https://github.com/scrapy/scrapy/issues/4015){.reference
    .external})
:::

::: {#id88 .section}
[]{#id89}

##### Deprecations[¶](#id88 "Permalink to this heading"){.headerlink}

-   The [LevelDB](https://github.com/google/leveldb){.reference
    .external} storage backend
    ([`scrapy.extensions.httpcache.LeveldbCacheStorage`{.docutils
    .literal .notranslate}]{.pre}) of [[`HttpCacheMiddleware`{.xref .py
    .py-class .docutils .literal
    .notranslate}]{.pre}](index.html#scrapy.downloadermiddlewares.httpcache.HttpCacheMiddleware "scrapy.downloadermiddlewares.httpcache.HttpCacheMiddleware"){.reference
    .internal} is deprecated ([issue
    4085](https://github.com/scrapy/scrapy/issues/4085){.reference
    .external}, [issue
    4092](https://github.com/scrapy/scrapy/issues/4092){.reference
    .external})

-   Use of the undocumented
    [`SCRAPY_PICKLED_SETTINGS_TO_OVERRIDE`{.docutils .literal
    .notranslate}]{.pre} environment variable is deprecated ([issue
    3910](https://github.com/scrapy/scrapy/issues/3910){.reference
    .external})

-   [`scrapy.item.DictItem`{.docutils .literal .notranslate}]{.pre} is
    deprecated, use [`Item`{.xref .py .py-class .docutils .literal
    .notranslate}]{.pre} instead ([issue
    3999](https://github.com/scrapy/scrapy/issues/3999){.reference
    .external})
:::

::: {#other-changes .section}
##### Other changes[¶](#other-changes "Permalink to this heading"){.headerlink}

-   Minimum versions of optional Scrapy requirements that are covered by
    continuous integration tests have been updated:

    -   [botocore](https://github.com/boto/botocore){.reference
        .external} 1.3.23

    -   [Pillow](https://python-pillow.org/){.reference .external} 3.4.2

    Lower versions of these optional requirements may work, but it is
    not guaranteed ([issue
    3892](https://github.com/scrapy/scrapy/issues/3892){.reference
    .external})

-   GitHub templates for bug reports and feature requests ([issue
    3126](https://github.com/scrapy/scrapy/issues/3126){.reference
    .external}, [issue
    3471](https://github.com/scrapy/scrapy/issues/3471){.reference
    .external}, [issue
    3749](https://github.com/scrapy/scrapy/issues/3749){.reference
    .external}, [issue
    3754](https://github.com/scrapy/scrapy/issues/3754){.reference
    .external})

-   Continuous integration fixes ([issue
    3923](https://github.com/scrapy/scrapy/issues/3923){.reference
    .external})

-   Code cleanup ([issue
    3391](https://github.com/scrapy/scrapy/issues/3391){.reference
    .external}, [issue
    3907](https://github.com/scrapy/scrapy/issues/3907){.reference
    .external}, [issue
    3946](https://github.com/scrapy/scrapy/issues/3946){.reference
    .external}, [issue
    3950](https://github.com/scrapy/scrapy/issues/3950){.reference
    .external}, [issue
    4023](https://github.com/scrapy/scrapy/issues/4023){.reference
    .external}, [issue
    4031](https://github.com/scrapy/scrapy/issues/4031){.reference
    .external})
:::
:::

::: {#scrapy-1-7-4-2019-10-21 .section}
[]{#release-1-7-4}

#### Scrapy 1.7.4 (2019-10-21)[¶](#scrapy-1-7-4-2019-10-21 "Permalink to this heading"){.headerlink}

Revert the fix for [issue
3804](https://github.com/scrapy/scrapy/issues/3804){.reference
.external} ([issue
3819](https://github.com/scrapy/scrapy/issues/3819){.reference
.external}), which has a few undesired side effects ([issue
3897](https://github.com/scrapy/scrapy/issues/3897){.reference
.external}, [issue
3976](https://github.com/scrapy/scrapy/issues/3976){.reference
.external}).

As a result, when an item loader is initialized with an item,
[[`ItemLoader.load_item()`{.xref .py .py-meth .docutils .literal
.notranslate}]{.pre}](index.html#scrapy.loader.ItemLoader.load_item "scrapy.loader.ItemLoader.load_item"){.reference
.internal} once again makes later calls to
[[`ItemLoader.get_output_value()`{.xref .py .py-meth .docutils .literal
.notranslate}]{.pre}](index.html#scrapy.loader.ItemLoader.get_output_value "scrapy.loader.ItemLoader.get_output_value"){.reference
.internal} or [[`ItemLoader.load_item()`{.xref .py .py-meth .docutils
.literal
.notranslate}]{.pre}](index.html#scrapy.loader.ItemLoader.load_item "scrapy.loader.ItemLoader.load_item"){.reference
.internal} return empty data.
:::

::: {#scrapy-1-7-3-2019-08-01 .section}
[]{#release-1-7-3}

#### Scrapy 1.7.3 (2019-08-01)[¶](#scrapy-1-7-3-2019-08-01 "Permalink to this heading"){.headerlink}

Enforce lxml 4.3.5 or lower for Python 3.4 ([issue
3912](https://github.com/scrapy/scrapy/issues/3912){.reference
.external}, [issue
3918](https://github.com/scrapy/scrapy/issues/3918){.reference
.external}).
:::

::: {#scrapy-1-7-2-2019-07-23 .section}
[]{#release-1-7-2}

#### Scrapy 1.7.2 (2019-07-23)[¶](#scrapy-1-7-2-2019-07-23 "Permalink to this heading"){.headerlink}

Fix Python 2 support ([issue
3889](https://github.com/scrapy/scrapy/issues/3889){.reference
.external}, [issue
3893](https://github.com/scrapy/scrapy/issues/3893){.reference
.external}, [issue
3896](https://github.com/scrapy/scrapy/issues/3896){.reference
.external}).
:::

::: {#scrapy-1-7-1-2019-07-18 .section}
[]{#release-1-7-1}

#### Scrapy 1.7.1 (2019-07-18)[¶](#scrapy-1-7-1-2019-07-18 "Permalink to this heading"){.headerlink}

Re-packaging of Scrapy 1.7.0, which was missing some changes in PyPI.
:::

::: {#scrapy-1-7-0-2019-07-18 .section}
[]{#release-1-7-0}

#### Scrapy 1.7.0 (2019-07-18)[¶](#scrapy-1-7-0-2019-07-18 "Permalink to this heading"){.headerlink}

::: {.admonition .note}
Note

Make sure you install Scrapy 1.7.1. The Scrapy 1.7.0 package in PyPI is
the result of an erroneous commit tagging and does not include all the
changes described below.
:::

Highlights:

-   Improvements for crawls targeting multiple domains

-   A cleaner way to pass arguments to callbacks

-   A new class for JSON requests

-   Improvements for rule-based spiders

-   New features for feed exports

::: {#id90 .section}
##### Backward-incompatible changes[¶](#id90 "Permalink to this heading"){.headerlink}

-   [`429`{.docutils .literal .notranslate}]{.pre} is now part of the
    [[`RETRY_HTTP_CODES`{.xref .std .std-setting .docutils .literal
    .notranslate}]{.pre}](index.html#std-setting-RETRY_HTTP_CODES){.hoverxref
    .tooltip .reference .internal} setting by default

    This change is **backward incompatible**. If you don't want to retry
    [`429`{.docutils .literal .notranslate}]{.pre}, you must override
    [[`RETRY_HTTP_CODES`{.xref .std .std-setting .docutils .literal
    .notranslate}]{.pre}](index.html#std-setting-RETRY_HTTP_CODES){.hoverxref
    .tooltip .reference .internal} accordingly.

-   [[`Crawler`{.xref .py .py-class .docutils .literal
    .notranslate}]{.pre}](index.html#scrapy.crawler.Crawler "scrapy.crawler.Crawler"){.reference
    .internal}, [[`CrawlerRunner.crawl`{.xref .py .py-class .docutils
    .literal
    .notranslate}]{.pre}](index.html#scrapy.crawler.CrawlerRunner.crawl "scrapy.crawler.CrawlerRunner.crawl"){.reference
    .internal} and [[`CrawlerRunner.create_crawler`{.xref .py .py-class
    .docutils .literal
    .notranslate}]{.pre}](index.html#scrapy.crawler.CrawlerRunner.create_crawler "scrapy.crawler.CrawlerRunner.create_crawler"){.reference
    .internal} no longer accept a [[`Spider`{.xref .py .py-class
    .docutils .literal
    .notranslate}]{.pre}](index.html#scrapy.spiders.Spider "scrapy.spiders.Spider"){.reference
    .internal} subclass instance, they only accept a [[`Spider`{.xref
    .py .py-class .docutils .literal
    .notranslate}]{.pre}](index.html#scrapy.spiders.Spider "scrapy.spiders.Spider"){.reference
    .internal} subclass now.

    [[`Spider`{.xref .py .py-class .docutils .literal
    .notranslate}]{.pre}](index.html#scrapy.spiders.Spider "scrapy.spiders.Spider"){.reference
    .internal} subclass instances were never meant to work, and they
    were not working as one would expect: instead of using the passed
    [[`Spider`{.xref .py .py-class .docutils .literal
    .notranslate}]{.pre}](index.html#scrapy.spiders.Spider "scrapy.spiders.Spider"){.reference
    .internal} subclass instance, their [`from_crawler`{.xref .py
    .py-class .docutils .literal .notranslate}]{.pre} method was called
    to generate a new instance.

-   Non-default values for the [[`SCHEDULER_PRIORITY_QUEUE`{.xref .std
    .std-setting .docutils .literal
    .notranslate}]{.pre}](index.html#std-setting-SCHEDULER_PRIORITY_QUEUE){.hoverxref
    .tooltip .reference .internal} setting may stop working. Scheduler
    priority queue classes now need to handle [[`Request`{.xref .py
    .py-class .docutils .literal
    .notranslate}]{.pre}](index.html#scrapy.http.Request "scrapy.http.Request"){.reference
    .internal} objects instead of arbitrary Python data structures.

-   An additional [`crawler`{.docutils .literal .notranslate}]{.pre}
    parameter has been added to the [`__init__`{.docutils .literal
    .notranslate}]{.pre} method of the [[`Scheduler`{.xref .py .py-class
    .docutils .literal
    .notranslate}]{.pre}](index.html#scrapy.core.scheduler.Scheduler "scrapy.core.scheduler.Scheduler"){.reference
    .internal} class. Custom scheduler subclasses which don't accept
    arbitrary parameters in their [`__init__`{.docutils .literal
    .notranslate}]{.pre} method might break because of this change.

    For more information, see [[`SCHEDULER`{.xref .std .std-setting
    .docutils .literal
    .notranslate}]{.pre}](index.html#std-setting-SCHEDULER){.hoverxref
    .tooltip .reference .internal}.

See also [[Deprecation removals]{.std .std-ref}](#id94){.hoverxref
.tooltip .reference .internal} below.
:::

::: {#id91 .section}
##### New features[¶](#id91 "Permalink to this heading"){.headerlink}

-   A new scheduler priority queue,
    [`scrapy.pqueues.DownloaderAwarePriorityQueue`{.docutils .literal
    .notranslate}]{.pre}, may be [[enabled]{.std
    .std-ref}](index.html#broad-crawls-scheduler-priority-queue){.hoverxref
    .tooltip .reference .internal} for a significant scheduling
    improvement on crawls targeting multiple web domains, at the cost of
    no [[`CONCURRENT_REQUESTS_PER_IP`{.xref .std .std-setting .docutils
    .literal
    .notranslate}]{.pre}](index.html#std-setting-CONCURRENT_REQUESTS_PER_IP){.hoverxref
    .tooltip .reference .internal} support ([issue
    3520](https://github.com/scrapy/scrapy/issues/3520){.reference
    .external})

-   A new [[`Request.cb_kwargs`{.xref .py .py-attr .docutils .literal
    .notranslate}]{.pre}](index.html#scrapy.http.Request.cb_kwargs "scrapy.http.Request.cb_kwargs"){.reference
    .internal} attribute provides a cleaner way to pass keyword
    arguments to callback methods ([issue
    1138](https://github.com/scrapy/scrapy/issues/1138){.reference
    .external}, [issue
    3563](https://github.com/scrapy/scrapy/issues/3563){.reference
    .external})

-   A new [[`JSONRequest`{.xref .py .py-class .docutils .literal
    .notranslate}]{.pre}](index.html#scrapy.http.JsonRequest "scrapy.http.JsonRequest"){.reference
    .internal} class offers a more convenient way to build JSON requests
    ([issue
    3504](https://github.com/scrapy/scrapy/issues/3504){.reference
    .external}, [issue
    3505](https://github.com/scrapy/scrapy/issues/3505){.reference
    .external})

-   A [`process_request`{.docutils .literal .notranslate}]{.pre}
    callback passed to the [[`Rule`{.xref .py .py-class .docutils
    .literal
    .notranslate}]{.pre}](index.html#scrapy.spiders.Rule "scrapy.spiders.Rule"){.reference
    .internal} [`__init__`{.docutils .literal .notranslate}]{.pre}
    method now receives the [[`Response`{.xref .py .py-class .docutils
    .literal
    .notranslate}]{.pre}](index.html#scrapy.http.Response "scrapy.http.Response"){.reference
    .internal} object that originated the request as its second argument
    ([issue
    3682](https://github.com/scrapy/scrapy/issues/3682){.reference
    .external})

-   A new [`restrict_text`{.docutils .literal .notranslate}]{.pre}
    parameter for the [[`LinkExtractor`{.xref .py .py-attr .docutils
    .literal
    .notranslate}]{.pre}](index.html#scrapy.linkextractors.lxmlhtml.LxmlLinkExtractor "scrapy.linkextractors.lxmlhtml.LxmlLinkExtractor"){.reference
    .internal} [`__init__`{.docutils .literal .notranslate}]{.pre}
    method allows filtering links by linking text ([issue
    3622](https://github.com/scrapy/scrapy/issues/3622){.reference
    .external}, [issue
    3635](https://github.com/scrapy/scrapy/issues/3635){.reference
    .external})

-   A new [[`FEED_STORAGE_S3_ACL`{.xref .std .std-setting .docutils
    .literal
    .notranslate}]{.pre}](index.html#std-setting-FEED_STORAGE_S3_ACL){.hoverxref
    .tooltip .reference .internal} setting allows defining a custom ACL
    for feeds exported to Amazon S3 ([issue
    3607](https://github.com/scrapy/scrapy/issues/3607){.reference
    .external})

-   A new [[`FEED_STORAGE_FTP_ACTIVE`{.xref .std .std-setting .docutils
    .literal
    .notranslate}]{.pre}](index.html#std-setting-FEED_STORAGE_FTP_ACTIVE){.hoverxref
    .tooltip .reference .internal} setting allows using FTP's active
    connection mode for feeds exported to FTP servers ([issue
    3829](https://github.com/scrapy/scrapy/issues/3829){.reference
    .external})

-   A new [[`METAREFRESH_IGNORE_TAGS`{.xref .std .std-setting .docutils
    .literal
    .notranslate}]{.pre}](index.html#std-setting-METAREFRESH_IGNORE_TAGS){.hoverxref
    .tooltip .reference .internal} setting allows overriding which HTML
    tags are ignored when searching a response for HTML meta tags that
    trigger a redirect ([issue
    1422](https://github.com/scrapy/scrapy/issues/1422){.reference
    .external}, [issue
    3768](https://github.com/scrapy/scrapy/issues/3768){.reference
    .external})

-   A new [[`redirect_reasons`{.xref .std .std-reqmeta .docutils
    .literal
    .notranslate}]{.pre}](index.html#std-reqmeta-redirect_reasons){.hoverxref
    .tooltip .reference .internal} request meta key exposes the reason
    (status code, meta refresh) behind every followed redirect ([issue
    3581](https://github.com/scrapy/scrapy/issues/3581){.reference
    .external}, [issue
    3687](https://github.com/scrapy/scrapy/issues/3687){.reference
    .external})

-   The [`SCRAPY_CHECK`{.docutils .literal .notranslate}]{.pre} variable
    is now set to the [`true`{.docutils .literal .notranslate}]{.pre}
    string during runs of the [[`check`{.xref .std .std-command
    .docutils .literal
    .notranslate}]{.pre}](index.html#std-command-check){.hoverxref
    .tooltip .reference .internal} command, which allows [[detecting
    contract check runs from code]{.std
    .std-ref}](index.html#detecting-contract-check-runs){.hoverxref
    .tooltip .reference .internal} ([issue
    3704](https://github.com/scrapy/scrapy/issues/3704){.reference
    .external}, [issue
    3739](https://github.com/scrapy/scrapy/issues/3739){.reference
    .external})

-   A new [`Item.deepcopy()`{.xref .py .py-meth .docutils .literal
    .notranslate}]{.pre} method makes it easier to [[deep-copy
    items]{.std .std-ref}](index.html#copying-items){.hoverxref .tooltip
    .reference .internal} ([issue
    1493](https://github.com/scrapy/scrapy/issues/1493){.reference
    .external}, [issue
    3671](https://github.com/scrapy/scrapy/issues/3671){.reference
    .external})

-   [[`CoreStats`{.xref .py .py-class .docutils .literal
    .notranslate}]{.pre}](index.html#scrapy.extensions.corestats.CoreStats "scrapy.extensions.corestats.CoreStats"){.reference
    .internal} also logs [`elapsed_time_seconds`{.docutils .literal
    .notranslate}]{.pre} now ([issue
    3638](https://github.com/scrapy/scrapy/issues/3638){.reference
    .external})

-   Exceptions from [[`ItemLoader`{.xref .py .py-class .docutils
    .literal
    .notranslate}]{.pre}](index.html#scrapy.loader.ItemLoader "scrapy.loader.ItemLoader"){.reference
    .internal} [[input and output processors]{.std
    .std-ref}](index.html#topics-loaders-processors){.hoverxref .tooltip
    .reference .internal} are now more verbose ([issue
    3836](https://github.com/scrapy/scrapy/issues/3836){.reference
    .external}, [issue
    3840](https://github.com/scrapy/scrapy/issues/3840){.reference
    .external})

-   [[`Crawler`{.xref .py .py-class .docutils .literal
    .notranslate}]{.pre}](index.html#scrapy.crawler.Crawler "scrapy.crawler.Crawler"){.reference
    .internal}, [[`CrawlerRunner.crawl`{.xref .py .py-class .docutils
    .literal
    .notranslate}]{.pre}](index.html#scrapy.crawler.CrawlerRunner.crawl "scrapy.crawler.CrawlerRunner.crawl"){.reference
    .internal} and [[`CrawlerRunner.create_crawler`{.xref .py .py-class
    .docutils .literal
    .notranslate}]{.pre}](index.html#scrapy.crawler.CrawlerRunner.create_crawler "scrapy.crawler.CrawlerRunner.create_crawler"){.reference
    .internal} now fail gracefully if they receive a [[`Spider`{.xref
    .py .py-class .docutils .literal
    .notranslate}]{.pre}](index.html#scrapy.spiders.Spider "scrapy.spiders.Spider"){.reference
    .internal} subclass instance instead of the subclass itself ([issue
    2283](https://github.com/scrapy/scrapy/issues/2283){.reference
    .external}, [issue
    3610](https://github.com/scrapy/scrapy/issues/3610){.reference
    .external}, [issue
    3872](https://github.com/scrapy/scrapy/issues/3872){.reference
    .external})
:::

::: {#id92 .section}
##### Bug fixes[¶](#id92 "Permalink to this heading"){.headerlink}

-   [[`process_spider_exception()`{.xref .py .py-meth .docutils .literal
    .notranslate}]{.pre}](index.html#scrapy.spidermiddlewares.SpiderMiddleware.process_spider_exception "scrapy.spidermiddlewares.SpiderMiddleware.process_spider_exception"){.reference
    .internal} is now also invoked for generators ([issue
    220](https://github.com/scrapy/scrapy/issues/220){.reference
    .external}, [issue
    2061](https://github.com/scrapy/scrapy/issues/2061){.reference
    .external})

-   System exceptions like
    [KeyboardInterrupt](https://docs.python.org/3/library/exceptions.html#KeyboardInterrupt){.reference
    .external} are no longer caught ([issue
    3726](https://github.com/scrapy/scrapy/issues/3726){.reference
    .external})

-   [[`ItemLoader.load_item()`{.xref .py .py-meth .docutils .literal
    .notranslate}]{.pre}](index.html#scrapy.loader.ItemLoader.load_item "scrapy.loader.ItemLoader.load_item"){.reference
    .internal} no longer makes later calls to
    [[`ItemLoader.get_output_value()`{.xref .py .py-meth .docutils
    .literal
    .notranslate}]{.pre}](index.html#scrapy.loader.ItemLoader.get_output_value "scrapy.loader.ItemLoader.get_output_value"){.reference
    .internal} or [[`ItemLoader.load_item()`{.xref .py .py-meth
    .docutils .literal
    .notranslate}]{.pre}](index.html#scrapy.loader.ItemLoader.load_item "scrapy.loader.ItemLoader.load_item"){.reference
    .internal} return empty data ([issue
    3804](https://github.com/scrapy/scrapy/issues/3804){.reference
    .external}, [issue
    3819](https://github.com/scrapy/scrapy/issues/3819){.reference
    .external})

-   The images pipeline ([[`ImagesPipeline`{.xref .py .py-class
    .docutils .literal
    .notranslate}]{.pre}](index.html#scrapy.pipelines.images.ImagesPipeline "scrapy.pipelines.images.ImagesPipeline"){.reference
    .internal}) no longer ignores these Amazon S3 settings:
    [[`AWS_ENDPOINT_URL`{.xref .std .std-setting .docutils .literal
    .notranslate}]{.pre}](index.html#std-setting-AWS_ENDPOINT_URL){.hoverxref
    .tooltip .reference .internal}, [[`AWS_REGION_NAME`{.xref .std
    .std-setting .docutils .literal
    .notranslate}]{.pre}](index.html#std-setting-AWS_REGION_NAME){.hoverxref
    .tooltip .reference .internal}, [[`AWS_USE_SSL`{.xref .std
    .std-setting .docutils .literal
    .notranslate}]{.pre}](index.html#std-setting-AWS_USE_SSL){.hoverxref
    .tooltip .reference .internal}, [[`AWS_VERIFY`{.xref .std
    .std-setting .docutils .literal
    .notranslate}]{.pre}](index.html#std-setting-AWS_VERIFY){.hoverxref
    .tooltip .reference .internal} ([issue
    3625](https://github.com/scrapy/scrapy/issues/3625){.reference
    .external})

-   Fixed a memory leak in
    [`scrapy.pipelines.media.MediaPipeline`{.docutils .literal
    .notranslate}]{.pre} affecting, for example, non-200 responses and
    exceptions from custom middlewares ([issue
    3813](https://github.com/scrapy/scrapy/issues/3813){.reference
    .external})

-   Requests with private callbacks are now correctly unserialized from
    disk ([issue
    3790](https://github.com/scrapy/scrapy/issues/3790){.reference
    .external})

-   [`FormRequest.from_response()`{.xref .py .py-meth .docutils .literal
    .notranslate}]{.pre} now handles invalid methods like major web
    browsers ([issue
    3777](https://github.com/scrapy/scrapy/issues/3777){.reference
    .external}, [issue
    3794](https://github.com/scrapy/scrapy/issues/3794){.reference
    .external})
:::

::: {#id93 .section}
##### Documentation[¶](#id93 "Permalink to this heading"){.headerlink}

-   A new topic, [[Selecting dynamically-loaded content]{.std
    .std-ref}](index.html#topics-dynamic-content){.hoverxref .tooltip
    .reference .internal}, covers recommended approaches to read
    dynamically-loaded data ([issue
    3703](https://github.com/scrapy/scrapy/issues/3703){.reference
    .external})

-   [[Broad Crawls]{.std
    .std-ref}](index.html#topics-broad-crawls){.hoverxref .tooltip
    .reference .internal} now features information about memory usage
    ([issue
    1264](https://github.com/scrapy/scrapy/issues/1264){.reference
    .external}, [issue
    3866](https://github.com/scrapy/scrapy/issues/3866){.reference
    .external})

-   The documentation of [[`Rule`{.xref .py .py-class .docutils .literal
    .notranslate}]{.pre}](index.html#scrapy.spiders.Rule "scrapy.spiders.Rule"){.reference
    .internal} now covers how to access the text of a link when using
    [[`CrawlSpider`{.xref .py .py-class .docutils .literal
    .notranslate}]{.pre}](index.html#scrapy.spiders.CrawlSpider "scrapy.spiders.CrawlSpider"){.reference
    .internal} ([issue
    3711](https://github.com/scrapy/scrapy/issues/3711){.reference
    .external}, [issue
    3712](https://github.com/scrapy/scrapy/issues/3712){.reference
    .external})

-   A new section, [[Writing your own storage backend]{.std
    .std-ref}](index.html#httpcache-storage-custom){.hoverxref .tooltip
    .reference .internal}, covers writing a custom cache storage backend
    for [[`HttpCacheMiddleware`{.xref .py .py-class .docutils .literal
    .notranslate}]{.pre}](index.html#scrapy.downloadermiddlewares.httpcache.HttpCacheMiddleware "scrapy.downloadermiddlewares.httpcache.HttpCacheMiddleware"){.reference
    .internal} ([issue
    3683](https://github.com/scrapy/scrapy/issues/3683){.reference
    .external}, [issue
    3692](https://github.com/scrapy/scrapy/issues/3692){.reference
    .external})

-   A new [[FAQ]{.std .std-ref}](index.html#faq){.hoverxref .tooltip
    .reference .internal} entry, [[How to split an item into multiple
    items in an item pipeline?]{.std
    .std-ref}](index.html#faq-split-item){.hoverxref .tooltip .reference
    .internal}, explains what to do when you want to split an item into
    multiple items from an item pipeline ([issue
    2240](https://github.com/scrapy/scrapy/issues/2240){.reference
    .external}, [issue
    3672](https://github.com/scrapy/scrapy/issues/3672){.reference
    .external})

-   Updated the [[FAQ entry about crawl order]{.std
    .std-ref}](index.html#faq-bfo-dfo){.hoverxref .tooltip .reference
    .internal} to explain why the first few requests rarely follow the
    desired order ([issue
    1739](https://github.com/scrapy/scrapy/issues/1739){.reference
    .external}, [issue
    3621](https://github.com/scrapy/scrapy/issues/3621){.reference
    .external})

-   The [[`LOGSTATS_INTERVAL`{.xref .std .std-setting .docutils .literal
    .notranslate}]{.pre}](index.html#std-setting-LOGSTATS_INTERVAL){.hoverxref
    .tooltip .reference .internal} setting ([issue
    3730](https://github.com/scrapy/scrapy/issues/3730){.reference
    .external}), the [[`FilesPipeline.file_path`{.xref .py .py-meth
    .docutils .literal
    .notranslate}]{.pre}](index.html#scrapy.pipelines.files.FilesPipeline.file_path "scrapy.pipelines.files.FilesPipeline.file_path"){.reference
    .internal} and [[`ImagesPipeline.file_path`{.xref .py .py-meth
    .docutils .literal
    .notranslate}]{.pre}](index.html#scrapy.pipelines.images.ImagesPipeline.file_path "scrapy.pipelines.images.ImagesPipeline.file_path"){.reference
    .internal} methods ([issue
    2253](https://github.com/scrapy/scrapy/issues/2253){.reference
    .external}, [issue
    3609](https://github.com/scrapy/scrapy/issues/3609){.reference
    .external}) and the [[`Crawler.stop()`{.xref .py .py-meth .docutils
    .literal
    .notranslate}]{.pre}](index.html#scrapy.crawler.Crawler.stop "scrapy.crawler.Crawler.stop"){.reference
    .internal} method ([issue
    3842](https://github.com/scrapy/scrapy/issues/3842){.reference
    .external}) are now documented

-   Some parts of the documentation that were confusing or misleading
    are now clearer ([issue
    1347](https://github.com/scrapy/scrapy/issues/1347){.reference
    .external}, [issue
    1789](https://github.com/scrapy/scrapy/issues/1789){.reference
    .external}, [issue
    2289](https://github.com/scrapy/scrapy/issues/2289){.reference
    .external}, [issue
    3069](https://github.com/scrapy/scrapy/issues/3069){.reference
    .external}, [issue
    3615](https://github.com/scrapy/scrapy/issues/3615){.reference
    .external}, [issue
    3626](https://github.com/scrapy/scrapy/issues/3626){.reference
    .external}, [issue
    3668](https://github.com/scrapy/scrapy/issues/3668){.reference
    .external}, [issue
    3670](https://github.com/scrapy/scrapy/issues/3670){.reference
    .external}, [issue
    3673](https://github.com/scrapy/scrapy/issues/3673){.reference
    .external}, [issue
    3728](https://github.com/scrapy/scrapy/issues/3728){.reference
    .external}, [issue
    3762](https://github.com/scrapy/scrapy/issues/3762){.reference
    .external}, [issue
    3861](https://github.com/scrapy/scrapy/issues/3861){.reference
    .external}, [issue
    3882](https://github.com/scrapy/scrapy/issues/3882){.reference
    .external})

-   Minor documentation fixes ([issue
    3648](https://github.com/scrapy/scrapy/issues/3648){.reference
    .external}, [issue
    3649](https://github.com/scrapy/scrapy/issues/3649){.reference
    .external}, [issue
    3662](https://github.com/scrapy/scrapy/issues/3662){.reference
    .external}, [issue
    3674](https://github.com/scrapy/scrapy/issues/3674){.reference
    .external}, [issue
    3676](https://github.com/scrapy/scrapy/issues/3676){.reference
    .external}, [issue
    3694](https://github.com/scrapy/scrapy/issues/3694){.reference
    .external}, [issue
    3724](https://github.com/scrapy/scrapy/issues/3724){.reference
    .external}, [issue
    3764](https://github.com/scrapy/scrapy/issues/3764){.reference
    .external}, [issue
    3767](https://github.com/scrapy/scrapy/issues/3767){.reference
    .external}, [issue
    3791](https://github.com/scrapy/scrapy/issues/3791){.reference
    .external}, [issue
    3797](https://github.com/scrapy/scrapy/issues/3797){.reference
    .external}, [issue
    3806](https://github.com/scrapy/scrapy/issues/3806){.reference
    .external}, [issue
    3812](https://github.com/scrapy/scrapy/issues/3812){.reference
    .external})
:::

::: {#id94 .section}
[]{#id95}

##### Deprecation removals[¶](#id94 "Permalink to this heading"){.headerlink}

The following deprecated APIs have been removed ([issue
3578](https://github.com/scrapy/scrapy/issues/3578){.reference
.external}):

-   [`scrapy.conf`{.docutils .literal .notranslate}]{.pre} (use
    [[`Crawler.settings`{.xref .py .py-attr .docutils .literal
    .notranslate}]{.pre}](index.html#scrapy.crawler.Crawler.settings "scrapy.crawler.Crawler.settings"){.reference
    .internal})

-   From [`scrapy.core.downloader.handlers`{.docutils .literal
    .notranslate}]{.pre}:

    -   [`http.HttpDownloadHandler`{.docutils .literal
        .notranslate}]{.pre} (use
        [`http10.HTTP10DownloadHandler`{.docutils .literal
        .notranslate}]{.pre})

-   [`scrapy.loader.ItemLoader._get_values`{.docutils .literal
    .notranslate}]{.pre} (use [`_get_xpathvalues`{.docutils .literal
    .notranslate}]{.pre})

-   [`scrapy.loader.XPathItemLoader`{.docutils .literal
    .notranslate}]{.pre} (use [[`ItemLoader`{.xref .py .py-class
    .docutils .literal
    .notranslate}]{.pre}](index.html#scrapy.loader.ItemLoader "scrapy.loader.ItemLoader"){.reference
    .internal})

-   [`scrapy.log`{.docutils .literal .notranslate}]{.pre} (see
    [[Logging]{.std .std-ref}](index.html#topics-logging){.hoverxref
    .tooltip .reference .internal})

-   From [`scrapy.pipelines`{.docutils .literal .notranslate}]{.pre}:

    -   [`files.FilesPipeline.file_key`{.docutils .literal
        .notranslate}]{.pre} (use [`file_path`{.docutils .literal
        .notranslate}]{.pre})

    -   [`images.ImagesPipeline.file_key`{.docutils .literal
        .notranslate}]{.pre} (use [`file_path`{.docutils .literal
        .notranslate}]{.pre})

    -   [`images.ImagesPipeline.image_key`{.docutils .literal
        .notranslate}]{.pre} (use [`file_path`{.docutils .literal
        .notranslate}]{.pre})

    -   [`images.ImagesPipeline.thumb_key`{.docutils .literal
        .notranslate}]{.pre} (use [`thumb_path`{.docutils .literal
        .notranslate}]{.pre})

-   From both [`scrapy.selector`{.docutils .literal .notranslate}]{.pre}
    and [`scrapy.selector.lxmlsel`{.docutils .literal
    .notranslate}]{.pre}:

    -   [`HtmlXPathSelector`{.docutils .literal .notranslate}]{.pre}
        (use [[`Selector`{.xref .py .py-class .docutils .literal
        .notranslate}]{.pre}](index.html#scrapy.selector.Selector "scrapy.selector.Selector"){.reference
        .internal})

    -   [`XmlXPathSelector`{.docutils .literal .notranslate}]{.pre} (use
        [[`Selector`{.xref .py .py-class .docutils .literal
        .notranslate}]{.pre}](index.html#scrapy.selector.Selector "scrapy.selector.Selector"){.reference
        .internal})

    -   [`XPathSelector`{.docutils .literal .notranslate}]{.pre} (use
        [[`Selector`{.xref .py .py-class .docutils .literal
        .notranslate}]{.pre}](index.html#scrapy.selector.Selector "scrapy.selector.Selector"){.reference
        .internal})

    -   [`XPathSelectorList`{.docutils .literal .notranslate}]{.pre}
        (use [[`Selector`{.xref .py .py-class .docutils .literal
        .notranslate}]{.pre}](index.html#scrapy.selector.Selector "scrapy.selector.Selector"){.reference
        .internal})

-   From [`scrapy.selector.csstranslator`{.docutils .literal
    .notranslate}]{.pre}:

    -   [`ScrapyGenericTranslator`{.docutils .literal
        .notranslate}]{.pre} (use
        [parsel.csstranslator.GenericTranslator](https://parsel.readthedocs.io/en/latest/parsel.html#parsel.csstranslator.GenericTranslator){.reference
        .external})

    -   [`ScrapyHTMLTranslator`{.docutils .literal .notranslate}]{.pre}
        (use
        [parsel.csstranslator.HTMLTranslator](https://parsel.readthedocs.io/en/latest/parsel.html#parsel.csstranslator.HTMLTranslator){.reference
        .external})

    -   [`ScrapyXPathExpr`{.docutils .literal .notranslate}]{.pre} (use
        [parsel.csstranslator.XPathExpr](https://parsel.readthedocs.io/en/latest/parsel.html#parsel.csstranslator.XPathExpr){.reference
        .external})

-   From [[`Selector`{.xref .py .py-class .docutils .literal
    .notranslate}]{.pre}](index.html#scrapy.selector.Selector "scrapy.selector.Selector"){.reference
    .internal}:

    -   [`_root`{.docutils .literal .notranslate}]{.pre} (both the
        [`__init__`{.docutils .literal .notranslate}]{.pre} method
        argument and the object property, use [`root`{.docutils .literal
        .notranslate}]{.pre})

    -   [`extract_unquoted`{.docutils .literal .notranslate}]{.pre} (use
        [`getall`{.docutils .literal .notranslate}]{.pre})

    -   [`select`{.docutils .literal .notranslate}]{.pre} (use
        [`xpath`{.docutils .literal .notranslate}]{.pre})

-   From [[`SelectorList`{.xref .py .py-class .docutils .literal
    .notranslate}]{.pre}](index.html#scrapy.selector.SelectorList "scrapy.selector.SelectorList"){.reference
    .internal}:

    -   [`extract_unquoted`{.docutils .literal .notranslate}]{.pre} (use
        [`getall`{.docutils .literal .notranslate}]{.pre})

    -   [`select`{.docutils .literal .notranslate}]{.pre} (use
        [`xpath`{.docutils .literal .notranslate}]{.pre})

    -   [`x`{.docutils .literal .notranslate}]{.pre} (use
        [`xpath`{.docutils .literal .notranslate}]{.pre})

-   [`scrapy.spiders.BaseSpider`{.docutils .literal .notranslate}]{.pre}
    (use [[`Spider`{.xref .py .py-class .docutils .literal
    .notranslate}]{.pre}](index.html#scrapy.spiders.Spider "scrapy.spiders.Spider"){.reference
    .internal})

-   From [[`Spider`{.xref .py .py-class .docutils .literal
    .notranslate}]{.pre}](index.html#scrapy.spiders.Spider "scrapy.spiders.Spider"){.reference
    .internal} (and subclasses):

    -   [`DOWNLOAD_DELAY`{.docutils .literal .notranslate}]{.pre} (use
        [[download_delay]{.std
        .std-ref}](index.html#spider-download-delay-attribute){.hoverxref
        .tooltip .reference .internal})

    -   [`set_crawler`{.docutils .literal .notranslate}]{.pre} (use
        [`from_crawler()`{.xref .py .py-meth .docutils .literal
        .notranslate}]{.pre})

-   [`scrapy.spiders.spiders`{.docutils .literal .notranslate}]{.pre}
    (use [[`SpiderLoader`{.xref .py .py-class .docutils .literal
    .notranslate}]{.pre}](index.html#scrapy.spiderloader.SpiderLoader "scrapy.spiderloader.SpiderLoader"){.reference
    .internal})

-   [`scrapy.telnet`{.docutils .literal .notranslate}]{.pre} (use
    [[`scrapy.extensions.telnet`{.xref .py .py-mod .docutils .literal
    .notranslate}]{.pre}](index.html#module-scrapy.extensions.telnet "scrapy.extensions.telnet: Telnet console"){.reference
    .internal})

-   From [`scrapy.utils.python`{.docutils .literal .notranslate}]{.pre}:

    -   [`str_to_unicode`{.docutils .literal .notranslate}]{.pre} (use
        [`to_unicode`{.docutils .literal .notranslate}]{.pre})

    -   [`unicode_to_str`{.docutils .literal .notranslate}]{.pre} (use
        [`to_bytes`{.docutils .literal .notranslate}]{.pre})

-   [`scrapy.utils.response.body_or_str`{.docutils .literal
    .notranslate}]{.pre}

The following deprecated settings have also been removed ([issue
3578](https://github.com/scrapy/scrapy/issues/3578){.reference
.external}):

-   [`SPIDER_MANAGER_CLASS`{.docutils .literal .notranslate}]{.pre} (use
    [[`SPIDER_LOADER_CLASS`{.xref .std .std-setting .docutils .literal
    .notranslate}]{.pre}](index.html#std-setting-SPIDER_LOADER_CLASS){.hoverxref
    .tooltip .reference .internal})
:::

::: {#id96 .section}
[]{#id97}

##### Deprecations[¶](#id96 "Permalink to this heading"){.headerlink}

-   The [`queuelib.PriorityQueue`{.docutils .literal
    .notranslate}]{.pre} value for the
    [[`SCHEDULER_PRIORITY_QUEUE`{.xref .std .std-setting .docutils
    .literal
    .notranslate}]{.pre}](index.html#std-setting-SCHEDULER_PRIORITY_QUEUE){.hoverxref
    .tooltip .reference .internal} setting is deprecated. Use
    [`scrapy.pqueues.ScrapyPriorityQueue`{.docutils .literal
    .notranslate}]{.pre} instead.

-   [`process_request`{.docutils .literal .notranslate}]{.pre} callbacks
    passed to [[`Rule`{.xref .py .py-class .docutils .literal
    .notranslate}]{.pre}](index.html#scrapy.spiders.Rule "scrapy.spiders.Rule"){.reference
    .internal} that do not accept two arguments are deprecated.

-   The following modules are deprecated:

    -   [`scrapy.utils.http`{.docutils .literal .notranslate}]{.pre}
        (use
        [w3lib.http](https://w3lib.readthedocs.io/en/latest/w3lib.html#module-w3lib.http){.reference
        .external})

    -   [`scrapy.utils.markup`{.docutils .literal .notranslate}]{.pre}
        (use
        [w3lib.html](https://w3lib.readthedocs.io/en/latest/w3lib.html#module-w3lib.html){.reference
        .external})

    -   [`scrapy.utils.multipart`{.docutils .literal
        .notranslate}]{.pre} (use
        [urllib3](https://urllib3.readthedocs.io/en/latest/index.html){.reference
        .external})

-   The [`scrapy.utils.datatypes.MergeDict`{.docutils .literal
    .notranslate}]{.pre} class is deprecated for Python 3 code bases.
    Use [[`ChainMap`{.xref .py .py-class .docutils .literal
    .notranslate}]{.pre}](https://docs.python.org/3/library/collections.html#collections.ChainMap "(in Python v3.12)"){.reference
    .external} instead. ([issue
    3878](https://github.com/scrapy/scrapy/issues/3878){.reference
    .external})

-   The [`scrapy.utils.gz.is_gzipped`{.docutils .literal
    .notranslate}]{.pre} function is deprecated. Use
    [`scrapy.utils.gz.gzip_magic_number`{.docutils .literal
    .notranslate}]{.pre} instead.
:::

::: {#id98 .section}
##### Other changes[¶](#id98 "Permalink to this heading"){.headerlink}

-   It is now possible to run all tests from the same
    [tox](https://pypi.org/project/tox/){.reference .external}
    environment in parallel; the documentation now covers [[this and
    other ways to run tests]{.std
    .std-ref}](index.html#running-tests){.hoverxref .tooltip .reference
    .internal} ([issue
    3707](https://github.com/scrapy/scrapy/issues/3707){.reference
    .external})

-   It is now possible to generate an API documentation coverage report
    ([issue
    3806](https://github.com/scrapy/scrapy/issues/3806){.reference
    .external}, [issue
    3810](https://github.com/scrapy/scrapy/issues/3810){.reference
    .external}, [issue
    3860](https://github.com/scrapy/scrapy/issues/3860){.reference
    .external})

-   The [[documentation policies]{.std
    .std-ref}](index.html#documentation-policies){.hoverxref .tooltip
    .reference .internal} now require
    [docstrings](https://docs.python.org/3/glossary.html#term-docstring){.reference
    .external} ([issue
    3701](https://github.com/scrapy/scrapy/issues/3701){.reference
    .external}) that follow [PEP
    257](https://www.python.org/dev/peps/pep-0257/){.reference
    .external} ([issue
    3748](https://github.com/scrapy/scrapy/issues/3748){.reference
    .external})

-   Internal fixes and cleanup ([issue
    3629](https://github.com/scrapy/scrapy/issues/3629){.reference
    .external}, [issue
    3643](https://github.com/scrapy/scrapy/issues/3643){.reference
    .external}, [issue
    3684](https://github.com/scrapy/scrapy/issues/3684){.reference
    .external}, [issue
    3698](https://github.com/scrapy/scrapy/issues/3698){.reference
    .external}, [issue
    3734](https://github.com/scrapy/scrapy/issues/3734){.reference
    .external}, [issue
    3735](https://github.com/scrapy/scrapy/issues/3735){.reference
    .external}, [issue
    3736](https://github.com/scrapy/scrapy/issues/3736){.reference
    .external}, [issue
    3737](https://github.com/scrapy/scrapy/issues/3737){.reference
    .external}, [issue
    3809](https://github.com/scrapy/scrapy/issues/3809){.reference
    .external}, [issue
    3821](https://github.com/scrapy/scrapy/issues/3821){.reference
    .external}, [issue
    3825](https://github.com/scrapy/scrapy/issues/3825){.reference
    .external}, [issue
    3827](https://github.com/scrapy/scrapy/issues/3827){.reference
    .external}, [issue
    3833](https://github.com/scrapy/scrapy/issues/3833){.reference
    .external}, [issue
    3857](https://github.com/scrapy/scrapy/issues/3857){.reference
    .external}, [issue
    3877](https://github.com/scrapy/scrapy/issues/3877){.reference
    .external})
:::
:::

::: {#scrapy-1-6-0-2019-01-30 .section}
[]{#release-1-6-0}

#### Scrapy 1.6.0 (2019-01-30)[¶](#scrapy-1-6-0-2019-01-30 "Permalink to this heading"){.headerlink}

Highlights:

-   better Windows support;

-   Python 3.7 compatibility;

-   big documentation improvements, including a switch from
    [`.extract_first()`{.docutils .literal .notranslate}]{.pre} +
    [`.extract()`{.docutils .literal .notranslate}]{.pre} API to
    [`.get()`{.docutils .literal .notranslate}]{.pre} +
    [`.getall()`{.docutils .literal .notranslate}]{.pre} API;

-   feed exports, FilePipeline and MediaPipeline improvements;

-   better extensibility: [[`item_error`{.xref .std .std-signal
    .docutils .literal
    .notranslate}]{.pre}](index.html#std-signal-item_error){.hoverxref
    .tooltip .reference .internal} and
    [[`request_reached_downloader`{.xref .std .std-signal .docutils
    .literal
    .notranslate}]{.pre}](index.html#std-signal-request_reached_downloader){.hoverxref
    .tooltip .reference .internal} signals; [`from_crawler`{.docutils
    .literal .notranslate}]{.pre} support for feed exporters, feed
    storages and dupefilters.

-   [`scrapy.contracts`{.docutils .literal .notranslate}]{.pre} fixes
    and new features;

-   telnet console security improvements, first released as a backport
    in [[Scrapy 1.5.2 (2019-01-22)]{.std
    .std-ref}](#release-1-5-2){.hoverxref .tooltip .reference
    .internal};

-   clean-up of the deprecated code;

-   various bug fixes, small new features and usability improvements
    across the codebase.

::: {#selector-api-changes .section}
##### Selector API changes[¶](#selector-api-changes "Permalink to this heading"){.headerlink}

While these are not changes in Scrapy itself, but rather in the
[parsel](https://github.com/scrapy/parsel){.reference .external} library
which Scrapy uses for xpath/css selectors, these changes are worth
mentioning here. Scrapy now depends on parsel \>= 1.5, and Scrapy
documentation is updated to follow recent [`parsel`{.docutils .literal
.notranslate}]{.pre} API conventions.

Most visible change is that [`.get()`{.docutils .literal
.notranslate}]{.pre} and [`.getall()`{.docutils .literal
.notranslate}]{.pre} selector methods are now preferred over
[`.extract_first()`{.docutils .literal .notranslate}]{.pre} and
[`.extract()`{.docutils .literal .notranslate}]{.pre}. We feel that
these new methods result in a more concise and readable code. See
[[extract() and extract_first()]{.std
.std-ref}](index.html#old-extraction-api){.hoverxref .tooltip .reference
.internal} for more details.

::: {.admonition .note}
Note

There are currently **no plans** to deprecate [`.extract()`{.docutils
.literal .notranslate}]{.pre} and [`.extract_first()`{.docutils .literal
.notranslate}]{.pre} methods.
:::

Another useful new feature is the introduction of
[`Selector.attrib`{.docutils .literal .notranslate}]{.pre} and
[`SelectorList.attrib`{.docutils .literal .notranslate}]{.pre}
properties, which make it easier to get attributes of HTML elements. See
[[Selecting element attributes]{.std
.std-ref}](index.html#selecting-attributes){.hoverxref .tooltip
.reference .internal}.

CSS selectors are cached in parsel \>= 1.5, which makes them faster when
the same CSS path is used many times. This is very common in case of
Scrapy spiders: callbacks are usually called several times, on different
pages.

If you're using custom [`Selector`{.docutils .literal
.notranslate}]{.pre} or [`SelectorList`{.docutils .literal
.notranslate}]{.pre} subclasses, a **backward incompatible** change in
parsel may affect your code. See [parsel
changelog](https://parsel.readthedocs.io/en/latest/history.html){.reference
.external} for a detailed description, as well as for the full list of
improvements.
:::

::: {#telnet-console .section}
##### Telnet console[¶](#telnet-console "Permalink to this heading"){.headerlink}

**Backward incompatible**: Scrapy's telnet console now requires username
and password. See [[Telnet Console]{.std
.std-ref}](index.html#topics-telnetconsole){.hoverxref .tooltip
.reference .internal} for more details. This change fixes a **security
issue**; see [[Scrapy 1.5.2 (2019-01-22)]{.std
.std-ref}](#release-1-5-2){.hoverxref .tooltip .reference .internal}
release notes for details.
:::

::: {#new-extensibility-features .section}
##### New extensibility features[¶](#new-extensibility-features "Permalink to this heading"){.headerlink}

-   [`from_crawler`{.docutils .literal .notranslate}]{.pre} support is
    added to feed exporters and feed storages. This, among other things,
    allows to access Scrapy settings from custom feed storages and
    exporters ([issue
    1605](https://github.com/scrapy/scrapy/issues/1605){.reference
    .external}, [issue
    3348](https://github.com/scrapy/scrapy/issues/3348){.reference
    .external}).

-   [`from_crawler`{.docutils .literal .notranslate}]{.pre} support is
    added to dupefilters ([issue
    2956](https://github.com/scrapy/scrapy/issues/2956){.reference
    .external}); this allows to access e.g. settings or a spider from a
    dupefilter.

-   [[`item_error`{.xref .std .std-signal .docutils .literal
    .notranslate}]{.pre}](index.html#std-signal-item_error){.hoverxref
    .tooltip .reference .internal} is fired when an error happens in a
    pipeline ([issue
    3256](https://github.com/scrapy/scrapy/issues/3256){.reference
    .external});

-   [[`request_reached_downloader`{.xref .std .std-signal .docutils
    .literal
    .notranslate}]{.pre}](index.html#std-signal-request_reached_downloader){.hoverxref
    .tooltip .reference .internal} is fired when Downloader gets a new
    Request; this signal can be useful e.g. for custom Schedulers
    ([issue
    3393](https://github.com/scrapy/scrapy/issues/3393){.reference
    .external}).

-   new SitemapSpider [[`sitemap_filter()`{.xref .py .py-meth .docutils
    .literal
    .notranslate}]{.pre}](index.html#scrapy.spiders.SitemapSpider.sitemap_filter "scrapy.spiders.SitemapSpider.sitemap_filter"){.reference
    .internal} method which allows to select sitemap entries based on
    their attributes in SitemapSpider subclasses ([issue
    3512](https://github.com/scrapy/scrapy/issues/3512){.reference
    .external}).

-   Lazy loading of Downloader Handlers is now optional; this enables
    better initialization error handling in custom Downloader Handlers
    ([issue
    3394](https://github.com/scrapy/scrapy/issues/3394){.reference
    .external}).
:::

::: {#new-filepipeline-and-mediapipeline-features .section}
##### New FilePipeline and MediaPipeline features[¶](#new-filepipeline-and-mediapipeline-features "Permalink to this heading"){.headerlink}

-   Expose more options for S3FilesStore: [[`AWS_ENDPOINT_URL`{.xref
    .std .std-setting .docutils .literal
    .notranslate}]{.pre}](index.html#std-setting-AWS_ENDPOINT_URL){.hoverxref
    .tooltip .reference .internal}, [[`AWS_USE_SSL`{.xref .std
    .std-setting .docutils .literal
    .notranslate}]{.pre}](index.html#std-setting-AWS_USE_SSL){.hoverxref
    .tooltip .reference .internal}, [[`AWS_VERIFY`{.xref .std
    .std-setting .docutils .literal
    .notranslate}]{.pre}](index.html#std-setting-AWS_VERIFY){.hoverxref
    .tooltip .reference .internal}, [[`AWS_REGION_NAME`{.xref .std
    .std-setting .docutils .literal
    .notranslate}]{.pre}](index.html#std-setting-AWS_REGION_NAME){.hoverxref
    .tooltip .reference .internal}. For example, this allows to use
    alternative or self-hosted AWS-compatible providers ([issue
    2609](https://github.com/scrapy/scrapy/issues/2609){.reference
    .external}, [issue
    3548](https://github.com/scrapy/scrapy/issues/3548){.reference
    .external}).

-   ACL support for Google Cloud Storage: [[`FILES_STORE_GCS_ACL`{.xref
    .std .std-setting .docutils .literal
    .notranslate}]{.pre}](index.html#std-setting-FILES_STORE_GCS_ACL){.hoverxref
    .tooltip .reference .internal} and [[`IMAGES_STORE_GCS_ACL`{.xref
    .std .std-setting .docutils .literal
    .notranslate}]{.pre}](index.html#std-setting-IMAGES_STORE_GCS_ACL){.hoverxref
    .tooltip .reference .internal} ([issue
    3199](https://github.com/scrapy/scrapy/issues/3199){.reference
    .external}).
:::

::: {#scrapy-contracts-improvements .section}
##### [`scrapy.contracts`{.docutils .literal .notranslate}]{.pre} improvements[¶](#scrapy-contracts-improvements "Permalink to this heading"){.headerlink}

-   Exceptions in contracts code are handled better ([issue
    3377](https://github.com/scrapy/scrapy/issues/3377){.reference
    .external});

-   [`dont_filter=True`{.docutils .literal .notranslate}]{.pre} is used
    for contract requests, which allows to test different callbacks with
    the same URL ([issue
    3381](https://github.com/scrapy/scrapy/issues/3381){.reference
    .external});

-   [`request_cls`{.docutils .literal .notranslate}]{.pre} attribute in
    Contract subclasses allow to use different Request classes in
    contracts, for example FormRequest ([issue
    3383](https://github.com/scrapy/scrapy/issues/3383){.reference
    .external}).

-   Fixed errback handling in contracts, e.g. for cases where a contract
    is executed for URL which returns non-200 response ([issue
    3371](https://github.com/scrapy/scrapy/issues/3371){.reference
    .external}).
:::

::: {#usability-improvements .section}
##### Usability improvements[¶](#usability-improvements "Permalink to this heading"){.headerlink}

-   more stats for RobotsTxtMiddleware ([issue
    3100](https://github.com/scrapy/scrapy/issues/3100){.reference
    .external})

-   INFO log level is used to show telnet host/port ([issue
    3115](https://github.com/scrapy/scrapy/issues/3115){.reference
    .external})

-   a message is added to IgnoreRequest in RobotsTxtMiddleware ([issue
    3113](https://github.com/scrapy/scrapy/issues/3113){.reference
    .external})

-   better validation of [`url`{.docutils .literal .notranslate}]{.pre}
    argument in [`Response.follow`{.docutils .literal
    .notranslate}]{.pre} ([issue
    3131](https://github.com/scrapy/scrapy/issues/3131){.reference
    .external})

-   non-zero exit code is returned from Scrapy commands when error
    happens on spider initialization ([issue
    3226](https://github.com/scrapy/scrapy/issues/3226){.reference
    .external})

-   Link extraction improvements: "ftp" is added to scheme list ([issue
    3152](https://github.com/scrapy/scrapy/issues/3152){.reference
    .external}); "flv" is added to common video extensions ([issue
    3165](https://github.com/scrapy/scrapy/issues/3165){.reference
    .external})

-   better error message when an exporter is disabled ([issue
    3358](https://github.com/scrapy/scrapy/issues/3358){.reference
    .external});

-   [`scrapy`{.docutils .literal .notranslate}]{.pre}` `{.docutils
    .literal .notranslate}[`shell`{.docutils .literal
    .notranslate}]{.pre}` `{.docutils .literal
    .notranslate}[`--help`{.docutils .literal .notranslate}]{.pre}
    mentions syntax required for local files ([`./file.html`{.docutils
    .literal .notranslate}]{.pre}) - [issue
    3496](https://github.com/scrapy/scrapy/issues/3496){.reference
    .external}.

-   Referer header value is added to RFPDupeFilter log messages ([issue
    3588](https://github.com/scrapy/scrapy/issues/3588){.reference
    .external})
:::

::: {#id99 .section}
##### Bug fixes[¶](#id99 "Permalink to this heading"){.headerlink}

-   fixed issue with extra blank lines in .csv exports under Windows
    ([issue
    3039](https://github.com/scrapy/scrapy/issues/3039){.reference
    .external});

-   proper handling of pickling errors in Python 3 when serializing
    objects for disk queues ([issue
    3082](https://github.com/scrapy/scrapy/issues/3082){.reference
    .external})

-   flags are now preserved when copying Requests ([issue
    3342](https://github.com/scrapy/scrapy/issues/3342){.reference
    .external});

-   FormRequest.from_response clickdata shouldn't ignore elements with
    [`input[type=image]`{.docutils .literal .notranslate}]{.pre} ([issue
    3153](https://github.com/scrapy/scrapy/issues/3153){.reference
    .external}).

-   FormRequest.from_response should preserve duplicate keys ([issue
    3247](https://github.com/scrapy/scrapy/issues/3247){.reference
    .external})
:::

::: {#documentation-improvements .section}
##### Documentation improvements[¶](#documentation-improvements "Permalink to this heading"){.headerlink}

-   Docs are re-written to suggest .get/.getall API instead of
    .extract/.extract_first. Also, [[Selectors]{.std
    .std-ref}](index.html#topics-selectors){.hoverxref .tooltip
    .reference .internal} docs are updated and re-structured to match
    latest parsel docs; they now contain more topics, such as
    [[Selecting element attributes]{.std
    .std-ref}](index.html#selecting-attributes){.hoverxref .tooltip
    .reference .internal} or [[Extensions to CSS Selectors]{.std
    .std-ref}](index.html#topics-selectors-css-extensions){.hoverxref
    .tooltip .reference .internal} ([issue
    3390](https://github.com/scrapy/scrapy/issues/3390){.reference
    .external}).

-   [[Using your browser's Developer Tools for scraping]{.std
    .std-ref}](index.html#topics-developer-tools){.hoverxref .tooltip
    .reference .internal} is a new tutorial which replaces old Firefox
    and Firebug tutorials ([issue
    3400](https://github.com/scrapy/scrapy/issues/3400){.reference
    .external}).

-   SCRAPY_PROJECT environment variable is documented ([issue
    3518](https://github.com/scrapy/scrapy/issues/3518){.reference
    .external});

-   troubleshooting section is added to install instructions ([issue
    3517](https://github.com/scrapy/scrapy/issues/3517){.reference
    .external});

-   improved links to beginner resources in the tutorial ([issue
    3367](https://github.com/scrapy/scrapy/issues/3367){.reference
    .external}, [issue
    3468](https://github.com/scrapy/scrapy/issues/3468){.reference
    .external});

-   fixed [[`RETRY_HTTP_CODES`{.xref .std .std-setting .docutils
    .literal
    .notranslate}]{.pre}](index.html#std-setting-RETRY_HTTP_CODES){.hoverxref
    .tooltip .reference .internal} default values in docs ([issue
    3335](https://github.com/scrapy/scrapy/issues/3335){.reference
    .external});

-   remove unused [`DEPTH_STATS`{.docutils .literal .notranslate}]{.pre}
    option from docs ([issue
    3245](https://github.com/scrapy/scrapy/issues/3245){.reference
    .external});

-   other cleanups ([issue
    3347](https://github.com/scrapy/scrapy/issues/3347){.reference
    .external}, [issue
    3350](https://github.com/scrapy/scrapy/issues/3350){.reference
    .external}, [issue
    3445](https://github.com/scrapy/scrapy/issues/3445){.reference
    .external}, [issue
    3544](https://github.com/scrapy/scrapy/issues/3544){.reference
    .external}, [issue
    3605](https://github.com/scrapy/scrapy/issues/3605){.reference
    .external}).
:::

::: {#id100 .section}
##### Deprecation removals[¶](#id100 "Permalink to this heading"){.headerlink}

Compatibility shims for pre-1.0 Scrapy module names are removed ([issue
3318](https://github.com/scrapy/scrapy/issues/3318){.reference
.external}):

-   [`scrapy.command`{.docutils .literal .notranslate}]{.pre}

-   [`scrapy.contrib`{.docutils .literal .notranslate}]{.pre} (with all
    submodules)

-   [`scrapy.contrib_exp`{.docutils .literal .notranslate}]{.pre} (with
    all submodules)

-   [`scrapy.dupefilter`{.docutils .literal .notranslate}]{.pre}

-   [`scrapy.linkextractor`{.docutils .literal .notranslate}]{.pre}

-   [`scrapy.project`{.docutils .literal .notranslate}]{.pre}

-   [`scrapy.spider`{.docutils .literal .notranslate}]{.pre}

-   [`scrapy.spidermanager`{.docutils .literal .notranslate}]{.pre}

-   [`scrapy.squeue`{.docutils .literal .notranslate}]{.pre}

-   [`scrapy.stats`{.docutils .literal .notranslate}]{.pre}

-   [`scrapy.statscol`{.docutils .literal .notranslate}]{.pre}

-   [`scrapy.utils.decorator`{.docutils .literal .notranslate}]{.pre}

See [[Module Relocations]{.std
.std-ref}](#module-relocations){.hoverxref .tooltip .reference
.internal} for more information, or use suggestions from Scrapy 1.5.x
deprecation warnings to update your code.

Other deprecation removals:

-   Deprecated scrapy.interfaces.ISpiderManager is removed; please use
    scrapy.interfaces.ISpiderLoader.

-   Deprecated [`CrawlerSettings`{.docutils .literal
    .notranslate}]{.pre} class is removed ([issue
    3327](https://github.com/scrapy/scrapy/issues/3327){.reference
    .external}).

-   Deprecated [`Settings.overrides`{.docutils .literal
    .notranslate}]{.pre} and [`Settings.defaults`{.docutils .literal
    .notranslate}]{.pre} attributes are removed ([issue
    3327](https://github.com/scrapy/scrapy/issues/3327){.reference
    .external}, [issue
    3359](https://github.com/scrapy/scrapy/issues/3359){.reference
    .external}).
:::

::: {#other-improvements-cleanups .section}
##### Other improvements, cleanups[¶](#other-improvements-cleanups "Permalink to this heading"){.headerlink}

-   All Scrapy tests now pass on Windows; Scrapy testing suite is
    executed in a Windows environment on CI ([issue
    3315](https://github.com/scrapy/scrapy/issues/3315){.reference
    .external}).

-   Python 3.7 support ([issue
    3326](https://github.com/scrapy/scrapy/issues/3326){.reference
    .external}, [issue
    3150](https://github.com/scrapy/scrapy/issues/3150){.reference
    .external}, [issue
    3547](https://github.com/scrapy/scrapy/issues/3547){.reference
    .external}).

-   Testing and CI fixes ([issue
    3526](https://github.com/scrapy/scrapy/issues/3526){.reference
    .external}, [issue
    3538](https://github.com/scrapy/scrapy/issues/3538){.reference
    .external}, [issue
    3308](https://github.com/scrapy/scrapy/issues/3308){.reference
    .external}, [issue
    3311](https://github.com/scrapy/scrapy/issues/3311){.reference
    .external}, [issue
    3309](https://github.com/scrapy/scrapy/issues/3309){.reference
    .external}, [issue
    3305](https://github.com/scrapy/scrapy/issues/3305){.reference
    .external}, [issue
    3210](https://github.com/scrapy/scrapy/issues/3210){.reference
    .external}, [issue
    3299](https://github.com/scrapy/scrapy/issues/3299){.reference
    .external})

-   [`scrapy.http.cookies.CookieJar.clear`{.docutils .literal
    .notranslate}]{.pre} accepts "domain", "path" and "name" optional
    arguments ([issue
    3231](https://github.com/scrapy/scrapy/issues/3231){.reference
    .external}).

-   additional files are included to sdist ([issue
    3495](https://github.com/scrapy/scrapy/issues/3495){.reference
    .external});

-   code style fixes ([issue
    3405](https://github.com/scrapy/scrapy/issues/3405){.reference
    .external}, [issue
    3304](https://github.com/scrapy/scrapy/issues/3304){.reference
    .external});

-   unneeded .strip() call is removed ([issue
    3519](https://github.com/scrapy/scrapy/issues/3519){.reference
    .external});

-   collections.deque is used to store MiddlewareManager methods instead
    of a list ([issue
    3476](https://github.com/scrapy/scrapy/issues/3476){.reference
    .external})
:::
:::

::: {#scrapy-1-5-2-2019-01-22 .section}
[]{#release-1-5-2}

#### Scrapy 1.5.2 (2019-01-22)[¶](#scrapy-1-5-2-2019-01-22 "Permalink to this heading"){.headerlink}

-   *Security bugfix*: Telnet console extension can be easily exploited
    by rogue websites POSTing content to
    [http://localhost:6023](http://localhost:6023){.reference
    .external}, we haven't found a way to exploit it from Scrapy, but it
    is very easy to trick a browser to do so and elevates the risk for
    local development environment.

    *The fix is backward incompatible*, it enables telnet user-password
    authentication by default with a random generated password. If you
    can't upgrade right away, please consider setting
    [[`TELNETCONSOLE_PORT`{.xref .std .std-setting .docutils .literal
    .notranslate}]{.pre}](index.html#std-setting-TELNETCONSOLE_PORT){.hoverxref
    .tooltip .reference .internal} out of its default value.

    See [[telnet console]{.std
    .std-ref}](index.html#topics-telnetconsole){.hoverxref .tooltip
    .reference .internal} documentation for more info

-   Backport CI build failure under GCE environment due to boto import
    error.
:::

::: {#scrapy-1-5-1-2018-07-12 .section}
[]{#release-1-5-1}

#### Scrapy 1.5.1 (2018-07-12)[¶](#scrapy-1-5-1-2018-07-12 "Permalink to this heading"){.headerlink}

This is a maintenance release with important bug fixes, but no new
features:

-   [`O(N^2)`{.docutils .literal .notranslate}]{.pre} gzip decompression
    issue which affected Python 3 and PyPy is fixed ([issue
    3281](https://github.com/scrapy/scrapy/issues/3281){.reference
    .external});

-   skipping of TLS validation errors is improved ([issue
    3166](https://github.com/scrapy/scrapy/issues/3166){.reference
    .external});

-   Ctrl-C handling is fixed in Python 3.5+ ([issue
    3096](https://github.com/scrapy/scrapy/issues/3096){.reference
    .external});

-   testing fixes ([issue
    3092](https://github.com/scrapy/scrapy/issues/3092){.reference
    .external}, [issue
    3263](https://github.com/scrapy/scrapy/issues/3263){.reference
    .external});

-   documentation improvements ([issue
    3058](https://github.com/scrapy/scrapy/issues/3058){.reference
    .external}, [issue
    3059](https://github.com/scrapy/scrapy/issues/3059){.reference
    .external}, [issue
    3089](https://github.com/scrapy/scrapy/issues/3089){.reference
    .external}, [issue
    3123](https://github.com/scrapy/scrapy/issues/3123){.reference
    .external}, [issue
    3127](https://github.com/scrapy/scrapy/issues/3127){.reference
    .external}, [issue
    3189](https://github.com/scrapy/scrapy/issues/3189){.reference
    .external}, [issue
    3224](https://github.com/scrapy/scrapy/issues/3224){.reference
    .external}, [issue
    3280](https://github.com/scrapy/scrapy/issues/3280){.reference
    .external}, [issue
    3279](https://github.com/scrapy/scrapy/issues/3279){.reference
    .external}, [issue
    3201](https://github.com/scrapy/scrapy/issues/3201){.reference
    .external}, [issue
    3260](https://github.com/scrapy/scrapy/issues/3260){.reference
    .external}, [issue
    3284](https://github.com/scrapy/scrapy/issues/3284){.reference
    .external}, [issue
    3298](https://github.com/scrapy/scrapy/issues/3298){.reference
    .external}, [issue
    3294](https://github.com/scrapy/scrapy/issues/3294){.reference
    .external}).
:::

::: {#scrapy-1-5-0-2017-12-29 .section}
[]{#release-1-5-0}

#### Scrapy 1.5.0 (2017-12-29)[¶](#scrapy-1-5-0-2017-12-29 "Permalink to this heading"){.headerlink}

This release brings small new features and improvements across the
codebase. Some highlights:

-   Google Cloud Storage is supported in FilesPipeline and
    ImagesPipeline.

-   Crawling with proxy servers becomes more efficient, as connections
    to proxies can be reused now.

-   Warnings, exception and logging messages are improved to make
    debugging easier.

-   [`scrapy`{.docutils .literal .notranslate}]{.pre}` `{.docutils
    .literal .notranslate}[`parse`{.docutils .literal
    .notranslate}]{.pre} command now allows to set custom request meta
    via [`--meta`{.docutils .literal .notranslate}]{.pre} argument.

-   Compatibility with Python 3.6, PyPy and PyPy3 is improved; PyPy and
    PyPy3 are now supported officially, by running tests on CI.

-   Better default handling of HTTP 308, 522 and 524 status codes.

-   Documentation is improved, as usual.

::: {#id101 .section}
##### Backward Incompatible Changes[¶](#id101 "Permalink to this heading"){.headerlink}

-   Scrapy 1.5 drops support for Python 3.3.

-   Default Scrapy User-Agent now uses https link to scrapy.org ([issue
    2983](https://github.com/scrapy/scrapy/issues/2983){.reference
    .external}). **This is technically backward-incompatible**; override
    [[`USER_AGENT`{.xref .std .std-setting .docutils .literal
    .notranslate}]{.pre}](index.html#std-setting-USER_AGENT){.hoverxref
    .tooltip .reference .internal} if you relied on old value.

-   Logging of settings overridden by [`custom_settings`{.docutils
    .literal .notranslate}]{.pre} is fixed; **this is technically
    backward-incompatible** because the logger changes from
    [`[scrapy.utils.log]`{.docutils .literal .notranslate}]{.pre} to
    [`[scrapy.crawler]`{.docutils .literal .notranslate}]{.pre}. If
    you're parsing Scrapy logs, please update your log parsers ([issue
    1343](https://github.com/scrapy/scrapy/issues/1343){.reference
    .external}).

-   LinkExtractor now ignores [`m4v`{.docutils .literal
    .notranslate}]{.pre} extension by default, this is change in
    behavior.

-   522 and 524 status codes are added to [`RETRY_HTTP_CODES`{.docutils
    .literal .notranslate}]{.pre} ([issue
    2851](https://github.com/scrapy/scrapy/issues/2851){.reference
    .external})
:::

::: {#id102 .section}
##### New features[¶](#id102 "Permalink to this heading"){.headerlink}

-   Support [`<link>`{.docutils .literal .notranslate}]{.pre} tags in
    [`Response.follow`{.docutils .literal .notranslate}]{.pre} ([issue
    2785](https://github.com/scrapy/scrapy/issues/2785){.reference
    .external})

-   Support for [`ptpython`{.docutils .literal .notranslate}]{.pre} REPL
    ([issue
    2654](https://github.com/scrapy/scrapy/issues/2654){.reference
    .external})

-   Google Cloud Storage support for FilesPipeline and ImagesPipeline
    ([issue
    2923](https://github.com/scrapy/scrapy/issues/2923){.reference
    .external}).

-   New [`--meta`{.docutils .literal .notranslate}]{.pre} option of the
    "scrapy parse" command allows to pass additional request.meta
    ([issue
    2883](https://github.com/scrapy/scrapy/issues/2883){.reference
    .external})

-   Populate spider variable when using
    [`shell.inspect_response`{.docutils .literal .notranslate}]{.pre}
    ([issue
    2812](https://github.com/scrapy/scrapy/issues/2812){.reference
    .external})

-   Handle HTTP 308 Permanent Redirect ([issue
    2844](https://github.com/scrapy/scrapy/issues/2844){.reference
    .external})

-   Add 522 and 524 to [`RETRY_HTTP_CODES`{.docutils .literal
    .notranslate}]{.pre} ([issue
    2851](https://github.com/scrapy/scrapy/issues/2851){.reference
    .external})

-   Log versions information at startup ([issue
    2857](https://github.com/scrapy/scrapy/issues/2857){.reference
    .external})

-   [`scrapy.mail.MailSender`{.docutils .literal .notranslate}]{.pre}
    now works in Python 3 (it requires Twisted 17.9.0)

-   Connections to proxy servers are reused ([issue
    2743](https://github.com/scrapy/scrapy/issues/2743){.reference
    .external})

-   Add template for a downloader middleware ([issue
    2755](https://github.com/scrapy/scrapy/issues/2755){.reference
    .external})

-   Explicit message for NotImplementedError when parse callback not
    defined ([issue
    2831](https://github.com/scrapy/scrapy/issues/2831){.reference
    .external})

-   CrawlerProcess got an option to disable installation of root log
    handler ([issue
    2921](https://github.com/scrapy/scrapy/issues/2921){.reference
    .external})

-   LinkExtractor now ignores [`m4v`{.docutils .literal
    .notranslate}]{.pre} extension by default

-   Better log messages for responses over [[`DOWNLOAD_WARNSIZE`{.xref
    .std .std-setting .docutils .literal
    .notranslate}]{.pre}](index.html#std-setting-DOWNLOAD_WARNSIZE){.hoverxref
    .tooltip .reference .internal} and [[`DOWNLOAD_MAXSIZE`{.xref .std
    .std-setting .docutils .literal
    .notranslate}]{.pre}](index.html#std-setting-DOWNLOAD_MAXSIZE){.hoverxref
    .tooltip .reference .internal} limits ([issue
    2927](https://github.com/scrapy/scrapy/issues/2927){.reference
    .external})

-   Show warning when a URL is put to
    [`Spider.allowed_domains`{.docutils .literal .notranslate}]{.pre}
    instead of a domain ([issue
    2250](https://github.com/scrapy/scrapy/issues/2250){.reference
    .external}).
:::

::: {#id103 .section}
##### Bug fixes[¶](#id103 "Permalink to this heading"){.headerlink}

-   Fix logging of settings overridden by [`custom_settings`{.docutils
    .literal .notranslate}]{.pre}; **this is technically
    backward-incompatible** because the logger changes from
    [`[scrapy.utils.log]`{.docutils .literal .notranslate}]{.pre} to
    [`[scrapy.crawler]`{.docutils .literal .notranslate}]{.pre}, so
    please update your log parsers if needed ([issue
    1343](https://github.com/scrapy/scrapy/issues/1343){.reference
    .external})

-   Default Scrapy User-Agent now uses https link to scrapy.org ([issue
    2983](https://github.com/scrapy/scrapy/issues/2983){.reference
    .external}). **This is technically backward-incompatible**; override
    [[`USER_AGENT`{.xref .std .std-setting .docutils .literal
    .notranslate}]{.pre}](index.html#std-setting-USER_AGENT){.hoverxref
    .tooltip .reference .internal} if you relied on old value.

-   Fix PyPy and PyPy3 test failures, support them officially ([issue
    2793](https://github.com/scrapy/scrapy/issues/2793){.reference
    .external}, [issue
    2935](https://github.com/scrapy/scrapy/issues/2935){.reference
    .external}, [issue
    2990](https://github.com/scrapy/scrapy/issues/2990){.reference
    .external}, [issue
    3050](https://github.com/scrapy/scrapy/issues/3050){.reference
    .external}, [issue
    2213](https://github.com/scrapy/scrapy/issues/2213){.reference
    .external}, [issue
    3048](https://github.com/scrapy/scrapy/issues/3048){.reference
    .external})

-   Fix DNS resolver when [`DNSCACHE_ENABLED=False`{.docutils .literal
    .notranslate}]{.pre} ([issue
    2811](https://github.com/scrapy/scrapy/issues/2811){.reference
    .external})

-   Add [`cryptography`{.docutils .literal .notranslate}]{.pre} for
    Debian Jessie tox test env ([issue
    2848](https://github.com/scrapy/scrapy/issues/2848){.reference
    .external})

-   Add verification to check if Request callback is callable ([issue
    2766](https://github.com/scrapy/scrapy/issues/2766){.reference
    .external})

-   Port [`extras/qpsclient.py`{.docutils .literal .notranslate}]{.pre}
    to Python 3 ([issue
    2849](https://github.com/scrapy/scrapy/issues/2849){.reference
    .external})

-   Use getfullargspec under the scenes for Python 3 to stop
    DeprecationWarning ([issue
    2862](https://github.com/scrapy/scrapy/issues/2862){.reference
    .external})

-   Update deprecated test aliases ([issue
    2876](https://github.com/scrapy/scrapy/issues/2876){.reference
    .external})

-   Fix [`SitemapSpider`{.docutils .literal .notranslate}]{.pre} support
    for alternate links ([issue
    2853](https://github.com/scrapy/scrapy/issues/2853){.reference
    .external})
:::

::: {#docs .section}
##### Docs[¶](#docs "Permalink to this heading"){.headerlink}

-   Added missing bullet point for the
    [`AUTOTHROTTLE_TARGET_CONCURRENCY`{.docutils .literal
    .notranslate}]{.pre} setting. ([issue
    2756](https://github.com/scrapy/scrapy/issues/2756){.reference
    .external})

-   Update Contributing docs, document new support channels ([issue
    2762](https://github.com/scrapy/scrapy/issues/2762){.reference
    .external}, issue:3038)

-   Include references to Scrapy subreddit in the docs

-   Fix broken links; use [https://](https://){.reference .external} for
    external links ([issue
    2978](https://github.com/scrapy/scrapy/issues/2978){.reference
    .external}, [issue
    2982](https://github.com/scrapy/scrapy/issues/2982){.reference
    .external}, [issue
    2958](https://github.com/scrapy/scrapy/issues/2958){.reference
    .external})

-   Document CloseSpider extension better ([issue
    2759](https://github.com/scrapy/scrapy/issues/2759){.reference
    .external})

-   Use [`pymongo.collection.Collection.insert_one()`{.docutils .literal
    .notranslate}]{.pre} in MongoDB example ([issue
    2781](https://github.com/scrapy/scrapy/issues/2781){.reference
    .external})

-   Spelling mistake and typos ([issue
    2828](https://github.com/scrapy/scrapy/issues/2828){.reference
    .external}, [issue
    2837](https://github.com/scrapy/scrapy/issues/2837){.reference
    .external}, [issue
    2884](https://github.com/scrapy/scrapy/issues/2884){.reference
    .external}, [issue
    2924](https://github.com/scrapy/scrapy/issues/2924){.reference
    .external})

-   Clarify [`CSVFeedSpider.headers`{.docutils .literal
    .notranslate}]{.pre} documentation ([issue
    2826](https://github.com/scrapy/scrapy/issues/2826){.reference
    .external})

-   Document [`DontCloseSpider`{.docutils .literal .notranslate}]{.pre}
    exception and clarify [`spider_idle`{.docutils .literal
    .notranslate}]{.pre} ([issue
    2791](https://github.com/scrapy/scrapy/issues/2791){.reference
    .external})

-   Update "Releases" section in README ([issue
    2764](https://github.com/scrapy/scrapy/issues/2764){.reference
    .external})

-   Fix rst syntax in [`DOWNLOAD_FAIL_ON_DATALOSS`{.docutils .literal
    .notranslate}]{.pre} docs ([issue
    2763](https://github.com/scrapy/scrapy/issues/2763){.reference
    .external})

-   Small fix in description of startproject arguments ([issue
    2866](https://github.com/scrapy/scrapy/issues/2866){.reference
    .external})

-   Clarify data types in Response.body docs ([issue
    2922](https://github.com/scrapy/scrapy/issues/2922){.reference
    .external})

-   Add a note about [`request.meta['depth']`{.docutils .literal
    .notranslate}]{.pre} to DepthMiddleware docs ([issue
    2374](https://github.com/scrapy/scrapy/issues/2374){.reference
    .external})

-   Add a note about [`request.meta['dont_merge_cookies']`{.docutils
    .literal .notranslate}]{.pre} to CookiesMiddleware docs ([issue
    2999](https://github.com/scrapy/scrapy/issues/2999){.reference
    .external})

-   Up-to-date example of project structure ([issue
    2964](https://github.com/scrapy/scrapy/issues/2964){.reference
    .external}, [issue
    2976](https://github.com/scrapy/scrapy/issues/2976){.reference
    .external})

-   A better example of ItemExporters usage ([issue
    2989](https://github.com/scrapy/scrapy/issues/2989){.reference
    .external})

-   Document [`from_crawler`{.docutils .literal .notranslate}]{.pre}
    methods for spider and downloader middlewares ([issue
    3019](https://github.com/scrapy/scrapy/issues/3019){.reference
    .external})
:::
:::

::: {#scrapy-1-4-0-2017-05-18 .section}
[]{#release-1-4-0}

#### Scrapy 1.4.0 (2017-05-18)[¶](#scrapy-1-4-0-2017-05-18 "Permalink to this heading"){.headerlink}

Scrapy 1.4 does not bring that many breathtaking new features but quite
a few handy improvements nonetheless.

Scrapy now supports anonymous FTP sessions with customizable user and
password via the new [[`FTP_USER`{.xref .std .std-setting .docutils
.literal
.notranslate}]{.pre}](index.html#std-setting-FTP_USER){.hoverxref
.tooltip .reference .internal} and [[`FTP_PASSWORD`{.xref .std
.std-setting .docutils .literal
.notranslate}]{.pre}](index.html#std-setting-FTP_PASSWORD){.hoverxref
.tooltip .reference .internal} settings. And if you're using Twisted
version 17.1.0 or above, FTP is now available with Python 3.

There's a new [[`response.follow`{.xref .py .py-meth .docutils .literal
.notranslate}]{.pre}](index.html#scrapy.http.TextResponse.follow "scrapy.http.TextResponse.follow"){.reference
.internal} method for creating requests; **it is now a recommended way
to create Requests in Scrapy spiders**. This method makes it easier to
write correct spiders; [`response.follow`{.docutils .literal
.notranslate}]{.pre} has several advantages over creating
[`scrapy.Request`{.docutils .literal .notranslate}]{.pre} objects
directly:

-   it handles relative URLs;

-   it works properly with non-ascii URLs on non-UTF8 pages;

-   in addition to absolute and relative URLs it supports Selectors; for
    [`<a>`{.docutils .literal .notranslate}]{.pre} elements it can also
    extract their href values.

For example, instead of this:

::: {.highlight-default .notranslate}
::: highlight
    for href in response.css('li.page a::attr(href)').extract():
        url = response.urljoin(href)
        yield scrapy.Request(url, self.parse, encoding=response.encoding)
:::
:::

One can now write this:

::: {.highlight-default .notranslate}
::: highlight
    for a in response.css('li.page a'):
        yield response.follow(a, self.parse)
:::
:::

Link extractors are also improved. They work similarly to what a regular
modern browser would do: leading and trailing whitespace are removed
from attributes (think [`href="`{.docutils .literal
.notranslate}]{.pre}`   `{.docutils .literal
.notranslate}[`http://example.com"`{.docutils .literal
.notranslate}]{.pre}) when building [`Link`{.docutils .literal
.notranslate}]{.pre} objects. This whitespace-stripping also happens for
[`action`{.docutils .literal .notranslate}]{.pre} attributes with
[`FormRequest`{.docutils .literal .notranslate}]{.pre}.

**Please also note that link extractors do not canonicalize URLs by
default anymore.** This was puzzling users every now and then, and it's
not what browsers do in fact, so we removed that extra transformation on
extracted links.

For those of you wanting more control on the [`Referer:`{.docutils
.literal .notranslate}]{.pre} header that Scrapy sends when following
links, you can set your own [`Referrer`{.docutils .literal
.notranslate}]{.pre}` `{.docutils .literal
.notranslate}[`Policy`{.docutils .literal .notranslate}]{.pre}. Prior to
Scrapy 1.4, the default [`RefererMiddleware`{.docutils .literal
.notranslate}]{.pre} would simply and blindly set it to the URL of the
response that generated the HTTP request (which could leak information
on your URL seeds). By default, Scrapy now behaves much like your
regular browser does. And this policy is fully customizable with W3C
standard values (or with something really custom of your own if you
wish). See [[`REFERRER_POLICY`{.xref .std .std-setting .docutils
.literal
.notranslate}]{.pre}](index.html#std-setting-REFERRER_POLICY){.hoverxref
.tooltip .reference .internal} for details.

To make Scrapy spiders easier to debug, Scrapy logs more stats by
default in 1.4: memory usage stats, detailed retry stats, detailed HTTP
error code stats. A similar change is that HTTP cache path is also
visible in logs now.

Last but not least, Scrapy now has the option to make JSON and XML items
more human-readable, with newlines between items and even custom
indenting offset, using the new [[`FEED_EXPORT_INDENT`{.xref .std
.std-setting .docutils .literal
.notranslate}]{.pre}](index.html#std-setting-FEED_EXPORT_INDENT){.hoverxref
.tooltip .reference .internal} setting.

Enjoy! (Or read on for the rest of changes in this release.)

::: {#deprecations-and-backward-incompatible-changes .section}
##### Deprecations and Backward Incompatible Changes[¶](#deprecations-and-backward-incompatible-changes "Permalink to this heading"){.headerlink}

-   Default to [`canonicalize=False`{.docutils .literal
    .notranslate}]{.pre} in
    [[`scrapy.linkextractors.LinkExtractor`{.xref .py .py-class
    .docutils .literal
    .notranslate}]{.pre}](index.html#scrapy.linkextractors.lxmlhtml.LxmlLinkExtractor "scrapy.linkextractors.lxmlhtml.LxmlLinkExtractor"){.reference
    .internal} ([issue
    2537](https://github.com/scrapy/scrapy/issues/2537){.reference
    .external}, fixes [issue
    1941](https://github.com/scrapy/scrapy/issues/1941){.reference
    .external} and [issue
    1982](https://github.com/scrapy/scrapy/issues/1982){.reference
    .external}): **warning, this is technically backward-incompatible**

-   Enable memusage extension by default ([issue
    2539](https://github.com/scrapy/scrapy/issues/2539){.reference
    .external}, fixes [issue
    2187](https://github.com/scrapy/scrapy/issues/2187){.reference
    .external}); **this is technically backward-incompatible** so please
    check if you have any non-default [`MEMUSAGE_***`{.docutils .literal
    .notranslate}]{.pre} options set.

-   [`EDITOR`{.docutils .literal .notranslate}]{.pre} environment
    variable now takes precedence over [`EDITOR`{.docutils .literal
    .notranslate}]{.pre} option defined in settings.py ([issue
    1829](https://github.com/scrapy/scrapy/issues/1829){.reference
    .external}); Scrapy default settings no longer depend on environment
    variables. **This is technically a backward incompatible change**.

-   [`Spider.make_requests_from_url`{.docutils .literal
    .notranslate}]{.pre} is deprecated ([issue
    1728](https://github.com/scrapy/scrapy/issues/1728){.reference
    .external}, fixes [issue
    1495](https://github.com/scrapy/scrapy/issues/1495){.reference
    .external}).
:::

::: {#id104 .section}
##### New Features[¶](#id104 "Permalink to this heading"){.headerlink}

-   Accept proxy credentials in [[`proxy`{.xref .std .std-reqmeta
    .docutils .literal
    .notranslate}]{.pre}](index.html#std-reqmeta-proxy){.hoverxref
    .tooltip .reference .internal} request meta key ([issue
    2526](https://github.com/scrapy/scrapy/issues/2526){.reference
    .external})

-   Support
    [brotli-compressed](https://www.ietf.org/rfc/rfc7932.txt){.reference
    .external} content; requires optional
    [brotlipy](https://github.com/python-hyper/brotlipy/){.reference
    .external} ([issue
    2535](https://github.com/scrapy/scrapy/issues/2535){.reference
    .external})

-   New [[response.follow]{.std
    .std-ref}](index.html#response-follow-example){.hoverxref .tooltip
    .reference .internal} shortcut for creating requests ([issue
    1940](https://github.com/scrapy/scrapy/issues/1940){.reference
    .external})

-   Added [`flags`{.docutils .literal .notranslate}]{.pre} argument and
    attribute to [[`Request`{.xref .py .py-class .docutils .literal
    .notranslate}]{.pre}](index.html#scrapy.http.Request "scrapy.http.Request"){.reference
    .internal} objects ([issue
    2047](https://github.com/scrapy/scrapy/issues/2047){.reference
    .external})

-   Support Anonymous FTP ([issue
    2342](https://github.com/scrapy/scrapy/issues/2342){.reference
    .external})

-   Added [`retry/count`{.docutils .literal .notranslate}]{.pre},
    [`retry/max_reached`{.docutils .literal .notranslate}]{.pre} and
    [`retry/reason_count/<reason>`{.docutils .literal
    .notranslate}]{.pre} stats to [[`RetryMiddleware`{.xref .py
    .py-class .docutils .literal
    .notranslate}]{.pre}](index.html#scrapy.downloadermiddlewares.retry.RetryMiddleware "scrapy.downloadermiddlewares.retry.RetryMiddleware"){.reference
    .internal} ([issue
    2543](https://github.com/scrapy/scrapy/issues/2543){.reference
    .external})

-   Added [`httperror/response_ignored_count`{.docutils .literal
    .notranslate}]{.pre} and
    [`httperror/response_ignored_status_count/<status>`{.docutils
    .literal .notranslate}]{.pre} stats to [[`HttpErrorMiddleware`{.xref
    .py .py-class .docutils .literal
    .notranslate}]{.pre}](index.html#scrapy.spidermiddlewares.httperror.HttpErrorMiddleware "scrapy.spidermiddlewares.httperror.HttpErrorMiddleware"){.reference
    .internal} ([issue
    2566](https://github.com/scrapy/scrapy/issues/2566){.reference
    .external})

-   Customizable [[`Referrer`{.xref .std .std-setting .docutils .literal
    .notranslate}]{.pre}` `{.xref .std .std-setting .docutils .literal
    .notranslate}[`policy`{.xref .std .std-setting .docutils .literal
    .notranslate}]{.pre}](index.html#std-setting-REFERRER_POLICY){.hoverxref
    .tooltip .reference .internal} in [[`RefererMiddleware`{.xref .py
    .py-class .docutils .literal
    .notranslate}]{.pre}](index.html#scrapy.spidermiddlewares.referer.RefererMiddleware "scrapy.spidermiddlewares.referer.RefererMiddleware"){.reference
    .internal} ([issue
    2306](https://github.com/scrapy/scrapy/issues/2306){.reference
    .external})

-   New [`data:`{.docutils .literal .notranslate}]{.pre} URI download
    handler ([issue
    2334](https://github.com/scrapy/scrapy/issues/2334){.reference
    .external}, fixes [issue
    2156](https://github.com/scrapy/scrapy/issues/2156){.reference
    .external})

-   Log cache directory when HTTP Cache is used ([issue
    2611](https://github.com/scrapy/scrapy/issues/2611){.reference
    .external}, fixes [issue
    2604](https://github.com/scrapy/scrapy/issues/2604){.reference
    .external})

-   Warn users when project contains duplicate spider names (fixes
    [issue
    2181](https://github.com/scrapy/scrapy/issues/2181){.reference
    .external})

-   [`scrapy.utils.datatypes.CaselessDict`{.docutils .literal
    .notranslate}]{.pre} now accepts [`Mapping`{.docutils .literal
    .notranslate}]{.pre} instances and not only dicts ([issue
    2646](https://github.com/scrapy/scrapy/issues/2646){.reference
    .external})

-   [[Media downloads]{.std
    .std-ref}](index.html#topics-media-pipeline){.hoverxref .tooltip
    .reference .internal}, with [[`FilesPipeline`{.xref .py .py-class
    .docutils .literal
    .notranslate}]{.pre}](index.html#scrapy.pipelines.files.FilesPipeline "scrapy.pipelines.files.FilesPipeline"){.reference
    .internal} or [[`ImagesPipeline`{.xref .py .py-class .docutils
    .literal
    .notranslate}]{.pre}](index.html#scrapy.pipelines.images.ImagesPipeline "scrapy.pipelines.images.ImagesPipeline"){.reference
    .internal}, can now optionally handle HTTP redirects using the new
    [[`MEDIA_ALLOW_REDIRECTS`{.xref .std .std-setting .docutils .literal
    .notranslate}]{.pre}](index.html#std-setting-MEDIA_ALLOW_REDIRECTS){.hoverxref
    .tooltip .reference .internal} setting ([issue
    2616](https://github.com/scrapy/scrapy/issues/2616){.reference
    .external}, fixes [issue
    2004](https://github.com/scrapy/scrapy/issues/2004){.reference
    .external})

-   Accept non-complete responses from websites using a new
    [[`DOWNLOAD_FAIL_ON_DATALOSS`{.xref .std .std-setting .docutils
    .literal
    .notranslate}]{.pre}](index.html#std-setting-DOWNLOAD_FAIL_ON_DATALOSS){.hoverxref
    .tooltip .reference .internal} setting ([issue
    2590](https://github.com/scrapy/scrapy/issues/2590){.reference
    .external}, fixes [issue
    2586](https://github.com/scrapy/scrapy/issues/2586){.reference
    .external})

-   Optional pretty-printing of JSON and XML items via
    [[`FEED_EXPORT_INDENT`{.xref .std .std-setting .docutils .literal
    .notranslate}]{.pre}](index.html#std-setting-FEED_EXPORT_INDENT){.hoverxref
    .tooltip .reference .internal} setting ([issue
    2456](https://github.com/scrapy/scrapy/issues/2456){.reference
    .external}, fixes [issue
    1327](https://github.com/scrapy/scrapy/issues/1327){.reference
    .external})

-   Allow dropping fields in [`FormRequest.from_response`{.docutils
    .literal .notranslate}]{.pre} formdata when [`None`{.docutils
    .literal .notranslate}]{.pre} value is passed ([issue
    667](https://github.com/scrapy/scrapy/issues/667){.reference
    .external})

-   Per-request retry times with the new [[`max_retry_times`{.xref .std
    .std-reqmeta .docutils .literal
    .notranslate}]{.pre}](index.html#std-reqmeta-max_retry_times){.hoverxref
    .tooltip .reference .internal} meta key ([issue
    2642](https://github.com/scrapy/scrapy/issues/2642){.reference
    .external})

-   [`python`{.docutils .literal .notranslate}]{.pre}` `{.docutils
    .literal .notranslate}[`-m`{.docutils .literal
    .notranslate}]{.pre}` `{.docutils .literal
    .notranslate}[`scrapy`{.docutils .literal .notranslate}]{.pre} as a
    more explicit alternative to [`scrapy`{.docutils .literal
    .notranslate}]{.pre} command ([issue
    2740](https://github.com/scrapy/scrapy/issues/2740){.reference
    .external})
:::

::: {#id105 .section}
##### Bug fixes[¶](#id105 "Permalink to this heading"){.headerlink}

-   LinkExtractor now strips leading and trailing whitespaces from
    attributes ([issue
    2547](https://github.com/scrapy/scrapy/issues/2547){.reference
    .external}, fixes [issue
    1614](https://github.com/scrapy/scrapy/issues/1614){.reference
    .external})

-   Properly handle whitespaces in action attribute in
    [`FormRequest`{.xref .py .py-class .docutils .literal
    .notranslate}]{.pre} ([issue
    2548](https://github.com/scrapy/scrapy/issues/2548){.reference
    .external})

-   Buffer CONNECT response bytes from proxy until all HTTP headers are
    received ([issue
    2495](https://github.com/scrapy/scrapy/issues/2495){.reference
    .external}, fixes [issue
    2491](https://github.com/scrapy/scrapy/issues/2491){.reference
    .external})

-   FTP downloader now works on Python 3, provided you use
    Twisted\>=17.1 ([issue
    2599](https://github.com/scrapy/scrapy/issues/2599){.reference
    .external})

-   Use body to choose response type after decompressing content ([issue
    2393](https://github.com/scrapy/scrapy/issues/2393){.reference
    .external}, fixes [issue
    2145](https://github.com/scrapy/scrapy/issues/2145){.reference
    .external})

-   Always decompress [`Content-Encoding:`{.docutils .literal
    .notranslate}]{.pre}` `{.docutils .literal
    .notranslate}[`gzip`{.docutils .literal .notranslate}]{.pre} at
    [[`HttpCompressionMiddleware`{.xref .py .py-class .docutils .literal
    .notranslate}]{.pre}](index.html#scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware "scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware"){.reference
    .internal} stage ([issue
    2391](https://github.com/scrapy/scrapy/issues/2391){.reference
    .external})

-   Respect custom log level in [`Spider.custom_settings`{.docutils
    .literal .notranslate}]{.pre} ([issue
    2581](https://github.com/scrapy/scrapy/issues/2581){.reference
    .external}, fixes [issue
    1612](https://github.com/scrapy/scrapy/issues/1612){.reference
    .external})

-   'make htmlview' fix for macOS ([issue
    2661](https://github.com/scrapy/scrapy/issues/2661){.reference
    .external})

-   Remove "commands" from the command list ([issue
    2695](https://github.com/scrapy/scrapy/issues/2695){.reference
    .external})

-   Fix duplicate Content-Length header for POST requests with empty
    body ([issue
    2677](https://github.com/scrapy/scrapy/issues/2677){.reference
    .external})

-   Properly cancel large downloads, i.e. above
    [[`DOWNLOAD_MAXSIZE`{.xref .std .std-setting .docutils .literal
    .notranslate}]{.pre}](index.html#std-setting-DOWNLOAD_MAXSIZE){.hoverxref
    .tooltip .reference .internal} ([issue
    1616](https://github.com/scrapy/scrapy/issues/1616){.reference
    .external})

-   ImagesPipeline: fixed processing of transparent PNG images with
    palette ([issue
    2675](https://github.com/scrapy/scrapy/issues/2675){.reference
    .external})
:::

::: {#cleanups-refactoring .section}
##### Cleanups & Refactoring[¶](#cleanups-refactoring "Permalink to this heading"){.headerlink}

-   Tests: remove temp files and folders ([issue
    2570](https://github.com/scrapy/scrapy/issues/2570){.reference
    .external}), fixed ProjectUtilsTest on macOS ([issue
    2569](https://github.com/scrapy/scrapy/issues/2569){.reference
    .external}), use portable pypy for Linux on Travis CI ([issue
    2710](https://github.com/scrapy/scrapy/issues/2710){.reference
    .external})

-   Separate building request from [`_requests_to_follow`{.docutils
    .literal .notranslate}]{.pre} in CrawlSpider ([issue
    2562](https://github.com/scrapy/scrapy/issues/2562){.reference
    .external})

-   Remove "Python 3 progress" badge ([issue
    2567](https://github.com/scrapy/scrapy/issues/2567){.reference
    .external})

-   Add a couple more lines to [`.gitignore`{.docutils .literal
    .notranslate}]{.pre} ([issue
    2557](https://github.com/scrapy/scrapy/issues/2557){.reference
    .external})

-   Remove bumpversion prerelease configuration ([issue
    2159](https://github.com/scrapy/scrapy/issues/2159){.reference
    .external})

-   Add codecov.yml file ([issue
    2750](https://github.com/scrapy/scrapy/issues/2750){.reference
    .external})

-   Set context factory implementation based on Twisted version ([issue
    2577](https://github.com/scrapy/scrapy/issues/2577){.reference
    .external}, fixes [issue
    2560](https://github.com/scrapy/scrapy/issues/2560){.reference
    .external})

-   Add omitted [`self`{.docutils .literal .notranslate}]{.pre}
    arguments in default project middleware template ([issue
    2595](https://github.com/scrapy/scrapy/issues/2595){.reference
    .external})

-   Remove redundant [`slot.add_request()`{.docutils .literal
    .notranslate}]{.pre} call in ExecutionEngine ([issue
    2617](https://github.com/scrapy/scrapy/issues/2617){.reference
    .external})

-   Catch more specific [`os.error`{.docutils .literal
    .notranslate}]{.pre} exception in
    [`scrapy.pipelines.files.FSFilesStore`{.docutils .literal
    .notranslate}]{.pre} ([issue
    2644](https://github.com/scrapy/scrapy/issues/2644){.reference
    .external})

-   Change "localhost" test server certificate ([issue
    2720](https://github.com/scrapy/scrapy/issues/2720){.reference
    .external})

-   Remove unused [`MEMUSAGE_REPORT`{.docutils .literal
    .notranslate}]{.pre} setting ([issue
    2576](https://github.com/scrapy/scrapy/issues/2576){.reference
    .external})
:::

::: {#id106 .section}
##### Documentation[¶](#id106 "Permalink to this heading"){.headerlink}

-   Binary mode is required for exporters ([issue
    2564](https://github.com/scrapy/scrapy/issues/2564){.reference
    .external}, fixes [issue
    2553](https://github.com/scrapy/scrapy/issues/2553){.reference
    .external})

-   Mention issue with [`FormRequest.from_response`{.xref .py .py-meth
    .docutils .literal .notranslate}]{.pre} due to bug in lxml ([issue
    2572](https://github.com/scrapy/scrapy/issues/2572){.reference
    .external})

-   Use single quotes uniformly in templates ([issue
    2596](https://github.com/scrapy/scrapy/issues/2596){.reference
    .external})

-   Document [[`ftp_user`{.xref .std .std-reqmeta .docutils .literal
    .notranslate}]{.pre}](index.html#std-reqmeta-ftp_user){.hoverxref
    .tooltip .reference .internal} and [[`ftp_password`{.xref .std
    .std-reqmeta .docutils .literal
    .notranslate}]{.pre}](index.html#std-reqmeta-ftp_password){.hoverxref
    .tooltip .reference .internal} meta keys ([issue
    2587](https://github.com/scrapy/scrapy/issues/2587){.reference
    .external})

-   Removed section on deprecated [`contrib/`{.docutils .literal
    .notranslate}]{.pre} ([issue
    2636](https://github.com/scrapy/scrapy/issues/2636){.reference
    .external})

-   Recommend Anaconda when installing Scrapy on Windows ([issue
    2477](https://github.com/scrapy/scrapy/issues/2477){.reference
    .external}, fixes [issue
    2475](https://github.com/scrapy/scrapy/issues/2475){.reference
    .external})

-   FAQ: rewrite note on Python 3 support on Windows ([issue
    2690](https://github.com/scrapy/scrapy/issues/2690){.reference
    .external})

-   Rearrange selector sections ([issue
    2705](https://github.com/scrapy/scrapy/issues/2705){.reference
    .external})

-   Remove [`__nonzero__`{.docutils .literal .notranslate}]{.pre} from
    [[`SelectorList`{.xref .py .py-class .docutils .literal
    .notranslate}]{.pre}](index.html#scrapy.selector.SelectorList "scrapy.selector.SelectorList"){.reference
    .internal} docs ([issue
    2683](https://github.com/scrapy/scrapy/issues/2683){.reference
    .external})

-   Mention how to disable request filtering in documentation of
    [[`DUPEFILTER_CLASS`{.xref .std .std-setting .docutils .literal
    .notranslate}]{.pre}](index.html#std-setting-DUPEFILTER_CLASS){.hoverxref
    .tooltip .reference .internal} setting ([issue
    2714](https://github.com/scrapy/scrapy/issues/2714){.reference
    .external})

-   Add sphinx_rtd_theme to docs setup readme ([issue
    2668](https://github.com/scrapy/scrapy/issues/2668){.reference
    .external})

-   Open file in text mode in JSON item writer example ([issue
    2729](https://github.com/scrapy/scrapy/issues/2729){.reference
    .external})

-   Clarify [`allowed_domains`{.docutils .literal .notranslate}]{.pre}
    example ([issue
    2670](https://github.com/scrapy/scrapy/issues/2670){.reference
    .external})
:::
:::

::: {#scrapy-1-3-3-2017-03-10 .section}
[]{#release-1-3-3}

#### Scrapy 1.3.3 (2017-03-10)[¶](#scrapy-1-3-3-2017-03-10 "Permalink to this heading"){.headerlink}

::: {#id107 .section}
##### Bug fixes[¶](#id107 "Permalink to this heading"){.headerlink}

-   Make [`SpiderLoader`{.docutils .literal .notranslate}]{.pre} raise
    [`ImportError`{.docutils .literal .notranslate}]{.pre} again by
    default for missing dependencies and wrong [[`SPIDER_MODULES`{.xref
    .std .std-setting .docutils .literal
    .notranslate}]{.pre}](index.html#std-setting-SPIDER_MODULES){.hoverxref
    .tooltip .reference .internal}. These exceptions were silenced as
    warnings since 1.3.0. A new setting is introduced to toggle between
    warning or exception if needed ; see
    [[`SPIDER_LOADER_WARN_ONLY`{.xref .std .std-setting .docutils
    .literal
    .notranslate}]{.pre}](index.html#std-setting-SPIDER_LOADER_WARN_ONLY){.hoverxref
    .tooltip .reference .internal} for details.
:::
:::

::: {#scrapy-1-3-2-2017-02-13 .section}
[]{#release-1-3-2}

#### Scrapy 1.3.2 (2017-02-13)[¶](#scrapy-1-3-2-2017-02-13 "Permalink to this heading"){.headerlink}

::: {#id108 .section}
##### Bug fixes[¶](#id108 "Permalink to this heading"){.headerlink}

-   Preserve request class when converting to/from dicts (utils.reqser)
    ([issue
    2510](https://github.com/scrapy/scrapy/issues/2510){.reference
    .external}).

-   Use consistent selectors for author field in tutorial ([issue
    2551](https://github.com/scrapy/scrapy/issues/2551){.reference
    .external}).

-   Fix TLS compatibility in Twisted 17+ ([issue
    2558](https://github.com/scrapy/scrapy/issues/2558){.reference
    .external})
:::
:::

::: {#scrapy-1-3-1-2017-02-08 .section}
[]{#release-1-3-1}

#### Scrapy 1.3.1 (2017-02-08)[¶](#scrapy-1-3-1-2017-02-08 "Permalink to this heading"){.headerlink}

::: {#id109 .section}
##### New features[¶](#id109 "Permalink to this heading"){.headerlink}

-   Support [`'True'`{.docutils .literal .notranslate}]{.pre} and
    [`'False'`{.docutils .literal .notranslate}]{.pre} string values for
    boolean settings ([issue
    2519](https://github.com/scrapy/scrapy/issues/2519){.reference
    .external}); you can now do something like [`scrapy`{.docutils
    .literal .notranslate}]{.pre}` `{.docutils .literal
    .notranslate}[`crawl`{.docutils .literal
    .notranslate}]{.pre}` `{.docutils .literal
    .notranslate}[`myspider`{.docutils .literal
    .notranslate}]{.pre}` `{.docutils .literal
    .notranslate}[`-s`{.docutils .literal
    .notranslate}]{.pre}` `{.docutils .literal
    .notranslate}[`REDIRECT_ENABLED=False`{.docutils .literal
    .notranslate}]{.pre}.

-   Support kwargs with [`response.xpath()`{.docutils .literal
    .notranslate}]{.pre} to use [[XPath variables]{.std
    .std-ref}](index.html#topics-selectors-xpath-variables){.hoverxref
    .tooltip .reference .internal} and ad-hoc namespaces declarations ;
    this requires at least Parsel v1.1 ([issue
    2457](https://github.com/scrapy/scrapy/issues/2457){.reference
    .external}).

-   Add support for Python 3.6 ([issue
    2485](https://github.com/scrapy/scrapy/issues/2485){.reference
    .external}).

-   Run tests on PyPy (warning: some tests still fail, so PyPy is not
    supported yet).
:::

::: {#id110 .section}
##### Bug fixes[¶](#id110 "Permalink to this heading"){.headerlink}

-   Enforce [`DNS_TIMEOUT`{.docutils .literal .notranslate}]{.pre}
    setting ([issue
    2496](https://github.com/scrapy/scrapy/issues/2496){.reference
    .external}).

-   Fix [[`view`{.xref .std .std-command .docutils .literal
    .notranslate}]{.pre}](index.html#std-command-view){.hoverxref
    .tooltip .reference .internal} command ; it was a regression in
    v1.3.0 ([issue
    2503](https://github.com/scrapy/scrapy/issues/2503){.reference
    .external}).

-   Fix tests regarding [`*_EXPIRES`{.docutils .literal
    .notranslate}]{.pre}` `{.docutils .literal
    .notranslate}[`settings`{.docutils .literal .notranslate}]{.pre}
    with Files/Images pipelines ([issue
    2460](https://github.com/scrapy/scrapy/issues/2460){.reference
    .external}).

-   Fix name of generated pipeline class when using basic project
    template ([issue
    2466](https://github.com/scrapy/scrapy/issues/2466){.reference
    .external}).

-   Fix compatibility with Twisted 17+ ([issue
    2496](https://github.com/scrapy/scrapy/issues/2496){.reference
    .external}, [issue
    2528](https://github.com/scrapy/scrapy/issues/2528){.reference
    .external}).

-   Fix [`scrapy.Item`{.docutils .literal .notranslate}]{.pre}
    inheritance on Python 3.6 ([issue
    2511](https://github.com/scrapy/scrapy/issues/2511){.reference
    .external}).

-   Enforce numeric values for components order in
    [`SPIDER_MIDDLEWARES`{.docutils .literal .notranslate}]{.pre},
    [`DOWNLOADER_MIDDLEWARES`{.docutils .literal .notranslate}]{.pre},
    [`EXTENSIONS`{.docutils .literal .notranslate}]{.pre} and
    [`SPIDER_CONTRACTS`{.docutils .literal .notranslate}]{.pre} ([issue
    2420](https://github.com/scrapy/scrapy/issues/2420){.reference
    .external}).
:::

::: {#id111 .section}
##### Documentation[¶](#id111 "Permalink to this heading"){.headerlink}

-   Reword Code of Conduct section and upgrade to Contributor Covenant
    v1.4 ([issue
    2469](https://github.com/scrapy/scrapy/issues/2469){.reference
    .external}).

-   Clarify that passing spider arguments converts them to spider
    attributes ([issue
    2483](https://github.com/scrapy/scrapy/issues/2483){.reference
    .external}).

-   Document [`formid`{.docutils .literal .notranslate}]{.pre} argument
    on [`FormRequest.from_response()`{.docutils .literal
    .notranslate}]{.pre} ([issue
    2497](https://github.com/scrapy/scrapy/issues/2497){.reference
    .external}).

-   Add .rst extension to README files ([issue
    2507](https://github.com/scrapy/scrapy/issues/2507){.reference
    .external}).

-   Mention LevelDB cache storage backend ([issue
    2525](https://github.com/scrapy/scrapy/issues/2525){.reference
    .external}).

-   Use [`yield`{.docutils .literal .notranslate}]{.pre} in sample
    callback code ([issue
    2533](https://github.com/scrapy/scrapy/issues/2533){.reference
    .external}).

-   Add note about HTML entities decoding with
    [`.re()/.re_first()`{.docutils .literal .notranslate}]{.pre} ([issue
    1704](https://github.com/scrapy/scrapy/issues/1704){.reference
    .external}).

-   Typos ([issue
    2512](https://github.com/scrapy/scrapy/issues/2512){.reference
    .external}, [issue
    2534](https://github.com/scrapy/scrapy/issues/2534){.reference
    .external}, [issue
    2531](https://github.com/scrapy/scrapy/issues/2531){.reference
    .external}).
:::

::: {#cleanups .section}
##### Cleanups[¶](#cleanups "Permalink to this heading"){.headerlink}

-   Remove redundant check in [`MetaRefreshMiddleware`{.docutils
    .literal .notranslate}]{.pre} ([issue
    2542](https://github.com/scrapy/scrapy/issues/2542){.reference
    .external}).

-   Faster checks in [`LinkExtractor`{.docutils .literal
    .notranslate}]{.pre} for allow/deny patterns ([issue
    2538](https://github.com/scrapy/scrapy/issues/2538){.reference
    .external}).

-   Remove dead code supporting old Twisted versions ([issue
    2544](https://github.com/scrapy/scrapy/issues/2544){.reference
    .external}).
:::
:::

::: {#scrapy-1-3-0-2016-12-21 .section}
[]{#release-1-3-0}

#### Scrapy 1.3.0 (2016-12-21)[¶](#scrapy-1-3-0-2016-12-21 "Permalink to this heading"){.headerlink}

This release comes rather soon after 1.2.2 for one main reason: it was
found out that releases since 0.18 up to 1.2.2 (included) use some
backported code from Twisted ([`scrapy.xlib.tx.*`{.docutils .literal
.notranslate}]{.pre}), even if newer Twisted modules are available.
Scrapy now uses [`twisted.web.client`{.docutils .literal
.notranslate}]{.pre} and [`twisted.internet.endpoints`{.docutils
.literal .notranslate}]{.pre} directly. (See also cleanups below.)

As it is a major change, we wanted to get the bug fix out quickly while
not breaking any projects using the 1.2 series.

::: {#id112 .section}
##### New Features[¶](#id112 "Permalink to this heading"){.headerlink}

-   [`MailSender`{.docutils .literal .notranslate}]{.pre} now accepts
    single strings as values for [`to`{.docutils .literal
    .notranslate}]{.pre} and [`cc`{.docutils .literal
    .notranslate}]{.pre} arguments ([issue
    2272](https://github.com/scrapy/scrapy/issues/2272){.reference
    .external})

-   [`scrapy`{.docutils .literal .notranslate}]{.pre}` `{.docutils
    .literal .notranslate}[`fetch`{.docutils .literal
    .notranslate}]{.pre}` `{.docutils .literal
    .notranslate}[`url`{.docutils .literal .notranslate}]{.pre},
    [`scrapy`{.docutils .literal .notranslate}]{.pre}` `{.docutils
    .literal .notranslate}[`shell`{.docutils .literal
    .notranslate}]{.pre}` `{.docutils .literal
    .notranslate}[`url`{.docutils .literal .notranslate}]{.pre} and
    [`fetch(url)`{.docutils .literal .notranslate}]{.pre} inside Scrapy
    shell now follow HTTP redirections by default ([issue
    2290](https://github.com/scrapy/scrapy/issues/2290){.reference
    .external}); See [[`fetch`{.xref .std .std-command .docutils
    .literal
    .notranslate}]{.pre}](index.html#std-command-fetch){.hoverxref
    .tooltip .reference .internal} and [[`shell`{.xref .std .std-command
    .docutils .literal
    .notranslate}]{.pre}](index.html#std-command-shell){.hoverxref
    .tooltip .reference .internal} for details.

-   [`HttpErrorMiddleware`{.docutils .literal .notranslate}]{.pre} now
    logs errors with [`INFO`{.docutils .literal .notranslate}]{.pre}
    level instead of [`DEBUG`{.docutils .literal .notranslate}]{.pre};
    this is technically **backward incompatible** so please check your
    log parsers.

-   By default, logger names now use a long-form path, e.g.
    [`[scrapy.extensions.logstats]`{.docutils .literal
    .notranslate}]{.pre}, instead of the shorter "top-level" variant of
    prior releases (e.g. [`[scrapy]`{.docutils .literal
    .notranslate}]{.pre}); this is **backward incompatible** if you have
    log parsers expecting the short logger name part. You can switch
    back to short logger names using [[`LOG_SHORT_NAMES`{.xref .std
    .std-setting .docutils .literal
    .notranslate}]{.pre}](index.html#std-setting-LOG_SHORT_NAMES){.hoverxref
    .tooltip .reference .internal} set to [`True`{.docutils .literal
    .notranslate}]{.pre}.
:::

::: {#dependencies-cleanups .section}
##### Dependencies & Cleanups[¶](#dependencies-cleanups "Permalink to this heading"){.headerlink}

-   Scrapy now requires Twisted \>= 13.1 which is the case for many
    Linux distributions already.

-   As a consequence, we got rid of [`scrapy.xlib.tx.*`{.docutils
    .literal .notranslate}]{.pre} modules, which copied some of Twisted
    code for users stuck with an "old" Twisted version

-   [`ChunkedTransferMiddleware`{.docutils .literal .notranslate}]{.pre}
    is deprecated and removed from the default downloader middlewares.
:::
:::

::: {#scrapy-1-2-3-2017-03-03 .section}
[]{#release-1-2-3}

#### Scrapy 1.2.3 (2017-03-03)[¶](#scrapy-1-2-3-2017-03-03 "Permalink to this heading"){.headerlink}

-   Packaging fix: disallow unsupported Twisted versions in setup.py
:::

::: {#scrapy-1-2-2-2016-12-06 .section}
[]{#release-1-2-2}

#### Scrapy 1.2.2 (2016-12-06)[¶](#scrapy-1-2-2-2016-12-06 "Permalink to this heading"){.headerlink}

::: {#id113 .section}
##### Bug fixes[¶](#id113 "Permalink to this heading"){.headerlink}

-   Fix a cryptic traceback when a pipeline fails on
    [`open_spider()`{.docutils .literal .notranslate}]{.pre} ([issue
    2011](https://github.com/scrapy/scrapy/issues/2011){.reference
    .external})

-   Fix embedded IPython shell variables (fixing [issue
    396](https://github.com/scrapy/scrapy/issues/396){.reference
    .external} that re-appeared in 1.2.0, fixed in [issue
    2418](https://github.com/scrapy/scrapy/issues/2418){.reference
    .external})

-   A couple of patches when dealing with robots.txt:

    -   handle (non-standard) relative sitemap URLs ([issue
        2390](https://github.com/scrapy/scrapy/issues/2390){.reference
        .external})

    -   handle non-ASCII URLs and User-Agents in Python 2 ([issue
        2373](https://github.com/scrapy/scrapy/issues/2373){.reference
        .external})
:::

::: {#id114 .section}
##### Documentation[¶](#id114 "Permalink to this heading"){.headerlink}

-   Document [`"download_latency"`{.docutils .literal
    .notranslate}]{.pre} key in [`Request`{.docutils .literal
    .notranslate}]{.pre}'s [`meta`{.docutils .literal
    .notranslate}]{.pre} dict ([issue
    2033](https://github.com/scrapy/scrapy/issues/2033){.reference
    .external})

-   Remove page on (deprecated & unsupported) Ubuntu packages from ToC
    ([issue
    2335](https://github.com/scrapy/scrapy/issues/2335){.reference
    .external})

-   A few fixed typos ([issue
    2346](https://github.com/scrapy/scrapy/issues/2346){.reference
    .external}, [issue
    2369](https://github.com/scrapy/scrapy/issues/2369){.reference
    .external}, [issue
    2369](https://github.com/scrapy/scrapy/issues/2369){.reference
    .external}, [issue
    2380](https://github.com/scrapy/scrapy/issues/2380){.reference
    .external}) and clarifications ([issue
    2354](https://github.com/scrapy/scrapy/issues/2354){.reference
    .external}, [issue
    2325](https://github.com/scrapy/scrapy/issues/2325){.reference
    .external}, [issue
    2414](https://github.com/scrapy/scrapy/issues/2414){.reference
    .external})
:::

::: {#id115 .section}
##### Other changes[¶](#id115 "Permalink to this heading"){.headerlink}

-   Advertize
    [conda-forge](https://anaconda.org/conda-forge/scrapy){.reference
    .external} as Scrapy's official conda channel ([issue
    2387](https://github.com/scrapy/scrapy/issues/2387){.reference
    .external})

-   More helpful error messages when trying to use [`.css()`{.docutils
    .literal .notranslate}]{.pre} or [`.xpath()`{.docutils .literal
    .notranslate}]{.pre} on non-Text Responses ([issue
    2264](https://github.com/scrapy/scrapy/issues/2264){.reference
    .external})

-   [`startproject`{.docutils .literal .notranslate}]{.pre} command now
    generates a sample [`middlewares.py`{.docutils .literal
    .notranslate}]{.pre} file ([issue
    2335](https://github.com/scrapy/scrapy/issues/2335){.reference
    .external})

-   Add more dependencies' version info in [`scrapy`{.docutils .literal
    .notranslate}]{.pre}` `{.docutils .literal
    .notranslate}[`version`{.docutils .literal .notranslate}]{.pre}
    verbose output ([issue
    2404](https://github.com/scrapy/scrapy/issues/2404){.reference
    .external})

-   Remove all [`*.pyc`{.docutils .literal .notranslate}]{.pre} files
    from source distribution ([issue
    2386](https://github.com/scrapy/scrapy/issues/2386){.reference
    .external})
:::
:::

::: {#scrapy-1-2-1-2016-10-21 .section}
[]{#release-1-2-1}

#### Scrapy 1.2.1 (2016-10-21)[¶](#scrapy-1-2-1-2016-10-21 "Permalink to this heading"){.headerlink}

::: {#id116 .section}
##### Bug fixes[¶](#id116 "Permalink to this heading"){.headerlink}

-   Include OpenSSL's more permissive default ciphers when establishing
    TLS/SSL connections ([issue
    2314](https://github.com/scrapy/scrapy/issues/2314){.reference
    .external}).

-   Fix "Location" HTTP header decoding on non-ASCII URL redirects
    ([issue
    2321](https://github.com/scrapy/scrapy/issues/2321){.reference
    .external}).
:::

::: {#id117 .section}
##### Documentation[¶](#id117 "Permalink to this heading"){.headerlink}

-   Fix JsonWriterPipeline example ([issue
    2302](https://github.com/scrapy/scrapy/issues/2302){.reference
    .external}).

-   Various notes: [issue
    2330](https://github.com/scrapy/scrapy/issues/2330){.reference
    .external} on spider names, [issue
    2329](https://github.com/scrapy/scrapy/issues/2329){.reference
    .external} on middleware methods processing order, [issue
    2327](https://github.com/scrapy/scrapy/issues/2327){.reference
    .external} on getting multi-valued HTTP headers as lists.
:::

::: {#id118 .section}
##### Other changes[¶](#id118 "Permalink to this heading"){.headerlink}

-   Removed [`www.`{.docutils .literal .notranslate}]{.pre} from
    [`start_urls`{.docutils .literal .notranslate}]{.pre} in built-in
    spider templates ([issue
    2299](https://github.com/scrapy/scrapy/issues/2299){.reference
    .external}).
:::
:::

::: {#scrapy-1-2-0-2016-10-03 .section}
[]{#release-1-2-0}

#### Scrapy 1.2.0 (2016-10-03)[¶](#scrapy-1-2-0-2016-10-03 "Permalink to this heading"){.headerlink}

::: {#id119 .section}
##### New Features[¶](#id119 "Permalink to this heading"){.headerlink}

-   New [[`FEED_EXPORT_ENCODING`{.xref .std .std-setting .docutils
    .literal
    .notranslate}]{.pre}](index.html#std-setting-FEED_EXPORT_ENCODING){.hoverxref
    .tooltip .reference .internal} setting to customize the encoding
    used when writing items to a file. This can be used to turn off
    [`\uXXXX`{.docutils .literal .notranslate}]{.pre} escapes in JSON
    output. This is also useful for those wanting something else than
    UTF-8 for XML or CSV output ([issue
    2034](https://github.com/scrapy/scrapy/issues/2034){.reference
    .external}).

-   [`startproject`{.docutils .literal .notranslate}]{.pre} command now
    supports an optional destination directory to override the default
    one based on the project name ([issue
    2005](https://github.com/scrapy/scrapy/issues/2005){.reference
    .external}).

-   New [[`SCHEDULER_DEBUG`{.xref .std .std-setting .docutils .literal
    .notranslate}]{.pre}](index.html#std-setting-SCHEDULER_DEBUG){.hoverxref
    .tooltip .reference .internal} setting to log requests serialization
    failures ([issue
    1610](https://github.com/scrapy/scrapy/issues/1610){.reference
    .external}).

-   JSON encoder now supports serialization of [`set`{.docutils .literal
    .notranslate}]{.pre} instances ([issue
    2058](https://github.com/scrapy/scrapy/issues/2058){.reference
    .external}).

-   Interpret [`application/json-amazonui-streaming`{.docutils .literal
    .notranslate}]{.pre} as [`TextResponse`{.docutils .literal
    .notranslate}]{.pre} ([issue
    1503](https://github.com/scrapy/scrapy/issues/1503){.reference
    .external}).

-   [`scrapy`{.docutils .literal .notranslate}]{.pre} is imported by
    default when using shell tools ([[`shell`{.xref .std .std-command
    .docutils .literal
    .notranslate}]{.pre}](index.html#std-command-shell){.hoverxref
    .tooltip .reference .internal}, [[inspect_response]{.std
    .std-ref}](index.html#topics-shell-inspect-response){.hoverxref
    .tooltip .reference .internal}) ([issue
    2248](https://github.com/scrapy/scrapy/issues/2248){.reference
    .external}).
:::

::: {#id120 .section}
##### Bug fixes[¶](#id120 "Permalink to this heading"){.headerlink}

-   DefaultRequestHeaders middleware now runs before UserAgent
    middleware ([issue
    2088](https://github.com/scrapy/scrapy/issues/2088){.reference
    .external}). **Warning: this is technically backward incompatible**,
    though we consider this a bug fix.

-   HTTP cache extension and plugins that use the [`.scrapy`{.docutils
    .literal .notranslate}]{.pre} data directory now work outside
    projects ([issue
    1581](https://github.com/scrapy/scrapy/issues/1581){.reference
    .external}). **Warning: this is technically backward incompatible**,
    though we consider this a bug fix.

-   [`Selector`{.docutils .literal .notranslate}]{.pre} does not allow
    passing both [`response`{.docutils .literal .notranslate}]{.pre} and
    [`text`{.docutils .literal .notranslate}]{.pre} anymore ([issue
    2153](https://github.com/scrapy/scrapy/issues/2153){.reference
    .external}).

-   Fixed logging of wrong callback name with [`scrapy`{.docutils
    .literal .notranslate}]{.pre}` `{.docutils .literal
    .notranslate}[`parse`{.docutils .literal .notranslate}]{.pre}
    ([issue
    2169](https://github.com/scrapy/scrapy/issues/2169){.reference
    .external}).

-   Fix for an odd gzip decompression bug ([issue
    1606](https://github.com/scrapy/scrapy/issues/1606){.reference
    .external}).

-   Fix for selected callbacks when using [`CrawlSpider`{.docutils
    .literal .notranslate}]{.pre} with [[`scrapy`{.xref .std
    .std-command .docutils .literal .notranslate}]{.pre}` `{.xref .std
    .std-command .docutils .literal .notranslate}[`parse`{.xref .std
    .std-command .docutils .literal
    .notranslate}]{.pre}](index.html#std-command-parse){.hoverxref
    .tooltip .reference .internal} ([issue
    2225](https://github.com/scrapy/scrapy/issues/2225){.reference
    .external}).

-   Fix for invalid JSON and XML files when spider yields no items
    ([issue 872](https://github.com/scrapy/scrapy/issues/872){.reference
    .external}).

-   Implement [`flush()`{.docutils .literal .notranslate}]{.pre} for
    [`StreamLogger`{.docutils .literal .notranslate}]{.pre} avoiding a
    warning in logs ([issue
    2125](https://github.com/scrapy/scrapy/issues/2125){.reference
    .external}).
:::

::: {#refactoring .section}
##### Refactoring[¶](#refactoring "Permalink to this heading"){.headerlink}

-   [`canonicalize_url`{.docutils .literal .notranslate}]{.pre} has been
    moved to
    [w3lib.url](https://w3lib.readthedocs.io/en/latest/w3lib.html#w3lib.url.canonicalize_url){.reference
    .external} ([issue
    2168](https://github.com/scrapy/scrapy/issues/2168){.reference
    .external}).
:::

::: {#tests-requirements .section}
##### Tests & Requirements[¶](#tests-requirements "Permalink to this heading"){.headerlink}

Scrapy's new requirements baseline is Debian 8 "Jessie". It was
previously Ubuntu 12.04 Precise. What this means in practice is that we
run continuous integration tests with these (main) packages versions at
a minimum: Twisted 14.0, pyOpenSSL 0.14, lxml 3.4.

Scrapy may very well work with older versions of these packages (the
code base still has switches for older Twisted versions for example) but
it is not guaranteed (because it's not tested anymore).
:::

::: {#id121 .section}
##### Documentation[¶](#id121 "Permalink to this heading"){.headerlink}

-   Grammar fixes: [issue
    2128](https://github.com/scrapy/scrapy/issues/2128){.reference
    .external}, [issue
    1566](https://github.com/scrapy/scrapy/issues/1566){.reference
    .external}.

-   Download stats badge removed from README ([issue
    2160](https://github.com/scrapy/scrapy/issues/2160){.reference
    .external}).

-   New Scrapy [[architecture diagram]{.std
    .std-ref}](index.html#topics-architecture){.hoverxref .tooltip
    .reference .internal} ([issue
    2165](https://github.com/scrapy/scrapy/issues/2165){.reference
    .external}).

-   Updated [`Response`{.docutils .literal .notranslate}]{.pre}
    parameters documentation ([issue
    2197](https://github.com/scrapy/scrapy/issues/2197){.reference
    .external}).

-   Reworded misleading [[`RANDOMIZE_DOWNLOAD_DELAY`{.xref .std
    .std-setting .docutils .literal
    .notranslate}]{.pre}](index.html#std-setting-RANDOMIZE_DOWNLOAD_DELAY){.hoverxref
    .tooltip .reference .internal} description ([issue
    2190](https://github.com/scrapy/scrapy/issues/2190){.reference
    .external}).

-   Add StackOverflow as a support channel ([issue
    2257](https://github.com/scrapy/scrapy/issues/2257){.reference
    .external}).
:::
:::

::: {#scrapy-1-1-4-2017-03-03 .section}
[]{#release-1-1-4}

#### Scrapy 1.1.4 (2017-03-03)[¶](#scrapy-1-1-4-2017-03-03 "Permalink to this heading"){.headerlink}

-   Packaging fix: disallow unsupported Twisted versions in setup.py
:::

::: {#scrapy-1-1-3-2016-09-22 .section}
[]{#release-1-1-3}

#### Scrapy 1.1.3 (2016-09-22)[¶](#scrapy-1-1-3-2016-09-22 "Permalink to this heading"){.headerlink}

::: {#id122 .section}
##### Bug fixes[¶](#id122 "Permalink to this heading"){.headerlink}

-   Class attributes for subclasses of [`ImagesPipeline`{.docutils
    .literal .notranslate}]{.pre} and [`FilesPipeline`{.docutils
    .literal .notranslate}]{.pre} work as they did before 1.1.1 ([issue
    2243](https://github.com/scrapy/scrapy/issues/2243){.reference
    .external}, fixes [issue
    2198](https://github.com/scrapy/scrapy/issues/2198){.reference
    .external})
:::

::: {#id123 .section}
##### Documentation[¶](#id123 "Permalink to this heading"){.headerlink}

-   [[Overview]{.std .std-ref}](index.html#intro-overview){.hoverxref
    .tooltip .reference .internal} and [[tutorial]{.std
    .std-ref}](index.html#intro-tutorial){.hoverxref .tooltip .reference
    .internal} rewritten to use
    [http://toscrape.com](http://toscrape.com){.reference .external}
    websites ([issue
    2236](https://github.com/scrapy/scrapy/issues/2236){.reference
    .external}, [issue
    2249](https://github.com/scrapy/scrapy/issues/2249){.reference
    .external}, [issue
    2252](https://github.com/scrapy/scrapy/issues/2252){.reference
    .external}).
:::
:::

::: {#scrapy-1-1-2-2016-08-18 .section}
[]{#release-1-1-2}

#### Scrapy 1.1.2 (2016-08-18)[¶](#scrapy-1-1-2-2016-08-18 "Permalink to this heading"){.headerlink}

::: {#id124 .section}
##### Bug fixes[¶](#id124 "Permalink to this heading"){.headerlink}

-   Introduce a missing [[`IMAGES_STORE_S3_ACL`{.xref .std .std-setting
    .docutils .literal
    .notranslate}]{.pre}](index.html#std-setting-IMAGES_STORE_S3_ACL){.hoverxref
    .tooltip .reference .internal} setting to override the default ACL
    policy in [`ImagesPipeline`{.docutils .literal .notranslate}]{.pre}
    when uploading images to S3 (note that default ACL policy is
    "private" -- instead of "public-read" -- since Scrapy 1.1.0)

-   [[`IMAGES_EXPIRES`{.xref .std .std-setting .docutils .literal
    .notranslate}]{.pre}](index.html#std-setting-IMAGES_EXPIRES){.hoverxref
    .tooltip .reference .internal} default value set back to 90 (the
    regression was introduced in 1.1.1)
:::
:::

::: {#scrapy-1-1-1-2016-07-13 .section}
[]{#release-1-1-1}

#### Scrapy 1.1.1 (2016-07-13)[¶](#scrapy-1-1-1-2016-07-13 "Permalink to this heading"){.headerlink}

::: {#id125 .section}
##### Bug fixes[¶](#id125 "Permalink to this heading"){.headerlink}

-   Add "Host" header in CONNECT requests to HTTPS proxies ([issue
    2069](https://github.com/scrapy/scrapy/issues/2069){.reference
    .external})

-   Use response [`body`{.docutils .literal .notranslate}]{.pre} when
    choosing response class ([issue
    2001](https://github.com/scrapy/scrapy/issues/2001){.reference
    .external}, fixes [issue
    2000](https://github.com/scrapy/scrapy/issues/2000){.reference
    .external})

-   Do not fail on canonicalizing URLs with wrong netlocs ([issue
    2038](https://github.com/scrapy/scrapy/issues/2038){.reference
    .external}, fixes [issue
    2010](https://github.com/scrapy/scrapy/issues/2010){.reference
    .external})

-   a few fixes for [`HttpCompressionMiddleware`{.docutils .literal
    .notranslate}]{.pre} (and [`SitemapSpider`{.docutils .literal
    .notranslate}]{.pre}):

    -   Do not decode HEAD responses ([issue
        2008](https://github.com/scrapy/scrapy/issues/2008){.reference
        .external}, fixes [issue
        1899](https://github.com/scrapy/scrapy/issues/1899){.reference
        .external})

    -   Handle charset parameter in gzip Content-Type header ([issue
        2050](https://github.com/scrapy/scrapy/issues/2050){.reference
        .external}, fixes [issue
        2049](https://github.com/scrapy/scrapy/issues/2049){.reference
        .external})

    -   Do not decompress gzip octet-stream responses ([issue
        2065](https://github.com/scrapy/scrapy/issues/2065){.reference
        .external}, fixes [issue
        2063](https://github.com/scrapy/scrapy/issues/2063){.reference
        .external})

-   Catch (and ignore with a warning) exception when verifying
    certificate against IP-address hosts ([issue
    2094](https://github.com/scrapy/scrapy/issues/2094){.reference
    .external}, fixes [issue
    2092](https://github.com/scrapy/scrapy/issues/2092){.reference
    .external})

-   Make [`FilesPipeline`{.docutils .literal .notranslate}]{.pre} and
    [`ImagesPipeline`{.docutils .literal .notranslate}]{.pre} backward
    compatible again regarding the use of legacy class attributes for
    customization ([issue
    1989](https://github.com/scrapy/scrapy/issues/1989){.reference
    .external}, fixes [issue
    1985](https://github.com/scrapy/scrapy/issues/1985){.reference
    .external})
:::

::: {#id126 .section}
##### New features[¶](#id126 "Permalink to this heading"){.headerlink}

-   Enable genspider command outside project folder ([issue
    2052](https://github.com/scrapy/scrapy/issues/2052){.reference
    .external})

-   Retry HTTPS CONNECT [`TunnelError`{.docutils .literal
    .notranslate}]{.pre} by default ([issue
    1974](https://github.com/scrapy/scrapy/issues/1974){.reference
    .external})
:::

::: {#id127 .section}
##### Documentation[¶](#id127 "Permalink to this heading"){.headerlink}

-   [`FEED_TEMPDIR`{.docutils .literal .notranslate}]{.pre} setting at
    lexicographical position ([commit
    9b3c72c](https://github.com/scrapy/scrapy/commit/9b3c72c){.reference
    .external})

-   Use idiomatic [`.extract_first()`{.docutils .literal
    .notranslate}]{.pre} in overview ([issue
    1994](https://github.com/scrapy/scrapy/issues/1994){.reference
    .external})

-   Update years in copyright notice ([commit
    c2c8036](https://github.com/scrapy/scrapy/commit/c2c8036){.reference
    .external})

-   Add information and example on errbacks ([issue
    1995](https://github.com/scrapy/scrapy/issues/1995){.reference
    .external})

-   Use "url" variable in downloader middleware example ([issue
    2015](https://github.com/scrapy/scrapy/issues/2015){.reference
    .external})

-   Grammar fixes ([issue
    2054](https://github.com/scrapy/scrapy/issues/2054){.reference
    .external}, [issue
    2120](https://github.com/scrapy/scrapy/issues/2120){.reference
    .external})

-   New FAQ entry on using BeautifulSoup in spider callbacks ([issue
    2048](https://github.com/scrapy/scrapy/issues/2048){.reference
    .external})

-   Add notes about Scrapy not working on Windows with Python 3 ([issue
    2060](https://github.com/scrapy/scrapy/issues/2060){.reference
    .external})

-   Encourage complete titles in pull requests ([issue
    2026](https://github.com/scrapy/scrapy/issues/2026){.reference
    .external})
:::

::: {#tests .section}
##### Tests[¶](#tests "Permalink to this heading"){.headerlink}

-   Upgrade py.test requirement on Travis CI and Pin pytest-cov to 2.2.1
    ([issue
    2095](https://github.com/scrapy/scrapy/issues/2095){.reference
    .external})
:::
:::

::: {#scrapy-1-1-0-2016-05-11 .section}
[]{#release-1-1-0}

#### Scrapy 1.1.0 (2016-05-11)[¶](#scrapy-1-1-0-2016-05-11 "Permalink to this heading"){.headerlink}

This 1.1 release brings a lot of interesting features and bug fixes:

-   Scrapy 1.1 has beta Python 3 support (requires Twisted \>= 15.5).
    See [[Beta Python 3 Support]{.std
    .std-ref}](#news-betapy3){.hoverxref .tooltip .reference .internal}
    for more details and some limitations.

-   Hot new features:

    -   Item loaders now support nested loaders ([issue
        1467](https://github.com/scrapy/scrapy/issues/1467){.reference
        .external}).

    -   [`FormRequest.from_response`{.docutils .literal
        .notranslate}]{.pre} improvements ([issue
        1382](https://github.com/scrapy/scrapy/issues/1382){.reference
        .external}, [issue
        1137](https://github.com/scrapy/scrapy/issues/1137){.reference
        .external}).

    -   Added setting [[`AUTOTHROTTLE_TARGET_CONCURRENCY`{.xref .std
        .std-setting .docutils .literal
        .notranslate}]{.pre}](index.html#std-setting-AUTOTHROTTLE_TARGET_CONCURRENCY){.hoverxref
        .tooltip .reference .internal} and improved AutoThrottle docs
        ([issue
        1324](https://github.com/scrapy/scrapy/issues/1324){.reference
        .external}).

    -   Added [`response.text`{.docutils .literal .notranslate}]{.pre}
        to get body as unicode ([issue
        1730](https://github.com/scrapy/scrapy/issues/1730){.reference
        .external}).

    -   Anonymous S3 connections ([issue
        1358](https://github.com/scrapy/scrapy/issues/1358){.reference
        .external}).

    -   Deferreds in downloader middlewares ([issue
        1473](https://github.com/scrapy/scrapy/issues/1473){.reference
        .external}). This enables better robots.txt handling ([issue
        1471](https://github.com/scrapy/scrapy/issues/1471){.reference
        .external}).

    -   HTTP caching now follows RFC2616 more closely, added settings
        [[`HTTPCACHE_ALWAYS_STORE`{.xref .std .std-setting .docutils
        .literal
        .notranslate}]{.pre}](index.html#std-setting-HTTPCACHE_ALWAYS_STORE){.hoverxref
        .tooltip .reference .internal} and
        [[`HTTPCACHE_IGNORE_RESPONSE_CACHE_CONTROLS`{.xref .std
        .std-setting .docutils .literal
        .notranslate}]{.pre}](index.html#std-setting-HTTPCACHE_IGNORE_RESPONSE_CACHE_CONTROLS){.hoverxref
        .tooltip .reference .internal} ([issue
        1151](https://github.com/scrapy/scrapy/issues/1151){.reference
        .external}).

    -   Selectors were extracted to the
        [parsel](https://github.com/scrapy/parsel){.reference .external}
        library ([issue
        1409](https://github.com/scrapy/scrapy/issues/1409){.reference
        .external}). This means you can use Scrapy Selectors without
        Scrapy and also upgrade the selectors engine without needing to
        upgrade Scrapy.

    -   HTTPS downloader now does TLS protocol negotiation by default,
        instead of forcing TLS 1.0. You can also set the SSL/TLS method
        using the new [[`DOWNLOADER_CLIENT_TLS_METHOD`{.xref .std
        .std-setting .docutils .literal
        .notranslate}]{.pre}](index.html#std-setting-DOWNLOADER_CLIENT_TLS_METHOD){.hoverxref
        .tooltip .reference .internal}.

-   These bug fixes may require your attention:

    -   Don't retry bad requests (HTTP 400) by default ([issue
        1289](https://github.com/scrapy/scrapy/issues/1289){.reference
        .external}). If you need the old behavior, add [`400`{.docutils
        .literal .notranslate}]{.pre} to [[`RETRY_HTTP_CODES`{.xref .std
        .std-setting .docutils .literal
        .notranslate}]{.pre}](index.html#std-setting-RETRY_HTTP_CODES){.hoverxref
        .tooltip .reference .internal}.

    -   Fix shell files argument handling ([issue
        1710](https://github.com/scrapy/scrapy/issues/1710){.reference
        .external}, [issue
        1550](https://github.com/scrapy/scrapy/issues/1550){.reference
        .external}). If you try [`scrapy`{.docutils .literal
        .notranslate}]{.pre}` `{.docutils .literal
        .notranslate}[`shell`{.docutils .literal
        .notranslate}]{.pre}` `{.docutils .literal
        .notranslate}[`index.html`{.docutils .literal
        .notranslate}]{.pre} it will try to load the URL
        [http://index.html](http://index.html){.reference .external},
        use [`scrapy`{.docutils .literal
        .notranslate}]{.pre}` `{.docutils .literal
        .notranslate}[`shell`{.docutils .literal
        .notranslate}]{.pre}` `{.docutils .literal
        .notranslate}[`./index.html`{.docutils .literal
        .notranslate}]{.pre} to load a local file.

    -   Robots.txt compliance is now enabled by default for
        newly-created projects ([issue
        1724](https://github.com/scrapy/scrapy/issues/1724){.reference
        .external}). Scrapy will also wait for robots.txt to be
        downloaded before proceeding with the crawl ([issue
        1735](https://github.com/scrapy/scrapy/issues/1735){.reference
        .external}). If you want to disable this behavior, update
        [[`ROBOTSTXT_OBEY`{.xref .std .std-setting .docutils .literal
        .notranslate}]{.pre}](index.html#std-setting-ROBOTSTXT_OBEY){.hoverxref
        .tooltip .reference .internal} in [`settings.py`{.docutils
        .literal .notranslate}]{.pre} file after creating a new project.

    -   Exporters now work on unicode, instead of bytes by default
        ([issue
        1080](https://github.com/scrapy/scrapy/issues/1080){.reference
        .external}). If you use [[`PythonItemExporter`{.xref .py
        .py-class .docutils .literal
        .notranslate}]{.pre}](index.html#scrapy.exporters.PythonItemExporter "scrapy.exporters.PythonItemExporter"){.reference
        .internal}, you may want to update your code to disable binary
        mode which is now deprecated.

    -   Accept XML node names containing dots as valid ([issue
        1533](https://github.com/scrapy/scrapy/issues/1533){.reference
        .external}).

    -   When uploading files or images to S3 (with
        [`FilesPipeline`{.docutils .literal .notranslate}]{.pre} or
        [`ImagesPipeline`{.docutils .literal .notranslate}]{.pre}), the
        default ACL policy is now "private" instead of "public"
        **Warning: backward incompatible!**. You can use
        [[`FILES_STORE_S3_ACL`{.xref .std .std-setting .docutils
        .literal
        .notranslate}]{.pre}](index.html#std-setting-FILES_STORE_S3_ACL){.hoverxref
        .tooltip .reference .internal} to change it.

    -   We've reimplemented [`canonicalize_url()`{.docutils .literal
        .notranslate}]{.pre} for more correct output, especially for
        URLs with non-ASCII characters ([issue
        1947](https://github.com/scrapy/scrapy/issues/1947){.reference
        .external}). This could change link extractors output compared
        to previous Scrapy versions. This may also invalidate some cache
        entries you could still have from pre-1.1 runs. **Warning:
        backward incompatible!**.

Keep reading for more details on other improvements and bug fixes.

::: {#beta-python-3-support .section}
[]{#news-betapy3}

##### Beta Python 3 Support[¶](#beta-python-3-support "Permalink to this heading"){.headerlink}

We have been [hard at work to make Scrapy run on Python
3](https://github.com/scrapy/scrapy/wiki/Python-3-Porting){.reference
.external}. As a result, now you can run spiders on Python 3.3, 3.4 and
3.5 (Twisted \>= 15.5 required). Some features are still missing (and
some may never be ported).

Almost all builtin extensions/middlewares are expected to work. However,
we are aware of some limitations in Python 3:

-   Scrapy does not work on Windows with Python 3

-   Sending emails is not supported

-   FTP download handler is not supported

-   Telnet console is not supported
:::

::: {#additional-new-features-and-enhancements .section}
##### Additional New Features and Enhancements[¶](#additional-new-features-and-enhancements "Permalink to this heading"){.headerlink}

-   Scrapy now has a [Code of
    Conduct](https://github.com/scrapy/scrapy/blob/master/CODE_OF_CONDUCT.md){.reference
    .external} ([issue
    1681](https://github.com/scrapy/scrapy/issues/1681){.reference
    .external}).

-   Command line tool now has completion for zsh ([issue
    934](https://github.com/scrapy/scrapy/issues/934){.reference
    .external}).

-   Improvements to [`scrapy`{.docutils .literal
    .notranslate}]{.pre}` `{.docutils .literal
    .notranslate}[`shell`{.docutils .literal .notranslate}]{.pre}:

    -   Support for bpython and configure preferred Python shell via
        [`SCRAPY_PYTHON_SHELL`{.docutils .literal .notranslate}]{.pre}
        ([issue
        1100](https://github.com/scrapy/scrapy/issues/1100){.reference
        .external}, [issue
        1444](https://github.com/scrapy/scrapy/issues/1444){.reference
        .external}).

    -   Support URLs without scheme ([issue
        1498](https://github.com/scrapy/scrapy/issues/1498){.reference
        .external}) **Warning: backward incompatible!**

    -   Bring back support for relative file path ([issue
        1710](https://github.com/scrapy/scrapy/issues/1710){.reference
        .external}, [issue
        1550](https://github.com/scrapy/scrapy/issues/1550){.reference
        .external}).

-   Added [[`MEMUSAGE_CHECK_INTERVAL_SECONDS`{.xref .std .std-setting
    .docutils .literal
    .notranslate}]{.pre}](index.html#std-setting-MEMUSAGE_CHECK_INTERVAL_SECONDS){.hoverxref
    .tooltip .reference .internal} setting to change default check
    interval ([issue
    1282](https://github.com/scrapy/scrapy/issues/1282){.reference
    .external}).

-   Download handlers are now lazy-loaded on first request using their
    scheme ([issue
    1390](https://github.com/scrapy/scrapy/issues/1390){.reference
    .external}, [issue
    1421](https://github.com/scrapy/scrapy/issues/1421){.reference
    .external}).

-   HTTPS download handlers do not force TLS 1.0 anymore; instead,
    OpenSSL's [`SSLv23_method()/TLS_method()`{.docutils .literal
    .notranslate}]{.pre} is used allowing to try negotiating with the
    remote hosts the highest TLS protocol version it can ([issue
    1794](https://github.com/scrapy/scrapy/issues/1794){.reference
    .external}, [issue
    1629](https://github.com/scrapy/scrapy/issues/1629){.reference
    .external}).

-   [`RedirectMiddleware`{.docutils .literal .notranslate}]{.pre} now
    skips the status codes from [`handle_httpstatus_list`{.docutils
    .literal .notranslate}]{.pre} on spider attribute or in
    [`Request`{.docutils .literal .notranslate}]{.pre}'s
    [`meta`{.docutils .literal .notranslate}]{.pre} key ([issue
    1334](https://github.com/scrapy/scrapy/issues/1334){.reference
    .external}, [issue
    1364](https://github.com/scrapy/scrapy/issues/1364){.reference
    .external}, [issue
    1447](https://github.com/scrapy/scrapy/issues/1447){.reference
    .external}).

-   Form submission:

    -   now works with [`<button>`{.docutils .literal
        .notranslate}]{.pre} elements too ([issue
        1469](https://github.com/scrapy/scrapy/issues/1469){.reference
        .external}).

    -   an empty string is now used for submit buttons without a value
        ([issue
        1472](https://github.com/scrapy/scrapy/issues/1472){.reference
        .external})

-   Dict-like settings now have per-key priorities ([issue
    1135](https://github.com/scrapy/scrapy/issues/1135){.reference
    .external}, [issue
    1149](https://github.com/scrapy/scrapy/issues/1149){.reference
    .external} and [issue
    1586](https://github.com/scrapy/scrapy/issues/1586){.reference
    .external}).

-   Sending non-ASCII emails ([issue
    1662](https://github.com/scrapy/scrapy/issues/1662){.reference
    .external})

-   [`CloseSpider`{.docutils .literal .notranslate}]{.pre} and
    [`SpiderState`{.docutils .literal .notranslate}]{.pre} extensions
    now get disabled if no relevant setting is set ([issue
    1723](https://github.com/scrapy/scrapy/issues/1723){.reference
    .external}, [issue
    1725](https://github.com/scrapy/scrapy/issues/1725){.reference
    .external}).

-   Added method [`ExecutionEngine.close`{.docutils .literal
    .notranslate}]{.pre} ([issue
    1423](https://github.com/scrapy/scrapy/issues/1423){.reference
    .external}).

-   Added method [`CrawlerRunner.create_crawler`{.docutils .literal
    .notranslate}]{.pre} ([issue
    1528](https://github.com/scrapy/scrapy/issues/1528){.reference
    .external}).

-   Scheduler priority queue can now be customized via
    [[`SCHEDULER_PRIORITY_QUEUE`{.xref .std .std-setting .docutils
    .literal
    .notranslate}]{.pre}](index.html#std-setting-SCHEDULER_PRIORITY_QUEUE){.hoverxref
    .tooltip .reference .internal} ([issue
    1822](https://github.com/scrapy/scrapy/issues/1822){.reference
    .external}).

-   [`.pps`{.docutils .literal .notranslate}]{.pre} links are now
    ignored by default in link extractors ([issue
    1835](https://github.com/scrapy/scrapy/issues/1835){.reference
    .external}).

-   temporary data folder for FTP and S3 feed storages can be customized
    using a new [[`FEED_TEMPDIR`{.xref .std .std-setting .docutils
    .literal
    .notranslate}]{.pre}](index.html#std-setting-FEED_TEMPDIR){.hoverxref
    .tooltip .reference .internal} setting ([issue
    1847](https://github.com/scrapy/scrapy/issues/1847){.reference
    .external}).

-   [`FilesPipeline`{.docutils .literal .notranslate}]{.pre} and
    [`ImagesPipeline`{.docutils .literal .notranslate}]{.pre} settings
    are now instance attributes instead of class attributes, enabling
    spider-specific behaviors ([issue
    1891](https://github.com/scrapy/scrapy/issues/1891){.reference
    .external}).

-   [`JsonItemExporter`{.docutils .literal .notranslate}]{.pre} now
    formats opening and closing square brackets on their own line (first
    and last lines of output file) ([issue
    1950](https://github.com/scrapy/scrapy/issues/1950){.reference
    .external}).

-   If available, [`botocore`{.docutils .literal .notranslate}]{.pre} is
    used for [`S3FeedStorage`{.docutils .literal .notranslate}]{.pre},
    [`S3DownloadHandler`{.docutils .literal .notranslate}]{.pre} and
    [`S3FilesStore`{.docutils .literal .notranslate}]{.pre} ([issue
    1761](https://github.com/scrapy/scrapy/issues/1761){.reference
    .external}, [issue
    1883](https://github.com/scrapy/scrapy/issues/1883){.reference
    .external}).

-   Tons of documentation updates and related fixes ([issue
    1291](https://github.com/scrapy/scrapy/issues/1291){.reference
    .external}, [issue
    1302](https://github.com/scrapy/scrapy/issues/1302){.reference
    .external}, [issue
    1335](https://github.com/scrapy/scrapy/issues/1335){.reference
    .external}, [issue
    1683](https://github.com/scrapy/scrapy/issues/1683){.reference
    .external}, [issue
    1660](https://github.com/scrapy/scrapy/issues/1660){.reference
    .external}, [issue
    1642](https://github.com/scrapy/scrapy/issues/1642){.reference
    .external}, [issue
    1721](https://github.com/scrapy/scrapy/issues/1721){.reference
    .external}, [issue
    1727](https://github.com/scrapy/scrapy/issues/1727){.reference
    .external}, [issue
    1879](https://github.com/scrapy/scrapy/issues/1879){.reference
    .external}).

-   Other refactoring, optimizations and cleanup ([issue
    1476](https://github.com/scrapy/scrapy/issues/1476){.reference
    .external}, [issue
    1481](https://github.com/scrapy/scrapy/issues/1481){.reference
    .external}, [issue
    1477](https://github.com/scrapy/scrapy/issues/1477){.reference
    .external}, [issue
    1315](https://github.com/scrapy/scrapy/issues/1315){.reference
    .external}, [issue
    1290](https://github.com/scrapy/scrapy/issues/1290){.reference
    .external}, [issue
    1750](https://github.com/scrapy/scrapy/issues/1750){.reference
    .external}, [issue
    1881](https://github.com/scrapy/scrapy/issues/1881){.reference
    .external}).
:::

::: {#deprecations-and-removals .section}
##### Deprecations and Removals[¶](#deprecations-and-removals "Permalink to this heading"){.headerlink}

-   Added [`to_bytes`{.docutils .literal .notranslate}]{.pre} and
    [`to_unicode`{.docutils .literal .notranslate}]{.pre}, deprecated
    [`str_to_unicode`{.docutils .literal .notranslate}]{.pre} and
    [`unicode_to_str`{.docutils .literal .notranslate}]{.pre} functions
    ([issue 778](https://github.com/scrapy/scrapy/issues/778){.reference
    .external}).

-   [`binary_is_text`{.docutils .literal .notranslate}]{.pre} is
    introduced, to replace use of [`isbinarytext`{.docutils .literal
    .notranslate}]{.pre} (but with inverse return value) ([issue
    1851](https://github.com/scrapy/scrapy/issues/1851){.reference
    .external})

-   The [`optional_features`{.docutils .literal .notranslate}]{.pre} set
    has been removed ([issue
    1359](https://github.com/scrapy/scrapy/issues/1359){.reference
    .external}).

-   The [`--lsprof`{.docutils .literal .notranslate}]{.pre} command line
    option has been removed ([issue
    1689](https://github.com/scrapy/scrapy/issues/1689){.reference
    .external}). **Warning: backward incompatible**, but doesn't break
    user code.

-   The following datatypes were deprecated ([issue
    1720](https://github.com/scrapy/scrapy/issues/1720){.reference
    .external}):

    -   [`scrapy.utils.datatypes.MultiValueDictKeyError`{.docutils
        .literal .notranslate}]{.pre}

    -   [`scrapy.utils.datatypes.MultiValueDict`{.docutils .literal
        .notranslate}]{.pre}

    -   [`scrapy.utils.datatypes.SiteNode`{.docutils .literal
        .notranslate}]{.pre}

-   The previously bundled [`scrapy.xlib.pydispatch`{.docutils .literal
    .notranslate}]{.pre} library was deprecated and replaced by
    [pydispatcher](https://pypi.org/project/PyDispatcher/){.reference
    .external}.
:::

::: {#relocations .section}
##### Relocations[¶](#relocations "Permalink to this heading"){.headerlink}

-   [`telnetconsole`{.docutils .literal .notranslate}]{.pre} was
    relocated to [`extensions/`{.docutils .literal .notranslate}]{.pre}
    ([issue
    1524](https://github.com/scrapy/scrapy/issues/1524){.reference
    .external}).

    -   Note: telnet is not enabled on Python 3
        ([https://github.com/scrapy/scrapy/pull/1524#issuecomment-146985595](https://github.com/scrapy/scrapy/pull/1524#issuecomment-146985595){.reference
        .external})
:::

::: {#bugfixes .section}
##### Bugfixes[¶](#bugfixes "Permalink to this heading"){.headerlink}

-   Scrapy does not retry requests that got a [`HTTP`{.docutils .literal
    .notranslate}]{.pre}` `{.docutils .literal
    .notranslate}[`400`{.docutils .literal
    .notranslate}]{.pre}` `{.docutils .literal
    .notranslate}[`Bad`{.docutils .literal
    .notranslate}]{.pre}` `{.docutils .literal
    .notranslate}[`Request`{.docutils .literal .notranslate}]{.pre}
    response anymore ([issue
    1289](https://github.com/scrapy/scrapy/issues/1289){.reference
    .external}). **Warning: backward incompatible!**

-   Support empty password for http_proxy config ([issue
    1274](https://github.com/scrapy/scrapy/issues/1274){.reference
    .external}).

-   Interpret [`application/x-json`{.docutils .literal
    .notranslate}]{.pre} as [`TextResponse`{.docutils .literal
    .notranslate}]{.pre} ([issue
    1333](https://github.com/scrapy/scrapy/issues/1333){.reference
    .external}).

-   Support link rel attribute with multiple values ([issue
    1201](https://github.com/scrapy/scrapy/issues/1201){.reference
    .external}).

-   Fixed [`scrapy.http.FormRequest.from_response`{.docutils .literal
    .notranslate}]{.pre} when there is a [`<base>`{.docutils .literal
    .notranslate}]{.pre} tag ([issue
    1564](https://github.com/scrapy/scrapy/issues/1564){.reference
    .external}).

-   Fixed [[`TEMPLATES_DIR`{.xref .std .std-setting .docutils .literal
    .notranslate}]{.pre}](index.html#std-setting-TEMPLATES_DIR){.hoverxref
    .tooltip .reference .internal} handling ([issue
    1575](https://github.com/scrapy/scrapy/issues/1575){.reference
    .external}).

-   Various [`FormRequest`{.docutils .literal .notranslate}]{.pre} fixes
    ([issue
    1595](https://github.com/scrapy/scrapy/issues/1595){.reference
    .external}, [issue
    1596](https://github.com/scrapy/scrapy/issues/1596){.reference
    .external}, [issue
    1597](https://github.com/scrapy/scrapy/issues/1597){.reference
    .external}).

-   Makes [`_monkeypatches`{.docutils .literal .notranslate}]{.pre} more
    robust ([issue
    1634](https://github.com/scrapy/scrapy/issues/1634){.reference
    .external}).

-   Fixed bug on [`XMLItemExporter`{.docutils .literal
    .notranslate}]{.pre} with non-string fields in items ([issue
    1738](https://github.com/scrapy/scrapy/issues/1738){.reference
    .external}).

-   Fixed startproject command in macOS ([issue
    1635](https://github.com/scrapy/scrapy/issues/1635){.reference
    .external}).

-   Fixed [[`PythonItemExporter`{.xref .py .py-class .docutils .literal
    .notranslate}]{.pre}](index.html#scrapy.exporters.PythonItemExporter "scrapy.exporters.PythonItemExporter"){.reference
    .internal} and CSVExporter for non-string item types ([issue
    1737](https://github.com/scrapy/scrapy/issues/1737){.reference
    .external}).

-   Various logging related fixes ([issue
    1294](https://github.com/scrapy/scrapy/issues/1294){.reference
    .external}, [issue
    1419](https://github.com/scrapy/scrapy/issues/1419){.reference
    .external}, [issue
    1263](https://github.com/scrapy/scrapy/issues/1263){.reference
    .external}, [issue
    1624](https://github.com/scrapy/scrapy/issues/1624){.reference
    .external}, [issue
    1654](https://github.com/scrapy/scrapy/issues/1654){.reference
    .external}, [issue
    1722](https://github.com/scrapy/scrapy/issues/1722){.reference
    .external}, [issue
    1726](https://github.com/scrapy/scrapy/issues/1726){.reference
    .external} and [issue
    1303](https://github.com/scrapy/scrapy/issues/1303){.reference
    .external}).

-   Fixed bug in [`utils.template.render_templatefile()`{.docutils
    .literal .notranslate}]{.pre} ([issue
    1212](https://github.com/scrapy/scrapy/issues/1212){.reference
    .external}).

-   sitemaps extraction from [`robots.txt`{.docutils .literal
    .notranslate}]{.pre} is now case-insensitive ([issue
    1902](https://github.com/scrapy/scrapy/issues/1902){.reference
    .external}).

-   HTTPS+CONNECT tunnels could get mixed up when using multiple proxies
    to same remote host ([issue
    1912](https://github.com/scrapy/scrapy/issues/1912){.reference
    .external}).
:::
:::

::: {#scrapy-1-0-7-2017-03-03 .section}
[]{#release-1-0-7}

#### Scrapy 1.0.7 (2017-03-03)[¶](#scrapy-1-0-7-2017-03-03 "Permalink to this heading"){.headerlink}

-   Packaging fix: disallow unsupported Twisted versions in setup.py
:::

::: {#scrapy-1-0-6-2016-05-04 .section}
[]{#release-1-0-6}

#### Scrapy 1.0.6 (2016-05-04)[¶](#scrapy-1-0-6-2016-05-04 "Permalink to this heading"){.headerlink}

-   FIX: RetryMiddleware is now robust to non-standard HTTP status codes
    ([issue
    1857](https://github.com/scrapy/scrapy/issues/1857){.reference
    .external})

-   FIX: Filestorage HTTP cache was checking wrong modified time ([issue
    1875](https://github.com/scrapy/scrapy/issues/1875){.reference
    .external})

-   DOC: Support for Sphinx 1.4+ ([issue
    1893](https://github.com/scrapy/scrapy/issues/1893){.reference
    .external})

-   DOC: Consistency in selectors examples ([issue
    1869](https://github.com/scrapy/scrapy/issues/1869){.reference
    .external})
:::

::: {#scrapy-1-0-5-2016-02-04 .section}
[]{#release-1-0-5}

#### Scrapy 1.0.5 (2016-02-04)[¶](#scrapy-1-0-5-2016-02-04 "Permalink to this heading"){.headerlink}

-   FIX: \[Backport\] Ignore bogus links in LinkExtractors (fixes [issue
    907](https://github.com/scrapy/scrapy/issues/907){.reference
    .external}, [commit
    108195e](https://github.com/scrapy/scrapy/commit/108195e){.reference
    .external})

-   TST: Changed buildbot makefile to use 'pytest' ([commit
    1f3d90a](https://github.com/scrapy/scrapy/commit/1f3d90a){.reference
    .external})

-   DOC: Fixed typos in tutorial and media-pipeline ([commit
    808a9ea](https://github.com/scrapy/scrapy/commit/808a9ea){.reference
    .external} and [commit
    803bd87](https://github.com/scrapy/scrapy/commit/803bd87){.reference
    .external})

-   DOC: Add AjaxCrawlMiddleware to DOWNLOADER_MIDDLEWARES_BASE in
    settings docs ([commit
    aa94121](https://github.com/scrapy/scrapy/commit/aa94121){.reference
    .external})
:::

::: {#scrapy-1-0-4-2015-12-30 .section}
[]{#release-1-0-4}

#### Scrapy 1.0.4 (2015-12-30)[¶](#scrapy-1-0-4-2015-12-30 "Permalink to this heading"){.headerlink}

-   Ignoring xlib/tx folder, depending on Twisted version. ([commit
    7dfa979](https://github.com/scrapy/scrapy/commit/7dfa979){.reference
    .external})

-   Run on new travis-ci infra ([commit
    6e42f0b](https://github.com/scrapy/scrapy/commit/6e42f0b){.reference
    .external})

-   Spelling fixes ([commit
    823a1cc](https://github.com/scrapy/scrapy/commit/823a1cc){.reference
    .external})

-   escape nodename in xmliter regex ([commit
    da3c155](https://github.com/scrapy/scrapy/commit/da3c155){.reference
    .external})

-   test xml nodename with dots ([commit
    4418fc3](https://github.com/scrapy/scrapy/commit/4418fc3){.reference
    .external})

-   TST don't use broken Pillow version in tests ([commit
    a55078c](https://github.com/scrapy/scrapy/commit/a55078c){.reference
    .external})

-   disable log on version command. closes #1426 ([commit
    86fc330](https://github.com/scrapy/scrapy/commit/86fc330){.reference
    .external})

-   disable log on startproject command ([commit
    db4c9fe](https://github.com/scrapy/scrapy/commit/db4c9fe){.reference
    .external})

-   Add PyPI download stats badge ([commit
    df2b944](https://github.com/scrapy/scrapy/commit/df2b944){.reference
    .external})

-   don't run tests twice on Travis if a PR is made from a scrapy/scrapy
    branch ([commit
    a83ab41](https://github.com/scrapy/scrapy/commit/a83ab41){.reference
    .external})

-   Add Python 3 porting status badge to the README ([commit
    73ac80d](https://github.com/scrapy/scrapy/commit/73ac80d){.reference
    .external})

-   fixed RFPDupeFilter persistence ([commit
    97d080e](https://github.com/scrapy/scrapy/commit/97d080e){.reference
    .external})

-   TST a test to show that dupefilter persistence is not working
    ([commit
    97f2fb3](https://github.com/scrapy/scrapy/commit/97f2fb3){.reference
    .external})

-   explicit close file on [file://](file://){.reference .external}
    scheme handler ([commit
    d9b4850](https://github.com/scrapy/scrapy/commit/d9b4850){.reference
    .external})

-   Disable dupefilter in shell ([commit
    c0d0734](https://github.com/scrapy/scrapy/commit/c0d0734){.reference
    .external})

-   DOC: Add captions to toctrees which appear in sidebar ([commit
    aa239ad](https://github.com/scrapy/scrapy/commit/aa239ad){.reference
    .external})

-   DOC Removed pywin32 from install instructions as it's already
    declared as dependency. ([commit
    10eb400](https://github.com/scrapy/scrapy/commit/10eb400){.reference
    .external})

-   Added installation notes about using Conda for Windows and other
    OSes. ([commit
    1c3600a](https://github.com/scrapy/scrapy/commit/1c3600a){.reference
    .external})

-   Fixed minor grammar issues. ([commit
    7f4ddd5](https://github.com/scrapy/scrapy/commit/7f4ddd5){.reference
    .external})

-   fixed a typo in the documentation. ([commit
    b71f677](https://github.com/scrapy/scrapy/commit/b71f677){.reference
    .external})

-   Version 1 now exists ([commit
    5456c0e](https://github.com/scrapy/scrapy/commit/5456c0e){.reference
    .external})

-   fix another invalid xpath error ([commit
    0a1366e](https://github.com/scrapy/scrapy/commit/0a1366e){.reference
    .external})

-   fix ValueError: Invalid XPath: //div/\[id="not-exists"\]/text() on
    selectors.rst ([commit
    ca8d60f](https://github.com/scrapy/scrapy/commit/ca8d60f){.reference
    .external})

-   Typos corrections ([commit
    7067117](https://github.com/scrapy/scrapy/commit/7067117){.reference
    .external})

-   fix typos in downloader-middleware.rst and exceptions.rst, middlware
    -\> middleware ([commit
    32f115c](https://github.com/scrapy/scrapy/commit/32f115c){.reference
    .external})

-   Add note to Ubuntu install section about Debian compatibility
    ([commit
    23fda69](https://github.com/scrapy/scrapy/commit/23fda69){.reference
    .external})

-   Replace alternative macOS install workaround with virtualenv
    ([commit
    98b63ee](https://github.com/scrapy/scrapy/commit/98b63ee){.reference
    .external})

-   Reference Homebrew's homepage for installation instructions ([commit
    1925db1](https://github.com/scrapy/scrapy/commit/1925db1){.reference
    .external})

-   Add oldest supported tox version to contributing docs ([commit
    5d10d6d](https://github.com/scrapy/scrapy/commit/5d10d6d){.reference
    .external})

-   Note in install docs about pip being already included in
    python\>=2.7.9 ([commit
    85c980e](https://github.com/scrapy/scrapy/commit/85c980e){.reference
    .external})

-   Add non-python dependencies to Ubuntu install section in the docs
    ([commit
    fbd010d](https://github.com/scrapy/scrapy/commit/fbd010d){.reference
    .external})

-   Add macOS installation section to docs ([commit
    d8f4cba](https://github.com/scrapy/scrapy/commit/d8f4cba){.reference
    .external})

-   DOC(ENH): specify path to rtd theme explicitly ([commit
    de73b1a](https://github.com/scrapy/scrapy/commit/de73b1a){.reference
    .external})

-   minor: scrapy.Spider docs grammar ([commit
    1ddcc7b](https://github.com/scrapy/scrapy/commit/1ddcc7b){.reference
    .external})

-   Make common practices sample code match the comments ([commit
    1b85bcf](https://github.com/scrapy/scrapy/commit/1b85bcf){.reference
    .external})

-   nextcall repetitive calls (heartbeats). ([commit
    55f7104](https://github.com/scrapy/scrapy/commit/55f7104){.reference
    .external})

-   Backport fix compatibility with Twisted 15.4.0 ([commit
    b262411](https://github.com/scrapy/scrapy/commit/b262411){.reference
    .external})

-   pin pytest to 2.7.3 ([commit
    a6535c2](https://github.com/scrapy/scrapy/commit/a6535c2){.reference
    .external})

-   Merge pull request #1512 from mgedmin/patch-1 ([commit
    8876111](https://github.com/scrapy/scrapy/commit/8876111){.reference
    .external})

-   Merge pull request #1513 from mgedmin/patch-2 ([commit
    5d4daf8](https://github.com/scrapy/scrapy/commit/5d4daf8){.reference
    .external})

-   Typo ([commit
    f8d0682](https://github.com/scrapy/scrapy/commit/f8d0682){.reference
    .external})

-   Fix list formatting ([commit
    5f83a93](https://github.com/scrapy/scrapy/commit/5f83a93){.reference
    .external})

-   fix Scrapy squeue tests after recent changes to queuelib ([commit
    3365c01](https://github.com/scrapy/scrapy/commit/3365c01){.reference
    .external})

-   Merge pull request #1475 from rweindl/patch-1 ([commit
    2d688cd](https://github.com/scrapy/scrapy/commit/2d688cd){.reference
    .external})

-   Update tutorial.rst ([commit
    fbc1f25](https://github.com/scrapy/scrapy/commit/fbc1f25){.reference
    .external})

-   Merge pull request #1449 from rhoekman/patch-1 ([commit
    7d6538c](https://github.com/scrapy/scrapy/commit/7d6538c){.reference
    .external})

-   Small grammatical change ([commit
    8752294](https://github.com/scrapy/scrapy/commit/8752294){.reference
    .external})

-   Add openssl version to version command ([commit
    13c45ac](https://github.com/scrapy/scrapy/commit/13c45ac){.reference
    .external})
:::

::: {#scrapy-1-0-3-2015-08-11 .section}
[]{#release-1-0-3}

#### Scrapy 1.0.3 (2015-08-11)[¶](#scrapy-1-0-3-2015-08-11 "Permalink to this heading"){.headerlink}

-   add service_identity to Scrapy install_requires ([commit
    cbc2501](https://github.com/scrapy/scrapy/commit/cbc2501){.reference
    .external})

-   Workaround for travis#296 ([commit
    66af9cd](https://github.com/scrapy/scrapy/commit/66af9cd){.reference
    .external})
:::

::: {#scrapy-1-0-2-2015-08-06 .section}
[]{#release-1-0-2}

#### Scrapy 1.0.2 (2015-08-06)[¶](#scrapy-1-0-2-2015-08-06 "Permalink to this heading"){.headerlink}

-   Twisted 15.3.0 does not raises PicklingError serializing lambda
    functions ([commit
    b04dd7d](https://github.com/scrapy/scrapy/commit/b04dd7d){.reference
    .external})

-   Minor method name fix ([commit
    6f85c7f](https://github.com/scrapy/scrapy/commit/6f85c7f){.reference
    .external})

-   minor: scrapy.Spider grammar and clarity ([commit
    9c9d2e0](https://github.com/scrapy/scrapy/commit/9c9d2e0){.reference
    .external})

-   Put a blurb about support channels in CONTRIBUTING ([commit
    c63882b](https://github.com/scrapy/scrapy/commit/c63882b){.reference
    .external})

-   Fixed typos ([commit
    a9ae7b0](https://github.com/scrapy/scrapy/commit/a9ae7b0){.reference
    .external})

-   Fix doc reference. ([commit
    7c8a4fe](https://github.com/scrapy/scrapy/commit/7c8a4fe){.reference
    .external})
:::

::: {#scrapy-1-0-1-2015-07-01 .section}
[]{#release-1-0-1}

#### Scrapy 1.0.1 (2015-07-01)[¶](#scrapy-1-0-1-2015-07-01 "Permalink to this heading"){.headerlink}

-   Unquote request path before passing to FTPClient, it already escape
    paths ([commit
    cc00ad2](https://github.com/scrapy/scrapy/commit/cc00ad2){.reference
    .external})

-   include tests/ to source distribution in MANIFEST.in ([commit
    eca227e](https://github.com/scrapy/scrapy/commit/eca227e){.reference
    .external})

-   DOC Fix SelectJmes documentation ([commit
    b8567bc](https://github.com/scrapy/scrapy/commit/b8567bc){.reference
    .external})

-   DOC Bring Ubuntu and Archlinux outside of Windows subsection
    ([commit
    392233f](https://github.com/scrapy/scrapy/commit/392233f){.reference
    .external})

-   DOC remove version suffix from Ubuntu package ([commit
    5303c66](https://github.com/scrapy/scrapy/commit/5303c66){.reference
    .external})

-   DOC Update release date for 1.0 ([commit
    c89fa29](https://github.com/scrapy/scrapy/commit/c89fa29){.reference
    .external})
:::

::: {#scrapy-1-0-0-2015-06-19 .section}
[]{#release-1-0-0}

#### Scrapy 1.0.0 (2015-06-19)[¶](#scrapy-1-0-0-2015-06-19 "Permalink to this heading"){.headerlink}

You will find a lot of new features and bugfixes in this major release.
Make sure to check our updated [[overview]{.std
.std-ref}](index.html#intro-overview){.hoverxref .tooltip .reference
.internal} to get a glance of some of the changes, along with our
brushed [[tutorial]{.std
.std-ref}](index.html#intro-tutorial){.hoverxref .tooltip .reference
.internal}.

::: {#support-for-returning-dictionaries-in-spiders .section}
##### Support for returning dictionaries in spiders[¶](#support-for-returning-dictionaries-in-spiders "Permalink to this heading"){.headerlink}

Declaring and returning Scrapy Items is no longer necessary to collect
the scraped data from your spider, you can now return explicit
dictionaries instead.

*Classic version*

::: {.highlight-default .notranslate}
::: highlight
    class MyItem(scrapy.Item):
        url = scrapy.Field()

    class MySpider(scrapy.Spider):
        def parse(self, response):
            return MyItem(url=response.url)
:::
:::

*New version*

::: {.highlight-default .notranslate}
::: highlight
    class MySpider(scrapy.Spider):
        def parse(self, response):
            return {'url': response.url}
:::
:::
:::

::: {#per-spider-settings-gsoc-2014 .section}
##### Per-spider settings (GSoC 2014)[¶](#per-spider-settings-gsoc-2014 "Permalink to this heading"){.headerlink}

Last Google Summer of Code project accomplished an important redesign of
the mechanism used for populating settings, introducing explicit
priorities to override any given setting. As an extension of that goal,
we included a new level of priority for settings that act exclusively
for a single spider, allowing them to redefine project settings.

Start using it by defining a [`custom_settings`{.xref .py .py-attr
.docutils .literal .notranslate}]{.pre} class variable in your spider:

::: {.highlight-default .notranslate}
::: highlight
    class MySpider(scrapy.Spider):
        custom_settings = {
            "DOWNLOAD_DELAY": 5.0,
            "RETRY_ENABLED": False,
        }
:::
:::

Read more about settings population: [[Settings]{.std
.std-ref}](index.html#topics-settings){.hoverxref .tooltip .reference
.internal}
:::

::: {#python-logging .section}
##### Python Logging[¶](#python-logging "Permalink to this heading"){.headerlink}

Scrapy 1.0 has moved away from Twisted logging to support Python built
in's as default logging system. We're maintaining backward compatibility
for most of the old custom interface to call logging functions, but
you'll get warnings to switch to the Python logging API entirely.

*Old version*

::: {.highlight-default .notranslate}
::: highlight
    from scrapy import log
    log.msg('MESSAGE', log.INFO)
:::
:::

*New version*

::: {.highlight-default .notranslate}
::: highlight
    import logging
    logging.info('MESSAGE')
:::
:::

Logging with spiders remains the same, but on top of the [`log()`{.xref
.py .py-meth .docutils .literal .notranslate}]{.pre} method you'll have
access to a custom [`logger`{.xref .py .py-attr .docutils .literal
.notranslate}]{.pre} created for the spider to issue log events:

::: {.highlight-default .notranslate}
::: highlight
    class MySpider(scrapy.Spider):
        def parse(self, response):
            self.logger.info('Response received')
:::
:::

Read more in the logging documentation: [[Logging]{.std
.std-ref}](index.html#topics-logging){.hoverxref .tooltip .reference
.internal}
:::

::: {#crawler-api-refactoring-gsoc-2014 .section}
##### Crawler API refactoring (GSoC 2014)[¶](#crawler-api-refactoring-gsoc-2014 "Permalink to this heading"){.headerlink}

Another milestone for last Google Summer of Code was a refactoring of
the internal API, seeking a simpler and easier usage. Check new core
interface in: [[Core API]{.std
.std-ref}](index.html#topics-api){.hoverxref .tooltip .reference
.internal}

A common situation where you will face these changes is while running
Scrapy from scripts. Here's a quick example of how to run a Spider
manually with the new API:

::: {.highlight-default .notranslate}
::: highlight
    from scrapy.crawler import CrawlerProcess

    process = CrawlerProcess({
        'USER_AGENT': 'Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 5.1)'
    })
    process.crawl(MySpider)
    process.start()
:::
:::

Bear in mind this feature is still under development and its API may
change until it reaches a stable status.

See more examples for scripts running Scrapy: [[Common Practices]{.std
.std-ref}](index.html#topics-practices){.hoverxref .tooltip .reference
.internal}
:::

::: {#module-relocations .section}
[]{#id128}

##### Module Relocations[¶](#module-relocations "Permalink to this heading"){.headerlink}

There's been a large rearrangement of modules trying to improve the
general structure of Scrapy. Main changes were separating various
subpackages into new projects and dissolving both
[`scrapy.contrib`{.docutils .literal .notranslate}]{.pre} and
[`scrapy.contrib_exp`{.docutils .literal .notranslate}]{.pre} into top
level packages. Backward compatibility was kept among internal
relocations, while importing deprecated modules expect warnings
indicating their new place.

::: {#full-list-of-relocations .section}
###### Full list of relocations[¶](#full-list-of-relocations "Permalink to this heading"){.headerlink}

Outsourced packages

::: {.admonition .note}
Note

These extensions went through some minor changes, e.g. some setting
names were changed. Please check the documentation in each new
repository to get familiar with the new usage.
:::

+-----------------------------------+-----------------------------------+
| Old location                      | New location                      |
+===================================+===================================+
| scrapy.commands.deploy            | [sc                               |
|                                   | rapyd-client](https://github.com/ |
|                                   | scrapy/scrapyd-client){.reference |
|                                   | .external} (See other             |
|                                   | alternatives here: [[Deploying    |
|                                   | Spiders]{.std                     |
|                                   | .std-ref}](ind                    |
|                                   | ex.html#topics-deploy){.hoverxref |
|                                   | .tooltip .reference .internal})   |
+-----------------------------------+-----------------------------------+
| scrapy.contrib.djangoitem         | [scrapy-djangoite                 |
|                                   | m](https://github.com/scrapy-plug |
|                                   | ins/scrapy-djangoitem){.reference |
|                                   | .external}                        |
+-----------------------------------+-----------------------------------+
| scrapy.webservice                 | [scrapy-jso                       |
|                                   | nrpc](https://github.com/scrapy-p |
|                                   | lugins/scrapy-jsonrpc){.reference |
|                                   | .external}                        |
+-----------------------------------+-----------------------------------+

[`scrapy.contrib_exp`{.docutils .literal .notranslate}]{.pre} and
[`scrapy.contrib`{.docutils .literal .notranslate}]{.pre} dissolutions

+-----------------------------------+-----------------------------------+
| Old location                      | New location                      |
+===================================+===================================+
| scrapy.contrib_exp.d              | scrapy.do                         |
| ownloadermiddleware.decompression | wnloadermiddlewares.decompression |
+-----------------------------------+-----------------------------------+
| scrapy.contrib_exp.iterators      | scrapy.utils.iterators            |
+-----------------------------------+-----------------------------------+
| sc                                | scrapy.downloadermiddlewares      |
| rapy.contrib.downloadermiddleware |                                   |
+-----------------------------------+-----------------------------------+
| scrapy.contrib.exporter           | scrapy.exporters                  |
+-----------------------------------+-----------------------------------+
| scrapy.contrib.linkextractors     | scrapy.linkextractors             |
+-----------------------------------+-----------------------------------+
| scrapy.contrib.loader             | scrapy.loader                     |
+-----------------------------------+-----------------------------------+
| scrapy.contrib.loader.processor   | scrapy.loader.processors          |
+-----------------------------------+-----------------------------------+
| scrapy.contrib.pipeline           | scrapy.pipelines                  |
+-----------------------------------+-----------------------------------+
| scrapy.contrib.spidermiddleware   | scrapy.spidermiddlewares          |
+-----------------------------------+-----------------------------------+
| scrapy.contrib.spiders            | scrapy.spiders                    |
+-----------------------------------+-----------------------------------+
| -   scrapy.contrib.closespider    | scrapy.extensions.\*              |
|                                   |                                   |
| -   scrapy.contrib.corestats      |                                   |
|                                   |                                   |
| -   scrapy.contrib.debug          |                                   |
|                                   |                                   |
| -   scrapy.contrib.feedexport     |                                   |
|                                   |                                   |
| -   scrapy.contrib.httpcache      |                                   |
|                                   |                                   |
| -   scrapy.contrib.logstats       |                                   |
|                                   |                                   |
| -   scrapy.contrib.memdebug       |                                   |
|                                   |                                   |
| -   scrapy.contrib.memusage       |                                   |
|                                   |                                   |
| -   scrapy.contrib.spiderstate    |                                   |
|                                   |                                   |
| -   scrapy.contrib.statsmailer    |                                   |
|                                   |                                   |
| -   scrapy.contrib.throttle       |                                   |
+-----------------------------------+-----------------------------------+

Plural renames and Modules unification

+-----------------------------------+-----------------------------------+
| Old location                      | New location                      |
+===================================+===================================+
| scrapy.command                    | scrapy.commands                   |
+-----------------------------------+-----------------------------------+
| scrapy.dupefilter                 | scrapy.dupefilters                |
+-----------------------------------+-----------------------------------+
| scrapy.linkextractor              | scrapy.linkextractors             |
+-----------------------------------+-----------------------------------+
| scrapy.spider                     | scrapy.spiders                    |
+-----------------------------------+-----------------------------------+
| scrapy.squeue                     | scrapy.squeues                    |
+-----------------------------------+-----------------------------------+
| scrapy.statscol                   | scrapy.statscollectors            |
+-----------------------------------+-----------------------------------+
| scrapy.utils.decorator            | scrapy.utils.decorators           |
+-----------------------------------+-----------------------------------+

Class renames

+-----------------------------------+-----------------------------------+
| Old location                      | New location                      |
+===================================+===================================+
| s                                 | scrapy.spiderloader.SpiderLoader  |
| crapy.spidermanager.SpiderManager |                                   |
+-----------------------------------+-----------------------------------+

Settings renames

+-----------------------------------+-----------------------------------+
| Old location                      | New location                      |
+===================================+===================================+
| SPIDER_MANAGER_CLASS              | SPIDER_LOADER_CLASS               |
+-----------------------------------+-----------------------------------+
:::
:::

::: {#changelog .section}
##### Changelog[¶](#changelog "Permalink to this heading"){.headerlink}

New Features and Enhancements

-   Python logging ([issue
    1060](https://github.com/scrapy/scrapy/issues/1060){.reference
    .external}, [issue
    1235](https://github.com/scrapy/scrapy/issues/1235){.reference
    .external}, [issue
    1236](https://github.com/scrapy/scrapy/issues/1236){.reference
    .external}, [issue
    1240](https://github.com/scrapy/scrapy/issues/1240){.reference
    .external}, [issue
    1259](https://github.com/scrapy/scrapy/issues/1259){.reference
    .external}, [issue
    1278](https://github.com/scrapy/scrapy/issues/1278){.reference
    .external}, [issue
    1286](https://github.com/scrapy/scrapy/issues/1286){.reference
    .external})

-   FEED_EXPORT_FIELDS option ([issue
    1159](https://github.com/scrapy/scrapy/issues/1159){.reference
    .external}, [issue
    1224](https://github.com/scrapy/scrapy/issues/1224){.reference
    .external})

-   Dns cache size and timeout options ([issue
    1132](https://github.com/scrapy/scrapy/issues/1132){.reference
    .external})

-   support namespace prefix in xmliter_lxml ([issue
    963](https://github.com/scrapy/scrapy/issues/963){.reference
    .external})

-   Reactor threadpool max size setting ([issue
    1123](https://github.com/scrapy/scrapy/issues/1123){.reference
    .external})

-   Allow spiders to return dicts. ([issue
    1081](https://github.com/scrapy/scrapy/issues/1081){.reference
    .external})

-   Add Response.urljoin() helper ([issue
    1086](https://github.com/scrapy/scrapy/issues/1086){.reference
    .external})

-   look in \~/.config/scrapy.cfg for user config ([issue
    1098](https://github.com/scrapy/scrapy/issues/1098){.reference
    .external})

-   handle TLS SNI ([issue
    1101](https://github.com/scrapy/scrapy/issues/1101){.reference
    .external})

-   Selectorlist extract first ([issue
    624](https://github.com/scrapy/scrapy/issues/624){.reference
    .external}, [issue
    1145](https://github.com/scrapy/scrapy/issues/1145){.reference
    .external})

-   Added JmesSelect ([issue
    1016](https://github.com/scrapy/scrapy/issues/1016){.reference
    .external})

-   add gzip compression to filesystem http cache backend ([issue
    1020](https://github.com/scrapy/scrapy/issues/1020){.reference
    .external})

-   CSS support in link extractors ([issue
    983](https://github.com/scrapy/scrapy/issues/983){.reference
    .external})

-   httpcache dont_cache meta #19 #689 ([issue
    821](https://github.com/scrapy/scrapy/issues/821){.reference
    .external})

-   add signal to be sent when request is dropped by the scheduler
    ([issue 961](https://github.com/scrapy/scrapy/issues/961){.reference
    .external})

-   avoid download large response ([issue
    946](https://github.com/scrapy/scrapy/issues/946){.reference
    .external})

-   Allow to specify the quotechar in CSVFeedSpider ([issue
    882](https://github.com/scrapy/scrapy/issues/882){.reference
    .external})

-   Add referer to "Spider error processing" log message ([issue
    795](https://github.com/scrapy/scrapy/issues/795){.reference
    .external})

-   process robots.txt once ([issue
    896](https://github.com/scrapy/scrapy/issues/896){.reference
    .external})

-   GSoC Per-spider settings ([issue
    854](https://github.com/scrapy/scrapy/issues/854){.reference
    .external})

-   Add project name validation ([issue
    817](https://github.com/scrapy/scrapy/issues/817){.reference
    .external})

-   GSoC API cleanup ([issue
    816](https://github.com/scrapy/scrapy/issues/816){.reference
    .external}, [issue
    1128](https://github.com/scrapy/scrapy/issues/1128){.reference
    .external}, [issue
    1147](https://github.com/scrapy/scrapy/issues/1147){.reference
    .external}, [issue
    1148](https://github.com/scrapy/scrapy/issues/1148){.reference
    .external}, [issue
    1156](https://github.com/scrapy/scrapy/issues/1156){.reference
    .external}, [issue
    1185](https://github.com/scrapy/scrapy/issues/1185){.reference
    .external}, [issue
    1187](https://github.com/scrapy/scrapy/issues/1187){.reference
    .external}, [issue
    1258](https://github.com/scrapy/scrapy/issues/1258){.reference
    .external}, [issue
    1268](https://github.com/scrapy/scrapy/issues/1268){.reference
    .external}, [issue
    1276](https://github.com/scrapy/scrapy/issues/1276){.reference
    .external}, [issue
    1285](https://github.com/scrapy/scrapy/issues/1285){.reference
    .external}, [issue
    1284](https://github.com/scrapy/scrapy/issues/1284){.reference
    .external})

-   Be more responsive with IO operations ([issue
    1074](https://github.com/scrapy/scrapy/issues/1074){.reference
    .external} and [issue
    1075](https://github.com/scrapy/scrapy/issues/1075){.reference
    .external})

-   Do leveldb compaction for httpcache on closing ([issue
    1297](https://github.com/scrapy/scrapy/issues/1297){.reference
    .external})

Deprecations and Removals

-   Deprecate htmlparser link extractor ([issue
    1205](https://github.com/scrapy/scrapy/issues/1205){.reference
    .external})

-   remove deprecated code from FeedExporter ([issue
    1155](https://github.com/scrapy/scrapy/issues/1155){.reference
    .external})

-   a leftover for.15 compatibility ([issue
    925](https://github.com/scrapy/scrapy/issues/925){.reference
    .external})

-   drop support for CONCURRENT_REQUESTS_PER_SPIDER ([issue
    895](https://github.com/scrapy/scrapy/issues/895){.reference
    .external})

-   Drop old engine code ([issue
    911](https://github.com/scrapy/scrapy/issues/911){.reference
    .external})

-   Deprecate SgmlLinkExtractor ([issue
    777](https://github.com/scrapy/scrapy/issues/777){.reference
    .external})

Relocations

-   Move exporters/\_\_init\_\_.py to exporters.py ([issue
    1242](https://github.com/scrapy/scrapy/issues/1242){.reference
    .external})

-   Move base classes to their packages ([issue
    1218](https://github.com/scrapy/scrapy/issues/1218){.reference
    .external}, [issue
    1233](https://github.com/scrapy/scrapy/issues/1233){.reference
    .external})

-   Module relocation ([issue
    1181](https://github.com/scrapy/scrapy/issues/1181){.reference
    .external}, [issue
    1210](https://github.com/scrapy/scrapy/issues/1210){.reference
    .external})

-   rename SpiderManager to SpiderLoader ([issue
    1166](https://github.com/scrapy/scrapy/issues/1166){.reference
    .external})

-   Remove djangoitem ([issue
    1177](https://github.com/scrapy/scrapy/issues/1177){.reference
    .external})

-   remove scrapy deploy command ([issue
    1102](https://github.com/scrapy/scrapy/issues/1102){.reference
    .external})

-   dissolve contrib_exp ([issue
    1134](https://github.com/scrapy/scrapy/issues/1134){.reference
    .external})

-   Deleted bin folder from root, fixes #913 ([issue
    914](https://github.com/scrapy/scrapy/issues/914){.reference
    .external})

-   Remove jsonrpc based webservice ([issue
    859](https://github.com/scrapy/scrapy/issues/859){.reference
    .external})

-   Move Test cases under project root dir ([issue
    827](https://github.com/scrapy/scrapy/issues/827){.reference
    .external}, [issue
    841](https://github.com/scrapy/scrapy/issues/841){.reference
    .external})

-   Fix backward incompatibility for relocated paths in settings ([issue
    1267](https://github.com/scrapy/scrapy/issues/1267){.reference
    .external})

Documentation

-   CrawlerProcess documentation ([issue
    1190](https://github.com/scrapy/scrapy/issues/1190){.reference
    .external})

-   Favoring web scraping over screen scraping in the descriptions
    ([issue
    1188](https://github.com/scrapy/scrapy/issues/1188){.reference
    .external})

-   Some improvements for Scrapy tutorial ([issue
    1180](https://github.com/scrapy/scrapy/issues/1180){.reference
    .external})

-   Documenting Files Pipeline together with Images Pipeline ([issue
    1150](https://github.com/scrapy/scrapy/issues/1150){.reference
    .external})

-   deployment docs tweaks ([issue
    1164](https://github.com/scrapy/scrapy/issues/1164){.reference
    .external})

-   Added deployment section covering scrapyd-deploy and shub ([issue
    1124](https://github.com/scrapy/scrapy/issues/1124){.reference
    .external})

-   Adding more settings to project template ([issue
    1073](https://github.com/scrapy/scrapy/issues/1073){.reference
    .external})

-   some improvements to overview page ([issue
    1106](https://github.com/scrapy/scrapy/issues/1106){.reference
    .external})

-   Updated link in docs/topics/architecture.rst ([issue
    647](https://github.com/scrapy/scrapy/issues/647){.reference
    .external})

-   DOC reorder topics ([issue
    1022](https://github.com/scrapy/scrapy/issues/1022){.reference
    .external})

-   updating list of Request.meta special keys ([issue
    1071](https://github.com/scrapy/scrapy/issues/1071){.reference
    .external})

-   DOC document download_timeout ([issue
    898](https://github.com/scrapy/scrapy/issues/898){.reference
    .external})

-   DOC simplify extension docs ([issue
    893](https://github.com/scrapy/scrapy/issues/893){.reference
    .external})

-   Leaks docs ([issue
    894](https://github.com/scrapy/scrapy/issues/894){.reference
    .external})

-   DOC document from_crawler method for item pipelines ([issue
    904](https://github.com/scrapy/scrapy/issues/904){.reference
    .external})

-   Spider_error doesn't support deferreds ([issue
    1292](https://github.com/scrapy/scrapy/issues/1292){.reference
    .external})

-   Corrections & Sphinx related fixes ([issue
    1220](https://github.com/scrapy/scrapy/issues/1220){.reference
    .external}, [issue
    1219](https://github.com/scrapy/scrapy/issues/1219){.reference
    .external}, [issue
    1196](https://github.com/scrapy/scrapy/issues/1196){.reference
    .external}, [issue
    1172](https://github.com/scrapy/scrapy/issues/1172){.reference
    .external}, [issue
    1171](https://github.com/scrapy/scrapy/issues/1171){.reference
    .external}, [issue
    1169](https://github.com/scrapy/scrapy/issues/1169){.reference
    .external}, [issue
    1160](https://github.com/scrapy/scrapy/issues/1160){.reference
    .external}, [issue
    1154](https://github.com/scrapy/scrapy/issues/1154){.reference
    .external}, [issue
    1127](https://github.com/scrapy/scrapy/issues/1127){.reference
    .external}, [issue
    1112](https://github.com/scrapy/scrapy/issues/1112){.reference
    .external}, [issue
    1105](https://github.com/scrapy/scrapy/issues/1105){.reference
    .external}, [issue
    1041](https://github.com/scrapy/scrapy/issues/1041){.reference
    .external}, [issue
    1082](https://github.com/scrapy/scrapy/issues/1082){.reference
    .external}, [issue
    1033](https://github.com/scrapy/scrapy/issues/1033){.reference
    .external}, [issue
    944](https://github.com/scrapy/scrapy/issues/944){.reference
    .external}, [issue
    866](https://github.com/scrapy/scrapy/issues/866){.reference
    .external}, [issue
    864](https://github.com/scrapy/scrapy/issues/864){.reference
    .external}, [issue
    796](https://github.com/scrapy/scrapy/issues/796){.reference
    .external}, [issue
    1260](https://github.com/scrapy/scrapy/issues/1260){.reference
    .external}, [issue
    1271](https://github.com/scrapy/scrapy/issues/1271){.reference
    .external}, [issue
    1293](https://github.com/scrapy/scrapy/issues/1293){.reference
    .external}, [issue
    1298](https://github.com/scrapy/scrapy/issues/1298){.reference
    .external})

Bugfixes

-   Item multi inheritance fix ([issue
    353](https://github.com/scrapy/scrapy/issues/353){.reference
    .external}, [issue
    1228](https://github.com/scrapy/scrapy/issues/1228){.reference
    .external})

-   ItemLoader.load_item: iterate over copy of fields ([issue
    722](https://github.com/scrapy/scrapy/issues/722){.reference
    .external})

-   Fix Unhandled error in Deferred (RobotsTxtMiddleware) ([issue
    1131](https://github.com/scrapy/scrapy/issues/1131){.reference
    .external}, [issue
    1197](https://github.com/scrapy/scrapy/issues/1197){.reference
    .external})

-   Force to read DOWNLOAD_TIMEOUT as int ([issue
    954](https://github.com/scrapy/scrapy/issues/954){.reference
    .external})

-   scrapy.utils.misc.load_object should print full traceback ([issue
    902](https://github.com/scrapy/scrapy/issues/902){.reference
    .external})

-   Fix bug for ".local" host name ([issue
    878](https://github.com/scrapy/scrapy/issues/878){.reference
    .external})

-   Fix for Enabled extensions, middlewares, pipelines info not printed
    anymore ([issue
    879](https://github.com/scrapy/scrapy/issues/879){.reference
    .external})

-   fix dont_merge_cookies bad behaviour when set to false on meta
    ([issue 846](https://github.com/scrapy/scrapy/issues/846){.reference
    .external})

Python 3 In Progress Support

-   disable scrapy.telnet if twisted.conch is not available ([issue
    1161](https://github.com/scrapy/scrapy/issues/1161){.reference
    .external})

-   fix Python 3 syntax errors in ajaxcrawl.py ([issue
    1162](https://github.com/scrapy/scrapy/issues/1162){.reference
    .external})

-   more python3 compatibility changes for urllib ([issue
    1121](https://github.com/scrapy/scrapy/issues/1121){.reference
    .external})

-   assertItemsEqual was renamed to assertCountEqual in Python 3.
    ([issue
    1070](https://github.com/scrapy/scrapy/issues/1070){.reference
    .external})

-   Import unittest.mock if available. ([issue
    1066](https://github.com/scrapy/scrapy/issues/1066){.reference
    .external})

-   updated deprecated cgi.parse_qsl to use six's parse_qsl ([issue
    909](https://github.com/scrapy/scrapy/issues/909){.reference
    .external})

-   Prevent Python 3 port regressions ([issue
    830](https://github.com/scrapy/scrapy/issues/830){.reference
    .external})

-   PY3: use MutableMapping for python 3 ([issue
    810](https://github.com/scrapy/scrapy/issues/810){.reference
    .external})

-   PY3: use six.BytesIO and six.moves.cStringIO ([issue
    803](https://github.com/scrapy/scrapy/issues/803){.reference
    .external})

-   PY3: fix xmlrpclib and email imports ([issue
    801](https://github.com/scrapy/scrapy/issues/801){.reference
    .external})

-   PY3: use six for robotparser and urlparse ([issue
    800](https://github.com/scrapy/scrapy/issues/800){.reference
    .external})

-   PY3: use six.iterkeys, six.iteritems, and tempfile ([issue
    799](https://github.com/scrapy/scrapy/issues/799){.reference
    .external})

-   PY3: fix has_key and use six.moves.configparser ([issue
    798](https://github.com/scrapy/scrapy/issues/798){.reference
    .external})

-   PY3: use six.moves.cPickle ([issue
    797](https://github.com/scrapy/scrapy/issues/797){.reference
    .external})

-   PY3 make it possible to run some tests in Python3 ([issue
    776](https://github.com/scrapy/scrapy/issues/776){.reference
    .external})

Tests

-   remove unnecessary lines from py3-ignores ([issue
    1243](https://github.com/scrapy/scrapy/issues/1243){.reference
    .external})

-   Fix remaining warnings from pytest while collecting tests ([issue
    1206](https://github.com/scrapy/scrapy/issues/1206){.reference
    .external})

-   Add docs build to travis ([issue
    1234](https://github.com/scrapy/scrapy/issues/1234){.reference
    .external})

-   TST don't collect tests from deprecated modules. ([issue
    1165](https://github.com/scrapy/scrapy/issues/1165){.reference
    .external})

-   install service_identity package in tests to prevent warnings
    ([issue
    1168](https://github.com/scrapy/scrapy/issues/1168){.reference
    .external})

-   Fix deprecated settings API in tests ([issue
    1152](https://github.com/scrapy/scrapy/issues/1152){.reference
    .external})

-   Add test for webclient with POST method and no body given ([issue
    1089](https://github.com/scrapy/scrapy/issues/1089){.reference
    .external})

-   py3-ignores.txt supports comments ([issue
    1044](https://github.com/scrapy/scrapy/issues/1044){.reference
    .external})

-   modernize some of the asserts ([issue
    835](https://github.com/scrapy/scrapy/issues/835){.reference
    .external})

-   selector.\_\_repr\_\_ test ([issue
    779](https://github.com/scrapy/scrapy/issues/779){.reference
    .external})

Code refactoring

-   CSVFeedSpider cleanup: use iterate_spider_output ([issue
    1079](https://github.com/scrapy/scrapy/issues/1079){.reference
    .external})

-   remove unnecessary check from scrapy.utils.spider.iter_spider_output
    ([issue
    1078](https://github.com/scrapy/scrapy/issues/1078){.reference
    .external})

-   Pydispatch pep8 ([issue
    992](https://github.com/scrapy/scrapy/issues/992){.reference
    .external})

-   Removed unused 'load=False' parameter from walk_modules() ([issue
    871](https://github.com/scrapy/scrapy/issues/871){.reference
    .external})

-   For consistency, use [`job_dir`{.docutils .literal
    .notranslate}]{.pre} helper in [`SpiderState`{.docutils .literal
    .notranslate}]{.pre} extension. ([issue
    805](https://github.com/scrapy/scrapy/issues/805){.reference
    .external})

-   rename "sflo" local variables to less cryptic "log_observer" ([issue
    775](https://github.com/scrapy/scrapy/issues/775){.reference
    .external})
:::
:::

::: {#scrapy-0-24-6-2015-04-20 .section}
#### Scrapy 0.24.6 (2015-04-20)[¶](#scrapy-0-24-6-2015-04-20 "Permalink to this heading"){.headerlink}

-   encode invalid xpath with unicode_escape under PY2 ([commit
    07cb3e5](https://github.com/scrapy/scrapy/commit/07cb3e5){.reference
    .external})

-   fix IPython shell scope issue and load IPython user config ([commit
    2c8e573](https://github.com/scrapy/scrapy/commit/2c8e573){.reference
    .external})

-   Fix small typo in the docs ([commit
    d694019](https://github.com/scrapy/scrapy/commit/d694019){.reference
    .external})

-   Fix small typo ([commit
    f92fa83](https://github.com/scrapy/scrapy/commit/f92fa83){.reference
    .external})

-   Converted sel.xpath() calls to response.xpath() in Extracting the
    data ([commit
    c2c6d15](https://github.com/scrapy/scrapy/commit/c2c6d15){.reference
    .external})
:::

::: {#scrapy-0-24-5-2015-02-25 .section}
#### Scrapy 0.24.5 (2015-02-25)[¶](#scrapy-0-24-5-2015-02-25 "Permalink to this heading"){.headerlink}

-   Support new \_getEndpoint Agent signatures on Twisted 15.0.0
    ([commit
    540b9bc](https://github.com/scrapy/scrapy/commit/540b9bc){.reference
    .external})

-   DOC a couple more references are fixed ([commit
    b4c454b](https://github.com/scrapy/scrapy/commit/b4c454b){.reference
    .external})

-   DOC fix a reference ([commit
    e3c1260](https://github.com/scrapy/scrapy/commit/e3c1260){.reference
    .external})

-   t.i.b.ThreadedResolver is now a new-style class ([commit
    9e13f42](https://github.com/scrapy/scrapy/commit/9e13f42){.reference
    .external})

-   S3DownloadHandler: fix auth for requests with quoted paths/query
    params ([commit
    cdb9a0b](https://github.com/scrapy/scrapy/commit/cdb9a0b){.reference
    .external})

-   fixed the variable types in mailsender documentation ([commit
    bb3a848](https://github.com/scrapy/scrapy/commit/bb3a848){.reference
    .external})

-   Reset items_scraped instead of item_count ([commit
    edb07a4](https://github.com/scrapy/scrapy/commit/edb07a4){.reference
    .external})

-   Tentative attention message about what document to read for
    contributions ([commit
    7ee6f7a](https://github.com/scrapy/scrapy/commit/7ee6f7a){.reference
    .external})

-   mitmproxy 0.10.1 needs netlib 0.10.1 too ([commit
    874fcdd](https://github.com/scrapy/scrapy/commit/874fcdd){.reference
    .external})

-   pin mitmproxy 0.10.1 as \>0.11 does not work with tests ([commit
    c6b21f0](https://github.com/scrapy/scrapy/commit/c6b21f0){.reference
    .external})

-   Test the parse command locally instead of against an external url
    ([commit
    c3a6628](https://github.com/scrapy/scrapy/commit/c3a6628){.reference
    .external})

-   Patches Twisted issue while closing the connection pool on
    HTTPDownloadHandler ([commit
    d0bf957](https://github.com/scrapy/scrapy/commit/d0bf957){.reference
    .external})

-   Updates documentation on dynamic item classes. ([commit
    eeb589a](https://github.com/scrapy/scrapy/commit/eeb589a){.reference
    .external})

-   Merge pull request #943 from Lazar-T/patch-3 ([commit
    5fdab02](https://github.com/scrapy/scrapy/commit/5fdab02){.reference
    .external})

-   typo ([commit
    b0ae199](https://github.com/scrapy/scrapy/commit/b0ae199){.reference
    .external})

-   pywin32 is required by Twisted. closes #937 ([commit
    5cb0cfb](https://github.com/scrapy/scrapy/commit/5cb0cfb){.reference
    .external})

-   Update install.rst ([commit
    781286b](https://github.com/scrapy/scrapy/commit/781286b){.reference
    .external})

-   Merge pull request #928 from Lazar-T/patch-1 ([commit
    b415d04](https://github.com/scrapy/scrapy/commit/b415d04){.reference
    .external})

-   comma instead of fullstop ([commit
    627b9ba](https://github.com/scrapy/scrapy/commit/627b9ba){.reference
    .external})

-   Merge pull request #885 from jsma/patch-1 ([commit
    de909ad](https://github.com/scrapy/scrapy/commit/de909ad){.reference
    .external})

-   Update request-response.rst ([commit
    3f3263d](https://github.com/scrapy/scrapy/commit/3f3263d){.reference
    .external})

-   SgmlLinkExtractor - fix for parsing \<area\> tag with Unicode
    present ([commit
    49b40f0](https://github.com/scrapy/scrapy/commit/49b40f0){.reference
    .external})
:::

::: {#scrapy-0-24-4-2014-08-09 .section}
#### Scrapy 0.24.4 (2014-08-09)[¶](#scrapy-0-24-4-2014-08-09 "Permalink to this heading"){.headerlink}

-   pem file is used by mockserver and required by scrapy bench ([commit
    5eddc68](https://github.com/scrapy/scrapy/commit/5eddc68){.reference
    .external})

-   scrapy bench needs scrapy.tests\* ([commit
    d6cb999](https://github.com/scrapy/scrapy/commit/d6cb999){.reference
    .external})
:::

::: {#scrapy-0-24-3-2014-08-09 .section}
#### Scrapy 0.24.3 (2014-08-09)[¶](#scrapy-0-24-3-2014-08-09 "Permalink to this heading"){.headerlink}

-   no need to waste travis-ci time on py3 for 0.24 ([commit
    8e080c1](https://github.com/scrapy/scrapy/commit/8e080c1){.reference
    .external})

-   Update installation docs ([commit
    1d0c096](https://github.com/scrapy/scrapy/commit/1d0c096){.reference
    .external})

-   There is a trove classifier for Scrapy framework! ([commit
    4c701d7](https://github.com/scrapy/scrapy/commit/4c701d7){.reference
    .external})

-   update other places where w3lib version is mentioned ([commit
    d109c13](https://github.com/scrapy/scrapy/commit/d109c13){.reference
    .external})

-   Update w3lib requirement to 1.8.0 ([commit
    39d2ce5](https://github.com/scrapy/scrapy/commit/39d2ce5){.reference
    .external})

-   Use w3lib.html.replace_entities() (remove_entities() is deprecated)
    ([commit
    180d3ad](https://github.com/scrapy/scrapy/commit/180d3ad){.reference
    .external})

-   set zip_safe=False ([commit
    a51ee8b](https://github.com/scrapy/scrapy/commit/a51ee8b){.reference
    .external})

-   do not ship tests package ([commit
    ee3b371](https://github.com/scrapy/scrapy/commit/ee3b371){.reference
    .external})

-   scrapy.bat is not needed anymore ([commit
    c3861cf](https://github.com/scrapy/scrapy/commit/c3861cf){.reference
    .external})

-   Modernize setup.py ([commit
    362e322](https://github.com/scrapy/scrapy/commit/362e322){.reference
    .external})

-   headers can not handle non-string values ([commit
    94a5c65](https://github.com/scrapy/scrapy/commit/94a5c65){.reference
    .external})

-   fix ftp test cases ([commit
    a274a7f](https://github.com/scrapy/scrapy/commit/a274a7f){.reference
    .external})

-   The sum up of travis-ci builds are taking like 50min to complete
    ([commit
    ae1e2cc](https://github.com/scrapy/scrapy/commit/ae1e2cc){.reference
    .external})

-   Update shell.rst typo ([commit
    e49c96a](https://github.com/scrapy/scrapy/commit/e49c96a){.reference
    .external})

-   removes weird indentation in the shell results ([commit
    1ca489d](https://github.com/scrapy/scrapy/commit/1ca489d){.reference
    .external})

-   improved explanations, clarified blog post as source, added link for
    XPath string functions in the spec ([commit
    65c8f05](https://github.com/scrapy/scrapy/commit/65c8f05){.reference
    .external})

-   renamed UserTimeoutError and ServerTimeouterror #583 ([commit
    037f6ab](https://github.com/scrapy/scrapy/commit/037f6ab){.reference
    .external})

-   adding some xpath tips to selectors docs ([commit
    2d103e0](https://github.com/scrapy/scrapy/commit/2d103e0){.reference
    .external})

-   fix tests to account for
    [https://github.com/scrapy/w3lib/pull/23](https://github.com/scrapy/w3lib/pull/23){.reference
    .external} ([commit
    f8d366a](https://github.com/scrapy/scrapy/commit/f8d366a){.reference
    .external})

-   get_func_args maximum recursion fix #728 ([commit
    81344ea](https://github.com/scrapy/scrapy/commit/81344ea){.reference
    .external})

-   Updated input/output processor example according to #560. ([commit
    f7c4ea8](https://github.com/scrapy/scrapy/commit/f7c4ea8){.reference
    .external})

-   Fixed Python syntax in tutorial. ([commit
    db59ed9](https://github.com/scrapy/scrapy/commit/db59ed9){.reference
    .external})

-   Add test case for tunneling proxy ([commit
    f090260](https://github.com/scrapy/scrapy/commit/f090260){.reference
    .external})

-   Bugfix for leaking Proxy-Authorization header to remote host when
    using tunneling ([commit
    d8793af](https://github.com/scrapy/scrapy/commit/d8793af){.reference
    .external})

-   Extract links from XHTML documents with MIME-Type "application/xml"
    ([commit
    ed1f376](https://github.com/scrapy/scrapy/commit/ed1f376){.reference
    .external})

-   Merge pull request #793 from roysc/patch-1 ([commit
    91a1106](https://github.com/scrapy/scrapy/commit/91a1106){.reference
    .external})

-   Fix typo in commands.rst ([commit
    743e1e2](https://github.com/scrapy/scrapy/commit/743e1e2){.reference
    .external})

-   better testcase for settings.overrides.setdefault ([commit
    e22daaf](https://github.com/scrapy/scrapy/commit/e22daaf){.reference
    .external})

-   Using CRLF as line marker according to http 1.1 definition ([commit
    5ec430b](https://github.com/scrapy/scrapy/commit/5ec430b){.reference
    .external})
:::

::: {#scrapy-0-24-2-2014-07-08 .section}
#### Scrapy 0.24.2 (2014-07-08)[¶](#scrapy-0-24-2-2014-07-08 "Permalink to this heading"){.headerlink}

-   Use a mutable mapping to proxy deprecated settings.overrides and
    settings.defaults attribute ([commit
    e5e8133](https://github.com/scrapy/scrapy/commit/e5e8133){.reference
    .external})

-   there is not support for python3 yet ([commit
    3cd6146](https://github.com/scrapy/scrapy/commit/3cd6146){.reference
    .external})

-   Update python compatible version set to Debian packages ([commit
    fa5d76b](https://github.com/scrapy/scrapy/commit/fa5d76b){.reference
    .external})

-   DOC fix formatting in release notes ([commit
    c6a9e20](https://github.com/scrapy/scrapy/commit/c6a9e20){.reference
    .external})
:::

::: {#scrapy-0-24-1-2014-06-27 .section}
#### Scrapy 0.24.1 (2014-06-27)[¶](#scrapy-0-24-1-2014-06-27 "Permalink to this heading"){.headerlink}

-   Fix deprecated CrawlerSettings and increase backward compatibility
    with .defaults attribute ([commit
    8e3f20a](https://github.com/scrapy/scrapy/commit/8e3f20a){.reference
    .external})
:::

::: {#scrapy-0-24-0-2014-06-26 .section}
#### Scrapy 0.24.0 (2014-06-26)[¶](#scrapy-0-24-0-2014-06-26 "Permalink to this heading"){.headerlink}

::: {#enhancements .section}
##### Enhancements[¶](#enhancements "Permalink to this heading"){.headerlink}

-   Improve Scrapy top-level namespace ([issue
    494](https://github.com/scrapy/scrapy/issues/494){.reference
    .external}, [issue
    684](https://github.com/scrapy/scrapy/issues/684){.reference
    .external})

-   Add selector shortcuts to responses ([issue
    554](https://github.com/scrapy/scrapy/issues/554){.reference
    .external}, [issue
    690](https://github.com/scrapy/scrapy/issues/690){.reference
    .external})

-   Add new lxml based LinkExtractor to replace unmaintained
    SgmlLinkExtractor ([issue
    559](https://github.com/scrapy/scrapy/issues/559){.reference
    .external}, [issue
    761](https://github.com/scrapy/scrapy/issues/761){.reference
    .external}, [issue
    763](https://github.com/scrapy/scrapy/issues/763){.reference
    .external})

-   Cleanup settings API - part of per-spider settings **GSoC project**
    ([issue 737](https://github.com/scrapy/scrapy/issues/737){.reference
    .external})

-   Add UTF8 encoding header to templates ([issue
    688](https://github.com/scrapy/scrapy/issues/688){.reference
    .external}, [issue
    762](https://github.com/scrapy/scrapy/issues/762){.reference
    .external})

-   Telnet console now binds to 127.0.0.1 by default ([issue
    699](https://github.com/scrapy/scrapy/issues/699){.reference
    .external})

-   Update Debian/Ubuntu install instructions ([issue
    509](https://github.com/scrapy/scrapy/issues/509){.reference
    .external}, [issue
    549](https://github.com/scrapy/scrapy/issues/549){.reference
    .external})

-   Disable smart strings in lxml XPath evaluations ([issue
    535](https://github.com/scrapy/scrapy/issues/535){.reference
    .external})

-   Restore filesystem based cache as default for http cache middleware
    ([issue 541](https://github.com/scrapy/scrapy/issues/541){.reference
    .external}, [issue
    500](https://github.com/scrapy/scrapy/issues/500){.reference
    .external}, [issue
    571](https://github.com/scrapy/scrapy/issues/571){.reference
    .external})

-   Expose current crawler in Scrapy shell ([issue
    557](https://github.com/scrapy/scrapy/issues/557){.reference
    .external})

-   Improve testsuite comparing CSV and XML exporters ([issue
    570](https://github.com/scrapy/scrapy/issues/570){.reference
    .external})

-   New [`offsite/filtered`{.docutils .literal .notranslate}]{.pre} and
    [`offsite/domains`{.docutils .literal .notranslate}]{.pre} stats
    ([issue 566](https://github.com/scrapy/scrapy/issues/566){.reference
    .external})

-   Support process_links as generator in CrawlSpider ([issue
    555](https://github.com/scrapy/scrapy/issues/555){.reference
    .external})

-   Verbose logging and new stats counters for DupeFilter ([issue
    553](https://github.com/scrapy/scrapy/issues/553){.reference
    .external})

-   Add a mimetype parameter to [`MailSender.send()`{.docutils .literal
    .notranslate}]{.pre} ([issue
    602](https://github.com/scrapy/scrapy/issues/602){.reference
    .external})

-   Generalize file pipeline log messages ([issue
    622](https://github.com/scrapy/scrapy/issues/622){.reference
    .external})

-   Replace unencodeable codepoints with html entities in
    SGMLLinkExtractor ([issue
    565](https://github.com/scrapy/scrapy/issues/565){.reference
    .external})

-   Converted SEP documents to rst format ([issue
    629](https://github.com/scrapy/scrapy/issues/629){.reference
    .external}, [issue
    630](https://github.com/scrapy/scrapy/issues/630){.reference
    .external}, [issue
    638](https://github.com/scrapy/scrapy/issues/638){.reference
    .external}, [issue
    632](https://github.com/scrapy/scrapy/issues/632){.reference
    .external}, [issue
    636](https://github.com/scrapy/scrapy/issues/636){.reference
    .external}, [issue
    640](https://github.com/scrapy/scrapy/issues/640){.reference
    .external}, [issue
    635](https://github.com/scrapy/scrapy/issues/635){.reference
    .external}, [issue
    634](https://github.com/scrapy/scrapy/issues/634){.reference
    .external}, [issue
    639](https://github.com/scrapy/scrapy/issues/639){.reference
    .external}, [issue
    637](https://github.com/scrapy/scrapy/issues/637){.reference
    .external}, [issue
    631](https://github.com/scrapy/scrapy/issues/631){.reference
    .external}, [issue
    633](https://github.com/scrapy/scrapy/issues/633){.reference
    .external}, [issue
    641](https://github.com/scrapy/scrapy/issues/641){.reference
    .external}, [issue
    642](https://github.com/scrapy/scrapy/issues/642){.reference
    .external})

-   Tests and docs for clickdata's nr index in FormRequest ([issue
    646](https://github.com/scrapy/scrapy/issues/646){.reference
    .external}, [issue
    645](https://github.com/scrapy/scrapy/issues/645){.reference
    .external})

-   Allow to disable a downloader handler just like any other component
    ([issue 650](https://github.com/scrapy/scrapy/issues/650){.reference
    .external})

-   Log when a request is discarded after too many redirections ([issue
    654](https://github.com/scrapy/scrapy/issues/654){.reference
    .external})

-   Log error responses if they are not handled by spider callbacks
    ([issue 612](https://github.com/scrapy/scrapy/issues/612){.reference
    .external}, [issue
    656](https://github.com/scrapy/scrapy/issues/656){.reference
    .external})

-   Add content-type check to http compression mw ([issue
    193](https://github.com/scrapy/scrapy/issues/193){.reference
    .external}, [issue
    660](https://github.com/scrapy/scrapy/issues/660){.reference
    .external})

-   Run pypy tests using latest pypi from ppa ([issue
    674](https://github.com/scrapy/scrapy/issues/674){.reference
    .external})

-   Run test suite using pytest instead of trial ([issue
    679](https://github.com/scrapy/scrapy/issues/679){.reference
    .external})

-   Build docs and check for dead links in tox environment ([issue
    687](https://github.com/scrapy/scrapy/issues/687){.reference
    .external})

-   Make scrapy.version_info a tuple of integers ([issue
    681](https://github.com/scrapy/scrapy/issues/681){.reference
    .external}, [issue
    692](https://github.com/scrapy/scrapy/issues/692){.reference
    .external})

-   Infer exporter's output format from filename extensions ([issue
    546](https://github.com/scrapy/scrapy/issues/546){.reference
    .external}, [issue
    659](https://github.com/scrapy/scrapy/issues/659){.reference
    .external}, [issue
    760](https://github.com/scrapy/scrapy/issues/760){.reference
    .external})

-   Support case-insensitive domains in
    [`url_is_from_any_domain()`{.docutils .literal .notranslate}]{.pre}
    ([issue 693](https://github.com/scrapy/scrapy/issues/693){.reference
    .external})

-   Remove pep8 warnings in project and spider templates ([issue
    698](https://github.com/scrapy/scrapy/issues/698){.reference
    .external})

-   Tests and docs for [`request_fingerprint`{.docutils .literal
    .notranslate}]{.pre} function ([issue
    597](https://github.com/scrapy/scrapy/issues/597){.reference
    .external})

-   Update SEP-19 for GSoC project [`per-spider`{.docutils .literal
    .notranslate}]{.pre}` `{.docutils .literal
    .notranslate}[`settings`{.docutils .literal .notranslate}]{.pre}
    ([issue 705](https://github.com/scrapy/scrapy/issues/705){.reference
    .external})

-   Set exit code to non-zero when contracts fails ([issue
    727](https://github.com/scrapy/scrapy/issues/727){.reference
    .external})

-   Add a setting to control what class is instantiated as Downloader
    component ([issue
    738](https://github.com/scrapy/scrapy/issues/738){.reference
    .external})

-   Pass response in [`item_dropped`{.docutils .literal
    .notranslate}]{.pre} signal ([issue
    724](https://github.com/scrapy/scrapy/issues/724){.reference
    .external})

-   Improve [`scrapy`{.docutils .literal
    .notranslate}]{.pre}` `{.docutils .literal
    .notranslate}[`check`{.docutils .literal .notranslate}]{.pre}
    contracts command ([issue
    733](https://github.com/scrapy/scrapy/issues/733){.reference
    .external}, [issue
    752](https://github.com/scrapy/scrapy/issues/752){.reference
    .external})

-   Document [`spider.closed()`{.docutils .literal .notranslate}]{.pre}
    shortcut ([issue
    719](https://github.com/scrapy/scrapy/issues/719){.reference
    .external})

-   Document [`request_scheduled`{.docutils .literal
    .notranslate}]{.pre} signal ([issue
    746](https://github.com/scrapy/scrapy/issues/746){.reference
    .external})

-   Add a note about reporting security issues ([issue
    697](https://github.com/scrapy/scrapy/issues/697){.reference
    .external})

-   Add LevelDB http cache storage backend ([issue
    626](https://github.com/scrapy/scrapy/issues/626){.reference
    .external}, [issue
    500](https://github.com/scrapy/scrapy/issues/500){.reference
    .external})

-   Sort spider list output of [`scrapy`{.docutils .literal
    .notranslate}]{.pre}` `{.docutils .literal
    .notranslate}[`list`{.docutils .literal .notranslate}]{.pre} command
    ([issue 742](https://github.com/scrapy/scrapy/issues/742){.reference
    .external})

-   Multiple documentation enhancements and fixes ([issue
    575](https://github.com/scrapy/scrapy/issues/575){.reference
    .external}, [issue
    587](https://github.com/scrapy/scrapy/issues/587){.reference
    .external}, [issue
    590](https://github.com/scrapy/scrapy/issues/590){.reference
    .external}, [issue
    596](https://github.com/scrapy/scrapy/issues/596){.reference
    .external}, [issue
    610](https://github.com/scrapy/scrapy/issues/610){.reference
    .external}, [issue
    617](https://github.com/scrapy/scrapy/issues/617){.reference
    .external}, [issue
    618](https://github.com/scrapy/scrapy/issues/618){.reference
    .external}, [issue
    627](https://github.com/scrapy/scrapy/issues/627){.reference
    .external}, [issue
    613](https://github.com/scrapy/scrapy/issues/613){.reference
    .external}, [issue
    643](https://github.com/scrapy/scrapy/issues/643){.reference
    .external}, [issue
    654](https://github.com/scrapy/scrapy/issues/654){.reference
    .external}, [issue
    675](https://github.com/scrapy/scrapy/issues/675){.reference
    .external}, [issue
    663](https://github.com/scrapy/scrapy/issues/663){.reference
    .external}, [issue
    711](https://github.com/scrapy/scrapy/issues/711){.reference
    .external}, [issue
    714](https://github.com/scrapy/scrapy/issues/714){.reference
    .external})
:::

::: {#id129 .section}
##### Bugfixes[¶](#id129 "Permalink to this heading"){.headerlink}

-   Encode unicode URL value when creating Links in RegexLinkExtractor
    ([issue 561](https://github.com/scrapy/scrapy/issues/561){.reference
    .external})

-   Ignore None values in ItemLoader processors ([issue
    556](https://github.com/scrapy/scrapy/issues/556){.reference
    .external})

-   Fix link text when there is an inner tag in SGMLLinkExtractor and
    HtmlParserLinkExtractor ([issue
    485](https://github.com/scrapy/scrapy/issues/485){.reference
    .external}, [issue
    574](https://github.com/scrapy/scrapy/issues/574){.reference
    .external})

-   Fix wrong checks on subclassing of deprecated classes ([issue
    581](https://github.com/scrapy/scrapy/issues/581){.reference
    .external}, [issue
    584](https://github.com/scrapy/scrapy/issues/584){.reference
    .external})

-   Handle errors caused by inspect.stack() failures ([issue
    582](https://github.com/scrapy/scrapy/issues/582){.reference
    .external})

-   Fix a reference to unexistent engine attribute ([issue
    593](https://github.com/scrapy/scrapy/issues/593){.reference
    .external}, [issue
    594](https://github.com/scrapy/scrapy/issues/594){.reference
    .external})

-   Fix dynamic itemclass example usage of type() ([issue
    603](https://github.com/scrapy/scrapy/issues/603){.reference
    .external})

-   Use lucasdemarchi/codespell to fix typos ([issue
    628](https://github.com/scrapy/scrapy/issues/628){.reference
    .external})

-   Fix default value of attrs argument in SgmlLinkExtractor to be tuple
    ([issue 661](https://github.com/scrapy/scrapy/issues/661){.reference
    .external})

-   Fix XXE flaw in sitemap reader ([issue
    676](https://github.com/scrapy/scrapy/issues/676){.reference
    .external})

-   Fix engine to support filtered start requests ([issue
    707](https://github.com/scrapy/scrapy/issues/707){.reference
    .external})

-   Fix offsite middleware case on urls with no hostnames ([issue
    745](https://github.com/scrapy/scrapy/issues/745){.reference
    .external})

-   Testsuite doesn't require PIL anymore ([issue
    585](https://github.com/scrapy/scrapy/issues/585){.reference
    .external})
:::
:::

::: {#scrapy-0-22-2-released-2014-02-14 .section}
#### Scrapy 0.22.2 (released 2014-02-14)[¶](#scrapy-0-22-2-released-2014-02-14 "Permalink to this heading"){.headerlink}

-   fix a reference to unexistent engine.slots. closes #593 ([commit
    13c099a](https://github.com/scrapy/scrapy/commit/13c099a){.reference
    .external})

-   downloaderMW doc typo (spiderMW doc copy remnant) ([commit
    8ae11bf](https://github.com/scrapy/scrapy/commit/8ae11bf){.reference
    .external})

-   Correct typos ([commit
    1346037](https://github.com/scrapy/scrapy/commit/1346037){.reference
    .external})
:::

::: {#scrapy-0-22-1-released-2014-02-08 .section}
#### Scrapy 0.22.1 (released 2014-02-08)[¶](#scrapy-0-22-1-released-2014-02-08 "Permalink to this heading"){.headerlink}

-   localhost666 can resolve under certain circumstances ([commit
    2ec2279](https://github.com/scrapy/scrapy/commit/2ec2279){.reference
    .external})

-   test inspect.stack failure ([commit
    cc3eda3](https://github.com/scrapy/scrapy/commit/cc3eda3){.reference
    .external})

-   Handle cases when inspect.stack() fails ([commit
    8cb44f9](https://github.com/scrapy/scrapy/commit/8cb44f9){.reference
    .external})

-   Fix wrong checks on subclassing of deprecated classes. closes #581
    ([commit
    46d98d6](https://github.com/scrapy/scrapy/commit/46d98d6){.reference
    .external})

-   Docs: 4-space indent for final spider example ([commit
    13846de](https://github.com/scrapy/scrapy/commit/13846de){.reference
    .external})

-   Fix HtmlParserLinkExtractor and tests after #485 merge ([commit
    368a946](https://github.com/scrapy/scrapy/commit/368a946){.reference
    .external})

-   BaseSgmlLinkExtractor: Fixed the missing space when the link has an
    inner tag ([commit
    b566388](https://github.com/scrapy/scrapy/commit/b566388){.reference
    .external})

-   BaseSgmlLinkExtractor: Added unit test of a link with an inner tag
    ([commit
    c1cb418](https://github.com/scrapy/scrapy/commit/c1cb418){.reference
    .external})

-   BaseSgmlLinkExtractor: Fixed unknown_endtag() so that it only set
    current_link=None when the end tag match the opening tag ([commit
    7e4d627](https://github.com/scrapy/scrapy/commit/7e4d627){.reference
    .external})

-   Fix tests for Travis-CI build ([commit
    76c7e20](https://github.com/scrapy/scrapy/commit/76c7e20){.reference
    .external})

-   replace unencodeable codepoints with html entities. fixes #562 and
    #285 ([commit
    5f87b17](https://github.com/scrapy/scrapy/commit/5f87b17){.reference
    .external})

-   RegexLinkExtractor: encode URL unicode value when creating Links
    ([commit
    d0ee545](https://github.com/scrapy/scrapy/commit/d0ee545){.reference
    .external})

-   Updated the tutorial crawl output with latest output. ([commit
    8da65de](https://github.com/scrapy/scrapy/commit/8da65de){.reference
    .external})

-   Updated shell docs with the crawler reference and fixed the actual
    shell output. ([commit
    875b9ab](https://github.com/scrapy/scrapy/commit/875b9ab){.reference
    .external})

-   PEP8 minor edits. ([commit
    f89efaf](https://github.com/scrapy/scrapy/commit/f89efaf){.reference
    .external})

-   Expose current crawler in the Scrapy shell. ([commit
    5349cec](https://github.com/scrapy/scrapy/commit/5349cec){.reference
    .external})

-   Unused re import and PEP8 minor edits. ([commit
    387f414](https://github.com/scrapy/scrapy/commit/387f414){.reference
    .external})

-   Ignore None's values when using the ItemLoader. ([commit
    0632546](https://github.com/scrapy/scrapy/commit/0632546){.reference
    .external})

-   DOC Fixed HTTPCACHE_STORAGE typo in the default value which is now
    Filesystem instead Dbm. ([commit
    cde9a8c](https://github.com/scrapy/scrapy/commit/cde9a8c){.reference
    .external})

-   show Ubuntu setup instructions as literal code ([commit
    fb5c9c5](https://github.com/scrapy/scrapy/commit/fb5c9c5){.reference
    .external})

-   Update Ubuntu installation instructions ([commit
    70fb105](https://github.com/scrapy/scrapy/commit/70fb105){.reference
    .external})

-   Merge pull request #550 from stray-leone/patch-1 ([commit
    6f70b6a](https://github.com/scrapy/scrapy/commit/6f70b6a){.reference
    .external})

-   modify the version of Scrapy Ubuntu package ([commit
    725900d](https://github.com/scrapy/scrapy/commit/725900d){.reference
    .external})

-   fix 0.22.0 release date ([commit
    af0219a](https://github.com/scrapy/scrapy/commit/af0219a){.reference
    .external})

-   fix typos in news.rst and remove (not released yet) header ([commit
    b7f58f4](https://github.com/scrapy/scrapy/commit/b7f58f4){.reference
    .external})
:::

::: {#scrapy-0-22-0-released-2014-01-17 .section}
#### Scrapy 0.22.0 (released 2014-01-17)[¶](#scrapy-0-22-0-released-2014-01-17 "Permalink to this heading"){.headerlink}

::: {#id130 .section}
##### Enhancements[¶](#id130 "Permalink to this heading"){.headerlink}

-   \[**Backward incompatible**\] Switched HTTPCacheMiddleware backend
    to filesystem ([issue
    541](https://github.com/scrapy/scrapy/issues/541){.reference
    .external}) To restore old backend set
    [`HTTPCACHE_STORAGE`{.docutils .literal .notranslate}]{.pre} to
    [`scrapy.contrib.httpcache.DbmCacheStorage`{.docutils .literal
    .notranslate}]{.pre}

-   Proxy https:// urls using CONNECT method ([issue
    392](https://github.com/scrapy/scrapy/issues/392){.reference
    .external}, [issue
    397](https://github.com/scrapy/scrapy/issues/397){.reference
    .external})

-   Add a middleware to crawl ajax crawlable pages as defined by google
    ([issue 343](https://github.com/scrapy/scrapy/issues/343){.reference
    .external})

-   Rename scrapy.spider.BaseSpider to scrapy.spider.Spider ([issue
    510](https://github.com/scrapy/scrapy/issues/510){.reference
    .external}, [issue
    519](https://github.com/scrapy/scrapy/issues/519){.reference
    .external})

-   Selectors register EXSLT namespaces by default ([issue
    472](https://github.com/scrapy/scrapy/issues/472){.reference
    .external})

-   Unify item loaders similar to selectors renaming ([issue
    461](https://github.com/scrapy/scrapy/issues/461){.reference
    .external})

-   Make [`RFPDupeFilter`{.docutils .literal .notranslate}]{.pre} class
    easily subclassable ([issue
    533](https://github.com/scrapy/scrapy/issues/533){.reference
    .external})

-   Improve test coverage and forthcoming Python 3 support ([issue
    525](https://github.com/scrapy/scrapy/issues/525){.reference
    .external})

-   Promote startup info on settings and middleware to INFO level
    ([issue 520](https://github.com/scrapy/scrapy/issues/520){.reference
    .external})

-   Support partials in [`get_func_args`{.docutils .literal
    .notranslate}]{.pre} util ([issue
    506](https://github.com/scrapy/scrapy/issues/506){.reference
    .external}, issue:504)

-   Allow running individual tests via tox ([issue
    503](https://github.com/scrapy/scrapy/issues/503){.reference
    .external})

-   Update extensions ignored by link extractors ([issue
    498](https://github.com/scrapy/scrapy/issues/498){.reference
    .external})

-   Add middleware methods to get files/images/thumbs paths ([issue
    490](https://github.com/scrapy/scrapy/issues/490){.reference
    .external})

-   Improve offsite middleware tests ([issue
    478](https://github.com/scrapy/scrapy/issues/478){.reference
    .external})

-   Add a way to skip default Referer header set by RefererMiddleware
    ([issue 475](https://github.com/scrapy/scrapy/issues/475){.reference
    .external})

-   Do not send [`x-gzip`{.docutils .literal .notranslate}]{.pre} in
    default [`Accept-Encoding`{.docutils .literal .notranslate}]{.pre}
    header ([issue
    469](https://github.com/scrapy/scrapy/issues/469){.reference
    .external})

-   Support defining http error handling using settings ([issue
    466](https://github.com/scrapy/scrapy/issues/466){.reference
    .external})

-   Use modern python idioms wherever you find legacies ([issue
    497](https://github.com/scrapy/scrapy/issues/497){.reference
    .external})

-   Improve and correct documentation ([issue
    527](https://github.com/scrapy/scrapy/issues/527){.reference
    .external}, [issue
    524](https://github.com/scrapy/scrapy/issues/524){.reference
    .external}, [issue
    521](https://github.com/scrapy/scrapy/issues/521){.reference
    .external}, [issue
    517](https://github.com/scrapy/scrapy/issues/517){.reference
    .external}, [issue
    512](https://github.com/scrapy/scrapy/issues/512){.reference
    .external}, [issue
    505](https://github.com/scrapy/scrapy/issues/505){.reference
    .external}, [issue
    502](https://github.com/scrapy/scrapy/issues/502){.reference
    .external}, [issue
    489](https://github.com/scrapy/scrapy/issues/489){.reference
    .external}, [issue
    465](https://github.com/scrapy/scrapy/issues/465){.reference
    .external}, [issue
    460](https://github.com/scrapy/scrapy/issues/460){.reference
    .external}, [issue
    425](https://github.com/scrapy/scrapy/issues/425){.reference
    .external}, [issue
    536](https://github.com/scrapy/scrapy/issues/536){.reference
    .external})
:::

::: {#fixes .section}
##### Fixes[¶](#fixes "Permalink to this heading"){.headerlink}

-   Update Selector class imports in CrawlSpider template ([issue
    484](https://github.com/scrapy/scrapy/issues/484){.reference
    .external})

-   Fix unexistent reference to [`engine.slots`{.docutils .literal
    .notranslate}]{.pre} ([issue
    464](https://github.com/scrapy/scrapy/issues/464){.reference
    .external})

-   Do not try to call [`body_as_unicode()`{.docutils .literal
    .notranslate}]{.pre} on a non-TextResponse instance ([issue
    462](https://github.com/scrapy/scrapy/issues/462){.reference
    .external})

-   Warn when subclassing XPathItemLoader, previously it only warned on
    instantiation. ([issue
    523](https://github.com/scrapy/scrapy/issues/523){.reference
    .external})

-   Warn when subclassing XPathSelector, previously it only warned on
    instantiation. ([issue
    537](https://github.com/scrapy/scrapy/issues/537){.reference
    .external})

-   Multiple fixes to memory stats ([issue
    531](https://github.com/scrapy/scrapy/issues/531){.reference
    .external}, [issue
    530](https://github.com/scrapy/scrapy/issues/530){.reference
    .external}, [issue
    529](https://github.com/scrapy/scrapy/issues/529){.reference
    .external})

-   Fix overriding url in [`FormRequest.from_response()`{.docutils
    .literal .notranslate}]{.pre} ([issue
    507](https://github.com/scrapy/scrapy/issues/507){.reference
    .external})

-   Fix tests runner under pip 1.5 ([issue
    513](https://github.com/scrapy/scrapy/issues/513){.reference
    .external})

-   Fix logging error when spider name is unicode ([issue
    479](https://github.com/scrapy/scrapy/issues/479){.reference
    .external})
:::
:::

::: {#scrapy-0-20-2-released-2013-12-09 .section}
#### Scrapy 0.20.2 (released 2013-12-09)[¶](#scrapy-0-20-2-released-2013-12-09 "Permalink to this heading"){.headerlink}

-   Update CrawlSpider Template with Selector changes ([commit
    6d1457d](https://github.com/scrapy/scrapy/commit/6d1457d){.reference
    .external})

-   fix method name in tutorial. closes GH-480 ([commit
    b4fc359](https://github.com/scrapy/scrapy/commit/b4fc359){.reference
    .external}
:::

::: {#scrapy-0-20-1-released-2013-11-28 .section}
#### Scrapy 0.20.1 (released 2013-11-28)[¶](#scrapy-0-20-1-released-2013-11-28 "Permalink to this heading"){.headerlink}

-   include_package_data is required to build wheels from published
    sources ([commit
    5ba1ad5](https://github.com/scrapy/scrapy/commit/5ba1ad5){.reference
    .external})

-   process_parallel was leaking the failures on its internal deferreds.
    closes #458 ([commit
    419a780](https://github.com/scrapy/scrapy/commit/419a780){.reference
    .external})
:::

::: {#scrapy-0-20-0-released-2013-11-08 .section}
#### Scrapy 0.20.0 (released 2013-11-08)[¶](#scrapy-0-20-0-released-2013-11-08 "Permalink to this heading"){.headerlink}

::: {#id131 .section}
##### Enhancements[¶](#id131 "Permalink to this heading"){.headerlink}

-   New Selector's API including CSS selectors ([issue
    395](https://github.com/scrapy/scrapy/issues/395){.reference
    .external} and [issue
    426](https://github.com/scrapy/scrapy/issues/426){.reference
    .external}),

-   Request/Response url/body attributes are now immutable (modifying
    them had been deprecated for a long time)

-   [[`ITEM_PIPELINES`{.xref .std .std-setting .docutils .literal
    .notranslate}]{.pre}](index.html#std-setting-ITEM_PIPELINES){.hoverxref
    .tooltip .reference .internal} is now defined as a dict (instead of
    a list)

-   Sitemap spider can fetch alternate URLs ([issue
    360](https://github.com/scrapy/scrapy/issues/360){.reference
    .external})

-   [`Selector.remove_namespaces()`{.docutils .literal
    .notranslate}]{.pre} now remove namespaces from element's
    attributes. ([issue
    416](https://github.com/scrapy/scrapy/issues/416){.reference
    .external})

-   Paved the road for Python 3.3+ ([issue
    435](https://github.com/scrapy/scrapy/issues/435){.reference
    .external}, [issue
    436](https://github.com/scrapy/scrapy/issues/436){.reference
    .external}, [issue
    431](https://github.com/scrapy/scrapy/issues/431){.reference
    .external}, [issue
    452](https://github.com/scrapy/scrapy/issues/452){.reference
    .external})

-   New item exporter using native python types with nesting support
    ([issue 366](https://github.com/scrapy/scrapy/issues/366){.reference
    .external})

-   Tune HTTP1.1 pool size so it matches concurrency defined by settings
    ([commit
    b43b5f575](https://github.com/scrapy/scrapy/commit/b43b5f575){.reference
    .external})

-   scrapy.mail.MailSender now can connect over TLS or upgrade using
    STARTTLS ([issue
    327](https://github.com/scrapy/scrapy/issues/327){.reference
    .external})

-   New FilesPipeline with functionality factored out from
    ImagesPipeline ([issue
    370](https://github.com/scrapy/scrapy/issues/370){.reference
    .external}, [issue
    409](https://github.com/scrapy/scrapy/issues/409){.reference
    .external})

-   Recommend Pillow instead of PIL for image handling ([issue
    317](https://github.com/scrapy/scrapy/issues/317){.reference
    .external})

-   Added Debian packages for Ubuntu Quantal and Raring ([commit
    86230c0](https://github.com/scrapy/scrapy/commit/86230c0){.reference
    .external})

-   Mock server (used for tests) can listen for HTTPS requests ([issue
    410](https://github.com/scrapy/scrapy/issues/410){.reference
    .external})

-   Remove multi spider support from multiple core components ([issue
    422](https://github.com/scrapy/scrapy/issues/422){.reference
    .external}, [issue
    421](https://github.com/scrapy/scrapy/issues/421){.reference
    .external}, [issue
    420](https://github.com/scrapy/scrapy/issues/420){.reference
    .external}, [issue
    419](https://github.com/scrapy/scrapy/issues/419){.reference
    .external}, [issue
    423](https://github.com/scrapy/scrapy/issues/423){.reference
    .external}, [issue
    418](https://github.com/scrapy/scrapy/issues/418){.reference
    .external})

-   Travis-CI now tests Scrapy changes against development versions of
    [`w3lib`{.docutils .literal .notranslate}]{.pre} and
    [`queuelib`{.docutils .literal .notranslate}]{.pre} python packages.

-   Add pypy 2.1 to continuous integration tests ([commit
    ecfa7431](https://github.com/scrapy/scrapy/commit/ecfa7431){.reference
    .external})

-   Pylinted, pep8 and removed old-style exceptions from source ([issue
    430](https://github.com/scrapy/scrapy/issues/430){.reference
    .external}, [issue
    432](https://github.com/scrapy/scrapy/issues/432){.reference
    .external})

-   Use importlib for parametric imports ([issue
    445](https://github.com/scrapy/scrapy/issues/445){.reference
    .external})

-   Handle a regression introduced in Python 2.7.5 that affects
    XmlItemExporter ([issue
    372](https://github.com/scrapy/scrapy/issues/372){.reference
    .external})

-   Bugfix crawling shutdown on SIGINT ([issue
    450](https://github.com/scrapy/scrapy/issues/450){.reference
    .external})

-   Do not submit [`reset`{.docutils .literal .notranslate}]{.pre} type
    inputs in FormRequest.from_response ([commit
    b326b87](https://github.com/scrapy/scrapy/commit/b326b87){.reference
    .external})

-   Do not silence download errors when request errback raises an
    exception ([commit
    684cfc0](https://github.com/scrapy/scrapy/commit/684cfc0){.reference
    .external})
:::

::: {#id132 .section}
##### Bugfixes[¶](#id132 "Permalink to this heading"){.headerlink}

-   Fix tests under Django 1.6 ([commit
    b6bed44c](https://github.com/scrapy/scrapy/commit/b6bed44c){.reference
    .external})

-   Lot of bugfixes to retry middleware under disconnections using HTTP
    1.1 download handler

-   Fix inconsistencies among Twisted releases ([issue
    406](https://github.com/scrapy/scrapy/issues/406){.reference
    .external})

-   Fix Scrapy shell bugs ([issue
    418](https://github.com/scrapy/scrapy/issues/418){.reference
    .external}, [issue
    407](https://github.com/scrapy/scrapy/issues/407){.reference
    .external})

-   Fix invalid variable name in setup.py ([issue
    429](https://github.com/scrapy/scrapy/issues/429){.reference
    .external})

-   Fix tutorial references ([issue
    387](https://github.com/scrapy/scrapy/issues/387){.reference
    .external})

-   Improve request-response docs ([issue
    391](https://github.com/scrapy/scrapy/issues/391){.reference
    .external})

-   Improve best practices docs ([issue
    399](https://github.com/scrapy/scrapy/issues/399){.reference
    .external}, [issue
    400](https://github.com/scrapy/scrapy/issues/400){.reference
    .external}, [issue
    401](https://github.com/scrapy/scrapy/issues/401){.reference
    .external}, [issue
    402](https://github.com/scrapy/scrapy/issues/402){.reference
    .external})

-   Improve django integration docs ([issue
    404](https://github.com/scrapy/scrapy/issues/404){.reference
    .external})

-   Document [`bindaddress`{.docutils .literal .notranslate}]{.pre}
    request meta ([commit
    37c24e01d7](https://github.com/scrapy/scrapy/commit/37c24e01d7){.reference
    .external})

-   Improve [`Request`{.docutils .literal .notranslate}]{.pre} class
    documentation ([issue
    226](https://github.com/scrapy/scrapy/issues/226){.reference
    .external})
:::

::: {#other .section}
##### Other[¶](#other "Permalink to this heading"){.headerlink}

-   Dropped Python 2.6 support ([issue
    448](https://github.com/scrapy/scrapy/issues/448){.reference
    .external})

-   Add [[cssselect]{.xref .std
    .std-doc}](https://cssselect.readthedocs.io/en/latest/index.html "(in cssselect v1.2.0)"){.reference
    .external} python package as install dependency

-   Drop libxml2 and multi selector's backend support,
    [lxml](https://lxml.de/){.reference .external} is required from now
    on.

-   Minimum Twisted version increased to 10.0.0, dropped Twisted 8.0
    support.

-   Running test suite now requires [`mock`{.docutils .literal
    .notranslate}]{.pre} python library ([issue
    390](https://github.com/scrapy/scrapy/issues/390){.reference
    .external})
:::

::: {#thanks .section}
##### Thanks[¶](#thanks "Permalink to this heading"){.headerlink}

Thanks to everyone who contribute to this release!

List of contributors sorted by number of commits:

::: {.highlight-default .notranslate}
::: highlight
    69 Daniel Graña <dangra@...>
    37 Pablo Hoffman <pablo@...>
    13 Mikhail Korobov <kmike84@...>
     9 Alex Cepoi <alex.cepoi@...>
     9 alexanderlukanin13 <alexander.lukanin.13@...>
     8 Rolando Espinoza La fuente <darkrho@...>
     8 Lukasz Biedrycki <lukasz.biedrycki@...>
     6 Nicolas Ramirez <nramirez.uy@...>
     3 Paul Tremberth <paul.tremberth@...>
     2 Martin Olveyra <molveyra@...>
     2 Stefan <misc@...>
     2 Rolando Espinoza <darkrho@...>
     2 Loren Davie <loren@...>
     2 irgmedeiros <irgmedeiros@...>
     1 Stefan Koch <taikano@...>
     1 Stefan <cct@...>
     1 scraperdragon <dragon@...>
     1 Kumara Tharmalingam <ktharmal@...>
     1 Francesco Piccinno <stack.box@...>
     1 Marcos Campal <duendex@...>
     1 Dragon Dave <dragon@...>
     1 Capi Etheriel <barraponto@...>
     1 cacovsky <amarquesferraz@...>
     1 Berend Iwema <berend@...>
:::
:::
:::
:::

::: {#scrapy-0-18-4-released-2013-10-10 .section}
#### Scrapy 0.18.4 (released 2013-10-10)[¶](#scrapy-0-18-4-released-2013-10-10 "Permalink to this heading"){.headerlink}

-   IPython refuses to update the namespace. fix #396 ([commit
    3d32c4f](https://github.com/scrapy/scrapy/commit/3d32c4f){.reference
    .external})

-   Fix AlreadyCalledError replacing a request in shell command. closes
    #407 ([commit
    b1d8919](https://github.com/scrapy/scrapy/commit/b1d8919){.reference
    .external})

-   Fix start_requests laziness and early hangs ([commit
    89faf52](https://github.com/scrapy/scrapy/commit/89faf52){.reference
    .external})
:::

::: {#scrapy-0-18-3-released-2013-10-03 .section}
#### Scrapy 0.18.3 (released 2013-10-03)[¶](#scrapy-0-18-3-released-2013-10-03 "Permalink to this heading"){.headerlink}

-   fix regression on lazy evaluation of start requests ([commit
    12693a5](https://github.com/scrapy/scrapy/commit/12693a5){.reference
    .external})

-   forms: do not submit reset inputs ([commit
    e429f63](https://github.com/scrapy/scrapy/commit/e429f63){.reference
    .external})

-   increase unittest timeouts to decrease travis false positive
    failures ([commit
    912202e](https://github.com/scrapy/scrapy/commit/912202e){.reference
    .external})

-   backport master fixes to json exporter ([commit
    cfc2d46](https://github.com/scrapy/scrapy/commit/cfc2d46){.reference
    .external})

-   Fix permission and set umask before generating sdist tarball
    ([commit
    06149e0](https://github.com/scrapy/scrapy/commit/06149e0){.reference
    .external})
:::

::: {#scrapy-0-18-2-released-2013-09-03 .section}
#### Scrapy 0.18.2 (released 2013-09-03)[¶](#scrapy-0-18-2-released-2013-09-03 "Permalink to this heading"){.headerlink}

-   Backport [`scrapy`{.docutils .literal
    .notranslate}]{.pre}` `{.docutils .literal
    .notranslate}[`check`{.docutils .literal .notranslate}]{.pre}
    command fixes and backward compatible multi crawler process([issue
    339](https://github.com/scrapy/scrapy/issues/339){.reference
    .external})
:::

::: {#scrapy-0-18-1-released-2013-08-27 .section}
#### Scrapy 0.18.1 (released 2013-08-27)[¶](#scrapy-0-18-1-released-2013-08-27 "Permalink to this heading"){.headerlink}

-   remove extra import added by cherry picked changes ([commit
    d20304e](https://github.com/scrapy/scrapy/commit/d20304e){.reference
    .external})

-   fix crawling tests under twisted pre 11.0.0 ([commit
    1994f38](https://github.com/scrapy/scrapy/commit/1994f38){.reference
    .external})

-   py26 can not format zero length fields {} ([commit
    abf756f](https://github.com/scrapy/scrapy/commit/abf756f){.reference
    .external})

-   test PotentiaDataLoss errors on unbound responses ([commit
    b15470d](https://github.com/scrapy/scrapy/commit/b15470d){.reference
    .external})

-   Treat responses without content-length or Transfer-Encoding as good
    responses ([commit
    c4bf324](https://github.com/scrapy/scrapy/commit/c4bf324){.reference
    .external})

-   do no include ResponseFailed if http11 handler is not enabled
    ([commit
    6cbe684](https://github.com/scrapy/scrapy/commit/6cbe684){.reference
    .external})

-   New HTTP client wraps connection lost in ResponseFailed exception.
    fix #373 ([commit
    1a20bba](https://github.com/scrapy/scrapy/commit/1a20bba){.reference
    .external})

-   limit travis-ci build matrix ([commit
    3b01bb8](https://github.com/scrapy/scrapy/commit/3b01bb8){.reference
    .external})

-   Merge pull request #375 from peterarenot/patch-1 ([commit
    fa766d7](https://github.com/scrapy/scrapy/commit/fa766d7){.reference
    .external})

-   Fixed so it refers to the correct folder ([commit
    3283809](https://github.com/scrapy/scrapy/commit/3283809){.reference
    .external})

-   added Quantal & Raring to support Ubuntu releases ([commit
    1411923](https://github.com/scrapy/scrapy/commit/1411923){.reference
    .external})

-   fix retry middleware which didn't retry certain connection errors
    after the upgrade to http1 client, closes GH-373 ([commit
    bb35ed0](https://github.com/scrapy/scrapy/commit/bb35ed0){.reference
    .external})

-   fix XmlItemExporter in Python 2.7.4 and 2.7.5 ([commit
    de3e451](https://github.com/scrapy/scrapy/commit/de3e451){.reference
    .external})

-   minor updates to 0.18 release notes ([commit
    c45e5f1](https://github.com/scrapy/scrapy/commit/c45e5f1){.reference
    .external})

-   fix contributors list format ([commit
    0b60031](https://github.com/scrapy/scrapy/commit/0b60031){.reference
    .external})
:::

::: {#scrapy-0-18-0-released-2013-08-09 .section}
#### Scrapy 0.18.0 (released 2013-08-09)[¶](#scrapy-0-18-0-released-2013-08-09 "Permalink to this heading"){.headerlink}

-   Lot of improvements to testsuite run using Tox, including a way to
    test on pypi

-   Handle GET parameters for AJAX crawlable urls ([commit
    3fe2a32](https://github.com/scrapy/scrapy/commit/3fe2a32){.reference
    .external})

-   Use lxml recover option to parse sitemaps ([issue
    347](https://github.com/scrapy/scrapy/issues/347){.reference
    .external})

-   Bugfix cookie merging by hostname and not by netloc ([issue
    352](https://github.com/scrapy/scrapy/issues/352){.reference
    .external})

-   Support disabling [`HttpCompressionMiddleware`{.docutils .literal
    .notranslate}]{.pre} using a flag setting ([issue
    359](https://github.com/scrapy/scrapy/issues/359){.reference
    .external})

-   Support xml namespaces using [`iternodes`{.docutils .literal
    .notranslate}]{.pre} parser in [`XMLFeedSpider`{.docutils .literal
    .notranslate}]{.pre} ([issue
    12](https://github.com/scrapy/scrapy/issues/12){.reference
    .external})

-   Support [`dont_cache`{.docutils .literal .notranslate}]{.pre}
    request meta flag ([issue
    19](https://github.com/scrapy/scrapy/issues/19){.reference
    .external})

-   Bugfix [`scrapy.utils.gz.gunzip`{.docutils .literal
    .notranslate}]{.pre} broken by changes in python 2.7.4 ([commit
    4dc76e](https://github.com/scrapy/scrapy/commit/4dc76e){.reference
    .external})

-   Bugfix url encoding on [`SgmlLinkExtractor`{.docutils .literal
    .notranslate}]{.pre} ([issue
    24](https://github.com/scrapy/scrapy/issues/24){.reference
    .external})

-   Bugfix [`TakeFirst`{.docutils .literal .notranslate}]{.pre}
    processor shouldn't discard zero (0) value ([issue
    59](https://github.com/scrapy/scrapy/issues/59){.reference
    .external})

-   Support nested items in xml exporter ([issue
    66](https://github.com/scrapy/scrapy/issues/66){.reference
    .external})

-   Improve cookies handling performance ([issue
    77](https://github.com/scrapy/scrapy/issues/77){.reference
    .external})

-   Log dupe filtered requests once ([issue
    105](https://github.com/scrapy/scrapy/issues/105){.reference
    .external})

-   Split redirection middleware into status and meta based middlewares
    ([issue 78](https://github.com/scrapy/scrapy/issues/78){.reference
    .external})

-   Use HTTP1.1 as default downloader handler ([issue
    109](https://github.com/scrapy/scrapy/issues/109){.reference
    .external} and [issue
    318](https://github.com/scrapy/scrapy/issues/318){.reference
    .external})

-   Support xpath form selection on
    [`FormRequest.from_response`{.docutils .literal .notranslate}]{.pre}
    ([issue 185](https://github.com/scrapy/scrapy/issues/185){.reference
    .external})

-   Bugfix unicode decoding error on [`SgmlLinkExtractor`{.docutils
    .literal .notranslate}]{.pre} ([issue
    199](https://github.com/scrapy/scrapy/issues/199){.reference
    .external})

-   Bugfix signal dispatching on pypi interpreter ([issue
    205](https://github.com/scrapy/scrapy/issues/205){.reference
    .external})

-   Improve request delay and concurrency handling ([issue
    206](https://github.com/scrapy/scrapy/issues/206){.reference
    .external})

-   Add RFC2616 cache policy to [`HttpCacheMiddleware`{.docutils
    .literal .notranslate}]{.pre} ([issue
    212](https://github.com/scrapy/scrapy/issues/212){.reference
    .external})

-   Allow customization of messages logged by engine ([issue
    214](https://github.com/scrapy/scrapy/issues/214){.reference
    .external})

-   Multiples improvements to [`DjangoItem`{.docutils .literal
    .notranslate}]{.pre} ([issue
    217](https://github.com/scrapy/scrapy/issues/217){.reference
    .external}, [issue
    218](https://github.com/scrapy/scrapy/issues/218){.reference
    .external}, [issue
    221](https://github.com/scrapy/scrapy/issues/221){.reference
    .external})

-   Extend Scrapy commands using setuptools entry points ([issue
    260](https://github.com/scrapy/scrapy/issues/260){.reference
    .external})

-   Allow spider [`allowed_domains`{.docutils .literal
    .notranslate}]{.pre} value to be set/tuple ([issue
    261](https://github.com/scrapy/scrapy/issues/261){.reference
    .external})

-   Support [`settings.getdict`{.docutils .literal .notranslate}]{.pre}
    ([issue 269](https://github.com/scrapy/scrapy/issues/269){.reference
    .external})

-   Simplify internal [`scrapy.core.scraper`{.docutils .literal
    .notranslate}]{.pre} slot handling ([issue
    271](https://github.com/scrapy/scrapy/issues/271){.reference
    .external})

-   Added [`Item.copy`{.docutils .literal .notranslate}]{.pre} ([issue
    290](https://github.com/scrapy/scrapy/issues/290){.reference
    .external})

-   Collect idle downloader slots ([issue
    297](https://github.com/scrapy/scrapy/issues/297){.reference
    .external})

-   Add [`ftp://`{.docutils .literal .notranslate}]{.pre} scheme
    downloader handler ([issue
    329](https://github.com/scrapy/scrapy/issues/329){.reference
    .external})

-   Added downloader benchmark webserver and spider tools
    [[Benchmarking]{.std .std-ref}](index.html#benchmarking){.hoverxref
    .tooltip .reference .internal}

-   Moved persistent (on disk) queues to a separate project
    ([queuelib](https://github.com/scrapy/queuelib){.reference
    .external}) which Scrapy now depends on

-   Add Scrapy commands using external libraries ([issue
    260](https://github.com/scrapy/scrapy/issues/260){.reference
    .external})

-   Added [`--pdb`{.docutils .literal .notranslate}]{.pre} option to
    [`scrapy`{.docutils .literal .notranslate}]{.pre} command line tool

-   Added [[`XPathSelector.remove_namespaces`{.xref .py .py-meth
    .docutils .literal
    .notranslate}]{.pre}](index.html#scrapy.selector.Selector.remove_namespaces "scrapy.selector.Selector.remove_namespaces"){.reference
    .internal} which allows to remove all namespaces from XML documents
    for convenience (to work with namespace-less XPaths). Documented in
    [[Selectors]{.std .std-ref}](index.html#topics-selectors){.hoverxref
    .tooltip .reference .internal}.

-   Several improvements to spider contracts

-   New default middleware named MetaRefreshMiddleware that handles
    meta-refresh html tag redirections,

-   MetaRefreshMiddleware and RedirectMiddleware have different
    priorities to address #62

-   added from_crawler method to spiders

-   added system tests with mock server

-   more improvements to macOS compatibility (thanks Alex Cepoi)

-   several more cleanups to singletons and multi-spider support (thanks
    Nicolas Ramirez)

-   support custom download slots

-   added --spider option to "shell" command.

-   log overridden settings when Scrapy starts

Thanks to everyone who contribute to this release. Here is a list of
contributors sorted by number of commits:

::: {.highlight-default .notranslate}
::: highlight
    130 Pablo Hoffman <pablo@...>
     97 Daniel Graña <dangra@...>
     20 Nicolás Ramírez <nramirez.uy@...>
     13 Mikhail Korobov <kmike84@...>
     12 Pedro Faustino <pedrobandim@...>
     11 Steven Almeroth <sroth77@...>
      5 Rolando Espinoza La fuente <darkrho@...>
      4 Michal Danilak <mimino.coder@...>
      4 Alex Cepoi <alex.cepoi@...>
      4 Alexandr N Zamaraev (aka tonal) <tonal@...>
      3 paul <paul.tremberth@...>
      3 Martin Olveyra <molveyra@...>
      3 Jordi Llonch <llonchj@...>
      3 arijitchakraborty <myself.arijit@...>
      2 Shane Evans <shane.evans@...>
      2 joehillen <joehillen@...>
      2 Hart <HartSimha@...>
      2 Dan <ellisd23@...>
      1 Zuhao Wan <wanzuhao@...>
      1 whodatninja <blake@...>
      1 vkrest <v.krestiannykov@...>
      1 tpeng <pengtaoo@...>
      1 Tom Mortimer-Jones <tom@...>
      1 Rocio Aramberri <roschegel@...>
      1 Pedro <pedro@...>
      1 notsobad <wangxiaohugg@...>
      1 Natan L <kuyanatan.nlao@...>
      1 Mark Grey <mark.grey@...>
      1 Luan <luanpab@...>
      1 Libor Nenadál <libor.nenadal@...>
      1 Juan M Uys <opyate@...>
      1 Jonas Brunsgaard <jonas.brunsgaard@...>
      1 Ilya Baryshev <baryshev@...>
      1 Hasnain Lakhani <m.hasnain.lakhani@...>
      1 Emanuel Schorsch <emschorsch@...>
      1 Chris Tilden <chris.tilden@...>
      1 Capi Etheriel <barraponto@...>
      1 cacovsky <amarquesferraz@...>
      1 Berend Iwema <berend@...>
:::
:::
:::

::: {#scrapy-0-16-5-released-2013-05-30 .section}
#### Scrapy 0.16.5 (released 2013-05-30)[¶](#scrapy-0-16-5-released-2013-05-30 "Permalink to this heading"){.headerlink}

-   obey request method when Scrapy deploy is redirected to a new
    endpoint ([commit
    8c4fcee](https://github.com/scrapy/scrapy/commit/8c4fcee){.reference
    .external})

-   fix inaccurate downloader middleware documentation. refs #280
    ([commit
    40667cb](https://github.com/scrapy/scrapy/commit/40667cb){.reference
    .external})

-   doc: remove links to diveintopython.org, which is no longer
    available. closes #246 ([commit
    bd58bfa](https://github.com/scrapy/scrapy/commit/bd58bfa){.reference
    .external})

-   Find form nodes in invalid html5 documents ([commit
    e3d6945](https://github.com/scrapy/scrapy/commit/e3d6945){.reference
    .external})

-   Fix typo labeling attrs type bool instead of list ([commit
    a274276](https://github.com/scrapy/scrapy/commit/a274276){.reference
    .external})
:::

::: {#scrapy-0-16-4-released-2013-01-23 .section}
#### Scrapy 0.16.4 (released 2013-01-23)[¶](#scrapy-0-16-4-released-2013-01-23 "Permalink to this heading"){.headerlink}

-   fixes spelling errors in documentation ([commit
    6d2b3aa](https://github.com/scrapy/scrapy/commit/6d2b3aa){.reference
    .external})

-   add doc about disabling an extension. refs #132 ([commit
    c90de33](https://github.com/scrapy/scrapy/commit/c90de33){.reference
    .external})

-   Fixed error message formatting. log.err() doesn't support cool
    formatting and when error occurred, the message was: "ERROR: Error
    processing %(item)s" ([commit
    c16150c](https://github.com/scrapy/scrapy/commit/c16150c){.reference
    .external})

-   lint and improve images pipeline error logging ([commit
    56b45fc](https://github.com/scrapy/scrapy/commit/56b45fc){.reference
    .external})

-   fixed doc typos ([commit
    243be84](https://github.com/scrapy/scrapy/commit/243be84){.reference
    .external})

-   add documentation topics: Broad Crawls & Common Practices ([commit
    1fbb715](https://github.com/scrapy/scrapy/commit/1fbb715){.reference
    .external})

-   fix bug in Scrapy parse command when spider is not specified
    explicitly. closes #209 ([commit
    c72e682](https://github.com/scrapy/scrapy/commit/c72e682){.reference
    .external})

-   Update docs/topics/commands.rst ([commit
    28eac7a](https://github.com/scrapy/scrapy/commit/28eac7a){.reference
    .external})
:::

::: {#scrapy-0-16-3-released-2012-12-07 .section}
#### Scrapy 0.16.3 (released 2012-12-07)[¶](#scrapy-0-16-3-released-2012-12-07 "Permalink to this heading"){.headerlink}

-   Remove concurrency limitation when using download delays and still
    ensure inter-request delays are enforced ([commit
    487b9b5](https://github.com/scrapy/scrapy/commit/487b9b5){.reference
    .external})

-   add error details when image pipeline fails ([commit
    8232569](https://github.com/scrapy/scrapy/commit/8232569){.reference
    .external})

-   improve macOS compatibility ([commit
    8dcf8aa](https://github.com/scrapy/scrapy/commit/8dcf8aa){.reference
    .external})

-   setup.py: use README.rst to populate long_description ([commit
    7b5310d](https://github.com/scrapy/scrapy/commit/7b5310d){.reference
    .external})

-   doc: removed obsolete references to ClientForm ([commit
    80f9bb6](https://github.com/scrapy/scrapy/commit/80f9bb6){.reference
    .external})

-   correct docs for default storage backend ([commit
    2aa491b](https://github.com/scrapy/scrapy/commit/2aa491b){.reference
    .external})

-   doc: removed broken proxyhub link from FAQ ([commit
    bdf61c4](https://github.com/scrapy/scrapy/commit/bdf61c4){.reference
    .external})

-   Fixed docs typo in SpiderOpenCloseLogging example ([commit
    7184094](https://github.com/scrapy/scrapy/commit/7184094){.reference
    .external})
:::

::: {#scrapy-0-16-2-released-2012-11-09 .section}
#### Scrapy 0.16.2 (released 2012-11-09)[¶](#scrapy-0-16-2-released-2012-11-09 "Permalink to this heading"){.headerlink}

-   Scrapy contracts: python2.6 compat ([commit
    a4a9199](https://github.com/scrapy/scrapy/commit/a4a9199){.reference
    .external})

-   Scrapy contracts verbose option ([commit
    ec41673](https://github.com/scrapy/scrapy/commit/ec41673){.reference
    .external})

-   proper unittest-like output for Scrapy contracts ([commit
    86635e4](https://github.com/scrapy/scrapy/commit/86635e4){.reference
    .external})

-   added open_in_browser to debugging doc ([commit
    c9b690d](https://github.com/scrapy/scrapy/commit/c9b690d){.reference
    .external})

-   removed reference to global Scrapy stats from settings doc ([commit
    dd55067](https://github.com/scrapy/scrapy/commit/dd55067){.reference
    .external})

-   Fix SpiderState bug in Windows platforms ([commit
    58998f4](https://github.com/scrapy/scrapy/commit/58998f4){.reference
    .external})
:::

::: {#scrapy-0-16-1-released-2012-10-26 .section}
#### Scrapy 0.16.1 (released 2012-10-26)[¶](#scrapy-0-16-1-released-2012-10-26 "Permalink to this heading"){.headerlink}

-   fixed LogStats extension, which got broken after a wrong merge
    before the 0.16 release ([commit
    8c780fd](https://github.com/scrapy/scrapy/commit/8c780fd){.reference
    .external})

-   better backward compatibility for scrapy.conf.settings ([commit
    3403089](https://github.com/scrapy/scrapy/commit/3403089){.reference
    .external})

-   extended documentation on how to access crawler stats from
    extensions ([commit
    c4da0b5](https://github.com/scrapy/scrapy/commit/c4da0b5){.reference
    .external})

-   removed .hgtags (no longer needed now that Scrapy uses git) ([commit
    d52c188](https://github.com/scrapy/scrapy/commit/d52c188){.reference
    .external})

-   fix dashes under rst headers ([commit
    fa4f7f9](https://github.com/scrapy/scrapy/commit/fa4f7f9){.reference
    .external})

-   set release date for 0.16.0 in news ([commit
    e292246](https://github.com/scrapy/scrapy/commit/e292246){.reference
    .external})
:::

::: {#scrapy-0-16-0-released-2012-10-18 .section}
#### Scrapy 0.16.0 (released 2012-10-18)[¶](#scrapy-0-16-0-released-2012-10-18 "Permalink to this heading"){.headerlink}

Scrapy changes:

-   added [[Spiders Contracts]{.std
    .std-ref}](index.html#topics-contracts){.hoverxref .tooltip
    .reference .internal}, a mechanism for testing spiders in a
    formal/reproducible way

-   added options [`-o`{.docutils .literal .notranslate}]{.pre} and
    [`-t`{.docutils .literal .notranslate}]{.pre} to the
    [[`runspider`{.xref .std .std-command .docutils .literal
    .notranslate}]{.pre}](index.html#std-command-runspider){.hoverxref
    .tooltip .reference .internal} command

-   documented [[AutoThrottle
    extension]{.doc}](index.html#document-topics/autothrottle){.reference
    .internal} and added to extensions installed by default. You still
    need to enable it with [[`AUTOTHROTTLE_ENABLED`{.xref .std
    .std-setting .docutils .literal
    .notranslate}]{.pre}](index.html#std-setting-AUTOTHROTTLE_ENABLED){.hoverxref
    .tooltip .reference .internal}

-   major Stats Collection refactoring: removed separation of
    global/per-spider stats, removed stats-related signals
    ([`stats_spider_opened`{.docutils .literal .notranslate}]{.pre},
    etc). Stats are much simpler now, backward compatibility is kept on
    the Stats Collector API and signals.

-   added [[`process_start_requests()`{.xref .py .py-meth .docutils
    .literal
    .notranslate}]{.pre}](index.html#scrapy.spidermiddlewares.SpiderMiddleware.process_start_requests "scrapy.spidermiddlewares.SpiderMiddleware.process_start_requests"){.reference
    .internal} method to spider middlewares

-   dropped Signals singleton. Signals should now be accessed through
    the Crawler.signals attribute. See the signals documentation for
    more info.

-   dropped Stats Collector singleton. Stats can now be accessed through
    the Crawler.stats attribute. See the stats collection documentation
    for more info.

-   documented [[Core API]{.std
    .std-ref}](index.html#topics-api){.hoverxref .tooltip .reference
    .internal}

-   [`lxml`{.docutils .literal .notranslate}]{.pre} is now the default
    selectors backend instead of [`libxml2`{.docutils .literal
    .notranslate}]{.pre}

-   ported FormRequest.from_response() to use
    [lxml](https://lxml.de/){.reference .external} instead of
    [ClientForm](http://wwwsearch.sourceforge.net/old/ClientForm/){.reference
    .external}

-   removed modules: [`scrapy.xlib.BeautifulSoup`{.docutils .literal
    .notranslate}]{.pre} and [`scrapy.xlib.ClientForm`{.docutils
    .literal .notranslate}]{.pre}

-   SitemapSpider: added support for sitemap urls ending in .xml and
    .xml.gz, even if they advertise a wrong content type ([commit
    10ed28b](https://github.com/scrapy/scrapy/commit/10ed28b){.reference
    .external})

-   StackTraceDump extension: also dump trackref live references
    ([commit
    fe2ce93](https://github.com/scrapy/scrapy/commit/fe2ce93){.reference
    .external})

-   nested items now fully supported in JSON and JSONLines exporters

-   added [[`cookiejar`{.xref .std .std-reqmeta .docutils .literal
    .notranslate}]{.pre}](index.html#std-reqmeta-cookiejar){.hoverxref
    .tooltip .reference .internal} Request meta key to support multiple
    cookie sessions per spider

-   decoupled encoding detection code to
    [w3lib.encoding](https://github.com/scrapy/w3lib/blob/master/w3lib/encoding.py){.reference
    .external}, and ported Scrapy code to use that module

-   dropped support for Python 2.5. See
    [https://blog.scrapinghub.com/2012/02/27/scrapy-0-15-dropping-support-for-python-2-5/](https://blog.scrapinghub.com/2012/02/27/scrapy-0-15-dropping-support-for-python-2-5/){.reference
    .external}

-   dropped support for Twisted 2.5

-   added [[`REFERER_ENABLED`{.xref .std .std-setting .docutils .literal
    .notranslate}]{.pre}](index.html#std-setting-REFERER_ENABLED){.hoverxref
    .tooltip .reference .internal} setting, to control referer
    middleware

-   changed default user agent to: [`Scrapy/VERSION`{.docutils .literal
    .notranslate}]{.pre}` `{.docutils .literal
    .notranslate}[`(+http://scrapy.org)`{.docutils .literal
    .notranslate}]{.pre}

-   removed (undocumented) [`HTMLImageLinkExtractor`{.docutils .literal
    .notranslate}]{.pre} class from
    [`scrapy.contrib.linkextractors.image`{.docutils .literal
    .notranslate}]{.pre}

-   removed per-spider settings (to be replaced by instantiating
    multiple crawler objects)

-   [`USER_AGENT`{.docutils .literal .notranslate}]{.pre} spider
    attribute will no longer work, use [`user_agent`{.docutils .literal
    .notranslate}]{.pre} attribute instead

-   [`DOWNLOAD_TIMEOUT`{.docutils .literal .notranslate}]{.pre} spider
    attribute will no longer work, use [`download_timeout`{.docutils
    .literal .notranslate}]{.pre} attribute instead

-   removed [`ENCODING_ALIASES`{.docutils .literal .notranslate}]{.pre}
    setting, as encoding auto-detection has been moved to the
    [w3lib](https://github.com/scrapy/w3lib){.reference .external}
    library

-   promoted [[DjangoItem]{.std
    .std-ref}](index.html#topics-djangoitem){.hoverxref .tooltip
    .reference .internal} to main contrib

-   LogFormatter method now return dicts(instead of strings) to support
    lazy formatting ([issue
    164](https://github.com/scrapy/scrapy/issues/164){.reference
    .external}, [commit
    dcef7b0](https://github.com/scrapy/scrapy/commit/dcef7b0){.reference
    .external})

-   downloader handlers ([[`DOWNLOAD_HANDLERS`{.xref .std .std-setting
    .docutils .literal
    .notranslate}]{.pre}](index.html#std-setting-DOWNLOAD_HANDLERS){.hoverxref
    .tooltip .reference .internal} setting) now receive settings as the
    first argument of the [`__init__`{.docutils .literal
    .notranslate}]{.pre} method

-   replaced memory usage accounting with (more portable)
    [resource](https://docs.python.org/2/library/resource.html){.reference
    .external} module, removed [`scrapy.utils.memory`{.docutils .literal
    .notranslate}]{.pre} module

-   removed signal: [`scrapy.mail.mail_sent`{.docutils .literal
    .notranslate}]{.pre}

-   removed [`TRACK_REFS`{.docutils .literal .notranslate}]{.pre}
    setting, now [[trackrefs]{.std
    .std-ref}](index.html#topics-leaks-trackrefs){.hoverxref .tooltip
    .reference .internal} is always enabled

-   DBM is now the default storage backend for HTTP cache middleware

-   number of log messages (per level) are now tracked through Scrapy
    stats (stat name: [`log_count/LEVEL`{.docutils .literal
    .notranslate}]{.pre})

-   number received responses are now tracked through Scrapy stats (stat
    name: [`response_received_count`{.docutils .literal
    .notranslate}]{.pre})

-   removed [`scrapy.log.started`{.docutils .literal
    .notranslate}]{.pre} attribute
:::

::: {#scrapy-0-14-4 .section}
#### Scrapy 0.14.4[¶](#scrapy-0-14-4 "Permalink to this heading"){.headerlink}

-   added precise to supported Ubuntu distros ([commit
    b7e46df](https://github.com/scrapy/scrapy/commit/b7e46df){.reference
    .external})

-   fixed bug in json-rpc webservice reported in
    [https://groups.google.com/forum/#!topic/scrapy-users/qgVBmFybNAQ/discussion](https://groups.google.com/forum/#!topic/scrapy-users/qgVBmFybNAQ/discussion){.reference
    .external}. also removed no longer supported 'run' command from
    extras/scrapy-ws.py ([commit
    340fbdb](https://github.com/scrapy/scrapy/commit/340fbdb){.reference
    .external})

-   meta tag attributes for content-type http equiv can be in any order.
    #123 ([commit
    0cb68af](https://github.com/scrapy/scrapy/commit/0cb68af){.reference
    .external})

-   replace "import Image" by more standard "from PIL import Image".
    closes #88 ([commit
    4d17048](https://github.com/scrapy/scrapy/commit/4d17048){.reference
    .external})

-   return trial status as bin/runtests.sh exit value. #118 ([commit
    b7b2e7f](https://github.com/scrapy/scrapy/commit/b7b2e7f){.reference
    .external})
:::

::: {#scrapy-0-14-3 .section}
#### Scrapy 0.14.3[¶](#scrapy-0-14-3 "Permalink to this heading"){.headerlink}

-   forgot to include pydispatch license. #118 ([commit
    fd85f9c](https://github.com/scrapy/scrapy/commit/fd85f9c){.reference
    .external})

-   include egg files used by testsuite in source distribution. #118
    ([commit
    c897793](https://github.com/scrapy/scrapy/commit/c897793){.reference
    .external})

-   update docstring in project template to avoid confusion with
    genspider command, which may be considered as an advanced feature.
    refs #107 ([commit
    2548dcc](https://github.com/scrapy/scrapy/commit/2548dcc){.reference
    .external})

-   added note to docs/topics/firebug.rst about google directory being
    shut down ([commit
    668e352](https://github.com/scrapy/scrapy/commit/668e352){.reference
    .external})

-   don't discard slot when empty, just save in another dict in order to
    recycle if needed again. ([commit
    8e9f607](https://github.com/scrapy/scrapy/commit/8e9f607){.reference
    .external})

-   do not fail handling unicode xpaths in libxml2 backed selectors
    ([commit
    b830e95](https://github.com/scrapy/scrapy/commit/b830e95){.reference
    .external})

-   fixed minor mistake in Request objects documentation ([commit
    bf3c9ee](https://github.com/scrapy/scrapy/commit/bf3c9ee){.reference
    .external})

-   fixed minor defect in link extractors documentation ([commit
    ba14f38](https://github.com/scrapy/scrapy/commit/ba14f38){.reference
    .external})

-   removed some obsolete remaining code related to sqlite support in
    Scrapy ([commit
    0665175](https://github.com/scrapy/scrapy/commit/0665175){.reference
    .external})
:::

::: {#scrapy-0-14-2 .section}
#### Scrapy 0.14.2[¶](#scrapy-0-14-2 "Permalink to this heading"){.headerlink}

-   move buffer pointing to start of file before computing checksum.
    refs #92 ([commit
    6a5bef2](https://github.com/scrapy/scrapy/commit/6a5bef2){.reference
    .external})

-   Compute image checksum before persisting images. closes #92 ([commit
    9817df1](https://github.com/scrapy/scrapy/commit/9817df1){.reference
    .external})

-   remove leaking references in cached failures ([commit
    673a120](https://github.com/scrapy/scrapy/commit/673a120){.reference
    .external})

-   fixed bug in MemoryUsage extension: get_engine_status() takes
    exactly 1 argument (0 given) ([commit
    11133e9](https://github.com/scrapy/scrapy/commit/11133e9){.reference
    .external})

-   fixed struct.error on http compression middleware. closes #87
    ([commit
    1423140](https://github.com/scrapy/scrapy/commit/1423140){.reference
    .external})

-   ajax crawling wasn't expanding for unicode urls ([commit
    0de3fb4](https://github.com/scrapy/scrapy/commit/0de3fb4){.reference
    .external})

-   Catch start_requests iterator errors. refs #83 ([commit
    454a21d](https://github.com/scrapy/scrapy/commit/454a21d){.reference
    .external})

-   Speed-up libxml2 XPathSelector ([commit
    2fbd662](https://github.com/scrapy/scrapy/commit/2fbd662){.reference
    .external})

-   updated versioning doc according to recent changes ([commit
    0a070f5](https://github.com/scrapy/scrapy/commit/0a070f5){.reference
    .external})

-   scrapyd: fixed documentation link ([commit
    2b4e4c3](https://github.com/scrapy/scrapy/commit/2b4e4c3){.reference
    .external})

-   extras/makedeb.py: no longer obtaining version from git ([commit
    caffe0e](https://github.com/scrapy/scrapy/commit/caffe0e){.reference
    .external})
:::

::: {#scrapy-0-14-1 .section}
#### Scrapy 0.14.1[¶](#scrapy-0-14-1 "Permalink to this heading"){.headerlink}

-   extras/makedeb.py: no longer obtaining version from git ([commit
    caffe0e](https://github.com/scrapy/scrapy/commit/caffe0e){.reference
    .external})

-   bumped version to 0.14.1 ([commit
    6cb9e1c](https://github.com/scrapy/scrapy/commit/6cb9e1c){.reference
    .external})

-   fixed reference to tutorial directory ([commit
    4b86bd6](https://github.com/scrapy/scrapy/commit/4b86bd6){.reference
    .external})

-   doc: removed duplicated callback argument from Request.replace()
    ([commit
    1aeccdd](https://github.com/scrapy/scrapy/commit/1aeccdd){.reference
    .external})

-   fixed formatting of scrapyd doc ([commit
    8bf19e6](https://github.com/scrapy/scrapy/commit/8bf19e6){.reference
    .external})

-   Dump stacks for all running threads and fix engine status dumped by
    StackTraceDump extension ([commit
    14a8e6e](https://github.com/scrapy/scrapy/commit/14a8e6e){.reference
    .external})

-   added comment about why we disable ssl on boto images upload
    ([commit
    5223575](https://github.com/scrapy/scrapy/commit/5223575){.reference
    .external})

-   SSL handshaking hangs when doing too many parallel connections to S3
    ([commit
    63d583d](https://github.com/scrapy/scrapy/commit/63d583d){.reference
    .external})

-   change tutorial to follow changes on dmoz site ([commit
    bcb3198](https://github.com/scrapy/scrapy/commit/bcb3198){.reference
    .external})

-   Avoid \_disconnectedDeferred AttributeError exception in
    Twisted\>=11.1.0 ([commit
    98f3f87](https://github.com/scrapy/scrapy/commit/98f3f87){.reference
    .external})

-   allow spider to set autothrottle max concurrency ([commit
    175a4b5](https://github.com/scrapy/scrapy/commit/175a4b5){.reference
    .external})
:::

::: {#scrapy-0-14 .section}
#### Scrapy 0.14[¶](#scrapy-0-14 "Permalink to this heading"){.headerlink}

::: {#new-features-and-settings .section}
##### New features and settings[¶](#new-features-and-settings "Permalink to this heading"){.headerlink}

-   Support for [AJAX crawlable
    urls](https://developers.google.com/search/docs/ajax-crawling/docs/getting-started?csw=1){.reference
    .external}

-   New persistent scheduler that stores requests on disk, allowing to
    suspend and resume crawls
    ([r2737](http://hg.scrapy.org/scrapy/changeset/2737){.reference
    .external})

-   added [`-o`{.docutils .literal .notranslate}]{.pre} option to
    [`scrapy`{.docutils .literal .notranslate}]{.pre}` `{.docutils
    .literal .notranslate}[`crawl`{.docutils .literal
    .notranslate}]{.pre}, a shortcut for dumping scraped items into a
    file (or standard output using [`-`{.docutils .literal
    .notranslate}]{.pre})

-   Added support for passing custom settings to Scrapyd
    [`schedule.json`{.docutils .literal .notranslate}]{.pre} api
    ([r2779](http://hg.scrapy.org/scrapy/changeset/2779){.reference
    .external},
    [r2783](http://hg.scrapy.org/scrapy/changeset/2783){.reference
    .external})

-   New [`ChunkedTransferMiddleware`{.docutils .literal
    .notranslate}]{.pre} (enabled by default) to support [chunked
    transfer
    encoding](https://en.wikipedia.org/wiki/Chunked_transfer_encoding){.reference
    .external}
    ([r2769](http://hg.scrapy.org/scrapy/changeset/2769){.reference
    .external})

-   Add boto 2.0 support for S3 downloader handler
    ([r2763](http://hg.scrapy.org/scrapy/changeset/2763){.reference
    .external})

-   Added
    [marshal](https://docs.python.org/2/library/marshal.html){.reference
    .external} to formats supported by feed exports
    ([r2744](http://hg.scrapy.org/scrapy/changeset/2744){.reference
    .external})

-   In request errbacks, offending requests are now received in
    [`failure.request`{.docutils .literal .notranslate}]{.pre} attribute
    ([r2738](http://hg.scrapy.org/scrapy/changeset/2738){.reference
    .external})

-   

    Big downloader refactoring to support per domain/ip concurrency limits ([r2732](http://hg.scrapy.org/scrapy/changeset/2732){.reference .external})

    :   -   

            [`CONCURRENT_REQUESTS_PER_SPIDER`{.docutils .literal .notranslate}]{.pre} setting has been deprecated and replaced by:

            :   -   [[`CONCURRENT_REQUESTS`{.xref .std .std-setting
                    .docutils .literal
                    .notranslate}]{.pre}](index.html#std-setting-CONCURRENT_REQUESTS){.hoverxref
                    .tooltip .reference .internal},
                    [[`CONCURRENT_REQUESTS_PER_DOMAIN`{.xref .std
                    .std-setting .docutils .literal
                    .notranslate}]{.pre}](index.html#std-setting-CONCURRENT_REQUESTS_PER_DOMAIN){.hoverxref
                    .tooltip .reference .internal},
                    [[`CONCURRENT_REQUESTS_PER_IP`{.xref .std
                    .std-setting .docutils .literal
                    .notranslate}]{.pre}](index.html#std-setting-CONCURRENT_REQUESTS_PER_IP){.hoverxref
                    .tooltip .reference .internal}

        -   check the documentation for more details

-   Added builtin caching DNS resolver
    ([r2728](http://hg.scrapy.org/scrapy/changeset/2728){.reference
    .external})

-   Moved Amazon AWS-related components/extensions (SQS spider queue,
    SimpleDB stats collector) to a separate project:
    \[scaws\]([https://github.com/scrapinghub/scaws](https://github.com/scrapinghub/scaws){.reference
    .external})
    ([r2706](http://hg.scrapy.org/scrapy/changeset/2706){.reference
    .external},
    [r2714](http://hg.scrapy.org/scrapy/changeset/2714){.reference
    .external})

-   Moved spider queues to scrapyd: [`scrapy.spiderqueue`{.docutils
    .literal .notranslate}]{.pre} -\> [`scrapyd.spiderqueue`{.docutils
    .literal .notranslate}]{.pre}
    ([r2708](http://hg.scrapy.org/scrapy/changeset/2708){.reference
    .external})

-   Moved sqlite utils to scrapyd: [`scrapy.utils.sqlite`{.docutils
    .literal .notranslate}]{.pre} -\> [`scrapyd.sqlite`{.docutils
    .literal .notranslate}]{.pre}
    ([r2781](http://hg.scrapy.org/scrapy/changeset/2781){.reference
    .external})

-   Real support for returning iterators on
    [`start_requests()`{.docutils .literal .notranslate}]{.pre} method.
    The iterator is now consumed during the crawl when the spider is
    getting idle
    ([r2704](http://hg.scrapy.org/scrapy/changeset/2704){.reference
    .external})

-   Added [[`REDIRECT_ENABLED`{.xref .std .std-setting .docutils
    .literal
    .notranslate}]{.pre}](index.html#std-setting-REDIRECT_ENABLED){.hoverxref
    .tooltip .reference .internal} setting to quickly enable/disable the
    redirect middleware
    ([r2697](http://hg.scrapy.org/scrapy/changeset/2697){.reference
    .external})

-   Added [[`RETRY_ENABLED`{.xref .std .std-setting .docutils .literal
    .notranslate}]{.pre}](index.html#std-setting-RETRY_ENABLED){.hoverxref
    .tooltip .reference .internal} setting to quickly enable/disable the
    retry middleware
    ([r2694](http://hg.scrapy.org/scrapy/changeset/2694){.reference
    .external})

-   Added [`CloseSpider`{.docutils .literal .notranslate}]{.pre}
    exception to manually close spiders
    ([r2691](http://hg.scrapy.org/scrapy/changeset/2691){.reference
    .external})

-   Improved encoding detection by adding support for HTML5 meta charset
    declaration
    ([r2690](http://hg.scrapy.org/scrapy/changeset/2690){.reference
    .external})

-   Refactored close spider behavior to wait for all downloads to finish
    and be processed by spiders, before closing the spider
    ([r2688](http://hg.scrapy.org/scrapy/changeset/2688){.reference
    .external})

-   Added [`SitemapSpider`{.docutils .literal .notranslate}]{.pre} (see
    documentation in Spiders page)
    ([r2658](http://hg.scrapy.org/scrapy/changeset/2658){.reference
    .external})

-   Added [`LogStats`{.docutils .literal .notranslate}]{.pre} extension
    for periodically logging basic stats (like crawled pages and scraped
    items)
    ([r2657](http://hg.scrapy.org/scrapy/changeset/2657){.reference
    .external})

-   Make handling of gzipped responses more robust (#319,
    [r2643](http://hg.scrapy.org/scrapy/changeset/2643){.reference
    .external}). Now Scrapy will try and decompress as much as possible
    from a gzipped response, instead of failing with an
    [`IOError`{.docutils .literal .notranslate}]{.pre}.

-   Simplified !MemoryDebugger extension to use stats for dumping memory
    debugging info
    ([r2639](http://hg.scrapy.org/scrapy/changeset/2639){.reference
    .external})

-   Added new command to edit spiders: [`scrapy`{.docutils .literal
    .notranslate}]{.pre}` `{.docutils .literal
    .notranslate}[`edit`{.docutils .literal .notranslate}]{.pre}
    ([r2636](http://hg.scrapy.org/scrapy/changeset/2636){.reference
    .external}) and [`-e`{.docutils .literal .notranslate}]{.pre} flag
    to [`genspider`{.docutils .literal .notranslate}]{.pre} command that
    uses it
    ([r2653](http://hg.scrapy.org/scrapy/changeset/2653){.reference
    .external})

-   Changed default representation of items to pretty-printed dicts.
    ([r2631](http://hg.scrapy.org/scrapy/changeset/2631){.reference
    .external}). This improves default logging by making log more
    readable in the default case, for both Scraped and Dropped lines.

-   Added [[`spider_error`{.xref .std .std-signal .docutils .literal
    .notranslate}]{.pre}](index.html#std-signal-spider_error){.hoverxref
    .tooltip .reference .internal} signal
    ([r2628](http://hg.scrapy.org/scrapy/changeset/2628){.reference
    .external})

-   Added [[`COOKIES_ENABLED`{.xref .std .std-setting .docutils .literal
    .notranslate}]{.pre}](index.html#std-setting-COOKIES_ENABLED){.hoverxref
    .tooltip .reference .internal} setting
    ([r2625](http://hg.scrapy.org/scrapy/changeset/2625){.reference
    .external})

-   Stats are now dumped to Scrapy log (default value of
    [[`STATS_DUMP`{.xref .std .std-setting .docutils .literal
    .notranslate}]{.pre}](index.html#std-setting-STATS_DUMP){.hoverxref
    .tooltip .reference .internal} setting has been changed to
    [`True`{.docutils .literal .notranslate}]{.pre}). This is to make
    Scrapy users more aware of Scrapy stats and the data that is
    collected there.

-   Added support for dynamically adjusting download delay and maximum
    concurrent requests
    ([r2599](http://hg.scrapy.org/scrapy/changeset/2599){.reference
    .external})

-   Added new DBM HTTP cache storage backend
    ([r2576](http://hg.scrapy.org/scrapy/changeset/2576){.reference
    .external})

-   Added [`listjobs.json`{.docutils .literal .notranslate}]{.pre} API
    to Scrapyd
    ([r2571](http://hg.scrapy.org/scrapy/changeset/2571){.reference
    .external})

-   [`CsvItemExporter`{.docutils .literal .notranslate}]{.pre}: added
    [`join_multivalued`{.docutils .literal .notranslate}]{.pre}
    parameter
    ([r2578](http://hg.scrapy.org/scrapy/changeset/2578){.reference
    .external})

-   Added namespace support to [`xmliter_lxml`{.docutils .literal
    .notranslate}]{.pre}
    ([r2552](http://hg.scrapy.org/scrapy/changeset/2552){.reference
    .external})

-   Improved cookies middleware by making [`COOKIES_DEBUG`{.docutils
    .literal .notranslate}]{.pre} nicer and documenting it
    ([r2579](http://hg.scrapy.org/scrapy/changeset/2579){.reference
    .external})

-   Several improvements to Scrapyd and Link extractors
:::

::: {#code-rearranged-and-removed .section}
##### Code rearranged and removed[¶](#code-rearranged-and-removed "Permalink to this heading"){.headerlink}

-   

    Merged item passed and item scraped concepts, as they have often proved confusing in the past. This means: ([r2630](http://hg.scrapy.org/scrapy/changeset/2630){.reference .external})

    :   -   original item_scraped signal was removed

        -   original item_passed signal was renamed to item_scraped

        -   old log lines [`Scraped`{.docutils .literal
            .notranslate}]{.pre}` `{.docutils .literal
            .notranslate}[`Item...`{.docutils .literal
            .notranslate}]{.pre} were removed

        -   old log lines [`Passed`{.docutils .literal
            .notranslate}]{.pre}` `{.docutils .literal
            .notranslate}[`Item...`{.docutils .literal
            .notranslate}]{.pre} were renamed to [`Scraped`{.docutils
            .literal .notranslate}]{.pre}` `{.docutils .literal
            .notranslate}[`Item...`{.docutils .literal
            .notranslate}]{.pre} lines and downgraded to
            [`DEBUG`{.docutils .literal .notranslate}]{.pre} level

-   

    Reduced Scrapy codebase by striping part of Scrapy code into two new libraries:

    :   -   [w3lib](https://github.com/scrapy/w3lib){.reference
            .external} (several functions from
            [`scrapy.utils.{http,markup,multipart,response,url}`{.docutils
            .literal .notranslate}]{.pre}, done in
            [r2584](http://hg.scrapy.org/scrapy/changeset/2584){.reference
            .external})

        -   [scrapely](https://github.com/scrapy/scrapely){.reference
            .external} (was [`scrapy.contrib.ibl`{.docutils .literal
            .notranslate}]{.pre}, done in
            [r2586](http://hg.scrapy.org/scrapy/changeset/2586){.reference
            .external})

-   Removed unused function:
    [`scrapy.utils.request.request_info()`{.docutils .literal
    .notranslate}]{.pre}
    ([r2577](http://hg.scrapy.org/scrapy/changeset/2577){.reference
    .external})

-   Removed googledir project from [`examples/googledir`{.docutils
    .literal .notranslate}]{.pre}. There's now a new example project
    called [`dirbot`{.docutils .literal .notranslate}]{.pre} available
    on GitHub:
    [https://github.com/scrapy/dirbot](https://github.com/scrapy/dirbot){.reference
    .external}

-   Removed support for default field values in Scrapy items
    ([r2616](http://hg.scrapy.org/scrapy/changeset/2616){.reference
    .external})

-   Removed experimental crawlspider v2
    ([r2632](http://hg.scrapy.org/scrapy/changeset/2632){.reference
    .external})

-   Removed scheduler middleware to simplify architecture. Duplicates
    filter is now done in the scheduler itself, using the same dupe
    filtering class as before ([`DUPEFILTER_CLASS`{.docutils .literal
    .notranslate}]{.pre} setting)
    ([r2640](http://hg.scrapy.org/scrapy/changeset/2640){.reference
    .external})

-   Removed support for passing urls to [`scrapy`{.docutils .literal
    .notranslate}]{.pre}` `{.docutils .literal
    .notranslate}[`crawl`{.docutils .literal .notranslate}]{.pre}
    command (use [`scrapy`{.docutils .literal
    .notranslate}]{.pre}` `{.docutils .literal
    .notranslate}[`parse`{.docutils .literal .notranslate}]{.pre}
    instead)
    ([r2704](http://hg.scrapy.org/scrapy/changeset/2704){.reference
    .external})

-   Removed deprecated Execution Queue
    ([r2704](http://hg.scrapy.org/scrapy/changeset/2704){.reference
    .external})

-   Removed (undocumented) spider context extension (from
    scrapy.contrib.spidercontext)
    ([r2780](http://hg.scrapy.org/scrapy/changeset/2780){.reference
    .external})

-   removed [`CONCURRENT_SPIDERS`{.docutils .literal
    .notranslate}]{.pre} setting (use scrapyd maxproc instead)
    ([r2789](http://hg.scrapy.org/scrapy/changeset/2789){.reference
    .external})

-   Renamed attributes of core components: downloader.sites -\>
    downloader.slots, scraper.sites -\> scraper.slots
    ([r2717](http://hg.scrapy.org/scrapy/changeset/2717){.reference
    .external},
    [r2718](http://hg.scrapy.org/scrapy/changeset/2718){.reference
    .external})

-   Renamed setting [`CLOSESPIDER_ITEMPASSED`{.docutils .literal
    .notranslate}]{.pre} to [[`CLOSESPIDER_ITEMCOUNT`{.xref .std
    .std-setting .docutils .literal
    .notranslate}]{.pre}](index.html#std-setting-CLOSESPIDER_ITEMCOUNT){.hoverxref
    .tooltip .reference .internal}
    ([r2655](http://hg.scrapy.org/scrapy/changeset/2655){.reference
    .external}). Backward compatibility kept.
:::
:::

::: {#scrapy-0-12 .section}
#### Scrapy 0.12[¶](#scrapy-0-12 "Permalink to this heading"){.headerlink}

The numbers like #NNN reference tickets in the old issue tracker (Trac)
which is no longer available.

::: {#new-features-and-improvements .section}
##### New features and improvements[¶](#new-features-and-improvements "Permalink to this heading"){.headerlink}

-   Passed item is now sent in the [`item`{.docutils .literal
    .notranslate}]{.pre} argument of the [[`item_passed`{.xref .std
    .std-signal .docutils .literal
    .notranslate}]{.pre}](index.html#std-signal-item_scraped){.hoverxref
    .tooltip .reference .internal} (#273)

-   Added verbose option to [`scrapy`{.docutils .literal
    .notranslate}]{.pre}` `{.docutils .literal
    .notranslate}[`version`{.docutils .literal .notranslate}]{.pre}
    command, useful for bug reports (#298)

-   HTTP cache now stored by default in the project data dir (#279)

-   Added project data storage directory (#276, #277)

-   Documented file structure of Scrapy projects (see command-line tool
    doc)

-   New lxml backend for XPath selectors (#147)

-   Per-spider settings (#245)

-   Support exit codes to signal errors in Scrapy commands (#248)

-   Added [`-c`{.docutils .literal .notranslate}]{.pre} argument to
    [`scrapy`{.docutils .literal .notranslate}]{.pre}` `{.docutils
    .literal .notranslate}[`shell`{.docutils .literal
    .notranslate}]{.pre} command

-   Made [`libxml2`{.docutils .literal .notranslate}]{.pre} optional
    (#260)

-   New [`deploy`{.docutils .literal .notranslate}]{.pre} command (#261)

-   Added [[`CLOSESPIDER_PAGECOUNT`{.xref .std .std-setting .docutils
    .literal
    .notranslate}]{.pre}](index.html#std-setting-CLOSESPIDER_PAGECOUNT){.hoverxref
    .tooltip .reference .internal} setting (#253)

-   Added [[`CLOSESPIDER_ERRORCOUNT`{.xref .std .std-setting .docutils
    .literal
    .notranslate}]{.pre}](index.html#std-setting-CLOSESPIDER_ERRORCOUNT){.hoverxref
    .tooltip .reference .internal} setting (#254)
:::

::: {#scrapyd-changes .section}
##### Scrapyd changes[¶](#scrapyd-changes "Permalink to this heading"){.headerlink}

-   Scrapyd now uses one process per spider

-   It stores one log file per spider run, and rotate them keeping the
    latest 5 logs per spider (by default)

-   A minimal web ui was added, available at
    [http://localhost:6800](http://localhost:6800){.reference .external}
    by default

-   There is now a [`scrapy`{.docutils .literal
    .notranslate}]{.pre}` `{.docutils .literal
    .notranslate}[`server`{.docutils .literal .notranslate}]{.pre}
    command to start a Scrapyd server of the current project
:::

::: {#changes-to-settings .section}
##### Changes to settings[¶](#changes-to-settings "Permalink to this heading"){.headerlink}

-   added [`HTTPCACHE_ENABLED`{.docutils .literal .notranslate}]{.pre}
    setting (False by default) to enable HTTP cache middleware

-   changed [`HTTPCACHE_EXPIRATION_SECS`{.docutils .literal
    .notranslate}]{.pre} semantics: now zero means "never expire".
:::

::: {#deprecated-obsoleted-functionality .section}
##### Deprecated/obsoleted functionality[¶](#deprecated-obsoleted-functionality "Permalink to this heading"){.headerlink}

-   Deprecated [`runserver`{.docutils .literal .notranslate}]{.pre}
    command in favor of [`server`{.docutils .literal
    .notranslate}]{.pre} command which starts a Scrapyd server. See
    also: Scrapyd changes

-   Deprecated [`queue`{.docutils .literal .notranslate}]{.pre} command
    in favor of using Scrapyd [`schedule.json`{.docutils .literal
    .notranslate}]{.pre} API. See also: Scrapyd changes

-   Removed the !LxmlItemLoader (experimental contrib which never
    graduated to main contrib)
:::
:::

::: {#scrapy-0-10 .section}
#### Scrapy 0.10[¶](#scrapy-0-10 "Permalink to this heading"){.headerlink}

The numbers like #NNN reference tickets in the old issue tracker (Trac)
which is no longer available.

::: {#id133 .section}
##### New features and improvements[¶](#id133 "Permalink to this heading"){.headerlink}

-   New Scrapy service called [`scrapyd`{.docutils .literal
    .notranslate}]{.pre} for deploying Scrapy crawlers in production
    (#218) (documentation available)

-   Simplified Images pipeline usage which doesn't require subclassing
    your own images pipeline now (#217)

-   Scrapy shell now shows the Scrapy log by default (#206)

-   Refactored execution queue in a common base code and pluggable
    backends called "spider queues" (#220)

-   New persistent spider queue (based on SQLite) (#198), available by
    default, which allows to start Scrapy in server mode and then
    schedule spiders to run.

-   Added documentation for Scrapy command-line tool and all its
    available sub-commands. (documentation available)

-   Feed exporters with pluggable backends (#197) (documentation
    available)

-   Deferred signals (#193)

-   Added two new methods to item pipeline open_spider(), close_spider()
    with deferred support (#195)

-   Support for overriding default request headers per spider (#181)

-   Replaced default Spider Manager with one with similar functionality
    but not depending on Twisted Plugins (#186)

-   Split Debian package into two packages - the library and the service
    (#187)

-   Scrapy log refactoring (#188)

-   New extension for keeping persistent spider contexts among different
    runs (#203)

-   Added [`dont_redirect`{.docutils .literal .notranslate}]{.pre}
    request.meta key for avoiding redirects (#233)

-   Added [`dont_retry`{.docutils .literal .notranslate}]{.pre}
    request.meta key for avoiding retries (#234)
:::

::: {#command-line-tool-changes .section}
##### Command-line tool changes[¶](#command-line-tool-changes "Permalink to this heading"){.headerlink}

-   New [`scrapy`{.docutils .literal .notranslate}]{.pre} command which
    replaces the old [`scrapy-ctl.py`{.docutils .literal
    .notranslate}]{.pre} (#199) - there is only one global
    [`scrapy`{.docutils .literal .notranslate}]{.pre} command now,
    instead of one [`scrapy-ctl.py`{.docutils .literal
    .notranslate}]{.pre} per project - Added [`scrapy.bat`{.docutils
    .literal .notranslate}]{.pre} script for running more conveniently
    from Windows

-   Added bash completion to command-line tool (#210)

-   Renamed command [`start`{.docutils .literal .notranslate}]{.pre} to
    [`runserver`{.docutils .literal .notranslate}]{.pre} (#209)
:::

::: {#api-changes .section}
##### API changes[¶](#api-changes "Permalink to this heading"){.headerlink}

-   [`url`{.docutils .literal .notranslate}]{.pre} and [`body`{.docutils
    .literal .notranslate}]{.pre} attributes of Request objects are now
    read-only (#230)

-   [`Request.copy()`{.docutils .literal .notranslate}]{.pre} and
    [`Request.replace()`{.docutils .literal .notranslate}]{.pre} now
    also copies their [`callback`{.docutils .literal
    .notranslate}]{.pre} and [`errback`{.docutils .literal
    .notranslate}]{.pre} attributes (#231)

-   Removed [`UrlFilterMiddleware`{.docutils .literal
    .notranslate}]{.pre} from [`scrapy.contrib`{.docutils .literal
    .notranslate}]{.pre} (already disabled by default)

-   Offsite middleware doesn't filter out any request coming from a
    spider that doesn't have a allowed_domains attribute (#225)

-   Removed Spider Manager [`load()`{.docutils .literal
    .notranslate}]{.pre} method. Now spiders are loaded in the
    [`__init__`{.docutils .literal .notranslate}]{.pre} method itself.

-   

    Changes to Scrapy Manager (now called "Crawler"):

    :   -   [`scrapy.core.manager.ScrapyManager`{.docutils .literal
            .notranslate}]{.pre} class renamed to
            [`scrapy.crawler.Crawler`{.docutils .literal
            .notranslate}]{.pre}

        -   [`scrapy.core.manager.scrapymanager`{.docutils .literal
            .notranslate}]{.pre} singleton moved to
            [`scrapy.project.crawler`{.docutils .literal
            .notranslate}]{.pre}

-   Moved module: [`scrapy.contrib.spidermanager`{.docutils .literal
    .notranslate}]{.pre} to [`scrapy.spidermanager`{.docutils .literal
    .notranslate}]{.pre}

-   Spider Manager singleton moved from
    [`scrapy.spider.spiders`{.docutils .literal .notranslate}]{.pre} to
    the [`` spiders` ``{.docutils .literal
    .notranslate}]{.pre}` `{.docutils .literal
    .notranslate}[`attribute`{.docutils .literal
    .notranslate}]{.pre}` `{.docutils .literal
    .notranslate}[`of`{.docutils .literal
    .notranslate}]{.pre}` `{.docutils .literal
    .notranslate}[``` ``scrapy.project.crawler ```{.docutils .literal
    .notranslate}]{.pre} singleton.

-   

    moved Stats Collector classes: (#204)

    :   -   [`scrapy.stats.collector.StatsCollector`{.docutils .literal
            .notranslate}]{.pre} to
            [`scrapy.statscol.StatsCollector`{.docutils .literal
            .notranslate}]{.pre}

        -   [`scrapy.stats.collector.SimpledbStatsCollector`{.docutils
            .literal .notranslate}]{.pre} to
            [`scrapy.contrib.statscol.SimpledbStatsCollector`{.docutils
            .literal .notranslate}]{.pre}

-   default per-command settings are now specified in the
    [`default_settings`{.docutils .literal .notranslate}]{.pre}
    attribute of command object class (#201)

-   

    changed arguments of Item pipeline [`process_item()`{.docutils .literal .notranslate}]{.pre} method from [`(spider,`{.docutils .literal .notranslate}]{.pre}` `{.docutils .literal .notranslate}[`item)`{.docutils .literal .notranslate}]{.pre} to [`(item,`{.docutils .literal .notranslate}]{.pre}` `{.docutils .literal .notranslate}[`spider)`{.docutils .literal .notranslate}]{.pre}

    :   -   backward compatibility kept (with deprecation warning)

-   

    moved [`scrapy.core.signals`{.docutils .literal .notranslate}]{.pre} module to [`scrapy.signals`{.docutils .literal .notranslate}]{.pre}

    :   -   backward compatibility kept (with deprecation warning)

-   

    moved [`scrapy.core.exceptions`{.docutils .literal .notranslate}]{.pre} module to [`scrapy.exceptions`{.docutils .literal .notranslate}]{.pre}

    :   -   backward compatibility kept (with deprecation warning)

-   added [`handles_request()`{.docutils .literal .notranslate}]{.pre}
    class method to [`BaseSpider`{.docutils .literal
    .notranslate}]{.pre}

-   dropped [`scrapy.log.exc()`{.docutils .literal .notranslate}]{.pre}
    function (use [`scrapy.log.err()`{.docutils .literal
    .notranslate}]{.pre} instead)

-   dropped [`component`{.docutils .literal .notranslate}]{.pre}
    argument of [`scrapy.log.msg()`{.docutils .literal
    .notranslate}]{.pre} function

-   dropped [`scrapy.log.log_level`{.docutils .literal
    .notranslate}]{.pre} attribute

-   Added [`from_settings()`{.docutils .literal .notranslate}]{.pre}
    class methods to Spider Manager, and Item Pipeline Manager
:::

::: {#id134 .section}
##### Changes to settings[¶](#id134 "Permalink to this heading"){.headerlink}

-   Added [`HTTPCACHE_IGNORE_SCHEMES`{.docutils .literal
    .notranslate}]{.pre} setting to ignore certain schemes on
    !HttpCacheMiddleware (#225)

-   Added [`SPIDER_QUEUE_CLASS`{.docutils .literal .notranslate}]{.pre}
    setting which defines the spider queue to use (#220)

-   Added [`KEEP_ALIVE`{.docutils .literal .notranslate}]{.pre} setting
    (#220)

-   Removed [`SERVICE_QUEUE`{.docutils .literal .notranslate}]{.pre}
    setting (#220)

-   Removed [`COMMANDS_SETTINGS_MODULE`{.docutils .literal
    .notranslate}]{.pre} setting (#201)

-   Renamed [`REQUEST_HANDLERS`{.docutils .literal .notranslate}]{.pre}
    to [`DOWNLOAD_HANDLERS`{.docutils .literal .notranslate}]{.pre} and
    make download handlers classes (instead of functions)
:::
:::

::: {#scrapy-0-9 .section}
#### Scrapy 0.9[¶](#scrapy-0-9 "Permalink to this heading"){.headerlink}

The numbers like #NNN reference tickets in the old issue tracker (Trac)
which is no longer available.

::: {#id135 .section}
##### New features and improvements[¶](#id135 "Permalink to this heading"){.headerlink}

-   Added SMTP-AUTH support to scrapy.mail

-   New settings added: [`MAIL_USER`{.docutils .literal
    .notranslate}]{.pre}, [`MAIL_PASS`{.docutils .literal
    .notranslate}]{.pre}
    ([r2065](http://hg.scrapy.org/scrapy/changeset/2065){.reference
    .external} \| #149)

-   Added new scrapy-ctl view command - To view URL in the browser, as
    seen by Scrapy
    ([r2039](http://hg.scrapy.org/scrapy/changeset/2039){.reference
    .external})

-   Added web service for controlling Scrapy process (this also
    deprecates the web console.
    ([r2053](http://hg.scrapy.org/scrapy/changeset/2053){.reference
    .external} \| #167)

-   Support for running Scrapy as a service, for production systems
    ([r1988](http://hg.scrapy.org/scrapy/changeset/1988){.reference
    .external},
    [r2054](http://hg.scrapy.org/scrapy/changeset/2054){.reference
    .external},
    [r2055](http://hg.scrapy.org/scrapy/changeset/2055){.reference
    .external},
    [r2056](http://hg.scrapy.org/scrapy/changeset/2056){.reference
    .external},
    [r2057](http://hg.scrapy.org/scrapy/changeset/2057){.reference
    .external} \| #168)

-   Added wrapper induction library (documentation only available in
    source code for now).
    ([r2011](http://hg.scrapy.org/scrapy/changeset/2011){.reference
    .external})

-   Simplified and improved response encoding support
    ([r1961](http://hg.scrapy.org/scrapy/changeset/1961){.reference
    .external},
    [r1969](http://hg.scrapy.org/scrapy/changeset/1969){.reference
    .external})

-   Added [`LOG_ENCODING`{.docutils .literal .notranslate}]{.pre}
    setting
    ([r1956](http://hg.scrapy.org/scrapy/changeset/1956){.reference
    .external}, documentation available)

-   Added [`RANDOMIZE_DOWNLOAD_DELAY`{.docutils .literal
    .notranslate}]{.pre} setting (enabled by default)
    ([r1923](http://hg.scrapy.org/scrapy/changeset/1923){.reference
    .external}, doc available)

-   [`MailSender`{.docutils .literal .notranslate}]{.pre} is no longer
    IO-blocking
    ([r1955](http://hg.scrapy.org/scrapy/changeset/1955){.reference
    .external} \| #146)

-   Linkextractors and new Crawlspider now handle relative base tag urls
    ([r1960](http://hg.scrapy.org/scrapy/changeset/1960){.reference
    .external} \| #148)

-   Several improvements to Item Loaders and processors
    ([r2022](http://hg.scrapy.org/scrapy/changeset/2022){.reference
    .external},
    [r2023](http://hg.scrapy.org/scrapy/changeset/2023){.reference
    .external},
    [r2024](http://hg.scrapy.org/scrapy/changeset/2024){.reference
    .external},
    [r2025](http://hg.scrapy.org/scrapy/changeset/2025){.reference
    .external},
    [r2026](http://hg.scrapy.org/scrapy/changeset/2026){.reference
    .external},
    [r2027](http://hg.scrapy.org/scrapy/changeset/2027){.reference
    .external},
    [r2028](http://hg.scrapy.org/scrapy/changeset/2028){.reference
    .external},
    [r2029](http://hg.scrapy.org/scrapy/changeset/2029){.reference
    .external},
    [r2030](http://hg.scrapy.org/scrapy/changeset/2030){.reference
    .external})

-   Added support for adding variables to telnet console
    ([r2047](http://hg.scrapy.org/scrapy/changeset/2047){.reference
    .external} \| #165)

-   Support for requests without callbacks
    ([r2050](http://hg.scrapy.org/scrapy/changeset/2050){.reference
    .external} \| #166)
:::

::: {#id136 .section}
##### API changes[¶](#id136 "Permalink to this heading"){.headerlink}

-   Change [`Spider.domain_name`{.docutils .literal .notranslate}]{.pre}
    to [`Spider.name`{.docutils .literal .notranslate}]{.pre} (SEP-012,
    [r1975](http://hg.scrapy.org/scrapy/changeset/1975){.reference
    .external})

-   [`Response.encoding`{.docutils .literal .notranslate}]{.pre} is now
    the detected encoding
    ([r1961](http://hg.scrapy.org/scrapy/changeset/1961){.reference
    .external})

-   [`HttpErrorMiddleware`{.docutils .literal .notranslate}]{.pre} now
    returns None or raises an exception
    ([r2006](http://hg.scrapy.org/scrapy/changeset/2006){.reference
    .external} \| #157)

-   [`scrapy.command`{.docutils .literal .notranslate}]{.pre} modules
    relocation
    ([r2035](http://hg.scrapy.org/scrapy/changeset/2035){.reference
    .external},
    [r2036](http://hg.scrapy.org/scrapy/changeset/2036){.reference
    .external},
    [r2037](http://hg.scrapy.org/scrapy/changeset/2037){.reference
    .external})

-   Added [`ExecutionQueue`{.docutils .literal .notranslate}]{.pre} for
    feeding spiders to scrape
    ([r2034](http://hg.scrapy.org/scrapy/changeset/2034){.reference
    .external})

-   Removed [`ExecutionEngine`{.docutils .literal .notranslate}]{.pre}
    singleton
    ([r2039](http://hg.scrapy.org/scrapy/changeset/2039){.reference
    .external})

-   Ported [`S3ImagesStore`{.docutils .literal .notranslate}]{.pre}
    (images pipeline) to use boto and threads
    ([r2033](http://hg.scrapy.org/scrapy/changeset/2033){.reference
    .external})

-   Moved module: [`scrapy.management.telnet`{.docutils .literal
    .notranslate}]{.pre} to [`scrapy.telnet`{.docutils .literal
    .notranslate}]{.pre}
    ([r2047](http://hg.scrapy.org/scrapy/changeset/2047){.reference
    .external})
:::

::: {#changes-to-default-settings .section}
##### Changes to default settings[¶](#changes-to-default-settings "Permalink to this heading"){.headerlink}

-   Changed default [`SCHEDULER_ORDER`{.docutils .literal
    .notranslate}]{.pre} to [`DFO`{.docutils .literal
    .notranslate}]{.pre}
    ([r1939](http://hg.scrapy.org/scrapy/changeset/1939){.reference
    .external})
:::
:::

::: {#scrapy-0-8 .section}
#### Scrapy 0.8[¶](#scrapy-0-8 "Permalink to this heading"){.headerlink}

The numbers like #NNN reference tickets in the old issue tracker (Trac)
which is no longer available.

::: {#id137 .section}
##### New features[¶](#id137 "Permalink to this heading"){.headerlink}

-   Added DEFAULT_RESPONSE_ENCODING setting
    ([r1809](http://hg.scrapy.org/scrapy/changeset/1809){.reference
    .external})

-   Added [`dont_click`{.docutils .literal .notranslate}]{.pre} argument
    to [`FormRequest.from_response()`{.docutils .literal
    .notranslate}]{.pre} method
    ([r1813](http://hg.scrapy.org/scrapy/changeset/1813){.reference
    .external},
    [r1816](http://hg.scrapy.org/scrapy/changeset/1816){.reference
    .external})

-   Added [`clickdata`{.docutils .literal .notranslate}]{.pre} argument
    to [`FormRequest.from_response()`{.docutils .literal
    .notranslate}]{.pre} method
    ([r1802](http://hg.scrapy.org/scrapy/changeset/1802){.reference
    .external},
    [r1803](http://hg.scrapy.org/scrapy/changeset/1803){.reference
    .external})

-   Added support for HTTP proxies ([`HttpProxyMiddleware`{.docutils
    .literal .notranslate}]{.pre})
    ([r1781](http://hg.scrapy.org/scrapy/changeset/1781){.reference
    .external},
    [r1785](http://hg.scrapy.org/scrapy/changeset/1785){.reference
    .external})

-   Offsite spider middleware now logs messages when filtering out
    requests
    ([r1841](http://hg.scrapy.org/scrapy/changeset/1841){.reference
    .external})
:::

::: {#id138 .section}
##### Backward-incompatible changes[¶](#id138 "Permalink to this heading"){.headerlink}

-   Changed [`scrapy.utils.response.get_meta_refresh()`{.docutils
    .literal .notranslate}]{.pre} signature
    ([r1804](http://hg.scrapy.org/scrapy/changeset/1804){.reference
    .external})

-   Removed deprecated [`scrapy.item.ScrapedItem`{.docutils .literal
    .notranslate}]{.pre} class - use [`scrapy.item.Item`{.docutils
    .literal .notranslate}]{.pre}` `{.docutils .literal
    .notranslate}[`instead`{.docutils .literal .notranslate}]{.pre}
    ([r1838](http://hg.scrapy.org/scrapy/changeset/1838){.reference
    .external})

-   Removed deprecated [`scrapy.xpath`{.docutils .literal
    .notranslate}]{.pre} module - use [`scrapy.selector`{.docutils
    .literal .notranslate}]{.pre} instead.
    ([r1836](http://hg.scrapy.org/scrapy/changeset/1836){.reference
    .external})

-   Removed deprecated [`core.signals.domain_open`{.docutils .literal
    .notranslate}]{.pre} signal - use
    [`core.signals.domain_opened`{.docutils .literal
    .notranslate}]{.pre} instead
    ([r1822](http://hg.scrapy.org/scrapy/changeset/1822){.reference
    .external})

-   

    [`log.msg()`{.docutils .literal .notranslate}]{.pre} now receives a [`spider`{.docutils .literal .notranslate}]{.pre} argument ([r1822](http://hg.scrapy.org/scrapy/changeset/1822){.reference .external})

    :   -   Old domain argument has been deprecated and will be removed
            in 0.9. For spiders, you should always use the
            [`spider`{.docutils .literal .notranslate}]{.pre} argument
            and pass spider references. If you really want to pass a
            string, use the [`component`{.docutils .literal
            .notranslate}]{.pre} argument instead.

-   Changed core signals [`domain_opened`{.docutils .literal
    .notranslate}]{.pre}, [`domain_closed`{.docutils .literal
    .notranslate}]{.pre}, [`domain_idle`{.docutils .literal
    .notranslate}]{.pre}

-   

    Changed Item pipeline to use spiders instead of domains

    :   -   The [`domain`{.docutils .literal .notranslate}]{.pre}
            argument of [`process_item()`{.docutils .literal
            .notranslate}]{.pre} item pipeline method was changed to
            [`spider`{.docutils .literal .notranslate}]{.pre}, the new
            signature is: [`process_item(spider,`{.docutils .literal
            .notranslate}]{.pre}` `{.docutils .literal
            .notranslate}[`item)`{.docutils .literal
            .notranslate}]{.pre}
            ([r1827](http://hg.scrapy.org/scrapy/changeset/1827){.reference
            .external} \| #105)

        -   To quickly port your code (to work with Scrapy 0.8) just use
            [`spider.domain_name`{.docutils .literal
            .notranslate}]{.pre} where you previously used
            [`domain`{.docutils .literal .notranslate}]{.pre}.

-   

    Changed Stats API to use spiders instead of domains ([r1849](http://hg.scrapy.org/scrapy/changeset/1849){.reference .external} \| #113)

    :   -   [`StatsCollector`{.docutils .literal .notranslate}]{.pre}
            was changed to receive spider references (instead of
            domains) in its methods ([`set_value`{.docutils .literal
            .notranslate}]{.pre}, [`inc_value`{.docutils .literal
            .notranslate}]{.pre}, etc).

        -   added [`StatsCollector.iter_spider_stats()`{.docutils
            .literal .notranslate}]{.pre} method

        -   removed [`StatsCollector.list_domains()`{.docutils .literal
            .notranslate}]{.pre} method

        -   Also, Stats signals were renamed and now pass around spider
            references (instead of domains). Here's a summary of the
            changes:

        -   To quickly port your code (to work with Scrapy 0.8) just use
            [`spider.domain_name`{.docutils .literal
            .notranslate}]{.pre} where you previously used
            [`domain`{.docutils .literal .notranslate}]{.pre}.
            [`spider_stats`{.docutils .literal .notranslate}]{.pre}
            contains exactly the same data as [`domain_stats`{.docutils
            .literal .notranslate}]{.pre}.

-   

    [`CloseDomain`{.docutils .literal .notranslate}]{.pre} extension moved to [`scrapy.contrib.closespider.CloseSpider`{.docutils .literal .notranslate}]{.pre} ([r1833](http://hg.scrapy.org/scrapy/changeset/1833){.reference .external})

    :   -   

            Its settings were also renamed:

            :   -   [`CLOSEDOMAIN_TIMEOUT`{.docutils .literal
                    .notranslate}]{.pre} to
                    [`CLOSESPIDER_TIMEOUT`{.docutils .literal
                    .notranslate}]{.pre}

                -   [`CLOSEDOMAIN_ITEMCOUNT`{.docutils .literal
                    .notranslate}]{.pre} to
                    [`CLOSESPIDER_ITEMCOUNT`{.docutils .literal
                    .notranslate}]{.pre}

-   Removed deprecated [`SCRAPYSETTINGS_MODULE`{.docutils .literal
    .notranslate}]{.pre} environment variable - use
    [`SCRAPY_SETTINGS_MODULE`{.docutils .literal .notranslate}]{.pre}
    instead
    ([r1840](http://hg.scrapy.org/scrapy/changeset/1840){.reference
    .external})

-   Renamed setting: [`REQUESTS_PER_DOMAIN`{.docutils .literal
    .notranslate}]{.pre} to [`CONCURRENT_REQUESTS_PER_SPIDER`{.docutils
    .literal .notranslate}]{.pre}
    ([r1830](http://hg.scrapy.org/scrapy/changeset/1830){.reference
    .external},
    [r1844](http://hg.scrapy.org/scrapy/changeset/1844){.reference
    .external})

-   Renamed setting: [`CONCURRENT_DOMAINS`{.docutils .literal
    .notranslate}]{.pre} to [`CONCURRENT_SPIDERS`{.docutils .literal
    .notranslate}]{.pre}
    ([r1830](http://hg.scrapy.org/scrapy/changeset/1830){.reference
    .external})

-   Refactored HTTP Cache middleware

-   HTTP Cache middleware has been heavily refactored, retaining the
    same functionality except for the domain sectorization which was
    removed.
    ([r1843](http://hg.scrapy.org/scrapy/changeset/1843){.reference
    .external} )

-   Renamed exception: [`DontCloseDomain`{.docutils .literal
    .notranslate}]{.pre} to [`DontCloseSpider`{.docutils .literal
    .notranslate}]{.pre}
    ([r1859](http://hg.scrapy.org/scrapy/changeset/1859){.reference
    .external} \| #120)

-   Renamed extension: [`DelayedCloseDomain`{.docutils .literal
    .notranslate}]{.pre} to [`SpiderCloseDelay`{.docutils .literal
    .notranslate}]{.pre}
    ([r1861](http://hg.scrapy.org/scrapy/changeset/1861){.reference
    .external} \| #121)

-   Removed obsolete
    [`scrapy.utils.markup.remove_escape_chars`{.docutils .literal
    .notranslate}]{.pre} function - use
    [`scrapy.utils.markup.replace_escape_chars`{.docutils .literal
    .notranslate}]{.pre} instead
    ([r1865](http://hg.scrapy.org/scrapy/changeset/1865){.reference
    .external})
:::
:::

::: {#scrapy-0-7 .section}
#### Scrapy 0.7[¶](#scrapy-0-7 "Permalink to this heading"){.headerlink}

First release of Scrapy.
:::
:::

[]{#document-contributing}

::: {#contributing-to-scrapy .section}
[]{#topics-contributing}

### Contributing to Scrapy[¶](#contributing-to-scrapy "Permalink to this heading"){.headerlink}

::: {.admonition .important}
Important

Double check that you are reading the most recent version of this
document at
[https://docs.scrapy.org/en/master/contributing.html](https://docs.scrapy.org/en/master/contributing.html){.reference
.external}
:::

There are many ways to contribute to Scrapy. Here are some of them:

-   Report bugs and request features in the [issue
    tracker](https://github.com/scrapy/scrapy/issues){.reference
    .external}, trying to follow the guidelines detailed in [Reporting
    bugs](#reporting-bugs){.reference .internal} below.

-   Submit patches for new functionalities and/or bug fixes. Please read
    [[Writing patches]{.std .std-ref}](#writing-patches){.hoverxref
    .tooltip .reference .internal} and [Submitting
    patches](#id2){.reference .internal} below for details on how to
    write and submit a patch.

-   Blog about Scrapy. Tell the world how you're using Scrapy. This will
    help newcomers with more examples and will help the Scrapy project
    to increase its visibility.

-   Join the [Scrapy subreddit](https://reddit.com/r/scrapy){.reference
    .external} and share your ideas on how to improve Scrapy. We're
    always open to suggestions.

-   Answer Scrapy questions at [Stack
    Overflow](https://stackoverflow.com/questions/tagged/scrapy){.reference
    .external}.

::: {#reporting-bugs .section}
#### Reporting bugs[¶](#reporting-bugs "Permalink to this heading"){.headerlink}

::: {.admonition .note}
Note

Please report security issues **only** to
[scrapy-security@googlegroups.com](mailto:scrapy-security%40googlegroups.com){.reference
.external}. This is a private list only open to trusted Scrapy
developers, and its archives are not public.
:::

Well-written bug reports are very helpful, so keep in mind the following
guidelines when you're going to report a new bug.

-   check the [[FAQ]{.std .std-ref}](index.html#faq){.hoverxref .tooltip
    .reference .internal} first to see if your issue is addressed in a
    well-known question

-   if you have a general question about Scrapy usage, please ask it at
    [Stack
    Overflow](https://stackoverflow.com/questions/tagged/scrapy){.reference
    .external} (use "scrapy" tag).

-   check the [open
    issues](https://github.com/scrapy/scrapy/issues){.reference
    .external} to see if the issue has already been reported. If it has,
    don't dismiss the report, but check the ticket history and comments.
    If you have additional useful information, please leave a comment,
    or consider [[sending a pull request]{.std
    .std-ref}](#writing-patches){.hoverxref .tooltip .reference
    .internal} with a fix.

-   search the
    [scrapy-users](https://groups.google.com/forum/#!forum/scrapy-users){.reference
    .external} list and [Scrapy
    subreddit](https://reddit.com/r/scrapy){.reference .external} to see
    if it has been discussed there, or if you're not sure if what you're
    seeing is a bug. You can also ask in the [`#scrapy`{.docutils
    .literal .notranslate}]{.pre} IRC channel.

-   write **complete, reproducible, specific bug reports**. The smaller
    the test case, the better. Remember that other developers won't have
    your project to reproduce the bug, so please include all relevant
    files required to reproduce it. See for example StackOverflow's
    guide on creating a [Minimal, Complete, and Verifiable
    example](https://stackoverflow.com/help/mcve){.reference .external}
    exhibiting the issue.

-   the most awesome way to provide a complete reproducible example is
    to send a pull request which adds a failing test case to the Scrapy
    testing suite (see [[Submitting patches]{.std
    .std-ref}](#submitting-patches){.hoverxref .tooltip .reference
    .internal}). This is helpful even if you don't have an intention to
    fix the issue yourselves.

-   include the output of [`scrapy`{.docutils .literal
    .notranslate}]{.pre}` `{.docutils .literal
    .notranslate}[`version`{.docutils .literal
    .notranslate}]{.pre}` `{.docutils .literal
    .notranslate}[`-v`{.docutils .literal .notranslate}]{.pre} so
    developers working on your bug know exactly which version and
    platform it occurred on, which is often very helpful for reproducing
    it, or knowing if it was already fixed.
:::

::: {#writing-patches .section}
[]{#id1}

#### Writing patches[¶](#writing-patches "Permalink to this heading"){.headerlink}

Scrapy has a list of [good first
issues](https://github.com/scrapy/scrapy/issues?q=is%3Aissue+is%3Aopen+label%3A%22good+first+issue%22){.reference
.external} and [help wanted
issues](https://github.com/scrapy/scrapy/issues?q=is%3Aissue+is%3Aopen+label%3A%22help+wanted%22){.reference
.external} that you can work on. These issues are a great way to get
started with contributing to Scrapy. If you're new to the codebase, you
may want to focus on documentation or testing-related issues, as they
are always useful and can help you get more familiar with the project.
You can also check Scrapy's [test
coverage](https://app.codecov.io/gh/scrapy/scrapy){.reference .external}
to see which areas may benefit from more tests.

The better a patch is written, the higher the chances that it'll get
accepted and the sooner it will be merged.

Well-written patches should:

-   contain the minimum amount of code required for the specific change.
    Small patches are easier to review and merge. So, if you're doing
    more than one change (or bug fix), please consider submitting one
    patch per change. Do not collapse multiple changes into a single
    patch. For big changes consider using a patch queue.

-   pass all unit-tests. See [Running tests](#id6){.reference .internal}
    below.

-   include one (or more) test cases that check the bug fixed or the new
    functionality added. See [Writing tests](#writing-tests){.reference
    .internal} below.

-   if you're adding or changing a public (documented) API, please
    include the documentation changes in the same patch. See
    [Documentation policies](#id5){.reference .internal} below.

-   if you're adding a private API, please add a regular expression to
    the [`coverage_ignore_pyobjects`{.docutils .literal
    .notranslate}]{.pre} variable of [`docs/conf.py`{.docutils .literal
    .notranslate}]{.pre} to exclude the new private API from
    documentation coverage checks.

    To see if your private API is skipped properly, generate a
    documentation coverage report as follows:

    ::: {.highlight-default .notranslate}
    ::: highlight
        tox -e docs-coverage
    :::
    :::

-   if you are removing deprecated code, first make sure that at least 1
    year (12 months) has passed since the release that introduced the
    deprecation. See [[Deprecation policy]{.std
    .std-ref}](index.html#deprecation-policy){.hoverxref .tooltip
    .reference .internal}.
:::

::: {#submitting-patches .section}
[]{#id2}

#### Submitting patches[¶](#submitting-patches "Permalink to this heading"){.headerlink}

The best way to submit a patch is to issue a [pull
request](https://help.github.com/en/github/collaborating-with-issues-and-pull-requests/creating-a-pull-request){.reference
.external} on GitHub, optionally creating a new issue first.

Remember to explain what was fixed or the new functionality (what it is,
why it's needed, etc). The more info you include, the easier will be for
core developers to understand and accept your patch.

You can also discuss the new functionality (or bug fix) before creating
the patch, but it's always good to have a patch ready to illustrate your
arguments and show that you have put some additional thought into the
subject. A good starting point is to send a pull request on GitHub. It
can be simple enough to illustrate your idea, and leave
documentation/tests for later, after the idea has been validated and
proven useful. Alternatively, you can start a conversation in the
[Scrapy subreddit](https://reddit.com/r/scrapy){.reference .external} to
discuss your idea first.

Sometimes there is an existing pull request for the problem you'd like
to solve, which is stalled for some reason. Often the pull request is in
a right direction, but changes are requested by Scrapy maintainers, and
the original pull request author hasn't had time to address them. In
this case consider picking up this pull request: open a new pull request
with all commits from the original pull request, as well as additional
changes to address the raised issues. Doing so helps a lot; it is not
considered rude as long as the original author is acknowledged by
keeping his/her commits.

You can pull an existing pull request to a local branch by running
[`git`{.docutils .literal .notranslate}]{.pre}` `{.docutils .literal
.notranslate}[`fetch`{.docutils .literal
.notranslate}]{.pre}` `{.docutils .literal
.notranslate}[`upstream`{.docutils .literal
.notranslate}]{.pre}` `{.docutils .literal
.notranslate}[`pull/$PR_NUMBER/head:$BRANCH_NAME_TO_CREATE`{.docutils
.literal .notranslate}]{.pre} (replace 'upstream' with a remote name for
scrapy repository, [`$PR_NUMBER`{.docutils .literal .notranslate}]{.pre}
with an ID of the pull request, and [`$BRANCH_NAME_TO_CREATE`{.docutils
.literal .notranslate}]{.pre} with a name of the branch you want to
create locally). See also:
[https://help.github.com/en/github/collaborating-with-issues-and-pull-requests/checking-out-pull-requests-locally#modifying-an-inactive-pull-request-locally](https://help.github.com/en/github/collaborating-with-issues-and-pull-requests/checking-out-pull-requests-locally#modifying-an-inactive-pull-request-locally){.reference
.external}.

When writing GitHub pull requests, try to keep titles short but
descriptive. E.g. For bug #411: "Scrapy hangs if an exception raises in
start_requests" prefer "Fix hanging when exception occurs in
start_requests (#411)" instead of "Fix for #411". Complete titles make
it easy to skim through the issue tracker.

Finally, try to keep aesthetic changes ([]{#index-0 .target}[**PEP
8**](https://peps.python.org/pep-0008/){.pep .reference .external}
compliance, unused imports removal, etc) in separate commits from
functional changes. This will make pull requests easier to review and
more likely to get merged.
:::

::: {#coding-style .section}
[]{#id3}

#### Coding style[¶](#coding-style "Permalink to this heading"){.headerlink}

Please follow these coding conventions when writing code for inclusion
in Scrapy:

-   We use [black](https://black.readthedocs.io/en/stable/){.reference
    .external} for code formatting. There is a hook in the pre-commit
    config that will automatically format your code before every commit.
    You can also run black manually with [`tox`{.docutils .literal
    .notranslate}]{.pre}` `{.docutils .literal
    .notranslate}[`-e`{.docutils .literal
    .notranslate}]{.pre}` `{.docutils .literal
    .notranslate}[`black`{.docutils .literal .notranslate}]{.pre}.

-   Don't put your name in the code you contribute; git provides enough
    metadata to identify author of the code. See
    [https://help.github.com/en/github/using-git/setting-your-username-in-git](https://help.github.com/en/github/using-git/setting-your-username-in-git){.reference
    .external} for setup instructions.
:::

::: {#pre-commit .section}
[]{#scrapy-pre-commit}

#### Pre-commit[¶](#pre-commit "Permalink to this heading"){.headerlink}

We use [pre-commit](https://pre-commit.com/){.reference .external} to
automatically address simple code issues before every commit.

After your create a local clone of your fork of the Scrapy repository:

1.  [Install
    pre-commit](https://pre-commit.com/#installation){.reference
    .external}.

2.  On the root of your local clone of the Scrapy repository, run the
    following command:

    ::: {.highlight-bash .notranslate}
    ::: highlight
        pre-commit install
    :::
    :::

Now pre-commit will check your changes every time you create a Git
commit. Upon finding issues, pre-commit aborts your commit, and either
fixes those issues automatically, or only reports them to you. If it
fixes those issues automatically, creating your commit again should
succeed. Otherwise, you may need to address the corresponding issues
manually first.
:::

::: {#documentation-policies .section}
[]{#id5}

#### Documentation policies[¶](#documentation-policies "Permalink to this heading"){.headerlink}

For reference documentation of API members (classes, methods, etc.) use
docstrings and make sure that the Sphinx documentation uses the
[[`autodoc`{.xref .py .py-mod .docutils .literal
.notranslate}]{.pre}](https://www.sphinx-doc.org/en/master/usage/extensions/autodoc.html#module-sphinx.ext.autodoc "(in Sphinx v7.3.0)"){.reference
.external} extension to pull the docstrings. API reference documentation
should follow docstring conventions ([PEP
257](https://www.python.org/dev/peps/pep-0257/){.reference .external})
and be IDE-friendly: short, to the point, and it may provide short
examples.

Other types of documentation, such as tutorials or topics, should be
covered in files within the [`docs/`{.docutils .literal
.notranslate}]{.pre} directory. This includes documentation that is
specific to an API member, but goes beyond API reference documentation.

In any case, if something is covered in a docstring, use the
[[`autodoc`{.xref .py .py-mod .docutils .literal
.notranslate}]{.pre}](https://www.sphinx-doc.org/en/master/usage/extensions/autodoc.html#module-sphinx.ext.autodoc "(in Sphinx v7.3.0)"){.reference
.external} extension to pull the docstring into the documentation
instead of duplicating the docstring in files within the
[`docs/`{.docutils .literal .notranslate}]{.pre} directory.

Documentation updates that cover new or modified features must use
Sphinx's [[`versionadded`{.xref .rst .rst-dir .docutils .literal
.notranslate}]{.pre}](https://www.sphinx-doc.org/en/master/usage/restructuredtext/directives.html#directive-versionadded "(in Sphinx v7.3.0)"){.reference
.external} and [[`versionchanged`{.xref .rst .rst-dir .docutils .literal
.notranslate}]{.pre}](https://www.sphinx-doc.org/en/master/usage/restructuredtext/directives.html#directive-versionchanged "(in Sphinx v7.3.0)"){.reference
.external} directives. Use [`VERSION`{.docutils .literal
.notranslate}]{.pre} as version, we will replace it with the actual
version right before the corresponding release. When we release a new
major or minor version of Scrapy, we remove these directives if they are
older than 3 years.

Documentation about deprecated features must be removed as those
features are deprecated, so that new readers do not run into it. New
deprecations and deprecation removals are documented in the [[release
notes]{.std .std-ref}](index.html#news){.hoverxref .tooltip .reference
.internal}.
:::

::: {#tests .section}
#### Tests[¶](#tests "Permalink to this heading"){.headerlink}

Tests are implemented using the [[Twisted unit-testing framework]{.xref
.std
.std-doc}](https://docs.twisted.org/en/stable/development/test-standard.html "(in Twisted v23.10)"){.reference
.external}. Running tests requires [[tox]{.xref .std
.std-doc}](https://tox.wiki/en/latest/index.html "(in Python v4.11)"){.reference
.external}.

::: {#running-tests .section}
[]{#id6}

##### Running tests[¶](#running-tests "Permalink to this heading"){.headerlink}

To run all tests:

::: {.highlight-default .notranslate}
::: highlight
    tox
:::
:::

To run a specific test (say [`tests/test_loader.py`{.docutils .literal
.notranslate}]{.pre}) use:

> <div>
>
> [`tox`{.docutils .literal .notranslate}]{.pre}` `{.docutils .literal
> .notranslate}[`--`{.docutils .literal
> .notranslate}]{.pre}` `{.docutils .literal
> .notranslate}[`tests/test_loader.py`{.docutils .literal
> .notranslate}]{.pre}
>
> </div>

To run the tests on a specific [[tox]{.xref .std
.std-doc}](https://tox.wiki/en/latest/index.html "(in Python v4.11)"){.reference
.external} environment, use [`-e`{.docutils .literal
.notranslate}]{.pre}` `{.docutils .literal
.notranslate}[`<name>`{.docutils .literal .notranslate}]{.pre} with an
environment name from [`tox.ini`{.docutils .literal
.notranslate}]{.pre}. For example, to run the tests with Python 3.10
use:

::: {.highlight-default .notranslate}
::: highlight
    tox -e py310
:::
:::

You can also specify a comma-separated list of environments, and use
[[tox's parallel mode]{.xref .std
.std-ref}](https://tox.wiki/en/latest/user_guide.html#parallel-mode "(in Python v4.11)"){.reference
.external} to run the tests on multiple environments in parallel:

::: {.highlight-default .notranslate}
::: highlight
    tox -e py39,py310 -p auto
:::
:::

To pass command-line options to [[pytest]{.xref .std
.std-doc}](https://docs.pytest.org/en/latest/index.html "(in pytest v0.1.dev83+g81c06b3)"){.reference
.external}, add them after [`--`{.docutils .literal .notranslate}]{.pre}
in your call to [[tox]{.xref .std
.std-doc}](https://tox.wiki/en/latest/index.html "(in Python v4.11)"){.reference
.external}. Using [`--`{.docutils .literal .notranslate}]{.pre}
overrides the default positional arguments defined in
[`tox.ini`{.docutils .literal .notranslate}]{.pre}, so you must include
those default positional arguments ([`scrapy`{.docutils .literal
.notranslate}]{.pre}` `{.docutils .literal
.notranslate}[`tests`{.docutils .literal .notranslate}]{.pre}) after
[`--`{.docutils .literal .notranslate}]{.pre} as well:

::: {.highlight-default .notranslate}
::: highlight
    tox -- scrapy tests -x  # stop after first failure
:::
:::

You can also use the
[pytest-xdist](https://github.com/pytest-dev/pytest-xdist){.reference
.external} plugin. For example, to run all tests on the Python 3.10
[[tox]{.xref .std
.std-doc}](https://tox.wiki/en/latest/index.html "(in Python v4.11)"){.reference
.external} environment using all your CPU cores:

::: {.highlight-default .notranslate}
::: highlight
    tox -e py310 -- scrapy tests -n auto
:::
:::

To see coverage report install [[coverage]{.xref .std
.std-doc}](https://coverage.readthedocs.io/en/latest/index.html "(in Coverage.py v7.3.2)"){.reference
.external} ([`pip`{.docutils .literal .notranslate}]{.pre}` `{.docutils
.literal .notranslate}[`install`{.docutils .literal
.notranslate}]{.pre}` `{.docutils .literal
.notranslate}[`coverage`{.docutils .literal .notranslate}]{.pre}) and
run:

> <div>
>
> [`coverage`{.docutils .literal .notranslate}]{.pre}` `{.docutils
> .literal .notranslate}[`report`{.docutils .literal
> .notranslate}]{.pre}
>
> </div>

see output of [`coverage`{.docutils .literal
.notranslate}]{.pre}` `{.docutils .literal
.notranslate}[`--help`{.docutils .literal .notranslate}]{.pre} for more
options like html or xml report.
:::

::: {#writing-tests .section}
##### Writing tests[¶](#writing-tests "Permalink to this heading"){.headerlink}

All functionality (including new features and bug fixes) must include a
test case to check that it works as expected, so please include tests
for your patches if you want them to get accepted sooner.

Scrapy uses unit-tests, which are located in the
[tests/](https://github.com/scrapy/scrapy/tree/master/tests){.reference
.external} directory. Their module name typically resembles the full
path of the module they're testing. For example, the item loaders code
is in:

::: {.highlight-default .notranslate}
::: highlight
    scrapy.loader
:::
:::

And their unit-tests are in:

::: {.highlight-default .notranslate}
::: highlight
    tests/test_loader.py
:::
:::
:::
:::
:::

[]{#document-versioning}

::: {#versioning-and-api-stability .section}
[]{#versioning}

### Versioning and API stability[¶](#versioning-and-api-stability "Permalink to this heading"){.headerlink}

::: {#id1 .section}
#### Versioning[¶](#id1 "Permalink to this heading"){.headerlink}

There are 3 numbers in a Scrapy version: *A.B.C*

-   *A* is the major version. This will rarely change and will signify
    very large changes.

-   *B* is the release number. This will include many changes including
    features and things that possibly break backward compatibility,
    although we strive to keep these cases at a minimum.

-   *C* is the bugfix release number.

Backward-incompatibilities are explicitly mentioned in the [[release
notes]{.std .std-ref}](index.html#news){.hoverxref .tooltip .reference
.internal}, and may require special attention before upgrading.

Development releases do not follow 3-numbers version and are generally
released as [`dev`{.docutils .literal .notranslate}]{.pre} suffixed
versions, e.g. [`1.3dev`{.docutils .literal .notranslate}]{.pre}.

::: {.admonition .note}
Note

With Scrapy 0.\* series, Scrapy used [odd-numbered versions for
development
releases](https://en.wikipedia.org/wiki/Software_versioning#Odd-numbered_versions_for_development_releases){.reference
.external}. This is not the case anymore from Scrapy 1.0 onwards.

Starting with Scrapy 1.0, all releases should be considered
production-ready.
:::

For example:

-   *1.1.1* is the first bugfix release of the *1.1* series (safe to use
    in production)
:::

::: {#api-stability .section}
#### API stability[¶](#api-stability "Permalink to this heading"){.headerlink}

API stability was one of the major goals for the *1.0* release.

Methods or functions that start with a single dash ([`_`{.docutils
.literal .notranslate}]{.pre}) are private and should never be relied as
stable.

Also, keep in mind that stable doesn't mean complete: stable APIs could
grow new methods or functionality but the existing methods should keep
working the same way.
:::

::: {#deprecation-policy .section}
[]{#id2}

#### Deprecation policy[¶](#deprecation-policy "Permalink to this heading"){.headerlink}

We aim to maintain support for deprecated Scrapy features for at least 1
year.

For example, if a feature is deprecated in a Scrapy version released on
June 15th 2020, that feature should continue to work in versions
released on June 14th 2021 or before that.

Any new Scrapy release after a year *may* remove support for that
deprecated feature.

All deprecated features removed in a Scrapy release are explicitly
mentioned in the [[release notes]{.std
.std-ref}](index.html#news){.hoverxref .tooltip .reference .internal}.
:::
:::
:::

[[Release notes]{.doc}](index.html#document-news){.reference .internal}

:   See what has changed in recent Scrapy versions.

[[Contributing to Scrapy]{.doc}](index.html#document-contributing){.reference .internal}

:   Learn how to contribute to the Scrapy project.

[[Versioning and API stability]{.doc}](index.html#document-versioning){.reference .internal}

:   Understand Scrapy versioning and API stability.
:::
:::
:::
:::

------------------------------------------------------------------------

::: {role="contentinfo"}
© Copyright 2008--2023, Scrapy developers. [Revision `70ba3a08`.
]{.commit} [Last updated on Nov 30, 2023. ]{.lastupdated}
:::

Built with [Sphinx](https://www.sphinx-doc.org/) using a
[theme](https://github.com/readthedocs/sphinx_rtd_theme) provided by
[Read the Docs](https://readthedocs.org).
:::
:::
:::
:::

::: {.rst-versions toggle="rst-versions" role="note" aria-label="Versions"}
[ [ Read the Docs]{.fa .fa-book} v: master []{.fa .fa-caret-down}
]{.rst-current-version toggle="rst-current-version"}

::: rst-other-versions

Versions
:   [master](/en/master/)
:   [latest](/en/latest/)
:   [stable](/en/stable/)
:   [2.11](/en/2.11/)
:   [2.10](/en/2.10/)
:   [2.9](/en/2.9/)
:   [2.8](/en/2.8/)
:   [2.7](/en/2.7/)
:   [2.6](/en/2.6/)
:   [2.5](/en/2.5/)
:   [2.4](/en/2.4/)
:   [2.3](/en/2.3/)
:   [2.2](/en/2.2/)
:   [2.1](/en/2.1/)
:   [2.0](/en/2.0/)
:   [1.8](/en/1.8/)
:   [1.7](/en/1.7/)
:   [1.6](/en/1.6/)
:   [1.5](/en/1.5/)
:   [1.4](/en/1.4/)
:   [1.3](/en/1.3/)
:   [1.2](/en/1.2/)
:   [1.1](/en/1.1/)
:   [1.0](/en/1.0/)
:   [0.24](/en/0.24/)
:   [0.22](/en/0.22/)
:   [0.20](/en/0.20/)
:   [0.18](/en/0.18/)
:   [0.16](/en/0.16/)
:   [0.14](/en/0.14/)
:   [0.12](/en/0.12/)
:   [0.10.3](/en/0.10.3/)
:   [0.9](/en/0.9/)
:   [xpath-tutorial](/en/xpath-tutorial/)

```{=html}
<!-- -->
```

Downloads
:   [pdf](//docs.scrapy.org/_/downloads/en/master/pdf/)
:   [html](//docs.scrapy.org/_/downloads/en/master/htmlzip/)
:   [epub](//docs.scrapy.org/_/downloads/en/master/epub/)

```{=html}
<!-- -->
```

On Read the Docs
:   [Project Home](//readthedocs.org/projects/scrapy/?fromdocs=scrapy)
:   [Builds](//readthedocs.org/builds/scrapy/?fromdocs=scrapy)
:::
:::
